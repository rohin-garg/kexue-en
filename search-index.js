window.SEARCH_INDEX = [{"file": "translation_10001.html", "title": "Configuring different learning rates, can LoRA gain a bit more?", "content": "← Back to Index Configuring different learning rates, can LoRA gain a bit more? By 苏剑林 | February 27, 2024 LoRA (Low-Rank Adaptation) is one of the current parameter-efficient fine-tuning methods for LLMs. We previously had a brief discussion in \"LoRA from a Gradient Perspective: Introduction, Analysis, Conjectures, and Generalization\" . In this article, we will study a new conclusion regarding LoRA: By assigning different learning rates to the two matrices in LoRA, the effectiveness of LoRA can be further improved. This conclusion comes from the recent paper \"LoRA+: Efficient Low Rank Adaptation of Large Models\" (hereinafter referred to as \"LoRA+\"). At first glance, this conclusion might not seem particularly special, because configuring different learning rates is equivalent to introducing new hyperparameters, and generally, fine-tuning additional hyperparameters leads to improvements. However, the significance of \"LoRA+\" lies in the fact that it validates this necessity from a theoretical perspective and determines that the optimal solution inevitably requires the learning rate of the right matrix to be greater than that of the left matrix. In short, \"LoRA+\" is a classic example"}, {"file": "translation_10007.html", "title": "Fitting One-Dimensional Probability Density Functions with Fourier Series", "content": "← Back to Index Fitting One-Dimensional Probability Density Functions with Fourier Series By 苏剑林 | March 07, 2024 In \"Musings on Multi-modal Approaches (Part 1): Lossless Input\" , we mentioned that the fundamental difficulty of image generation is that there is no universal fitter for continuous probability densities. Of course, one cannot say there are none at all; for example, Gaussian Mixture Models (GMM) can theoretically fit any probability density, and even GANs can essentially be understood as GMMs mixing an infinite number of Gaussian models. However, although GMM's theoretical capacity is sufficient, its Maximum Likelihood Estimation is very difficult, especially because it is usually not suitable for gradient-based optimizers, which limits its application scenarios. Recently, a new paper from Google, \"Fourier Basis Density Model\" , proposed a new solution for the one-dimensional case—using Fourier series for fitting. The analysis process in the paper is quite interesting, and the construction form is very clever, making it well worth studying. Problem Description Some readers might question: what is the value of studying only the one-dimensional case? Indeed, if only imag"}, {"file": "translation_10017.html", "title": "Chapter of Space and Time: Viewing Attention as an RNN with Squared Complexity", "content": "← Back to Index Chapter of Space and Time: Viewing Attention as an RNN with Squared Complexity By 苏剑林 | March 18, 2024 In recent years, Recurrent Neural Networks (RNNs) have regained significant interest among researchers and users due to their linear training and inference efficiency, suggesting a sort of \"renaissance.\" Representative works include RWKV , RetNet , and Mamba . When RNNs are used for language modeling, their typical characteristic is that each generation step has constant space and time complexity; for the entire sequence, this results in constant space complexity and linear time complexity. Of course, everything has two sides. Compared to the dynamically growing KV Cache of Attention, the constant space complexity of RNNs often leads people to suspect a finite memory capacity, making it difficult for them to match Attention's performance on Long Context tasks. In this article, we show that Causal Attention can be rewritten in the form of an RNN, and its generation at each step can theoretically be performed with $\\mathcal{O}(1)$ space complexity (at the cost of extremely high time complexity, far exceeding squared complexity). This indicates that the advantage of A"}, {"file": "translation_10040.html", "title": "Transformer Upgrade Road: 17. Simple Reflections on Multimodal Position Encoding", "content": "← Back to Index Transformer Upgrade Road: 17. Simple Reflections on Multimodal Position Encoding By 苏剑林 | March 29, 2024 In the second article of this series, \"Transformer Upgrade Road: 2. Rotary Position Embedding (RoPE) and Its Advantages,\" I proposed Rotary Position Embedding (RoPE)—a scheme that implements relative position encoding through an absolute position form. Initially, RoPE was designed for one-dimensional sequences such as text and audio (RoPE-1D). Later, in \"Transformer Upgrade Road: 4. Rotary Position Embedding for 2D Positions,\" we extended it to two-dimensional sequences (RoPE-2D), which is applicable to the ViT (Vision Transformer) for images. However, whether it is RoPE-1D or RoPE-2D, their common characteristic is a single modality—scenarios with either pure text or pure image inputs. So, for multimodal scenarios like mixed text-image inputs, how should RoPE be adjusted? I searched for related literature and found few works discussing this problem. The mainstream approach seems to be directly flattening all inputs and treating them as a one-dimensional input to apply RoPE-1D; consequently, even RoPE-2D is rarely seen. Not to mention whether this practice will b"}, {"file": "translation_10047.html", "title": "Generating Diffusion Models Chat (22): SNR and High-Resolution Image Generation (Part 1)", "content": "← Back to Index Generating Diffusion Models Chat (22): SNR and High-Resolution Image Generation (Part 1) By 苏剑林 | April 08, 2024 If we take stock of mainstream image diffusion model works, we will find a common characteristic: currently, most work on high-resolution image generation (hereinafter referred to as \"large image generation\") is carried out by first transforming into a Latent space through an Encoder (i.e., LDM, Latent Diffusion Model ). Diffusion models trained directly in the original Pixel space mostly have resolutions not exceeding 64*64, and coincidentally, the Latent transformed by LDM through an AutoEncoder usually does not exceed 64*64 either. This naturally leads to a series of questions: Is there an inherent difficulty in high-resolution generation for diffusion models? Can high-resolution images be generated directly in Pixel space? The paper \"Simple diffusion: End-to-end diffusion for high resolution images\" attempts to answer this question. It analyzes the difficulty of large image generation through the concept of \"Signal-to-Noise Ratio\" (SNR) and uses it to optimize the noise schedule. At the same time, it proposes techniques such as scaling up the architec"}, {"file": "translation_10055.html", "title": "Generating Diffusion Model Conversations (23): SNR and Large Image Generation (Part 2)", "content": "← Back to Index Generating Diffusion Model Conversations (23): SNR and Large Image Generation (Part 2) By 苏剑林 | April 17, 2024 In the previous article , \"Generating Diffusion Model Conversations (22): SNR and Large Image Generation (Part 1)\", we introduced how to improve the noise schedule by aligning low-resolution signal-to-noise ratios (SNR), thereby enhancing the performance of diffusion models for high-resolution image generation trained directly in pixel space. The protagonist of this article is also SNR and high-resolution image generation (large image generation), but it achieves something even more astonishing—directly using a diffusion model trained on low-resolution images for high-resolution image generation without any additional training, with performance and inference costs comparable to models trained directly for large images! This work comes from the recent paper \"Upsample Guidance: Scale Up Diffusion Models without Training\" . It cleverly uses the upsampled output of a low-resolution model as a guidance signal and combines it with the translation invariance of CNNs for texture details, successfully achieving training-free high-resolution image generation. Concept"}, {"file": "translation_10077.html", "title": "Diffusion Model Discourse (24): Taking Fewer Shortcuts to Arrive Faster", "content": "← Back to Index Diffusion Model Discourse (24): Taking Fewer Shortcuts to Arrive Faster By 苏剑林 | April 23, 2024 How to reduce the number of sampling steps while ensuring generation quality is a key issue in the application of diffusion models. Among early efforts, \"Diffusion Model Discourse (4): DDIM = High-Perspective DDPM\" introduced DDIM as the first attempt at accelerated sampling. Later, works introduced in \"Diffusion Model Discourse (5): General Framework - SDE Edition\" and \"Diffusion Model Discourse (5): General Framework - ODE Edition\" connected diffusion models with SDEs and ODEs. Consequently, corresponding numerical integration techniques were directly applied to accelerate diffusion model sampling. Among these, the relatively simple ODE acceleration techniques are the most abundant, an example of which we introduced in \"Diffusion Model Discourse (21): Accelerating ODE Sampling with the Mean Value Theorem\" . In this article, we introduce another particularly simple and effective acceleration trick—Skip Tuning, from the paper \"The Surprising Effectiveness of Skip-Tuning in Diffusion Sampling\" . To be precise, it is used in conjunction with existing acceleration techniques"}, {"file": "translation_10085.html", "title": "Diffusion Model Notes (25): Identity-Based Distillation (Part I)", "content": "← Back to Index Diffusion Model Notes (25): Identity-Based Distillation (Part I) By 苏剑林 | May 01, 2024 Today, we share the paper \"Score identity Distillation: Exponentially Fast Distillation of Pretrained Diffusion Models for One-Step Generation\" . As the name suggests, this paper explores how to distill diffusion models faster and better. Even without prior experience in distillation, one can likely guess the conventional steps: randomly sample a large number of inputs, use the diffusion model to generate corresponding results as outputs, and then use these input-output pairs as training data to supervise the training of a new model. However, it is well known that the original diffusion model (the teacher) usually requires multiple steps (e.g., 1000 steps) of iteration to generate high-quality outputs. Therefore, regardless of internal training details, a significant drawback of this scheme is that generating training data is time-consuming and labor-intensive. Additionally, the student model after distillation usually suffers from some degree of performance loss. Is there a method that can solve these two drawbacks at once? This is exactly what the aforementioned paper attempts t"}, {"file": "translation_10088.html", "title": "Cool Papers Update: Simply Built an In-site Search System", "content": "← Back to Index Cool Papers Update: Simply Built an In-site Search System By 苏剑林 | May 07, 2024 Since \"A More Convenient Way to Open Cool Papers: Chrome Redirect Extension\" , Cool Papers has undergone two major changes. One is the introduction of the venue branch, which is gradually including the paper sets of various conferences over the years, such as ICLR, ICML, etc. This part is manually expanded dynamically, and readers are welcome to suggest more of their favorite conferences. The other change is the subject of this article: the new in-site search function added the day before yesterday. This article will briefly introduce the new features and summarize the process of building the in-site search system. Introduction On the homepage of Cool Papers, we see the search entry: Cool Papers (2024.05.07) The features of the search function are as follows: Only searches the 'title' and 'summary' fields; specifying other fields is not yet supported. You can specify to search either the arxiv branch or the venue branch; mixed searching across branches is not supported. Special characters (non-English letters and numbers) in the search query will be removed. Search query words are not au"}, {"file": "translation_10091.html", "title": "The Ultimate Tug-of-War between Cache and Performance: From MHA, MQA, GQA to MLA", "content": "← Back to Index The Ultimate Tug-of-War between Cache and Performance: From MHA, MQA, GQA to MLA By 苏剑林 | May 13, 2024 A few days ago, the release of DeepSeek-V2 by High-Flyer Quant sparked heated discussions. First, the most shocking aspect was the price—1 RMB per million tokens—which is generally two orders of magnitude cheaper than existing competitors' APIs. This led some to joke, \"At this price, even if it outputs gibberish, I would consider that gibberish a form of art.\" Secondly, according to the technical report, one of the key technologies behind this low price is the newly proposed MLA ( M ulti-head L atent A ttention). It is an improvement over GQA, claimed to be both more efficient and effective, which has drawn extensive attention from readers. In this article, we will trace the evolution from MHA, MQA, and GQA to MLA, with a particular focus on the design philosophy behind MLA. MHA MHA ( M ulti- H ead A ttention), or Multi-Head Attention, is the attention mechanism proposed in the pioneering work \"Attention is all you need\" . It can be said that it is the fundamental building block of current mainstream LLMs. Mathematically, MHA is equivalent to the concatenation of m"}, {"file": "translation_10114.html", "title": "Revisiting SSM (Part 1): Linear Systems and HiPPO Matrices", "content": "← Back to Index Revisiting SSM (Part 1): Linear Systems and HiPPO Matrices By 苏剑林 | May 24, 2024 A few days ago, I read several articles introducing SSM (State Space Models) and realized that I had never seriously understood the core of SSM. Therefore, I decided to study the relevant content of SSM in depth and, in passing, started this new series to record what I have learned. The concept of SSM has a long history, but here we specifically refer to SSM in the context of deep learning. Generally, the seminal work is considered to be S4 from 2021, which is not too old. The most recent and popular variant of SSM is likely last year's Mamba . Of course, when we talk about SSM, we might also refer broadly to all linear RNN models; in this sense, RWKV , RetNet , and the LRU we introduced in \"Google's New Work Attempts to 'Revive' RNN: Can RNN Shine Again?\" can all be categorized as such. Many SSM variants strive to become competitors to the Transformer. Although I do not believe there is a possibility of complete replacement, the elegant mathematical properties of SSM themselves are well worth studying. Although we say that SSM originated with S4, before S4, there was a very powerful fo"}, {"file": "translation_10122.html", "title": "Transformer Upgrade Roadmap: 18. Principles for Choosing the Base of RoPE", "content": "← Back to Index Transformer Upgrade Roadmap: 18. Principles for Choosing the Base of RoPE By 苏剑林 | May 29, 2024 We know that in RoPE , the formula for calculating frequency is $\\theta_i = b^{-2i/d}$, where the default value for the base $b$ is 10000. Currently, a mainstream approach for Long Context is to first pre-train on short text with $b=10000$, and then increase $b$ and fine-tune on long text. The starting point for this is NTK-RoPE, introduced in \"Transformer Upgrade Roadmap: 10. RoPE is a β-进制 Encoding\" , which inherently possesses good length extrapolation properties. Using a larger $b$ for fine-tuning results in a smaller initial loss and faster convergence compared to fine-tuning without modifications. This process gives the impression that increasing $b$ is entirely due to the \"short-then-long\" training strategy. If one always trained on long text, would there still be a need to increase $b$? A paper from last week, \"Base of RoPE Bounds Context Length\" , attempts to answer this question. It studies the lower bound of $b$ based on a certain desired property, pointing out that the training length itself necessitates choosing a larger base, regardless of the training strat"}, {"file": "translation_10137.html", "title": "Revisit SSM (II): Remaining Issues of HiPPO", "content": "← Back to Index Revisit SSM (II): Remaining Issues of HiPPO By 苏剑林 | June 5, 2024 Picking up from where we left off, in the previous article \"Revisit SSM (I): Linear Systems and the HiPPO Matrix\" , we discussed in detail the derivation of the HiPPO matrix within the HiPPO approximation framework. Its principle is to dynamically approximate a real-time updated function using orthogonal function bases. The dynamics of the projection coefficients happen to be a linear system, and if orthogonal polynomials are used as the basis, the core matrix of the linear system can be solved analytically; this matrix is known as the HiPPO matrix. Of course, the previous article focused on the derivation of the HiPPO matrix and did not further analyze its properties. Additionally, questions such as \"how to discretize it for application to actual data\" and \"whether bases other than polynomial bases can be solved analytically\" were not discussed in detail. Next, we will supplement the discussion on these related issues. Discretization Formats Assuming the reader has read and understood the content of the previous article, we will not provide excessive preamble here. In the previous article, we derived"}, {"file": "translation_10145.html", "title": "The Road to Probability Distributions: A Review of Softmax and Its Alternatives", "content": "← Back to Index The Road to Probability Distributions: A Review of Softmax and Its Alternatives By 苏剑林 | June 14, 2024 Whether in basic classification tasks or in the ubiquitous attention mechanisms of today, the construction of probability distributions is a critical step. Specifically, this involves converting an $n$-dimensional arbitrary vector into an $n$-element discrete probability distribution. As is well known, the standard answer to this problem is Softmax, which takes the form of exponential normalization. It is relatively simple and intuitive, while also possessing many elegant properties, making it the \"standard equipment\" in most scenarios. Nevertheless, Softmax has some unsatisfying aspects in certain contexts, such as being insufficiently sparse or unable to reach absolute zero. Consequently, many alternatives have emerged. In this article, we will briefly summarize the relevant properties of Softmax and review and compare some of its alternatives. Softmax Review First, let's introduce some general notation: $\\boldsymbol{x} = (x_1,x_2,\\cdots,x_n)\\in\\mathbb{R}^n$ is the $n$-dimensional vector to be converted into a probability distribution. Its components can be posit"}, {"file": "translation_10162.html", "title": "Revisiting SSM (III): Efficient Computation of HiPPO (S4)", "content": "← Back to Index Revisiting SSM (III): Efficient Computation of HiPPO (S4) By 苏剑林 | June 20, 2024 In the previous two articles, \"Revisiting SSM (I): Linear Systems and the HiPPO Matrix\" and \"Revisiting SSM (II): Some Remaining Issues of HiPPO\" , we introduced the core ideas and derivation of HiPPO—using orthogonal function bases to approximate continuously updating functions in real-time. The dynamics of the fitting coefficients can be expressed as a linear ODE system, and for specific bases and approximation methods, we can precisely calculate the key matrices of the linear system. Additionally, we discussed the discretization and properties of HiPPO. These contents laid the theoretical foundation for subsequent SSM (State Space Model) work. Next, we will introduce the follow-up application paper, \"Efficiently Modeling Long Sequences with Structured State Spaces\" (abbreviated as S4). It utilizes the derivation results of HiPPO as a basic tool for sequence modeling and explores efficient computation and training methods from a new perspective. Finally, it validates its effectiveness on many long-sequence modeling tasks, becoming one of the representative works in the revival of SSMs"}, {"file": "translation_10180.html", "title": "Revisiting SSM (IV): A New Perspective of Rational Generating Functions", "content": "← Back to Index Revisiting SSM (IV): A New Perspective of Rational Generating Functions By 苏剑林 | June 27, 2024 In the first three articles, we discussed most of the mathematical details of HiPPO and S4 in considerable depth. So, for this fourth article, what work do you expect us to discuss? S5, Mamba, or even Mamba2? None of the above. This series is primarily concerned with the mathematical foundations of SSMs, aiming to supplement our mathematical toolkit while understanding SSMs. In the previous article, we briefly mentioned S5 and Mamba. S5 is a simplified version of S4 and essentially introduces no new mathematical tricks. While the Mamba series performs excellently, it has already simplified $A$ to a diagonal matrix, requiring even fewer mathematical tricks; it reflects more on engineering capabilities. In this article, we will study a relatively new and not yet widely known work: \"State-Free Inference of State-Space Models: The Transfer Function Approach\" (referred to as RFT). It proposes a new scheme that completely shifts the training, inference, and even parameterization of SSMs into the space of generating functions, opening up a new perspective for understanding and ap"}, {"file": "translation_10197.html", "title": "\"Behind Closed Doors\" Thoughts on Multimodal Approaches (II): Autoregression", "content": "← Back to Index \"Behind Closed Doors\" Thoughts on Multimodal Approaches (II): Autoregression By 苏剑林 | July 08, 2024 In this article, we continue to \"build wheels behind closed doors\" and share some of the author's recent new understandings of multimodal learning. In the previous post \"Thoughts on Multimodal Approaches (I): Lossless Input\" , we emphasized the importance of lossless input for an ideal multimodal model. If this viewpoint holds, then the current mainstream approaches of discretizing images based on VQ-VAE, VQ-GAN, etc., present a performance bottleneck. This is because a simple entropy calculation shows that discretization inevitably leads to severe information loss. Therefore, a more promising or long-term solution should be using continuous features as input, such as directly \"patchifying\" the original pixel features of an image before feeding them into the model. However, while continuous input is naturally simple for image understanding, it introduces additional difficulties for image generation. This is because non-discretized data cannot directly use the autoregressive framework applied to text; it requires incorporating new elements like diffusion. This leads us"}, {"file": "translation_10226.html", "title": "Aligning with Full Fine-Tuning! This is the Most Brilliant LoRA Improvement I've Seen (Part 1)", "content": "← Back to Index Aligning with Full Fine-Tuning! This is the Most Brilliant LoRA Improvement I've Seen (Part 1) By 苏剑林 | July 12, 2024 As is well-known, LoRA is a common parameter-efficient fine-tuning method, which we briefly introduced in \"LoRA from a Gradient Perspective: Introduction, Analysis, Conjectures, and Extensions\" . LoRA utilizes low-rank decomposition to reduce the number of trainable parameters and save fine-tuning VRAM. At the same time, the trained weights can be merged back into the original weights, meaning the inference architecture does not need to change, making it a friendly fine-tuning solution for both training and inference. Furthermore, in \"Can LoRA Improve Further with Different Learning Rates?\" , we discussed the asymmetry of LoRA and pointed out that setting different learning rates for $A$ and $B$ can achieve better results, a conclusion known as \"LoRA+.\" To further improve performance, researchers have proposed many other LoRA variants, such as AdaLoRA , rsLoRA , DoRA , and PiSSA . While these modifications all have some merit, none felt particularly profound. However, a few days ago, \"LoRA-GA: Low-Rank Adaptation with Gradient Approximation\" caught m"}, {"file": "translation_10240.html", "title": "[Life Journal] Cooking Rice Soup with an Electric Rice Cooker", "content": "← Back to Index [Life Journal] Cooking Rice Soup with an Electric Rice Cooker By 苏剑林 | July 17, 2024 A while ago, I accidentally came across the concept of a \"low-sugar rice cooker\" (also called a \"low-starch rice cooker\"). At first, I thought it was some new high-tech product, but after a closer look, I realized it simply involves draining some of the rice water while cooking the rice. Since the rice water contains some starch, pouring it away is equivalent to eating slightly less starch—hence the so-called \"low-sugar/low-starch\" label. Although this type of product seems like a \"marketing gimmick\" (to reduce sugar, you might as well just eat half a mouthful less rice), it triggered childhood memories of cooking and a nostalgia for rice soup. Classic wood-fired stove (Source: Internet) In my memory, and in the recollections of my parents, I started trying to cook when I was about five years old. At that time, electric rice cookers were not yet widespread, and we used traditional wood-burning stoves. The basic steps for cooking rice with firewood were as follows: Wash the rice, add a generous amount of water, and start the fire; Once the water boils, pour out the excess water; Cont"}, {"file": "translation_10249.html", "title": "Monarch Matrix: Computationally Efficient Sparse Matrix Decomposition", "content": "← Back to Index Monarch Matrix: Computationally Efficient Sparse Matrix Decomposition By 苏剑林 | July 24, 2024 In the problem of matrix compression, we usually have two strategies to choose from: low-rankness and sparsity . Low-rankness reduces matrix dimensions by finding a low-rank approximation, while sparsity reduces the complexity of the matrix by decreasing the number of its non-zero elements. If SVD is aimed at the low-rank approximation of a matrix, then what is the corresponding algorithm for finding a sparse approximation of a matrix? Next, we are going to learn from the paper \"Monarch: Expressive Structured Matrices for Efficient and Accurate Training\" . It provides an answer to the above question—the \"Monarch matrix.\" This is a family of matrices that can be decomposed into the product of several permutation matrices and sparse matrices, characterized by being both computationally efficient and expressive. The paper also discusses how to find the Monarch approximation of a general matrix and how to use Monarch matrices to parameterize Large Language Models (LLMs) to improve their speed. It is worth noting that the author of this paper is also the author of the famous Flas"}, {"file": "translation_10266.html", "title": "Aligning Full Fine-Tuning! The Most Brilliant LoRA I've Seen (Part 2)", "content": "← Back to Index Aligning Full Fine-Tuning! The Most Brilliant LoRA I've Seen (Part 2) By 苏剑林 | July 29, 2024 Two weeks ago, I wrote \"Aligning with Full Fine-Tuning! This is the most brilliant LoRA improvement I've seen (Part 1)\" (at that time, it wasn't numbered \"Part 1\"), which introduced a LoRA variant called \"LoRA-GA.\" It improves the initialization of LoRA through gradient SVD to achieve alignment between LoRA and full fine-tuning. Of course, theoretically, this only attempts to align the first step update $W_1$, leading some readers to ask: \"What about $W_2, W_3, \\dots$ later on?\" At the time, I didn't think too deeply about it, simply assuming that after aligning the first step, subsequent optimizations would strictly follow a superior trajectory. Interestingly, not long after LoRA-GA was released, a new paper appeared on arXiv titled \"LoRA-Pro: Are Low-Rank Adapters Properly Optimized?\" . The proposed LoRA-Pro happened to answer exactly that question! LoRA-Pro also aims to align with full fine-tuning, but it aligns the gradient at every step, thereby aligning the entire optimization trajectory. This is a complementary improvement to LoRA-GA. Aligning Full Fine-Tuning This ar"}, {"file": "translation_10289.html", "title": "The Path to Optimal Distribution: Minimization in Probability Space", "content": "← Back to Index The Path to Optimal Distribution: Minimization in Probability Space By 苏剑林 | August 6, 2024 When finding the minimum value of a function, we typically find the derivative of the function and then look for its zero points. In fortunate cases, one of these zeros is the minimum point of the original function. For vector functions, we change the derivative to a gradient and find its zero points. When it is difficult to find the zero points of the gradient, we can use gradient descent to gradually approach the minimum point. These are basic results of unconstrained optimization, which many readers are likely familiar with. However, the theme of this article is optimization in the probability space—that is, the input to the objective function is a probability distribution. Optimization for this type of objective is more complex because its search space is no longer unconstrained. If we still solve for zero gradients or perform standard gradient descent, the results might not be guaranteed to remain probability distributions. Therefore, we need to find a new method of analysis and calculation to ensure that the optimization results conform to the characteristics of probabi"}, {"file": "translation_10311.html", "title": "New Attempts at \"Cool Papers + Site Search\"", "content": "← Back to Index New Attempts at \"Cool Papers + Site Search\" By 苏剑林 | August 12, 2024 In the article \"Cool Papers Update: A Simple Site-wide Search System\" , we introduced the new site-wide search system for Cool Papers . The goal of a search system is, naturally, to help users find the papers they need as quickly as possible. However, efficiently retrieving results that are valuable to oneself is not a simple task; it often requires certain skills, such as precise keyword extraction. This is where the value of algorithms comes in. Some steps that are tedious for humans to perform manually are very simple for algorithms. Therefore, in this post, we will introduce several new attempts to improve the efficiency of searching and filtering papers on Cool Papers using algorithms. Related Papers The technology behind the site-wide search is a Full-text Search Engine. Simply put, this is a search algorithm based on keyword matching, and its similarity metric is BM25. Since keywords are at the core, we can put some effort into them. Thus, we extracted 10 keywords for each paper based on its title and abstract to serve as a compressed representation of the paper. The first use of these keywo"}, {"file": "translation_10320.html", "title": "Making MathJax Better Compatible with Google Translate and Lazy Loading", "content": "← Back to Index Making MathJax Better Compatible with Google Translate and Lazy Loading By 苏剑林 | August 15, 2024 A long time ago, readers suggested rendering the mathematical formulas on Cool Papers. Many math-heavy papers have abstracts or even titles containing LaTeX code. If these formulas aren't rendered, they look like a jumble of garbled code, which significantly impacts the reading experience. However, previous tests showed that MathJax, the library responsible for rendering formulas, was quite incompatible with Google Translate and lazy loading. Consequently, despite the long-standing demand, I hadn't implemented it. But there’s good news: after repeated research and debugging over the past few days, I have finally resolved the compatibility issues. Cool Papers can now render mathematical formulas. This article summarizes the solution for your reference. Formula Rendering For displaying mathematical formulas (LaTeX) on web pages, there are currently two mainstream solutions: MathJax and KaTeX. KaTeX is relatively more lightweight, but its support for LaTeX is not as comprehensive as MathJax. Furthermore, since this blog has always used MathJax, it was my first choice when c"}, {"file": "translation_10332.html", "title": "A Near-Perfect Solution to the Conflict between MathJax and Marked", "content": "← Back to Index A Near-Perfect Solution to the Conflict between MathJax and Marked By 苏剑林 | August 26, 2024 In \"Making MathJax Better Compatible with Google Translate and Lazy Loading,\" we mentioned that Cool Papers added MathJax to parse LaTeX formulas. However, it was unexpected that this would trigger many compatibility issues. While some problems were purely the result of the author's obsessive-compulsive tendencies, a solution that is as perfect as possible is ultimately gratifying. Therefore, I am willing to spend a little more thought on it. In the previous article, we resolved the compatibility between MathJax, Google Translate, and lazy loading. In this article, we will resolve the conflict between MathJax and Marked. Problem Description Markdown is a lightweight markup language that allows people to write documents in an easy-to-read, easy-to-write plain text format. It is arguably one of the most popular writing syntaxes today. The [Kimi] function in Cool Papers also essentially outputs according to Markdown syntax. However, Markdown is not a language directly intended for browsers; the language for browsers is HTML. Therefore, there is a process of converting Markdown t"}, {"file": "translation_10347.html", "title": "Why Do Decoder-only LLMs Need Positional Encodings?", "content": "← Back to Index Why Do Decoder-only LLMs Need Positional Encodings? By 苏剑林 | September 01, 2024 As is widely known, the current mainstream Large Language Models (LLMs) are all Decoder-only models based on Causal Attention (we have previously discussed this in \"Why are current LLMs all using the Decoder-only architecture?\" ). Regarding Causal Attention, several studies have shown that it does not require additional positional encoding (abbreviated as NoPE) to achieve non-trivial results. However, the reality is that mainstream Decoder-only LLMs still include additional positional encodings, such as RoPE, ALiBi, etc. So the question arises: if we say we can do without positional encoding, why do mainstream LLMs still include it? Isn't it a case of \"the fewer complications, the better\"? In this article, we provide the author's perspective from three angles: What is the role of positional encoding for Attention? How does Causal Attention without positional encoding (NoPE) implement positional information? What are the shortcomings of the positional encoding implemented by NoPE? Positional Encoding In this section, let's first think about the first question: the significance of position"}, {"file": "translation_10352.html", "title": "\"Behind Closed Doors\" Brief Discussion on Multimodal Ideas (III): Position Encoding", "content": "← Back to Index \"Behind Closed Doors\" Brief Discussion on Multimodal Ideas (III): Position Encoding By 苏剑林 | September 6, 2024 In previous articles, we expressed the view that the main difference between multimodal LLMs and pure text LLMs is that the former has not yet formed a universally recognized standard methodology. This methodology includes not only the generation and training strategies discussed earlier but also the design of basic architectures, such as the \"multimodal position encoding\" we will discuss in this article. Regarding this topic, we already discussed it once in \"Path to Transformer Upgrade: 17. Simple Reflections on Multimodal Position Encoding\" and proposed a scheme (RoPE-Tie). However, at that time, the author's thinking on this issue was only in its early stages, and there were issues such as imprecise considerations of details and insufficient understanding. Therefore, looking back from today's perspective, the scheme proposed then is still clearly distant from a perfect answer. Therefore, in this article, we will once again comb through this issue from the top down and provide what we consider to be a more ideal result. Multimodal Position It might surpri"}, {"file": "translation_10366.html", "title": "Low-Rank Approximation Series (1): Pseudo Inverse", "content": "← Back to Index Low-Rank Approximation Series (1): Pseudo Inverse By 苏剑林 | September 15, 2024 Many readers, like myself, may have a feeling about matrix low-rank approximation that is both familiar and yet somewhat distant. Familiar because the concept and significance of low-rank approximation are not difficult to understand, and with the current proliferation of low-rank approximation-based fine-tuning techniques like LoRA, the concept has become subtly ingrained in our minds through daily exposure. However, the content covered by low-rank approximation is vast; in papers related to it, we often see unfamiliar yet astounding new techniques, which leads to a sense of \"knowing it without truly understanding it.\" Therefore, in this series of articles, I will attempt to systematically organize the theoretical content related to matrix low-rank approximation to complete our understanding. In this first article, we will primarily introduce a relatively simple concept in the low-rank approximation series—the pseudo inverse. Optimization Perspective The Pseudo Inverse , also known as the \"Generalized Inverse,\" is, as the name suggests, a \"generalized version of an inverse matrix.\" It is "}, {"file": "translation_10373.html", "title": "Softmax Sequel: Searching for a Smooth Approximation of Top-K", "content": "← Back to Index Softmax Sequel: Searching for a Smooth Approximation of Top-K By 苏剑林 | September 19, 2024 Softmax, as the name suggests, is a \"soft max\"—a smooth approximation of the $\\max$ operator (more precisely, the $\\text{argmax}$). It transforms any vector $\\boldsymbol{x} \\in \\mathbb{R}^n$ into a new vector with non-negative components that sum to 1 through exponential normalization, allowing us to adjust its degree of approximation to $\\text{argmax}$ (in its one-hot form) via a temperature parameter. In addition to exponential normalization, we previously introduced other schemes achieving similar effects in \"Path to Probability Distributions: A Survey of Softmax and Its Alternatives\" . We know that the maximum value is often referred to as Top-1, and its smooth approximation schemes seem quite mature. However, has the reader ever wondered what a smooth approximation for a general Top-$k$ would look like? Let us explore this problem together below. Problem Description Let $\\boldsymbol{x}=(x_1,x_2,\\cdots,x_n)\\in\\mathbb{R}^n$. For simplicity, we assume they are pairwise distinct, i.e., $i\\neq j \\Leftrightarrow x_i\\neq x_j$. Let $\\Omega_k(\\boldsymbol{x})$ denote the set of indi"}, {"file": "translation_10394.html", "title": "Achieving Smart Gas Stove Shut-off Using \"Flameout Protection + Smart Switch\"", "content": "← Back to Index Achieving Smart Gas Stove Shut-off Using \"Flameout Protection + Smart Switch\" By 苏剑林 | September 26, 2024 Smart gas stoves primarily move in two directions: first, detecting the ON/OFF state of the flame to enable linkage with other devices like range hoods; second, achieving smart shut-off, which includes scheduled turn-off and integration with Mi Home (or other smart home systems) for voice-controlled or remote shut-off. Currently, there are not many gas stove options with these two features, and they are significantly more expensive than standard gas stoves. Replacing a functional stove just for these features is not very cost-effective, which has led to several \"modding\" solutions for standard gas stoves. Setup Diagram This article shares a method based on the gas stove's built-in flameout protection device, using a smart switch (breaker) to integrate the stove into Mi Home and achieve smart shut-off functionality. Existing Products If we are strictly talking about \"timed shut-off,\" there are already products on the market. Their principle is to replace the gas stove knob with an electric knob that has a timer function. When the time is up, it physically turns t"}, {"file": "translation_10407.html", "title": "The Path to Low-Rank Approximation (II): SVD", "content": "← Back to Index The Path to Low-Rank Approximation (II): SVD By 苏剑林 | October 01, 2024 In the previous article , we introduced the \"pseudo-inverse,\" which relates to the optimal solution of the optimization objective $\\Vert \\boldsymbol{A}\\boldsymbol{B} - \\boldsymbol{M}\\Vert_F^2$ when the matrices $\\boldsymbol{M}$ and $\\boldsymbol{A}$ (or $\\boldsymbol{B}$) are given. In this article, we focus on the optimal solution when both $\\boldsymbol{A}$ and $\\boldsymbol{B}$ are not given, i.e., \\begin{equation}\\mathop{\\text{argmin}}_{\\boldsymbol{A},\\boldsymbol{B}}\\Vert \\boldsymbol{A}\\boldsymbol{B} - \\boldsymbol{M}\\Vert_F^2\\label{eq:loss-ab}\\end{equation} where $\\boldsymbol{A}\\in\\mathbb{R}^{n\\times r}, \\boldsymbol{B}\\in\\mathbb{R}^{r\\times m}, \\boldsymbol{M}\\in\\mathbb{R}^{n\\times m}, r < \\min(n,m)$. Simply put, this is the search for the \"optimal $r$-rank approximation (the best approximation with rank not exceeding $r$)\" of the matrix $\\boldsymbol{M}$. To solve this problem, we need to bring out the famous \"SVD (Singular Value Decomposition).\" Although this series started with the pseudo-inverse, its \"fame\" is far less than that of SVD. There are likely many people who have heard of or even use"}, {"file": "translation_10427.html", "title": "Low-Rank Approximation Road (III): CR", "content": "← Back to Index Low-Rank Approximation Road (III): CR By 苏剑林 | October 11, 2024 In \"Low-Rank Approximation Road (II): SVD\" , we proved that SVD provides the optimal low-rank approximation for any arbitrary matrix. The optimal approximation there was unconstrained, meaning that the result provided by SVD only focuses on minimizing the error and does not care about the specific structure of the matrix. However, in many application scenarios, due to requirements like interpretability or non-linear processing, we often hope to obtain an approximate decomposition that possesses certain special structures. Therefore, starting from this article, we will explore low-rank approximations with specific structures. This article will focus on the CR approximation (Column-Row Approximation), which provides a simple scheme for accelerating matrix multiplication operations. Problem Background The general formulation of the optimal rank-$r$ approximation of a matrix is: \\begin{equation}\\mathop{\\text{argmin}}_{\\text{rank}(\\tilde{\\boldsymbol{M}})\\leq r}\\Vert \\tilde{\\boldsymbol{M}} - \\boldsymbol{M}\\Vert_F^2\\label{eq:loss-m2}\\end{equation} where $\\boldsymbol{M}, \\tilde{\\boldsymbol{M}}\\in\\mathbb{R}^{n\\t"}, {"file": "translation_10474.html", "title": "Making MathJax Math Formulas Automatically Scale with Window Size", "content": "← Back to Index Making MathJax Math Formulas Automatically Scale with Window Size By 苏剑林 | October 15, 2024 With the emergence and popularity of MathJax, displaying mathematical formulas on web pages has gradually become the standard solution. However, MathJax (including its competitor KaTeX) is only responsible for converting web LaTeX code into mathematical formulas; it still lacks a robust method for adaptive resolution. Like some of the mathematical articles on this site, because they were composed on a PC, the browsing effect is acceptable on the PC side, but it can become somewhat unsightly when viewed on a mobile phone. After some testing, I have developed a solution that allows MathJax mathematical formulas to automatically scale with the window size, just like images, thereby ensuring the best possible display effect on mobile devices. I would like to share this with you all. Background Idea The origin of this problem is that even when composing on a PC, there are times when the length of a single-line formula exceeds the width of the webpage, but it is not convenient to break the line. In such cases, one solution is to manually adjust the font size of the formula using HT"}, {"file": "translation_10480.html", "title": "Cool Papers Browser Extension Upgrade to v0.2.0", "content": "← Back to Index Cool Papers Browser Extension Upgrade to v0.2.0 By 苏剑林 | October 16, 2024 At the beginning of the year, in \"A More Convenient Way to Open Cool Papers: Chrome Redirect Extension\" , we released a Chrome browser plugin (Cool Papers Redirector v0.1.0). It allows users to redirect to Cool Papers from any page via a right-click menu, making it easier to access Kimi's understanding of papers. A few days ago, we upgraded the plugin to v0.2.0 and successfully listed it on the Chrome Web Store, so I am here to share the news with everyone. Update Summary Compared to the old version v0.1.0, the main updates in the current version v0.2.0 are as follows: 1. Right-click menu jumps are now opened in a new tab; 2. Right-click menu supports accessing multiple paper IDs simultaneously; 3. Right-click menu supports PDF pages; 4. Right-click menu adds more paper sources (arXiv, OpenReview, ACL, IJCAI, PMLR); 5. Right-click menu falls back to site search (i.e., highlight-to-search) if no paper ID is found; 6. Shortcut jump links are inserted at appropriate positions on certain websites (arXiv, OpenReview, ACL). Below is a detailed introduction to the updates. Right-click Menu The first "}, {"file": "translation_10489.html", "title": "The Rotation Trick for VQ: A General Generalization of Straight-Through Estimation", "content": "← Back to Index The Rotation Trick for VQ: A General Generalization of Straight-Through Estimation By 苏剑林 | October 24, 2024 With the rapid development of multimodal LLMs, the status of VQ (Vector Quantization) has also risen. It can serve as a Tokenizer for visual and even any other modalities, unifying multimodal data into an autoregressive generation framework. Regrettably, since VQ-VAE was first proposed, its theory has not seen significant progress. Issues like codebook collapse or low utilization remain urgent and unresolved. Instead, alternative solutions such as FSQ have been proposed, becoming strong \"competitors\" to VQ. However, FSQ cannot replace VQ in every scenario, so improvements to VQ itself remain valuable. Recently, I read the paper \"Restructuring Vector Quantization with the Rotation Trick\" , which proposes a rotation trick claiming to improve a series of issues with VQ. In this article, let's examine it together. Review As early as five years ago, in the blog post \"A Concise Introduction to VQ-VAE: Quantized Autoencoder\" , we introduced VQ-VAE. Later, in \"Embarrassingly Simple FSQ: 'Rounding' Surpasses VQ-VAE\" when introducing FSQ, we revisited VQ-VAE in detail."}, {"file": "translation_10501.html", "title": "The Path to Low-Rank Approximation (Part 4): ID", "content": "← Back to Index The Path to Low-Rank Approximation (Part 4): ID By 苏剑林 | October 30, 2024 The protagonist of this article is ID (Interpolative Decomposition). It can also be understood as a low-rank decomposition with a specific structure, where one side consists of several columns of the matrix (of course, if you prefer rows, selecting rows is also fine). In other words, ID attempts to find several key columns from a matrix as a \"skeleton\" (usually also called a \"sketch\") to approximate the original matrix. Many readers may have never heard of ID, and even Wikipedia has only a few vague sentences of introduction ( Link ). However, ID, like SVD, has long been built into SciPy (see scipy.linalg.interpolative ), which side-evidences its practical value. Basic Definition In the previous three articles, we introduced Pseudo-inverse , SVD , and CR Approximation . They can all be viewed as seeking a low-rank approximation with a specific structure:\n\\begin{equation}\\mathop{\\text{argmin}}_{\\text{rank}(\\tilde{\\boldsymbol{M}})\\leq r}\\Vert \\tilde{\\boldsymbol{M}} - \\boldsymbol{M}\\Vert_F^2\\end{equation}\nwhere $\\boldsymbol{M}\\in\\mathbb{R}^{n\\times m}$. When no other constraints are added, the op"}, {"file": "translation_10519.html", "title": "Another VQ Trick: Adding a Linear Transformation to the Codebook", "content": "← Back to Index Another VQ Trick: Adding a Linear Transformation to the Codebook By 苏剑林 | November 06, 2024 In \"The Rotation Trick for VQ: A General Extension of Straight-Through Estimation\" , we introduced the Rotation Trick for VQ (Vector Quantization). Its idea is to design better gradients for VQ by generalizing the STE (Straight-Through Estimator), thereby alleviating issues such as codebook collapse and low codebook utilization. Coincidentally, a paper released yesterday on arXiv titled \"Addressing Representation Collapse in Vector Quantized Models with One Linear Layer\" proposed another trick to improve VQ: adding a linear transformation to the codebook. This trick simply changes the parameterization of the codebook without altering the underlying theoretical framework of VQ, but the experimental results are remarkably excellent, making it a classic example of something \"simple and effective.\" Foundation Since we have already introduced VQ and VQ-VAE multiple times in articles like \"A Simple Introduction to VQ-VAE: Quantized Autoencoders\" and \"Embarrassingly Simple FSQ: 'Rounding' Surpasses VQ-VAE\" , we won't start from scratch here. Let's directly provide the mathematical f"}, {"file": "translation_10542.html", "title": "When Batch Size Increases, How Should the Learning Rate Change Accordingly?", "content": "← Back to Index When Batch Size Increases, How Should the Learning Rate Change Accordingly? By 苏剑林 | November 14, 2024 With the rapid advancement of computing power, more and more scenarios hope to achieve \"trading compute for time\"—that is, shortening model training time by piling on more computing resources. Ideally, we hope that if we invest $n$ times the compute, the time to reach the same level of performance will be reduced to $1/n$, while the total compute cost remains identical. This \"hope\" seems reasonable and natural, but in reality, it is not trivial. Even if we ignore communication bottlenecks, when compute exceeds a certain scale or models are smaller than a certain scale, increasing compute often only results in increasing the Batch Size. However, does increasing the Batch Size necessarily shorten training time while maintaining performance? This is the topic we are about to discuss: when the Batch Size increases, how should various hyperparameters, especially the learning rate, be adjusted to maintain the original training effect and maximize training efficiency? We can also call this the Scaling Law between Batch Size and Learning Rate. Variance Perspective Intuitiv"}, {"file": "translation_10563.html", "title": "How does Adam's epsilon affect the Scaling Law of learning rate?", "content": "← Back to Index How does Adam's epsilon affect the Scaling Law of learning rate? By 苏剑林 | November 18, 2024 In the previous article \"When Batch Size increases, how should the learning rate change?\" we discussed the scaling laws between learning rate and Batch Size from multiple perspectives. For the Adam optimizer, we used the SignSGD approximation, which is a common technique for analyzing Adam. A natural question follows: how scientifically sound is it to approximate Adam using SignSGD? We know that the denominator of the Adam optimizer's update includes an $\\epsilon$. Its original purpose was to prevent division-by-zero errors, so its value is usually set very close to zero, to the point where we typically choose to ignore it in theoretical analysis. However, in current LLM (Large Language Model) training, especially low-precision training, we often choose a relatively large $\\epsilon$. This results in $\\epsilon$ often exceeding the magnitude of the gradient squares in the middle and late stages of training, making the existence of $\\epsilon$ 사실상 non-negligible. Therefore, in this article, we attempt to explore how $\\epsilon$ affects the Scaling Law of Adam's learning rate and B"}, {"file": "translation_10567.html", "title": "Diffusion Models Talk (26): Identity-based Distillation (Part 2)", "content": "← Back to Index Diffusion Models Talk (26): Identity-based Distillation (Part 2) By 苏剑林 | November 22, 2024 Continuing our diffusion series. In \"Diffusion Models Talk (25): Identity-based Distillation (Part 1)\" , we introduced SiD (Score identity Distillation), a diffusion model distillation scheme that requires no real data and no sampling from a teacher model. Its form is similar to a GAN but possesses better training stability. The core of SiD is to construct a better loss function for the student model through identity transforms. This point is pioneering, but it also left some questions. For example, SiD's identity transform of the loss function is incomplete; what would happen if it were completely transformed? How can the necessity of the parameter $\\lambda$ introduced by SiD be explained theoretically? The paper \"Flow Generator Matching\" (FGM for short), released last month, successfully explained the choice of $\\lambda=0.5$ from a more fundamental gradient perspective. Inspired by FGM, I further discovered an explanation for $\\lambda = 1$. Next, we will introduce these theoretical advances in SiD in detail. Review of Ideas According to the introduction in the previous arti"}, {"file": "translation_10588.html", "title": "Looking at Adaptive Learning Rate Optimizers from the Perspective of Hessian Approximation", "content": "← Back to Index Looking at Adaptive Learning Rate Optimizers from the Perspective of Hessian Approximation By 苏剑林 | November 29, 2024 Recently, I have been revisiting a Meta paper from last year titled \"A Theory on Adam Instability in Large-Scale Machine Learning\" , which presents a fresh perspective on looking at adaptive learning rate optimizers like Adam: it points out that the moving average of squared gradients approximates, in some sense, the square of the Hessian matrix. Thus, optimizers such as Adam and RMSprop are actually approximations of the second-order Newton's method. This perspective is quite novel and appears to differ significantly from some previous Hessian approximations, so it is well worth our study and reflection. Newton's Method Suppose the loss function is $\\mathcal{L}(\\boldsymbol{\\theta})$, where the parameters to be optimized are $\\boldsymbol{\\theta}$. Our optimization goal is\n\\begin{equation}\\boldsymbol{\\theta}^* = \\mathop{\\text{argmin}}_{\\boldsymbol{\\theta}} \\mathcal{L}(\\boldsymbol{\\theta})\\label{eq:loss}\\end{equation}\nAssuming the current value of $\\boldsymbol{\\theta}$ is $\\boldsymbol{\\theta}_t$, Newton's method seeks $\\boldsymbol{\\theta}_{t+1}$ by exp"}, {"file": "translation_10592.html", "title": "An Appreciation of the Muon Optimizer: A Fundamental Leap from Vectors to Matrices", "content": "← Back to Index An Appreciation of the Muon Optimizer: A Fundamental Leap from Vectors to Matrices By 苏剑林 | December 10, 2024 With the arrival of the LLM era, research enthusiasm for optimizers in academia seems to have waned. This is primarily because the current mainstream AdamW is already sufficient to meet most needs, and making major changes to an optimizer involves massive verification costs. Therefore, current developments in optimizers are mostly \"minor tweaks\" to AdamW made by the industry based on their own training experiences. However, an optimizer named \" Muon \" has recently gained considerable attention on Twitter. It claims to be more efficient than AdamW and is not just a \"minor adjustment\" on top of Adam; rather, it reflects some profound principles regarding the differences between vectors and matrices. In this article, let us appreciate it together. Comparison of Muon and AdamW performance (Source: Twitter @Yuchenj_UW) Initial Exploration of the Algorithm Muon stands for \"MomentUm Orthogonalized by Newton-schulz.\" It is applicable to matrix parameters $\\boldsymbol{W} \\in \\mathbb{R}^{n \\times m}$, and its update rule is: \\begin{equation}\n    \\begin{aligned}\n    \\b"}, {"file": "translation_10617.html", "title": "Generative Diffusion Models (27): Taking Step Size as Conditional Input", "content": "← Back to Index Generative Diffusion Models (27): Taking Step Size as Conditional Input By 苏剑林 | December 15, 2024 In this article, we once again focus on the acceleration of sampling in diffusion models. As is well known, there are two main approaches to sampling acceleration: developing more efficient solvers or post-distillation. However, as observed by the author, apart from SiD introduced in previous articles, both of these schemes rarely achieve results that reduce the generation process to a single step. Although SiD can achieve single-step generation, it requires additional distillation costs and involves a GAN-like alternating training process, which feels somewhat unsatisfying. The work to be introduced in this article is \"One Step Diffusion via Shortcut Models\" . Its breakthrough idea is to treat the generation step size as a conditional input to the diffusion model and add an intuitive regularization term to the training objective. This allows for the direct and stable training of models capable of single-step generation, making it a classic piece of simple yet effective work. ODE Diffusion The conclusions of the original paper are based on ODE-based diffusion models. T"}, {"file": "translation_10633.html", "title": "Generative Diffusion Models (Part 28): A Step-by-Step Understanding of Consistency Models", "content": "← Back to Index Generative Diffusion Models (Part 28): A Step-by-Step Understanding of Consistency Models By 苏剑林 | December 18, 2024 Following up from the previous post, in \"Generative Diffusion Models (Part 27): Using Step Size as a Conditional Input\" , we introduced the Shortcut model for accelerated sampling. One of the models it was compared against is the \" Consistency Model \". In fact, as early as \"Generative Diffusion Models (Part 17): General Steps for Constructing ODEs (Part 2)\" when introducing ReFlow, some readers mentioned Consistency Models. However, I always felt it was more of a practical trick with slightly thin theoretical foundations, so I wasn't very interested initially. However, since we have started focusing on the progress of accelerated sampling in diffusion models, Consistency Models are a body of work that cannot be ignored. Therefore, taking this opportunity, I would like to share my understanding of Consistency Models here. A Familiar Recipe Using a familiar recipe, our starting point remains ReFlow , as it is perhaps the simplest way to understand ODE-based diffusion. Let $\\boldsymbol{x}_0 \\sim p_0(\\boldsymbol{x}_0)$ be a real sample from the target dis"}, {"file": "translation_10648.html", "title": "Reflections from Spectral Norm Gradients to a New Type of Weight Decay", "content": "← Back to Index Reflections from Spectral Norm Gradients to a New Type of Weight Decay By 苏剑林 | December 25, 2024 In the article \"Appreciating the Muon Optimizer: A Fundamental Leap from Vectors to Matrices,\" we introduced a new optimizer named \"Muon.\" One perspective to understand it is as steepest gradient descent under spectral norm regularization, which seems to reveal a more fundamental optimization direction for matrix parameters. As we all know, we frequently apply weight decay to matrix parameters, which can be understood as the gradient of the squared $F$-norm. From the perspective of Muon, would constructing a new weight decay using the gradient of the squared spectral norm yield better results? The question then arises: what does the gradient—or rather, the derivative—of the spectral norm look like? And what would a new weight decay designed with it look like? Next, we will explore these questions. Basic Review The spectral norm (Spectral Norm), also known as the \"$2$-norm,\" is one of the most commonly used matrix norms. Compared to the simpler $F$-norm (Frobenius Norm), it often reveals more fundamental signals related to matrix multiplication. This is because its defin"}, {"file": "translation_10657.html", "title": "Why is the Default Norm for Gradient Clipping 1?", "content": "← Back to Index Why is the Default Norm for Gradient Clipping 1? By 苏剑林 | January 02, 2025 As we know, Gradient Clipping is a common technique used to make model training more stable. Generally, gradient clipping is performed based on the total norm of the gradients of all parameters. This operation can be expressed as: \\begin{equation}\n    \\text{clip}(\\boldsymbol{g},\\tau)=\\left\\{\\begin{aligned}&\\boldsymbol{g}, &\\Vert\\boldsymbol{g}\\Vert\\leq \\tau \\\\ \n    &\\frac{\\tau}{\\Vert\\boldsymbol{g}\\Vert}\\boldsymbol{g},&\\Vert\\boldsymbol{g}\\Vert > \\tau \n    \\end{aligned}\\right.\n    \\end{equation} In this way, $\\text{clip}(\\boldsymbol{g},\\tau)$ maintains the same direction as $\\boldsymbol{g}$, but its norm does not exceed $\\tau$. Note that $\\Vert\\boldsymbol{g}\\Vert$ here is the \"Global Gradient Norm,\" calculated by treating all parameters of the entire model as a single vector. Have you ever noticed a specific detail? Whether a model has millions of parameters or tens of billions, the value of $\\tau$ is often set to 1. What does this mean? Is it simply a matter of reusing a default value, or is there a profound principle hidden behind it? The \"What\" Some readers might feel that since the default v"}, {"file": "translation_10662.html", "title": "The Road to Low-Rank Approximation (Part 5): CUR Decomposition", "content": "← Back to Index The Road to Low-Rank Approximation (Part 5): CUR Decomposition By 苏剑林 | January 12, 2025 Returning once again to the path of low-rank approximation. In \"The Road to Low-Rank Approximation (Part 4): ID\" , we introduced \"Interpolative Decomposition (ID),\" which is the process of finding an approximation of the form $\\boldsymbol{C}\\boldsymbol{Z}$ for a matrix $\\boldsymbol{M}\\in\\mathbb{R}^{n\\times m}$, where $\\boldsymbol{C}\\in\\mathbb{R}^{n\\times r}$ consists of selected columns of $\\boldsymbol{M}$, and $\\boldsymbol{Z}\\in\\mathbb{R}^{r\\times m}$ is an arbitrary matrix. In this article, we will introduce the CUR decomposition. It shares the same lineage as Interpolative Decomposition, as both use the rows and columns of the original matrix as a \"skeleton\" to construct an approximation of the original matrix. Unlike ID, which uses either rows or columns, CUR decomposition uses both rows and columns simultaneously. Basic Definition Actually, this is not the first time CUR decomposition has appeared on this site. As early as \"Nyströmformer: A Linearized Attention Scheme Based on Matrix Decomposition\" , we introduced the Nyström approximation of a matrix, which is essentially "}, {"file": "translation_10667.html", "title": "Small Flow Series: TARFLOW: Flow Models Returning at Full Power?", "content": "← Back to Index Small Flow Series: TARFLOW: Flow Models Returning at Full Power? By 苏剑林 | January 17, 2025 I wonder if any readers still remember this series? This series, titled \"Small Flow\" (细水长flow), primarily introduces work related to flow models. It began back in 2018 when OpenAI released a new flow model, Glow , which was quite stunning at a time when GANs were the mainstream. But as impressive as it was, for a long time, Glow and its subsequent improvements couldn't match the generation quality of GANs, let alone the currently dominant diffusion models. However, the tide may be turning. Last month's paper \"Normalizing Flows are Capable Generative Models\" introduced a new flow model called TARFLOW. It approaches current SOTA results across almost all generative tasks, representing a \"full power\" return for flow models. Write at the Front The flow models discussed here specifically refer to Normalizing Flows, which are characterized by reversible architectures, training via maximum likelihood, and the ability to achieve one-step generation. The Flow Matching branch of diffusion models is not included in this category. Since the debut of Glow, progress in flow models has been "}, {"file": "translation_10684.html", "title": "Intersection Coordinates of Three Spheres (Trilateration)", "content": "← Back to Index Intersection Coordinates of Three Spheres (Trilateration) By 苏剑林 | January 28, 2025 A few days ago, while pondering a problem, I thought of the three-sphere intersection problem—that is, given the coordinates of the centers and the radii of three spheres, find the coordinates of their intersection points. Logically, this is a well-defined and concise problem with clear application backgrounds (such as satellite positioning), so a \"standard answer\" should have been provided long ago. However, after searching around, I found that neither English nor Chinese resources seem to provide a readable, standard derivation process. Of course, this is not to say that the problem is so difficult that no one can solve it; in fact, it is a classic problem that was solved long ago. I was simply surprised that no one seemed to have written down the derivation process on the internet in a highly readable way, so this article attempts to fill that gap. Special Case First, let the equations of the three spheres be:\n    \\begin{align}\n    &\\text{Sphere 1:}\\quad (\\boldsymbol{x} - \\boldsymbol{o}_1)^2 = r_1^2 \\label{eq:s1} \\\\\n    &\\text{Sphere 2:}\\quad (\\boldsymbol{x} - \\boldsymbol{o}_2)^2 "}, {"file": "translation_10699.html", "title": "MoE Tour: 1. Starting from Geometric Meaning", "content": "← Back to Index MoE Tour: 1. Starting from Geometric Meaning By 苏剑林 | February 8, 2025 Two years ago, in a moment of inspiration, I started the \" Transformer Upgrade Road \" series, where I sequentially shared some improvements and personal insights on mainstream Transformer architectures, which was well-received by many readers. Starting with this article, we will follow the same style to introduce another currently mainstream architecture: MoE (Mixture of Experts). The popularity of MoE needs no introduction. The recently viral DeepSeek-V3 uses the MoE architecture, GPT-4 is rumored to be MoE-based, and many recent models in China have also adopted MoE. However, although research on MoE has a long history, its application remained lukewarm for a long time. It was roughly starting from last year's 《Mixtral of Experts》 that MoE gradually began to capture everyone's attention. Its significant advantage is having a large number of parameters while maintaining significantly lower training and inference costs. At the same time, MoE also faces several challenges, such as training instability, load imbalance, and suboptimal performance, which were the primary reasons it did not gain wides"}, {"file": "translation_10711.html", "title": "Discussions on Generative Diffusion Models (29): Discrete Encoding with DDPM", "content": "← Back to Index Discussions on Generative Diffusion Models (29): Discrete Encoding with DDPM By 苏剑林 | February 14, 2025 A couple of days ago, I came across a new paper on arXiv titled \"Compressed Image Generation with Denoising Diffusion Codebook Models\" . I was truly amazed by the author's imaginative approach and couldn't help but share it with everyone. As the title of this post suggests, the author proposed an idea called DDCM (Denoising Diffusion Codebook Models). It restricts the noise sampling of DDPM to a finite set, which achieves some wonderful effects, such as encoding samples into discrete ID sequences and reconstructing them back, similar to VQ-VAE. Note that all these operations are performed on a pre-trained DDPM without any additional training. Finite Sets Since DDCM only requires a pre-trained DDPM model to perform sampling, we will not repeat the model details of DDPM here. Readers who are not yet familiar with DDPM can review the (1) , (2) , and (3) parts of our \"Discussions on Generative Diffusion Models\" series. We know that the generative sampling of DDPM starts from $\\boldsymbol{x}_T\\sim\\mathcal{N}(\\boldsymbol{0},\\boldsymbol{I})$ and iterates to $\\boldsymbol{"}, {"file": "translation_10735.html", "title": "MoE Grand Tour: 2. Not Worried about Scarcity, but about Inequality", "content": "← Back to Index MoE Grand Tour: 2. Not Worried about Scarcity, but about Inequality By 苏剑林 | February 21, 2025 In the previous article \"MoE Grand Tour: 1. Starting from Geometric Significance\" , we introduced a geometric interpretation of MoE, aiming to derive and understand MoE starting from the best approximation of a Dense model. At the same time, we mentioned at the end of the article that giving the MoE calculation formula is only the beginning. Training a practical and effective MoE model requiring many details to be filled in, such as the load balance problem to be discussed in this article. Load balancing, or \"not worrying about scarcity but rather about inequality,\" simply means making sure every Expert is working, and that they are all doing as equal an amount of work as possible to avoid wasting computing power on certain Experts. Load balancing is both a requirement for fully utilizing training computing power and a requirement for maximizing the potential of MoE's large parameter count. Needs Analysis We know that the basic form of MoE is:\n\\begin{equation}\\boldsymbol{y} = \\sum_{i\\in \\mathop{\\text{argtop}}_k \\boldsymbol{\\rho}} \\rho_i \\boldsymbol{e}_i\\end{equation}\nFor t"}, {"file": "translation_10739.html", "title": "Muon Sequel: Why did we choose to try Muon?", "content": "← Back to Index Muon Sequel: Why did we choose to try Muon? By 苏剑林 | Feb 27, 2025 In this post, I will interpret our latest technical report \"Muon is Scalable for LLM Training\" . It shares a relatively large-scale practice of the Muon optimizer, which we previously introduced in \"Muon Optimizer Appreciation: An Essential Leap from Vectors to Matrices\" . We have open-sourced the corresponding models (which we call \" Moonlight \", currently a 3B/16B MoE model). We found a somewhat startling conclusion: under our experimental setup, Muon achieved nearly twice the training efficiency compared to Adam. The Scaling Law of Muon and the MMLU performance of Moonlight Research on optimizers is neither too much nor too little, but why did we choose Muon as a new direction for exploration? How can an Adam optimizer with already tuned hyperparameters be quickly switched to Muon for experimentation? After scaling the model up, how does the performance difference between Muon and Adam manifest? Next, I will share our thought process. Optimization Principle Regarding optimizers, I actually gave a brief review in \"Muon Optimizer Appreciation: An Essential Leap from Vectors to Matrices\" . Most optimi"}, {"file": "translation_10757.html", "title": "MoE Tour: 3. A Different Approach to Allocation", "content": "← Back to Index MoE Tour: 3. A Different Approach to Allocation By 苏剑林 | March 5, 2025 In this article, we continue our exploration of the load balancing problem in Mixture-of-Experts (MoE). In the previous post, \"MoE Tour: 2. Concern is Not with Scarcity, but with Inequality,\" we primarily discussed the approach of promoting load balance through Auxiliary Loss (Aux Loss). While Aux Loss is simple and intuitive, it has a significant drawback—the weight is difficult to tune. If the weight is too low, it fails to promote balance; if it is too high, it easily harms the Language Model (LM) Loss. Consequently, the industry has been constantly searching for alternative solutions. The method we want to share today is called the \"Loss-Free\" scheme, proposed by DeepSeek in \"Auxiliary-Loss-Free Load Balancing Strategy for Mixture-of-Experts.\" Compared to DeepSeek's many dazzling open-source works, this paper might seem less prominent. However, in my view, its potential academic influence may far surpass other works because the proposed method is not only simple and effective but also highly universal, making it an instant classic. General Idea Faced with load imbalance, the idea of Aux Loss "}, {"file": "translation_10770.html", "title": "A Preliminary Exploration of muP: Laws of Cross-Model Scale Hyperparameter Transfer", "content": "← Back to Index A Preliminary Exploration of muP: Laws of Cross-Model Scale Hyperparameter Transfer By 苏剑林 | March 13, 2025 As is well known, the cost of fully training a large LLM is expensive, which dictates that we cannot directly test hyperparameters on large LLMs repeatedly. A natural idea is to hope that we can carefully search for hyperparameters on small models with the same structure and then transfer the optimal combination directly to the large model. Although this idea is simple, realizing it is non-trivial; it requires us to understand the scaling laws between common hyperparameters and model scales. muP is precisely a practical implementation of this idea. muP, sometimes written as $\\mu P$, stands for Maximal Update Parametrization. it originated from the paper \"Tensor Programs V: Tuning Large Neural Networks via Zero-Shot Hyperparameter Transfer\" . With the popularization of LLM training, it has gradually become one of the standard benchmarks for scientific model training. Core Ideas Before diving into the main topic, I must vent a bit: the original muP paper is written in an extremely obscure manner, and the presentation of the conclusions is cluttered, which adds a"}, {"file": "translation_10795.html", "title": "Higher-order muP: Simpler but Smarter Spectral Condition Scaling", "content": "← Back to Index Higher-order muP: Simpler but Smarter Spectral Condition Scaling By 苏剑林 | March 24, 2025 In the article \"A First Look at muP: Scaling Laws for Cross-Model Hyperparameter Transfer\" , we derived muP (Maximal Update Parametrization) based on the scale invariance of forward propagation, backpropagation, loss increment, and feature changes. For some readers, this process may still seem somewhat tedious, though it has already been significantly simplified compared to the original paper. It should be noted that we introduced muP relatively completely in a single article, whereas the original muP paper is actually the fifth in the author's Tensor Programs series! However, the good news is that in subsequent research \"A Spectral Condition for Feature Learning\" , the authors discovered a new way of understanding it (referred to below as the \"Spectral Condition\"). It is more intuitive and concise than the original derivation of muP or my own derivation, yet it yields richer results than muP. It can be considered a \"higher-order\" version of muP—a representative work that is both simple and sophisticated. Preparation As the name suggests, the Spectral Condition is related to the"}, {"file": "translation_10815.html", "title": "MoE Grand Tour: 4. More Resources for Difficulty", "content": "← Back to Index MoE Grand Tour: 4. More Resources for Difficulty By 苏剑林 | March 28, 2025 In the previous two articles, we discussed load balancing. In \"MoE Grand Tour: 3. A Different Way to Allocate\" , while introducing the Loss-Free scheme, I left a cliffhanger: the Bias term it introduces has a redundant degree of freedom, which can be used for other interesting things. In this article, we will discuss exactly that. We know that MoE selects the top $k$ most matching experts for each token for computation, thereby increasing parameter capacity while saving computational costs. However, upon closer reflection, this strategy actually has obvious room for improvement: intuitively, every token has a different level of difficulty. A more reasonable scheme would be to allocate more computational resources to difficult tokens and fewer resources to simple tokens, which might maximize performance under the same limited resources. The extra degree of freedom in the Bias mentioned earlier happens to provide a simple way to achieve this goal. Design Philosophy First, let's review the basic form of MoE:\n\\begin{equation}\\boldsymbol{y} = \\sum_{i\\in \\mathop{\\text{argtop}}_k \\boldsymbol{\\rho}} \\r"}, {"file": "translation_10831.html", "title": "Finding a Substitute for Normalization via Gradient Approximation", "content": "← Back to Index Finding a Substitute for Normalization via Gradient Approximation By 苏剑林 | April 02, 2025 I wonder if everyone noticed the paper 《Transformers without Normalization》 that came out recently? This paper attempts to replace the Normalization layers in Transformer models with an element-wise operation called DyT, aiming to improve speed while maintaining performance. The theme of fundamental architecture itself carries a certain level of attraction, and with the names of big names like Kaiming He and Yann LeCun attached, the paper caused quite a stir when it was released, receiving both praise and criticism. Coincidentally, a new paper last week titled 《The Mathematical Relationship Between Layer Normalization and Dynamic Activation Functions》 interpreted DyT from the perspective of gradient analysis and differential equations, and proposed new alternatives. Personally, I feel this perspective is very fundamental, so I have studied it and would like to share it here. Write at the Front DyT stands for Dynamic Tanh, which replaces the Normalization layer with the following operation: \\begin{equation}\\mathop{\\text{DyT}}(\\boldsymbol{x}) = \\boldsymbol{\\gamma} \\odot \\tanh(\\al"}, {"file": "translation_10847.html", "title": "Matrix Effective Rank", "content": "← Back to Index Matrix Effective Rank By 苏剑林 | Apr 10, 2025 Rank is an important concept in linear algebra, representing the inherent dimensionality of a matrix. However, the strict mathematical definition of rank is often not fully applicable to numerical computation scenarios. This is because rank equals the number of non-zero singular values, and the mathematical understanding of \"equal to zero\" differs from numerical computation. In mathematics, \"equal to zero\" means absolutely and strictly zero—even $10^{-100}$ is not zero. But numerical computation is different; in many cases, $10^{-10}$ can be treated as zero. Therefore, we hope to generalize the concept of rank to a form that better fits the characteristics of numerical computation. This is the origin of the concept of Effective Rank. Error Truncation It should be noted that there is currently no unified definition of effective rank in academic circles. In the following, we introduce several approaches to defining effective rank from different perspectives. For practical problems, readers can choose the definition that best suits their needs. We primarily consider rank from the perspective of singular values. For a matrix $"}, {"file": "translation_10862.html", "title": "The Road to Transformer Upgrades: 19. The Second Type of Rotary Positional Encoding", "content": "← Back to Index The Road to Transformer Upgrades: 19. The Second Type of Rotary Positional Encoding By 苏剑林 | April 18, 2025 Readers who have been following the \"Road to Transformer Upgrades\" series up to this point are likely already familiar with Rotary Positional Encoding (RoPE) . Simply put, RoPE is a rotary transformation applied to the Query ($\\boldsymbol{Q}$) and Key ($\\boldsymbol{K}$) of the Attention mechanism. Formally, it belongs to the category of absolute positional encodings, but when combined with the dot-product property of Attention, it automatically achieves a relative position effect. So, can RoPE be added to the Value ($\\boldsymbol{V}$)? At first glance, it seems not, because rotating $\\boldsymbol{V}$ doesn't result in relative positional encoding. However, things are not so absolute. This article discusses applying RoPE to $\\boldsymbol{V}$, which we can call the \"Second Type of Rotary Positional Encoding.\" Basic Review We decompose Dot-Product Attention as follows:\n\\begin{equation}\\boldsymbol{o}_i = \\sum_j a_{i,j}\\boldsymbol{v}_j,\\qquad a_{i,j} = \\frac{e^{s_{i,j}}}{\\sum\\limits_j e^{s_{i,j}}},\\qquad s_{i,j} = \\boldsymbol{q}_i^{\\top}\\boldsymbol{k}_j\\end{equation}\n"}, {"file": "translation_10869.html", "title": "Smart Home: DIY a Mi Home-compatible \"Zero-Cold-Water\" Device", "content": "← Back to Index Smart Home: DIY a Mi Home-compatible \"Zero-Cold-Water\" Device By 苏剑林 | April 22, 2025 Previously, in \"A Preliminary Analysis of the Technical Principles of Zero-Cold-Water Water Heaters\" , we introduced the principles of zero-cold-water technology in detail. We concluded by pointing out that, at the time, only a device called \"Aixiyi\" on the market achieved the ideal design described in the article. I have been using it for the past two years. However, my unit recently malfunctioned, and since it cannot be integrated into Mi Home, I don’t really want to repair it. Furthermore, the newer versions of the \"Aixiyi\" devices have become increasingly expensive, giving off a bit of a \"the dragon slayer has finally become a dragon itself\" vibe. Therefore, I decided to DIY a set of zero-cold-water devices that can be integrated into Mi Home based on the same principles. I am briefly recording the production process as follows. With a Return Pipe Of course, while I say \"DIY,\" it’s actually just assembling various off-the-shelf components into a complete system. In fact, once you understand the principles from the previous article, the production logic is not difficult. The onl"}, {"file": "translation_10878.html", "title": "Derivatives of SVD", "content": "← Back to Index Derivatives of SVD By 苏剑林 | April 26, 2025 By Jianlin Su |\n  April 26, 2025 |\n  19,522 Readers SVD (Singular Value Decomposition) is a common matrix decomposition algorithm that many readers are likely familiar with; we previously introduced it in \"The Road to Low-Rank Approximation (II): SVD\" . However, have you ever considered that SVD is actually differentiable? When I first learned of this conclusion, I was quite surprised, as intuition suggests that \"decompositions\" are often non-differentiable. But the truth is, SVD is indeed differentiable in general cases, which means that theoretically, we can embed SVD into models and train them end-to-end using gradient-based optimizers. The question arises: since SVD is differentiable, what do its derivative functions look like? Next, we will refer to the paper \"Differentiating the Singular Value Decomposition\" to step-by-step derive the differentiation formulas for SVD. Derivation Basis Assume $\\boldsymbol{W}$ is a full-rank $n \\times n$ matrix and all singular values are mutually distinct. This is a relatively easy case to discuss, and we will later address which conditions can be relaxed. Next, we set the SVD of $\\bol"}, {"file": "translation_10902.html", "title": "A Probability Inequality: Staring at it Until it Becomes Obviously True!", "content": "← Back to Index A Probability Inequality: Staring at it Until it Becomes Obviously True! By 苏剑林 | April 30, 2025 Two days ago, a member in a QQ group shared an inequality for proof: A probability-related inequality, from \"There is no fast single hashing algorithm\" The concise problem statement, combined with the hint that it is \"easily\" checked, makes one feel that this is an obviously true result. However, the person who asked mentioned they had tried for a long time without success. So, what is the actual situation? Is it truly obviously true? Preliminary Attempt The problem is equivalent to proving\n\\begin{equation}\\sum_{i=0}^j p^i \\leq \\sum_{i=0}^j \\left(\\log\\frac{1}{1-p}\\right)^i/i!,\\qquad p\\in[0, 1)\\label{eq:q}\\end{equation}\nThe given hint is\n\\begin{equation}\\log\\frac{1}{1-p} = \\sum_{i=1}^{\\infty} \\frac{p^i}{i} = p + \\frac{p^2}{2} + \\frac{p^3}{3} + \\cdots \\label{eq:log-1-p}\\end{equation}\nThis hint seems to guide us toward replacing $\\log\\frac{1}{1-p}$ on the right side of Eq. $\\eqref{eq:q}$ and then expanding it into a power series in $p$ for term-by-term comparison. Since all coefficients in the expansion of $\\log\\frac{1}{1-p}$ are positive, every coefficient in the power ser"}, {"file": "translation_10907.html", "title": "The Road to Transformer Upgrades: 20. What Makes MLA So Good? (Part 1)", "content": "← Back to Index The Road to Transformer Upgrades: 20. What Makes MLA So Good? (Part 1) By 苏剑林 | May 04, 2025 Since the explosive popularity of DeepSeek, its proposed Attention variant, MLA ( M ulti-head L atent A ttention), has garnered increasing attention. Through ingenious design, MLA achieves free switching between MHA (Multi-Head Attention) and MQA (Multi-Query Attention), allowing the model to choose the optimal form based on different training and inference characteristics (Compute-Bound or Memory-Bound), maximizing efficiency as much as possible. Admittedly, while MLA is highly effective, some argue it is not \"elegant\" enough, and efforts to find alternatives to MLA have persisted, including our own attempts. However, after a period of experimentation, we found that many Attention variants with the same or even larger KV Cache sizes ultimately do not perform as well as MLA. This has forced us to reflect: What exactly is the key reason behind MLA's outstanding performance? In this article, I will detail my thought process and the related experimental results surrounding this question. Observations MLA was introduced in DeepSeek-V2 . This article assumes the reader is already"}, {"file": "translation_10922.html", "title": "Newton-Schulz Iteration for the msign Operator (Part 1)", "content": "← Back to Index Newton-Schulz Iteration for the msign Operator (Part 1) By 苏剑林 | May 11, 2025 In previous articles such as \"Appreciating the Muon Optimizer: A Fundamental Leap from Vectors to Matrices\" and \"Muon Sequel: Why We Choose to Try Muon?\" , we introduced a highly promising emerging optimizer named \"Muon,\" which has the potential to replace Adam. As research continues to deepen, the attention surrounding the Muon optimizer is increasing day by day. Readers familiar with Muon know that its core operation is the $\\msign$ operator. Finding more efficient calculation methods for it is an ongoing goal of the academic community. This article summarizes its latest progress. Preliminaries The definition of $\\msign$ is closely related to singular value decomposition (SVD). Suppose a matrix $\\boldsymbol{M}\\in\\mathbb{R}^{n\\times m}$, then\n\\begin{equation}\\boldsymbol{U},\\boldsymbol{\\Sigma},\\boldsymbol{V}^{\\top} = \\text{SVD}(\\boldsymbol{M}) \\quad\\Rightarrow\\quad \\msign(\\boldsymbol{M}) = \\boldsymbol{U}_{[:,:r]}\\boldsymbol{V}_{[:,:r]}^{\\top}\\end{equation}\nwhere $\\boldsymbol{U}\\in\\mathbb{R}^{n\\times n}, \\boldsymbol{\\Sigma}\\in\\mathbb{R}^{n\\times m}, \\boldsymbol{V}\\in\\mathbb{R}^{m\\times m}$,"}, {"file": "translation_10945.html", "title": "MoE Travelogue: 5. Reflections on Uniform Distribution", "content": "← Back to Index MoE Travelogue: 5. Reflections on Uniform Distribution By 苏剑林 | May 16, 2025 If Meta's LLAMA series established the standard architecture for Dense models, then DeepSeek might be the founder of the standard MoE architecture. Of course, this doesn't mean DeepSeek invented MoE, nor that its MoE cannot be surpassed. Rather, it means that the improvements DeepSeek proposed for MoE are likely directions where performance gains are significant, gradually becoming the standard configuration for MoE. These include the Loss-Free load balancing scheme introduced in \"MoE Travelogue: 3. Another Perspective on Allocation\" , as well as the Shared Expert and Fine-Grained Expert strategies to be introduced in this article. When it comes to load balancing, it is undoubtedly a crucial goal for MoE; parts 2–4 of this series have largely centered around it. However, some readers have begun to realize that there is an underlying essential question yet to be answered: Putting aside efficiency requirements, is a uniform distribution necessarily the direction for the best performance? This article takes this question forward to understand Shared Expert and Fine-Grained Expert strategies. S"}, {"file": "translation_10958.html", "title": "Generative Diffusion Model Chat (30): From Instantaneous Velocity to Average Velocity", "content": "← Back to Index Generative Diffusion Model Chat (30): From Instantaneous Velocity to Average Velocity By 苏剑林 | May 26, 2025 As is well known, slow generation speed has always been a pain point for diffusion models. To solve this problem, researchers have \"shown their prowess\" by proposing a variety of solutions. However, for a long time, no single work has managed to stand out as a standard. What kind of work would reach such a standard? In my view, it must satisfy at least a few conditions: 1. The mathematical principles are clear, revealing the essence of fast generation; 2. It can be trained from scratch with a single objective, without requiring additional means like GANs or distillation; 3. Single-step generation is close to SOTA, and quality can be improved by increasing the number of steps. According to my reading experience, almost no work has met all three criteria simultaneously. However, a few days ago, an arXiv paper titled \"Mean Flows for One-step Generative Modeling\" (referred to as \"MeanFlow\") appeared, which seems very promising. Next, we will use this as an opportunity to discuss related ideas and progress. Existing Approaches There have been many works on accelera"}, {"file": "translation_10972.html", "title": "Equioscillation Theorem: Necessary and Sufficient Conditions for Optimal Polynomial Approximation", "content": "← Back to Index Equioscillation Theorem: Necessary and Sufficient Conditions for Optimal Polynomial Approximation By 苏剑林 | June 02, 2025 Recently, while reading, I encountered a theorem regarding optimal polynomial approximation known as the \" Equioscillation Theorem .\" The proof process involves the derivative of the infinity norm, and I found the conclusions and the proof quite novel, so I've decided to record them here. References: 《Notes on how to prove Chebyshev’s equioscillation theorem》 and 《Approximation Theory – Lecture 5》 . Equioscillation We first present the conclusion: Equioscillation Theorem Let $f(x)$ be a polynomial of degree at most $n$, and let $g(x)$ be a continuous function on the interval $[a,b]$. Then\n        \\begin{equation}f^* = \\mathop{\\text{argmin}}_f \\max_{x\\in[a,b]} |f(x) - g(x)|\\end{equation}\n        if and only if there exist $a\\le x_0 < x_1 < \\cdots < x_{n+1} \\le b$ and $\\sigma\\in\\{0,1\\}$ such that\n        \\begin{equation}f^*(x_k) - g(x_k) = (-1)^{k+\\sigma} \\max_{x\\in[a,b]} |f^*(x) - g(x)|\\end{equation} The Equioscillation Theorem is also sometimes directly called \"Chebyshev's Theorem\" because it was first discovered by Chebyshev. Although the text de"}, {"file": "translation_10996.html", "title": "Newton-Schulz Iteration for the msign Operator (Part 2)", "content": "← Back to Index Newton-Schulz Iteration for the msign Operator (Part 2) By 苏剑林 | June 05, 2025 In the previous article \"Newton-Schulz Iteration for the msign Operator (Part 1)\" , we attempted to find better Newton-Schulz iterations for the $\\mathop{\\text{msign}}$ operator, aiming to achieve the highest possible approximation within a limited number of iteration steps. This process can be transformed into finding polynomial iterations of the same form for the scalar function $\\mathop{\\text{sign}}(x)$. At that time, our approach was to use the Adam optimizer to find a local optimal solution end-to-end, which, while effective, was somewhat crude. A few days ago, a new paper appeared on arXiv titled \"The Polar Express: Optimal Matrix Sign Methods and Their Application to the Muon Algorithm\" . The authors used a series of exquisite mathematical conclusions to provide a more elegant and hardcore answer. In this article, let's appreciate and learn from this brilliant paper. Problem Description We will not repeat the relevant background and transformation process. The problem we want to solve directly is: \\begin{equation}\\mathop{\\text{argmin}}_f d(f(x),1)\\end{equation} where $f = f_T \\circ"}, {"file": "translation_11006.html", "title": "Calculating Singular Value Clipping (mclip) via msign (Part 1)", "content": "← Back to Index Calculating Singular Value Clipping (mclip) via msign (Part 1) By 苏剑林 | June 07, 2025 Previously, we used two articles, \"Newton-Schulz Iteration for the msign Operator (Part 1)\" and \"Newton-Schulz Iteration for the msign Operator (Part 2)\" , to discuss the numerical computation of the $\\newcommand{msign}{\\mathop{\\text{msign}}}\\newcommand{sign}{\\mathop{\\text{sign}}}\\newcommand{clip}{\\mathop{\\text{clip}}}\\newcommand{mclip}{\\mathop{\\text{mclip}}}\\msign$ operator for matrices. In this article, we shift our focus to the \"Singular Value Clipping\" operation. This operation recently sparked heated discussions on @_arohan_'s Twitter, and we also touched upon it in \"Higher-Order MuP: A Simpler yet Smarter Spectral Condition Scaling.\" In the following, we will refer to it simply as $\\mclip$. Basic Concepts For a scalar $x$, the $\\clip$ operation is defined as:\n\\begin{equation}\\clip(x) = \\max(\\min(x, 1), -1) = \\left\\{\\begin{aligned}1, &\\quad x\\geq 1 \\\\\nx, &\\quad x\\in(-1, 1)\\\\\n-1, &\\quad x\\leq -1\n\\end{aligned}\\right.\\end{equation}\nThat is, values greater than $1$ or less than $-1$ are truncated; otherwise, the value remains unchanged. For a matrix $\\boldsymbol{M}\\in\\mathbb{R}^{n"}, {"file": "translation_11025.html", "title": "The Derivative of msign", "content": "← Back to Index The Derivative of msign By 苏剑林 | June 13, 2025 In this article, we will derive the derivative formula for the $\\msign$ operator. If you are looking to combine TTT (Test-Time Training) with Muon , as seen in \"Test-Time Training Done Right\" , then this article may be helpful to you. Two Definitions This article assumes that readers already have some understanding of $\\msign$. If not, you can first read \"Appreciating the Muon Optimizer: The Essential Leap from Vectors to Matrices\" and \"Newton-Schulz Iteration for the msign Operator (Part 1)\" . Let there be a matrix $\\boldsymbol{M}\\in\\mathbb{R}^{n\\times m}$, then: \\begin{equation}\\boldsymbol{U},\\boldsymbol{\\Sigma},\\boldsymbol{V}^{\\top} = \\text{SVD}(\\boldsymbol{M}) \\quad\\Rightarrow\\quad \\msign(\\boldsymbol{M}) = \\boldsymbol{U}_{[:,:r]}\\boldsymbol{V}_{[:,:r]}^{\\top}\\end{equation} where $\\boldsymbol{U}\\in\\mathbb{R}^{n\\times n}, \\boldsymbol{\\Sigma}\\in\\mathbb{R}^{n\\times m}, \\boldsymbol{V}\\in\\mathbb{R}^{m\\times m}$, and $r$ is the rank of $\\boldsymbol{M}$. Simply put, $\\msign$ is the new matrix obtained by changing all non-zero singular values of the matrix to 1. Based on SVD, we can also prove: \\begin{equation}\\msign(\\boldsy"}, {"file": "translation_11033.html", "title": "A Brief History of Linear Attention: From Imitation and Innovation to Nourishing Back", "content": "← Back to Index A Brief History of Linear Attention: From Imitation and Innovation to Nourishing Back By 苏剑林 | June 20, 2025 In the Chinese-speaking community, this site was among the earliest to focus on Linear Attention. When I wrote the first related blog post in 2020, \"Exploration of Linear Attention: Must Attention Have a Softmax?\" , the primary discussion was still centered on Softmax Attention related to BERT. In hindsight, considering Linear Attention during the BERT era wasn't the wisest move because training lengths were relatively short, and models were primarily Encoders—where Linear Attention offers no real advantage. At the time, I even wrote \"Linear Transformers May Not Be the Model You Are Waiting For\" to express this viewpoint. It wasn't until the emergence of ChatGPT that everyone was forced toward Decoder-only generative models, which are highly compatible with the RNN form of Linear Attention. At the same time, the pursuit of increasingly long training sequences made the quadratic complexity bottleneck of Softmax Attention increasingly obvious. In this new context, Linear Attention has shown growing competitiveness, even showing signs of \"nourishing back\" into S"}, {"file": "translation_11056.html", "title": "What can the matrix sign function mcsgn compute?", "content": "← Back to Index What can the matrix sign function mcsgn compute? By 苏剑林 | June 23, 2025 In the article \"The Derivative of msign\" , we formally introduced two matrix sign functions $\\newcommand{msign}{\\mathop{\\text{msign}}}\\msign$ and $\\newcommand{mcsgn}{\\mathop{\\text{mcsgn}}}\\mcsgn$, where $\\msign$ is the core operation of Muon, while $\\mcsgn$ is used to solve the Sylvester equation . So, what else can $\\mcsgn$ do besides solving the Sylvester equation? This article aims to summarize the answers to this question. Two Signs Let the matrix $\\boldsymbol{M}\\in\\mathbb{R}^{n\\times m}$. We have two types of matrix sign functions: \\begin{gather}\n    \\msign(\\boldsymbol{M}) = (\\boldsymbol{M}\\boldsymbol{M}^{\\top})^{-1/2}\\boldsymbol{M}= \\boldsymbol{M}(\\boldsymbol{M}^{\\top}\\boldsymbol{M})^{-1/2} \\\\[6pt]\n    \\mcsgn(\\boldsymbol{M}) = (\\boldsymbol{M}^2)^{-1/2}\\boldsymbol{M}= \\boldsymbol{M}(\\boldsymbol{M}^2)^{-1/2}\n    \\end{gather} The first type is applicable to matrices of any shape, while the second type is only applicable to square matrices. The exponent $^{-1/2}$ refers to the inverse of the matrix square root; if it is not invertible, it is calculated according to the \" Pseudo-inverse \". Gene"}, {"file": "translation_11059.html", "title": "Calculating Singular Value Clipping mclip via msign (Part 2)", "content": "← Back to Index Calculating Singular Value Clipping mclip via msign (Part 2) By 苏剑林 | June 23, 2025 Previously, in \"Calculating Singular Value Clipping mclip via msign (Part 1)\" , we discussed the numerical calculation of singular value clipping $\\mclip$. The core idea came from @leloykun’s article \"Numerically Stable Spectral Clipping Via Newton-Schulz Iteration\" (now revised and renamed). By finding an expression based on $\\msign$, we avoid needing to find a separate Newton-Schulz iteration. In that article, the author proposed a nested $\\msign$ scheme with lower computational costs. However, a few days ago, @leloykun pointed out on Twitter that the author's scheme suffers from high numerical errors in actual calculations. This article specifically analyzes this issue and provides a more efficient, lower-error new scheme. Basic Concepts As per convention, let's first organize the basic concepts. First is the $\\clip$ operator for a scalar $x$, which we define generally as: \\begin{equation}\\clip\\nolimits_{[\\alpha,\\beta]}(x) = \\max(\\min(x, \\beta), \\alpha) = \\left\\{\\begin{aligned}\\beta, & \\quad x \\geq \\beta \\\\ \nx, & \\quad x\\in(\\alpha, \\beta)\\\\ \n\\alpha, & \\quad x\\leq \\alpha \n\\end{alig"}, {"file": "translation_11072.html", "title": "Efficient Inversion Method for \"Diagonal + Low-Rank\" Triangular Matrices", "content": "← Back to Index Efficient Inversion Method for \"Diagonal + Low-Rank\" Triangular Matrices By 苏剑林 | July 1, 2025 From the article \"A Brief History of Linear Attention: From Imitation and Innovation to Feedback\" , we can find that DeltaNet and subsequent linear attention models basically all involve the inverse matrix $(\\boldsymbol{I} + \\boldsymbol{K}\\boldsymbol{K}^{\\top}\\odot\\boldsymbol{M}^-)^{-1}$. This post is specifically dedicated to exploring the calculation of the inverse of such triangular matrices characterized by a \"diagonal + low-rank\" structure. Basic Results We define the problem generally as follows: Given matrices $\\boldsymbol{Q}, \\boldsymbol{K} \\in \\mathbb{R}^{n \\times d}$ and a diagonal matrix $\\boldsymbol{\\Lambda} \\in \\mathbb{R}^{n \\times n}$, satisfying $n \\gg d$, define\n        \\begin{equation}\\boldsymbol{T} = \\boldsymbol{\\Lambda} + \\boldsymbol{Q}\\boldsymbol{K}^{\\top}\\odot\\boldsymbol{M}^-\\end{equation}\n        where $\\boldsymbol{M}^- = \\boldsymbol{M} - \\boldsymbol{I}$, and the matrix $\\boldsymbol{M}$ is defined as\n        \\begin{equation}M_{i,j} = \\left\\{\\begin{aligned} &1, &i \\geq j \\\\ &0, &i < j\\end{aligned}\\right.\\end{equation}\n        The goal is to find the in"}, {"file": "translation_11111.html", "title": "Transformer Upgrade Path: 21. What makes MLA so good? (Part 2)", "content": "← Back to Index Transformer Upgrade Path: 21. What makes MLA so good? (Part 2) By 苏剑林 | July 10, 2025 In the article \"Transformer Upgrade Path: 20. What makes MLA so good? (Part 1)\" , we conducted ablation experiments on several changes in MLA compared to common MHA, GQA, and MQA. These changes included \"increasing head_dims,\" \"Partial RoPE,\" and \"KV sharing.\" The preliminary results of these experiments suggested that all three changes are likely reasons for MLA's excellent performance. In this article, we will depart from a more theoretical perspective to understand the success of MLA. Partial Rotation First, let's put the final assertion up front: Under the same training and inference costs, MLA is likely the best-performing Full Attention variant. Clearly, this judgment places MLA in a very high position. This conclusion is based on the experimental results from the previous article and the theoretical analysis to follow, under relatively ideal and simplified assumptions. Since actual training and inference involve many complex factors, this conclusion is likely to have some deviations, but we can at least conclude that MLA is moving in the right direction for improvement. The "}, {"file": "translation_11126.html", "title": "QK-Clip: Taking Muon One Step Further on the Path to Scaling Up", "content": "← Back to Index QK-Clip: Taking Muon One Step Further on the Path to Scaling Up By 苏剑林 | July 12, 2025 Four months ago, we released Moonlight , validating the effectiveness of the Muon optimizer on a 16B MoE model. In Moonlight, we confirmed the necessity of adding Weight Decay to Muon and proposed the technique of migrating Adam hyperparameters through Update RMS alignment, which allowed Muon to be quickly applied to LLM training. However, as we attempted to further scale Muon to models with over 100 billion parameters, we encountered a new \"stumbling block\"—MaxLogit explosion. To solve this problem, we have proposed a simple yet extremely effective new method, which we call \"QK-Clip.\" This method addresses the MaxLogit phenomenon from a very fundamental perspective without harming model performance. It has become one of the key training technologies for our latest trillion-parameter model, \" Kimi K2 .\" Problem Description Let's first briefly introduce the MaxLogit explosion phenomenon. Recalling the definition of Attention:\n\\begin{equation}\\boldsymbol{O} = softmax(\\boldsymbol{Q}\\boldsymbol{K}^{\\top})\\boldsymbol{V}\\end{equation}\nThe scaling factor $1/\\sqrt{d}$ is omitted here beca"}, {"file": "translation_11158.html", "title": "Efficient Calculation of Matrix Square Root and Inverse Square Root", "content": "← Back to Index Efficient Calculation of Matrix Square Root and Inverse Square Root By 苏剑林 | July 19, 2025 Let \\(\\boldsymbol{P}\\in\\mathbb{R}^{n\\times n}\\) be a square matrix of order \\(n\\) whose eigenvalues are all non-negative real numbers. In this article, we discuss the calculation of its square root \\(\\boldsymbol{P}^{1/2}\\) and its inverse square root \\(\\boldsymbol{P}^{-1/2}\\). Basic Concepts The square root of a matrix \\(\\boldsymbol{P}\\) refers to a matrix \\(\\boldsymbol{X}\\) that satisfies \\(\\boldsymbol{X}^2=\\boldsymbol{P}\\). We know that positive numbers have two square roots, so it is not difficult to imagine that matrix square roots are generally not unique. However, the \"arithmetic square root\" is unique. The arithmetic square root of a positive number is the positive square root; similarly, we define the arithmetic square root of \\(\\boldsymbol{P}\\) as the square root whose eigenvalues are all non-negative. The matrix square roots sought in this article default to the arithmetic square root. The calculations in this article rely on the matrix sign function \\(\\text{mcsgn}\\) discussed in \"What can the matrix sign function mcsgn calculate?\" : \\begin{equation}\\newcommand{mcsgn"}, {"file": "translation_11175.html", "title": "Efficient Calculation of Matrix r-th Roots and Inverse r-th Roots", "content": "← Back to Index Efficient Calculation of Matrix r-th Roots and Inverse r-th Roots By 苏剑林 | July 21, 2025 In the previous article \"Efficient Calculation of Matrix Square Root and Inverse Square Root\" , starting from the $\\mcsgn$ operator, I proposed a very elegant calculation method for the matrix square root and inverse square root. Interestingly, after simplifying the scheme, the final formulas no longer resemble the original $\\mcsgn$ form. This leads to deeper thinking: what is the more fundamental principle behind this scheme? Is there a possibility of generalizing it to an arbitrary $r$-th root? Analyzing from this perspective, I surprisingly discovered that we can understand the previous iteration algorithm from a simpler angle, and from this new perspective, it can be easily generalized to the calculation of arbitrary $r$-th roots and inverse $r$-th roots. Next, we will share this process. Previous Review # Let $\\boldsymbol{G}\\in\\mathbb{R}^{m\\times n}$ be an arbitrary matrix, and $\\boldsymbol{P}\\in\\mathbb{R}^{n\\times n}$ be any matrix whose eigenvalues are all within $[0,1]$. The previous article provided: \\begin{gather}\n    \\boldsymbol{G}_0 = \\boldsymbol{G}, \\quad \\boldsymbo"}, {"file": "translation_11196.html", "title": "Steepest Descent on Manifolds: 1. SGD + Hypersphere", "content": "← Back to Index Steepest Descent on Manifolds: 1. SGD + Hypersphere By 苏剑林 | August 01, 2025 Descriptions such as \"the opposite direction of the gradient is the direction of steepest descent\" are frequently used to introduce the principles of Stochastic Gradient Descent (SGD). However, this statement is conditional. For instance, \"direction\" is mathematically a unit vector, which depends on the definition of the \"norm (magnitude)\"; different norms lead to different conclusions. Muon , in fact, simply replaces the Frobenius norm for matrix parameters with a spectral norm, thereby obtaining a new descent direction. Furthermore, when we shift from unconstrained optimization to constrained optimization, the direction of steepest descent is not necessarily the opposite direction of the gradient. To this end, in this article, we will start a new series using \"constraints\" as the main thread to re-examine the proposition of \"steepest descent\" and explore where the \"direction of steepest descent\" points under different conditions. Optimization Principle As the first article, let us begin with SGD to understand the mathematical meaning behind the statement \"the opposite direction of the gra"}, {"file": "translation_11206.html", "title": "Building a Portable Side Router with Raspberry Pi Zero 2W", "content": "← Back to Index Building a Portable Side Router with Raspberry Pi Zero 2W By 苏剑林 | August 02, 2025 A while ago, I got a very mini development board, the Raspberry Pi Zero 2W (hereafter referred to as \" Pi \"), and paired it with a USB Key adapter board. I've spent the last few days tinkering with it to implement a portable side router. This article records the key technical points for reference by readers with similar needs. Background Students who have tinkered with proxies should be familiar with the concept of a \"side router\" (bypass router), which is mainly used here as a transparent proxy. Nowadays, the relevant technology is quite mature, so building a transparent proxy is not particularly difficult if you have a small host machine. The main focus of this article, however, is on it being \"portable,\" which requires the small host to meet the following characteristics: 1. Compact (otherwise, it's not suitable for carrying around); 2. Equipped with Wi-Fi connectivity; 3. Capable of switching networks at any time. The third point is the most troublesome. Suppose I bring my laptop and the small host to a new environment and learn the local Wi-Fi credentials; how can I get the small"}, {"file": "translation_11215.html", "title": "Steepest Descent on Manifolds: 2. Muon + Orthogonality", "content": "← Back to Index Steepest Descent on Manifolds: 2. Muon + Orthogonality By 苏剑林 | August 06, 2025 This article continues our series on constrained optimization. In the previous post, \"Steepest Descent on Manifolds: 1. SGD + Hypersphere\" , we revisited the \"least action principle\" for optimizers, proposing that the core difference between various optimizers lies in the different constraints imposed on the update magnitude. If this constraint is the Euclidean norm, then the corresponding steepest descent is SGD. Furthermore, we discussed the result of adding a magnitude constraint to the parameters, which constitutes steepest descent on a hypersphere manifold. However, the previous article was merely a \"warm-up\" because it dealt with relatively simple vector parameter optimization. This article formally enters the more challenging part—where optimization parameters transition from vectors to matrices, and the increment constraint is changed to the spectral norm, giving rise to the Muon optimizer. Next, we add an orthogonality constraint to the parameters, which leads to the Muon optimizer under an orthogonal manifold. Proposition Description Let the parameters to be optimized have a ma"}, {"file": "translation_11221.html", "title": "Steepest Descent on Manifolds: 3. Muon + Stiefel", "content": "← Back to Index Steepest Descent on Manifolds: 3. Muon + Stiefel By 苏剑林 | August 08, 2025 As mentioned last time, when we transition the optimization target from vector parameters to matrix parameters and choose the spectral norm constraint more suitable for matrices, the Muon optimizer emerges naturally. Furthermore, we considered the steepest descent direction under orthogonal constraints on parameters, which was divided into two parts: square matrices and non-square matrices. The solution for square matrices was completed in the previous article, but the non-square matrix part remained unresolved. The goal of this article is to fill in the solution for the non-square matrix part, allowing optimization under orthogonal constraints to be fully resolved. Task Information Let's briefly review the results from the previous article, \"Steepest Descent on Manifolds: 2. Muon + Orthogonality\" . The objective we want to solve is: \\begin{equation} \\max_{\\boldsymbol{\\Phi}} \\tr(\\boldsymbol{G}^{\\top}\\boldsymbol{\\Phi}) \\qquad \\text{s.t.}\\qquad \\Vert\\boldsymbol{\\Phi}\\Vert_2 = 1,\\,\\,\\, \\boldsymbol{W}^{\\top}\\boldsymbol{W}=\\boldsymbol{I},\\,\\,\\,(\\boldsymbol{W} - \\eta \\boldsymbol{\\Phi})^{\\top}(\\bolds"}, {"file": "translation_11233.html", "title": "An Identity for ReLU/GeLU/Swish", "content": "← Back to Index An Identity for ReLU/GeLU/Swish By 苏剑林 | August 16, 2025 Today I'll post something light and easy, based on an identity I realized over the last couple of days. This identity is actually quite simple, but at first glance, it feels a bit unexpected, so I am recording it here. Basic Result We know that $\\relu(x) = \\max(x, 0)$, and it is easy to prove the following identity: \\begin{equation}x = \\relu(x) - \\relu(-x)\\end{equation} If $x$ is a vector, this equation is even more intuitive: $\\relu(x)$ extracts the positive components of $x$, and $-\\relu(-x)$ extracts the negative components of $x$; adding the two together yields the original vector. General Conclusion The next question is: do activation functions like GeLU and Swish satisfy similar identities? At first glance, it seems they do not, but in fact, they do! We even have a more general conclusion: Let $\\phi(x)$ be any odd function, and let $f(x)=\\frac{1}{2}(\\phi(x) + 1)x$. Then the following always holds: \\begin{equation}x = f(x) - f(-x)\\end{equation} Proving this conclusion is also quite easy, so I won't elaborate further here. For Swish, we have $\\phi(x) = \\tanh(\\frac{x}{2})$, and for GeLU, we have $\\phi(x)=\\m"}, {"file": "translation_11241.html", "title": "Steepest Descent on Manifolds: 4. Muon + Spectral Sphere", "content": "← Back to Index Steepest Descent on Manifolds: 4. Muon + Spectral Sphere By 苏剑林 | August 21, 2025 Readers who have finished the first three articles should already be familiar with the \"routine\" of this series—first propose a constraint on the update amount, find the steepest descent direction, and then add constraints to the parameters themselves to find a new steepest descent direction. When solving parameter constraint problems, we adopt the \"first-order approximation is enough\" principle to simplify the constraint form, which geometrically corresponds to the \"tangent space.\" Then, we use the method of undetermined coefficients to convert the problem into an unconstrained form to write out an analytical solution, and finally numerically solve for the undetermined coefficients. In this article, we will solve a new example—Muon under the spectral sphere constraint—which is an analogous extension of the first article \"Steepest Descent on Manifolds: 1. SGD + Hypersphere.\" We can consider it when we want the spectral norm of the parameters to remain constant. Of course, it can also be used simply as an exercise to practice your skills. Problem Description In \"Steepest Descent on Mani"}, {"file": "translation_11250.html", "title": "Cool Papers Update: Simple Integration with Zotero Connector", "content": "← Back to Index Cool Papers Update: Simple Integration with Zotero Connector By 苏剑林 | August 25, 2025 Long ago, some readers suggested adding a feature to import papers into Zotero for Cool Papers . However, since I don't use Zotero and I was being a bit lazy, I hadn't put it on the schedule. This weekend, I happened to have some spare time, so I researched it and made a simple integration. Single Paper Import First, we need to install Zotero (obviously), and then install the Zotero Connector plugin for your browser. Once installed, visit a single paper page on Cool Papers, such as https://papers.cool/arxiv/2104.09864 or https://papers.cool/venue/2024.naacl-long.431@ACL , and then click the Zotero Connector icon. It will automatically import the paper, including the PDF file. Single paper import to Zotero The saved information includes the paper title, authors, abstract, date, primary category (arXiv), or the corresponding conference (for conference papers). This is implemented by embedding Metadata in the webpage header. It requires no extra configuration from the user. The disadvantage is that it only supports importing one paper at a time per page. Batch Import To support batch "}, {"file": "translation_11260.html", "title": "Rethinking Learning Rate and Batch Size (I): Current Status", "content": "← Back to Index Rethinking Learning Rate and Batch Size (I): Current Status By 苏剑林 | September 01, 2025 In previous articles \"How Should the Learning Rate Change as the Batch Size Increases?\" and \"How Does Adam's epsilon Affect the Learning Rate Scaling Law?\" , we theoretically discussed the law of how the learning rate changes with the batch size. The most classic part of this analysis is the second-order expansion approach proposed by OpenAI. However, when dealing with non-SGD optimizers, this analytical method's calculation process often becomes quite complex, leaving one feeling at a loss as to where to begin. In the next few articles, I will reorganize and rethink the relevant details in the aforementioned posts, attempting to simplify some of the derivation steps. I aim to provide a more general and lightweight derivation path and explore the possibility of extending it to the Muon optimizer. General Idea First, let's review the previous analysis method. In \"How Should the Learning Rate Change as the Batch Size Increases?\" , we introduced various perspectives for analyzing the relationship between learning rate and batch size. The second-order approximate analysis proposed by"}, {"file": "translation_11267.html", "title": "Why is Adam's Update RMS 0.2?", "content": "← Back to Index Why is Adam's Update RMS 0.2? By 苏剑林 | September 02, 2025 As is well known, we started experimenting with applying Muon to large-scale LLM training quite early. Specifically, in \"Muon Sequel: Why We Choose to Try Muon?\" , we proposed the \"Match Adam Update RMS\" trick to quickly migrate from Adam to Muon. This trick was also used in the training of Kimi K2. This technique refers to unifying the Update RMS of Muon to 0.2, which allows us to reuse Adam's learning rate and weight decay rate. Behind this trick is our observation that Adam's Update RMS is approximately equal to 0.2, and this phenomenon is stable and reproducible. This raises an interesting question: Why is Adam's Update RMS 0.2? Can we explain it theoretically? Problem Introduction First, let's describe the phenomenon: From experiments, we observed that roughly after the warmup ends and the model enters formal training, Adam's Update RMS remains almost entirely between 0.2 and 0.3, and models of different sizes exhibit similar patterns. A common point among these models is that they are all trained with Adam, with parameters $\\beta_1=0.9, \\beta_2=0.95$. Since the commonality is so obvious, this is likely "}, {"file": "translation_11280.html", "title": "Rethinking Learning Rate and Batch Size (II): Mean Field", "content": "← Back to Index Rethinking Learning Rate and Batch Size (II): Mean Field By 苏剑林 | September 10, 2025 At the end of the previous article \"Rethinking Learning Rate and Batch Size (I): The Current State\" , we mentioned that for cases where $\\tilde{\\boldsymbol{\\varphi}}_B$ depends non-linearly on $\\tilde{\\boldsymbol{g}}_B$, such as SignSGD and SoftSignSGD, the mental burden of the calculation process is quite heavy and faces difficulties in generalization. To this end, I invested some effort in trying to simplify these derivations. Fortunately, there were some gains, and the key idea is the theme of this article—Mean Field. Mean field is a common approximation method in physics. It doesn't have a fixed form, but the general idea is to move the expectation (average) inside the function. In fact, we already caught a glimpse of the charm of mean field in \"Why is the Update RMS of Adam 0.2?\" , and in this article, we will see its miraculous effect on calculating the learning rate laws of SignSGD/SoftSignSGD. Core Idea Following the notation of the previous article, for SignSGD, we have $\\newcommand{sign}{\\mathop{\\text{sign}}}\\tilde{\\boldsymbol{\\varphi}}_B=\\sign(\\tilde{\\boldsymbol{g}}_B)$. "}, {"file": "translation_11285.html", "title": "Rethinking Learning Rate and Batch Size (Part 3): Muon", "content": "← Back to Index Rethinking Learning Rate and Batch Size (Part 3): Muon By 苏剑林 | September 15, 2025 In the previous two articles, \"Rethinking Learning Rate and Batch Size (Part 1): Status Quo\" and \"Rethinking Learning Rate and Batch Size (Part 2): Mean Field\" , we primarily proposed the mean-field method to simplify the calculations related to learning rate and batch size. At that time, we analyzed the SGD, SignSGD, and SoftSignSGD optimizers, and the primary goal was simplification, with no essentially new conclusions. However, in today's feast of optimizers, how could we miss a place for Muon? Therefore, in this article, we will attempt to calculate the relevant conclusions for Muon to see if the relationship between its learning rate and batch size exhibits any new patterns. Basic Notation As is well known, the primary characteristic of Muon is its non-element-wise update rule. Therefore, the element-wise calculation methods used in \"How Should the Learning Rate Change as Batch Size Increases?\" and \"How Does Adam's epsilon Affect the Learning Rate Scaling Law?\" will be completely inapplicable. Fortunately, the mean-field method introduced in the previous article remains effective"}, {"file": "translation_11301.html", "title": "Rethinking Learning Rate and Batch Size (IV): EMA", "content": "← Back to Index Rethinking Learning Rate and Batch Size (IV): EMA By 苏剑林 | Sep 22, 2025 In \"Rethinking Learning Rate and Batch Size (II): Mean Field\" , we mentioned that one reason for focusing on SignSGD is that we typically use it as a theoretical approximation for Adam. This is a common simplification strategy used in the theoretical analysis of Adam. Besides analyzing learning rates, we have also used this simplification in posts like \"Can LoRA Improve Further with Different Learning Rates?\" and \"A First Look at MuP: Hyperparameter Scaling Laws Across Model Scales\" . However, is SignSGD truly a good approximation for Adam? One obvious difference is that the Update RMS of SignSGD is always 1, while that is not the case for Adam. I have found that the core reason for this difference is momentum, which is ubiquitous in optimizers like Adam, Lion, and Muon. Therefore, in this article, we will examine the impact of momentum—or more generally, EMA (Exponential Moving Average). Problem Analysis From the perspective of Adam, SignSGD corresponds to the special case where $\\beta_1=\\beta_2=0$, or to the first update step of Adam (regardless of $\\beta_1, \\beta_2$). Thus, we believe they ce"}, {"file": "translation_11307.html", "title": "Asymptotic Estimation of Weight RMS for AdamW", "content": "← Back to Index Asymptotic Estimation of Weight RMS for AdamW By 苏剑林 | October 01, 2025 In \"Why Adam's Update RMS is 0.2?\" , we used the mean-field approximation to estimate the Update RMS of Adam. Shortly after, reader @EIFY pointed out that the same result already appears in the paper \"Rotational Equilibrium: How Weight Decay Balances Learning Across Neural Networks\" . After reading it, I found that it contains not only the estimation of Update RMS but also the estimation of Weight RMS. That is to say, for a model trained with AdamW, the RMS of its weights can be estimated asymptotically in advance. Do you find this conclusion a bit surprising? I was quite surprised when I first saw it; intuitively, the weight magnitude should be something the model learns based on the training set, but it turns out to be hidden within the optimizer's hyperparameters, which is rather counter-intuitive. In this article, we will again use the mean-field approximation method to reproduce the asymptotic estimation of the Weight RMS. Sliding View First, let's review the update rules of AdamW: \\begin{equation}\n    \\text{Adam}\\color{skyblue}{\\text{W}}:=\\left\\{\\begin{aligned}\n    &\\boldsymbol{m}_t = \\bet"}, {"file": "translation_11320.html", "title": "Why Add Short Conv to Linear Attention?", "content": "← Back to Index Why Add Short Conv to Linear Attention? By 苏剑林 | October 05, 2025 If you have been following the progress in model architectures, you will notice that newer Linear Attention (refer to \"A Brief History of Linear Attention: From Imitation, Innovation to Feedback\" ) models all add Short Conv to $\\boldsymbol{Q}, \\boldsymbol{K}, \\boldsymbol{V}$, such as DeltaNet shown in the figure below: Short Conv in DeltaNet Why add this Short Conv? The intuitive understanding might be to increase model depth or enhance the model's Token-Mixing ability—essentially compensating for the decline in expressive power caused by linearization. While this explanation is more or less correct, it belongs to the \"universal template\" style of answers. We want to have a more accurate understanding of its actual mechanism of action. Next, I will provide my own interpretation (or more accurately, a conjecture). Test-Time Training From \"A Brief History of Linear Attention: From Imitation, Innovation to Feedback\" , we know that the core idea behind current state-of-the-art linear attention is TTT (Test Time Training) or Online Learning. TTT is based on the similarity between optimizer updates and RNN "}, {"file": "translation_11328.html", "title": "DiVeQ: A Very Concise VQ Training Scheme", "content": "← Back to Index DiVeQ: A Very Concise VQ Training Scheme By 苏剑林 | October 08, 2025 For researchers who insist on the discretization route, VQ (Vector Quantization) is a key component of visual understanding and generation, acting as the \"Tokenizer\" for vision. It was introduced in the 2017 paper \"Neural Discrete Representation Learning\" , and I also introduced it in a 2019 blog post \"A Simple Introduction to VQ-VAE: Quantized Autoencoder\" . However, after all these years, the training techniques for VQ have remained almost unchanged, relying on STE (Straight-Through Estimator) plus additional Aux Loss. While STE is fine—it's essentially the standard way to design gradients for discrete operations—the presence of Aux Loss often feels less than fully end-to-end and introduces extra hyperparameters that need tuning. Fortunately, this situation may be coming to an end. Last week's paper \"DiVeQ: Differentiable Vector Quantization Using the Reparameterization Trick\" proposed a new STE trick. Its highlight is that it requires no Aux Loss, making it particularly concise and elegant! Discrete Encoding As usual, let's first review existing VQ training schemes. First, it should be noted that "}, {"file": "translation_11335.html", "title": "Fast Estimation of the Spectral Norm of Random Matrices", "content": "← Back to Index Fast Estimation of the Spectral Norm of Random Matrices By 苏剑林 | Oct 12, 2025 In the \"Approximate Estimation\" section of \"Higher-Order MuP: Simpler but Smarter Spectral Condition Scaling\" , we \"pre-spent\" a conclusion: \"For an $n\\times m$ random matrix whose elements follow a standard normal distribution, its spectral norm is approximately $\\sqrt{n}+\\sqrt{m}$.\" In this article, we supplement the discussion of this conclusion and provide a fast estimation method for the spectral norm of random matrices. Random Matrix Theory # Suppose we have a random matrix $\\boldsymbol{W}\\in\\mathbb{R}^{n\\times m}$, where each element is independently and identically sampled from the standard normal distribution $\\mathcal{N}(0,1)$. We are required to estimate the spectral norm of $\\boldsymbol{W}$, which is the largest singular value, and we take $\\mathbb{E}[\\|\\boldsymbol{W}\\|_2]$ as the final estimation result. First, it should be pointed out that the characterization analysis of random matrices has formed a specialized branch. Regarding the spectral norm estimation of normal matrices, relevant keywords include \" Marchenko–Pastur distribution \", \"Bai-Yin law\", \"Gordon’s theorem\", etc"}, {"file": "translation_11340.html", "title": "Beyond MuP: 1. Three Characteristics of a Good Model", "content": "← Back to Index Beyond MuP: 1. Three Characteristics of a Good Model By 苏剑林 | Oct 21, 2025 Have you ever noticed an interesting detail? Muon and MuP both start with \"Mu,\" but the original meanings of the two \"Mu\"s are completely different. The former stands for \" M oment U m Orthogonalized by Newton-Schulz,\" while the latter stands for \" M aximal U pdate Parametrization.\" Yet, there is a profound connection between the two. In other words, Muon and MuP have completely different starting points, but they ultimately head in the same direction, even unintentionally adopting similar names—as if it were \"predestined by fate.\" Let's get down to business. In short, by a series of coincidences, I happened to learn Muon and MuP at the same time. This has greatly deepened my understanding of model optimization and prompted me to think about more fundamental principles. After some trial and error, I have gained some modest insights, which I would like to share with you here. Write in Front In terms of publication order, MuP came before Muon. However, my learning sequence was exactly the opposite: I first studied Muon and then MuP. Looking back, this turned out to be a good learning path. In a"}, {"file": "translation_11371.html", "title": "Low-Precision Attention May Have Biased Rounding Errors", "content": "← Back to Index Low-Precision Attention May Have Biased Rounding Errors By 苏剑林 | October 27, 2025 Some time ago, I came across the paper \"Why Low-Precision Transformer Training Fails: An Analysis on Flash Attention\" on arXiv. The experimental phenomena described in it closely match some of the phenomena we observed while training Kimi K2 , such as problems starting to appear from the second Attention layer. The paper attributes this to the inherent biased errors in low-precision Attention. This analytical perspective was quite unexpected for me, so I read it with great interest. However, the paper's phrasing seemed somewhat difficult to understand—partly because I am not very familiar with low-precision arithmetic. In short, after consulting the authors multiple times, I finally managed to understand the paper and decided to record my understanding here for everyone's reference. Brief Conclusion It should be noted that although the paper's title specifically mentions \"Flash Attention,\" according to the paper's description, the same problem occurs even if the block_size is set as large as the training sequence length. Therefore, the block-wise calculation of Flash Attention is not t"}, {"file": "translation_11388.html", "title": "Steepest Descent on Manifolds: 5. Dual Gradient Descent", "content": "← Back to Index Steepest Descent on Manifolds: 5. Dual Gradient Descent By 苏剑林 | November 03, 2025 In the first four articles, we solved several specific steepest descent problems with equality constraints on parameters. Among them, the problems in the third and fourth articles could not be solved analytically, so I proposed corresponding fixed-point iteration methods. In particular, the study of \"Muon + Stiefel\" in the third article, \"Steepest Descent on Manifolds: 3. Muon + Stiefel\" , originated from Jeremy Bernstein's article \"Orthogonal manifold\" . For this problem, Jeremy Bernstein eventually provided his own solution, which I call \"Dual Gradient Descent,\" and it is also quite worth learning. Basic Concepts Jeremy Bernstein's solution was finally published in the Thinking Machines Lab blog post \"Modular Manifolds\" , the lab's second blog post. In that article, they refer to it as \"Dual Ascent,\" but here I will combine it with the content of the previous four articles and call it \"Dual Gradient Descent.\" In fact, dual gradient descent can be seen as a natural consequence of the Lagrange multiplier method. However, the rigorous discussion of Lagrange multipliers is quite cumbers"}, {"file": "translation_11390.html", "title": "Asymptotic Estimation of the Maximum of n Normal Random Variables", "content": "← Back to Index Asymptotic Estimation of the Maximum of n Normal Random Variables By 苏剑林 | November 06, 2025 Let $z_1, z_2, \\dots, z_n$ be $n$ independent and identically distributed (i.i.d.) random variables sampled from a standard normal distribution. From these, we can construct many derived random variables, such as $z_1 + z_2 + \\dots + z_n$, which still follows a normal distribution, or $z_1^2 + z_2^2 + \\dots + z_n^2$, which follows a chi-squared distribution. In this article, we are interested in the distribution information of its maximum value $z_{\\max} = \\max\\{z_1, z_2, \\dots, z_n\\}$, especially its mathematical expectation $\\mathbb{E}[z_{\\max}]$. Conclusion First The basic estimation result for $\\mathbb{E}[z_{\\max}]$ is: Let $z_1, z_2, \\dots, z_n \\sim \\mathcal{N}(0, 1)$ and $z_{\\max} = \\max\\{z_1, z_2, \\dots, z_n\\}$, then\n        \\begin{equation}\\mathbb{E}[z_{\\max}]\\sim \\sqrt{2\\log n}\\label{eq:E-z-max}\\end{equation} The meaning of $\\sim$ in Equation $\\eqref{eq:E-z-max}$ is:\n    \\begin{equation}\\lim_{n\\to\\infty} \\frac{\\mathbb{E}[z_{\\max}]}{\\sqrt{2\\log n}} = 1\\end{equation}\n    As $n$ grows, this result becomes increasingly accurate. A more precise result is:\n    \\begin{equa"}, {"file": "translation_11404.html", "title": "Asymptotic Estimation of Weight RMS of AdamW (Part 2)", "content": "← Back to Index Asymptotic Estimation of Weight RMS of AdamW (Part 2) By 苏剑林 | November 17, 2025 In the blog post \"Asymptotic Estimation of Weight RMS of AdamW (Part 1)\" , we derived the asymptotic expression for the RMS of model weights trained with AdamW. However, at that time, we assumed that Weight Decay and the learning rate were fixed throughout the training process, which doesn't perfectly match actual training. Therefore, in this article, we will generalize the previous conclusions into a dynamic version. The so-called dynamic version means allowing both Weight Decay and the learning rate to change as the number of training steps increases, such as the classic Cosine Decay, WSD (Warmup Stable Decay), etc., making the conclusions more general. Step 1 Our starting point is still the definition of AdamW: \\begin{equation}\\text{Adam}\\color{skyblue}{\\text{W}}:=\\left\\{\\begin{aligned}\n  &\\boldsymbol{m}_t = \\beta_1 \\boldsymbol{m}_{t-1} + \\left(1 - \\beta_1\\right) \\boldsymbol{g}_t\\\\\n  &\\boldsymbol{v}_t = \\beta_2 \\boldsymbol{v}_{t-1} + \\left(1 - \\beta_2\\right) \\boldsymbol{g}_t^2\\\\\n  &\\hat{\\boldsymbol{m}}_t = \\boldsymbol{m}_t\\left/\\left(1 - \\beta_1^t\\right)\\right.\\\\\n  &\\hat{\\boldsymbol{"}, {"file": "translation_11416.html", "title": "Muon Optimizer Guide: Quick Start and Key Details", "content": "← Back to Index Muon Optimizer Guide: Quick Start and Key Details By 苏剑林 | November 19, 2025 Recently, many readers have likely come across news regarding the Muon optimizer. In fact, Muon was first proposed about a year ago, around October last year, by Keller Jordan on Twitter. However, within just this year, Muon has already undergone the test of training models with tens of billions, hundreds of billions, and even trillions of parameters, indicating it is a highly competitive optimizer. Nowadays, Muon has been built into training frameworks like Torch and Keras , and even large-scale frameworks like Megatron are gradually starting to support it, meaning it has gained widespread industry recognition. However, for readers only familiar with Adam, how to quickly and effectively switch to Muon may still be a confusing matter. Therefore, this article attempts to provide a quick-start tutorial. Brief Introduction The formal proposer of Muon is Keller Jordan , who currently works at OpenAI. As mentioned at the beginning, Muon was first published on Twitter, and even now, the author has only written a blog post \"Muon: An optimizer for hidden layers in neural networks\" rather than a for"}, {"file": "translation_11428.html", "title": "Generative Diffusion Models (31): Predicting Data Rather Than Noise", "content": "← Back to Index Generative Diffusion Models (31): Predicting Data Rather Than Noise By 苏剑林 | November 24, 2025 To this day, LDM (Latent Diffusion Models) remains the mainstream paradigm for diffusion models. By utilizing an Encoder to highly compress the original image, LDM can significantly reduce training and inference computational costs while simultaneously lowering the training difficulty—a multi-win approach. However, high-ratio compression also implies information loss, and the \"compress, generate, decompress\" pipeline lacks some end-to-end aesthetic. Consequently, there has always been a group persistent in \"returning to pixel space,\" hoping to let diffusion models complete generation directly on the original data. The work introduced in this article, \"Back to Basics: Let Denoising Generative Models Denoise\" , follows this line of thought. Based on the fact that original data often resides on a low-dimensional sub-manifold, it proposes that models should predict the data rather than the noise. This results in \"JiT (Just image Transformers),\" which significantly simplifies the architecture of diffusion models in pixel space. Signal-to-Noise Ratio Undoubtedly, the \"main force"}, {"file": "translation_11459.html", "title": "Weight Decay and Learning Rate from the Perspective of Moving Averages", "content": "← Back to Index Weight Decay and Learning Rate from the Perspective of Moving Averages By 苏剑林 | December 05, 2025 Weight Decay (WD) and Learning Rate (LR) are essential components of LLM pre-training. Whether they are set appropriately is one of the key factors determining the ultimate success or failure of a model. Since AdamW , decoupling Weight Decay to replace traditional L2 regularization has basically become a consensus. However, beyond this, there has been no significant theoretical progress on how to rationally set Weight Decay and Learning Rate. This article aims to provide a fresh perspective: viewing the training process as an exponential moving average (EMA) memory of the training data, and exploring how to set Weight Decay and Learning Rate to make this memory more \"scientific.\" Moving Average The general form of Weight Decay is:\n    \\begin{equation}\\boldsymbol{\\theta}_t = \\boldsymbol{\\theta}_{t-1} - \\eta_t (\\boldsymbol{u}_t + \\lambda_t \\boldsymbol{\\theta}_{t-1})\\end{equation}\n    where $\\boldsymbol{\\theta}$ is the parameter, $\\boldsymbol{u}$ is the update amount provided by the optimizer, and $\\lambda_t, \\eta_t$ are the Weight Decay and Learning Rate, respectively. Th"}, {"file": "translation_11469.html", "title": "Making Alchemy More Scientific (II): Generalizing Conclusions to Unbounded Domains", "content": "← Back to Index Making Alchemy More Scientific (II): Generalizing Conclusions to Unbounded Domains By 苏剑林 | December 12, 2025 Two years ago, I planned to start a \"Scientific Alchemy\" series, with the intention of systematically organizing classical theoretical results for optimizers. However, after writing the first article, \"Making Alchemy More Scientific (I): Convergence of Average Loss in SGD,\" the project was shelved until now. The main reason was my feeling that the conditions these classical optimization conclusions depend on are too strict and far removed from practical applications. Especially in the LLM era, the reference value of these conclusions seems even more limited, so I lacked the motivation to continue writing. However, while recently reflecting on Scaling Law-related issues, I discovered that these theoretical results are not as \"useless\" as I imagined; they can provide beneficial theoretical insights for some empirical results. Therefore, I am restarting this series to move the project forward and \"repay\" the \"debts\" owed previously. Conclusion Review Regarding notation, we will continue using the terminology from the first article, so we will not repeat the int"}, {"file": "translation_11480.html", "title": "Making Alchemy More Scientific (Part 3): Final Loss Convergence of SGD", "content": "← Back to Index Making Alchemy More Scientific (Part 3): Final Loss Convergence of SGD By 苏剑林 | December 16, 2025 We already have two articles discussing the convergence properties of SGD. However, they only cover convergence results for the average loss values, which only guarantees that we can find the optimal loss value but does not guarantee finding the location $\\theta^*$ of that optimal value. This is a significant gap between current theoretical conclusions and practice. Intuitively, the weights $\\theta_T$ at the end of training should be closer to the theoretical optimum $\\theta^*$. We want to know if theory supports this point. Therefore, in this article, we will transform the convergence results of the average loss into convergence results for the final loss, providing a preliminary theoretical understanding of how far $\\theta_T$ is from $\\theta^*$. Finding the Location We start from the article \"Making Alchemy More Scientific (Part 2): Extending the Conclusion to Unbounded Domains\" . Its core result is the inequality: \\begin{equation}\n    \\sum_{t=1}^T \\eta_t \\mathbb{E}[L(\\theta_t) - L(\\varphi)] \\le \\frac{\\|\\theta_1 - \\phi\\|^2}{2} + \\frac{G^2}{2} \\sum_{t=1}^T \\eta_t^2 \\la"}, {"file": "translation_11486.html", "title": "Why does DeltaNet need L2 Normalize?", "content": "← Back to Index Why does DeltaNet need L2 Normalize? By 苏剑林 | Dec 23, 2025 In the article \"A Brief History of Linear Attention: From Imitation and Innovation to Feedforward\" , we introduced DeltaNet, which brought the Delta Rule into linear attention, making it a powerful tool and forming the foundation for subsequent works like GDN and KDA . However, that article focused primarily on the overall idea of DeltaNet and did not delve into many technical details. In this post, we will discuss one of them: Why do DeltaNet and its subsequent works apply L2 Normalize to $\\boldsymbol{Q}$ and $\\boldsymbol{K}$? Of course, explaining this operation directly from the perspective of eigenvalues is not difficult, but I personally feel that explanation is slightly lacking. A few days ago, I learned a new interpretation from the paper \"Error-Free Linear Attention is a Free Lunch: Exact Solution from Continuous-Time Dynamics\" , which I found quite valuable and would like to share. Basic Analysis The recursive format of DeltaNet is \\begin{equation}\\boldsymbol{S}_t = \\boldsymbol{S}_{t-1} - \\eta_t (\\boldsymbol{S}_{t-1} \\boldsymbol{k}_t - \\boldsymbol{v}_t)\\boldsymbol{k}_t^{\\top} = \\boldsymbol{S}_{t-1}("}, {"file": "translation_11494.html", "title": "Make Alchemy More Scientific (Part 4): New Identity, New Learning Rate", "content": "← Back to Index Make Alchemy More Scientific (Part 4): New Identity, New Learning Rate By 苏剑林 | December 26, 2025 In the previous article \"Make Alchemy More Scientific (Part 3): Last-Iterate Convergence of SGD\" , we successfully converted the convergence conclusion from average loss to last-iterate loss, obtaining a convergence rate of \\(O(\\ln T / \\sqrt{T})\\). However, upon careful reflection, we find that this result doesn't quite align with intuition: according to experience, the last-iterate loss should be closer to the optimal value. If the average loss convergence rate can reach \\(O(1/\\sqrt{T})\\), why would the last-iterate convergence be slower? The latest progress on this issue comes from \"Optimal Linear Decay Learning Rate Schedules and Further Refinements\" . The paper first generalizes the key identity proven previously and then points out the importance of the learning rate schedule for last-iterate convergence, thereby accelerating the last-iterate loss convergence to \\(O(1/\\sqrt{T})\\). New Identity The results of the original paper are very rich. We will introduce them across multiple articles, and this piece will primarily follow the logic of the previous one to provid"}, {"file": "translation_11530.html", "title": "Making Alchemizing More Scientific (Part 5): Fine-tuning Learning Rate Based on Gradients", "content": "← Back to Index Making Alchemizing More Scientific (Part 5): Fine-tuning Learning Rate Based on Gradients By 苏剑林 | January 09, 2026 In the previous four articles, we explored a series of convergence conclusions for SGD, moving from bounded to unbounded domains and from average loss to final loss. Perhaps some readers feel that, at the end of the day, we are still talking about SGD—results that might seem like they belong to the \"ancient era\"? That is certainly not the case! For instance, the core identity relied upon in the fourth article, \"Making Alchemizing More Scientific (Part 4): New Identity, New Learning Rate,\" comes from the not-so-distant year of 2023. The conclusions of the third article, \"Making Alchemizing More Scientific (Part 3): Convergence of Final Loss in SGD,\" are only slightly earlier, dating back to 2020. Also in the fourth article, we derived the common practical learning rate strategy \"linear decay,\" which shows that this series of theoretical derivations is not just \"armchair theorizing\" but can provide effective guidance for practice. Next, we will discuss more refined gradient-based learning rate strategies. These help us understand the principles of learni"}, {"file": "translation_3181.html", "title": "Transforming Coordinates Using Variational Methods", "content": "← Back to Index Transforming Coordinates Using Variational Methods By 苏剑林 | January 6, 2015 Coordinate Transformation for ODEs Readers familiar with theoretical mechanics should be able to appreciate the role of variational methods in transforming coordinate systems. For example, if we want to transform the following equations for the planar two-body problem: \\[\\left\\{\\begin{aligned}\\frac{d^2 x}{dt^2}&=\\frac{-\\mu x}{(x^2+y^2)^{3/2}}\\\\ \\frac{d^2 y}{dt^2}&=\\frac{-\\mu y}{(x^2+y^2)^{3/2}}\\end{aligned}\\right.\\tag{1}\\] into polar coordinates, directly substituting the variables would be a very tedious calculation. However, we know that the equations above are simply the Euler-Lagrange equations resulting from the variation of the action: \\[S=\\int \\left[\\frac{1}{2}\\left(\\dot{x}^2+\\dot{y}^2\\right)+\\frac{\\mu}{\\sqrt{x^2+y^2}}\\right]dt\\tag{2}\\] Thus, we can directly perform the coordinate transformation on the action itself. Since the action generally only involves first-order derivatives, transforming the action is usually quite simple. For instance, it is easy to write the polar form of $(2)$ as: \\[S=\\int \\left[\\frac{1}{2}\\left(\\dot{r}^2+r^2\\dot{\\theta}^2\\right)+\\frac{\\mu}{r}\\right]dt\\tag{3"}, {"file": "translation_3188.html", "title": "When Probability Meets Complex Variables: From Binomial Distribution to Poisson Distribution", "content": "← Back to Index When Probability Meets Complex Variables: From Binomial Distribution to Poisson Distribution By 苏剑林 | January 13, 2015 The Poisson distribution is suitable for describing the probability distribution of the number of random events occurring per unit of time, such as the number of service requests received by a service facility within a certain period, or the number of passengers waiting at a bus stop. [Wikipedia] The Poisson distribution can also serve as an approximation for the Binomial distribution when the probability of success is small. Its derivation process is covered in most general probability textbooks. However, the proofs provided in typical textbooks are not always so aesthetically pleasing, such as the proof on page 98 of \"A Course in Probability and Mathematical Statistics\" (2nd Edition, edited by Mao Shisong et al.). So, which derivation process is more worthy of praise? In my opinion, it is the one utilizing the generating function. The probability generating function of the Binomial distribution is: \\begin{equation}(q+px)^n,\\quad q=1-p\\end{equation} When the number of trials $n$ is very large and the probability $p$ is very small, we can consider i"}, {"file": "translation_3194.html", "title": "Lebesgue's Dominated Convergence Theorem", "content": "← Back to Index Lebesgue's Dominated Convergence Theorem By 苏剑林 | January 16, 2015 In Real Analysis, there is Lebesgue's Dominated Convergence Theorem, which is generally considered a very useful method for determining whether integration and taking limits can be interchanged. The Dominated Convergence Theorem states that if a sequence of functions $\\{f_n(x)\\}$ defined on a set $E$ satisfies $|f_n(x)| \\leq F(x)$, and $F(x)$ is integrable on $E$, then the integration and taking the limit can be interchanged, i.e.,\n    $$ \\lim_{n \\to \\infty} \\left( \\int_E f_n(x)dx \\right) = \\int_E \\left( \\lim_{n \\to \\infty} f_n(x) \\right) dx $$\n    This article does not intend to discuss the proof of the theorem, but rather the topics related to its application. First, for interested readers, please try the following problem:\n    $$ \\lim_{n \\to \\infty} \\left( \\int_0^1 \\frac{n^2 x}{1+n^4 x^4} dx \\right) $$\n    A few days ago, I posted this problem on QQ and asked the classmates around me. There were few responses, but several friends attempted it. The result was: students who had studied Real Analysis generally could not solve this problem, while students who had not studied Real Analysis but had only"}, {"file": "translation_3200.html", "title": "The Multiplicative Group of a Finite Prime Field is a Cyclic Group", "content": "← Back to Index The Multiplicative Group of a Finite Prime Field is a Cyclic Group By 苏剑林 | January 20, 2015 For any prime number $p$, the set $\\mathbb{Z}_p=\\{0,1,2,\\dots,p-1\\}$ under addition and multiplication modulo $p$ forms a field. This is a fact known to any reader who has studied abstract algebra or elementary number theory. According to the definition of a field, $\\mathbb{Z}_p$ must first be an abelian group under addition modulo $p$. Furthermore, due to the specific nature of $\\mathbb{Z}_p$, it is also a cyclic group, which is a relatively trivial fact. However, what about multiplication? First, $0$ has no inverse, so we consider multiplication on the set $\\mathbb{Z}^\\cdot_p = \\mathbb{Z}_p \\setminus \\{0\\} = \\{1,2,\\dots,p-1\\}$. If I say that $\\mathbb{Z}^\\cdot_p$ also forms a cyclic group under multiplication modulo $p$ , this conclusion is not quite so trivial! Yet it is indeed a fact, and it holds for all prime numbers $p$. With this fact, several results in number theory become quite obvious. For instance, when $d \\mid (p-1)$, there are exactly $\\frac{p-1}{d}$ $d$-th power residues in $\\mathbb{Z}_p$, which is a fundamental conclusion regarding cyclic groups. In the book "}, {"file": "translation_3204.html", "title": "I Am a Feynman Fan", "content": "← Back to Index I Am a Feynman Fan By 苏剑林 | January 20, 2015 A few days ago, the copies of Feynman Lectures on Statistical Mechanics and Feynman Lectures on Computation that I purchased from Taiwan (via Taobao proxy) arrived today. With this, my collection of Feynman's works is basically complete. Feynman Gravitation, Statistical Mechanics, and Computation I am a Feynman fan, attracted by his \"Peter Pan\" personality and folded by his physics talent. Because of this, I even idolize Feynman like an ordinary person follows a celebrity—collecting his books and studying the physics he invented. Feynman wrote many works. I have read basically all of them, and I have collected everything that was collectible. My collection of Feynman's works includes: 《The Feynman Lectures on Physics》 (3 Volumes) 《Feynman's Tips on Physics: A Problem-Solving Supplement to the Feynman Lectures on Physics》 《Quantum Mechanics and Path Integrals》 (Printed from electronic version; Higher Education Press is currently preparing a reprint) 《Quantum Electrodynamics》 《Feynman Lectures on Gravitation》 (Taiwan Traditional Chinese Edition) 《Statistical Mechanics: A Set of Lectures》 (Taiwan Traditional Chinese Edition)"}, {"file": "translation_3210.html", "title": "How Could It Be Such a Coincidence! The Hidden Information Behind It", "content": "← Back to Index How Could It Be Such a Coincidence! The Hidden Information Behind It By 苏剑林 | January 21, 2015 Suppose I am a middle school mathematics teacher, excitedly lecturing my students about \"prime numbers.\" After explaining the definition and related properties, just as I am about to continue, a mischievous student interrupts: \"Teacher, can you give an example of a three-digit prime number?\" However, I don't have a table of primes under 1,000 at hand, nor have I memorized primes beyond 100. What should I do? I am forced to write a few three-digit numbers on the blackboard, such as $173$, $211$, and $463$, and then tell the students, \"Let's check if these numbers are prime.\" The final result: they are all primes! Then a student asks: How could it be such a coincidence? Probability of Primes First, the question is: if you write any three-digit number at random, what is the probability that it is prime? There are $143$ prime numbers among the $900$ three-digit numbers. Thus, the probability should be $143/900$, which is approximately one-sixth. This seems quite low; \"guessing right\" doesn't seem easy. However, in the scenario described, since I intend to write a prime number,"}, {"file": "translation_3217.html", "title": "Perturbative Expansion of Gaussian-type Integrals (I)", "content": "← Back to Index Perturbative Expansion of Gaussian-type Integrals (I) By 苏剑林 | February 14, 2015 Some time ago, while researching Feynman's path integral theory, I encountered perturbative methods for path integrals—approximating the propagator step-by-step through expansions in small parameters. Such techniques carry very clear physical meaning; readers interested in learning about path integrals and quantum mechanics should read Feynman's Quantum Mechanics and Path Integrals . However, from a mathematical perspective, this approximation technique is actually quite crude, and its convergence range and speed are often difficult to guarantee. In fact, mathematics has developed various perturbation (perturbation) techniques to handle different scenarios. Below, we study the integral:\n$$\\int_{-\\infty}^{+\\infty} e^{-ax^2-\\varepsilon x^4} dx\\tag{1}$$\nor more generally:\n$$\\int_{-\\infty^{+\\infty} e^{-ax^2-\\varepsilon V(x)} dx\\tag{2}$$\nThe series expansion for path integrals is slightly more complex than this, but it follows a similar form. Simple and Direct Expansion To calculate the above integral, the simplest and most direct method is to perform a series expansion of $e^{-\\varepsilon V"}, {"file": "translation_3229.html", "title": "Starting from the Series Problem Written by Kontsevich on the Blackboard", "content": "← Back to Index Starting from the Series Problem Written by Kontsevich on the Blackboard By 苏剑林 | February 27, 2015 One day, while browsing the Weibo of an editor from Higher Education Press's \"i Mathematics,\" I found a problem written by Kontsevich on a blackboard that he thought was very interesting. The original URL is: http://weibo.com/3271276117/BBrL5foVz . The problem is as follows: $$\\sum_{n=0}^{\\infty} \\frac{n! (20n)!}{(4n)!(7n)!(10n)!}x^n\\tag{1}$$ The goal is likely to find an expression for the original function. Asymptotic Series It should be noted that the series here is not quite \"normal.\" A simple analysis reveals that the coefficients $\\frac{n! (20n)!}{(4n)!(7n)!(10n)!}$ grow extremely fast, even exceeding the growth rate of $n!$. As everyone knows, the growth rate of exponential functions is much weaker than factorials. Therefore, according to standard mathematical analysis theory, the convergence region of series $(1)$ is actually just a single point: $x=0$! However, such a series could be the asymptotic series form of a well-behaved function. A so-called asymptotic series means that the error estimation term has coefficients that grow very quickly. For example, if"}, {"file": "translation_3241.html", "title": "Perturbation Expansion of Gaussian Integrals (II)", "content": "← Back to Index Perturbation Expansion of Gaussian Integrals (II) By 苏剑林 | March 07, 2015 Why is the Second Part Long Overdue? Actually, before writing this series, I had already planned out the content for the next several posts. I originally intended to confidently introduce some integral expansion techniques I had thought of; moreover, since I am quite familiar with the perturbation method itself, normally it wouldn't have taken this long for the second part to appear. However, while I was finishing the first part and preparing to write the second, I came across this response on Zhihu: http://www.zhihu.com/question/24735673 That article greatly expanded my understanding of series. It mentioned that the expansion of an integral is an asymptotic series. This made me hesitate, wondering if this series had any value, because an asymptotic series implies that no matter what expansion technique is used, the radius of convergence of the resulting series is zero. Later, upon re-thinking, I realized that even for asymptotic series, there is room for improvement and methods to accelerate convergence. So I think these few articles of mine should still have a little bit of significance, and"}, {"file": "translation_3243.html", "title": "Parallel Line Problems You Have Never Thought About", "content": "← Back to Index Parallel Line Problems You Have Never Thought About By 苏剑林 | March 17, 2015 Euclid The theme of this article is parallel lines. Friends who are familiar with mathematics might expect me to write about non-Euclidean geometry. But not this time. The content of this article is purely about the Euclidean geometry we have studied since childhood, based on \"Euclid's Fifth Postulate\" (also known as the Parallel Postulate). However, even within the Euclidean geometry we've studied for so long, there are many problems regarding parallel lines that we might not have thought through clearly. Because parallelism is such a fundamental concept in geometry, discussions of such basic propositions are quite prone to circular reasoning or even putting the cart before the horse. Starting from middle school, we are indoctrinated with rules for judging parallel lines such as \"if corresponding angles are equal, the two lines are parallel\" and \"if alternate interior angles are equal, the two lines are parallel.\" Of course, we cannot forget: \"through a point outside a line, there is exactly one line parallel to the given line.\" However, among these concepts, how many are fundamental axioms"}, {"file": "translation_3252.html", "title": "A Unique Physical Derivation of Heron's Formula", "content": "← Back to Index A Unique Physical Derivation of Heron's Formula By 苏剑林 | March 27, 2015 Heron's formula is used to calculate the area $S$ of a triangle given the lengths of its three sides $a, b, c$. It is a quite beautiful formula—not overly complex, and symmetric with respect to $a, b, c$, fully reflecting the equal status of the three sides. However, the derivation of such a symmetrically beautiful formula often involves an asymmetric process, such as the proof found on Wikipedia , which is somewhat of a shortcoming. The purpose of this article is to supplement this with a symmetric derivation. The title calls it a \"physical derivation,\" but the key lies in \"derivation\" rather than \"rigorous proof,\" and the \"physics\" here does not refer to a physical analogy, but rather to the idea and method of derivation, which has a distinct \"physical flavor.\" \\[ \\sqrt{p(p-a)(p-b)(p-c)} \\]\n\n    Before beginning the derivation, I offer a comment: Heron's formula appears to be the simplest possible formula for calculating the area of a triangle from its three side lengths. Basic Assumptions As I mentioned, the thought process and method used here have a \"physical flavor.\" What exactly does that"}, {"file": "translation_3256.html", "title": "An Interesting Limit Problem: Scaling at Will", "content": "← Back to Index An Interesting Limit Problem: Scaling at Will By 苏剑林 | March 28, 2015 Yesterday, a friend asked me the following problem, to prove:\n        \\[\\lim_{n\\to\\infty} \\frac{1^n + 2^n +\\dots + n^n}{n^n}=\\frac{e}{e-1}\\]\n        I am briefly recording the solution process here. Solution First, it can be noted that when $n$ is sufficiently large,\n        \\[\\frac{1^n + 2^n +\\dots + n^n}{n^n}=\\left(\\frac{1}{n}\\right)^n+\\left(\\frac{2}{n}\\right)^n+\\dots+\\left(\\frac{n}{n}\\right)^n\\]\n        the main terms are concentrated in the last few terms. Therefore, we can calculate it in reverse:\n        \\[\\begin{aligned}\\frac{1^n + 2^n +\\dots + n^n}{n^n}=&\\left(\\frac{1}{n}\\right)^n+\\left(\\frac{2}{n}\\right)^n+\\dots+\\left(\\frac{n}{n}\\right)^n\\\\\n        =&\\left(\\frac{n}{n}\\right)^n+\\dots+\\left(\\frac{2}{n}\\right)^n+\\left(\\frac{1}{n}\\right)^n\\end{aligned}\\]\n        And since we have\n        \\[\\left(\\frac{n-i}{n}\\right)^n=\\left(1-\\frac{i}{n}\\right)^n\\]\n        which is an increasing function with respect to $n$, we have\n        \\[\\left(\\frac{n-i}{n}\\right)^n \\leq \\lim_{n\\to\\infty} \\left(1-\\frac{i}{n}\\right)^n =e^{-i}\\]\n        Thus,\n        \\[\\left(\\frac{n}{n}\\right)^n+\\dots+\\left(\\frac{2}{n}\\rig"}, {"file": "translation_3266.html", "title": "Sampling Theorem: Constructing an Entire Function from a Finite Number of Points", "content": "← Back to Index Sampling Theorem: Constructing an Entire Function from a Finite Number of Points By 苏剑林 | April 16, 2015 Suppose we are listening to a song. After listening, we have actually undergone a process where our ears received sound wave stimulation over a period of time, leading to changes in brain activity. This song—the sound wave over that duration—can be described by a function $f(t)$ of time $t$, where the interval of the function is finite, such as $t \\in [0, T]$. Now, consider another scenario: we want to record the song we are singing using a computer. What kind of process is this? It is important to note that computer signals are discrete, while sound waves are continuous. Therefore, to record the song, the computer can only sample and record the signal. In principle, the more points collected, the more realistically our singing can be restored. However, there is a question: how many points are enough? In information theory, a famous \"Sampling Theorem\" (also known as the Shannon Sampling Theorem or Nyquist Sampling Theorem) tells us: Collecting only a finite number of sample points is sufficient to completely restore our input signal! Restoring a continuous functi"}, {"file": "translation_3272.html", "title": "Cauchy's Proposition: Stare at it Until it Becomes Obvious!", "content": "← Back to Index Cauchy's Proposition: Stare at it Until it Becomes Obvious! By 苏剑林 | April 19, 2015 In the section on limits of sequences in mathematical analysis, there is a very fundamental \"Cauchy Proposition\": If $\\lim_{n\\to\\infty} x_n=a$, then\n        \\[\\lim_{n\\to\\infty}\\frac{x_1+x_2+\\dots+x_n}{n}=a\\] This article specifically discusses this proposition, as well as some similar problems. Proof of the Cauchy Proposition The proof of the Cauchy proposition is not difficult. We only need to use the definition of limit convergence. Since $\\lim_{n\\to\\infty} x_n=a$, for any given $\\varepsilon > 0$, there exists a sufficiently large $N$ such that for any $n > N$, we have \\[|x_n - a| < \\varepsilon/2 \\quad (\\forall n > N)\\] Then, for a sufficiently large $n$, we have \\[\\begin{aligned}\n    &\\left|\\frac{x_1+x_2+\\dots+x_n}{n}-a\\right| \\\\\n    =&\\left|\\frac{(x_1-a)+(x_2-a)+\\dots+(x_n-a)}{n}\\right| \\\\\n    \\leq &\\left|\\frac{(x_1-a)+(x_2-a)+\\dots+(x_N-a)}{n}\\right| \\\\\n    &\\quad+\\left|\\frac{(x_{N+1}-a)}{n}\\right|+\\left|\\frac{(x_{N+2}-a)}{n}\\right|+\\dots+\\left|\\frac{(x_{n}-a)}{n}\\right| \\\\ < & \\left|\\frac{(x_1-a)+(x_2-a)+\\dots+(x_N-a)}{n}\\right|+\\frac{n-N}{n}\\frac{\\varepsilon}{2} \\\\ < & \\left|\\"}, {"file": "translation_3280.html", "title": "Perturbation Expansion of Gaussian-type Integrals (Part III)", "content": "← Back to Index Perturbation Expansion of Gaussian-type Integrals (Part III) By 苏剑林 | April 26, 2015 Changing to a Small Parameter Comparing the two articles \"Perturbation Expansion of Gaussian-type Integrals (Part I)\" and \"Perturbation Expansion of Gaussian-type Integrals (Part II)\" , we can derive two conclusions regarding the integral: \\[\\int_{-\\infty}^{+\\infty} e^{-ax^2-\\varepsilon x^4} dx\\tag{1}\\] First, we found that approximation results similar to equation (4) possess good properties, yielding a relatively reliable approximation for any $\\varepsilon$. Second, we found that expanding term by term within the exponent produces better results than directly expanding as a power series. So, can we combine these two approaches? Let us rewrite equation (4) as: \\[\\int_{-\\infty}^{+\\infty} e^{-ax^2-\\varepsilon x^4} dx \\approx \\sqrt{\\frac{2\\pi}{a+\\sqrt{a^2+6 \\varepsilon}}}=\\sqrt{\\frac{\\pi}{a+\\frac{1}{2}\\left(\\sqrt{a^2+6 \\varepsilon}-a\\right)}}\\tag{6}\\] where \\[\\lambda=\\frac{1}{2}\\left(\\sqrt{a^2+6 \\varepsilon}-a\\right)\\tag{7}\\] is actually also a small quantity (when $\\varepsilon$ is small). Therefore, why not use $\\lambda$ as a small parameter to expand within the exponent? In fact, th"}, {"file": "translation_3290.html", "title": "Seeking a Smooth Maximum Function", "content": "← Back to Index Seeking a Smooth Maximum Function By 苏剑林 | May 02, 2015 In optimization problems, the most direct method to find the maximum or minimum of a function is to take the derivative and then compare the magnitudes of the various extreme values. However, the functions we want to optimize are not always differentiable—for example, functions containing the maximum function $\\max(x, y)$. In such cases, we must resort to other approaches. One very clever idea is to approximate these non-differentiable functions with a differentiable one, allowing us to find an approximate optimal value using standard extremum methods. The task of this article is to explore a simple and useful function that can serve as an approximation for the maximum function while possessing derivatives of multiple orders. Below is the derivation process provided by the author. In mathematical analysis, I have already learned a formula regarding the maximum function; namely, when $x \\geq 0$ and $y \\geq 0$, we have:\n$$\\max(x,y)=\\frac{1}{2}\\left(|x+y|+|x-y|\\right)\\tag{1}$$\nTherefore, to seek a smooth maximum function, we can first consider finding a function that can approximately represent the absolute value "}, {"file": "translation_3298.html", "title": "Recording a Process of Crawling Taobao/Tmall Review Data", "content": "← Back to Index Recording a Process of Crawling Taobao/Tmall Review Data By 苏剑林 | May 06, 2015 Recently, I have become fascinated with data mining and machine learning. To perform data analysis, one must first have data. For commoners like us, the cheapest way to acquire data is likely using a crawler to scrape it from the web. This article records the entire process of crawling a certain product on Tmall. The approach for Taobao stores is similar, so I will not elaborate further. The focus is on analyzing the page and implementing a simple and convenient crawl using Python. The tools I used are as follows: Python 3 —An extremely convenient programming language. Version 3.x was chosen because it handles Chinese characters more gracefully. Pandas —An additional Python library used for data organization. IE 11 —Used to analyze the page request process (any other similar traffic monitoring tool will also work). The remaining libraries include requests and re , which come with Python or are easily installed. Example page (a Midea water heater): http://detail.tmall.com/item.htm?id=41464129793 Where are the reviews? To crawl review data, you must first find where the reviews actually res"}, {"file": "translation_3319.html", "title": "It is time.", "content": "← Back to Index It is time. By 苏剑林 | May 24, 2015 Finally, I can catch a breath~~ Friends who follow Scientific Space may have noticed that updates have been quite slow during this period. This all started from this year's winter break... At the end of January, due to various reasons and combined with my own interests, I looked for an internship. The job was Python programming. The recruitment was posted on the South China University of Technology forum; it was written very concisely, and I also submitted my resume quite concisely. Unexpectedly, I received a reply and was hired. I started working in February, and after joining, I realized that the company was a relatively well-known domestic e-commerce enterprise. My main job was data mining... Although I had a little experience with Python, I was basically a novice at data mining, so I could only learn while working, frantically making up for data mining knowledge. During this process, I learned many things about data mining. You should know that before this, I didn't know what a \"feature\" was, or what \"logistic regression\" or \"SVM\" meant... I was completely ignorant back then. The new semester arrived quickly. Since there are few"}, {"file": "translation_3320.html", "title": "Victory in Fooling Around: Introducing Operators into Series Summation", "content": "← Back to Index Victory in Fooling Around: Introducing Operators into Series Summation By 苏剑林 | May 26, 2015 In the article \"An Interesting Limit Problem: Scaling at Will\" , the reader \"Recently Fell Down\" proposed a novel solution. However, this reader did not write it very clearly. More importantly, some of the techniques involved seem to be things I haven't encountered before. So, I analyzed it myself and provided the following explanation. The \"Nonsense\" Result Suppose we want to find the sum of a series: \\[ \\sum_{k=0}^n \\binom{n}{k}\\frac{A_k}{n^k} \\] where $A_0=1$. Generally, we use subscripts to denote different terms, such as $A_k, k=0,1,2,\\dots$ in the above formula. However, some people don't like this; they prefer using exponents to represent the terms of the sequence. They write the series above as: \\[ \\sum_{k=0}^n \\binom{n}{k}\\frac{A^k}{n^k} \\] Readers might object: \"Isn't this just fooling around? Wouldn't this be confused with the $k$-th power of $n$ in the denominator?\" But that person goes even further and writes the series as: \\[ \\sum_{k=0}^n \\binom{n}{k}\\frac{A^k}{n^k}=\\left(1+\\frac{A}{n}\\right)^n \\] See that? He simply treated $A$ as a number! It's complete nonse"}, {"file": "translation_3327.html", "title": "[Memo] Wikipedia and DNSCrypt", "content": "← Back to Index [Memo] Wikipedia and DNSCrypt By 苏剑林 | May 30, 2015 The domain for the Chinese Wikipedia, zh.wikipedia.org, was subjected to keyword filtering and DNS poisoning on May 19th. Currently, Chinese Wikipedia is inaccessible from within China, and its domain cannot resolve to the correct IP address. However, the English Wikipedia remains unaffected for now and can be accessed normally. From \"Moonlight Blog\": http://www.williamlong.info/archives/4240.html Similar news: http://www.freebuf.com/news/68011.html First Google was blocked, and now it's Wikipedia's turn. Of course, this isn't the first time Wikipedia has been blocked, and I imagine it won't be the last. To be honest, before this, I didn't even know what \"DNS poisoning\" was. I feel that my current skills in circumvention and using proxies are all the result of us commoners being forced by our Great Empire to find ways around the blocking of these excellent websites. I really want to vent: how are people in academia supposed to survive? Can Baidu Baike—which is essentially isomorphic to trash—really satisfy our needs? Does even a single person in the department responsible for the blocking actually do academic work?"}, {"file": "translation_3331.html", "title": "Chat: Neural Networks and Deep Learning", "content": "← Back to Index Chat: Neural Networks and Deep Learning By 苏剑林 | June 06, 2015 Among all machine learning models, perhaps the most interesting and profound is the neural network model. I would like to share my humble opinion and talk about neural networks. Of course, this article does not intend to introduce neural networks from scratch; I am simply discussing my personal understanding of them. For friends who wish to further understand neural networks and deep learning, please refer to the following tutorials: http://deeplearning.stanford.edu/wiki/index.php/UFLDL Tutorial http://blog.csdn.net/zouxy09/article/details/8775360 Machine Classification Using classification as an example: in data mining or machine learning, there are many classification problems. For instance, classifying a sentence as \"positive\" or \"negative,\" or more finely into categories like happy, angry, or sad. Another typical classification problem is handwritten digit recognition, which involves classifying images into 10 categories (0, 1, 2, 3, 4, 5, 6, 7, 8, 9). Consequently, many classification models have been developed. Classification models are essentially performing fitting —a model is actually a function"}, {"file": "translation_3345.html", "title": "Received the New Edition of \"Quantum Mechanics and Path Integrals\"", "content": "← Back to Index Received the New Edition of \"Quantum Mechanics and Path Integrals\" By 苏剑林 | June 06, 2015 Cover of \"Quantum Mechanics and Path Integrals\" Today, I received the new edition of Feynman's classic work Quantum Mechanics and Path Integrals , sent by editor Wang Chao from Higher Education Press. I'm so excited... Quantum Mechanics and Path Integrals is a classic masterpiece by Feynman, and indeed a classic of quantum mechanics—it is currently the only book I have read that starts from path integrals and treats them as the first principle (Anthony Zee's Quantum Field Theory in a Nutshell seems to be the only one I've read that is purely based on path integrals as a method for quantum field theory, which is also very good). Other types of quantum mechanics textbooks also touch on path integrals, but without exception, they all derive the path integral from the Hamiltonian formalism. In those cases, the path integral can only be considered a corollary. However, path integrals clearly constitute one of the three formulations of quantum mechanics; they should be presentable as a fundamental principle of quantum mechanics rather than a derivation from another form. Feynman made"}, {"file": "translation_3354.html", "title": "[Translation] Giant Telescope: To Continue, There Must Be Sacrifice!", "content": "← Back to Index [Translation] Giant Telescope: To Continue, There Must Be Sacrifice! By 苏剑林 | June 10, 2015 Rendering of the Thirty Meter Telescope released in late 2007 Article from: New Scientist . This is a news report regarding the Thirty Meter Telescope (TMT). The project faced opposition from local residents, the reasons for which are deep-seated and complex. For more news about the TMT, you can visit: http://www.ctmt.org/ Giant Telescope in Hawaii: To Continue, There Must Be Sacrifice! One in four must go! After a two-month hiatus, construction of the giant Thirty Meter Telescope (TMT) in Hawaii has returned to its development process—but at the expense of other telescopes. In April, construction of the telescope was forced to a halt as protests from local Hawaiian residents intensified. Compared to this telescope, all current telescopes in the world pale in size—it will allow astronomers to peer to the very edge of the visible universe. It is located on Mauna Kea, a dormant volcano that many Hawaiians consider \"sacred ground,\" and its presence is viewed by them as an insult—especially since there are already more than a dozen telescopes on the summit. \"When astronomers firs"}, {"file": "translation_3360.html", "title": "Text Sentiment Classification (I): Traditional Models", "content": "← Back to Index Text Sentiment Classification (I): Traditional Models By 苏剑林 | June 22, 2015 Foreword: Back in April and May, I participated in two data mining competitions: the \"Liangjian Cup\" organized by the School of Physics and Telecommunication Engineering, and the 3rd \"Teddy Cup\" National Undergraduate Data Mining Competition. Coincidentally, in both competitions, one problem primarily involved Chinese sentiment classification work. When doing the \"Liangjian Cup,\" as I was a novice with limited skills, I only implemented a simple text sentiment classification model based on traditional ideas. In the subsequent \"Teddy Cup,\" through deeper study, I had basic knowledge of deep learning concepts and implemented a text sentiment classification model using deep learning algorithms. Therefore, I plan to post both models on my blog for readers' reference. Beginners can use them to compare the differences between the two and understand the relevant approaches. Experts are welcome to pass by with a smile. Based on Sentiment Dictionary Simple human judgment thinking Traditional text sentiment classification based on sentiment dictionaries is the simplest simulation of human memory and "}, {"file": "translation_3380.html", "title": "Efficient Implementation of the Apriori Algorithm using Pandas", "content": "← Back to Index Efficient Implementation of the Apriori Algorithm using Pandas By 苏剑林 | July 02, 2015 Latest Update: \"Efficient Implementation of the Apriori Algorithm with Numpy\" Recently, while working on data mining, I came across the Apriori algorithm. Since I hadn't specialized in this field before, I wasn't familiar with it, but now that it's come up in my work, I had to study it seriously. The Apriori algorithm is used for finding association rules—essentially discovering potential logic from a large dataset. For example, \"Condition A + Condition B\" is very likely to lead to \"Condition C\" ($A + B \\to C$); this is an association rule. Specifically, for instance, after a customer buys product A, they often buy product B (conversely, buying B doesn't necessarily mean they buy A). Or more complexly, customers who buy both A and B are likely to buy C (again, the reverse is not necessarily true). With this information, we can bundle certain products to achieve higher revenue. The algorithms used to seek these association rules are called association analysis algorithms. Beer and Diapers Beer and Diapers In the world of association algorithms, the most frequently cited case is prob"}, {"file": "translation_3390.html", "title": "Random Talk on Models | Models and Picking Mangoes", "content": "← Back to Index Random Talk on Models | Models and Picking Mangoes By 苏剑林 | July 15, 2015 Many people feel that terms like \"Model,\" \"Big Data,\" and \"Machine Learning\" are high-end and mysterious. In fact, they are not much different from picking fruit in our daily lives. This article uses a few thousand words to try and teach everyone how to pick mangoes... The Model Metaphor Suppose I want to pick the most delicious mango from a batch. Since I cannot directly cut them open to taste them, I can only observe the mangoes. The observable quantities include color, surface aroma, size, etc. These are the pieces of information (features) we can collect. There are many such examples in life, such as buying matches (perhaps young city dwellers haven't seen matches?). How do you judge the quality of a box of matches? Must you strike every single match to see if it lights up? Obviously not; at most, we can strike a few. If we strike them all, the matches are no longer matches. Of course, we can also look at the appearance of the matches and smell their scent—these actions are acceptable. We might discover that yellow, large mangoes are very sweet, but we also find that some not-so-yellow, sm"}, {"file": "translation_3402.html", "title": "Starting from \"0.999... equals 1\"", "content": "← Back to Index Starting from \"0.999... equals 1\" By 苏剑林 | July 21, 2015 Among the questions that can be asked from primary school all the way to university, yet are not easy to answer well, \"Does $0.999...$ actually equal $1$?\" is certainly a classic. However, providing a clear answer to this question is no easy feat; often, the person being asked becomes inadvertently confused, and some \"fringe scientists\" have even used this problem to \"create new mathematics.\" This article attempts to provide a relatively accessible yet rigorous answer to this question. What is Equality? To answer whether $0.999...$ equals $1$, we must first define \"equality\"! What counts as being equal? Does it really have to be written exactly the same way to be called equal? If that were the case, then $2-1$ would not even equal $1$, because $2-1$ and $1$ look different. Clearly, we need to provide a definition of \"equality\" that is mathematically rigorous yet universally accepted before we can judge equality. Obviously, the following definition is acceptable to many people: $a = b$ if and only if $|a-b|=0$. Based on this definition, we need to calculate whether $1-0.999...\\stackrel{?}{=}0$. Answering this i"}, {"file": "translation_3414.html", "title": "Text Sentiment Classification (II): Deep Learning Models", "content": "← Back to Index Text Sentiment Classification (II): Deep Learning Models By 苏剑林 | August 04, 2015 In the article \"Text Sentiment Classification (I): Traditional Models,\" I briefly introduced the traditional approach to text sentiment classification. Traditional methods are easy to understand and relatively stable. However, they face two difficult-to-overcome limitations: 1. Accuracy issues —traditional methods are generally satisfactory for common applications, but there is a lack of effective ways to further improve precision; 2. Background knowledge issues —traditional methods require the prior extraction of an emotional dictionary. This step often requires manual intervention to ensure accuracy. In other words, the person doing this work must not only be a data mining expert but also a linguist. This dependency on background knowledge hinders progress in natural language processing (NLP). Fortunately, deep learning has solved this problem (at least to a large extent). It allows us to build models for practical problems in specific fields with almost \"zero background.\" This article continues the discussion on text sentiment classification by explaining deep learning models. Parts"}, {"file": "translation_3426.html", "title": "Graphical Technique for the Series Expansion of $\\exp(1/2 t^2+xt)$", "content": "← Back to Index Graphical Technique for the Series Expansion of $\\exp(1/2 t^2+xt)$ By 苏剑林 | August 13, 2015 This article will study the Taylor expansion of the function of $t$:\n    $$\\exp\\left(\\frac{1}{2}t^2+xt\\right)$$\n    at $t=0$. Clearly, this is not difficult; it can be calculated by hand or with software. The answer is:\n    $$1+x t+\\frac{1}{2} \\left(x^2+1\\right) t^2+\\frac{1}{6}\\left(x^3+3 x\\right) t^3 +\\frac{1}{24} \\left(x^4+6 x^2+3\\right) t^4 + \\dots$$\n    However, this article will present a graphical method I constructed for this series. This graphical method allows for a relatively intuitive and convenient way to manually calculate the first few terms of the expansion. Later, we will discuss the origins of this graphical technique and its further applications. Graphical Method for the Series: Explanation First, it is obvious that to write out this series, the key is to determine each term of the expansion, which means we need to find:\n    $$f_k (x) = \\left.\\frac{d^k}{dt^k}\\exp\\left(\\frac{1}{2}t^2+xt\\right)\\right|_{t=0}$$\n    $f_k (x)$ is a $k$-th degree polynomial with integer coefficients in terms of $x$, where $k$ is the order of the expansion and the order of different"}, {"file": "translation_3441.html", "title": "Area Enclosed by a Closed Curve: A New Technique", "content": "← Back to Index Area Enclosed by a Closed Curve: A New Technique By 苏剑林 | August 30, 2015 This article presents an attempt to achieve the conversion between the area of a closed curve and a line integral without relying on Green's Theorem. This approach is relatively easy to understand and generalize because it only utilizes the coordinate transformation of double integrals. As for whether this technique truly has practical value, I invite readers to comment. Suppose a simple closed curve on a plane is given by the following parametric equations: \\begin{equation}\\left\\{\\begin{aligned}x = f(t)\\\\y = g(t)\\end{aligned}\\right.\\end{equation} where the parameter $t$ lies within some interval $[a, b]$, meaning $f(a)=f(b)$ and $g(a)=g(b)$. The problem now is to find the area of the region enclosed by this closed curve. Green's Theorem The usual approach to solve this is by using Green's Theorem: \\begin{equation}\\iint\\limits_D \\left(\\frac{\\partial Q}{\\partial x} - \\frac{\\partial P}{\\partial y}\\right)dxdy = \\oint\\limits_{\\partial D} Pdx+Qdy \\end{equation} It tells us that area integrals and line integrals can be transformed into one another. When $\\frac{\\partial Q}{\\partial x} - \\frac{\\partia"}, {"file": "translation_3451.html", "title": "Solutions for Exercises in \"Quantum Mechanics and Path Integrals\" V0.1", "content": "← Back to Index Solutions for Exercises in \"Quantum Mechanics and Path Integrals\" V0.1 By 苏剑林 | September 14, 2015 I forgot to tell everyone that I am a normal school student and I am currently a senior. According to the plan, I have already started interning at a high school, so for these two months, updates may not be frequent and replies may not be timely. Please bear with me. During these two months, I will spend a little time every day working on the exercises from Quantum Mechanics and Path Integrals , organizing them to share with everyone. This is currently version V0.1, and for now, it only contains solutions to most of the exercises in Chapters 2 and 3. Exercise Solutions for \"Quantum Mechanics and Path Integrals\" By the way, I used to be quite averse to books of exercise solutions, but Feynman's book is different. The exercises inside are not traditional or conventional practice problems, let alone \"homework\"; they are truly meaningful thought problems. Every exercise represents a deeper understanding of the original work and is also a significant challenge to oneself. Therefore, I believe that reflecting on these exercises and sharing the insights gained with readers is"}, {"file": "translation_3473.html", "title": "2015 Nobel Prize in Medicine: Chinese Person Included", "content": "← Back to Index 2015 Nobel Prize in Medicine: Chinese Person Included By 苏剑林 | Oct 5, 2015 It has been a long time since I wrote news about the Nobel Prize. In the first few years, I would follow it very closely and repost updates to the blog as soon as they were announced. However, in recent years, I have only kept an eye on the lists without updating the blog. This time, I am suddenly updating because I saw that for the first time, a Chinese name—Tu Youyou—appeared in the Nobel Prize in Medicine. I'm writing this briefly, just to share in the joy with everyone. 2015 Nobel Prize in Medicine Official Nobel Prize website: http://www.nobelprize.org/nobel_prizes/medicine/laureates/2015/tu-facts.html The name Tu Youyou is not the first time I've heard it; it seems to have appeared a couple of years ago, and she was reportedly nominated for the Nobel Prize then. However, her success in winning this year still came as a surprise to me, though I am of course very happy. This year's Nobel Prize in Medicine is shared by two Japanese scientists (William C. Campbell and Satoshi Ōmura) and one Chinese scientist (Tu Youyou). The first two each share 1/4 of the prize money, while Tu Youyou share"}, {"file": "translation_3476.html", "title": "Exercise Solutions for \"Quantum Mechanics and Path Integrals\" V0.2", "content": "← Back to Index Exercise Solutions for \"Quantum Mechanics and Path Integrals\" V0.2 By 苏剑林 | October 17, 2015 Due to being in an internship, I have been quite busy and have had less time to work on the problems. Furthermore, as the problems become increasingly difficult later on, the update speed for the exercise solutions has slowed down. This is now version 0.2, which basically completes the exercises for the first five chapters, organizes the layout, and includes errata for the new edition of Quantum Mechanics and Path Integrals . If you find any issues, please point them out. Thank you. Download: Solutions to Exercises for \"Quantum Mechanics and Path Integrals\" V0.2.pdf If you found this article helpful, you are welcome to share or donate. Donations are not for profit, but to know how much sincere attention Scientific Spaces has received from its readers. Of course, if you choose to ignore it, it will not affect your reading. Thank you again for your visit and support! ,\n    author={Su Jianlin},\n    year={2015},\n    month={Oct},\n    url={\\url{https://kexue.fm/archives/3476}},\n} Citation This is a machine translation of the original Chinese article: https://kexue.fm/archives/3476"}, {"file": "translation_3477.html", "title": "Running Python Scripts on Your Phone on a Schedule", "content": "← Back to Index Running Python Scripts on Your Phone on a Schedule By 苏剑林 | Oct 21, 2015 Without a doubt, data is the foundation of data analysis. For ordinary people like us, the most natural way to obtain large amounts of data is through web scraping. For me, the most straightforward way to write a crawler is using Python. In just a few lines of code, you can complete a practical crawler—it's very clean. (Please refer to: \"Recording the Process of Crawling Taobao/Tmall Comment Data\" ) Where should the crawler reside? The next question is, where should this crawler run? To crawl daily updated data, it often needs to run once a day, specifically at a certain scheduled time. In this case, it's not very realistic to keep it running on your own computer, as computers eventually need to be turned off. Some readers might think of using a cloud server; that is one method, but it involves additional costs. Inspired by the Great God Xiaoxia , I started thinking about running it on a router. Some high-end routers allow external USB drives and can be flashed with the Open-Wrt system (a Linux-kernel router system that can install Python like a normal Linux). This was a very attractive approac"}, {"file": "translation_3491.html", "title": "Information Entropy Method and Implementation for New Word Discovery", "content": "← Back to Index Information Entropy Method and Implementation for New Word Discovery By 苏剑林 | October 26, 2015 In previous posts on this blog, I have briefly touched upon the issues of Chinese text processing and mining. The most significant difference between Chinese data mining and similar tasks in English is that Chinese lacks spaces. To effectively complete linguistic tasks, word segmentation must be performed first. Currently, popular segmentation methods are dictionary-based. However, a major problem arises: where do these dictionaries come from? While common words can be manually collected into a dictionary, this approach fails to keep up with the constant emergence of new words, especially internet slang—which is often the key to linguistic tasks. Therefore, a core task in Chinese language processing is refining new word discovery algorithms. New word discovery refers to automatically identifying potential word-like language fragments from massive corpora without adding any prior material. A few days ago, I visited Xiaoxia's company and tried joining one of their development projects, primarily focused on processing web articles. Consequently, I brushed up on the algorithmi"}, {"file": "translation_3495.html", "title": "Friends, Let's Have a Bottle of Soda! The Interesting Soda Exchange Problem", "content": "← Back to Index Friends, Let's Have a Bottle of Soda! The Interesting Soda Exchange Problem By 苏剑林 | October 28, 2015 By 苏剑林 | Oct 28, 2015 | 37034 readers ———— Reminiscing about the elementary school math competitions I once participated in. Starting from an Elementary School Competition Problem When I was in the fifth grade of elementary school, I participated in my first math competition called the \"Yu Miao Cup.\" I don't remember most of the questions, but the only one I remember clearly is the following (not exactly the same, but the meaning is similar): Suppose a bottle of soda costs one dollar, and 4 empty bottles can be exchanged for one new bottle of soda. If I have 30 dollars, what is the maximum number of bottles of soda I can drink? Of course, the above situation might be too ideal, but it must be admitted that similar cases exist extensively in real life. For example, when buying turtles for food, because turtle shells can be used in medicine, some people recycle them, which means a certain number of turtle shells can be exchanged for a turtle, and so on. Can the readers calculate this quickly? Of course, this problem is not difficult. With 30 dollars, you can buy 30 bo"}, {"file": "translation_3511.html", "title": "ARXIV Math Paper Distribution: Partial Differential Equations are the Most Popular!", "content": "← Back to Index ARXIV Math Paper Distribution: Partial Differential Equations are the Most Popular! By 苏剑林 | Nov 13, 2015 The author has successfully been recommended for admission to the Pure Mathematics graduate program at Sun Yat-sen University. While this major is quite theoretical, I will continue to maintain my interest in data analysis, computer science, and other fields. Recently, I felt inspired to conduct research combining my major with data mining. Therefore, I crawled math papers from ARXIV for the past five years (2010 to 2014), including data such as titles, categories, years, and months, to perform a simple analysis of the \"market\" of mathematics in recent years. Personally, I believe that as ARXIV is currently the world's largest electronic database for preprints of scientific papers, analyzing its data can lead to conclusions that are representative to a certain extent. Of course, this article is intended as a practice piece for web crawling and basic data analysis, and it hasn't excavated particularly high-value information. At the end of the post, I've attached the data I crawled for interested readers to further analyze and study. Overall Situation Over these f"}, {"file": "translation_3522.html", "title": "Solutions to \"Quantum Mechanics and Path Integrals\" Exercises V0.3", "content": "← Back to Index Solutions to \"Quantum Mechanics and Path Integrals\" Exercises V0.3 By 苏剑林 | Nov 18, 2015 A new version of the solutions to Quantum Mechanics and Path Integrals has been released. Unlike the previous two versions, where each update completed approximately two chapters' worth of exercises, this update only adds 22 exercises from Chapter 6 (out of a total of 29 in Chapter 6). There are many reasons for this—being busy notwithstanding—the main reason is that starting from Chapter 6, the problems become increasingly complex and the volume of calculations grows. Although I am from the mathematics department, progress has been arduous. Furthermore, while Chapters 4 and 5 combined have only 25 exercises, Chapter 6 alone contains 29. Therefore, the workload for this update is significantly greater than that of the previous two updates. Why are there only 22 problems? Naturally, because I haven't finished them all yet. Why release the update before finishing? Because I feel that the subsequent problems are more closely related to Chapter 7. To prevent readers from waiting too long, the remaining problems will be released together with Chapter 7. Additionally, I have been work"}, {"file": "translation_3534.html", "title": "Entropy Explained: From Entropy and the Maximum Entropy Principle to Maximum Entropy Models (Part 1)", "content": "← Back to Index Entropy Explained: From Entropy and the Maximum Entropy Principle to Maximum Entropy Models (Part 1) By 苏剑林 | December 1, 2015 The Concept of Entropy As a physics enthusiast, I have always been fascinated and curious about the concept of \"entropy\" in statistical mechanics. Therefore, when I encountered data science, I developed a strong interest in maximum entropy models as well. What is entropy? In popular introductions, entropy is generally given two explanations: (1) entropy is a measure of uncertainty; (2) entropy is a measure of information. While they might seem different, they actually refer to the same thing. First, entropy as a measure of uncertainty quantifies our \"degree of ignorance\" about a certain phenomenon. Why is entropy also a measure of information? Since entropy represents our ignorance, the process of going from \"ignorance\" to \"full knowledge\" results in obtaining a certain amount of information. The more ignorant we are at the start, the greater the amount of information we gain upon reaching \"full knowledge.\" Thus, entropy, as a measure of uncertainty, can also be seen as a measure of information—more precisely, the maximum amount of informati"}, {"file": "translation_3546.html", "title": "Life is Short, I Use Python!", "content": "← Back to Index Life is Short, I Use Python! By 苏剑林 | December 06, 2015 Python Data Analysis and Mining in Action During the summer, at the invitation of Teddy Company, I worked on the companion volume to their book MATLAB Data Analysis and Mining in Action , titled Python Data Analysis and Mining in Action (there is also a companion volume for the R language). My primary tasks were writing the introduction to Python and translating the MATLAB code from the book into Python versions. I gladly accepted this task, first to earn some pocket money through part-time work, second to systematically train myself in Python programming, and third, to experience a major \"showdown\" between MATLAB, R, and Python. The book has now been officially released and can be found on Amazon , Dangdang , JD.com , and Taobao. I am also honored to be listed as one of the authors, making this my first published book. While this post may seem like an advertisement—and indeed it is—I am not here just to promote this book, but to \"promote\" Python itself. Regarding the book, I am confident that the Python scripts included, whether in terms of code conciseness or execution efficiency, will not lose to MATLAB or t"}, {"file": "translation_3552.html", "title": "Can't Afford \"Entropy\": From Entropy, Maximum Entropy Principle to Maximum Entropy Models (II)", "content": "← Back to Index Can't Afford \"Entropy\": From Entropy, Maximum Entropy Principle to Maximum Entropy Models (II) By 苏剑林 | December 11, 2015 Recap of the Previous Episode In the first article, the author introduced the concept of \"entropy\" and its origins. The formula for entropy is: $$S=-\\sum_x p(x)\\log p(x)\\tag{1}$$ or $$S=-\\int p(x)\\log p(x) dx\\tag{2}$$ We also learned that entropy represents both uncertainty and the amount of information; in fact, they are the same concept. Having discussed the concept of entropy, next is the \"Maximum Entropy Principle.\" The Maximum Entropy Principle tells us that when we want to obtain the probability distribution of a random event, if there is not enough information to completely determine the distribution (it might be that the type of distribution is unknown, or the type is known but several parameters are undetermined), the most \"conservative\" approach is to choose the distribution that maximizes the entropy. Maximum Entropy Principle Admitting Our Ignorance Many articles, when introducing the Maximum Entropy Principle, use the famous saying—\"Don't put all your eggs in one basket\"—to explain it colloquially. However, the author humbly believes"}, {"file": "translation_3567.html", "title": "“Entropy” Can't Afford It: From Entropy and the Maximum Entropy Principle to the Maximum Entropy Model (Part 3)", "content": "← Back to Index “Entropy” Can't Afford It: From Entropy and the Maximum Entropy Principle to the Maximum Entropy Model (Part 3) By 苏剑林 | December 20, 2015 Recap of the Previous Episode # In the previous article , I shared my understanding of the Maximum Entropy Principle, including its significance, methods for solving it, and some simple yet common applications. At the end of the previous post, we also derived the Normal distribution using the Maximum Entropy Principle to illustrate its profound implications and broad significance. In this article, I will introduce a model based on the Maximum Entropy Principle — the Maximum Entropy Model. This article introduces the Maximum Entropy Model through the lens of a supervised classification problem . \"Supervised\" means that the model is built based on data that has already been labeled. In fact, the Maximum Entropy Principle discussed in the second article is the primary concept; the Maximum Entropy Model is essentially just an extension or application of that principle. Maximum Entropy Model # Classification: What Does It Mean? # Before introducing the Maximum Entropy Model, let's discuss what a classification problem entails. Suppose"}, {"file": "translation_3576.html", "title": "Modified the Display Method for Formulas (Mobile)", "content": "← Back to Index Modified the Display Method for Formulas (Mobile) By 苏剑林 | December 24, 2015 Due to reader Li xiaobo once again reporting that the site's formulas have poor support on mobile devices, I have made some modifications to the display of formulas on the website. If you are browsing with a computer, you should not notice any changes to the site. However, if you are browsing via a mobile terminal, you should find that formulas originally parsed by MathJax have turned into image-based formulas. That's right, this is a very compromise-based solution: the site identifies the client, and if it is a mobile device, it uses the image-based formula display method. So far, no errors have been found with image formulas on mobile terminals (please test this). This method has some drawbacks; for example, image-based formulas are not as aesthetically pleasing, and Chinese characters within formulas cannot be displayed. The formulas make calls to http://latex.codecogs.com/gif.latex , and I express my gratitude to them. Everyone is welcome to test and provide feedback on issues at: http://bbs.spaces.ac.cn/topic/show/9 Citation This is a machine translation of the original Chinese article"}, {"file": "translation_3580.html", "title": "APOD: Geminids Meteor Shower Over Xinglong Observatory", "content": "← Back to Index APOD: Geminids Meteor Shower Over Xinglong Observatory By 苏剑林 | December 28, 2015 I remember when Scientific Space first started, there was not much original content. For a while, I was translating APOD (Astronomy Picture of the Day) images, but later I gradually focused on original content and stopped translating. Today, I am sharing an image again: the Geminid meteor shower over Xinglong Mountain, taken by domestic enthusiast Steed Yu and featured on APOD. Geminids over Xinglong Mountain (Source: http://apod.nasa.gov/apod/ap151223.html ) Image Description: Where do Geminid meteors come from? In this wonderful composite image, you can find that the Geminids, formed by these sand-sized pieces of rock, have meteor trails with a radiant point located in the constellation of Gemini. The orbits of these meteors in the solar system point towards their parent body—Asteroid 3200 Phaethon. However, this conclusion is still somewhat debatable because this unusual object is usually in a dormant state. Perhaps Asteroid 3200 Phaethon experienced dust-releasing events more intense than we know, but even so, what exactly happened and why such violent eruptions occurred remains an"}, {"file": "translation_3582.html", "title": "Solutions to Exercises of \"Quantum Mechanics and Path Integrals\" V0.4", "content": "← Back to Index Solutions to Exercises of \"Quantum Mechanics and Path Integrals\" V0.4 By 苏剑林 | January 09, 2016 By 苏剑林 (Su Jianlin) | \n        2016-01-09 | \n        40,366 Readers | \n        Category: Physics and Chemistry The solution manual for the exercises in Quantum Mechanics and Path Integrals has finally progressed to version 0.4. At present, the exercises for the first seven chapters have been basically completed. Today is already January 9, 2016. 2015 has passed, and I forgot to wish everyone a Happy New Year. I am truly sorry. Let me add it here: I wish everyone a Happy New Year and may everything go as you wish! I am now a senior in college, approaching final exams and graduation. I have been very busy lately with many things. One of them is that I joined the startup team of a small internet company, where I am responsible for text mining and occasionally writing web crawlers, among other things. I feel that since joining, I have gained a lot of experience and technical knowledge, reaching a new level compared to my last internship. Right now, there are several tasks lined up, and I am busy in a fulfilling way. Additionally, I have started writing my graduation thesis. F"}, {"file": "translation_3587.html", "title": "When Big Data Enters the Kitchen: Let Big Data Teach You to Cook!", "content": "← Back to Index When Big Data Enters the Kitchen: Let Big Data Teach You to Cook! By 苏剑林 | January 18, 2016 Introduction # Food (images from the Internet) In the \"About the Author\" section of the sidebar on this site, one line reads \"Kitchen Enthusiast.\" Even though I'm not particularly good at cooking, the kitchen is indeed one of my hobbies. Of course, I have many interests—mathematics, physics, astronomy, computer science, etc.—I love them all and want to learn everything, which often leads to being a \"jack of all trades, master of none.\" As mentioned in previous articles, data mining is also a hobby of mine. What interesting results might emerge when my passion for data mining intersects with my passion for the kitchen? I did exactly that: I wrote a simple crawler to scrape a batch of recipe data from the \"Home Cooking\" directory of Meishi China and performed some basic data analysis. (I would like to express my sincere thanks to Meishi China. I chose them because their data is quite standardized.) The data analysis was conducted on my company's high-performance servers, which made the process incredibly smooth. In total, I collected 18,209 recipes, encompassing 9,700 types of "}, {"file": "translation_3594.html", "title": "Simple Xunlei VIP Account Getter (Python)", "content": "← Back to Index Simple Xunlei VIP Account Getter (Python) By 苏剑林 | Jan 20, 2016 When working on Windows, I often use Xunlei to download things. If the speed is slow or there are no resources, especially for some relatively niche videos, Xunlei's VIP member service can always be a big help. Later, I accidentally discovered a software called \"Xunlei VIP Account Getter,\" which can obtain some temporary VIP accounts for use. This is a great tool because while opening a Xunlei membership isn't expensive, I don't download things frequently, so it always felt like a bit of a waste. With this, I can use it for free whenever I need to download something. Simple Xunlei VIP Account Getter Recently, I moved to Mac, and while Mac also has a Xunlei client, that account getter is an .exe file and cannot run on Mac. I originally thought the construction of the getter would be very complex, but as it turns out, after some packet sniffing research, I found that the principle behind the account getter is extremely simple. To put it bluntly, it is just a simple crawler. It merely goes to the following two websites that provide accounts and crawls them accordingly: http://yunbo.xinjipin.com/ http://www"}, {"file": "translation_3604.html", "title": "Tinkering with HiWiFi on New Year's Eve: SSH Reverse Proxy", "content": "← Back to Index Tinkering with HiWiFi on New Year's Eve: SSH Reverse Proxy By 苏剑林 | February 07, 2016 Happy Year of the Monkey! Today is New Year's Eve. I would like to briefly wish everyone a Happy New Year's Eve and a Happy New Year! I hope everyone becomes a \"study god\" in the coming year. ^_^ I've spent the last couple of days tinkering with the router at home. Usually, only my parents are home, so to save money, we just bridge the neighbor's network to access the internet. Originally, we used a Xiaomi Router mini, but its functionality is extremely limited in relay mode, and I didn't want to flash third-party firmware (because I'd lose the app control features, which is inconvenient). So, I simply swapped it for a HiWiFi 3 (JeeWiFi). The HiWiFi retains most of its functions even in relay mode (which I think is how it should be; I don't understand the logic behind Xiaomi mini losing so many features after entering relay mode). As a tinkerer, once a new router arrives, there's a lot to configure. HiWiFi is based on OpenWrt, so it has high playability. First, complete the relay and get online—this is simple and won't be detailed here. Next is obtaining SSH permissions, which HiWi"}, {"file": "translation_3630.html", "title": "Extremum Principle for Integral Estimation—A Primary Version of the Variational Principle", "content": "← Back to Index Extremum Principle for Integral Estimation—A Primary Version of the Variational Principle By 苏剑林 | Feb 15, 2016 If you have been following Scientific Spaces for a while, you may have noticed that I have a particular preference for extremum principles. For instance, I have previously written a series of articles titled \"Natural Extremum,\" introducing some extremum problems and the calculus of variations. In physics, I favor the form of the principle of least action. In data mining, I am deeply interested in Maximum Entropy models based on the principle of maximum entropy. Recently, while working on exercises in \"Quantum Mechanics and Path Integrals,\" I became very interested in the variational principle discussed in Chapter 11. When learning something new, my method is to understand its principles and ideas using the simplest possible example before progressively making it more complex; this way, I won't get lost. The variational principle is a powerful method for estimating path integrals (which are functional integrals or infinite-dimensional integrals). It is natural to wonder whether there is a similar estimation principle for finite-dimensional integrals, such a"}, {"file": "translation_3638.html", "title": "The Intuitive Origin and Clever Uses of Entropy", "content": "← Back to Index The Intuitive Origin and Clever Uses of Entropy By 苏剑林 | February 20, 2016 In my previous work \"Can't Afford 'Entropy': From Entropy, the Maximum Entropy Principle to the Maximum Entropy Model (Part 1)\" , I introduced entropy from a more \"professional\" perspective and provided an interpretation. However, as a measure of uncertainty, entropy should have a more common and intuitive origin. This article aims to supplement that part and provide some clever applications as a result. The Intuitive Origin of Entropy Consider natural numbers composed of the ten digits 0–9. If we require the number to be less than 10,000, there are naturally 10,000 possibilities. If we say \"a certain natural number less than 10,000,\" then any number from 0 to 9,999 could appear, so 10,000 becomes a measure of the uncertainty of this event. Similarly, consider a sequence of length $m$ composed of $n$ different elements (where elements can be reused). There are $n^m$ possible cases for this sequence, and here $n^m$ also serves as a measure of the uncertainty of this situation. $n^m$ is in an exponential form, and the numbers can become exceptionally large. Therefore, we take the logarithm to g"}, {"file": "translation_3641.html", "title": "Interesting Problem: How to List All Subsets of a Set Programmatically", "content": "← Back to Index Interesting Problem: How to List All Subsets of a Set Programmatically By 苏剑林 | March 04, 2016 Recently, during a programming task, I needed to implement a function to list all subsets of a given set. Interested readers might want to think about how they would approach this themselves. While searching for resources, I discovered a very ingenious method. This ingenious method utilizes binary numbers. The inspiration comes from the fact that a set with $n$ elements has $2^n$ subsets, and an $n$-bit binary number also has exactly $2^n$ possible variations. Therefore, one only needs to iterate through the numbers from $0$ to $2^n - 1$, convert them to binary, and then read each bit; whenever a 1 is encountered, select the element from the original set corresponding to that position. If implemented in Python, the code is remarkably concise: import numpy as np\n\ns = np.array(range(5))\nn = len(s)\n\nfor i in range(2**n):\n    b = bin(i)[2:].zfill(n)\n    print(s[[j == '1' for j in b]]) The result is: [] [4] [3] [3 4] [2] [2 4] [2 3] [2 3 4] [1] [1 4] [1 3] [1 3 4] [1 2] [1 2 4] [1 2 3] [1 2 3 4] [0] [0 4] [0 3] [0 3 4] [0 2] [0 2 4] [0 2 3] [0 2 3 4] [0 1] [0 1 4] [0 1 3] [0 1 "}, {"file": "translation_3644.html", "title": "OpenWrt Automatically Scans for WiFi and Connects as a Repeater", "content": "← Back to Index OpenWrt Automatically Scans for WiFi and Connects as a Repeater By 苏剑林 | March 06, 2016 Recently, I acquired an extremely mini router—built from a 25 x 25mm VoCore development board. With its casing, it measures only 37.4 x 34 x 25.9mm, which is just slightly larger than a typical portable WiFi dongle. ( Link ) VoCore Router Despite its small size, its specifications are quite decent: CPU Processor: Ralink RT5350 360MHz MIPS 24KEc Memory: 32MB 133MHz SDRAM Flash: 16MB Expansion Interfaces: SPI, I2C, I2S WiFi Wireless: 802.11n Ethernet Network: 10/100MHz x 2 GPIO Expansion: 28 (Reused) UART Interfaces: UART Lite / UART Full USB Interface: USB 2.0, up to 480M Power Supply: 3.3V ~ 6V Simply put, it allows us to install OpenWrt, which opens up a lot of possibilities. Because it is so small and portable, it is perfect for use as a travel router. Let's get to work. As everyone knows, phones or laptops have a feature where if we have connected to several WiFi networks in different places, the device remembers that information. When we return to one of those locations, it automatically connects to the respective WiFi without manual intervention (except for those requiring w"}, {"file": "translation_3651.html", "title": "Sharing Campus Resources through SSH Dynamic Port Forwarding (including practical tips)", "content": "← Back to Index Sharing Campus Resources through SSH Dynamic Port Forwarding (including practical tips) By 苏剑林 | March 07, 2016 By 苏剑林 | March 07, 2016 As is widely known, the two most valuable resources of a campus network are: first, IPv6, which is the most ideal way to access websites like Google (though not all universities have IPv6); and second, paper databases. Generally, universities purchase download rights for various databases (CNKI, Wanfang, etc.) for campus users. While there are many ways for off-campus users to access Google, such as VPNs, accessing resources like CNKI is particularly precious. Usually, off-campus users have to ask someone on campus to download papers for them, or they have to pay for them (and it's expensive!). As the webmaster is still a student, I enjoy both IPv6 and paper database resources at school, which is indeed very convenient. Since I started using an OpenWrt router, I have been thinking about how to share these campus network resources. I previously considered setting up a PPTP VPN, but I felt it was slightly complicated (granted, compared to other VPNs, PPTP is very simple, but I still didn't like it much). Furthermore, I hadn't solved t"}, {"file": "translation_3680.html", "title": "[Euler Mathematics] A Summary of the Bernoulli Series and Related Series", "content": "← Back to Index [Euler Mathematics] A Summary of the Bernoulli Series and Related Series By 苏剑林 | March 20, 2016 Recently, while calculating path integrals, I frequently encountered the following two types of infinite series: \\[\\sum_n \\frac{1}{n^2\\pm\\omega^2}\\quad \\text{and} \\quad \\prod_n \\left(1\\pm\\frac{\\omega^2}{n^2}\\right)\\] Of course, one can straightforwardly compute the results using Mathematica, but I still want to know why, or at least have a general idea. Bernoulli Series When $\\omega=0$, the first series becomes the famous Bernoulli series (the Basel problem): \\[\\sum_n \\frac{1}{n^2}=1+\\frac{1}{4}+\\frac{1}{9}+\\frac{1}{16}+\\dots\\] Since it is related to the Bernoulli series, it is natural to start with the summation of the Bernoulli series. The most admired method for summing the Bernoulli series is the \"proof\" given by Euler through bold conjecture and analogy. Euler considered the series: \\[\\frac{\\sin \\sqrt{x}}{\\sqrt{x}}=1-\\frac{x}{6}+\\frac{x^2}{120}-\\frac{x^3}{5040}+\\dots\\] From the expression on the left, it is easy to see that the roots of $\\frac{\\sin \\sqrt{x}}{\\sqrt{x}}=0$ are $n^2 \\pi^2,\\,n=1,2,3,\\dots$. Next, we consider a polynomial equation of degree $n$: \\[1+a_1 "}, {"file": "translation_3691.html", "title": "[Memo] Solutions for Remote Control of Phones from a Computer", "content": "← Back to Index [Memo] Solutions for Remote Control of Phones from a Computer By 苏剑林 | March 29, 2016 Recently, due to research on data mining, I needed to find a way to remotely control a phone (mainly Android) from a computer. I searched for some tools on the internet and am recording the results here purely as a memo. Readers with similar needs may find this useful for reference. I had previously set up remote control on Alibaba Cloud servers and Raspberry Pi, and I remembered that the remote control tool under Linux is called VNC. So, I Googled and Baidu-ed for \"vnc server android,\" \"vnc server apk,\" etc. I found that there are indeed quite a few such tools, the most famous being \"droid vnc server.\" However, I tested several similar software, and while they were indeed VNC software, they did not display correctly on several of my Android 4.x devices (screen corruption/artifacts). I was forced to abandon them. Looking at the dates, I realized that these software programs basically stopped updating in 2013 and generally only supported up to Android 2.3, which explains it. Having failed to find a working VNC server, I had to settle for the next best thing and search for terms like"}, {"file": "translation_3692.html", "title": "Solutions to Exercises for \"Quantum Mechanics and Path Integrals\" V0.5", "content": "← Back to Index Solutions to Exercises for \"Quantum Mechanics and Path Integrals\" V0.5 By 苏剑林 | April 01, 2016 The work on the solutions to the exercises continues to progress with difficulty. This is version 0.5. Compared to version 0.4, I have skipped Chapters 8 and 9 for now and prioritized the exercises for Chapters 10 and 11, which cover the statistical mechanics sections. Chapter 10 contains 10 exercises, and Chapter 11 actually has no exercises. Although they seem few, the difficulty of each exercise is quite high. The main content of these two chapters involves using path integral methods to calculate partition functions in statistical mechanics, which is inherently a grueling task. Combined with Feynman's descriptive style, it is easy for readers to get a general idea but very difficult to carry out the actual calculations. In fact, I only managed to complete these exercises by referencing a significant amount of material, both in Chinese and English. Although I say they are \"completed,\" I have only finished 9 out of the 10 problems. I am still confused about Problem 10-3; the result I obtained differs from the one Feynman provides, so I could not proceed further. I am men"}, {"file": "translation_3696.html", "title": "An Implicit Solution to a Nonlinear Difference Equation", "content": "← Back to Index An Implicit Solution to a Nonlinear Difference Equation By 苏剑林 | April 09, 2016 Problem Source A thread on the Mathematics Research Forum, which I frequent, discussed the asymptotic solution for the following nonlinear difference equation: $$a_{n+1}=a_n+\\frac{1}{a_n^2},\\, a_1=1$$ The original post is here . I benefited a lot from that thread and learned many new techniques. The main idea is through cubing both sides and then setting $x_n=a_n^3$, transforming it into an equivalent recurrence problem: $$x_{n+1}=x_n+3+\\frac{3}{x_n}+\\frac{1}{x_n^2},\\,x_1=1$$ Then, by using clever techniques, the asymptotic expansion can be obtained: $$x_n = 3n+\\ln n+a+\\frac{\\frac{1}{3}(\\ln n+a)-\\frac{5}{18}}{n}+\\dots$$ I will not mention the specific process here; readers can go to the aforementioned post to learn on their own. However, although this form of the solution is exquisite, there are some aspects that I am not entirely satisfied with: 1. The solution is an asymptotic series, which means the radius of convergence is actually 0; 2. It is a solution in the form of $n^{-k}$, which makes it difficult to calculate for small $n$, making high-precision calculation relatively difficul"}, {"file": "translation_3728.html", "title": "[Memo] Using Raspberry Pi 3 as a Wireless Router", "content": "← Back to Index [Memo] Using Raspberry Pi 3 as a Wireless Router By 苏剑林 | April 12, 2016 The Raspberry Pi 3, released in early March, comes with built-in Wi-Fi and Bluetooth. Combined with its existing Ethernet port, it has essentially become a wireless router. I couldn't resist getting one, intending to use it as both a router and a NAS. There are already many tutorials on using a Raspberry Pi as a router, though most are based on the Raspberry Pi 2. Versions prior to the 3 did not have built-in Wi-Fi, requiring an external wireless adapter. The Raspberry Pi 3 makes configuration much more convenient since Wi-Fi is integrated. I successfully configured it by referring to two foreign tutorials and am recording the process here. Reference tutorials: https://frillip.com/using-your-raspberry-pi-3-as-a-wifi-access-point-with-hostapd/ https://gist.github.com/Lewiscowles1986/fecd4de0b45b2029c390#file-rpi3-ap-setup-sh Configuring the Wireless Hotspot The main software packages used are hostapd and dnsmasq : Then, add the following to the end of /etc/dnsmasq.conf (modify the IP and network segment yourself; this file already exists with detailed configuration examples, but all lines are co"}, {"file": "translation_3731.html", "title": "Stirling's Formula and Asymptotic Series", "content": "← Back to Index Stirling's Formula and Asymptotic Series By 苏剑林 | April 15, 2016 Stirling's approximation, or Stirling's formula, was initially proposed as an approximation for factorials:\n    $$n!\\sim \\sqrt{2\\pi n}\\left(\\frac{n}{e}\\right)^n$$\n    The symbol $\\sim$ means:\n    $$\\lim_{n\\to\\infty}\\frac{\\sqrt{2\\pi n}\\left(\\frac{n}{e}\\right)^n}{n!}=1$$\n    Further increasing the precision of Stirling's formula leads to the so-called Stirling series:\n    $$n!=\\sqrt{2\\pi n}\\left(\\frac{n}{e}\\right)^n\\left(1+\\frac{1}{12n}+\\frac{1}{288n^2}\\dots\\right)$$\n    Unfortunately, this is an asymptotic series. Related resources include: https://zh.wikipedia.org/zh-cn/Stirling's Formula https://en.wikipedia.org/wiki/Stirling's_approximation This article will discuss an improved derivation of Stirling's formula and its asymptotic series, and explain why asymptotic series are \"asymptotic.\" Stirling's Formula There are several ways to derive the Stirling series. Wikipedia provides a derivation based on the Euler-Maclaurin summation formula. However, it has some shortcomings. Because it calculates the series sum $n!=\\sum_{k=1}^n \\ln k$, it cannot directly yield the constant value $\\sqrt{2\\pi}$, and this "}, {"file": "translation_3735.html", "title": "Coming Back...", "content": "← Back to Index Coming Back... By 苏剑林 | May 15, 2016 The publication time of the previous blog post was April 15th, and as of today, it has been exactly a month without an update. However, the traffic to Scientific Spaces is still there. Thank you all for your support of this space; BoJone sincerely apologizes for the long silence. Before resuming updates, please allow me to recount the \"journal of events\" from the past month. During this \"vanishing\" month, my main focus was on my graduation thesis and a data mining competition . First, regarding the graduation thesis: it was submitted on April 22nd, and the defense was on April 29th. Once the defense was over, my work on the graduation thesis concluded. My thesis mainly covered the applications of path integrals in describing random walks, partial differential equations, and stochastic differential equations. Since it is an undergraduate thesis, it shouldn't be too obscure; therefore, as a whole, it is relatively easy to read and can serve as an introductory tutorial for path integrals. I will slightly refine it later and publish it in several parts on Scientific Spaces for everyone to critique and provide feedback. Speaking of pa"}, {"file": "translation_3739.html", "title": "A Banter: Universal Gravitation and Einstein's Theory", "content": "← Back to Index A Banter: Universal Gravitation and Einstein's Theory By 苏剑林 | May 18, 2016 I am not a researcher of gravity, nor have I studied gravity in great depth. In terms of theoretical physics, I have studied classical mechanics and quantum mechanics far more than general relativity. Therefore, I probably shouldn't be talking about gravity to avoid misleading others. However, during a bus ride, the driver's braking and acceleration made me think of some things related to gravity that I found quite interesting. So, I’m sharing them here for everyone to enjoy and for experts to correct. Equivalence Principle # Riding a Bus Gravity is more accurately called \"Universal Gravitation.\" The term \"Universal\" (万有) has two meanings: \n1. All objects can generate gravity; \n2. All objects are affected by gravity. Einstein found it extremely strange that a force could be \"universal.\" This is the most distinct difference between gravity and the other three fundamental forces. In contrast, electromagnetic interaction only exists where there is \"charge,\" weak interaction only exists between fermions, and so on. Besides gravity, do we encounter any other \"universal\" forces in our daily lives?"}, {"file": "translation_3749.html", "title": "Path Integral Series: 1. My Graduation Thesis", "content": "← Back to Index Path Integral Series: 1. My Graduation Thesis By 苏剑林 | May 30, 2016 Previously, I promised to share my graduation thesis for everyone's criticism and correction, but I have been lazy about taking action. In fact, the main content of the thesis consists of some introductory-level material on path integrals, titled \"Path Integral Methods for Random Walk, Stochastic Differential Equations, and Partial Differential Equations.\" My abstract was written as follows: Starting from the random walk model, this paper obtains general results concerning the random walk model. Then, based on the random walk model, path integrals are introduced. Through the path integral method, the transformation between random walks, stochastic differential equations, and parabolic partial differential equations is realized, and some calculation cases are provided. The path integral method is a form of quantum theory, but in practice, it can be abstracted into a useful mathematical tool. The primary method used in this paper is this abstracted path integral. Furthermore, quantum mechanics features a quite typical parabolic partial differential equation—the Schrödinger equation—which physicists ha"}, {"file": "translation_3750.html", "title": "Path Integral Series: 2. Random Walk Model", "content": "← Back to Index Path Integral Series: 2. Random Walk Model By 苏剑林 | May 30, 2016 The random walk model is simple in form, but through it, rich results can be derived. It is one of the foundations for various diffusion models in physics and is equivalent to Brownian motion in stochastic processes. Literature reviewed by the author indicates that mathematicians have conducted thorough research on the symmetric random walk problem [2], explored the relationship between random walks and partial differential equations [3], and also studied asymmetric random walks [4]. However, the shortcomings of existing results include: 1. The methods used to derive the probability distributions or partial differential equations for random walks are not concise or clear enough; 2. More general asymmetric random walk problems have not been researched. This chapter compensates for these deficiencies. First, through methods of generating functions and Fourier transforms, the partial differential equations satisfied by asymmetric random walks are derived. Furthermore, it is proposed that since random walks are easy to simulate on a computer, simulating the solutions of partial differential equations via r"}, {"file": "translation_3757.html", "title": "Path Integral Series: 3. Path Integral", "content": "← Back to Index Path Integral Series: 3. Path Integral By 苏剑林 | June 02, 2016 The path integral is a description of quantum mechanics that originated with the physicist Feynman [5]. It is a type of functional integral and has become the mainstream form of modern quantum theory. In recent years, interest in it has increased, especially in its applications outside the quantum field, and several works have appeared, such as [7]. However, few people in China are familiar with path integrals, and many students majoring in quantum physics may not have even heard of them. From a mathematical perspective, path integrals are a method for finding the Green's function of partial differential equations. We know that in the study of partial differential equations, if the corresponding Green's function can be found, it is of great help to the research. Usually, Green's functions are not easy to solve. However, constructing a path integral only requires the Green's function over an infinitesimal time interval, so it is quite simple in both form and concept. This chapter contains no new material but aims to provide a concise and direct introduction to path integrals starting from the random walk p"}, {"file": "translation_3762.html", "title": "Path Integral Series: 4. Stochastic Differential Equations", "content": "← Back to Index Path Integral Series: 4. Stochastic Differential Equations By 苏剑林 | June 09, 2016 This chapter applies path integrals to stochastic differential equations (SDEs) and obtains the same results as for asymmetric random walks, thereby proving the equivalence with that model. The idea of using path integrals for the study of stochastic differential equations has a long history. Feynman, in his work [5], already established the relationship between path integrals and linear stochastic differential equations. For non-linear cases, there have been numerous studies, but they are somewhat confusing; for example, reference [8] even provided incorrect results. Starting from the discrete concept of path integrals, this article clearly establishes the Jacobian determinant relationship between two path integral differentials, thereby establishing path integrals for non-linear stochastic differential equations as well. The results of this article are consistent with those in reference [9]. Concepts The study in this article is limited to stochastic ordinary differential equations. Their difference from general ordinary differential equations lies in the introduction of a Brownian m"}, {"file": "translation_3766.html", "title": "Path Integral Series: 5. Examples and Overview", "content": "← Back to Index Path Integral Series: 5. Examples and Overview By 苏剑林 | June 09, 2016 The path integral method provides a new perspective for solving certain stochastic problems. An Example: Stock Price Model Consider a risky asset (such as a stock) with a price $S_t$ at time $t$. We consider the time interval $[0,T]$, where 0 is the initial time and $T$ is the maturity date. $S_t$ is treated as a continuous-time variable that follows the stochastic differential equation: \\begin{equation}\n    dS_t^0=rS_t^0 dt; \\quad dS_t=S_t(\\mu dt+\\sigma dW_t). \\tag{70}\n    \\end{equation} Where $\\mu$ and $\\sigma$ are two constants, and $W_t$ is a standard Brownian motion. The equation for $S_t$ is a stochastic differential equation (SDE), which is generally solved using stochastic calculus. Stochastic calculus differs from ordinary calculus in that, when performing a first-order expansion, the $dS_t^2$ term cannot be ignored because $dW_t^2=dt$. For example, let $S_t=e^{x_t}$, then $x_t=\\ln S_t$: \\begin{align}\n    dx_t &= \\ln(S_t+dS_t)-\\ln S_t = \\frac{dS_t}{S_t} - \\frac{dS_t^2}{2S_t^2} \\nonumber \\\\\n    &= \\frac{S_t(\\mu dt+\\sigma dW_t)}{S_t} - \\frac{[S_t(\\mu dt+\\sigma dW_t)]^2}{2S_t^2} \\nonumber \\\\"}, {"file": "translation_3774.html", "title": "A Brief Exploration of OCR Technology: 1. Overview", "content": "← Back to Index A Brief Exploration of OCR Technology: 1. Overview By 苏剑林 | June 17, 2016 Preface: As mentioned in previous blog posts, last month I participated in the 4th Teddy Cup Data Mining Competition. I worked on Problem A, which is related to OCR systems, and I promised to open-source the final results. I've been busy with graduation and moving recently, so I haven't had time to organize this content until now. I am sharing these results not because they are particularly groundbreaking or advanced (on the contrary, after comparing them with Baidu's paper \"Progress in Image Recognition Based on Deep Learning: Several Practices from Baidu\" , I realized my approach still essentially follows the traditional framework and is far from the cutting edge). Rather, I'm sharing this because while OCR technology is relatively mature, there aren't many articles online providing a detailed explanation of an OCR system's implementation. This article aims to fill that gap. I have always believed that for technology to advance, it must be open-sourced (though this is debatable in China, as open-sourcing can easily lead to copycats). Whether it's research in mathematics and physics or data m"}, {"file": "translation_3781.html", "title": "OCR Technology Exploration: 2. Background and Assumptions", "content": "← Back to Index OCR Technology Exploration: 2. Background and Assumptions By 苏剑林 | June 17, 2016 Research Background Optical Character Recognition (OCR) refers to the process of converting text within images into computer-editable text content. Numerous researchers have studied related technologies for a long time, leading to the creation of many mature OCR technologies and products, such as Hanwang OCR, ABBYY FineReader, and Tesseract OCR. It is worth mentioning that ABBYY FineReader not only has high accuracy (including for Chinese recognition) but also preserves most of the original layout, making it a very powerful commercial OCR software. However, among the many established OCR products, except for Tesseract OCR, most are closed-source or even commercial software; we can neither embed them into our own programs nor improve upon them. The only choice for open source is Google's Tesseract OCR, but its recognition performance is not particularly good, and its Chinese recognition accuracy is relatively low, requiring further improvement. In summary, whether for academic research or practical application, it is necessary to explore and improve OCR technology. Our team has divided t"}, {"file": "translation_3785.html", "title": "Exploration of OCR Technology: 3. Feature Extraction (1)", "content": "← Back to Index Exploration of OCR Technology: 3. Feature Extraction (1) By 苏剑林 | June 18, 2016 As the first step of an OCR system, feature extraction aims to identify characteristics of candidate text regions in an image, facilitating text localization in the second step and recognition in the third step. In this section, we focus on imitating the human eye's process for handling images and Chinese characters, carving out an innovative path for image processing and character localization. This work is the core of the entire OCR system and the most pivotal part of our efforts. Traditional text segmentation approaches mostly follow the path of \"edge detection + erosion and dilation + connected component detection,\" such as in paper [1]. However, edge detection in images with complex backgrounds leads to excessive edges in background sections (i.e., increased noise), while edge information for the text itself is easily overlooked, resulting in poor performance. If erosion or dilation is performed at this stage, the background and text regions will merge, further worsening the outcome. (In fact, we have traveled far enough down this path ourselves—we even wrote our own edge detection "}, {"file": "translation_3802.html", "title": "A Brief Exploration of OCR Technology: 3. Feature Extraction (2)", "content": "← Back to Index A Brief Exploration of OCR Technology: 3. Feature Extraction (2) By 苏剑林 | June 18, 2016 Layer-by-layer Recognition Once the image is effectively layered, we can further design corresponding models based on our previous assumptions to identify text regions in the image through layer-by-layer processing. Connectivity 8-connectivity As can be seen, each layer of the image consists of several connected components. Since text itself is composed of relatively dense strokes, it often forms a connected region. The \"connected\" property here is defined as 8-connectivity, meaning the 8 pixels surrounding a specific pixel are defined as adjacent pixels, and adjacent pixels are defined as belonging to the same connected region. After defining connected regions, each layer is segmented into several connected components. In other words, we gradually decompose the original image, as shown in Figure 9. Figure 9: Image Decomposition Structure Diagram Erosion Resistance Once the image is decomposed to the level of connected regions, we stop further subdivision and start identifying which regions are potential text areas. Here we require the text to have a certain degree of erosion res"}, {"file": "translation_3818.html", "title": "A Brief Exploration of OCR Technology: 4. Text Localization", "content": "← Back to Index A Brief Exploration of OCR Technology: 4. Text Localization By 苏剑林 | June 24, 2016 After the first part, we have successfully extracted the text features of the image. Next, we will perform text localization. The main process is divided into two steps: 1. Proximity search, with the goal of circling single lines of text; 2. Text cutting, with the goal of segmenting single lines of text into individual characters. Proximity Search We can perform a connected component search on the extracted feature map, treating each resulting connected component as a Chinese character. This works for most Chinese characters, but it is not suitable for some simpler characters such as \"小\" (small), \"旦\" (dawn), \"八\" (eight), and \"元\" (yuan). Because these characters lack connectivity, they end up being split apart, as shown in Figure 13. Therefore, we need to use a proximity search algorithm to integrate regions that likely form a single character and obtain single-line text regions. Figure 13: Direct search for connected components will split characters like \"元\" apart The purpose of a proximity search is to perform dilation to \"stick\" together regions that likely form a character. If dila"}, {"file": "translation_3823.html", "title": "Exploration of OCR Technology: 5. Text Segmentation", "content": "← Back to Index Exploration of OCR Technology: 5. Text Segmentation By 苏剑林 | June 24, 2016 After the previous step of obtaining single-line text areas, we can then determine how to segment a single line of text into individual characters. Since the model in the third step is built for individual characters, this step is also necessary. Uniform Segmentation Based on the assumption of square Chinese characters, the simplest segmentation method is actually uniform segmentation. This means cutting the single-line text into square images according to the height without any additional judgment. This approach can handle most single-line text, as shown in the top image below. Uniform segmentation success Uniform segmentation failure Of course, the drawbacks of uniform segmentation are also obvious. Most Chinese characters are square, but most English letters and digits are not. Therefore, if mixed Chinese and English text occurs, uniform segmentation fails, as shown in the bottom image above. Statistical Segmentation As seen from Figure 15, after the preceding operations, characters are well-separated from one another. Thus, another relatively simple approach is to perform a vertical summa"}, {"file": "translation_3831.html", "title": "Celestial Events in June 2016", "content": "← Back to Index Celestial Events in June 2016 By 苏剑林 | June 25, 2016 Through the first and second steps, we have been able to identify the areas of individual characters in the image. Next, we can build a corresponding model to recognize single characters. Model Selection Regarding the model, we chose the Convolutional Neural Network (CNN) model from deep learning. By using multiple layers of convolutional neural networks, we constructed a recognition model for single characters. Convolutional Neural Networks are a type of artificial neural network that has become the mainstream model in the current field of image recognition. It reduces the complexity of the network model and the number of weights through 局部感知野 (Local Receptive Fields) and 权值共享 (Weight Sharing) methods. Its structure is more similar to biological neural networks, which predicts that it will inevitably have superior effects. In fact, the main reasons we chose convolutional neural networks are: Automatic feature extraction from original images: The convolutional neural network model can directly take the original image as input, eliminating the difficult core part of manual feature extraction required by traditional"}, {"file": "translation_3842.html", "title": "Preliminary Exploration of OCR Technology: 7. Language Models", "content": "← Back to Index Preliminary Exploration of OCR Technology: 7. Language Models By 苏剑林 | June 26, 2016 Due to reasons such as image quality, even the best-performing recognition models can have the possibility of recognition errors. To reduce the recognition error rate, the recognition problem can be combined with statistical language models, and the optimal recognition result can be provided through dynamic programming methods. This is one of the important methods for improving OCR recognition performance. Transition Probability In the process of analyzing our experimental results, we encountered the following case. Due to possible reasons such as image blurriness, the word \"电视\" (television) was recognized as \"电柳\" (electric willow). Using the image model alone cannot resolve this problem effectively because, from the image model's perspective, \"电柳\" appeared to be the optimal choice. However, a language model can solve this problem quite ingeniously. The reason is simple: based on a large amount of text data, we can calculate the probabilities of the word \"电视\" and the word \"电柳.\" We find that the probability of \"电视\" is far greater than that of \"电柳,\" so we would believe the word is \"电视"}, {"file": "translation_3854.html", "title": "A Preliminary Exploration of OCR Technology: 8. Comprehensive Evaluation", "content": "← Back to Index A Preliminary Exploration of OCR Technology: 8. Comprehensive Evaluation By 苏剑林 | June 26, 2016 Data Verification Although the model works well in a test environment, practice is the sole criterion for testing truth. In this section, we use our own model to perform comparative verification against JD.com's test data. Measuring the quality of an OCR system consists of two parts: (1) whether the text regions are successfully boxed; (2) whether the boxed text is successfully recognized. We use a scoring method to evaluate the recognition effect of each image. The scoring rules are as follows: If the boxed text area matches the box file provided in JD's detection samples, 1 point is added. If the text is correctly recognized, an additional 1 point is added. Finally, the score for each image is the total points divided by the total number of characters. According to this rule, the maximum score for each image is 2 points, and the minimum is 0 points. If the score exceeds 1, it indicates that the recognition effect is quite good. After comparison with JD's test data, our model's average score is approximately 0.84, which is passable but leaves room for improvement. Model "}, {"file": "translation_3856.html", "title": "Brief Exploration of OCR Technology: 9. Code Sharing (Conclusion)", "content": "← Back to Index Brief Exploration of OCR Technology: 9. Code Sharing (Conclusion) By 苏剑林 | June 26, 2016 File Description: 1. image.py — Image processing functions, primarily feature extraction; 2. model_training.py — Training the CNN single-character recognition model (requires a high-performance server, preferably with GPU acceleration, otherwise it is incredibly slow); 3. ocr.py — Recognition functions, including single-character segmentation, recognition using the previously trained model, and dynamic programming to enhance results; 4. main.py — Main file, used to call files 1 and 3; 5. Characters included in our model.txt (UTF-8 encoded) File 1: image.py File 2: model_training.py File 3: ocr.py File 4: main.py If you found this article helpful, you are welcome to share or donate to this article. Donations are not for profit, but to know how much sincere attention Scientific Space has received from its readers. Of course, if you ignore this, it will not affect your reading. Welcome and thank you again! Citation This is a machine translation of the original Chinese article: https://kexue.fm/archives/3856 Original author: 苏剑林 (Su Jianlin) Original publication: 科学空间 (Scientific Sp"}, {"file": "translation_3863.html", "title": "Text Sentiment Classification (3): To Segment OR Not To Segment", "content": "← Back to Index Text Sentiment Classification (3): To Segment OR Not To Segment By 苏剑林 | June 29, 2016 After the Teddy Cup competition last year, I wrote a brief blog post introducing the application of deep learning in sentiment analysis: \"Text Sentiment Classification (2): Deep Learning Models\" . Although the article was quite rough, it received a surprising amount of feedback from readers, which caught me off guard. However, there were some unclear points in the implementation of that article because: 1. Keras has undergone significant changes since then, rendering the original code non-functional; 2. The code included might have been randomly modified by me, so the version released wasn't the most appropriate one. Therefore, nearly a year later, I am revisiting this topic and completing some tests that were left unfinished. Why use deep learning models? Besides reasons like higher accuracy, another important reason is that it is currently the only model capable of achieving \"end-to-end\" learning . \"End-to-end\" means being able to input raw data and labels directly, letting the model complete the entire process—including feature extraction and model learning. Looking back at our"}, {"file": "translation_3873.html", "title": "From Boosting Learning to Neural Networks: Seeing Mountains as Mountains?", "content": "← Back to Index From Boosting Learning to Neural Networks: Seeing Mountains as Mountains? By 苏剑林 | July 01, 2016 A while ago, while teaching text mining to students at Hanshan Normal University in Chaozhou, I delved into Boosting learning algorithms and engaged in some brainstorming. Eventually, I clarified some essential characteristics of Boosting learning algorithms and obtained some unexpected results. For instance, some theoretical proofs of the AdaBoost algorithm can also be used to explain why neural network models are so powerful. AdaBoost Algorithm Boosting learning belongs to the category of ensemble models. Of course, rather than calling it an algorithm, it is better to describe it as a problem-solving strategy. Taking supervised classification as an example, it suggests that weak classifiers (as long as their accuracy is strictly greater than a random classifier) can be combined in a certain way to obtain an excellent classifier (theoretically, the accuracy can reach 100%). The AdaBoost algorithm is an example of a Boosting algorithm, proposed by Schapire in 1996. It constructed an explicit scheme for Boosting learning and provided a theoretical proof regarding the erro"}, {"file": "translation_3887.html", "title": "Action Camera Test: Starry Sky of My Hometown", "content": "← Back to Index Action Camera Test: Starry Sky of My Hometown By 苏剑林 | August 03, 2016 I remember wanting to try shooting the starry sky a long time ago, but unfortunately, I never had the equipment. Previously, I only knew that DSLRs could shoot the starry sky; therefore, my longstanding idea was to buy a DSLR as soon as I had the money. For various reasons, I kept procrastinating, and eventually, I gradually felt that for someone like me with \"three-minute heat\" (fleeting interests), the significance of a DSLR wasn't actually that great. In the past two years, with the promotion by Xiaomi, Yi Action Cameras have gradually stirred up a wave of action cameras in China. These cameras are characterized by being small, flexible, and not expensive (compared to DSLRs). Flexibility means not just that they are easy to carry, but also refers to functional flexibility—for example, the first-generation Yi camera even supported programmable shooting! (Writing programs to control the shutter, ISO, shooting interval, and implementing timed shooting, etc.) This naturally attracted me quickly. When the Yi Action Camera 2 was crowdfunding, I grit my teeth and bought one. I went home a few days ag"}, {"file": "translation_3889.html", "title": "An Implicit Function Solution for Nonlinear Difference Equations", "content": "← Back to Index An Implicit Function Solution for Nonlinear Difference Equations By 苏剑林 | August 04, 2016 Recently, I have been contemplating some natural language processing problems and some non-linear analysis issues, leaving no time to summarize and publish posts, for which I apologize. What this article discusses is a perturbation scheme for first-order non-linear difference equations (higher-order ones can be treated similarly). Theoretically, this method can provide an explicit asymptotic solution for any first-order non-linear difference equation. Non-linear Difference Equations For a general first-order non-linear difference equation \\begin{equation}\\label{chafenfangcheng}x_{n+1}-x_n = f(x_n)\\end{equation} Generally, difference equations rarely have analytical solutions; therefore, one must use tools such as asymptotic analysis to investigate the properties of non-linear difference equations. Often, we first consider replacing the difference with a derivative to obtain the differential equation \\begin{equation}\\label{weifenfangcheng}\\frac{dx}{dn}=f(x)\\end{equation} as an approximation of the difference equation $\\eqref{chafenfangcheng}$. Aside from the fact that differenti"}, {"file": "translation_3902.html", "title": "Two Amazing Python Libraries: tqdm and retry", "content": "← Back to Index Two Amazing Python Libraries: tqdm and retry By 苏剑林 | August 13, 2016 Python is basically the only programming language I currently use for work, calculation, and data mining (except for symbolic computation, where I use Mathematica). Of course, basic Python functions are not very powerful, but its strength lies in the vast number of third-party extension libraries. When selecting a third-party library for Python, I always consider it carefully, hoping to pick the simplest and most intuitive one (because I'm not that smart and can't use things that are too complex). In terms of data processing, I use Numpy and Pandas the most—these two are definitely king-level libraries. Of course, I must mention Scipy, but I rarely use it directly; it’s usually called indirectly through Pandas. For visualization, it goes without saying that it's Matplotlib. In terms of modeling, I use Keras for deep learning models directly, as Keras has become a very popular deep learning framework. If I'm doing text mining, I usually also use jieba (for word segmentation) and Gensim (for topic modeling, including models like word2vec). For machine learning, there is also the popular Scikit-Learn"}, {"file": "translation_3908.html", "title": "[Chinese Word Segmentation Series] 1. Fast Word Segmentation Based on AC Automaton", "content": "← Back to Index [Chinese Word Segmentation Series] 1. Fast Word Segmentation Based on AC Automaton By 苏剑林 | August 17, 2016 Foreword: This summer, I spent a lot of time on Chinese word segmentation and language models. I hit walls countless times and gained a few scattered insights. I plan to write a series to share these experiences. Although it is a \"series,\" it is more of a collection of notes rather than a systematic tutorial. I hope readers will understand. Chinese Word Segmentation I won't say much about the introduction and importance of Chinese word segmentation. matrix67 has a very clear introduction here regarding segmentation and segmentation algorithms, which is well worth reading. In text mining, although many articles have explored processing methods without segmentation, such as the blog post \"Text Sentiment Classification (3): To Segment or Not to Segment\" , segmentation is generally the first step for text mining. Therefore, an effective segmentation algorithm is crucial. Of course, as the first step, Chinese word segmentation has been explored for a long time. Much of the current work is summary-based or involves minor improvements rather than major shifts. Curren"}, {"file": "translation_3913.html", "title": "[Chinese Segmentation Series] 2. New Word Discovery Based on Segmentation", "content": "← Back to Index [Chinese Segmentation Series] 2. New Word Discovery Based on Segmentation By 苏剑林 | August 18, 2016 The previous article discussed fast segmentation based on dictionaries and AC automata. Dictionary-based segmentation has a distinct advantage: it is easy to maintain and adapts well to specific fields. If migrating to a new domain, one only needs to add new words corresponding to that field to achieve better results. Of course, whether a high-quality, domain-adapted dictionary is easy to obtain depends on the specific situation. This article focuses on the discovery of new words. This topic was previously discussed in last year's article \"Information Entropy Methods and Implementation of New Word Discovery\" . The algorithm was derived from matrix67's article \"Sociolinguistics in the Internet Era: SNS-Based Text Data Mining\" . In that article, three main indicators were used—frequency, cohesion (which, after taking the logarithm, is what we call Pointwise Mutual Information), and freedom (boundary entropy)—to judge whether a snippet constitutes a word. If you have actually tried to implement this algorithm, you will find several difficulties. First, to find $n$-charact"}, {"file": "translation_3922.html", "title": "[Chinese Word Segmentation Series] 3. Character Tagging and the HMM Model", "content": "← Back to Index [Chinese Word Segmentation Series] 3. Character Tagging and the HMM Model By 苏剑林 | August 19, 2016 In this article, we pause our introduction of dictionary-based methods and turn instead to character tagging methods. As previously mentioned, character tagging approaches word segmentation by assigning a label to each character in a sentence. For instance, as noted earlier, using a 4-tag set ( single , a single character as a word; begin , the start of a multi-character word; middle , the middle part of a word with three or more characters; end , the end of a multi-character word. All abbreviated by their first letters.), the phrase \"为人民服务\" (serve the people) could be tagged as \"sbebe\". The 4-tag set is not the only way to label; similarly, there is a 6-tag set. Theoretically, more tags allow for finer precision and better results, but too many tags might lead to the problem of insufficient samples. Generally, the 4-tag and 6-tag sets are the most commonly used. It is worth mentioning that transforming a problem into sequence-to-sequence learning by tagging each character is not just a word segmentation method, but a way of thinking for solving a wide range of natural"}, {"file": "translation_3924.html", "title": "[Chinese Word Segmentation Series] 4. seq2seq Character Labeling Based on Bidirectional LSTM", "content": "← Back to Index [Chinese Word Segmentation Series] 4. seq2seq Character Labeling Based on Bidirectional LSTM By 苏剑林 | August 22, 2016 About the Character Labeling Method The previous article discussed the character labeling method for word segmentation. It is important to note that the character labeling method has great potential; otherwise, it would not have achieved top results in public tests. In my view, there are two main reasons why character labeling is effective. The first reason is that it transforms the word segmentation problem into a sequence labeling problem, and this labeling is aligned—meaning the input characters and output labels have a one-to-one correspondence, which is a mature problem in sequence labeling. The second reason is that this labeling method is actually a process of summarizing semantic patterns. Taking 4-tag labeling as an example, we know that the character \"李\" (Li) is a common surname and often serves as the first character of a multi-character word (a person's name), thus it is labeled as 'b'. Similarly, \"想\" (Xiang), due to words like \"理想\" (ideal), has a high proportion of being labeled as 'e'. Consequently, when \"李想\" appear together, even if th"}, {"file": "translation_3936.html", "title": "Entering Sun Yat-sen University South Campus, Tinkering with the Campus Network", "content": "← Back to Index Entering Sun Yat-sen University South Campus, Tinkering with the Campus Network By 苏剑林 | September 05, 2016 Starting my journey as a graduate student, I hope that one day I can reach the realm of the \"Sweeper Monk\" (a hidden master). After entering Sun Yat-sen University (SYSU), several frustrating things happened. First, the most annoying thing is that the school starts extremely early; opening on August 26. It feels like at least a week earlier than most other schools. Is there any point in starting so early? Next, I feel that SYSU's management system is quite chaotic, far worse than my undergraduate institution, South China Normal University. Well, I won't complain further about these trifles. Next, I had to set up the campus network, which was where the \"struggle\" began. We are at the South Campus, and the campus network is authenticated via the Ruijie (StarView) client. I use a MacBook, and fortunately, SYSU humanely provides a Mac version of the Ruijie client, which is only about 1MB in size; quite good. However, as everyone knows, MacBooks do not have a built-in Ethernet port. Every time I want to go online, I have to plug in a USB network adapter and connect"}, {"file": "translation_3942.html", "title": "Core Entity Recognition based on Bidirectional LSTM and Transfer Learning", "content": "← Back to Index Core Entity Recognition based on Bidirectional LSTM and Transfer Learning By 苏剑林 | September 06, 2016 During the summer vacation, I participated in the Core Entity Recognition Competition jointly organized by Baidu and Xi'an Jiaotong University . The final results were quite good, so I'm recording it here. The performance of the model is not the absolute best, but it excels at being \" end-to-end \" and has strong transferability, which I believe will be of reference value to everyone. The theme of the competition is \"Core Entity Recognition,\" which actually involves two tasks: Core Recognition + Entity Recognition. Although these two tasks are related, in traditional natural language processing programs, they are generally handled separately. This time, they need to be combined. If we only look at \"Core Recognition,\" it is a traditional keyword extraction task. However, the difference is that traditional purely statistics-based approaches (such as TF-IDF extraction) are not feasible because a core entity in a single sentence might only appear once. In such cases, statistical estimation is unreliable, and it is better to understand it from a semantic perspective. I in"}, {"file": "translation_3956.html", "title": "[Chinese Word Segmentation Series] 5. Unsupervised Word Segmentation Based on Language Models", "content": "← Back to Index [Chinese Word Segmentation Series] 5. Unsupervised Word Segmentation Based on Language Models By 苏剑林 | September 12, 2016 So far, the first four articles have introduced several ideas for word segmentation, including dictionary-based methods using maximum probability and character-tagging methods based on HMM or LSTM. These are established research methods; what I have done is simply summarize them. Dictionary-based methods and character-tagging each have their own pros and cons. I have been wondering: can we create an unsupervised word segmentation model that only needs a large-scale corpus for training? That is to say, how to segment should be determined by the corpus itself, independent of the language. Simply put, given enough data, the corpus should tell us how to segment words. This sounds perfect, but how is it achieved? \"2. New Word Discovery Based on Segmentation\" provided an initial thought, but it wasn't thorough enough. The new word discovery method there can indeed be seen as a type of unsupervised segmentation approach, as it uses simple \"aggregation\" (cohesion) to judge whether a cut should be made. However, from the perspective of a full segmentation"}, {"file": "translation_3963.html", "title": "[Understanding Riemannian Geometry] 1. A Geometric Path", "content": "← Back to Index [Understanding Riemannian Geometry] 1. A Geometric Path By 苏剑林 | Oct 14, 2016 It has been a month since my last update. During this month, I have spent a considerable amount of time investigating the understanding of Riemannian geometry, and I would like to share some of my insights with everyone. I remember that the \"Understanding Matrices\" series written by Meng Yan and the \"New Understanding of Matrices\" written by myself both received very positive responses from readers. This time, I have adopted the same naming convention and titled this series \"Understanding Riemannian Geometry.\" Ants living in a two-dimensional space Riemannian geometry is the branch of geometry that studies intrinsic geometry. In simple terms, it considers the possibility that we live in a curved space. For example, imagine an ant living on a two-dimensional sphere. As individuals living within a curved space, we may not possess enough wisdom to \"embed\" our curvature into a higher-dimensional space for study. Just as the ant only knows how to crawl on the surface and cannot perceive the sphere from the perspective of a \"surface in three-dimensional space\" (because the sphere is its entire w"}, {"file": "translation_3969.html", "title": "[Understanding Riemannian Geometry] 2. From Pythagorean Theorem to Riemannian Metric", "content": "← Back to Index [Understanding Riemannian Geometry] 2. From Pythagorean Theorem to Riemannian Metric By 苏剑林 | October 14, 2016 Riemannian Metric Geometry, derived from the Greek \"Geometry,\" originally means earth-measurement. Since it is a measurement, there must be a frame of reference, and one must know how to calculate distances. With a reference, we can establish a coordinate system and record the coordinates of every point. As for calculating distances, we have the great Pythagorean theorem: $$ds^2 = dx^2 + dy^2 \\tag{1}$$ But we have overlooked two problems here. The first problem is that we do not necessarily use a Cartesian coordinate system. If we use polar coordinates, then it should be: $$ds^2 = dr^2 + r^2 d\\theta^2 \\tag{2}$$ Therefore, one can imagine that the most general form should be: $$ds^2 = E(x^1, x^2)(dx^1)^2 + 2F(x^1, x^2)dx^1 dx^2 + G(x^1, x^2)(dx^2)^2 \\tag{3}$$ What does this formula mean? It's simple: just as there is no reason to require the whole world to use the same currency, there is no need to require every part of the world to use the same coordinate system. A more reasonable approach is to let each location use its own coordinate system (local coordin"}, {"file": "translation_3977.html", "title": "【Understanding Riemannian Geometry】 3. Geodesics", "content": "← Back to Index 【Understanding Riemannian Geometry】 3. Geodesics By 苏剑林 | October 15, 2016 Geodesics Riemannian metrics should not be difficult to understand. In differential geometry textbooks, we have already studied the \"First Fundamental Form\" of surfaces. In fact, the two are the same thing; the only difference is the perspective. Differential geometry views a surface as a two-dimensional subset of three-dimensional space, while Riemannian geometry studies geometric problems intrinsically from the perspective of the two-dimensional surface itself. What does geometry care about? In fact, geometry is concerned with \" objective entities \" that are independent of transformations (or things that remain invariant under transformations); this is the definition of geometry. According to the \"Erlangen Program\" proposed by Klein, geometry is the study of properties that remain invariant under a certain group of transformations. If the transformations are limited to rigid transformations (translation, rotation, reflection), it is Euclidean geometry; if the transformations are general linear transformations, it is affine geometry. Riemannian geometry, on the other hand, is concerned with "}, {"file": "translation_3998.html", "title": "[Understanding Riemannian Geometry] 4. Connection and Covariant Derivative", "content": "← Back to Index [Understanding Riemannian Geometry] 4. Connection and Covariant Derivative By 苏剑林 | Oct 16, 2016 Vectors and Connection Once we establish our own coordinate system at our location, we can perform various measurements. The result of a measurement might be a scalar, such as temperature or mass; these quantities remain the same regardless of the coordinate system you use. However, sometimes we measure vectors, such as velocity, acceleration, or force. These quantities are objective entities, but because the measurement results are expressed in terms of coordinate components, the components will be completely different if we change coordinates. If all positions used the same coordinates, there would naturally be no controversy. However, as we have emphasized repeatedly, people at different locations may use different coordinate systems for various reasons. Therefore, when we write down a vector $A^{\\mu}$, strictly speaking, we should also specify that it was measured at position $\\boldsymbol{x}$: $A^{\\mu}(\\boldsymbol{x})$. We only omit this when there is no risk of ambiguity. At this point, we are already able to perform some calculations. For instance, if $A^{\\mu}$ is "}, {"file": "translation_4014.html", "title": "[Understanding Riemannian Geometry] 5. Riemann Curvature", "content": "← Back to Index [Understanding Riemannian Geometry] 5. Riemann Curvature By 苏剑林 | October 18, 2016 Now we turn our attention to Riemann curvature. Overall, Riemann curvature provides a scheme that allows people living inside a space to calculate the degree of curvature of the space they inhabit. As the saying goes, \"One does not know the true face of Mount Lu, only because one is inside the mountain,\" and \"The observer sees clearly, while the participant is mired in confusion.\" Therefore, being able to discover the curvature of a space while being inside it is a remarkable feat, as if we have transcended our existing space and gone to a higher-dimensional space to look down from above. Truly, \"As far as the heart can reach, that is how far the road and the world extend.\" If we look from the perspective of a higher-dimensional space, it is easy to spot the curvature of a space. For example, a geodesic in a curved space, when viewed from a higher-dimensional space, is a curve for which curvature can be calculated. However, in the original space, it is \"straight\"—the geodesic is a generalization of the concept of a straight line. Thus, it is impossible to discover the curvature of spa"}, {"file": "translation_4026.html", "title": "Independent Components of Curvature", "content": "← Back to Index Independent Components of Curvature By 苏剑林 | October 19, 2016 The Riemann curvature tensor is an extremely important tensor; the space is flat if and only if all its components are zero. It also appears in Einstein's field equations. In short, as long as Riemannian geometry is involved, the Riemann curvature tensor is bound to be the core content. As we have seen, the Riemann curvature tensor has 4 indices, which also means it has $n^4$ components, where $n$ is the dimension of the space. Thus, in 2, 3, and 4-dimensional spaces, it has 16, 81, and 256 components respectively. Clearly, calculating it is quite a painful task. Fortunately, this tensor has many symmetry properties that significantly reduce the number of independent components. Let's analyze this point. First, let's derive some symmetry properties of the Riemann curvature tensor, which are consistent with classic textbooks. Define:\n$$R_{\\mu\\alpha\\beta\\gamma}=g_{\\mu\\nu}R^{\\nu}_{\\alpha\\beta\\gamma} \\tag{50} $$\nThe reason for defining this quantity relates to the distinction between contravariant and covariant tensors; here we primarily care about the geometric perspective, so we will skip the detailed analy"}, {"file": "translation_4033.html", "title": "[Understanding Riemannian Geometry] 7. The Gauss-Bonnet Formula", "content": "← Back to Index [Understanding Riemannian Geometry] 7. The Gauss-Bonnet Formula By 苏剑林 | October 21, 2016 Excitingly, our approach to deriving Riemann curvature also allows us to catch a glimpse of the beauty of the Gauss–Bonnet formula, offering a true experience of researching intrinsic geometry. The Gauss–Bonnet formula is a classic formula in global differential geometry that establishes a link between the local and global properties of a space. Starting from a geometric path and combining elements of matrix transformations and mathematical analysis, we have step-by-step derived geodesics, covariant derivatives, and the curvature tensor. Now, we can even arrive at the classical Gauss–Bonnet formula, demonstrating how far we have come on this journey. Although the process is not perfect, it remains true to the core of this series: geometric intuition. The purpose of this article is to share an intuitive approach to Riemannian geometry; as it is a way of thinking, the focus is on the exchange of ideas rather than rigorous proof. Therefore, for all readers, please treat this series as supplementary material for Riemannian geometry. Formal Rewrite First, we can rewrite Equation $(4"}, {"file": "translation_4046.html", "title": "【Understanding Riemannian Geometry】8. Geometry Everywhere (Geometrization of Mechanics)", "content": "← Back to Index 【Understanding Riemannian Geometry】8. Geometry Everywhere (Geometrization of Mechanics) By 苏剑林 | November 02, 2016 The manifestation and application of Riemannian geometry in General Relativity, while perhaps not a household name, is likely something most readers have heard of. When one talks about the application of Riemannian geometry in physics, the first reflex is usually General Relativity. A common view is that the discovery of General Relativity greatly advanced the development of Riemannian geometry. While this is indeed a fact, what most people do not know is that there are traces of Riemannian geometry even in classical Newtonian mechanics. The content of this article discusses how to geometrize mechanics, thereby using the concepts of Riemannian geometry to describe it. This process essentially provides a framework that can incorporate theories from many other fields into the system of Riemannian geometry. The starting point of Riemannian geometry is the Riemannian metric, from which geodesics can be obtained through variation. In this sense, a Riemannian metric provides a variational principle. Conversely, can a variational principle provide a Riemannian"}, {"file": "translation_4051.html", "title": "An Introduction to Exterior Calculus: 1. Introduction and Inspiration", "content": "← Back to Index An Introduction to Exterior Calculus: 1. Introduction and Inspiration By 苏剑林 | Nov 04, 2016 Preface In the \"Understanding Riemannian Geometry\" series, I shared some \"geometric\" insights into Riemannian geometry, while leaving one question behind: how do we actually calculate the Riemann tensor? MTW's Gravitation mentions a method based on exterior calculus, but I was unfamiliar with exterior calculus, so I set out to learn it. Indeed, it was the efficient steps for calculating the curvature tensor in Gravitation that made me decide to delve deeper into exterior calculus. As it turns out, efficiency is a powerful primary motivator. This series of articles mainly shares some insights from learning exterior calculus. It has undergone multiple revisions and improvements and covers a wide range of content, such as exterior products, moving frames, exterior derivatives, and their applications in Riemannian geometry, finally concluding with an effective way to calculate curvature. Notation Notes: In this series, bold letters represent vectors, matrices, and bases; ordinary letters represent scalars, which may be scalar functions or vector components. Unless otherwise speci"}, {"file": "translation_4054.html", "title": "[A Brief Talk on Exterior Derivatives] 2. The Power of Antisymmetry", "content": "← Back to Index [A Brief Talk on Exterior Derivatives] 2. The Power of Antisymmetry By 苏剑林 | November 04, 2016 Inner Product and Exterior Product The power of vectors (referring here temporarily to vectors in two-dimensional or three-dimensional space) lies in the definition of inner products and exterior products (more often called cross products, vector products, etc.). Both are operations between two vectors, where the inner product is defined to be symmetric, while the exterior product is defined to be antisymmetric, and both satisfy the distributive law. Following the tradition of textbooks, we use $\\langle,\\rangle$ to denote the inner product and $\\land$ to denote the exterior product. For the exterior product, $\\times$ is more commonly used, but to avoid an excess of symbols, we will uniformly use $\\land$. We write vectors in terms of their basis, for example\n$$\\boldsymbol{A}=\\boldsymbol{e}_{\\mu}A^{\\mu} \\tag{1} $$\nwhere $\\boldsymbol{e}_{\\mu}$ represents a set of basis vectors, and $A^{\\mu}$ represents the components of the vector. Let's calculate the inner and exterior products of two vectors $\\boldsymbol{A}, \\boldsymbol{B}$:\n$$\\begin{aligned}&\\langle \\boldsymbol{A}, \\boldsy"}, {"file": "translation_4058.html", "title": "[Brief Discussion on Exterior Differential] 3. Orthogonal Frames", "content": "← Back to Index [Brief Discussion on Exterior Differential] 3. Orthogonal Frames By 苏剑林 | November 05, 2016 As is well known, mastering Riemannian geometry requires a strong sense of geometric intuition. But beyond that, Riemannian geometry described in the language of components also requires good analytical skills to untangle, because there are numerous indices representing components and summations, appearing everywhere at first glance. This cumbersome component language is not always pleasing; in many places, it is even notorious. In the language of components, we can essentially establish a coordinate system of any form locally, i.e., using an arbitrary set of bases $\\{\\boldsymbol{e}_{\\mu}\\}$, or natural frames. However, it is undeniable that under an orthogonal frame (orthonormal basis), many equations become much simpler. Given our familiarity with Euclidean space, research conducted under an orthogonal frame may feel more intuitive. Therefore, if conditions permit, we should use orthogonal frames $\\{\\hat{\\boldsymbol{e}}_{\\mu}\\}$, even if they are moving frames. Here, we use the $\\hat{}$ notation to mark the orthogonal frame. For example, we have the differential element \\[d"}, {"file": "translation_4059.html", "title": "A Brief Introduction to Exterior Differentiation (4): Differential is Not \"Micro\"", "content": "← Back to Index A Brief Introduction to Exterior Differentiation (4): Differential is Not \"Micro\" By 苏剑林 | November 05, 2016 Exterior Differentiation The exterior product of vectors is generally only defined for spaces of no more than 3 dimensions. To use antisymmetric operations in higher-dimensional spaces, we need the differential forms and exterior differentiation described below. We know that the differential of any function of $x$ can be written as a linear combination of $dx^{\\mu}$. Here, each $dx^{\\mu}$ actually plays the role of a basis. Therefore, we might as well regard $dx^{\\mu}$ as a set of basis vectors, and we call any arbitrary function a differential 0-form, while expressions like $\\omega_{\\mu}dx^{\\mu}$ are called differential 1-forms. On the basis of $dx^{\\mu}$, we define the wedge product $\\land$, which provides an antisymmetric operation $dx^{\\mu}\\land dx^{\\nu}$, and we call expressions such as $\\omega_{\\mu\\nu}dx^{\\mu}\\land dx^{\\nu}$ differential 2-forms. Note that this is the wedge product in an $n$-dimensional space; $dx^{\\mu}\\land dx^{\\nu}$ is actually a basis for a new space and cannot be represented as a linear combination of $dx^{\\mu}$. Next, we allow $\\la"}, {"file": "translation_4062.html", "title": "[An Introduction to Exterior Calculus] 5. Geometric Meaning", "content": "← Back to Index [An Introduction to Exterior Calculus] 5. Geometric Meaning By 苏剑林 | November 06, 2016 Regarding the previously described exterior differentiation, as well as the integration of differential forms that will be briefly mentioned later, these are purely algebraic definitions and do not inherently possess any geometric meaning. However, we can associate certain formulas or definitions with geometric content to help us understand them more deeply and apply them more flexibly. However, this is merely a correspondence, and it depends on our interpretation. For example, we say that the exterior derivative formula $$\\int_{\\partial D} Pdx+Qdy = \\int_{D} \\left(\\frac{\\partial Q}{\\partial x}-\\frac{\\partial P}{\\partial y}\\right)dx \\wedge dy \\tag{32}$$ corresponds to Green's formula $$\\int_{\\partial D} Pdx+Qdy = \\int_{D} \\left(\\frac{\\partial Q}{\\partial x}-\\frac{\\partial P}{\\partial y}\\right)dxdy \\tag{33}$$ . This is fine, but they are not equivalent; they are merely identical in form. Green's formula describes the relationship between the line integral along a closed curve and the double integral over the area, while the exterior derivative formula is a purely algebraic operatio"}, {"file": "translation_4065.html", "title": "[An Introduction to Exterior Differentiation] 6. Differential Geometry", "content": "← Back to Index [An Introduction to Exterior Differentiation] 6. Differential Geometry By 苏剑林 | November 07, 2016 We finally begin to discuss the highlight, which is the content that prompted me to learn exterior differentiation in the first place. Using exterior differentiation, one can conveniently derive many aspects of differential geometry, and it occasionally simplifies calculations. The primary reason for this is that exterior differentiation is itself a generalization of the differential in form; thus, it is not surprising that the concepts of differential geometry can be described using exterior differentiation. Most importantly, exterior differentiation treats $dx^{\\mu}$ as a set of basis elements. This effectively introduces two sets of bases into geometry: one is the inherent vector basis (the basis for contravariant vectors in tensor language), which allows for symmetric inner products, and the other is the $dx^{\\mu}$ basis, which allows for antisymmetric exterior products. Consequently, when exterior differentiation is introduced into geometry, differential geometry gains a full suite of \"ideal equipment\" including differentiation, integration, symmetric products, and"}, {"file": "translation_4076.html", "title": "[An Introduction to Exterior Calculus] 7. Powerful Calculations", "content": "← Back to Index [An Introduction to Exterior Calculus] 7. Powerful Calculations By 苏剑林 | November 11, 2016 Here we will demonstrate how powerful the method from the previous section is for calculating the Riemann curvature tensor! We list all the formulas we have obtained once again. First, the conceptual ones: \\begin{aligned}&\\omega^{\\mu}=h_{\\alpha}^{\\mu}dx^{\\alpha}\\\\\n&d\\boldsymbol{r}=\\hat{\\boldsymbol{e}}_{\\mu} \\omega^{\\mu}\\\\\n&ds^2 = \\eta_{\\mu\\nu} \\omega^{\\mu}\\omega^{\\nu}\\\\\n&\\langle \\hat{\\boldsymbol{e}}_{\\mu}, \\hat{\\boldsymbol{e}}_{\\nu}\\rangle = \\eta_{\\mu\\nu}\\end{aligned} \\tag{65} Then: \\begin{aligned}&d\\eta_{\\mu\\nu}=\\omega_{\\nu\\mu}+\\omega_{\\mu\\nu}=\\eta_{\\nu\\alpha}\\omega_{\\mu}^{\\alpha}+\\eta_{\\mu \\alpha}\\omega_{\\nu}^{\\alpha}\\\\\n&d\\omega^{\\mu}+\\omega_{\\nu}^{\\mu}\\land \\omega^{\\nu}=0\\end{aligned} \\tag{66} These two help us determine $\\omega_{\\nu}^{\\mu}$; next is: \\begin{equation}\n\\mathscr{R}_{\\nu}^{\\mu} = d\\omega_{\\nu}^{\\mu}+\\omega_{\\alpha}^{\\mu} \\land \\omega_{\\nu}^{\\alpha} \\tag{67}\n\\end{equation} Finally, if you want the components $\\hat{R}^{\\mu}_{\\nu\\beta\\gamma}$ in the orthogonal frame, you write: \\begin{equation}\n\\mathscr{R}_{\\nu}^{\\mu}=\\sum_{\\beta < \\gamma} \\hat{R}^{\\mu}_{\\nu\\beta"}, {"file": "translation_4083.html", "title": "Why is Lebesgue Integration Stronger Than Riemann Integration?", "content": "← Back to Index Why is Lebesgue Integration Stronger Than Riemann Integration? By 苏剑林 | November 16, 2016 Friends who have studied real analysis always know there is something called Lebesgue integration, which is claimed to be an improved version of Riemann integration. Although there is a saying that \"you study real analysis ten times, and functional analysis still makes your heart turn cold,\" while learning real analysis, we are usually in a fog. However, by the end, under the \"irrigation\" of teachers, we become familiar with certain conclusions, such as \"a Riemann-integrable function (on a finite interval) is also Lebesgue-integrable.\" Simply put, \"Lebesgue integration is stronger than Riemann integration.\" So, here comes the question: exactly where is it stronger? Why is it stronger? Riemann Lebesgue I didn't fully understand this question while studying real analysis and left it aside for a long time. It wasn't until recently, after carefully reading Revisiting Calculus , that I began to get a sense of it. By the way, Professor Qi Minyou's Revisiting Calculus is truly excellent and well worth reading. They share the same roots, so why be so eager to oppose one another? Reader"}, {"file": "translation_4096.html", "title": "Scientific Spaces \"WeChat Group | Chatbot\" Online Testing", "content": "← Back to Index Scientific Spaces \"WeChat Group | Chatbot\" Online Testing By 苏剑林 | November 24, 2016 I spent some time completing a WeChat chatbot and established a WeChat group. The currently implemented functions are as follows: 1. Search for the WeChat ID spaces_ac_cn ; after adding it as a friend, it will automatically send you a group invitation, and you can join the group chat once you accept; 2. Automatically send a welcome message after joining the group; 3. Record group chat logs and share them with everyone periodically, so you no longer have to worry about losing valuable group information; 4. If a group becomes full, a new group will be opened, and information from one group will be automatically synchronized to the other, so that no group is left isolated; 5. If you send a message to the WeChat ID spaces_ac_cn , it will automatically search for answers on Zhihu and return them; this is also a simple Zhihu search bot. There are also some functions used by administrators, which I won't list in detail. Everyone is welcome to join! Please provide feedback if you encounter any issues. There may be bugs in the code, so I hope everyone can help test it extensively. If you fin"}, {"file": "translation_4100.html", "title": "Three Visits to Shredded Paper Restoration: CNN-based Shredded Paper Restoration", "content": "← Back to Index Three Visits to Shredded Paper Restoration: CNN-based Shredded Paper Restoration By 苏剑林 | November 25, 2016 Problem Review I must say, Question B of the 2013 National Mathematical Modeling Contest is truly a once-in-a-century masterpiece among mathematical modeling competitions: the problem is concise and clear, its meaning is rich, the approaches are diverse, and it has strong extensibility—to the point that I have always been obsessed with it. Because of this problem, I have already written two articles on Scientific Space: \"A Person's Mathematical Modeling: Shredded Paper Restoration\" and \"Late by One Year Modeling: Re-exploring Shredded Paper Restoration\" . Previously, when I did this problem, I only had a little knowledge of mathematical modeling. Since learning about data mining, especially deep learning, I have always wanted to redo this problem, but I kept procrastinating. I finally implemented it these past few days. If readers are not clear about the problem, they can refer to the two previous articles. Shredded paper restoration has five attachments, representing five types of \"shredded paper fragments,\" i.e., fragments of five different granularities. At"}, {"file": "translation_4114.html", "title": "Lightweight Deep Learning Word Segmentation System: NNCWS v0.1", "content": "← Back to Index Lightweight Deep Learning Word Segmentation System: NNCWS v0.1 By 苏剑林 | November 29, 2016 Alright, I've played the clickbait artist once... In reality, the word segmentation system in this article is a three-layer neural network model, so it's actually \"shallow learning\"—writing \"deep learning\" just makes it more attractive. NNCWS stands for Neural Network based Chinese Word Segmentation System. It is written in Python and is currently completely open-source for readers to try out. Small Talk What are the special features of this program? Almost none! This article simply uses a neural network combined with character embeddings to implement an n-grams style (7-grams are used in the program) segmentation system. It doesn't use high-end models like \"Chinese Word Segmentation Series 4: seq2seq Character Labeling Based on Bi-LSTM\" , nor does it have unsupervised training like \"Chinese Word Segmentation Series 5: Unsupervised Segmentation Based on Language Models.\" This is a purely supervised simple model, trained on the 2014 People's Daily annotated corpus. So, what is the significance of this program? Two words: Lightweight ! Current deep learning programs are quite mas"}, {"file": "translation_4118.html", "title": "Aspect-based Sentiment Analysis Based on Bidirectional GRU and Language Model", "content": "← Back to Index Aspect-based Sentiment Analysis Based on Bidirectional GRU and Language Model By 苏剑林 | December 01, 2016 Some time ago, I participated in a ridiculous online competition—Aspect-based Sentiment Analysis in specific domains. The homepage is here . The task of the competition was to identify entities in a segment of text and then judge the sentiment. For example, in the sentence \"I like Honda, but I don't like Toyota,\" one needs to mark \"Honda\" and \"Toyota.\" From the perspective of Honda, the sentiment is positive, while from the perspective of Toyota, the sentiment is negative. In other words, it is equivalent to combining entity recognition and sentiment analysis. Rant # It sounds high-end, so why is it ridiculous? The competition task itself is quite good and worth researching. However, the organizers were very frustrating, mainly manifested in: 1. The competition was divided into preliminary, semi-final, and final stages. The preliminary lasted over a month, after which some were selected for the semi-final. The semi-final simply involved changing the data slightly, with no changes to the problem or the data domain. The semi-final also lasted a month. What on earth"}, {"file": "translation_4122.html", "title": "What Exactly Are Word Vectors and Embedding?", "content": "← Back to Index What Exactly Are Word Vectors and Embedding? By 苏剑林 | December 03, 2016 Word vectors, known in English as Word Embeddings, should literally be translated as \"word embedding.\" When mentioning word vectors, many readers will immediately think of Google's Word2Vec—the brand effect is indeed powerful. Additionally, frameworks like Keras have an Embedding layer, which is also said to map word IDs to vectors. Due to preconceived notions, people tend to equate word vectors with Word2Vec, and conversely ask questions like \"What kind of word vector is Embedding?\" Especially for beginners, this can be very confusing. In fact, even for veterans, it is not necessarily easy to explain clearly. All of this must start with one-hot... The Pot Calling the Kettle Black # one hot One-hot is the most primitive way to represent characters or words. For simplicity, this article uses characters as an example; words are similar. Suppose there are six characters in the vocabulary: \"科 (Sci), 学 (ence), 空 (S), 间 (pace), 不 (Not), 错 (Bad)\". One-hot assigns a 0-1 encoding to each of these six characters: $$\\begin{array}{c|c}\\hline\\text{科} & [1, 0, 0, 0, 0, 0]\\\\ \n\\text{学} & [0, 1, 0, 0, 0, 0]\\\\ \n\\"}, {"file": "translation_4138.html", "title": "End-to-End Tencent CAPTCHA Recognition (46% Accuracy)", "content": "← Back to Index End-to-End Tencent CAPTCHA Recognition (46% Accuracy) By 苏剑林 | December 14, 2016 For the latest results, please refer to: http://kexue.fm/archives/4503/ Some time ago, I was fortunate enough to obtain a batch of labeled Tencent CAPTCHA samples from a netizen (CAPTCHA sample URL: http://captcha.qq.com/getimage ). So, I took some time to test a CAPTCHA recognition model. Tencent CAPTCHA Samples This batch of CAPTCHAs is relatively simple, consisting of 4-digit English letters. They include both upper and lower case, but input is case-insensitive. The patterns have some level of confusion, and traditional segmentation-based solutions are likely difficult to implement. The end-to-end solution directly inputs the CAPTCHA, passes it through several convolutional layers, and then connects to several classifiers (26-way classification) to directly output four letter labels. In fact, there isn't much more to say; if you have samples, you can do it. Moreover, this framework is universal; it can be used for case-sensitive scenarios (52-way classification) or alphanumeric mixtures (just add 10 more categories). However, there is one thing I find quite difficult to handle: the l"}, {"file": "translation_4159.html", "title": "[Memo] Several Ideas for Interrupting Multiple Loops in Python", "content": "← Back to Index [Memo] Several Ideas for Interrupting Multiple Loops in Python By 苏剑林 | December 19, 2016 Breaking Out of a Single Loop Regardless of the programming language, there is often a need to break out of a loop—for example, when enumerating values and finding a number that satisfies a condition, you want to terminate. Breaking out of a single loop is very simple, for instance: for i in range(10):\n    if i > 5:\n        print i\n        break However, we sometimes need to break out of multiple loops, while break can only exit one level of a loop. For example: for i in range(10):\n    for j in range(10):\n        if i+j > 5:\n            print i,j\n            break This code does not stop as soon as it finds a single set where $i+j > 5$; instead, it will find 10 such sets, because the break only exits the for j in range(10) loop level. So, how can we break out of multiple loops? This post serves as a memo for those methods. Breaking Out of Multiple Loops In fact, Python's standard syntax does not support breaking out of multiple loops directly. Therefore, one must rely on certain techniques. The general ideas include: wrapping the logic into a function, utilizing Cartesian produ"}, {"file": "translation_4164.html", "title": "Happy 2017! Responsive Geekg for Typecho", "content": "← Back to Index Happy 2017! Responsive Geekg for Typecho By 苏剑林 | December 31, 2016 2016 is about to draw to a close. Here, I wish all readers a Happy 2017 and a smooth and prosperous New Year! As the saying goes, \"New Year, new look,\" and Scientific Spaces has also put on a new coat. Wait, you don't feel much of a change? Don't worry, please read on. Let's talk about some history first. Scientific Spaces started in March 2009. A major reason I fell in love with blogging back then was the theme I am currently using, originally created by geekg.com . The color scheme of this theme is very classic and immediately attracted me. Of course, it wasn't just me; at that time, various blogging platforms and even CMS portal platforms, including WordPress, PJBlog, etc., all imitated this theme, which shows its classic appeal. Later, someone ported this theme to Typecho, which led me to switch to Typecho—firstly because of Typecho's lightness and efficiency, and secondly because of the existence of this classic theme. Since 2009, I have consistently used this theme. In the past two years, I briefly switched to other themes, but eventually, I returned to this one. Over these seven years, front-"}, {"file": "translation_4171.html", "title": "2016 Annual Celestial Events", "content": "← Back to Index 2016 Annual Celestial Events By 苏剑林 | December 31, 2015 Astronomy Calendar of Celestial Events 2016 Annual Celestial Events Translated from NASA: http://eclipse.gsfc.nasa.gov/SKYCAL/SKYCAL.html (Beijing Time) 2011 Version 2012 Version 2013 Version 2014 Version 2015 Version Date Day Time Event January 01 Fri Venus: 37.9° W\n02 Sat 13:30:00 Last Quarter Moon\n02 Sat 19:53:00 Moon at Apogee: 404300 km\n03 Sun 08:59:00 Earth at Perihelion: 0.9833 AU\n04 Mon 02:45:00 Mars-Moon Conjunction: 1.6° S\n04 Mon 16:01:00 Quadrantid Meteor Shower: ZHR = 120\n07 Thu 07:57:00 Venus-Moon Conjunction: 3.3° S\n07 Thu 12:57:00 Saturn-Moon Conjunction: 3.6° S\n07 Thu 19:32:00 Venus-Antares Conjunction: 6.4° N\n09 Sat 01:56:00 Moon at Southern Declination Limit: 18.4° S\n09 Sat 15:42:00 Venus-Saturn Conjunction: 0.1° N\n10 Sun 09:30:00 New Moon\n14 Thu 22:02:00 Mercury at Inferior Conjunction\n14 Thu 23:48:00 Moon at Descending Node\n15 Fri 10:10:00 Moon at Perigee: 369600 km\n17 Sun 07:26:00 First Quarter Moon\n20 Wed 10:16:00 Aldebaran-Moon Conjunction: 0.5° S\n22 Fri 00:41:00 Moon at Northern Declination Limit: 18.4° N\n24 Sun 09:46:00 Full Moon\n26 Tue 13:10:00 Regulus-Moon Conjunction: 2.8° N\n28 Thu 0"}, {"file": "translation_4173.html", "title": "2017 Calendar of Celestial Events", "content": "← Back to Index 2017 Calendar of Celestial Events By 苏剑林 | December 31, 2016 Astronomy Calendar of Celestial Events 2017 Calendar of Celestial Events Translated from NASA: http://eclipse.gsfc.nasa.gov/SKYCAL/SKYCAL.html (Beijing Time) 2011 Version 2012 Version 2013 Version 2014 Version 2015 Version 2016 Version Day Week Time Event January 01 Sun Venus: 46.8° E 02 Mon 17:20:00 Venus conjunction Moon: 2° S 03 Tue 02:14:00 Moon at Descending Node 03 Tue 14:47:00 Mars conjunction Moon: 0.3° S 03 Tue 22:10:00 Quadrantids Meteor Shower: ZHR = 120 04 Wed 18:59:00 Earth at Perihelion: 0.9833 AU 06 Fri 03:47:00 First Quarter Moon 09 Mon 17:03:00 Mercury conjunction Saturn: 6.7° N 09 Mon 22:07:00 Aldebaran conjunction Moon: 0.4° S 10 Tue 14:07:00 Moon at Perigee: 363,200 km 11 Wed 17:32:00 Moon at North Declination: 18.9° N 12 Thu 19:34:00 Full Moon 12 Thu 20:59:00 Venus Greatest Elongation: 47.1° E 15 Sun 12:07:00 Regulus conjunction Moon: 0.9° N 15 Sun 18:45:00 Moon at Ascending Node 19 Thu 13:26:00 Jupiter conjunction Moon: 3° S 19 Thu 17:59:00 Mercury Greatest Elongation: 24.1° W 20 Fri 06:14:00 Last Quarter Moon 22 Sun 08:14:00 Moon at Apogee: 404,900 km 24 Tue 18:37:00 Saturn conjuncti"}, {"file": "translation_4176.html", "title": "Acquiring and Processing Chinese Wikipedia Corpus", "content": "← Back to Index Acquiring and Processing Chinese Wikipedia Corpus By 苏剑林 | January 06, 2017 Among Chinese corpora, the one that is highest in quality and easiest to obtain is likely the Wikipedia Chinese corpus. Furthermore, Wikipedia is quite generous, packaging all entries every month (Download address here: https://dumps.wikimedia.org/zhwiki/ ) for the world to use. This is truly \"taken from the people, given back to the people.\" Regrettably, due to the unreasonable blockade of the Great Firewall, the number of Chinese Wikipedia entries is currently only just over 910,000, while Baidu Baike and Hudong Baike both have tens of millions (the English Wikipedia also has over ten million). Despite this, it has not stopped Chinese Wikipedia from being arguably the highest quality Chinese corpus available. (Baidu Baike and Hudong Baike can only be obtained by crawling, and many records are of quite poor quality, often being fragments of mutual copying or even plagiarism.) Threshold While downloading is easy, there is a certain threshold to using the Wikipedia corpus. The raw material downloaded directly from Wikipedia is a compressed text package containing many HTML and Markdown marker"}, {"file": "translation_4182.html", "title": "Smoothing Formula Based on the Forgetting Hypothesis", "content": "← Back to Index Smoothing Formula Based on the Forgetting Hypothesis By 苏剑林 | January 07, 2017 Statistics is the process of estimating a true distribution from a large number of samples. A term that usually accompanies statistics is \"smoothing,\" which refers to the process of discounting or adjusting statistical results. The idea behind smoothing is that if the sample space is very large, the statistical results will be sparse. Due to various accidental factors, small statistical results become unreliable—for example, a frequency of 1 might be purely coincidental, and its true frequency is not necessarily close to $1/N$, while a frequency of 0 does not necessarily mean it will never occur. Thus, we need to smooth the statistical results to make the conclusions more reliable. There are many methods for smoothing. Here, we introduce a smoothing formula based on the forgetting hypothesis. Suppose our task is to count the frequency of each character from a corpus. We simulate the process of human forgetting: assume that every time a character appears, our memory of it increases by 1. However, if the character does not appear within a certain period (regardless of how long that period i"}, {"file": "translation_4187.html", "title": "Dirac Delta Function: Series Approximation", "content": "← Back to Index Dirac Delta Function: Series Approximation By 苏剑林 | January 11, 2017 Weierstrass Theorem Understanding the Dirac delta function as a limit of a sequence of functions can lead to very rich content, and this content is not far from rigorous proof. For example, define\n$$\\delta_n(x)=\\left\\{\\begin{aligned}&\\frac{(1-x^2)^n}{I_n},x\\in[-1,1]\\\\\n&0,\\text{otherwise}\\end{aligned}\\right.$$\nwhere $I_n = \\int_{-1}^1 (1-x^2)^n dx$. It is then not difficult to prove that\n$$\\delta(x)=\\lim_{n\\to\\infty}\\delta_n(x)$$\nThus, for a continuous function $f(x)$ on $[a,b]$, we obtain\n$$f(x)=\\int_{-1}^1 f(y)\\delta(x-y)dy = \\lim_{n\\to\\infty}\\int_{-1}^1 f(y)\\delta_n(x-y) dy$$\nHere $-1 < a < b < 1$, and we have \"non-rigorously\" swapped the integral sign and the limit sign, but this is not particularly critical. What is important is the result: it can be seen that\n$$P_n(x)=\\int_{-1}^1 f(y)\\delta_n(x-y) dy$$\nis a polynomial of degree $2n$ in $x$. Therefore, the above expression indicates that $f(x)$ is the limit of a polynomial of degree $2n$! This leads to the famous \"Weierstrass Theorem\": Any continuous function on a closed interval can be uniformly approximated by polynomials. Rigorous readers mi"}, {"file": "translation_4195.html", "title": "[Chinese Word Segmentation Series] 6. Chinese Word Segmentation Based on Full Convolutional Networks", "content": "← Back to Index [Chinese Word Segmentation Series] 6. Chinese Word Segmentation Based on Full Convolutional Networks By 苏剑林 | January 13, 2017 I have previously written about schemes using LSTM for word segmentation. Today, I'm presenting another one using CNNs—specifically, an FCN (Fully Convolutional Network). The primary goal of this model isn't actually to research Chinese word segmentation itself, but rather to practice using TensorFlow. I've been using Keras for two years now, and I'm quite familiar with it, but I've gradually discovered some of its limitations—such as the inconvenience of handling variable-length inputs and the difficulty of adding custom constraints. Thus, I decided to try native TensorFlow. After trying it, I found it's not actually that complex; after all, it's all Python, how complex can it be? This article serves as an exercise in using TensorFlow to handle variable-length sequences, using Chinese word segmentation as an example. Finally, I have incorporated Hard Decoding , which combines deep learning with dictionary-based segmentation . CNN Regarding FCNs: interpreted in the context of language tasks, a (one-dimensional) convolution is essentially an "}, {"file": "translation_4208.html", "title": "SVD Decomposition (Part 1): Autoencoders and Artificial Intelligence", "content": "← Back to Index SVD Decomposition (Part 1): Autoencoders and Artificial Intelligence By 苏剑林 | January 15, 2017 At first glance, SVD decomposition appears to be a traditional data mining technique, while autoencoders are a relatively \"advanced\" concept in deep learning; one might assume they have little in common. However, this article aims to show that if we ignore activation functions, the two are equivalent. Further reflection reveals that whether it is SVD or an autoencoder, we perform dimensionality reduction not purely to reduce storage or computation, but as a preliminary manifestation of \"intelligence.\" Equivalence Suppose we have a massive matrix $M_{m\\times n}$ with $m$ rows and $n$ columns. Performing calculations or even storing this matrix can be problematic. Therefore, we consider a decomposition, hoping to find matrices $A_{m\\times k}$ and $B_{k\\times n}$ such that:\n    $$M_{m\\times n}=A_{m\\times k}\\times B_{k\\times n}$$\n    where the multiplication is matrix multiplication. As shown below: SVD In this way, what originally had $mn$ elements is now transformed into $(m+n)k$ elements on the right side. If $k$ is small enough, then $(m+n)k < mn$, which reduces both stora"}, {"file": "translation_4216.html", "title": "SVD Decomposition (II): Why Does SVD Imply Clustering?", "content": "← Back to Index SVD Decomposition (II): Why Does SVD Imply Clustering? By 苏剑林 | Jan 26, 2017 Wishing all readers a Happy New Year in advance, and good luck for 2017! This article mainly aims to answer two \"why\" questions: 1. Why did I become interested in SVD? 2. Why do I say that SVD is a clustering process? The content of these answers is purely the result of my personal reflection and lacks formal references for now. Why Research SVD? Since I first encountered deep learning in 2015, I have been researching it for nearly two years. Nowadays, concepts like deep learning and data science are blooming everywhere. Why, in an era where deep learning is so popular, have I gone back to study the \"ancient\" SVD decomposition? I believe that as a matrix decomposition algorithm, the value of SVD is not only reflected in its wide range of applications but also in its deeper internal meaning—specifically, its interpretability. Even today, many people still feel that deep learning (neural networks) is just an effective \"black box\" model. However, simply using the words \"black box\" to explain the effectiveness of deep learning is clearly unsatisfying. As mentioned previously, SVD decomposition "}, {"file": "translation_4222.html", "title": "Trial shooting of starry sky and star trails on New Year's Eve~", "content": "← Back to Index Trial shooting of starry sky and star trails on New Year's Eve~ By 苏剑林 | Jan 27, 2017 Taking advantage of the fact that the light pollution in my hometown is not particularly serious yet, I took the opportunity to take some photos as souvenirs. Shot with a Huawei P9; the device is good, but my shooting technique is quite poor. Please feel free to give me some pointers. Star trail 1 Star trail 2 Starry sky 1 Starry sky 2 If you found this article helpful, you are welcome to share or donate to this article. Rewards are not intended for profit, but to know how much genuine attention Scientific Spaces has received from readers. Of course, if you choose to ignore it, it will not affect your reading. Once again, welcome and thank you! ,\n    author={Su Jianlin},\n    year={2017},\n    month={Jan},\n    url={\\url{https://kexue.fm/archives/4222}},\n} Citation This is a machine translation of the original Chinese article: https://kexue.fm/archives/4222 Original author: 苏剑林 (Su Jianlin) Original publication: 科学空间 (Scientific Spaces) Translated using Gemini 3 Flash. Please refer to the original for authoritative content."}, {"file": "translation_4231.html", "title": "Python Multiprocessing Programming Tips", "content": "← Back to Index Python Multiprocessing Programming Tips By 苏剑林 | February 19, 2017 The Process In Python, if you want to perform multiprocessing calculations, it is generally implemented through multiprocessing . The most commonly used feature is the process pool in multiprocessing , for example: Writing it this way is concise and clear, which is indeed convenient. Interestingly, you only need to replace multiprocessing with multiprocessing.dummy to change the program from multi-processing to multi-threading. Objects Python is an object-oriented programming language, and we often encapsulate certain programs into a class. However, within a class, the above method no longer works. For example: The code looks quite natural, but it throws an error during execution: cPickle.PicklingError: Can't pickle : attribute lookup __builtin__.function failed However, if you replace multiprocessing with multiprocessing.dummy , no error is reported. Simply put, this is still due to the issue that variables cannot be shared between multiple processes, whereas multiple threads reside within the same process and naturally do not have this problem. Drawing Inspiration To study multiprocessing programmi"}, {"file": "translation_4233.html", "title": "SVD Decomposition (III): Even Word2Vec is Just an SVD?", "content": "← Back to Index SVD Decomposition (III): Even Word2Vec is Just an SVD? By 苏剑林 | February 23, 2017 This article brings some \"heavyweight\" news, as the title suggests: even the famous deep learning word vector tool, Word2Vec, is essentially just an SVD! Of course, super loyal fans of Word2Vec need not get too excited. We are only saying that the model structures are equivalent, not that they are identical in every way. Word2Vec still has its unique aspects. However, once explained in this way, many problems can likely be understood through similar logic. Word Vector = One Hot Let's first review an article from last year, \"What is actually going on with Word Vectors and Embedding?\" . The main point of that article was: the so-called Embedding layer is simply a fully connected layer with a one-hot input (again, emphasizing that this is \"identical,\" not just \"equivalent to\"), and word vectors are the parameters of this fully connected layer. As for Word2Vec, it trains the Embedding layer through a greatly simplified language model to obtain word vectors (it uses many optimization tricks, but its model structure is that simple). Word vectors can reduce the risk of overfitting because too"}, {"file": "translation_4245.html", "title": "[Chinese Word Segmentation Series] 7. Deep Learning Segmentation? All You Need is a Dictionary!", "content": "← Back to Index [Chinese Word Segmentation Series] 7. Deep Learning Segmentation? All You Need is a Dictionary! By 苏剑林 | March 06, 2017 As this series slowly reaches its 7th post, we have basically clarified various models for word segmentation, except for some minor adjustments (like swapping the final classifier with a CRF); the rest is just about how to play with it. Generally speaking, if you want speed, you use dictionary-based segmentation. If you want better resolution of combination ambiguities and out-of-vocabulary (OOV) word recognition, you use complex models like the previously introduced LSTM or FCN. However, the problem is that training a segmenter with deep learning requires annotated corpora, which is time-consuming and labor-intensive. The few publicly available annotated corpora cannot keep up with the times—for example, almost no public segmentation system can correctly segment \"扫描二维码，关注微信号\" (Scan the QR code, follow the WeChat account). This article describes an experiment: using only a dictionary, I trained a deep learning segmenter, and the results were surprisingly good! This approach can be called semi-supervised or even unsupervised. Random Combinations are"}, {"file": "translation_4256.html", "title": "[Chinese Word Segmentation Series] 8. A Better New Word Discovery Algorithm", "content": "← Back to Index [Chinese Word Segmentation Series] 8. A Better New Word Discovery Algorithm By 苏剑林 | March 11, 2017 Readers who have followed this series in order will find that we have provided two unsupervised word segmentation schemes from scratch. The first is \"[Chinese Word Segmentation Series] 2. New Word Discovery Based on Segmentation,\" which uses the coagulation degree (mutual information) of adjacent characters to build a vocabulary (once you have a vocabulary, you can segment using dictionary-based methods). The second is \"[Chinese Word Segmentation Series] 5. Unsupervised Segmentation Based on Language Models,\" which essentially provides a complete unsupervised segmentation method independent of other literature. However, looking at them overall, the former feels fast and convenient but somewhat crude, while the latter is powerful but perhaps overly complex (Viterbi being one of the bottlenecks). Is it possible to find a compromise between the two? This led to the results of this article, achieving a balance between speed and effectiveness. As for why I call it \"better\"? I have been researching vocabulary construction for a while, and the vocabularies I built in the pas"}, {"file": "translation_4271.html", "title": "Teddy Cup Pre-competition Training: A \"Slow Talk\" on Data Mining and Modeling", "content": "← Back to Index Teddy Cup Pre-competition Training: A \"Slow Talk\" on Data Mining and Modeling By 苏剑林 | March 14, 2017 At the invitation of Guangzhou TipDM (Teddy) Technology Co., Ltd., I recorded pre-competition training videos for the Teddy Cup Data Mining Competition. The content basically covers various common mathematical models and their introductory usages. Using a unique approach, I have linked together Naive Bayes, HMM (Hidden Markov Model), Logistic Regression, Ensemble Models, Neural Networks, Deep Learning, and more. The video explanation is at an introductory difficulty level; of course, truly integrating and mastering all the content might require a \"hardcore\" level of expertise. Regardless, I'm sharing it briefly here. I welcome everyone to leave comments for discussion, suggestions, or even criticism. PPT Download: Teddy Cup Pre-competition Training ppt.zip Video Address: http://moodle.tipdm.com/course/view.php?id=18 When reposting, please include the address of this article: https://kexue.fm/archives/4271 If you have any doubts or suggestions, please feel free to discuss them in the comments section below. If you found this article helpful, you are welcome to share "}, {"file": "translation_4277.html", "title": "Gradient Descent and the EM Algorithm: Shared Roots and a Common Lineage", "content": "← Back to Index Gradient Descent and the EM Algorithm: Shared Roots and a Common Lineage By 苏剑林 | March 23, 2017 PS: This article summarizes the relationship between gradient descent and the EM algorithm. Using a unified perspective, it derives ordinary gradient descent, the EM algorithm in pLSA, and the EM algorithm in K-Means. The goal is to demonstrate that they are essentially different aspects of the same concept—much like \"viewing a mountain range from different angles, where it looks different from near, far, high, or low.\" In machine learning, we usually represent the problem we want to solve as a loss function with unknown parameters, such as Mean Squared Error (MSE). We then find ways to minimize this function to obtain the optimal parameter values and complete the model. Since multiplying a function by -1 turns a maximum into a minimum, we generally speak in terms of minimization. In the field of machine learning, two major directions are commonly taught for finding the minimum of a function: 1. Gradient Descent; 2. The EM algorithm (Expectation-Maximization), which is typically used for solving complex maximum likelihood problems. Standard tutorials often describe these"}, {"file": "translation_4293.html", "title": "Text Sentiment Classification (Part 4): A Better Loss Function", "content": "← Back to Index Text Sentiment Classification (Part 4): A Better Loss Function By 苏剑林 | March 30, 2017 Text sentiment classification is essentially a binary classification problem. In fact, classification models often suffer from a common issue: the optimization objective is inconsistent with the evaluation metrics. Generally, for classification (including multi-class), we use cross-entropy as the loss function, which originates from Maximum Likelihood Estimation (refer to \"Gradient Descent and EM Algorithm: From the Same Origin\" ). However, our final evaluation goal is not to see how small the cross-entropy is, but to look at the model's accuracy. Usually, a small cross-entropy leads to high accuracy, but this relationship is not absolute. Aiming for Average Doesn't Necessarily Mean Being Top-Tier A more common example is this: A math teacher is working hard to improve the students' average score, but the final assessment metric is the passing rate (passing at 60 points). If the average score is 100 points (implying everyone scored 100), then naturally the passing rate is 100%, which is ideal. But reality isn't always so perfect. As long as the average score hasn't reached 100, a "}, {"file": "translation_4299.html", "title": "[Incredible Word2Vec] 1. Mathematical Principles", "content": "← Back to Index [Incredible Word2Vec] 1. Mathematical Principles By 苏剑林 | April 02, 2017 For readers familiar with Deep Learning and Natural Language Processing (NLP), Word2Vec is a household name. Although not everyone has used it personally, most people have heard of it—Google's highly efficient tool for obtaining word vectors. Is Word2Vec Incredible? Most people treat Word2Vec as a synonymous term for word vectors. In other words, they use it purely as a tool for obtaining word representations, and few readers concern themselves with the model itself. This might be because the model is so simplified that people assume such a simple model must be inaccurate and therefore unusable for modeling language, even if its byproduct—the word vectors—is quite good. Indeed, if viewed as a language model, Word2Vec is far too crude. However, why should we view it only as a language model? Setting aside the constraints of language modeling and looking at the model itself, we find that the two Word2Vec models—CBOW and Skip-Gram—are actually extremely useful. They describe the relationship between surrounding words and the current word from different perspectives. Many basic NLP tasks, such as k"}, {"file": "translation_4304.html", "title": "[Incredible Word2Vec] 2. Trained Model", "content": "← Back to Index [Incredible Word2Vec] 2. Trained Model By 苏剑林 | April 3, 2017 Since the following posts will explain how to use Word2Vec, I have pre-trained a Word2Vec model. To save readers' time and ensure that everyone can reproduce the results in later sections, I have decided to share this pre-trained model, which was trained using Gensim. While simple word vectors aren't very large, as mentioned in the first post, we will need the complete Word2Vec model. Therefore, I am sharing the full model, which consists of four files, making the total size somewhat large. A reminder to readers: if you want to obtain a complete Word2Vec model without modifying the source code, Python's Gensim library is essentially your only choice. As far as I know, other versions of Word2Vec only provide the final word vectors and do not include the complete model parameters. For the purpose of knowledge mining, Word2Vec trained on knowledge base corpora (such as Encyclopedia/Wikipedia data) usually yields better results. I am still in the process of crawling encyclopedia data; once finished, I will train another model and share it then. Model Overview The general profile of this model is as follows: \\"}, {"file": "translation_4316.html", "title": "[Incredible Word2Vec] 3. Extracting Keywords", "content": "← Back to Index [Incredible Word2Vec] 3. Extracting Keywords By 苏剑林 | April 07, 2017 This article primarily provides a new definition for keywords and presents an implementation scheme based on Word2Vec. This definition of keywords is natural and reasonable; Word2Vec is merely a simplified implementation scheme. One could substitute it with other models based on the same definition. When it comes to extracting keywords, one typically thinks of TF-IDF and TextRank. Have you ever considered that Word2Vec can also be used for keyword extraction? Moreover, extracting keywords with Word2Vec already inherently includes a degree of semantic understanding, rather than just simple statistics—and it is unsupervised! What is a Keyword? # Admittedly, TF-IDF and TextRank are two classic algorithms for keyword extraction, and they both possess a certain degree of rationality. However, the problem represents a leap of logic for readers who have never seen these two algorithms; it is unlikely they could construct them from scratch. That is to say, although these two algorithms appear simple, they are not easy to conceive intuitively. For instance, a student who has not studied information theory m"}, {"file": "translation_4338.html", "title": "[Corpus] Baidu's Chinese Question Answering Dataset WebQA", "content": "← Back to Index [Corpus] Baidu's Chinese Question Answering Dataset WebQA By 苏剑林 | April 12, 2017 Information Extraction As is well known, a large number of people post numerous questions on Baidu Zhidao and receive a vast amount of replies. However, many respondents on Baidu Zhidao seem to be quite lazy; they often prefer to directly copy and paste large sections of text from the internet as their answers. These contents may or may not be relevant to the question. For example: https://zhidao.baidu.com/question/557785746.html Question: How high is the altitude of Guangzhou Baiyun Mountain? Answer: Guangzhou Baiyun Mountain is the head of the new \"Eight Sights of Guangzhou,\" a national 4A-level scenic spot, and a national key scenic resort. It is located in the northeast of Guangzhou and is one of the famous mountains in South Guangdong, known since ancient times as the \"First Show of Guangzhou.\" The mountain body is quite broad, consisting of more than 30 peaks, and is a branch of the Jiulian Mountains, the highest peak in Guangdong. It covers an area of 20.98 square kilometers, and the main peak, Moxing Ridge, is 382 meters high (Note: the latest surveyed height is 372.6 meters — "}, {"file": "translation_4356.html", "title": "Scientific Spaces Adds New Domain kexue.fm", "content": "← Back to Index Scientific Spaces Adds New Domain kexue.fm By 苏剑林 | April 23, 2017 By 苏剑林 (Su Jianlin) | April 23, 2017 | 39,205 readers Last month, I accidentally discovered that the domain kexue.fm was not yet registered. I felt it was quite good, so I quickly registered it. In fact, I have always liked domains with the .fm suffix because \"FM\" is also an abbreviation for radio. Sites with .fm domains give off a comfortable feeling, much like listening to the radio. It just so happened that I registered kexue.fm, and I feel it fits the name of this blog, \"Scientific Spaces,\" perfectly. It also aligns with the blog's original philosophy—to make science popular—implying that science can be as pleasant as listening to a radio station. On the other hand, it is also much easier to remember. The domain was registered about a month ago, but the domain filing (ICP) process took nearly a month, which is why it is only now being added to Scientific Spaces. The Scientific Spaces server has also been migrated to Alibaba Cloud. The original domain spaces.ac.cn will also be kept indefinitely, and both domains will be accessible. Furthermore, readers who applied for email addresses with the @spa"}, {"file": "translation_4359.html", "title": "[Corpus] 25 Million Chinese Triples!", "content": "← Back to Index [Corpus] 25 Million Chinese Triples! By 苏剑林 | April 24, 2017 Chit-chat Over the past two years, fields such as knowledge graphs, question-answering systems, and chatbots have become increasingly popular. A knowledge graph is a very generalized concept; in my view, anything involving the construction, retrieval, and utilization of knowledge bases related to machine learning can be considered part of knowledge graphs. Of course, this isn't a formal definition, just a personal intuition. Readers working on knowledge graphs know that triples are a method for structuring knowledge and are an important component of knowledge-based question-answering systems. For the English domain, there are already several large open-source triple corpora, but clearly, there has not been such a corpus shared for Chinese (even if someone has crawled one, they tend to keep it for themselves). Some time ago, I wrote a crawler for Baidu Baike and ran it for a while, capturing several million Baidu Baike entries. Many of these entries contain structured information that, when extracted directly, forms effective \"triples\" that can be used for knowledge graphs. The triple corpus shared in this "}, {"file": "translation_4368.html", "title": "【Unbelievable Word2Vec】 4. A Different Kind of \"Similarity\"", "content": "← Back to Index 【Unbelievable Word2Vec】 4. A Different Kind of \"Similarity\" By 苏剑林 | May 1, 2017 Definition of Similarity When we obtain word vectors using Word2Vec, we generally use cosine similarity to compare the degree of similarity between two words, defined as:\n        $$\\cos (\\boldsymbol{x}, \\boldsymbol{y}) = \\frac{\\boldsymbol{x}\\cdot\\boldsymbol{y}}{\\|\\boldsymbol{x}\\|\\times\\|\\boldsymbol{y}\\|}$$\n        With this concept of similarity, we can compare the similarity between any two words or find the words most similar to a given word. In gensim's Word2Vec, this is implemented by the most_similar function. Wait a minute! We quickly provided the mathematical formula for calculating similarity, but we haven't even \"defined\" what similarity is! How can we arrive at a formula to evaluate similarity without even defining it? It is important to note that this is not an issue that can be casually ignored. Often, we don't know exactly what we are doing before we start doing it. As mentioned in the previous article regarding keyword extraction, I believe many people have never considered what a keyword actually is—is it just a word that is \"key\"? If we think of keywords as words used to"}, {"file": "translation_4374.html", "title": "Recording a Trial of Semi-supervised Sentiment Analysis", "content": "← Back to Index Recording a Trial of Semi-supervised Sentiment Analysis By 苏剑林 | May 04, 2017 This article is a not-so-successful attempt at semi-supervised learning: on the IMDB dataset, a text sentiment classification model was trained using 1,000 randomly selected labeled samples, and achieved a test accuracy of 73.48% on the remaining 49,000 test samples. Idea The idea in this article originates from this OpenAI post: \"OpenAI Research Finds Unsupervised Sentiment Neuron: Can Directly Control Sentiment of Generated Text\" . That article introduced a method for training unsupervised (actually semi-supervised) sentiment classification models with excellent experimental results. However, the experiments in that article were massive and nearly impossible for an individual to replicate (training for one month on 4 Pascal GPUs). Nevertheless, the underlying idea is simple, so we can create a \"budget-friendly version.\" The logic is as follows: When we use deep learning for sentiment classification, a conventional approach is an Embedding layer + LSTM layer + Dense layer (Sigmoid activation). What we usually call word vectors are essentially a pre-trained Embedding layer (this layer has "}, {"file": "translation_4385.html", "title": "How to \"Scrape\" a Site? A Step-by-Step Guide to Crawling Baidu Baike", "content": "← Back to Index How to \"Scrape\" a Site? A Step-by-Step Guide to Crawling Baidu Baike By 苏剑林 | May 17, 2017 Recently, I had a requirement to crawl some children's story corpora to train word vectors. Therefore, I found several fairy tale websites and crawled the entire sites' worth of articles. Below, I will share the process implemented in Python and combine it with my previous experience crawling Baidu Baike. This tutorial is suitable for the following needs: requiring a traversal crawl of a specified website, where the specified website has no anti-crawler measures. Under this premise, the only challenges we face are the traversal algorithm and programming skills . Hypotheses To reiterate our assumptions: 1. We need to traverse the entire website to crawl the information we need; 2. The website has no anti-crawler measures; 3. Every page of the website can eventually be reached from the homepage by progressively clicking hyperlinks. What kind of websites fit these assumptions? The answer is: quite a few. For instance, the story website we are about to crawl, as well as Baidu Baike, Hudong Baike, etc. First, let's look at how to crawl this story website: http://wap.xigushi.com/ (F"}, {"file": "translation_4402.html", "title": "[The Incredible Word2Vec] 5. The TensorFlow Version of Word2Vec", "content": "← Back to Index [The Incredible Word2Vec] 5. The TensorFlow Version of Word2Vec By 苏剑林 | May 27, 2017 This article encapsulates a relatively complete Word2Vec implementation, with the model part written in TensorFlow. The purpose of this article is not merely to reinvent the Word2Vec wheel, but to use this example to become familiar with TensorFlow's syntax and to test the effectiveness of a new softmax loss I designed, laying the groundwork for future research on language models. What's Different For the basic mathematical principles of Word2Vec, please refer to the article \"[The Incredible Word2Vec] 1. Mathematical Principles.\" The primary models in this article are still CBOW and Skip-Gram, but the loss design is different. This article still utilizes a full softmax structure rather than a hierarchical softmax or negative sampling scheme, but during training, a cross-entropy loss based on random negative sampling is used. This loss differs from the existing nce_loss and sampled_softmax_loss . For now, let's name it random softmax loss . Additionally, in a softmax structure, the form is generally $\\text{softmax}(Wx+b)$. Considering that the shape of the $W$ matrix is actually ide"}, {"file": "translation_4413.html", "title": "Exploration of General Purpose Crawlers (Part I): A Crawler Suitable for General Websites", "content": "← Back to Index Exploration of General Purpose Crawlers (Part I): A Crawler Suitable for General Websites By 苏剑林 | June 06, 2017 This is a simplified version of the paper the author submitted for this year's Teddy Cup Question C. Although it ended up only receiving a consolation prize, I personally feel that some of the ideas presented here are still quite valuable for web crawling work. Therefore, I am sharing them here for everyone's reference. Introduction A crawler can be divided into two steps: 1. Downloading the web page; 2. Extracting the required information from the web page. Both steps present their own technical difficulties. For the first step, the difficulty lies in dealing with the anti-crawling measures and features of various websites, such as IP blocking or CAPTCHAs if the access frequency is too high. This requires designing specific anti-crawling strategies for different websites, and theoretically, a universal solution does not exist. For the second step, the traditional approach is to design corresponding regular expressions. However, as website designs become increasingly diverse, writing regular expressions has become correspondingly difficult. Clearly, tryin"}, {"file": "translation_4422.html", "title": "General Crawler Exploration (II): Implementation on Forum Crawling", "content": "← Back to Index General Crawler Exploration (II): Implementation on Forum Crawling By 苏剑林 | June 06, 2017 The solution mentioned previously is essentially sufficient if the crawled page has only a single effective area, such as a blog page or a news page. However, for websites with distinct hierarchical divisions, such as forums, we need further subdivision. This is because, after the aforementioned steps, while we can extract the effective text, the result is that all text is lumped together. Depth-First In order to further \"chunk\" the content, we also need to utilize the position information from the DOM tree. As shown in the DOM tree diagram in the previous article, we need to number every node and leaf, which requires a method to traverse the DOM tree. Here, we adopt a \"Depth-First\" approach. Depth-First Search (DFS) is an algorithm for traversing or searching tree or graph data structures. The algorithm starts at the root node (selecting some arbitrary node as the root node in the case of a graph) and explores as far as possible along each branch before backtracking. This process continues until all nodes reachable from the source node have been discovered. If any undiscovered"}, {"file": "translation_4430.html", "title": "Exploration of General Purpose Crawler (III): Results and Code", "content": "← Back to Index Exploration of General Purpose Crawler (III): Results and Code By 苏剑林 | June 7, 2017 Partial Results Crawling results for partial websites. Figure 1 shows the crawling effect of this blog, indicating that the solution is applicable to general websites; Figure 2 and Figure 3 show the crawling effects of forums built using two open-source forum programs, indicating that open-source programs can be crawled normally; Figure 4 is the crawling effect for the famous Tianya Forum, indicating that even for forums developed internally by a company, it still achieves good results. 6-blog 6-Discuz 6-phpbb 6-tianya Room for Improvement Overall, this is a high-efficiency, unsupervised general website crawling solution that can adapt to different websites (not limited to forums) and is suitable for large-scale enterprise deployment. Of course, this solution also has some areas to be improved: 1. To ensure efficiency and stability, universality was sacrificed. For example, when making choices about content, it directly judges based on the proportion of Chinese characters, without using more precise language model solutions; 2. The space for further improvement is small, because the"}, {"file": "translation_4439.html", "title": "The Art of Mutual Confrontation: From Zero to WGAN-GP", "content": "← Back to Index The Art of Mutual Confrontation: From Zero to WGAN-GP By 苏剑林 | June 08, 2017 Preface GAN, which stands for Generative Adversarial Nets, is known in Chinese as \"Shengcheng Duikang Shi Wangluo.\" The most common explanation for GAN is the \"forger-discriminator\" analogy, similar to a forger and a discriminator of art paintings. Initially, both the forger and the discriminator have low skill levels, but the discriminator still finds it relatively easy to identify the forger's works. However, as the forger learns new techniques, their forged paintings may cause the discriminator to make mistakes; conversely, as the discriminator improves their identification techniques, they can easily detect the forger's work again. This is a process where both parties continuously learn to reach the highest level of forgery and identification. However, readers who look a bit deeper will find that, unlike real-world forgers who might use new materials and technologies to forge items, the most miraculous and confusing thing about GAN is its ability to map random noise into the positive samples we desire. One has noise, and out comes a positive sample—isn't this a \"business with no capital"}, {"file": "translation_4486.html", "title": "Reference Solutions for \"Introduction to Commutative Algebra\"", "content": "← Back to Index Reference Solutions for \"Introduction to Commutative Algebra\" By 苏剑林 | July 03, 2017 This semester, one of our courses is \"Commutative Algebra,\" which is an upgraded version of undergraduate Abstract Algebra. The textbook we are using is Atiyah's Introduction to Commutative Algebra . According to the instructor's arrangement, we are also required to complete and present a portion of the exercises... I must say, this course has been truly exhausting~ As I reached the later exercises, I simply couldn't be bothered to write out drafts by hand anymore. I transitioned to inputting the solutions directly using LaTeX, which is convenient for both typesetting and making modifications. I am sharing them here for readers who might need them—the solutions are written in Chinese with detailed annotations, making them suitable for students who are just starting this course~ The part completed by the author: Reference Solutions for \"Introduction to Commutative Algebra\".pdf Of course, this solution set only includes the specific exercises required by our instructor. Below is a complete set of exercise solutions found online, in English: Solutions found online: Jeffrey Daniel Kasik"}, {"file": "translation_4491.html", "title": "Linux Pitfalls of Accidental Deletion and Simple Recovery Techniques", "content": "← Back to Index Linux Pitfalls of Accidental Deletion and Simple Recovery Techniques By 苏剑林 | July 16, 2017 Warning The following content contains many high-risk actions. Please do not imitate them casually. Minors should watch under the supervision of their parents~ (^_^) Suicidal Linux systems (the following also applies to Mac OS) are famous for being open-source and free. However, sometimes they are a bit too open, and I have been burned by these extremely open characteristics several times (mostly due to poor usage habits). I have summarized and shared these experiences for everyone's entertainment. The most classic example is achieving \"suicide\" through the following command: sudo rm / -rf This destroys your Linux system. Obviously, in Windows, this would be equivalent to formatting the system drive while inside the operating system, which is strictly forbidden. A similar command is mv . For example, the following command, while not quite as severe as rm , is enough to ruin your system: sudo mv /* /root There are many such high-risk commands, such as dd . In Linux or Mac OS, using dd to burn images is very common, but you must be certain of the target drive identifier. Otherw"}, {"file": "translation_4493.html", "title": "Customizing Complex Loss Functions in Keras", "content": "← Back to Index Customizing Complex Loss Functions in Keras By 苏剑林 | July 22, 2017 Keras is a building-block-style deep learning framework that makes it very convenient and intuitive to build common deep learning models. Before the emergence of TensorFlow, Keras was already arguably the most popular deep learning framework at the time, using Theano as its backend. Today, Keras supports four backends: Theano, TensorFlow, CNTK, and MXNet (the first three are officially supported, while MXNet integration is community-led), which speaks to the charm of Keras. While Keras is convenient, this convenience comes at a price. One of its most criticized drawbacks is its lower flexibility, making it difficult to build complex models. Indeed, Keras is not the most suitable for building extremely complex models; however, it is not impossible. It's just that the amount of code required for very complex models is often comparable to writing them directly in TensorFlow. Nevertheless, Keras's friendly and convenient features (like that cute training progress bar) mean there are always scenarios where we want to use it. Thus, learning how to flexibly customize Keras models becomes a valuable subject."}, {"file": "translation_4503.html", "title": "Based on Xception for Tencent Captcha Recognition (Samples + Code)", "content": "← Back to Index Based on Xception for Tencent Captcha Recognition (Samples + Code) By 苏剑林 | July 24, 2017 Last year, I was fortunate enough to receive a batch of Tencent captcha samples from a netizen. Consequently, I conducted some research on them, the process of which was documented in \"End-to-End Tencent Captcha Recognition (46% Accuracy)\" . Later, that article attracted considerable interest from readers, with some requesting samples, others asking for the model, and many engaging in discussions, which was quite unexpected. In fact, the original model was relatively crude, especially since its accuracy was not high enough for practical use, offering limited reference value. Over the past few days, I revisited this and developed a model with higher accuracy; simultaneously, I am making the samples public for everyone. The reasoning behind the model remains the same as in \"End-to-End Tencent Captcha Recognition (46% Accuracy)\" , except the CNN component has been replaced with the standard Xception architecture. Of course, readers could also experiment with VGG, ResNet50, etc.; in fact, for captcha recognition, these models are all quite capable. I chose Xception because it has f"}, {"file": "translation_4515.html", "title": "【The Incredible Word2Vec】6. Keras Version of Word2Vec", "content": "← Back to Index 【The Incredible Word2Vec】6. Keras Version of Word2Vec By 苏剑林 | August 06, 2017 Preface After seeing the TF version of Word2Vec I wrote before, Yin Shen from the Keras group asked me if there was a Keras version. In fact, before making the TF version, I had written a Keras version, but I didn't keep it, so I rewrote it—it's more efficient, and the code looks better. This is a pure Keras implementation of Word2Vec. The principles are the same as in \"【The Incredible Word2Vec】5. Tensorflow Version of Word2Vec\" . I'm releasing it now; I think someone will need it. (For example, adding extra inputs yourself to create a better word vector model?) Since Keras simultaneously supports multiple backends like TensorFlow, Theano, and CNTK, this is equivalent to implementing Word2Vec for multiple frameworks. Well, it sounds quite high-end when you think about it that way, haha~ Code GitHub: https://github.com/bojone/tf_word2vec/blob/master/word2vec_keras.py Key Points Above is the code for the CBOW model; if you need Skip-Gram, please modify it yourself. Keras code is so simple that it is easy to modify. Looking through the code, you will find that the part for building the model"}, {"file": "translation_4521.html", "title": "[Memo] Talking about Dropout", "content": "← Back to Index [Memo] Talking about Dropout By 苏剑林 | August 08, 2017 In fact, this is just a memo... Dropout is an effective measure to prevent overfitting in deep learning. Of course, in terms of its underlying philosophy, dropout is not just limited to deep learning; it can also be used in traditional machine learning methods. It just appears more natural within the neural network framework of deep learning. What does it do? How does dropout operate? Generally speaking, for an input tensor $x$, dropout involves setting some elements to zero and then performing a scale transformation on the result. Specifically, taking Keras's Dropout(0.6)(x) as an example, it is essentially equivalent to what follows in numpy: That is to say, 60% of the elements are set to 0, and the remaining 40% of the elements are scaled up to $1/40\\% = 2.5$ times their original value. It is worth noting that the 0.6 in Keras's Dropout(0.6)(x) represents the dropout rate (the proportion to discard), whereas in TensorFlow, the 0.6 in tf.nn.dropout(x, 0.6) represents the keep probability (the proportion to retain). You need to analyze the specific framework for its exact meaning (though if the dropout rate is 0"}, {"file": "translation_4540.html", "title": "A GAN Toy for Fashion-MNIST", "content": "← Back to Index A GAN Toy for Fashion-MNIST By 苏剑林 | August 26, 2017 fashion_mnist_demo The MNIST handwritten digit recognition dataset has always been one of the touchstones for various machine learning algorithms. Recently, a new dataset has emerged to challenge it, called fashion-mnist , which consists of categories like clothing, shoes, and bags. To make it easier for users to migrate to fashion-mnist, the authors made the dataset almost identical to the MNIST handwritten digit recognition dataset—the same number and size of images, the same 10 categories, and even the same data packaging and naming conventions as MNIST. It seems fashion-mnist is really going all out to replace MNIST, putting in the effort to keep everything exactly the same to minimize usage costs. This determination to challenge the status quo is quite firm. The reason for this challenge is simple—many people complain that if an algorithm doesn't work on MNIST, it definitely won't work anywhere else; but if an algorithm is effective on MNIST, it might not necessarily be effective on real-world problems. In other words, the original dataset is too simple and lacks representativeness. fashion-mnist GitHub: http"}, {"file": "translation_4556.html", "title": "A Baseline for Fashion MNIST (MobileNet 95%)", "content": "← Back to Index A Baseline for Fashion MNIST (MobileNet 95%) By 苏剑林 | August 27, 2017 First Taste Yesterday, I briefly tried a GAN model on Fashion MNIST and found that it could work. Of course, that attempt didn't involve much technical skill; it was just a matter of changing the paths in the original script and running it. Today, I returned to the main task of Fashion MNIST itself—10-class classification. I used Keras to test the effects of several models on it and eventually achieved an accuracy of around 94.5%. With data augmentation using random flipping, I was able to reach 95%. At first, I wrote several combinations of models by hand, but testing showed that the accuracy wasn't great. It seems that for this dataset, designing a custom model is quite difficult, so I thought about using existing model architectures. When it comes to off-the-shelf CNN models, we usually think of VGG, ResNet, Inception, Xception, etc. However, these models were designed for the 1,000-class classification problem of ImageNet. Using them on this entry-level dataset seems overkill, and they are prone to overfitting. Then, I suddenly remembered that Keras comes with a model called MobileNet. After c"}, {"file": "translation_4564.html", "title": "School Starts! Let's do Cloze Tests ~ (iFLYTEK Cup)", "content": "← Back to Index School Starts! Let's do Cloze Tests ~ (iFLYTEK Cup) By 苏剑林 | September 03, 2017 Preface Starting this year, the CCL conference (Chinese Computational Linguistics) plans to hold evaluation activities concurrently. I have been interning at a startup recently, and the company signed up for this evaluation. The implementation task ended up falling on me. This year's task is reading comprehension, titled \"The First 'iFLYTEK Cup' Chinese Machine Reading Comprehension Evaluation.\" Although it is called reading comprehension, the task is actually quite simple—it belongs to the Cloze Test type. Specifically, a blank is dug out of a passage, and you must select a word from the context to fill in this blank. Ultimately, our model ranked 6th among single systems, with an accuracy of 73.55% on the validation set and 75.77% on the test set. You can view the leaderboard here . (\"Guangzhou Flame Information Technology Co., Ltd.\" is the model described in this article.) In fact, this dataset and task format were proposed by the Harbin Institute of Technology (HIT) last year, so this evaluation was jointly organized by HIT and iFLYTEK. HIT's paper from last year, \"Consensus Attention"}, {"file": "translation_4582.html", "title": "Evaluation of Input Importance in RNN Models", "content": "← Back to Index Evaluation of Input Importance in RNN Models By 苏剑林 | September 10, 2017 Saliency Maps for RNN RNN is the preferred method for many sequence tasks. For example, a common approach for text classification is \"Word Vectors + LSTM + Fully Connected Classifier,\" as shown in the figure below: RNN Classifier If such a model works well, let us consider a task: how to measure the relative importance (Saliency) of inputs $w_1, \\dots, w_n$ on the final classification result? For instance, if this is a sentiment classification task, how do we identify which words had a more significant impact on the final classification? This article provides a relatively direct approach. The principle behind the idea is simple. Because we pass the state vector from the final step of the RNN (represented by the green shaded vector in the diagram) to the subsequent classifier for classification, the final state vector $\\boldsymbol{h}_n$ serves as the target vector. Since an RNN is a recursive process, $\\boldsymbol{h}_0, \\boldsymbol{h}_1, \\dots, \\boldsymbol{h}_{n-1}$ (where $\\boldsymbol{h}_0$ is typically initialized to all zeros) are steps that gradually approach $\\boldsymbol{h}_n$. RNN Classifi"}, {"file": "translation_4598.html", "title": "From Markov Processes to the Master Equation (Derivation Process)", "content": "← Back to Index From Markov Processes to the Master Equation (Derivation Process) By 苏剑林 | October 06, 2017 The master equation is an important method for modeling stochastic processes. It represents the differential form of a Markov process. In our field, it is one of our primary tools; to put it more broadly, even quantum mechanics and statistical mechanics are essentially special cases of the master equation. However, after reading several textbooks—such as A Modern Course in Statistical Physics and my supervisor's Stochastic Dynamics of Biological Systems —I found that their derivations of the master equation are quite vague. They focus heavily on explaining the meaning of the results but do not clarify the conceptual origins of those results, making the process difficult to find convincing. This is further evidenced by questions on Zhihu, such as \"How to understand the derivation process of the master equation for Markov processes?\" , which have yet to receive satisfactory answers. Markov Process The master equation is used to describe Markov processes. A Markov process can be understood as the property of \"memorylessness\" in motion. In simple terms, the probability distributi"}, {"file": "translation_4611.html", "title": "Image Classification Based on Fine-Tuning (Baidu Dog Classification Competition)", "content": "← Back to Index Image Classification Based on Fine-Tuning (Baidu Dog Classification Competition) By 苏剑林 | October 13, 2017 In the past two years, Baidu's Big Data competitions were focused on natural language processing. This year, the style changed significantly to fine-grained image classification. The task of the competition was to classify pet dogs into one of 100 categories. This task itself is quite standard, and the approach is conventional, involving three aspects: data augmentation, fine-tuning ImageNet models, and model ensemble. I am not particularly good at model ensemble, so I only performed the first two steps. My results were average (accuracy around 80%), but I feel that some of the code might be helpful to readers, so I am sharing it here. Below is an explanation combined with the code. Competition official website (may become invalid at any time): http://js.baidu.com Model The model is mainly implemented using TensorFlow and Keras. First, of course, is importing various modules: # [Code block for importing modules] Next is the model. The base model is Xception, followed by the use of the GLU (Gated Linear Unit) activation function to compress features, and finally"}, {"file": "translation_4637.html", "title": "[Snapshots] Canton Tower at My Doorstep", "content": "← Back to Index [Snapshots] Canton Tower at My Doorstep By 苏剑林 | October 13, 2017 At the end of last semester, I changed dorms and moved to the 7th floor. My doorstep happens to face the Canton Tower. Last night, the weather was clear and quite cool, so I felt inspired to take some photos. I used a tripod + Huawei P9, using \"Super Night View\" mode with an exposure of about 30 seconds (you can right-click to open links for high-definition versions). Canton Tower at My Doorstep Airplane over Canton Tower If you find this article helpful, you are welcome to share or donate to this article. Donations are not for profit, but to let me know how many readers are truly following Scientific Spaces. Of course, even if you disregard it, your reading experience will not be affected. Welcome and thank you! ,\n         author={Su Jianlin},\n         year={2017},\n         month={Oct},\n         url={\\url{https://kexue.fm/archives/4637}},\n} Category: Photography | \n        Tags: Starry Sky , Shooting , Photos Citation This is a machine translation of the original Chinese article: https://kexue.fm/archives/4637 Original author: 苏剑林 (Su Jianlin) Original publication: 科学空间 (Scientific Spaces) Translated"}, {"file": "translation_4638.html", "title": "The Significance of Training, Validation, and Test Sets", "content": "← Back to Index The Significance of Training, Validation, and Test Sets By 苏剑林 | Oct 14, 2017 In supervised machine learning, we often speak of training sets (train), validation sets (validation), and test sets (test). The distinction between these three sets can be confusing, especially for readers who are unclear about the difference between the validation set and the test set. Partitioning If we already have an existing large labeled dataset and want to complete a test of a supervised model, we typically use a uniform random sampling method to partition the dataset into a training set, a validation set, and a test set. These three sets must not have any overlap. A common ratio is 8:1:1, though the proportion is arbitrary. From this perspective, all three sets are identically distributed. If it is for a competition where the organizers provide a labeled dataset (as a training set) and an unlabeled test set, we usually manually partition a validation set from the training set ourselves. In this case, we typically do not partition a further test set. There are two likely reasons for this: 1. Competition organizers are generally very stingy, so the samples in the training set are al"}, {"file": "translation_4647.html", "title": "On the Design of Activation Functions in Neural Networks", "content": "← Back to Index On the Design of Activation Functions in Neural Networks By 苏剑林 | October 26, 2017 Activation functions are the source of non-linearity in neural networks. Without these functions, the entire network would consist only of linear operations. Since the composition of linear operations is still linear, the final effect would be equivalent to a single-layer linear model. So, what are the common activation functions? Furthermore, what principles should guide the choice of an activation function? Can any non-linear function serve as an activation function? The activation functions explored here are those for hidden layers, not for the output layer. The final output generally uses specific activation functions that cannot be changed arbitrarily; for example, binary classification typically uses the sigmoid function, multi-class classification typically uses softmax, and so on. In contrast, there is more room for choice regarding activation functions in the hidden layers. Even Floating Point Errors Work! Theoretically, any non-linear function has the potential to be an activation function. A very convincing example is a recent successful attempt by OpenAI to use floating-po"}, {"file": "translation_4667.html", "title": "A More Unique Word Vector Model (I): simpler glove", "content": "← Back to Index A More Unique Word Vector Model (I): simpler glove By 苏剑林 | November 19, 2017 By Su Jianlin | November 19, 2017 | 52,909 Readers If you ask me which is the most convenient and easiest-to-use word vector model, I think it should be word2vec. But if you ask me which is the most beautiful word vector model, I don't know; I feel that every model has its own deficiencies. Setting aside experimental results (which are often just a matter of evaluation metrics), purely from a theoretical perspective, no model can yet be called truly \"beautiful.\" This article discusses some common questions regarding word vectors that many people are concerned about. Many conclusions have been discovered primarily through experimentation but lack reasonable explanations, including: How should one construct a word vector model? Why use cosine similarity for synonym search? What does the inner product of vectors represent? Does the norm (length) of a word vector have any special meaning? Why do word vectors possess word analogy properties? (King - Man + Woman = Queen) After obtaining word vectors, how should sentence vectors be constructed? What is the basis for using the sum of word vectors "}, {"file": "translation_4669.html", "title": "A More Chic Word Vector Model (II): Modeling Language", "content": "← Back to Index A More Chic Word Vector Model (II): Modeling Language By 苏剑林 | November 19, 2017 From Conditional Probability to Mutual Information Currently, the principle behind most word vector models is that the distribution of a word's context can reveal its semantics, much like the saying \"show me who your friends are, and I'll tell you who you are.\" Therefore, the core of word vector models is modeling the relationship of the context. Except for GloVe, almost all word vector models attempt to model the conditional probability $P(w|\\text{context})$. For example, Word2Vec's skip-gram model models the conditional probability $P(w_2|w_1)$. However, this quantity has some drawbacks. First, it is asymmetric, meaning $P(w_2|w_1)$ does not necessarily equal $P(w_1|w_2)$. Consequently, when modeling, we must distinguish between context vectors and target vectors, and they cannot reside in the same vector space. Secondly, it is a bounded, normalized quantity, which means we must use methods like Softmax to compress and normalize it, leading to optimization difficulties. In fact, in the world of NLP, there is a more symmetric quantity that is more important than simple $P(w_2|w_1)$, an"}, {"file": "translation_4671.html", "title": "A More Distinctive Word Vector Model (III): Models Describing Correlation", "content": "← Back to Index A More Distinctive Word Vector Model (III): Models Describing Correlation By 苏剑林 | Nov 19, 2017 Geometric Word Vectors Although the \"matchmaker\" analogy mentioned earlier is just a metaphor, the problems it faces are real. According to traditional NLP methods, we can count the co-occurrence frequency of any two words and the frequency of each word itself, and then calculate their relevance to obtain a \"relevance matrix.\" However, as mentioned before, this co-occurrence matrix is far too large; it must be compressed and reduced in dimensionality. At the same time, data smoothing is required to give a reasonable estimate for the relevance of word pairs that have not appeared together. In existing machine learning solutions, we already have some experience in reducing the dimensionality of huge matrices, such as SVD and pLSA. SVD is for the dimensionality reduction of any matrix, while pLSA is for the dimensionality reduction of the transition probability matrix $P(j|i)$. The ideas behind both are similar: they decompose a large matrix $\\mathbf{A}$ into the product of two small matrices $\\mathbf{A} \\approx \\mathbf{BC}$, where the number of rows in $\\mathbf{B}$ equals t"}, {"file": "translation_4675.html", "title": "A More Elegant Word Vector Model (Part 4): Solving the Model", "content": "← Back to Index A More Elegant Word Vector Model (Part 4): Solving the Model By 苏剑林 | November 19, 2017 Loss Function Now, we define the loss function in order to solve for each word vector. Using $\\tilde{P}$ to represent the estimated frequency of $P$, we can directly define the loss as the following expression: \\[\\sum_{w_i,w_j}\\left(\\langle \\boldsymbol{v}_i, \\boldsymbol{v}_j\\rangle-\\log\\frac{\\tilde{P}(w_i,w_j)}{\\tilde{P}(w_i)\\tilde{P}(w_j)}\\right)^2 \\tag{16}\\] Compared to GloVe, this approach is simpler in both the number of parameters and the model form; therefore, we call it \"Simpler GloVe.\" The GloVe model is defined as: \\[\\sum_{w_i,w_j}\\left(\\langle \\boldsymbol{v}_i, \\boldsymbol{\\hat{v}}_j\\rangle+b_i+\\hat{b}_j-\\log X_{ij}\\right)^2 \\tag{17}\\] In the GloVe model, a distinction is made between center word vectors and context vectors, and the model suggests outputting the sum of these two sets of word vectors, which is said to yield better results. This is a somewhat strained trick, but not a major issue. The biggest problem is that the parameters $b_i, \\hat{b}_j$ are also trainable, which makes the model severely ill-posed! Consider: \\begin{equation}\n    \\begin{aligned}\n    &\\su"}, {"file": "translation_4677.html", "title": "A More Unique Word Vector Model (Part 5): Interesting Results", "content": "← Back to Index A More Unique Word Vector Model (Part 5): Interesting Results By 苏剑林 | November 19, 2017 Finally, let's take a look at what good properties the word vector model $(15)$ possesses, or rather, what rewards we get for going through such pains to construct a new word vector model? Meaning of the Norm It seems that in almost all word vector models, very little attention is paid to the norm (length) of the word vectors. Interestingly, in the word vectors obtained from our aforementioned model, the norm can represent the importance of a word to a certain extent. We can understand this fact from two perspectives. In a context within a single window, the probability of the center word reappearing is actually not large; it is a relatively random event. Therefore, we can roughly assume:\n\\[P(w,w) \\sim P(w)\\tag{24}\\] We can also understand it from another angle by decomposing each vector into its norm and direction:\n\\[\\boldsymbol{v}=\\Vert\\boldsymbol{v}\\Vert\\cdot\\frac{\\boldsymbol{v}}{\\Vert\\boldsymbol{v}\\Vert}\\tag{27}\\]\nWhere the norm $\\|\\boldsymbol{v}\\|$ is an independent parameter, and the direction vector $\\boldsymbol{v}/\\Vert\\boldsymbol{v}\\Vert$ consists of $n-1$ independent p"}, {"file": "translation_4681.html", "title": "A More Unique Word Vector Model (6): Code, Sharing, and Conclusion", "content": "← Back to Index A More Unique Word Vector Model (6): Code, Sharing, and Conclusion By 苏剑林 | November 19, 2017 List A More Unique Word Vector Model (1): simpler glove A More Unique Word Vector Model (2): Modeling Language A More Unique Word Vector Model (3): Description-Related Models A More Unique Word Vector Model (4): Solving the Model A More Unique Word Vector Model (5): Interesting Results A More Unique Word Vector Model (6): Code, Sharing, and Conclusion Code The implementation of this article is located at: https://github.com/bojone/simpler_glove The source code is modified from Stanford's original GloVe . I have only made minor modifications because the main difficulty lies in the statistics of co-occurrence frequencies, and I am grateful to the predecessors at Stanford for providing such a classic and excellent statistical implementation case. In fact, I am not familiar with the C language, so the modifications I have made might not be of high caliber; I hope experts will offer corrections. In addition, to implement the \"interesting results\" from the previous section, I have supplemented simpler_glove.py in the GitHub repository. It encapsulates a class that can directly re"}, {"file": "translation_4695.html", "title": "CRF In A Nutshell", "content": "← Back to Index CRF In A Nutshell By 苏剑林 | November 25, 2017 This article aims to explain the principles of CRF (Conditional Random Field) using language that is as concise as possible. The term \"In A Nutshell\" carries meanings such as \"introduction\" or \"popular science\" (Hawking wrote a book called The Universe in a Nutshell , and I am clumsily following suit here). Most articles introducing CRF on the web, whether in Chinese or English, usually start by discussing concepts of probability graphs and then introduce the exponential formula for features, claiming it as CRF. \"Probability graphs\" are merely an illustrative way of understanding; however, if the principles are not clear, providing too many figurative analogies will only confuse the reader, making it seem like you are just being pretentious. (Speaking of which, I want to vent again: solving a neural network is clearly about calculating a gradient and then iterating—this is so easy to understand—yet people gave it a pretentious name like \"backpropagation.\" If you don't clearly state that its essence is differentiation and iterative solving, how many readers will truly understand when you just say backpropagation?) Alright,"}, {"file": "translation_4718.html", "title": "The Method of Characteristics for First-Order Partial Differential Equations", "content": "← Back to Index The Method of Characteristics for First-Order Partial Differential Equations By 苏剑林 | December 07, 2017 This article introduces the method of characteristics for first-order partial differential equations in as clear and concise a manner as possible. Personally, I believe this is one of the simpler but often potentially confusing parts of partial differential equation theory. Therefore, I am attempting to introduce it in my own words. Of course, more accurately, this serves as a personal memorandum. Quasi-linear Case General Steps Consider the partial differential equation\n\\begin{equation}\\boldsymbol{\\alpha}(\\boldsymbol{x},u) \\cdot \\frac{\\partial}{\\partial \\boldsymbol{x}} u = \\beta(\\boldsymbol{x},u)\\end{equation}\nwhere $\\boldsymbol{\\alpha}$ is an $n$-dimensional vector function, $\\beta$ is a scalar function, $\\cdot$ denotes the dot product of vectors, $u \\equiv u(\\boldsymbol{x})$ is a function of $n$ variables, and $\\boldsymbol{x}$ represents the independent variables. The idea of the method of characteristics is to imagine $\\boldsymbol{x}$ as a function of some parameter $s$. In this case, $\\boldsymbol{x}(s)$ actually represents the parametric equation of a high-di"}, {"file": "translation_4733.html", "title": "From Loss Hard Truncation and Softening to Focal Loss", "content": "← Back to Index From Loss Hard Truncation and Softening to Focal Loss By 苏剑林 | December 25, 2017 Preface In a discussion in a QQ group today, I saw \"Focal Loss.\" After some searching, I found it is a loss function proposed by Kaiming He's team in their paper \"Focal Loss for Dense Object Detection,\" which they used to improve the results of image object detection. However, I rarely work on image-related tasks and don't pay much attention to image applications. Essentially, Focal Loss is a loss function designed to solve the problems of class imbalance and differences in classification difficulty in classification tasks. In short, this work has been widely acclaimed. You can also refer to the discussion on Zhihu: \"How to evaluate Kaiming's Focal Loss for Dense Object Detection?\" When I first saw this loss, it felt quite magical and seemed very useful. This is because in Natural Language Processing (NLP), there are also many tasks with significant class imbalance. The most classic example is sequence labeling, where categories are highly unbalanced. For instance, in Named Entity Recognition (NER), it's obvious that entities are much scarcer than non-entities within a sentence. I tried"}, {"file": "translation_4760.html", "title": "2018 Astronomy Calendar of Celestial Events", "content": "← Back to Index 2018 Astronomy Calendar of Celestial Events By 苏剑林 | December 31, 2017 Astronomy Calendar of Celestial Events 2018 Astronomy Calendar of Celestial Events Translated from NASA: http://eclipse.gsfc.nasa.gov/SKYCAL/SKYCAL.html (Beijing Time) 2011 Version 2012 Version 2013 Version 2014 Version 2015 Version 2016 Version 2017 Version Day Week Time Event January 01 Mon Venus: 1.9° W 02 Tue 03:59:00 Mercury Greatest Elongation: 22.7° W 02 Tue 05:54:00 Moon at Perigee: 356,600 km 02 Tue 08:01:00 Moon at North Declination: 20.1° N 02 Tue 10:24:00 Full Moon 03 Wed 10:59:00 Earth at Perihelion: 0.9833 AU 04 Thu 03:50:00 Beehive Cluster (M44) Conjunction with Moon: 2.3° N 04 Thu 04:19:00 Quadrantid Meteor Shower: ZHR = 120 04 Thu 15:48:00 Moon at Ascending Node 05 Fri 15:24:00 Regulus Conjunction with Moon: 0.9° S 07 Sun 08:39:00 Mars Conjunction with Jupiter: 0.2° N 09 Tue 06:25:00 Last Quarter Moon 09 Tue 14:16:00 Venus Superior Conjunction 11 Thu 13:59:00 Jupiter Conjunction with Moon: 4.7° S 13 Sat 15:58:00 Mercury Conjunction with Saturn: 0.7° N 15 Mon 10:09:00 Moon at Apogee: 406,500 km 15 Mon 10:13:00 Saturn Conjunction with Moon: 2.9° S 16 Tue 00:28:00 Moon at South Decl"}, {"file": "translation_4765.html", "title": "A Brief Reading of \"Attention is All You Need\" (Introduction + Code)", "content": "← Back to Index A Brief Reading of \"Attention is All You Need\" (Introduction + Code) By 苏剑林 | January 06, 2018 In mid-2017, there were two similar papers that I particularly appreciated: Facebook's \"Convolutional Sequence to Sequence Learning\" and Google's \"Attention is All You Need\" . Both are innovations in Seq2Seq, and essentially, both abandon the RNN structure to perform Seq2Seq tasks. In this blog post, I will provide a simple analysis of \"Attention is All You Need\" . Since both papers are quite popular and there are already many interpretations online (though many are direct translations of the paper with little personal insight), I will try to use my own words as much as possible and avoid repeating what others have already said. Sequence Encoding The standard approach for Deep Learning in NLP is to first tokenize a sentence and then convert each word into a corresponding word vector sequence. Thus, each sentence corresponds to a matrix $\\boldsymbol{X}=(\\boldsymbol{x}_1,\\boldsymbol{x}_2,\\dots,\\boldsymbol{x}_t)$, where $\\boldsymbol{x}_i$ represents the word vector (row vector) of the $i$-th word with dimension $d$. Therefore, $\\boldsymbol{X}\\in \\mathbb{R}^{n\\times d}$. The p"}, {"file": "translation_4797.html", "title": "Enhancing Typecho's Search Functionality", "content": "← Back to Index Enhancing Typecho's Search Functionality By 苏剑林 | January 09, 2018 Scientific Spaces is a blog built using the Typecho program. The sidebar provides a search function; however, Typecho's built-in search is merely a string-based exact match search. As a result, many reasonable queries fail to return results—for example, \"2018天象\" (2018 Astronomical Phenomena) or \"新词算法\" (New Word Algorithm) cannot produce results simply because those exact strings do not appear in the articles. This led to the idea of strengthening the search function, an improvement that some readers had previously suggested. Over the past few days, I did some research. Initially, I planned to use the Whoosh library in Python to build a full-text search engine, but I felt the workload for integration and future maintenance was too high, so I abandoned that path. Later, I thought about enhancing Typecho's own search directly. With the help of a colleague (a \"big shot\") at my company, I completed this improvement. Since the improvement is implemented by directly modifying Typecho's source files, it might be overwritten if Typecho is upgraded. Therefore, I am making a note of it here as a memo. Explorati"}, {"file": "translation_4819.html", "title": "Uncovering the Mist: A Delicious Capsule Feast", "content": "← Back to Index Uncovering the Mist: A Delicious Capsule Feast By 苏剑林 | January 23, 2018 Geoffrey Hinton at Google's Toronto Office The Capsule paper \"Dynamic Routing Between Capsules\" , open-sourced by deep learning pioneer Geoffrey Hinton, was undoubtedly one of the hottest topics in the deep learning community last year. Thanks to various media hyperboles, Capsules have been shrouded in mystery, with phrases like \"abandoning gradient descent\" and \"overthrowing deep learning\" appearing frequently. Yet, others feel that Capsule is nothing more than a new marketing hype. This article attempts to lift the confusing mist, grasp the principles and charm behind Capsules, and enjoy this \"Capsule Feast.\" Simultaneously, I have supplemented this with an experiment of my own design, which demonstrates the effectiveness of Capsules more powerfully than the experiments in the original paper. The Menu: 1. What is a Capsule? 2. Why do it this way? 3. Is Capsule truly good? 4. What do I think of Capsules? 5. A few side dishes. Preface The Capsule paper has been out for several months now, and many experts have provided interpretations and open-source implementations of CapsuleNet. These resourc"}, {"file": "translation_4823.html", "title": "Sharing a Slide: Fancy Natural Language Processing", "content": "← Back to Index Sharing a Slide: Fancy Natural Language Processing By 苏剑林 | January 23, 2018 By 苏剑林 |\nJan 23, 2018 |\n99428 Readers | Fancy Natural Language Processing This is the slide used during an exchange at South China Normal University a few days ago, which mainly introduces some techniques in natural language processing. The starting point for this slide is that many NLP groups in domestic universities essentially stay stuck in the mindset of RNNs. Therefore, I introduced some content on CNNs and Attention, and also introduced some techniques for model training, etc. The content is actually quite brief, but considering that many concepts are new to most students, the information density is quite high. This was also my first attempt at using $\\LaTeX$ to create a slide. It felt like it wasn't as difficult as I had imagined, and the resulting effect is quite fresh and clear. I should practice more in the future~ Download link: Fancy Natural Language Processing.pdf When reprinting, please include the original address of this article: https://kexue.fm/archives/4823 If you have any doubts or suggestions, you are welcome to continue the discussion in the comments section below. If "}, {"file": "translation_5048.html", "title": "[Science Students Read Novels] Let's Talk About \"Deflecting a Thousand Pounds with Four Ounces\"", "content": "← Back to Index [Science Students Read Novels] Let's Talk About \"Deflecting a Thousand Pounds with Four Ounces\" By 苏剑林 | January 28, 2018 The Colorful World of Jin Yong In the works of Jin Yong (and indeed many martial arts novels), martial arts can be divided into three types: The first type is raw, solid power, such as Hong Qigong's \"Eighteen Subduing Dragon Palms\" or Jiumozhi's \"Dragon-Elephant Wisdom Skill.\" Their main characteristic is fierce strength. For example: Qiao Feng's Twenty-Eight Subduing Dragon Palms were passed down by the previous beggar clan leader, Wang Jiantong. However, Qiao Feng was born with extraordinary talent and was blessed in the realm of martial arts. His Twenty-Eight Subduing Dragon Palms were like a crushing dry weed, invincible and even more powerful than Clan Leader Wang's. Seeing the opponent pushing with both palms, Qiao Feng felt that if he resisted with a single palm and fought to a draw, he might seem to have a slight advantage, which would be disrespectful. Thus, he also struck out with both palms. In his left and right palms, the force used was still \"three parts external, seven parts internal,\" keeping most of the strength in reserve. ——Fro"}, {"file": "translation_5066.html", "title": "Site Update Log (January 2018)", "content": "← Back to Index Site Update Log (January 2018) By 苏剑林 | January 29, 2018 By 苏剑林 (Jianlin Su) | January 29, 2018 | 36,959 Readers Readers may have noticed that visiting Scientific Spaces has been somewhat unstable over the past few days. The reason is that I have been making significant adjustments to the website during this time. Although the scope of these adjustments is quite large, they might be difficult to notice from the surface, so I am recording them here as a memento. The main updates include: 1. Theme Optimization: The geekg theme used by this blog is actually quite old. Last year, I paid someone to perform the first major upgrade, which added responsive design. Over the past few days, I mainly focused on resolving some legacy issues with this theme, including fine adjustments to image display, margins, and typography. 2. Internal Optimization: I have significantly reduced the use of plugins. Some basic functions (such as the site directory and archive pages) have been embedded directly into the theme. This reduces dependency on plugins and improves usability. 3. Article Optimization: This was also a legacy issue. In the early days of writing, I was quite casual about for"}, {"file": "translation_5067.html", "title": "[Share] 10-Million-Level Baidu Zhidao Corpus", "content": "← Back to Index [Share] 10-Million-Level Baidu Zhidao Corpus By 苏剑林 | January 30, 2018 Release January 30, 2018 Quantity Total of 10 million entries Format [\n {\n \"url\": \"http://zhidao.baidu.com/question/565618371557484884.html\",\n \"question\": \"What are the vocational colleges for learning to be a clerk?\",\n \"tags\": [\n \"School\",\n \"Junior College\",\n \"Institution Information\"\n ]\n },\n {\n \"url\": \"http://zhidao.baidu.com/question/2079794100345438428.html\",\n \"question\": \"Is there a difference between online gambling and gambling in Macau?\",\n \"tags\": [\n \"Network\",\n \"Macau\",\n \"Gambling\"\n ]\n }\n] Purpose Think for yourself Source Obtained through several months of self-conducted continuous monitoring and crawling. Instructions This shared data is for learning and research purposes only. Please do not use it for any commercial or illegal purposes. The user is solely responsible for any adverse consequences caused by the irregular use of this corpus. Author Su Jianlin ( http://kexue.fm ) Download Link: https://pan.baidu.com/s/1zzDobW9FY7JXP6c_9QChdg Password: 7shl Compressed size: 300MB+; Uncompressed size: 2GB+ When republishing, please include the address of this article: https://kexue.fm/archi"}, {"file": "translation_5112.html", "title": "Another New Year's Feast: From K-Means to Capsule", "content": "← Back to Index Another New Year's Feast: From K-Means to Capsule By 苏剑林 | February 12, 2018 In this article, we perform another analysis of Capsule. Overall, the details of the Capsule algorithm are not very complex; if you follow the process, implementing it with a framework is basically no problem. Therefore, the difficult part is understanding exactly what Capsule does and why it does it that way, especially those steps in Dynamic Routing. Why do I repeatedly analyze Capsule? This is not simply \"reheating old rice,\" but rather an attempt to gain an intuitive understanding of Capsule's principles. As everyone knows, the feeling Capsule gives is that there is \"too much man-made convention,\" lacking a direct sense of \"even if I don't fully understand it, I believe it should be this way.\" I hope to think through the origin and development of Capsule as clearly as possible, so that we can feel that Capsule is a natural and smooth model, and even draw inferences from it. In \"Unveiling the Mist: A Delicious Capsule Feast,\" I first analyzed the results of dynamic routing and then pointed out that the output is a kind of clustering of the input. This \"from result to cause\" process had m"}, {"file": "translation_5155.html", "title": "Three Flavors of Capsule: Matrix Capsules with EM Routing", "content": "← Back to Index Three Flavors of Capsule: Matrix Capsules with EM Routing By 苏剑林 | March 02, 2018 In fact, shortly after the release of the paper \"Dynamic Routing Between Capsules,\" a new Capsule paper, \"Matrix Capsules with EM Routing,\" was already anonymously disclosed (during the ICLR 2018 blind review process). Now that the authors have been revealed, they are Geoffrey Hinton, Sara Sabour, and Nicholas Frosst. As expected, Hinton is indeed among them. As everyone knows, results published by \"founding father\" figures like Hinton are generally quite \"heavyweight.\" So, what are the characteristics of this new paper? During my thinking process, the article \"Understanding Matrix capsules with EM Routing\" provided me with much inspiration, and the discussions by various experts on Zhihu also accelerated my reading. I would like to express my gratitude for those resources. Paper Abstract Let us first recall the diagram from the previous introduction, \"Another New Year's Feast: From K-Means to Capsule\" : A simple schematic of the Capsule framework This diagram shows that Capsule actually describes a modeling framework. Many components within this framework can be customized, most notab"}, {"file": "translation_5239.html", "title": "From Maximum Likelihood to EM Algorithm: A Consistent Way of Understanding", "content": "← Back to Index From Maximum Likelihood to EM Algorithm: A Consistent Way of Understanding By 苏剑林 | March 15, 2018 Recently, I have been thinking about content related to unsupervised learning in NLP and probabilistic graphical models, so I revisited some parameter estimation methods. In deep learning, parameter estimation is one of the most basic steps, which is what we call the model training process. To train a model, there must be a loss function. For readers who haven't systematically studied probability theory, the most natural loss function they might think of is Mean Squared Error (MSE), which corresponds to what we call Euclidean distance. Theoretically speaking, the best match for a probabilistic model should be the \"Cross-Entropy\" function, which originates from the Maximum Likelihood function in probability theory. Maximum Likelihood Rational Existence What is Maximum Likelihood? There is a philosophical saying: \"To exist is to be rational.\" Maximum Likelihood means \"to exist is to be most rational.\" Specifically, if the probability distribution of event $X$ is $p(X)$, and if the specific values observed in a single observation are $X_1, X_2, \\dots, X_n$, assuming they "}, {"file": "translation_5253.html", "title": "Variational Autoencoders (I): That's What It's All About", "content": "← Back to Index Variational Autoencoders (I): That's What It's All About By 苏剑林 | March 18, 2018 Although I hadn't looked into it closely in the past, I always had the impression that the Variational Autoencoder (VAE) was a fascinating tool. Taking advantage of a recent short-lived enthusiasm for Probabilistic Graphical Models, I decided to strive to understand VAE as well. As usual, I flipped through a lot of materials online and found that, without exception, they were quite vague. The general feeling was that while there were pages of formulas, everything remained blurry. When I finally thought I understood it and checked the implementation code, I felt the code and the theory were two completely different things. Finally, by piecing things together and combining them with my recent accumulation of knowledge on probabilistic models, and repeatedly comparing everything with the original paper \"Auto-Encoding Variational Bayes\" , I think I've finally figured it out. Actually, the real VAE is somewhat different from what many tutorials describe. Many tutorials write a lot without highlighting the key points of the model. Thus, I wrote this piece, hoping to explain VAE clearly throug"}, {"file": "translation_5332.html", "title": "Poetry Robot Based on CNN and VAE: Random Poetry Generation", "content": "← Back to Index Poetry Robot Based on CNN and VAE: Random Poetry Generation By 苏剑林 | March 24, 2018 A few days ago, I wrote a popular science interpretation of VAE , which received recognition from some readers. However, are you tired of every introduction only featuring a MNIST-level demo? Don't worry, I'm bringing you a more classic VAE toy: a poetry-composing robot. Why do I say \"more classic\"? In the previous article, we mentioned that images generated by VAE tend to be blurrier than those generated by GANs, meaning VAE is at a disadvantage in the image \"battle.\" However, in the realm of text generation, VAE has won quite handsomely. This is because GANs attempt to directly train a discriminator (metric); however, for text, this metric is likely discrete and non-differentiable, making pure GANs very difficult to train. VAE does not have this step; it operates by reconstructing the input, a process that can be performed for both images and text. Therefore, text generation is a basic, direct application for VAE, much like image generation; for (current) GANs, however, it remains a symbol of hardship and a persistent \"headache.\" Well, in ancient times Cao Zhi composed poetry in se"}, {"file": "translation_5343.html", "title": "Variational Autoencoders (Part 2): From a Bayesian Perspective", "content": "← Back to Index Variational Autoencoders (Part 2): From a Bayesian Perspective By 苏剑林 | March 28, 2018 Origins A few days ago, I wrote the blog post \"Variational Autoencoders (Part 1): So That's How It Is\" , which understood Variational Autoencoders (VAE) from a relatively intuitive perspective. From the viewpoint of that article, a VAE is not much different from a standard autoencoder, except for the addition of noise and constraints on that noise. However, my original intention for wanting to understand VAEs was to see exactly how the probabilistic graphical models of the Bayesian school work in conjunction with Deep Learning. Gaining only an intuitive understanding is clearly not enough. Therefore, I continued to think about VAEs for a few days, attempting to explain them clearly using more general, probabilistic language. In fact, this line of thinking can answer questions that the intuitive understanding cannot, such as whether it is better to use MSE or Cross-Entropy for reconstruction loss, how to balance the reconstruction loss and the KL loss, and so on. It is recommended to read \"Variational Autoencoders (Part 1): So That's How It Is\" before reading this article. This pos"}, {"file": "translation_5383.html", "title": "Variational Autoencoders (Part 2): From a Bayesian Perspective", "content": "← Back to Index Variational Autoencoders (Part 2): From a Bayesian Perspective By 苏剑林 | April 03, 2018 I feel like my recent articles have been becoming quite long, and they seem to come in clusters! After writing three consecutive posts introducing Capsules, it's now VAE's turn. This is the third exploration of VAE, and who knows, there might even be a fourth. Regardless, quantity isn't what matters; what's important is thinking through the problems clearly. Especially for a novel modeling approach like VAE, it's worth scrutinizing the details. The question we want to address this time is: Why does VAE actually work? I imagine readers of VAE go through several stages. Phase one: having just read an introduction and feeling lost in the fog—it seems like an autoencoder but also doesn't; after several readings and looking at the source code, you get a general idea. Phase two: building on the first, reading the principles deeply—latent variable models, KL divergence, variational inference, etc. As you dig deeper, you find that despite all the tossing and turning, you finally understand it. At this point, readers might enter the third stage. Here, various questions arise, particularly "}, {"file": "translation_5409.html", "title": "DGCNN: A CNN-based Reading Comprehension Question Answering Model", "content": "← Back to Index DGCNN: A CNN-based Reading Comprehension Question Answering Model By 苏剑林 | April 15, 2018 Update 2019.08.20: An open-source Keras version has been released ( https://kexue.fm/archives/6906 ) As early as the beginning of the year in the introductory article on \"Attention is All You Need\" , I promised to share my experience using CNNs in NLP. However, I haven't had the chance until now. These past few days, I finally decided to organize the relevant content. Background Without further ado, let's first introduce the basic situation of the model. Model Features This model—which I call DGCNN —is based on CNNs and a simple Attention mechanism. Since it does not use an RNN structure, it is quite fast. Moreover, it is specifically tailored for WebQA-style tasks, making it very lightweight. Models at the top of the SQuAD leaderboard, such as AoA and R-Net, all use RNNs and are accompanied by complex attention interaction mechanisms, none of which appear in DGCNN. This is a model that can be trained in just a few hours on a GTX 1060! DGCNN, which stands for Dilated Gated Convolutional Neural Network, as the name suggests, integrates two relatively new convolutional techniques"}, {"file": "translation_5448.html", "title": "Minimum Entropy Principle (I): The Principle of Unsupervised Learning", "content": "← Back to Index Minimum Entropy Principle (I): The Principle of Unsupervised Learning By 苏剑林 | April 18, 2018 Opening Words In today's world, where end-to-end schemes like deep learning have gradually swept through NLP, are you still willing to think about the fundamental principles behind natural language? We often use the term \"text mining\"—do you truly feel the sense of \"mining\"? A Chance Encounter A while ago, I read an article on unsupervised syntactic analysis . Subsequently, from its references, I discovered the paper \"Redundancy Reduction as a Strategy for Unsupervised Learning\" . This paper describes how to recover English words from English text with spaces removed. When applied to Chinese, isn't this exactly the construction of a lexicon? So, I read it with great interest and found the paper's reasoning clear, the theory complete, and the results beautiful; it was a delight to read. Although the value of this paper may not seem very high today, and its results might have already been studied by many, it is important to note: this is a paper from 1993! In an era before PCs became popular, such forward-looking research was conducted. While deep learning is popular now and "}, {"file": "translation_5476.html", "title": "Minimum Entropy Principle (II): Word Bank Construction via \"Prompt Decisions\"", "content": "← Back to Index Minimum Entropy Principle (II): Word Bank Construction via \"Prompt Decisions\" By 苏剑林 | April 24, 2018 In this article, we introduce the first move of our \"Routine Bible\"—\"Prompt Decisions\": 1. Derive the concept of average character information entropy, then deduce the mutual information formula based on the minimum entropy principle; 2. Complete the unsupervised construction of a word bank and provide an information entropy interpretation of the unigram segmentation model, thereby demonstrating the basic methods and techniques for generating and identifying \"routines\" (patterns). This is both the first use case of the minimum entropy principle and the general outline for the entire \"Routine Bible.\" Whether you practice it or not, the routine is there, neither increasing nor decreasing. Why do we need words? As seen in the previous article , assuming we don't understand Chinese at all, we initially view it as a series of randomly combined strings of \"characters.\" However, we gradually discover that the context is connected; it isn't a random combination of characters, but rather a random combination of \"routines.\" To reduce our memory burden, we attempt to mine thes"}, {"file": "translation_5505.html", "title": "Conv1D-Based Spectrum Classification Model (1D Sequence Classification)", "content": "← Back to Index Conv1D-Based Spectrum Classification Model (1D Sequence Classification) By 苏剑林 | May 02, 2018 Some time ago, Tianchi launched an Astronomical Data Mining Competition —LAMOST Spectrum Classification (identifying corresponding spectra into one of four categories). Although there was no prize money, I found it quite interesting, so I signed up. I worked on it for a while and felt my performance was decent. However, in the end, I forgot (or rather, I didn't notice at all) that there was one final step to submit results for a new test set during the last two days of the preliminary round. And that was it—I left behind an incomplete model. It could be described as \"falling before the battle was won,\" and I was the one who did myself in~ Astronomical Data Mining Contest—Intelligent Classification of Celestial Spectra Later, I discussed it with other contestants and found that my model was actually quite good. I remember the top score in the preliminary round was 0.83+, while my score at the time was 0.82+, ranking around 4th or 5th. Moreover, it is said that many teams with scores above 0.8 used ensemble models, while my 0.82+ score was just the result of a single model. I"}, {"file": "translation_5525.html", "title": "Efficient Implementation of Apriori Algorithm using Numpy", "content": "← Back to Index Efficient Implementation of Apriori Algorithm using Numpy By 苏剑林 | May 10, 2018 A classic example of association rules: Beer and Diapers Three years ago, I wrote \"Efficient Implementation of the Apriori Algorithm using Pandas\" , where I provided a Python implementation of the Apriori algorithm that was well-received by some readers. However, my Python skills weren't very advanced at the time, so in hindsight, that implementation wasn't particularly elegant (though its speed was acceptable), and it didn't support variable-length input data. I had promised to rewrite the algorithm to address these issues, and I have finally completed it~ I won't repeat the introduction to the Apriori algorithm here; I'll just post the code directly: Usage method: Output results: [(('A3', 'F4', 'H4'), (0.8795180722891566, 0.07849462365591398)), (('C3', 'F4', 'H4'), (0.875, 0.07526881720430108)), (('B2', 'F4', 'H4'), (0.7945205479452054, 0.06236559139784946)), (('C2', 'E3', 'D2'), (0.7543859649122807, 0.09247311827956989)), (('D2', 'F3', 'H4', 'A2'), (0.7532467532467533, 0.06236559139784946))] The meaning of the results is that the first $n-1$ items imply the $n$-th item, such as $A3 + "}, {"file": "translation_5533.html", "title": "[Tribute] The 100th Anniversary of Feynman's Birth", "content": "← Back to Index [Tribute] The 100th Anniversary of Feynman's Birth By 苏剑林 | May 11, 2018 By 苏剑林 | May 11, 2018 Feynman - Image from Baidu Baike May 11, 2018, marks the 100th anniversary of Richard Feynman's birth. I first saw this news early this morning in an article titled \"Commemorating Feynman | Feynman's Ten Great Contributions\" on the \"Jingshi Physics\" public account. I consider myself a Feynman fan, though I can never remember birthdays or zodiac information; what sticks with me are Feynman's stories, both scientific and personal. I own most of Feynman's works, and I previously wrote \"I am a Feynman Fan.\" Of course, I haven't understood most of them, but I remain fascinated by his theories and stories. I have read \"Surely You're Joking, Mr. Feynman!\" and \"What Do You Care What Other People Think?\" several times through. They are funny, deep, and to some extent, have influenced my own values. A true \"scientific Little Flying Man\" (scientific Peter Pan) not only produces admirable achievements but also leaves behind a life story that provides endless food for thought. Similar commemorations: \"Passing Through His Entire World — Commemorating the Most Interesting Physicist Feynm"}, {"file": "translation_5542.html", "title": "A Concise Introduction to Conditional Random Fields (CRF) (with a Pure Keras Implementation)", "content": "← Back to Index A Concise Introduction to Conditional Random Fields (CRF) (with a Pure Keras Implementation) By 苏剑林 | May 18, 2018 Last year, I wrote a blog post \"CRF In A Nutshell\" , which introduced the Conditional Random Field (CRF) model in a somewhat rough manner. However, that article clearly had many deficiencies, such as being insufficiently clear and incomplete, and lacking an implementation. Here, we revisit this model to supplement and complete the relevant content. This article provides a concise introduction to the basic principles of CRF. Of course, \"concise\" is relative; to truly understand CRF, it is inevitable to mention some formulas. Readers who only care about usage can skip directly to the end of the text. Illustration Following our previous line of thought, let's compare the similarities and differences between standard frame-wise softmax and CRF. Frame-wise softmax CRF is mainly used for sequence labeling problems, which can be simply understood as classifying each frame in a sequence. Since it is classification, it is natural to think of encoding the sequence using CNN or RNN and then connecting a fully connected layer with softmax activation, as shown below"}, {"file": "translation_5570.html", "title": "Kitchens and Wet Markets are actually the Martial Arts World", "content": "← Back to Index Kitchens and Wet Markets are actually the Martial Arts World By 苏剑林 | May 21, 2018 Garlic Steamed Shrimp - Preparation - 20180520 Garlic Steamed Shrimp - Out of the pot - 20180520 I like eating food, but I usually eat with an appreciative eye; I'm not exactly the hardcore \"foodie\" type. What I enjoy more is making food, witnessing the magnificent transformation of ingredients \"from the market to the dinner table.\" I believe this is an essential part of life. Garlic Steamed Shrimp - The Story of Scallions I believe many readers have had such thoughts: we once wanted to be like the protagonists of wuxia novels, \"traveling the four seas, undefeated under heaven,\" or \"creating our own martial arts, becoming a master in our own right.\" Although real life no longer allows us to be so unrestrained, I feel I can find the same sensation in the kitchen. My culinary skills are not high; I just want to say that in the kitchen, you find that sense of creation that comes naturally. You will be intoxicated by it—intoxicated by the process of processing fresh ingredients into delicious dishes. By extension, you might even fall in love with visiting wet markets, because that is the "}, {"file": "translation_5577.html", "title": "The Principle of Minimum Entropy (III): \"Flying Elephant Across the River\" — Sentence Templates and Language Structures", "content": "← Back to Index The Principle of Minimum Entropy (III): \"Flying Elephant Across the River\" — Sentence Templates and Language Structures By 苏剑林 | May 30, 2018 In the previous article, \"The Principle of Minimum Entropy (II): building a lexicon via 'Decisiveness'\" , we used the principle of minimum entropy as a starting point for a series of mathematical derivations, eventually arriving at equations $(2.15)$ and $(2.17)$. These tell us that elements with high mutual information should be merged, as this helps reduce \"learning difficulty.\" Using this principle, we achieved unsupervised lexicon generation through adjacent character mutual information. From characters to words, and from words to phrases, we have been examining whether adjacent elements can be merged into a good \"pattern.\" But why must patterns be adjacent? Of course, they don't have to be. When we learn a language, we learn not only words and phrases but also \"fixed collocations\"—that is, how words are used together reasonably. This is the manifestation of grammar, which is what this article aims to explore, hoping to eventually achieve a degree of unsupervised syntactic analysis. Since we are considering language associ"}, {"file": "translation_5597.html", "title": "NLP Library Based on Minimum Entropy Principle: nlp zero", "content": "← Back to Index NLP Library Based on Minimum Entropy Principle: nlp zero By 苏剑林 | May 31, 2018 I have written several blog posts about the Minimum Entropy principle, dedicated to some foundational work in unsupervised NLP. For the convenience of experimentation, I have encapsulated the algorithms mentioned in those articles into a library for interested readers to test and use. Since it is oriented towards unsupervised NLP scenarios and covers the basic tasks of NLP, it is named nlp zero . Address Github: https://github.com/bojone/nlp-zero Pypi: https://pypi.org/project/nlp-zero/ It can be installed directly via: pip install nlp-zero The entire library is implemented in pure Python with no third-party dependencies, supporting both Python 2.x and 3.x. Usage Default Tokenization The library comes with a built-in dictionary that can be used as a simple tokenization tool. from nlp_zero import *\ntokenizer = Tokenizer()\nprint(' '.join(tokenizer.tokenize(u'今天天气真好'))) The built-in dictionary includes some new words discovered through a new word discovery algorithm and has been manually optimized, so its quality is relatively high. Lexicon Construction Build a lexicon from a large volume of"}, {"file": "translation_5607.html", "title": "A Simple Python Implementation of Gillespie Simulation", "content": "← Back to Index A Simple Python Implementation of Gillespie Simulation By 苏剑林 | June 07, 2018 Due to professional requirements, I needed to perform stochastic simulations of the master equation. Since I couldn't find a suitable Python implementation online, I wrote one myself and am sharing the source code. As for the Gillespie algorithm itself, I won't introduce it here; readers who need it will naturally understand, and those who don't are advised not to bother. Source Code In fact, the basic Gillespie simulation algorithm is very simple and easy to implement. Below is a reference example: import numpy as np\n\nclass System:\n    def __init__(self, num_elements):\n        self.num_elements = num_elements\n        self.reactions = []\n    def add_reaction(self, rate, num_lefts, num_rights):\n        assert len(num_lefts) == self.num_elements\n        assert len(num_rights) == self.num_elements\n        self.reactions.append([rate, np.array(num_lefts), np.array(num_rights)])\n    def evolute(self, max_steps, init_nums):\n        init_nums = np.array(init_nums)\n        ts, nums = [0], [init_nums]\n        for i in range(max_steps):\n            propensities = []\n            for rate, num_lefts, "}, {"file": "translation_5617.html", "title": "Gossip on \"Noise Contrastive Estimation\": The Beauty of the Winding Path", "content": "← Back to Index Gossip on \"Noise Contrastive Estimation\": The Beauty of the Winding Path By 苏剑林 | June 13, 2018 When mentioning Noise Contrastive Estimation, or \"Negative Sampling,\" everyone likely thinks of Word2Vec immediately. In fact, its meaning goes far beyond that. Noise Contrastive Estimation (NCE) is a roundabout yet exceptionally exquisite technique. It allows us to estimate the parameters of a probability distribution when the normalization factor (also known as the partition function) is impossible to calculate directly. In this article, let us appreciate the subtle elegance of NCE's \"winding path.\" Note: Due to different starting points, the \"Noise Contrastive Estimation\" introduced in this article is actually more inclined towards the so-called \"Negative Sampling\" technique, but the two are essentially the same and will not be distinguished here. Origin of the Problem The root of the problem lies in the inextricable exponential probability distributions. Exponential Family Distributions In many problems, exponential family distributions appear. That is, for the probability $p(\\boldsymbol{x})$ of a variable $\\boldsymbol{x}$, we write it as:\n\\begin{equation}\np(\\boldsymb"}, {"file": "translation_5643.html", "title": "The Seemingly Distinct but Spiritually United RNN and ODE: An Introduction to Fancy RNNs", "content": "← Back to Index The Seemingly Distinct but Spiritually United RNN and ODE: An Introduction to Fancy RNNs By 苏剑林 | June 23, 2018 I had initially resolved to stop playing with RNNs, but while thinking last week, I suddenly realized that RNNs actually correspond to the numerical solutions of ODEs (Ordinary Differential Equations). This provided a line of thought for something I have always wanted to do—using deep learning to solve pure mathematical problems. In fact, this is an quite interesting and useful result, so I will introduce it. Incidentally, this article also involves writing your own RNN from scratch, so it can serve as a simple tutorial for writing custom RNN layers . Note: This article is not an introduction to the recent trending paper \" Neural ODEs \" (though there are some connections). RNN Basics What is an RNN? As is well known, RNN stands for \"Recurrent Neural Network.\" Unlike CNNs, RNN refers to a general category of models rather than a single specific model. Simply put, as long as the input is a sequence of vectors $(\\boldsymbol{x}_1, \\boldsymbol{x}_2, \\dots, \\boldsymbol{x}_T)$ and the output is another sequence of vectors $(\\boldsymbol{y}_1, \\boldsymbol{y}_2, \\do"}, {"file": "translation_5655.html", "title": "Optimization Algorithms from a Dynamical Perspective (I): From SGD to Momentum Acceleration", "content": "← Back to Index Optimization Algorithms from a Dynamical Perspective (I): From SGD to Momentum Acceleration By 苏剑林 | June 27, 2018 In this series, we focus on optimization algorithms. The theme of this article is SGD (Stochastic Gradient Descent), including versions with Momentum and Nesterov. Regarding SGD, several questions generally interest us: Why is SGD effective? Is the batch size for SGD better when it is larger? How should the learning rate for SGD be adjusted? How does Momentum accelerate optimization? Why is Nesterov slightly better than Momentum? ... Here, we attempt to analyze SGD from a dynamical perspective to provide some heuristic understandings of the above questions. Gradient Descent Since we want to compare what is better or worse, we need to know what the best looks like—that is, what is our ultimate goal? Analysis of Training Goals Suppose the set of all training samples is $\\boldsymbol{S}$ and the loss measure is $L(\\boldsymbol{x};\\boldsymbol{\\theta})$, where $\\boldsymbol{x}$ represents a single sample and $\\boldsymbol{\\theta}$ represents the optimization parameters. We can then construct the loss function: $$L(\\boldsymbol{\\theta}) = \\frac{1}{|\\boldsymbol{S}|"}, {"file": "translation_5693.html", "title": "From SamplePairing to mixup: Magical Regularization Terms", "content": "← Back to Index From SamplePairing to mixup: Magical Regularization Terms By 苏剑林 | July 07, 2018 SamplePairing and mixup are two image data augmentation techniques that share the same origin. They appear quite unreasonable and are simple to operate, yet the results are very impressive: multiple image classification tasks have shown that they can improve the final accuracy of classification models. Some readers might be confused by a question: why can such \"unreasonable\" data augmentation methods achieve such good results? This article aims to show that while they look like data augmentation methods, they are actually regularization schemes for the model. As the classic line from Stephen Chow's movie From Beijing with Love goes: On the surface, it looks like a hair dryer, but it is actually a razor. Data Augmentation Let's start with data augmentation. Data augmentation refers to the fact that after we apply some simple transformations to original data, their corresponding categories often do not change. Therefore, we can \"create\" more data based on the original data. For example, for a photo of a dog, after operations like horizontal flipping, slight rotation, cropping, or translat"}, {"file": "translation_5716.html", "title": "A Unified Understanding of Generative Models via Variational Inference (VAE, GAN, AAE, ALI)", "content": "← Back to Index A Unified Understanding of Generative Models via Variational Inference (VAE, GAN, AAE, ALI) By 苏剑林 | July 18, 2018 Preface: I have loved pure mathematics since elementary school, and later developed an interest in physics, even studying theoretical physics for a period of time until I gradually entered the field of machine learning after graduating from my undergraduate studies. Therefore, even in the field of machine learning, my research habits still retain the style of mathematics and physics: attempting to understand and derive as much as possible from the fewest principles. This article is one of the results of this philosophy, trying to use variational inference as a starting point to unifiedly understand various models in deep learning, especially the various dazzling GANs. This paper has already been posted to arXiv; those who need to read the original English manuscript can move to \"Variational Inference: A Unified Framework of Generative Models and Some Revelations\" . Below is the introduction to the article. Actually, the Chinese version of the information might be slightly richer than the English version, forgive my clumsy English... Abstract: This artic"}, {"file": "translation_5743.html", "title": "Sentence Similarity Model Based on GRU and AM-Softmax", "content": "← Back to Index Sentence Similarity Model Based on GRU and AM-Softmax By 苏剑林 | July 29, 2018 Friends in the Computer Vision community know that AM-Softmax is a milestone achievement in face recognition. This article explores borrowing techniques from face recognition to build sentence similarity models, while also introducing how to implement various margin losses in Keras. Background Upon closer inspection, sentence similarity and face recognition share many similarities. Existing Practices In the research I've searched, there are primarily two approaches for using deep learning for sentence similarity: the first is to input a pair of sentences and output a 0/1 label representing the degree of similarity, treating it as a binary classification problem. For example, the model in \"Learning Text Similarity with Siamese Recurrent Networks\" looks like this: Treating sentence similarity as a binary classification model This format was also used in this year's PPD \"Magic Mirror Cup.\" The second approach is to input a triplet \"(Sentence A, Sentence similar to A, Sentence dissimilar to A)\" and solve it using triplet loss, as described in \"Applying Deep Learning To Answer Selection: A Study"}, {"file": "translation_5765.html", "title": "“Make Keras a Bit Cooler!”: Ingenious Layers and Fancy Callbacks", "content": "← Back to Index “Make Keras a Bit Cooler!”: Ingenious Layers and Fancy Callbacks By 苏剑林 | August 06, 2018 Keras Accompanying Me Over the Years Reflecting on these past two or three years since entering the field of machine learning, Keras has always been by my side. If I hadn't come across such an easy-to-use framework as Keras when I first fell into this pit, allowing me to quickly implement my ideas, I am not sure if I would have had the perseverance to stick with it. After all, back then the world belonged to the likes of Theano, Pylearn, Caffe, and Torch, which still seem like \"books from heaven\" to me even today. Later, to broaden my horizons, I also spent some time learning TensorFlow and wrote several programs in pure TensorFlow, but no matter what, I still couldn't let go of Keras. As my understanding of Keras deepened—especially after spending a little time researching the source code—I discovered that Keras is not as \"inflexible\" as everyone complains it is. In fact, Keras's exquisite encapsulation allows us to easily implement many complex functions. I increasingly feel that Keras is like a very exquisite work of art, fully reflecting the profound creative prowess of its"}, {"file": "translation_5776.html", "title": "NICE in \"Steady\" Flows: Basic Concepts and Implementation of Flow Models", "content": "← Back to Index NICE in \"Steady\" Flows: Basic Concepts and Implementation of Flow Models By 苏剑林 | August 11, 2018 Preface: Ever since I saw the Glow model on Machine Heart (see \"The Next GAN? OpenAI Proposes Invertible Generative Model Glow\" ), I have been thinking about it constantly. Machine learning models emerge one after another nowadays; I often follow new model trends, but few have moved me as much as the Glow model, giving me that \"this is it\" feeling. What's even more surprising is that this model, which produces such good results, was something I had never heard of before. I've re-read it several times over the past few days, and the more I read, the more interesting it becomes, feeling like it connects many of my previous ideas. Here is a summary of this stage. Background This article mainly introduces and implements \"NICE: Non-linear Independent Components Estimation\" . This paper is one of the foundational works for the Glow model—it could be called the cornerstone of Glow. Difficult Distributions As is well known, mainstream generative models currently include VAEs and GANs. In fact, besides these two, there are also flow-based models (the concept of \"flow\" will be in"}, {"file": "translation_5807.html", "title": "Flow Models Series: RealNVP and Glow—Inheritance and Sublimation of Flow Models", "content": "← Back to Index Flow Models Series: RealNVP and Glow—Inheritance and Sublimation of Flow Models By 苏剑林 | August 26, 2018 Opening Remarks In the previous article, \"Flow Models Series: NICE—Basic Concepts and Implementation\" , we introduced the pioneering work of flow models: the NICE model. From NICE, we learned the basic concepts and ideas of flow models, and finally, I provided a Keras implementation of NICE. In this article, we will focus on the upgrades to NICE: RealNVP and Glow. Your browser does not support video Sampling demonstration of the Glow model (captured from the Glow official blog) The Ingenious Flow It must be said that the flow model is a very ingeniously designed model. Overall, flow aims to find an encoder that encodes the input $\\boldsymbol{x}$ into a latent variable $\\boldsymbol{z}$ such that $\\boldsymbol{z}$ follows a standard normal distribution. Thanks to the clever design of flow models, this encoder is invertible, allowing us to immediately write down the corresponding decoder (generator). Thus, once the encoder is trained, we simultaneously obtain the decoder, completing the construction of the generative model. To achieve this, the model must not only be"}, {"file": "translation_5861.html", "title": "Playing with Keras: Automatic Title Generation via seq2seq", "content": "← Back to Index Playing with Keras: Automatic Title Generation via seq2seq By 苏剑林 | September 01, 2018 Although I have claimed to be working in NLP for quite a while now, I had never truly run the classic masterpiece of NLP combined with Deep Learning—seq2seq. Recently, my interest was sparked, and I decided to learn and practice seq2seq, naturally ending with a Keras implementation. There are many things seq2seq can do. I have chosen a relatively simple task: generating titles (in Chinese) based on article content, which can also be understood as a form of automatic summarization. I selected this task primarily because \"article-title\" corpus pairs are relatively easy to find, allowing for quick experimentation. Introduction to seq2seq The so-called seq2seq refers to general sequence-to-sequence transformation tasks, such as machine translation, automatic summarization, etc. The characteristic of such tasks is that the input sequence and the output sequence are not aligned. If they were aligned, we would call it sequence labeling, which is much simpler than seq2seq. Therefore, although sequence labeling tasks can also be understood as sequence-to-sequence transformations, we genera"}, {"file": "translation_5879.html", "title": "“Make Keras Cooler!”: Niche Custom Optimizers", "content": "← Back to Index “Make Keras Cooler!”: Niche Custom Optimizers By 苏剑林 | September 08, 2018 Following up on my previous post \"Make Keras Cooler!\": Ingenious Layers and Fancy Callbacks ... Today, we will look at a niche requirement: custom optimizers. Upon reflection, regardless of the framework used, the need for a custom optimizer is truly a niche among niches. Generally, for most tasks, we can brainlessly use Adam, while \"alchemist\" tuning experts often use SGD to achieve better results. In other words, whether you are a novice or an expert, there is rarely a need to define a custom optimizer. So, what is the value of this article? In some scenarios, it can be quite useful. For instance, by learning how to write optimizers in Keras, you can gain a deeper understanding of algorithms like gradient descent, and you can see firsthand how concise and elegant the Keras source code is. Additionally, sometimes we can implement specific features through custom optimizers, such as rewriting optimizers for simple models (like Word2Vec) to use hard-coded gradients instead of automatic differentiation for speed, or implementing features like \"soft batching.\" Keras Optimizers First, let's look a"}, {"file": "translation_5887.html", "title": "Variational Autoencoders (IV): A One-Stop Clustering Scheme", "content": "← Back to Index Variational Autoencoders (IV): A One-Stop Clustering Scheme By 苏剑林 | September 17, 2018 Since VAEs contain both an encoder and a decoder (generator), and the distribution of latent variables is approximated as a standard normal distribution, a VAE is both a generative model and a feature extractor. In the field of images, because VAE-generated images tend to be blurry, people are usually more interested in the role of the VAE as an image feature extractor. Feature extraction is done to prepare for subsequent tasks, which can include many types, such as classification or clustering. This article focuses on the \"clustering\" task. Generally speaking, using AE or VAE for clustering is done in steps: first, train an ordinary VAE, then obtain the latent variables of the original data, and then apply K-Means or GMM to these latent variables. However, this approach clearly lacks a sense of overall integration, and the choice of clustering method can be perplexing. This article introduces a \"one-stop\" clustering idea based on VAE, which allows us to complete clustering and conditional generation simultaneously in an unsupervised manner. Theory General Framework Recalling the"}, {"file": "translation_5977.html", "title": "Flow Series: f-VAEs — The Marriage of Glow and VAEs", "content": "← Back to Index Flow Series: f-VAEs — The Marriage of Glow and VAEs By 苏剑林 | September 21, 2018 This article is the Chinese version of a paper we posted on arXiv a few days ago. In this paper, we present an approach to combine flow-based models (such as Glow introduced previously) and Variational Autoencoders (VAEs), which we call f-VAEs. Theoretically, it can be proven that f-VAEs constitute a more general framework encompassing both flow models and VAEs. Experiments show that compared to the original Glow model, f-VAEs converge faster and can achieve the same generation performance with a smaller network scale.\n    Original Paper: 《f-VAEs: Improve VAEs with Conditional Flows》 Recently, generative models have received widespread attention. Among them, Variational Autoencoders (VAEs) and Flow models are two types of generative models distinct from Generative Adversarial Networks (GANs), and they have also been extensively studied. However, each has its own advantages and disadvantages. This article attempts to combine them. Two real samples’ linear interpolation achieved by f-VAEs Basics Given the evidence distribution of the dataset as $\\tilde{p}(x)$, the basic idea of a generativ"}, {"file": "translation_6016.html", "title": "Introduction to f-GAN: A Production Workshop for GAN Models", "content": "← Back to Index Introduction to f-GAN: A Production Workshop for GAN Models By 苏剑林 | September 29, 2018 Today, I will introduce a classic piece of work titled f-GAN . In this paper, the authors provide a scheme for constructing general GANs using general $f$-divergences. It is no exaggeration to say that this paper is a \"production workshop\" for GAN models. It generalizes many GAN variants and can inspire us to quickly construct new ones (whether they are valuable is another matter, but theoretically, it is possible). Local Variation The entire article's treatment of $f$-divergence is actually based on what is known in machine learning as the \"local variational method.\" This is a very classic and useful estimation technique. In fact, most of this article will be spent introducing the results of applying this estimation technique to $f$-divergence. As for GANs, they are simply a basic application of these results. f-Divergence First, let's provide a basic introduction to $f$-divergence. The so-called $f$-divergence is a generalization of the KL divergence: \\begin{equation}\\mathcal{D}_f(P\\Vert Q) = \\int q(x) f\\left(\\frac{p(x)}{q(x)}\\right)dx\\label{eq:f-div}\\end{equation} Note that ac"}, {"file": "translation_6024.html", "title": "Mutual Information in Deep Learning: Unsupervised Feature Extraction", "content": "← Back to Index Mutual Information in Deep Learning: Unsupervised Feature Extraction By 苏剑林 | Oct 02, 2018 KNN samples from random sampling For NLP, mutual information is a very important indicator as it measures the essential correlation between two things. This blog has discussed mutual information several times, and I am quite interested in various articles that utilize it. A few days ago, I saw the recently proposed Deep INFOMAX model on Machine Heart (Jiqi Zhixin), which uses the maximization of mutual information to perform unsupervised learning on images. Naturally, I was intrigued and studied it, leading to this article. The overall structure of this article originates from the original Deep INFOMAX paper, but I have not copied the original model exactly. Instead, I have modified the model according to my own ideas (mainly the prior distribution part) and will provide notes at the corresponding positions. What Are We Doing? Autoencoders Feature extraction is a crucial and fundamental task in unsupervised learning. A common form is training an encoder to map the original dataset into fixed-length vectors. Naturally, our basic requirement for this encoder is: to preserve (as "}, {"file": "translation_6051.html", "title": "Lipschitz Constraint in Deep Learning: Generalization and Generative Models", "content": "← Back to Index Lipschitz Constraint in Deep Learning: Generalization and Generative Models By 苏剑林 | October 07, 2018 Preface: Last year, I wrote an introductory article on WGAN-GP titled \"The Art of Mutual Grumbling: From Zero to WGAN-GP\" , mentioning that a Lipschitz constraint (hereafter referred to as the \"L-constraint\") is added to the discriminator of WGAN through gradient penalty. A few days ago, while pondering, I thought about WGAN again and felt that the gradient penalty in WGAN-GP isn't elegant enough. I also heard that WGAN has difficulties with conditional generation (because random interpolation between different classes can become messy...). So, I wanted to explore whether a new scheme could be devised to add an L-constraint to the discriminator. After a few days of working behind closed doors, I discovered that what I had thought of had already been done by others—it seems there is truly nothing you can imagine that hasn't already been accomplished. It is mainly covered in these two papers: \"Spectral Norm Regularization for Improving the Generalizability of Deep Learning\" and \"Spectral Normalization for Generative Adversarial Networks\" . Therefore, this article will"}, {"file": "translation_6088.html", "title": "Variational Autoencoder = Minimizing Prior Distribution + Maximizing Mutual Information", "content": "← Back to Index Variational Autoencoder = Minimizing Prior Distribution + Maximizing Mutual Information By 苏剑林 | October 10, 2018 This article is very brief. It mainly describes a fact that is very useful and not complex, but I surprisingly only discovered it after such a long time— In the article \"Mutual Information in Deep Learning: Unsupervised Feature Extraction\" , we obtained the final loss for the Deep INFOMAX model through a weighted combination of a prior distribution loss and a mutual information maximization loss. In that article, while the story was told in full, in a sense, it was just a pieced-together loss. This article will prove that the loss can be naturally derived from the Variational Autoencoder (VAE). Process To repeat it once more, the loss that a Variational Autoencoder (VAE) needs to optimize is $$KL(\\tilde{p}(x)p(z|x)\\Vert q(z)q(x|z))$$ Relevant discussions have appeared on this blog many times. VAE contains both an encoder and a decoder. If we only need to encode features, then training a decoder seems redundant. So the focus is on how to remove the decoder. Actually, it's quite simple. Split the VAE loss into two parts: \\begin{equation}\n  \\begin{aligned}\n"}, {"file": "translation_6096.html", "title": "Rethinking the Determinant of Non-Square Matrices", "content": "← Back to Index Rethinking the Determinant of Non-Square Matrices By 苏剑林 | Oct 16, 2018 A few years ago, the author wrote an \"Understanding Matrices\" series based on his own modest understanding, including an article \"Why Do Only Square Matrices Have Determinants?\" which discussed the issue of determinants for non-square matrices. It presented views like \"determinants of non-square matrices are not elegant\" and \"square matrix determinants are sufficient.\" This article revisits this question. First, recall the determinant of a square matrix. Its most important value lies in its geometric meaning: The absolute value of the determinant of an $n$-dimensional square matrix is equal to the hypervolume of the $n$-dimensional solid spanned by its row (or column) vectors. This geometric meaning is the source of all the importance of determinants; related discussions can be found in \"Bits and Pieces of Determinants.\" It is also the basis for our discussion of non-square matrix determinants. Analysis For a square matrix $\\boldsymbol{A}_{n \\times n}$, it can be viewed as a combination of $n$ row vectors or $n$ column vectors. In either case, the absolute value of the determinant equals the hyp"}, {"file": "translation_6110.html", "title": "RSGAN: The \"Turing Test\" Thought in Adversarial Models", "content": "← Back to Index RSGAN: The \"Turing Test\" Thought in Adversarial Models By 苏剑林 | October 22, 2018 Recently, I came across a very meaningful piece of work called \"Relativistic GAN,\" abbreviated as RSGAN, from the paper \"The relativistic discriminator: a key element missing from standard GAN.\" It is said that this paper even received a \"like\" from the GAN founder, Ian Goodfellow. This article proposes using a relative discriminator to replace the original discriminator in standard GANs, making the generator converge faster and the training more stable. Unfortunately, the original paper primarily discusses the results from training and experimental perspectives without a more in-depth analysis, leading many to feel that it is just another GAN training trick. However, in my view, RSGAN possesses a more profound meaning; it could even be seen as having pioneered a new school of GANs. Therefore, I decided to provide a basic introduction to the RSGAN model and the connotations behind it. It should be noted that, although the results are the same, the introduction process in this article has almost no overlap with the original paper. The \"Turing Test\" Thought SGAN SGAN is the Standard GAN. "}, {"file": "translation_6131.html", "title": "In Memory of Jin Yong | May You Soar on Asteroid 10930", "content": "← Back to Index In Memory of Jin Yong | May You Soar on Asteroid 10930 By 苏剑林 | Oct 30, 2018 Master Jin Yong Jin Yong is gone, passing away at the age of 94. While it is true that these elderly masters, whether they are scientists or literati, generally have little output in their later years, a purely rational mind might feel that \"if they are gone, they are gone; there doesn't seem to be any loss.\" However, the reality is that the departure of a master always brings us a sense of sorrowful shock, making us feel as if an era has passed once again. It was so for Hawking, and it is so for Jin Yong. For Old Master Jin, an era of Wuxia has passed; a \"Jianghu\" has passed. Flying snow shoots the white deer in the sky, the laughing hero leans on the green phoenix under the shimmering water. This couplet describes 14 of Jin Yong's works, which, along with Sword of the Yue Maiden , constitute his 15 Wuxia novels. Using these 15 novels, Jin Yong described vivid and lifelike \"Jianghu\"—no, even saying Jianghu seems too small. After reading these 15 works, you feel that he has described thousands of years of Chinese history and the entirety of society. I wouldn't call myself a die-hard Jin Yon"}, {"file": "translation_6139.html", "title": "WGAN-div: An Obscure WGAN Gap-Filler", "content": "← Back to Index WGAN-div: An Obscure WGAN Gap-Filler By 苏剑林 | November 07, 2018 Today we will talk about Wasserstein Divergence, abbreviated as \"W-divergence.\" Note that this is different from Wasserstein distance (W-distance, also known as the Wasserstein metric). This article originates from the paper \"Wasserstein Divergence for GANs\" , which proposes a GAN training scheme called WGAN-div. This is a paper I greatly admire, yet it remains obscure; I only stumbled upon it while searching through literature. It doesn't seem to have gained much popularity in either English or Chinese circles, but I feel it presents a very elegant result. If readers need an introduction to WGAN-related knowledge, please feel free to read my humble work \"The Art of Mutual Confrontation: WGAN-GP from Scratch\" . WGAN We know that the original GAN (SGAN) may suffer from the problem of vanishing gradients, which is why WGAN was introduced. W-Distance WGAN introduces the W-distance from optimal transport to measure the distance between two distributions: \\begin{equation}W_c[\\tilde{p}(x), q(x)] = \\inf_{\\gamma\\in \\Pi(\\tilde{p}(x), q(x))} \\mathbb{E}_{(x,y)\\sim \\gamma}[c(x,y)] \\end{equation} Here $\\tilde{p}(x)$"}, {"file": "translation_6158.html", "title": "Another Sichuan Dish! \"Gua Yan Sui Du,\" Comparable to \"Steamed Chinese Cabbage in Supreme Soup\"", "content": "← Back to Index Another Sichuan Dish! \"Gua Yan Sui Du,\" Comparable to \"Steamed Chinese Cabbage in Supreme Soup\" By 苏剑林 | Nov 15, 2018 Steamed Chinese Cabbage in Supreme Soup (Kai Shui Bai Cai) is a very classic Sichuan masterpiece, a dish of state-banquet caliber. I previously wrote a popular science piece \"Not Seeking Delicacies, Only Wishing for Boiled Cabbage\" to introduce it. There are many delicious things, but what I remember most about Kai Shui Bai Cai is its pursuit of the ultimate refinement—that kind of restrained elegance where the brilliance remains hidden. While browsing videos just now, I discovered another similar dish: Gua Yan Sui Du (Winter Melon \"Bird's Nest\" and Tripe \"Wheat Ears\"). It is also a Sichuan dish—using pork stomach kernels cut into the shape of wheat ears and winter melon crafted into the shape of swallow's nests, paired with a top-tier clear broth identical to that of Kai Shui Bai Cai. Screenshot of \"Gua Yan Sui Du\" (There are no high-definition images; I took this directly from the video below) In fact, just like Kai Shui Bai Cai, the primary effort of this dish lies in \"clearing the broth\" ( diào tāng ). If you can clear the broth until it has the "}, {"file": "translation_6163.html", "title": "A GAN without Lipschitz constraints and without vanishing gradients, interested?", "content": "← Back to Index A GAN without Lipschitz constraints and without vanishing gradients, interested? By 苏剑林 | November 20, 2018 I don't know when it started, but I found that I have also fallen into the huge pit of GANs. Sigh, I'll try to jump out as soon as possible... This blog post introduces a new GAN framework that I recently submitted to arXiv. It primarily presents a new understanding of probability divergence and derives a new GAN based on this understanding. The entire article is theoretical, providing a complete demonstration of the relevant properties of this GAN. I believe it is a theoretically complete result. Article link: https://papers.cool/arxiv/1811.07296 Main conclusions first: 1. The paper provides a direct approach for analyzing and constructing probability divergences, thereby simplifying the process of building new GAN frameworks. 2. A GAN framework called GAN-QP is derived in \\eqref{eq:gan-gp-gd}. This GAN does not require Lipschitz constraints like WGAN, nor does it suffer from the vanishing gradient problem of SGAN. Experiments show that its performance is at least as good as, and sometimes superior to, WGAN. GAN-QP effect image The paper's experiments scaled "}, {"file": "translation_6181.html", "title": "From Variational Encoding and Information Bottleneck to Normal Distribution: On the Importance of Forgetting", "content": "← Back to Index From Variational Encoding and Information Bottleneck to Normal Distribution: On the Importance of Forgetting By 苏剑林 | November 27, 2018 This is an \"essay\" where we will discuss three things that are inextricably linked: Variational Autoencoders (VAE), Information Bottleneck (IB), and the Normal Distribution. As is well known, the Variational Autoencoder is a classic generative model, but it actually carries implications that go beyond generative modeling. Regarding Information Bottleneck, people might be relatively less familiar with it, although it did attract quite a bit of attention last year. As for the Normal Distribution, it goes without saying—it is connected to almost every field of machine learning to some extent. So, what story is there to tell when these three collide? And what do they have to do with \"forgetting\"? Variational Autoencoder You can find several articles introducing VAE on this blog. Below is a brief review. Review of Theoretical Form Simply put, the optimization objective of VAE is: \\begin{equation}KL(\\tilde{p}(x)p(z|x)\\Vert q(z)q(x|z))=\\iint \\tilde{p}(x)p(z|x)\\log \\frac{\\tilde{p}(x)p(z|x)}{q(x|z)q(z)} dzdx\\end{equation} where $q(z)$ is the"}, {"file": "translation_6191.html", "title": "Minimum Entropy Principle (IV): \"Birds of a Feather\" — From Libraries to Word Vectors", "content": "← Back to Index Minimum Entropy Principle (IV): \"Birds of a Feather\" — From Libraries to Word Vectors By 苏剑林 | December 02, 2018 Reading from the first article to this point, we know that the so-called \"Minimum Entropy Principle\" is dedicated to reducing learning costs, attempting to complete the same task with the minimum possible cost. Therefore, this entire series can be seen as a \"Laziness Guide.\" So, what is the secret to being lazy? The answer is \"routines\" (patterns), which is why this series is also called the \"Manual of Routines.\" In this article, we will introduce the \"routines\" found in libraries. First, let me pose a question: When did word vectors first appear? Was it Mikolov's Word2Vec in 2013? Or Bengio's neural language model in 2003? Neither. In fact, word vectors can be traced back thousands of years to those ancient libraries... Entering the Library Are there word vectors in a library? From a thousand years ago? In which book? Should I go and borrow it? The Routine of Placing Books In fact, it's not about a specific book, but the routine of how books are placed. Obviously, the arrangement of books in a library follows a \"routine\": they are not placed randomly but"}, {"file": "translation_6214.html", "title": "BiGAN-QP: A Simple and Clear Encoding & Generative Model", "content": "← Back to Index BiGAN-QP: A Simple and Clear Encoding & Generative Model By 苏剑林 | December 10, 2018 Not long ago, by directly analyzing in the dual space, I proposed an adversarial model framework called GAN-QP. Its characteristic is that it can be theoretically proven to neither suffer from vanishing gradients nor require Lipschitz (L) constraints, thus simplifying the construction and training of generative models. GAN-QP is an adversarial framework, so in theory, all original GAN tasks can be attempted within it. In the previous article \"A GAN that requires no L-constraint and does not suffer from vanishing gradients, want to know more?\" , we only tried the standard random generation task. In this article, we will experiment with a case that includes both a generator and an encoder: BiGAN-QP. BiGAN and BiGAN-QP Note that this is BiGAN, not the recently popular BigGAN. BiGAN is Bidirectional GAN, proposed in the paper \"Adversarial feature learning\" . At the same time, there was a very similar paper titled \"Adversarially Learned Inference\" , which proposed a model called ALI, which is essentially the same as BiGAN. In general, they both add an encoder to the ordinary GAN model, so"}, {"file": "translation_6234.html", "title": "Optimizing Algorithms from a Dynamical Perspective (II): Adaptive Learning Rate Algorithms", "content": "← Back to Index Optimizing Algorithms from a Dynamical Perspective (II): Adaptive Learning Rate Algorithms By 苏剑林 | December 20, 2018 In \"Optimizing Algorithms from a Dynamical Perspective (I): From SGD to Momentum Acceleration,\" we proposed that the SGD optimization algorithm actually corresponds to the numerical solution of Ordinary Differential Equations (ODEs). From this perspective, we can naturally analyze the convergence properties of the SGD algorithm, the principles of momentum acceleration, and other content. In this article, we continue along this line of thought to understand adaptive learning rate algorithms among optimization methods. RMSprop First, let's look at a classic adaptive learning rate optimization algorithm: RMSprop. Although RMSprop was not the first adaptive learning rate optimization algorithm proposed, it is a highly practical one. it serves as the foundation for more comprehensive algorithms like Adam. Through it, we can observe how adaptive learning rate optimization algorithms work. Algorithm Overview General gradient descent is as follows:\n    \\begin{equation}\\boldsymbol{\\theta}_{n+1}=\\boldsymbol{\\theta}_{n} - \\gamma \\nabla_{\\boldsymbol{\\theta}} L(\\"}, {"file": "translation_6240.html", "title": "【Learning List】Recently Important GAN Progress Papers", "content": "← Back to Index 【Learning List】Recently Important GAN Progress Papers By 苏剑林 | December 26, 2018 This article briefly lists what I consider to be important recent GAN progress papers. This basically serves as the primary research list I used while studying GANs. The Taste of Generative Models GAN is a deep pit, especially for an amateur player like me; diving in for a long time makes it difficult to produce any output, particularly as various large companies use massive computational power to create one giant model after another, making it nearly impossible for individuals to compete. However, I always feel that only by touching generative models does one feel they have encountered true machine learning. This is true whether in image or text processing. Therefore, I am still willing to pay attention to generative models. Of course, GAN is not the only choice for generative models, but it is a very interesting one. In images, there are at least GAN, flow, and pixelrnn/pixelcnn as options, but in terms of potential, I still feel GAN is the most promising—not just because of the results, but mainly because of its adversarial philosophy. In text, the seq2seq mechanism is actually alrea"}, {"file": "translation_6257.html", "title": "2019 Annual Astronomy Calendar", "content": "← Back to Index 2019 Annual Astronomy Calendar By 苏剑林 | January 01, 2019 Astronomy Calendar of Celestial Events 2019 Annual Astronomy Calendar Translated from NASA: http://eclipse.gsfc.nasa.gov/SKYCAL/SKYCAL.html (Beijing Time) 2011 Version 2012 Version 2013 Version 2014 Version 2015 Version 2016 Version 2017 Version 2018 Version Month Day Day-of-Week Time Event January 01 Tue Venus: $46.9^\\circ$ W 02 Wed 05:50:00 Venus in Conjunction with Moon: $1.4^\\circ$ S 02 Wed 12:53:00 Saturn in Conjunction with Sun 03 Thu 15:37:00 Jupiter in Conjunction with Moon: $3.4^\\circ$ S 03 Thu 17:59:00 Earth at Perihelion: 0.9833 AU 04 Fri 10:28:00 Quadrantids Meteor Shower: ZHR = 120 06 Sun 02:46:00 Moon at Southern Declination: $21.6^\\circ$ S 06 Sun 09:28:00 New Moon 06 Sun 09:41:00 Partial Solar Eclipse 06 Sun 12:59:00 Venus at Greatest Elongation: $47^\\circ$ W 07 Mon 08:08:00 Moon at Descending Node 09 Wed 12:29:00 Moon at Apogee: 406,100 km 14 Mon 14:46:00 First Quarter Moon 18 Fri 02:20:00 Aldebaran in Conjunction with Moon: $1.6^\\circ$ S 20 Sun 07:20:00 Moon at Northern Declination: $21.5^\\circ$ N 21 Mon 06:48:00 Moon at Ascending Node 21 Mon 13:12:00 Total Lunar Eclipse 21 Mon 13:16:00 Full M"}, {"file": "translation_6261.html", "title": "Optimization Algorithms from the Perspective of Dynamics (III): A More Holistic View", "content": "← Back to Index Optimization Algorithms from the Perspective of Dynamics (III): A More Holistic View By 苏剑林 | January 08, 2019 Lately, I've been increasingly excited about combining optimization algorithms with dynamics. This is the third installment in the series on optimization algorithms and dynamics, and I have a premonition there will be a fourth—stay tuned! A brief recap of the story so far: In the first article , we pointed out that SGD is essentially a numerical method for solving ordinary differential equations (ODEs): the Euler method. In the second article , from the perspective of error analysis in numerical methods, we analyzed why the gradient can be used to adjust the learning rate, thereby explaining the principle behind using gradients to regulate learning rates in algorithms like RMSprop and Adam. This article will provide a more unified viewpoint on these two matters and attempt to answer a more fundamental question: Why gradient descent? (Note: The discussion in this article does not involve the momentum acceleration component.) Gradient Descent Revisited The previous two articles discussed the viewpoint that \"gradient descent is equivalent to solving an ODE,\" b"}, {"file": "translation_6270.html", "title": "Couplet Robot Based on CNN and Sequence Labeling", "content": "← Back to Index Couplet Robot Based on CNN and Sequence Labeling By 苏剑林 | January 14, 2019 Origin # A few days ago, I saw an article titled \"This Brain-Twisting Couplet AI is Driving Everyone Crazy\" on the Liangziju (Quantum Bit) WeChat official account. I found it quite interesting, and more importantly, the author organized and released the dataset, so I decided to try it myself. Action # \"Writing couplets\" can be viewed as a sentence generation task, which can be completed using seq2seq, similar to my previous post \"Playing with Keras: seq2seq Automatic Title Generation,\" just with a slight modification to the input. The method used in the aforementioned article was also seq2seq, which seems to be the standard approach. Analysis # However, if we think about it further, we will find that compared to general sentence generation tasks, \"writing couplets\" is much more regular: 1. The number of characters in the upper and lower scrolls is the same; 2. There is a character-to-character correspondence between almost every character in the upper and lower scrolls. In this way, writing couplets can be directly treated as a sequence labeling task, following the same approach as word segme"}, {"file": "translation_6280.html", "title": "From Wasserstein Distance and Duality Theory to WGAN", "content": "← Back to Index From Wasserstein Distance and Duality Theory to WGAN By 苏剑林 | January 20, 2019 Which bulldozer is the strongest? For the lowest cost, look to Wasserstein. In 2017, I wrote a blog post \"The Art of Mutual Confrontation: Direct to WGAN-GP from Zero\" , which introduced WGAN from a relatively intuitive perspective. In that article, WGAN seemed more like a stroke of inspiration, and in truth, it didn't have a very deep connection to the actual Wasserstein distance definition in that explanation. In this article, we will revisit WGAN from a more mathematical perspective. Of course, this post isn't purely about GANs; it primarily focuses on the understanding of the Wasserstein distance and its duality theory. This article is inspired by the famous blog post \"Wasserstein GAN and the Kantorovich-Rubinstein Duality\" . The content is largely similar, but I have removed some redundant parts and supplemented areas that were insufficient or ambiguous. Regardless, I would first like to express my respect to the predecessors and their work. ( Note: To fully understand this article, you will likely need fundamental knowledge of multivariable calculus, probability theory, and linear a"}, {"file": "translation_6311.html", "title": "Making Keras Even Cooler: Custom Losses, Metrics, and More", "content": "← Back to Index Making Keras Even Cooler: Custom Losses, Metrics, and More By 苏剑林 | January 27, 2019 Continuing the \"Making Keras Cooler!\" series, let's make Keras even more interesting. This time, we will focus on Keras losses, metrics, weights, and progress bars. This kind of model is a standard input-output structure, where the loss is a calculation based on the output. However, for more complex models such as Autoencoders, GANs, and Seq2Seq, this approach is sometimes inconvenient, as the loss is not always just a function of the final output. Fortunately, newer versions of Keras already support more flexible loss definitions. For example, we can write an Autoencoder like this: x_in = Input(shape=(784,))\nx = x_in\nx = Dense(100, activation='relu')(x)\nx = Dense(784, activation='sigmoid')(x)\nmodel = Model(x_in, x)\n\nloss = K.mean((x - x_in)**2)\nmodel.add_loss(loss)\nmodel.compile(optimizer='adam')\nmodel.fit(x_train, None, epochs=5) The characteristics of the above approach are: 1. When calling compile , no loss is passed. Instead, the loss is defined in another way before compile and added to the model via add_loss . This allows for writing sufficiently flexible losses—for instance,"}, {"file": "translation_6316.html", "title": "GAN from an Energy Perspective (1): GAN = \"Digging Pits\" + \"Jumping into Pits\"", "content": "← Back to Index GAN from an Energy Perspective (1): GAN = \"Digging Pits\" + \"Jumping into Pits\" By 苏剑林 | January 30, 2019 \"Look at those digging pits, what's so different about them~\" In this series, we attempt to understand GANs from the perspective of energy. We will find this perspective so beautiful and intuitive that it is truly admirable. This perspective is directly inspired by the Bengio team's new work \"Maximum Entropy Generators for Energy-Based Models\" , which appeared on arXiv a few days ago. Of course, the connection between energy models and GANs has a long history and is not an original creation of this article; however, this paper makes the connection more meticulous and complete. Additionally, this article supplements some of my own understanding and reflections, striving to be more accessible and comprehensive. As the first article, let's start with a straightforward analogical derivation: GAN is actually a journey of \"digging pits\" and \"jumping into pits\" one after another (or digging then jumping?). In general, the main content of this article is as follows: Provides a clear and intuitive energy image of GAN/WGAN; Discusses the training conditions and strategies "}, {"file": "translation_6331.html", "title": "GAN from an Energy Perspective (II): GAN = \"Analysis\" + \"Sampling\"", "content": "← Back to Index GAN from an Energy Perspective (II): GAN = \"Analysis\" + \"Sampling\" By 苏剑林 | February 15, 2019 In this series, we attempt to understand GANs from the perspective of energy. We will find this perspective to be so beautiful and intuitive that it is truly remarkable. In the previous article, we provided a straightforward and powerful energy landscape. This landscape allowed us to easily understand many aspects of GANs; in other words, a popular explanation was sufficient to achieve most of the understanding, and the final conclusions were already laid out. In this article, we continue to understand GANs from the energy perspective, aiming this time to derive the previous simple and blunt descriptions using relatively rigorous mathematical language. As with the first article, this derivation process is directly inspired by the new work from Bengio's team: \"Maximum Entropy Generators for Energy-Based Models\" . Original author's open-source implementation: https://github.com/ritheshkumar95/energy_based_generative_models The main contents of this article are as follows: 1. Deriving the update formulas for the adversarial interaction between positive and negative phases unde"}, {"file": "translation_6377.html", "title": "Appreciation of the Identity \\( \\det(\\exp(\\boldsymbol{A})) = \\exp(\\text{Tr}(\\boldsymbol{A})) \\)", "content": "← Back to Index Appreciation of the Identity \\( \\det(\\exp(\\boldsymbol{A})) = \\exp(\\text{Tr}(\\boldsymbol{A})) \\) By 苏剑林 | February 18, 2019 The theme of this article is an interesting identity regarding the determinant of a matrix: \\begin{equation}\\det(\\exp(\\boldsymbol{A})) = \\exp(\\text{Tr}(\\boldsymbol{A}))\\label{eq:main}\\end{equation} This identity appears in many mathematical and physical calculations; the author has encountered it several times in different literatures. Note that the left-hand side involves the exponential of a matrix followed by the calculation of its determinant—both of which are computationally intensive operations. The right-hand side is simply the trace of the matrix (a scalar), followed by a scalar exponential. The computational cost between the two sides differs by an immense factor, yet they are surprisingly equal! This is undoubtedly a fascinating fact. Therefore, this article aims to appreciate this identity thoroughly. Matrix Exponential To appreciate this identity, some preparatory work is required. First, how should we understand \\( \\exp(\\boldsymbol{A}) \\)? Generally, it is defined according to the standard Taylor series expansion of \\( e^x \\): \\begi"}, {"file": "translation_6387.html", "title": "Cleverly Stopping Gradients: Implementing GAN Models with a Single Loss", "content": "← Back to Index Cleverly Stopping Gradients: Implementing GAN Models with a Single Loss By 苏剑林 | February 22, 2019 We know that for ordinary models, we simply build the architecture, define the loss, and throw it to the optimizer for training. However, GANs are different; generally, they involve two different losses that need to be optimized alternately. The current mainstream approach is to train the discriminator and the generator in a 1:1 alternating frequency (each trained once, with different learning rates aka TTUR if necessary). Alternating optimization means we need to pass data twice (from memory to GPU), and perform forward and backward propagation twice. If we could combine these two steps into one single optimization step, it would definitely save time. This is known as the synchronous training of GANs. (Note: This article is not introducing a new type of GAN, but rather a new way to write GANs. This is a programming problem, not an algorithmic one~) If in TensorFlow If you are using TensorFlow, implementing synchronous training is not difficult because we have already defined the training ops for the discriminator and the generator (let's assume they are D_solver and G"}, {"file": "translation_6394.html", "title": "A Brief Introduction to the Non-Adversarial Generative Model GLANN", "content": "← Back to Index A Brief Introduction to the Non-Adversarial Generative Model GLANN By 苏剑林 | February 26, 2019 A while ago, I saw that Facebook published a non-adversarial generative model called GLANN (posted on arXiv last December), claiming it could generate 1024 high-definition faces using a non-adversarial approach. I read it with great interest and indeed gained some insights, but I was also somewhat disappointed. As for why I was disappointed, you will understand as you read on. Original paper: \"Non-Adversarial Image Synthesis with Generative Latent Nearest Neighbors\" Introduction by Machine Heart (Ji Qi Zhi Xin): \"Why let GAN dominate? Facebook proposes GLANN, a non-adversarial generation method\" Sample results: Below is a simple breakdown of the content related to the GLANN model. Implicit Maximum Likelihood The foundation of the entire GLANN method is \"Implicit Maximum Likelihood Estimation\", from the article \"Implicit Maximum Likelihood Estimation\" , referred to as \"IMLE\". This article was only posted on arXiv last September, which surprised me quite a bit. Because the algorithm is so simple—I had already used it two years ago—I always felt it was an obviously valid metho"}, {"file": "translation_6407.html", "title": "Constructing an Explicit, Always Invertible Matrix", "content": "← Back to Index Constructing an Explicit, Always Invertible Matrix By 苏剑林 | March 1, 2019 From the article \"Appreciation of the Identity det(exp(A)) = exp(Tr(A))\" , we learned that the matrix $\\exp(\\boldsymbol{A})$ is always invertible, and its inverse is $\\exp(-\\boldsymbol{A})$. The problem is that $\\exp(\\boldsymbol{A})$ is only a theoretical definition; simply writing it this way has little practical value because it requires calculating every $\\boldsymbol{A}^n$. Are there any specific examples? Yes. This article will construct an explicit, always invertible matrix. The logic is actually very simple. Suppose $\\boldsymbol{x}, \\boldsymbol{y}$ are two $k$-dimensional column vectors. Then $\\boldsymbol{x}\\boldsymbol{y}^{\\top}$ is a $k \\times k$ matrix. Let's consider: \\begin{equation}\n\\begin{aligned}\n\\exp\\left(\\boldsymbol{x}\\boldsymbol{y}^{\\top}\\right)=&\\sum_{n=0}^{\\\\infty}\\frac{\\left(\\boldsymbol{x}\\boldsymbol{y}^{\\top}\\right)^n}{n!}\\\\\n=&\\boldsymbol{I}+\\boldsymbol{x}\\boldsymbol{y}^{\\top}+\\frac{\\boldsymbol{x}\\boldsymbol{y}^{\\top}\\boldsymbol{x}\\boldsymbol{y}^{\\top}}{2}+\\frac{\\boldsymbol{x}\\boldsymbol{y}^{\\top}\\boldsymbol{x}\\boldsymbol{y}^{\\top}\\boldsymbol{x}\\boldsymbol{y}^{\\top}}{6}+\\do"}, {"file": "translation_6409.html", "title": "O-GAN: A Simple Modification That Turns a GAN Discriminator into an Encoder!", "content": "← Back to Index O-GAN: A Simple Modification That Turns a GAN Discriminator into an Encoder! By 苏剑林 | March 06, 2019 In this post, I would like to share a recent work: by simply modifying the original GAN model, we can turn the discriminator into an encoder. This allows the GAN to simultaneously possess generation and encoding capabilities with almost no increase in training cost. This new model is called O-GAN (Orthogonal GAN) because it is based on the orthogonal decomposition of the discriminator's degrees of freedom, representing the most thorough utilization of the discriminator's capacity. Arxiv Link: https://papers.cool/arxiv/1903.01931 Open Source Code: https://github.com/bojone/o-gan Background I have been immersed in the field of generative models for a long time now. Not only have I written several blog posts about generative models, but I have also submitted a few small papers related to generative models to Arxiv. Since diving into this \"pit,\" although my understanding of generative models—especially GANs—has deepened, and sometimes I felt I made some incremental improvements (hence the Arxiv submissions), in reality, those were minor adjustments of little significance"}, {"file": "translation_6418.html", "title": "\"Make Keras Cooler!\": Layer-wise Learning Rates and Free Gradient Manipulation", "content": "← Back to Index \"Make Keras Cooler!\": Layer-wise Learning Rates and Free Gradient Manipulation By 苏剑林 | March 10, 2019 Raising the banner of \" Make Keras Cooler! \" to unlock the infinite possibilities of Keras~ Today, we will accomplish two very important things with Keras: setting layer-wise learning rates and flexibly manipulating gradients. First is layer-wise learning rates . The utility of this is obvious—for example, when fine-tuning an existing model, sometimes we want to freeze certain layers, but other times we don't want to freeze them entirely; instead, we want them to update with a lower learning rate than other layers. This requirement leads us to layer-wise learning rates. There has been some discussion online about implementing this in Keras, but the conclusions usually suggest rewriting the optimizer. Clearly, that approach is unfriendly in terms of both implementation and usage. Next is gradient manipulation . A direct example of manipulating gradients is gradient clipping, where gradients are kept within a certain range; Keras has this built-in. However, Keras provides global gradient clipping. What if I want to set different clipping methods for each gradient? Or"}, {"file": "translation_6469.html", "title": "Happy Pi Day! || It Turns Out I’ve Been Blogging for Ten Years~", "content": "← Back to Index Happy Pi Day! || It Turns Out I’ve Been Blogging for Ten Years~ By 苏剑林 | March 14, 2019 Today is March 14, which happens to be 3.14—the \"Pi Day\" ( $\\pi$ day) that many science students love to joke about~ Pi Day There are many stories to tell about $\\pi$. Using the \"most beautiful formula\" $e^{i\\pi}+1=0$ to tell the story of $\\pi$ seems a bit too cliché and lacks a bit of technical depth. However, I believe that the series of formulas produced by the \"genius\" mathematician Ramanujan will never go out of style, for example: $$\\sqrt{\\phi +2}-\\phi =\\frac{e^{{-{\\frac{2\\pi }{5}}}}}{1+{\\frac{e^{{-2\\pi }}}{1+{\\frac{e^{{-4\\pi }}}{1+{\\frac{e^{{-6\\pi }}}{1+\\,\\cdots }}}}}}}=0.2840...,\\quad \\phi=\\frac{1+\\sqrt{5}}{2}$$ Look, Euler's formula connecting $e,i,\\pi,1,0$ is great, but mine connects $e, \\pi$, and the golden ratio together in the form of an infinite continued fraction! Another example: $$\\frac{1}{\\pi}=\\frac{2\\sqrt{2}}{99^2}\\sum_{k=0}^{\\infty}\\frac{(4k)!}{(k!)^4}\\frac{1103+26390k}{396^{4k}}$$ Isn't it just a series for $\\pi$? What's the big deal? The big deal is that if you take only the first term of this series, you can get $\\pi=3.1415927...$. The very first term gives"}, {"file": "translation_6482.html", "title": "Narrow Streams of Flow: Invertible ResNet—The Ultimate Brute-Force Aesthetics", "content": "← Back to Index Narrow Streams of Flow: Invertible ResNet—The Ultimate Brute-Force Aesthetics By 苏剑林 | March 21, 2019 Today, we introduce a very \"brute-force\" model: Invertible ResNet. Why can a model be described as \"brute-force\"? Naturally, it’s because it truly is: it synthesizes many mathematical techniques to forcibly (under certain constraints) transform a conventional ResNet model into an invertible one! A comparison diagram between standard ResNet and Invertible ResNet. Invertible ResNet allows for lossless, reversible information flow, whereas standard ResNet exhibits \"collapse\" at certain points. The model comes from the paper \"Invertible Residual Networks\" , which was previously reported by Synced (Machine Heart) . In this article, let’s take a simple look at its principles and content. Bit by Bit on Invertible Models Why study Invertible ResNet models? What are the benefits? Has no one studied them before? Benefits of Invertibility What does invertibility mean? It means it is information-lossless , implying it might be used to create better classification networks, and it can be used to build generative models directly using maximum likelihood. Moreover, thanks to the p"}, {"file": "translation_6508.html", "title": "Scientific Spaces Browsing Guide (FAQ)", "content": "← Back to Index Scientific Spaces Browsing Guide (FAQ) By 苏剑林 | March 26, 2019 In fact, besides writing blog content over the past few years, I have spent a considerable amount of time on the \"surface work\" of Scientific Spaces. To this end, I specifically learned a bit of PHP, CSS, and JS. While I wouldn't claim it's perfect, the overall browsing experience should be much better than in previous years. Considering that some readers might need certain features but may not notice them immediately, I have organized some on-site tips here. Article Section What is the best environment for reading articles? Two years ago, Scientific Spaces adopted a responsive design that automatically adapts to screens of different resolutions. Therefore, the text content should be clear regardless of the resolution. The only issue is that on small-screen mobile phones, formulas may be incomplete or misaligned. For a better experience reading formulas, it is best to use a screen larger than 7 inches. If you must use a small-screen phone, consider reading in landscape mode. Are there PDF versions of the articles? Can they be printed? Articles are not provided in PDF format, nor are $\\LaTeX$ source files"}, {"file": "translation_6534.html", "title": "Sharing: Drawing a 3D 3rd-Order Magic Cube with LaTeX+MathJax", "content": "← Back to Index Sharing: Drawing a 3D 3rd-Order Magic Cube with LaTeX+MathJax By 苏剑林 | March 28, 2019 Yesterday, I noticed a discussion on the Mathematics Development Forum about 3D 3rd-order magic squares . The experts on the forum had already discussed it quite thoroughly, so there wasn't much for me to add. Then, I suddenly had a whim: could I draw such a 3D magic square using pure LaTeX? I spent quite a while yesterday afternoon tinkering with it and only managed a half-finished product. However, after further refinement by the forum expert mathe , it was finally successfully drawn: \\[\n\\begin{array}{ccccccccccc}\n& & & & 4 & —& —& — & — & 25 & —& —& — & — & 11\n\\\\\n& & & \\require{HTML} \\style{display: inline-block; transform: rotate(45deg)}{|} &\\require{HTML} \\style{display: inline-block; opacity:0.5;}{\\color{red}{\\vdots}} & && &\\require{HTML} \\style{display: inline-block; transform: rotate(45deg)}{|} &\\require{HTML} \\style{display: inline-block; opacity:0.5;}{\\color{red}{\\vdots}} && &&\\require{HTML} \\style{display: inline-block; transform: rotate(45deg)}{|} &|\n\\\\\n& & 14 & — & — & —& — & 22 & — & — & — & —& 7 & & |\n\\\\\n& \\require{HTML} \\style{display: inline-block; transform: rotat"}, {"file": "translation_6540.html", "title": "Sharing an Unsupervised Mining of Professional Domain Vocabulary", "content": "← Back to Index Sharing an Unsupervised Mining of Professional Domain Vocabulary By 苏剑林 | April 10, 2019 Last year, Data Fountain hosted a competition called \" Power Professional Domain Vocabulary Mining .\" The interesting thing about this competition was that it was \"unsupervised,\" meaning it tested the ability to mine professional vocabulary from a large amount of corpus data without labels. This is clearly a valuable capability in the industrial world. Since I had previously done some research on unsupervised new word discovery and was intrigued by the novelty of an \"unsupervised competition,\" I participated without hesitation. However, my final ranking wasn't particularly high. Regardless, I'd like to share my approach. This is a truly unsupervised method that might be of some reference value to some readers. Baseline Comparison First, for the new word discovery part, I used my own library, nlp zero . The basic idea is to perform new word discovery separately on the \"corpus provided by the competition\" and a \"corpus of Baidu Baike data I crawled myself.\" By comparing the two, I could identify characteristic words specific to the \"competition-provided corpus.\" The reference sour"}, {"file": "translation_6549.html", "title": "The Evolution of GAN Architectures", "content": "← Back to Index The Evolution of GAN Architectures By 苏剑林 | April 19, 2019 In fact, the discovery of O-GAN has reached my ideal pursuit for GANs, allowing me to comfortably leap out of the deep pit of GAN research. Therefore, I will now attempt to explore broader and more diverse research directions, such as tasks in NLP that haven't been done yet, Graph Neural Networks, or other interesting things. However, before that, I want to record my previous learning results regarding GANs. In this article, let's comb through the development of GAN architectures—primarily the development of generators, as discriminators haven't changed much over time. Also, this article introduces the architectural development of GANs in the field of images and has nothing to do with SeqGAN in NLP. Furthermore, this article will not repeat basic GAN introductions. A Word Up Front Of course, in a broad sense, any progress in classification models in the image domain can be considered progress for the discriminator (since they are both classifiers, related technologies can be applied to the discriminator). Since image classification models essentially haven't undergone qualitative changes since ResNet, this a"}, {"file": "translation_6575.html", "title": "“Make Keras a Bit Cooler!”: Intermediate Variables, Weight Averaging, and Safe Generators", "content": "← Back to Index “Make Keras a Bit Cooler!”: Intermediate Variables, Weight Averaging, and Safe Generators By 苏剑林 | April 28, 2019 By Su Jianlin | 2019-04-28 Continuing our journey to “Make Keras a Bit Cooler.” Today, we will implement the flexible output of arbitrary intermediate variables using Keras, perform seamless weight moving averaging, and finally, introduce the process-safe way to write generators. First is outputting intermediate variables . When customizing layers, we may want to inspect intermediate variables. Some of these requirements are relatively easy to implement, such as viewing the output of a specific layer—one simply needs to save the part of the model up to that layer as a new model. However, some requirements are more difficult, such as when using an Attention layer, where we might want to view the values of the Attention matrix; using the method of building a new model would be very cumbersome. This article provides a simple method to satisfy this requirement completely. Next is weight moving average . Weight moving average is an effective method for stabilizing and accelerating model training and even improving model performance. Many large-scale models (e"}, {"file": "translation_6583.html", "title": "Optimization Algorithms from a Dynamic Perspective (IV): The Third Stage of GAN", "content": "← Back to Index Optimization Algorithms from a Dynamic Perspective (IV): The Third Stage of GAN By 苏剑林 | May 3, 2019 In the process of learning and reflecting on GANs, I found that I have not only learned an effective generative model but that it has also comprehensively promoted my understanding of various aspects of models, such as optimization and perspectives of interpretation, the significance of regularization terms, the connection between loss functions and probability distributions, probabilistic inference, and so on. GAN is not merely a \"toy for making fakes\"; it is a probabilistic model and inference method with profound significance. As an ex-post summary, I feel that our understanding of GANs can be roughly divided into three stages: 1. Sample Stage : In this stage, we understand the \"Discriminator-Generator\" (or Counterfeiter) interpretation of GANs. We know how to write basic GAN formulas based on this principle (such as the original GAN, LSGAN), including the losses for the discriminator and generator, and can complete the training of simple GANs. At the same time, we know GANs have the ability to make images \"more realistic\" and can utilize this property to embed GA"}, {"file": "translation_6612.html", "title": "GAN Models from the Perspective of Energy (Part 3): Generative Model = Energy Model", "content": "← Back to Index GAN Models from the Perspective of Energy (Part 3): Generative Model = Energy Model By 苏剑林 | May 10, 2019 Conditional generation results on ImageNet (128x128) using the model discussed in this article. The results to be introduced today are still related to energy-based models, originating from the paper \"Implicit Generation and Generalization in Energy-Based Models\" . Of course, it is no longer strictly related to GANs, but since it shares a strong connection with the energy models introduced in the second post of this series , I decided to include it in this series as well. I originally noticed this paper due to a report from JiQiZhiXin titled \"MIT Undergraduate God Reboots Energy-Based Generative Models; New Framework Rivals GANs\" . To be honest, the article itself might feel redundant to some—it’s essentially a \"reinvention of the wheel.\" The media headline was somewhat accurate in using the word \"reboot.\" The paper points out that an energy-based model is essentially the stationary solution of a specific Langevin equation, and then uses that Langevin equation to perform sampling. With a sampling process in place, the training of the energy model can be complete"}, {"file": "translation_6620.html", "title": "Miscellaneous Talk on Function Smoothing: Differentiable Approximations of Non-differentiable Functions", "content": "← Back to Index Miscellaneous Talk on Function Smoothing: Differentiable Approximations of Non-differentiable Functions By 苏剑林 | May 20, 2019 Generally speaking, neural networks deal with continuous floating-point numbers, and standard outputs are also continuous values. However, in practical problems, we often need discrete results. For example, in classification problems, we want to output a correct category—the \"category\" is discrete, while the \"category probability\" is continuous. Similarly, the evaluation metrics for many tasks are actually discrete, such as accuracy and F1 score for classification, BLEU for machine translation, and so on. Taking classification as an example again: the common evaluation metric is accuracy, while the common loss function is cross-entropy. Although a decrease in cross-entropy is indeed correlated with an increase in accuracy, they do not have an absolutely monotonic relationship. In other words, if cross-entropy decreases, accuracy does not necessarily increase. Obviously, it would be ideal to use the negative of the accuracy as the loss function, but accuracy is non-differentiable (involving operations like $\\text{argmax}$), so it cannot be use"}, {"file": "translation_6621.html", "title": "ON-LSTM: Expressing Hierarchical Structure with Ordered Neurons", "content": "← Back to Index ON-LSTM: Expressing Hierarchical Structure with Ordered Neurons By 苏剑林 | May 28, 2019 Today, I will introduce an interesting LSTM variant: ON-LSTM, where \"ON\" stands for \"Ordered Neurons.\" In other words, the neurons within this LSTM are sorted in a specific order, allowing them to express richer information. ON-LSTM comes from the paper \"Ordered Neurons: Integrating Tree Structures into Recurrent Neural Networks.\" As the name suggests, sorting neurons in a specific way is intended to integrate hierarchical structures (tree structures) into the LSTM, thereby allowing the LSTM to automatically learn hierarchical information. This paper also has another identity: it was one of the two Best Paper award winners at ICLR 2019. This indicates that integrating hierarchical structures into neural networks (rather than purely simple omnidirectional connections) is a subject of common interest for many scholars. I noticed ON-LSTM because of an introduction by Heart of the Machine (JiQiZhiXin) , which mentioned that in addition to improving language model performance, it can even learn the syntactic structure of sentences in an unsupervised manner! It was this particular featur"}, {"file": "translation_6671.html", "title": "Lightweight Information Extraction Model Based on DGCNN and Probabilistic Graphs", "content": "← Back to Index Lightweight Information Extraction Model Based on DGCNN and Probabilistic Graphs By 苏剑林 | June 03, 2019 Background: A few months ago, Baidu held the \" 2019 Language and Intelligence Technology Competition ,\" which consisted of three tracks. I was quite interested in the \"Information Extraction\" track, so I signed up. After two months of intense work, the competition has finally concluded, and the final results have been announced. Starting from a total lack of knowledge about Information Extraction, I explored some experiences in supervised learning for information extraction through study and research during this competition, which I would like to share here. I ranked 7th on the final test set, with an F1 score of 0.8807 (Precision 0.8939, Recall 0.8679), about 0.01 behind the first place. From a competition perspective, this result is not exceptionally outstanding, but I believe the model has several innovative features, such as a self-designed extraction structure, CNN+Attention (making it fast enough), and the absence of pre-trained models like BERT. I believe this has certain reference value for both academic research and engineering applications of information"}, {"file": "translation_6704.html", "title": "Dragon Boat Festival & Gaokao Rambles: What we miss might just be nostalgia itself", "content": "← Back to Index Dragon Boat Festival & Gaokao Rambles: What we miss might just be nostalgia itself By 苏剑林 | June 07, 2019 Today is the Dragon Boat Festival; I wish everyone smooth sailing in all their endeavors. Additionally, today is also the first day of the Gaokao (National College Entrance Examination); again, I wish everyone the best of luck. On such festivals or special days, it is always possible to evoke many memories and produce much nostalgia. Yesterday, I also posted this message on my QQ Space and WeChat Moments: Thinking back to this day years ago, I observed the Transit of Venus. If you haven't seen it yet, I'm sorry, you'll have to wait another 98 years. The \"this day years ago\" I mentioned yesterday refers to June 6, 2012, which happened to be the \"twice-in-a-century\" Transit of Venus. The time before that was June 8, 2004, and the next one won't be until December 11, 2117—truly out of reach. Of course, besides being rare, the Transit of Venus isn't particularly spectacular. It works on the same principle as a solar eclipse: during the day, the Sun is partially blocked by a celestial body. In a solar eclipse, the Sun is blocked by the Moon and is basically clearly v"}, {"file": "translation_6705.html", "title": "Discussion on Reparameterization: From Normal Distribution to Gumbel Softmax", "content": "← Back to Index Discussion on Reparameterization: From Normal Distribution to Gumbel Softmax By 苏剑林 | June 10, 2019 Recently, when using VAE to handle some text problems, I encountered the issue of calculating the expectation of a posterior distribution in discrete form. Following the line of thought \"discrete distribution + reparameterization,\" I eventually searched my way to Gumbel Softmax. In the process of learning about Gumbel Softmax, I reviewed all the relevant content regarding reparameterization and learned some new knowledge about gradient estimation, which I would like to record here. This article starts introducing reparameterization from the continuous case, using the normal distribution's reparameterization as the primary example. Then, it introduces the reparameterization of discrete distributions, involving Gumbel Softmax, including some proofs and discussions. Finally, it talks about the stories behind reparameterization, mainly related to gradient estimation. Basic Concepts Reparameterization is actually a technique for handling objective functions in the following form of expectation: \\begin{equation}L_{\\theta}=\\mathbb{E}_{z\\sim p_{\\theta}(z)}[f(z)]\\label{eq:base"}, {"file": "translation_6736.html", "title": "When BERT Meets Keras: This Might Be the Simplest Way to Open BERT", "content": "← Back to Index When BERT Meets Keras: This Might Be the Simplest Way to Open BERT By 苏剑林 | June 18, 2019 What BERT is likely doesn't need much introduction by now. Although I am not particularly fond of BERT, I must say that it has truly caused a significant stir in the NLP world. Currently, whether in Chinese or English, explanations and interpretations of BERT are everywhere, and it seems its popularity has even surpassed the initial momentum when Word2Vec first came out. Interestingly, BERT was developed by Google, and the original Word2Vec was also developed by Google. Regardless of which one you use, you are essentially following in the footsteps of the big boss Google~ Shortly after BERT was released, some readers suggested I write an interpretation, but I ultimately didn't. Firstly, there are already many interpretations of BERT out there. Secondly, BERT is essentially a large-scale pre-trained model based on Attention. It isn't particularly innovative in terms of technology, and since I've already written an interpretation of Google's Attention , I couldn't quite find the motivation. BERT's pre-training and fine-tuning (Image from the original BERT paper) Overall, I person"}, {"file": "translation_6747.html", "title": "A Brief Discussion on Unbiased and Biased Estimation", "content": "← Back to Index A Brief Discussion on Unbiased and Biased Estimation By 苏剑林 | June 19, 2019 For most readers (including the author), the first biased estimator they encounter is likely the variance: \\begin{equation}\\hat{\\sigma}^2_{\\text{biased}} = \\frac{1}{n}\\sum_{i=1}^n \\left(x_i - \\hat{\\mu}\\right)^2,\\quad \\hat{\\mu} = \\frac{1}{n}\\sum_{i=1}^n x_i\\label{eq:youpianfangcha}\\end{equation} And then they learn that the corresponding unbiased estimator should be: \\begin{equation}\\hat{\\sigma}^2_{\\text{unbiased}} = \\frac{1}{n-1}\\sum_{i=1}^n \\left(x_i - \\hat{\\mu}\\right)^2\\label{eq:wupianfangcha}\\end{equation} In the eyes of many, formula $\\eqref{eq:youpianfangcha}$ seems the most reasonable, so how is it biased? Formula $\\eqref{eq:wupianfangcha}$ replaces $n$ with the counter-intuitive $n-1$, and suddenly it becomes unbiased? Below, I will attempt to discuss the concepts of unbiased and biased estimation using language that is as clear as possible. Suppose we could sample an infinite number of samples; then, theoretically, the following estimations would be exact: \\begin{equation}\\begin{aligned}\\sigma^2 =&\\, \\mathbb{E}\\left[(x - \\mu)^2\\right]=\\lim_{n\\to\\infty}\\frac{1}{n}\\sum_{i=1}^n \\left(x_"}, {"file": "translation_6760.html", "title": "A Concise Introduction to VQ-VAE: Quantized Autoencoder", "content": "← Back to Index A Concise Introduction to VQ-VAE: Quantized Autoencoder By 苏剑林 | June 24, 2019 I recall seeing VQ-VAE quite some time ago, but I didn't have much interest in it then. Recently, two things have reignited my interest. First, VQ-VAE-2 achieved generation results comparable to BigGAN (according to reports by Machine Heart ); second, while reading the NLP paper \"Unsupervised Paraphrasing without Translation\" , I noticed it also utilized VQ-VAE. These two events suggest that VQ-VAE is a fairly versatile and interesting model, so I decided to study it thoroughly. My personal reproduction of VQ-VAE reconstruction on CelebA. Note that details are preserved well, though some blurriness appears upon magnification. Model Overview VQ-VAE (Vector Quantised - Variational AutoEncoder) first appeared in the paper \"Neural Discrete Representation Learning\" , which, like VQ-VAE-2, is a major work from the Google team. Interesting yet Abstruse As an autoencoder, a distinct feature of VQ-VAE is that the vectors it encodes are discrete. In other words, every element of the final encoding vector is an integer. This is the meaning of \"Quantised\" (similar to \"Quantum\" in quantum mechanics, r"}, {"file": "translation_6771.html", "title": "NL2SQL Model Based on Bert: A Concise Baseline", "content": "← Back to Index NL2SQL Model Based on Bert: A Concise Baseline By 苏剑林 | June 29, 2019 In the previous article \"When Bert Meets Keras: This Might Be the Simplest Way to Open Bert,\" we introduced three NLP examples based on fine-tuning Bert, experiencing the power of Bert and the convenience of Keras. In this article, we add another example: an NL2SQL model based on Bert. NL2SQL stands for Natural Language to SQL, which means converting natural language into SQL statements. It has been a subject of much research in recent years and is considered a practical task in the field of artificial intelligence. The opportunity for me to build this model was the first \"Chinese NL2SQL Challenge\" hosted by our company: The first Chinese NL2SQL Challenge uses table data from the financial and general fields as data sources, providing natural language and SQL statement matching pairs annotated on this basis. It is hoped that contestants can use the data to train a model that can accurately convert natural language to SQL. This NL2SQL competition is a relatively large NLP event this year, with significant manpower and resources invested in promotion and a generous prize pool. The only issue is that"}, {"file": "translation_6784.html", "title": "When you jump rope, have you ever thought about what kind of curve the rope forms?", "content": "← Back to Index When you jump rope, have you ever thought about what kind of curve the rope forms? By 苏剑林 | July 06, 2019 A few days ago, several math/physics groups were forwarding a problem that teacher Li Yongle posted on his Weibo: The curve problem of a rope fixed on a pole and rotating. Realizing I hadn't done any math or physics problems in a while, I thought about it and searched for some materials, and now I'd like to share them with you. Related Content In fact, the way Li Yongle presents it makes it feel like it's just a pure physics exercise. However, this problem is actually related to the shapes of curves in several real-life examples. For instance, when jumping rope quickly (and the rope is already taut), what is the shape of the rope? The shape of a skipping rope and the original problem proposed by Teacher Li Yongle are essentially equivalent. The basic characteristics are: 1. There is a thin rope with uniform linear density; 2. The rope is rotating at high speed, and the angular velocity \\(\\omega\\) can be regarded as a constant; 3. The rope is also subject to the action of gravity; 4. Find the curve shape of the rope when it reaches (relative) static equilibrium. "}, {"file": "translation_6794.html", "title": "Trading Time for Effect: Keras Gradient Accumulation Optimizer", "content": "← Back to Index Trading Time for Effect: Keras Gradient Accumulation Optimizer By 苏剑林 | July 08, 2019 Now in Keras, you can also achieve the effect of a large batch size using a small batch size—as long as you are willing to spend $n$ times the time, you can achieve the effect of an $n$ times larger batch size without increasing VRAM. Github Address: https://github.com/bojone/accum_optimizer_for_keras Digression A year or two ago, you didn't really have to worry about OOM (Out of Memory) issues when doing NLP tasks because, compared to models in the CV field, most NLP models were quite shallow and rarely ran out of video memory. Fortunately or unfortunately, Bert was released and then became famous. Bert and its successors (GPT-2, XLNET, etc.) are all based on sufficiently massive Transformer models, pre-trained on large enough corpora, and then completed for specific NLP tasks through fine-tuning. Even if you really don't want to use Bert, the reality today is: the complex model you meticulously designed might not perform as well as simply fine-tuning Bert . So no matter what, to keep up with the times, you need to learn Bert's fine-tuning. The problem is \"you don't know until you"}, {"file": "translation_6810.html", "title": "\"Make Keras a Little Cooler!\": Layers-within-layers and Masking", "content": "← Back to Index \"Make Keras a Little Cooler!\": Layers-within-layers and Masking By 苏剑林 | July 16, 2019 This edition of \"Make Keras a Little Cooler!\" will share two topics with readers: the first is \"layers-within-layers,\" which as the name suggests, involves reusing existing layers when customizing layers in Keras to greatly reduce code volume; the other part, as requested by several readers, is an introduction to the principles and methods of masking in sequence models. Layers-within-layers In the article “Make Keras a Little Cooler!”: Exquisite Layers and Fancy Callbacks , we already introduced the basic methods for customizing Keras layers. The core steps are defining the build and call functions, where build is responsible for creating trainable weights and call defines the specific operations. Avoid Redundant Labor Readers who frequently use custom layers may feel they are doing redundant work. For example, if we want to add a linear transformation, we must add kernel and bias variables in build (manually defining initialization, regularization, etc.), then use K.dot in call , and sometimes consider dimension alignment issues. This process is tedious. In fact, a linear transfo"}, {"file": "translation_6818.html", "title": "Thinking: Can Two Elliptical Sheets Be Glued Together to Form a Solid?", "content": "← Back to Index Thinking: Can Two Elliptical Sheets Be Glued Together to Form a Solid? By 苏剑林 | July 21, 2019 Two weeks ago, I saw a rather interesting question in a group chat: Can two identical elliptical sheets be curved along their major axes and glued together along their edges to perfectly form a closed solid? The question originated from Zhihu: \"Can two elliptical sheets be perfectly joined at their edges by cylindrical bending?\" Illustration of gluing two elliptical sheets (extracted from the Zhihu question) The problem can be expressed in just a few words, and even general readers can understand it. However, the problem itself possesses a certain level of difficulty, which satisfies the conditions for a \"beautiful problem.\" Consequently, it attracted my attention, and I spent several days thinking about it. Finally, yesterday, I formulated a general equation-based approach and a numerical solution scheme for such problems. Today, I completed the theoretical proof, confirming that two identical elliptical sheets can always be perfectly joined. Arc Length Parametric Equation As preparation, we first find the arc length parametric equation of an ellipse. As shown in the figur"}, {"file": "translation_6853.html", "title": "Born for Efficiency: From Standard Attention to Sparse Attention", "content": "← Back to Index Born for Efficiency: From Standard Attention to Sparse Attention By 苏剑林 | July 27, 2019 attention, please! Nowadays, in the field of NLP, Attention is all the rage. Of course, it's not just NLP; Attention also holds a place in the CV field (Non-local, SAGAN, etc.). In the early 2018 article \"A Brief Reading of 'Attention is All You Need' (Introduction + Code)\" , we discussed the Attention mechanism. The core of Attention lies in the interaction and fusion of three vector sequences: $\\boldsymbol{Q}, \\boldsymbol{K}, \\boldsymbol{V}$. The interaction between $\\boldsymbol{Q}$ and $\\boldsymbol{K}$ provides a certain degree of correlation (weight) between pairs of vectors, and the final output sequence is obtained by summing $\\boldsymbol{V}$ according to these weights. Clearly, numerous achievements in NLP & CV have fully affirmed the effectiveness of Attention. In this article, we will introduce some variants of Attention. The common characteristic of these variants is that they are \"born for efficiency\"—saving both time and video memory. Background Brief \"Attention is All You Need\" discusses what we call \"multiplicative Attention,\" which is currently the most widely used"}, {"file": "translation_6869.html", "title": "Keras Implementation of Two Optimizers: Lookahead and LazyOptimizer", "content": "← Back to Index Keras Implementation of Two Optimizers: Lookahead and LazyOptimizer By 苏剑林 | July 30, 2019 Recently, I implemented two optimizers using Keras. They involve some interesting implementation tricks, so I've decided to write a brief article to introduce them (if it were just one, I might not have written it). The names of these two optimizers are quite interesting: one is \"Lookahead\" and the other is \"Lazy\". Do they represent completely different optimization strategies? Not necessarily—it's more that the inventors were very creative with their naming. Lookahead First up is the Lookahead optimizer, which originates from the paper \"Lookahead Optimizer: k steps forward, 1 step back\" . It is a recently proposed optimizer that features big names like Hinton and Jimmy Ba (one of the authors of Adam) in the author list. With the endorsement of these two giants, this optimizer has attracted significant attention. The idea behind Lookahead is very simple. To be precise, it's not a standalone optimizer but rather a strategy for using existing optimizers. In simple terms, it executes the following three steps in a loop: 1. Back up the current model weights $\\theta$; 2. Starting f"}, {"file": "translation_6877.html", "title": "Bidirectional Decoding in seq2seq", "content": "← Back to Index Bidirectional Decoding in seq2seq By 苏剑林 | August 09, 2019 In the article \"Playing with Keras: seq2seq for Automatic Title Generation,\" we basically explored seq2seq and provided a reference implementation in Keras. This article takes that seq2seq model one step further by introducing a bidirectional decoding mechanism. This can improve the quality of generated text to a certain extent (especially when generating longer texts). The bidirectional decoding mechanism introduced here is based on \"Synchronous Bidirectional Neural Machine Translation,\" and the author has implemented it using Keras. Background Introduction Readers who have studied seq2seq know that common seq2seq decoding processes generate text word-by-word (or character-by-character) from left to right. This involves generating the first word based on the encoder's output, then generating the second word based on the encoder's output and the generated first word, then the third word based on the encoder's output and the first two words, and so on. In general, this models the following probability decomposition: \\begin{equation}p(Y|X)=p(y_1|X)p(y_2|X,y_1)p(y_3|X,y_1,y_2)\\cdots\\label{eq:p}\\end{equation} Of"}, {"file": "translation_6906.html", "title": "Open Sourcing a Version of the DGCNN Reading Comprehension QA Model (Keras Version)", "content": "← Back to Index Open Sourcing a Version of the DGCNN Reading Comprehension QA Model (Keras Version) By 苏剑林 | August 20, 2019 Last year, I wrote \"DGCNN: A CNN-based Reading Comprehension Question Answering Model\" , which introduced a simple pure convolutional question-answering model. At that time, it was implemented in TensorFlow and was not open-sourced. These past few days, I took some time to reproduce it using Keras and decided to open-source it. Model Overview Regarding the basic introduction of DGCNN, I will not repeat it here. The model released here is not a simple repetition of the previous model but has some modifications. Here, I will only introduce the parts that have been changed. 1. The score of the model released here on the offline validation set is approximately 0.72 (previously it was approximately 0.75); 2. This model uses characters as the basic unit and utilizes the \" Mixed Character-Word Embedding \" explored by the author previously (previously it was word-based); 3. This model has completely removed manual features (previously 8 manual features were used); 4. This model has removed Position Embeddings (previously Position Embeddings were concatenated to the i"}, {"file": "translation_6910.html", "title": "Introduction to HSIC: An Interesting Approach to Determining Correlation", "content": "← Back to Index Introduction to HSIC: An Interesting Approach to Determining Correlation By 苏剑林 | August 26, 2019 A few days ago, I saw a push on Jiqizhixin (Machine Heart) titled \"Completely Solving the Gradient Explosion Problem: A New Method for Training ResNet Without Backpropagation\" . Of course, we can temporarily ignore the clickbait style of the media headline and focus on the content. The article introduces the results of the paper \"The HSIC Bottleneck: Deep Learning without Back-Propagation\" , which proposes an algorithm for training neural networks through the HSIC Bottleneck. To be honest, I haven't quite understood this paper yet because there are too many new concepts for me. However, the concept of \"HSIC\" in the paper caught my interest. After study, I have basically understood the meaning and origin of HSIC. Thus, this article aims to give as intuitive (though perhaps imprecise) an understanding of HSIC as possible. Background HSIC stands for \"Hilbert-Schmidt independence criterion.\" Like mutual information, it can be used to measure the independence between two variables. Measuring Correlation We know that the basic form of mutual information is:\n\\begin{equation}I("}, {"file": "translation_6915.html", "title": "I Implemented bert4keras Myself", "content": "← Back to Index I Implemented bert4keras Myself By 苏剑林 | August 27, 2019 By 苏剑林 | August 27, 2019 Sharing my personal implementation of bert4keras: https://github.com/bojone/bert4keras This is my re-implementation of BERT for Keras, dedicated to implementing BERT calls under Keras using code that is as clean and simple as possible. Explanation The basic implementation of BERT is now complete, and it successfully loads official weights. It has been verified that the model output is consistent with keras-bert , so you can use it with confidence. The original intention of this project was to provide convenience for modification and customization, so it may be updated frequently. Therefore, stars are welcome, but I do not recommend forking, as the version you fork may quickly become outdated. Usage Quick Installation: pip install bert4keras Reference Code: from bert4keras.models import build_transformer_model\nfrom bert4keras.tokenizers import Tokenizer\nimport numpy as np\n\nconfig_path = '/path/to/bert_config.json'\ncheckpoint_path = '/path/to/bert_model.ckpt'\ndict_path = '/path/to/vocab.txt'\n\ntokenizer = Tokenizer(dict_path, do_lower_case=True)  # Create tokenizer\nmodel = build_transform"}, {"file": "translation_6919.html", "title": "Baidu Entity Linking Competition Post-Mortem: Behavioral Modeling and Entity Linking", "content": "← Back to Index Baidu Entity Linking Competition Post-Mortem: Behavioral Modeling and Entity Linking By 苏剑林 | September 03, 2019 A few months ago, I participated in Baidu's Entity Linking competition , which was one of the evaluation tasks of CCKS2019. The official name was \"Entity Linking\" (实体链指). The competition completely ended a few weeks ago. My final F1 score was around 0.78 (the champion was 0.80), ranking 14th. The results aren't particularly prominent (the only highlight is that the model is very lightweight and can easily run on a GTX1060), so this article is purely to record the process. Experts, please take this with a grain of salt. Problem Introduction Entity Linking mainly refers to predicting which entity ID in a knowledge base a specific mention in an input query corresponds to, given an existing knowledge base. In other words, the knowledge base records many entities. For entities with the same name, there might be multiple interpretations, each assigned a unique ID. Our task is to predict which interpretation (ID) the mention in the query actually corresponds to. This is a necessary step for question-answering systems based on knowledge graphs. Data Format Entity"}, {"file": "translation_6920.html", "title": "Rewriting the Previous New Word Discovery Algorithm: Faster and Better New Word Discovery", "content": "← Back to Index Rewriting the Previous New Word Discovery Algorithm: Faster and Better New Word Discovery By 苏剑林 | September 09, 2019 New word discovery is one of the foundational tasks in NLP. It primarily aims to identify character sequences in a corpus that could potentially be \"new words\" through the unsupervised discovery of linguistic features (mainly statistical features). This site has published several articles on the topic of \"new word discovery,\" such as: among these articles, the author believes the one with the most elegant theory is \"Unsupervised Word Segmentation based on Language Models\" , while the one with better overall performance as a new word discovery algorithm is \"A Better New Word Discovery Algorithm\" . This article is a reimplementation of the algorithm from that post. Background When I wrote \"[Chinese Word Segmentation Series] 8. A Better New Word Discovery Algorithm\" , I already provided a basic implementation and verified it. However, the original version was written in pure Python and was only intended for quick verification of the effect, so it was written quite casually and suffered from serious efficiency issues. I recently recalled this and felt it"}, {"file": "translation_6933.html", "title": "From Language Models to Seq2Seq: Transformer is Like a Play, It All Depends on the Mask", "content": "← Back to Index From Language Models to Seq2Seq: Transformer is Like a Play, It All Depends on the Mask By 苏剑林 | September 18, 2019 I believe that over the past year (especially the last six months), everyone has frequently seen reports on various Transformer-related works (such as BERT, GPT, XLNet, etc.), along with the continuous refreshing of evaluation metrics for various basic tasks. At the same time, many blogs and columns have provided popular science and interpretations of these models. As the saying goes, \"laymen watch the spectacle, while insiders look at the mechanism.\" We not only need to understand these works at the \"what it is\" level, but we also need to think about \"why.\" This \"why\" is not just \"why do it this way,\" but also \"why can it be done this way.\" For instance, when discussing XLNet's Permutation Language Model, we might have understood its benefits from various introductions. However, it's worth thinking a step further: Why can the Transformer implement a Permutation Language Model? How is it implemented? Can an RNN implement it? This article analyzes the fundamental reasons why many Transformer models can be played so \"brilliantly\" from the perspective of "}, {"file": "translation_6985.html", "title": "\"Make Keras a Bit Cooler!\": Reuse Techniques for Layers and Models", "content": "← Back to Index \"Make Keras a Bit Cooler!\": Reuse Techniques for Layers and Models By 苏剑林 | September 29, 2019 Today we continue to dig deep into Keras, once again experiencing its unparalleled elegant design. This time, our focus is on \"reuse,\" primarily the repetitive use of layers and models. Generally, reuse is pursued for two goals: first, to share weights—meaning that two layers not only function the same but also share weights and update synchronously; second, to avoid rewriting code—for example, when we have already built a model and want to decompose it to construct sub-models, etc. Basics In fact, Keras has already considered many of these aspects for us, so in many cases, mastering the basic usage is sufficient to meet most of our needs. Layer Reuse Layer reuse is the simplest: initialize a layer, store it, and then call it repeatedly: x_in = Input(shape=(784,))\nx = x_in\n\nlayer = Dense(784, activation='relu') # Initialize a layer and store it\n\nx = layer(x) # First call\nx = layer(x) # Subsequent call\nx = layer(x) # Subsequent call It is important to note that you must first initialize a layer and store it as a variable before calling it to ensure that the repeated calls s"}, {"file": "translation_6992.html", "title": "What Role Does BN Really Play? A \"Closed-Door\" Analysis", "content": "← Back to Index What Role Does BN Really Play? A \"Closed-Door\" Analysis By 苏剑林 | Oct 11, 2019 BN, also known as Batch Normalization , is a critically important technique in current deep learning models (especially vision-related models). It accelerates training, possesses a certain anti-overfitting effect, and even allows us to use larger learning rates. Overall, it offers many benefits (provided you can afford a sufficiently large batch size). So, how does BN actually work? Early explanations were primarily based on probability distributions, roughly suggesting that normalizing the input distribution of each layer to $\\mathcal{N}(0,1)$ reduces so-called Internal Covariate Shift, thereby stabilizing and accelerating training. This explanation seems plausible at first glance, but upon closer inspection, it is problematic: the input of any layer cannot strictly satisfy a normal distribution, so simply standardizing the mean and variance cannot achieve a standard distribution $\\mathcal{N}(0,1)$. Furthermore, even if $\\mathcal{N}(0,1)$ could be achieved, this interpretation fails to explain why other normalization methods (such as Instance Normalization or Layer Normalization) also wor"}, {"file": "translation_7006.html", "title": "The Minimum Entropy Principle (V): \"Step by Step\" Community Detection and Clustering", "content": "← Back to Index The Minimum Entropy Principle (V): \"Step by Step\" Community Detection and Clustering By 苏剑林 | October 19, 2019 Let us tirelessly review: the Minimum Entropy Principle is an unsupervised learning principle. \"Entropy\" represents the learning cost, and reducing this cost is our constant pursuit. By \"minimizing the learning cost,\" we can unsupervisedly learn results that align with our intuition. This is the basic philosophy of the Minimum Entropy Principle. In this article, we will introduce a quite beautiful clustering algorithm that also embodies the Minimum Entropy Principle, or can be derived from it. It is called InfoMap , or MapEquation . In fact, InfoMap is a result from 2007, with the earliest paper being \"Maps of random walks on complex networks reveal community structure\" . Although it may seem old, I believe it remains the most beautiful clustering algorithm because it doesn't just tell us \"how to cluster,\" but more importantly, it provides an elegant information-theoretic explanation of \"why we cluster,\" and directly derives the entire clustering process from this explanation. A schematic diagram of a complex directed graph network. Image from the original "}, {"file": "translation_7031.html", "title": "When Can the Multi-processing Speedup Ratio Be Greater Than 1?", "content": "← Back to Index When Can the Multi-processing Speedup Ratio Be Greater Than 1? By 苏剑林 | October 27, 2019 Parallel acceleration via multi-processing or multi-threading is no longer a difficult task, and many readers have likely experienced it. Generally, we reach this conclusion: the speedup ratio of multi-processing rarely exceeds 1. In other words, when you use 10 processes to run a task in parallel, you generally only obtain a speedup of less than 10 times, and as the number of processes increases, this speedup ratio often becomes even lower. Note that we just said \"rarely exceeds 1,\" which implies that in our subconscious, we feel the speedup ratio should be at most 1. Theoretically, this is true—could 10 processes really achieve a 20-fold acceleration? Isn't that just a windfall? However, I did happen to encounter an example a few days ago where the speedup ratio was significantly greater than 1, so I would like to share it here. Word Frequency Statistics My original task was to count word frequencies: I have many articles, we need to perform word segmentation (tokenization) on these articles, and finally summarize them into a word frequency table. A typical implementation look"}, {"file": "translation_7038.html", "title": "From Denoising Autoencoders to Generative Models", "content": "← Back to Index From Denoising Autoencoders to Generative Models By 苏剑林 | October 31, 2019 In my opinion, among the major conferences, ICLR papers are usually the most interesting. This is because their topics and styles are generally relaxed, lively, and imaginative, often giving one the feeling of a wide-open \"brainstorm.\" Therefore, after the submitted paper list for ICLR 2020 came out, I took some time to browse through them and indeed found many interesting works. Among them, I discovered two papers that use the idea of denoising autoencoders to build generative models, namely \"Learning Generative Models using Denoising Density Estimators\" and \"Annealed Denoising Score Matching: Learning Energy-Based Models in High-Dimensional Spaces\" . Since I am already familiar with the conventional approaches to generative models, this \"unique\" perspective piqued my interest. Upon closer reading, I found that their starting points are the same, but their specific implementations differ, and their final resolutions converge again—a beautiful instance of \"multiple solutions to one problem.\" Thus, I have grouped these two papers together for a comparative analysis. Denoising Autoencoding The "}, {"file": "translation_7055.html", "title": "Keras: The Gold Standard for Tensorflow", "content": "← Back to Index Keras: The Gold Standard for Tensorflow By 苏剑林 | November 06, 2019 Over the past two weeks, I have invested a lot of energy into the development of bert4keras . Besides some API standardization work, the primary workload was building the pre-training section of the code. Yesterday, the pre-training code was basically completed and tested successfully in both TPU and multi-GPU environments. This provides another option for students who are motivated (and have the computing power) to improve pre-trained models. This might be the most clear and easy-to-understand code for BERT and its pre-training available currently. Pre-training code link: https://github.com/bojone/bert4keras/tree/master/pretraining After these two weeks of development (and filling in holes), my biggest takeaway is that Keras has already become the gold standard for TensorFlow. As long as your code is written according to Keras standards and specifications, it can be easily migrated to tf.keras and then very easily trained in TPU or multi-GPU environments—truly almost a once-and-for-all solution. Conversely, if your style of writing is too flexible, including many \"grafting\" style Keras tricks I intr"}, {"file": "translation_7063.html", "title": "JoSE: Word and Sentence Vectors on the Sphere", "content": "← Back to Index JoSE: Word and Sentence Vectors on the Sphere By 苏剑林 | November 11, 2019 This article introduces a model for word and sentence vectors named JoSE (Joint Spherical Embedding), which was published at NeurIPS 2019 under the title \"Spherical Text Embedding\" . In terms of idea and methodology, the JoSE model is a successor to Doc2Vec. The evaluation results are quite impressive, although the writing can feel a bit overly complicated for its own sake. However, I decided to write this post because some of the analysis within it is interesting and may hold reference value for general optimization problems. Optimization Objective In principle, this paper is largely consistent with Doc2Vec : to train sentence vectors, a sentence is represented by an ID and treated as a word that co-occurs with all the words within that sentence. Finally, a Skip-Gram model is trained, primarily using negative sampling. What distinguishes JoSE from Doc2Vec is that the lengths of all vectors are normalized (meaning only vectors on the unit sphere are considered), and the training objective uses a hinge loss instead of cross-entropy: \\begin{equation}\\max(0, m - \\cos(\\boldsymbol{u}, \\boldsymbol{v}"}, {"file": "translation_7076.html", "title": "The Angle Distribution of Two Random Vectors in $n$-dimensional Space", "content": "← Back to Index The Angle Distribution of Two Random Vectors in $n$-dimensional Space By 苏剑林 | Nov 13, 2019 Yesterday, there was a discussion in the group about some counter-intuitive phenomena regarding $n$-dimensional vectors. One topic was that \"generally, in $n$-dimensional space, two random vectors are almost always perpendicular,\" which markedly differs from our perception of 2D and 3D space. To understand this conclusion theoretically, we can consider the distribution of the angle $\\theta$ between two random vectors and calculate its mean and variance. Probability Density First, let's derive the probability density function of $\\theta$. In fact, there is no need for a lengthy derivation, as it is a direct consequence of $n$-dimensional hyperspherical coordinates. To find the distribution of the angle between two random vectors, it is clear that due to isotropy, we only need to consider unit vectors. Similarly, due to isotropy, we can fix one vector and let the other vary randomly. Without loss of generality, let the random vector be \\begin{equation}\\boldsymbol{x}=(x_1,x_2,\\dots,x_n)\\end{equation} and the fixed vector be \\begin{equation}\\boldsymbol{y}=(1,0,\\dots,0)\\end{equati"}, {"file": "translation_7094.html", "title": "A Brief Introduction and Implementation of 6 Derived Optimizers", "content": "← Back to Index A Brief Introduction and Implementation of 6 Derived Optimizers By 苏剑林 | Nov 25, 2019 Optimizers might be one of the most \"metaphysical\" modules in deep learning: sometimes switching an optimizer brings a significant improvement, while other times an optimizer that others claim is great does absolutely nothing for your own task. Optimizers with good theoretical properties don't necessarily work well, and those born purely from a stroke of inspiration aren't necessarily bad. Regardless, optimizers provide another choice for those who love \"deep learning alchemy.\" In recent years, work regarding optimizers seems to be slowly increasing. Many papers have proposed various improvements to commonly used optimizers (especially Adam ). This article summarizes several optimizer works or techniques and provides a unified code implementation for readers to use as needed. Basic Form The term \"derived\" refers to the fact that these techniques are built upon existing optimizers. Any existing optimizer can utilize these techniques to transform into a new optimizer. The basic form of an existing optimizer is: \\begin{equation}\n    \\begin{aligned}\n    \\boldsymbol{g}_t =&\\, \\nabla_{\\b"}, {"file": "translation_7105.html", "title": "Cascading Rejection: A Simple and Effective Way to Improve GAN Performance", "content": "← Back to Index Cascading Rejection: A Simple and Effective Way to Improve GAN Performance By 苏剑林 | December 01, 2019 Yesterday while browsing Arxiv, I discovered a paper from South Korea with a very straightforward title: \"A Simple yet Effective Way for Improving the Performance of GANs\" . Upon opening it, I found the content to be very concise, proposing a method to strengthen the GAN discriminator that can lead to certain improvements in generation metrics. The authors call this method \"Cascading Rejection.\" I wasn't quite sure how to translate it, but Baidu Translate suggested \"级联抑制\" (Cascading Suppression/Rejection), which seems to fit the logic, so I will refer to it as such for now. I am introducing this method not because it is exceptionally powerful, but because its geometric meaning is very interesting and seems to offer some inspiration. Orthogonal Decomposition A GAN discriminator generally produces a fixed-length vector $\\boldsymbol{v}$ through multiple layers of convolution followed by flattening or pooling. This vector is then used in an inner product with a weight vector $\\boldsymbol{w}$ to obtain a scalar score (ignoring bias terms and activation functions for simp"}, {"file": "translation_7115.html", "title": "Universal Seq2Seq: Reading Comprehension Question Answering Based on Seq2Seq", "content": "← Back to Index Universal Seq2Seq: Reading Comprehension Question Answering Based on Seq2Seq By 苏剑林 | December 05, 2019 Today, I added a new example to bert4keras : Reading Comprehension Question Answering ( task_reading_comprehension_by_seq2seq.py ). The corpora used are the same as before, WebQA and SogouQA . The final score is around 0.77 (single model, without fine-tuning). Method Description Since the primary purpose this time was to add a demo to bert4keras, efficiency was not the main concern. The goal was universality and ease of use, so I adopted the most \"universal\" solution—using seq2seq to implement reading comprehension. When using seq2seq, you basically don't need to worry about model design; you just concatenate the passage and the question and then predict the answer. Furthermore, the seq2seq approach naturally includes a method for determining whether a passage contains an answer and naturally leads to a multi-passage voting strategy. In short, if efficiency is not considered, seq2seq is a very elegant solution for reading comprehension. This implementation of seq2seq still uses the UNILM scheme. Readers who are not familiar with it can read \"From Language Models t"}, {"file": "translation_7124.html", "title": "Conditional Text Generation Based on Conditional Layer Normalization", "content": "← Back to Index Conditional Text Generation Based on Conditional Layer Normalization By 苏剑林 | December 14, 2019 From the article \"From Language Models to Seq2Seq: Transformer as a Play, Relying Entirely on Masking\" , we know that as long as it is paired with the appropriate Attention Mask, Bert (or other Transformer models) can be used for unconditional generation (Language Model) and sequence translation (Seq2Seq) tasks. But what if it is conditional generation? For example, controlling the category of text to generate text randomly based on a category, which is a Conditional Language Model; or another example, passing in an image to generate a related text description, which is Image Captioning. Related Work The August paper \"Encoder-Agnostic Adaptation for Conditional Language Generation\" systematically analyzes several schemes for using pre-trained models for conditional generation; a September paper \"CTRL: A Conditional Transformer Language Model for Controllable Generation\" provides a model pre-trained based on conditional generation, but this is essentially a language model like GPT that can only take text input as a condition; and the recent paper \"Plug and Play Language Mo"}, {"file": "translation_7144.html", "title": "2020 Astronomy Calendar of Celestial Events", "content": "← Back to Index 2020 Astronomy Calendar of Celestial Events By 苏剑林 | Dec 23, 2019 Astronomy Calendar of Celestial Events 2020 Astronomy Calendar of Celestial Events Translated from NASA: http://eclipse.gsfc.nasa.gov/SKYCAL/SKYCAL.html (Beijing Time) 2011 Version 2012 Version 2013 Version 2014 Version 2015 Version 2016 Version 2017 Version 2018 Version 2019 Version Month Day Day-of-Week Time Event January 01 Wed Venus: $34.6^\\circ$ E 02 Thu 09:30:00 Moon at Apogee: $404,600$ km 03 Fri 12:45:00 First Quarter Moon 04 Sat 16:38:00 Quadrantid Meteor Shower: $\\text{ZHR} = 120$ 05 Sun 17:59:00 Earth at Perihelion: $0.9832$ AU 10 Fri 07:29:00 Moon at Ascending Node 10 Fri 14:03:00 Moon at North Declination: $23.2^\\circ$ N 10 Fri 23:01:00 Mercury at Superior Conjunction 11 Sat 03:10:00 Penumbral Lunar Eclipse 11 Sat 03:21:00 Full Moon 12 Sun 07:54:00 Beehive Cluster conjunction with Moon: $1^\\circ$ S 13 Mon 22:23:00 Saturn in Solar Conjunction 14 Tue 04:20:00 Moon at Perigee: $366,000$ km 17 Fri 20:58:00 Last Quarter Moon 18 Sat 07:03:00 Mars conjunction with Antares: $4.8^\\circ$ N 21 Tue 03:13:00 Mars conjunction with Moon: $2.4^\\circ$ S 23 Thu 04:31:00 Moon at Descending Node 23 Thu 10:42"}, {"file": "translation_7148.html", "title": "\"Non-Autoregressive\" Isn't Bad Either: MLM-Based Reading Comprehension Question Answering", "content": "← Back to Index \"Non-Autoregressive\" Isn't Bad Either: MLM-Based Reading Comprehension Question Answering By 苏剑林 | December 26, 2019 Some time ago, I wrote \"The Universal seq2seq: Reading Comprehension QA Based on seq2seq\" , exploring how to perform reading comprehension question answering using the most general seq2seq approach, achieving quite good results (single model 0.77, exceeding the best fine-tuned model used during the competition). In this article, we continue with this task but change the approach to be directly based on the Masked Language Model (MLM). The final performance is basically consistent, but the prediction speed is significantly improved. Two Types of Generation Broadly speaking, the MLM generation method can also be considered a seq2seq model, but it belongs to \"non-autoregressive\" generation, whereas what we usually call (narrowly defined) seq2seq refers to autoregressive generation. This section provides a brief introduction to these two concepts. Autoregressive Generation As the name suggests, autoregressive generation refers to the decoding stage where tokens are generated recursively, character by character. It models the following probability distribu"}, {"file": "translation_7161.html", "title": "Triple Extraction with bert4keras", "content": "← Back to Index Triple Extraction with bert4keras By 苏剑林 | January 03, 2020 While developing bert4keras , I promised to gradually migrate the examples previously implemented with keras-bert over. One of those examples was triple extraction. Currently, the examples in bert4keras are becoming quite rich, but they were still missing tasks related to sequence labeling and information extraction. Since triple extraction fits this category perfectly, I have added it now. Schematic Diagram of BERT-based Triple Extraction Model Structure Model Introduction Regarding the data format and the basic logic of the model, these were detailed in the article \"A Lightweight Information Extraction Model Based on DGCNN and Probabilistic Graphs\" , so I will not repeat them here. The dataset has been made public by Baidu and can be downloaded here . Following the same strategy as before, the model is still based on the \"semi-pointer, semi-annotation\" method. The sequence is to first extract the subject ($s$), then pass $s$ into the model to extract the object ($o$) and predicate ($p$). The only difference is that the overall architecture of the model has been replaced with BERT: The original sequence is"}, {"file": "translation_7169.html", "title": "Self-Orthogonality Module: A Plug-and-play Kernel Orthogonalization Module", "content": "← Back to Index Self-Orthogonality Module: A Plug-and-play Kernel Orthogonalization Module By 苏剑林 | January 12, 2020 A few days ago, while browsing Arxiv, I saw a new paper titled \"Self-Orthogonality Module: A Network Architecture Plug-in for Learning Orthogonal Filters\" (hereinafter referred to as the \"original paper\"). It looked interesting, so I read through it. After finishing, I indeed gained some insights, which I will record and share here. Adding regularization terms with an orthogonalization tendency to the kernels of fully connected or convolutional models is a requirement for many models. For example, the famous BigGAN incorporates similar regularization terms. This paper introduces a new regularization term. I find the entire analysis process quite interesting and worth a read. Why Hope for Orthogonality? Before we begin, let us agree: all one-dimensional vectors appearing in this article represent column vectors. Now, suppose we have a $d$-dimensional input sample $\\boldsymbol{x} \\in \\mathbb{R}^d$. When it passes through a fully connected or convolutional layer, the core operation is: \\begin{equation}\\boldsymbol{y}^{\\top}=\\boldsymbol{x}^{\\top}\\boldsymbol{W},\\quad \\bold"}, {"file": "translation_7180.html", "title": "Understanding Model Parameter Initialization Strategies from a Geometric Perspective", "content": "← Back to Index Understanding Model Parameter Initialization Strategies from a Geometric Perspective By 苏剑林 | January 16, 2020 For complex models, parameter initialization is particularly important. A poor initialization often isn't just a matter of the model's performance suffering; it is more likely that the model will simply fail to train or converge entirely. A common adaptive initialization strategy in deep learning is Xavier initialization, which consists of weights randomly sampled from a normal distribution $\\mathcal{N}\\left(0, \\frac{2}{fan_{in} + fan_{out}}\\right)$, where $fan_{in}$ is the input dimension and $fan_{out}$ is the output dimension. Other initialization strategies are generally similar, albeit with different assumptions, leading to slight variations in their final forms. The derivation of standard initialization strategies is based on probability and statistics. The general idea is to assume that the input data has a mean of 0 and a variance of 1, and then expect the output data to also maintain a mean of 0 and a variance of 1, thereby deriving the conditions for the mean and variance that the initial transformation should satisfy. Theoretically, there is noth"}, {"file": "translation_7187.html", "title": "Leave Constraints Behind, Enhance the Model: One Line of Code to Improve ALBERT's Performance", "content": "← Back to Index Leave Constraints Behind, Enhance the Model: One Line of Code to Improve ALBERT's Performance By 苏剑林 | January 29, 2020 The title of this article might seem a bit like \"clickbait,\" but when applied within the bert4keras framework, it truly is a one-line code change. As for whether it provides an improvement, I cannot guarantee it for every case, but testing on several representative tasks has shown performance that is either on par with or better than the original, so the title is essentially a statement of fact. What exactly is this change? It can be explained in one sentence: In downstream tasks, abandon ALBERT's weight-sharing constraint—essentially, use ALBERT as if it were BERT. For more details on the logic, please read on. What is ALBERT? This modification is specifically designed for ALBERT. To understand it, you first need to know what ALBERT is. I will spend some space on a brief introduction to ALBERT here. I assume readers already have some understanding of BERT, so the focus will be on comparing ALBERT with BERT. Low-rank Decomposition First is the Embedding layer. Taking the Chinese version of BERT-base as an example, the total number of tokens is appr"}, {"file": "translation_7196.html", "title": "Your CRF Layer's Learning Rate Might Not Be Large Enough", "content": "← Back to Index Your CRF Layer's Learning Rate Might Not Be Large Enough By 苏剑林 | February 07, 2020 CRF (Conditional Random Field) is a classic method for sequence labeling. It is theoretically elegant and practically effective. For readers who are not yet familiar with CRF, feel free to read my previous post \"A Brief Introduction to Conditional Random Fields (CRF) (with pure Keras implementation)\" . After the emergence of the BERT model, much work has explored using BERT+CRF for sequence labeling tasks. However, many experimental results (such as those in the paper \"BERT Meets Chinese Word Segmentation\" ) show that for both Chinese word segmentation and named entity recognition, BERT+CRF does not seem to bring any significant improvement compared to a simple BERT+Softmax approach. This differs from the behavior observed in traditional BiLSTM+CRF or CNN+CRF models. Schematic of a 4-tag word segmentation model based on CRF Over the past couple of days, I added a Chinese word segmentation example using CRF to bert4keras ( task_sequence_labeling_cws_crf.py ). During debugging, I discovered that the CRF layer might suffer from insufficient learning. Further comparative experiments sugg"}, {"file": "translation_7210.html", "title": "Designing GANs: Another GAN Production Workshop", "content": "← Back to Index Designing GANs: Another GAN Production Workshop By 苏剑林 | Feb 13, 2020 In the 2018 article \"An Introduction to f-GAN: A Production Workshop for GAN Models,\" I introduced f-GAN and described it as a \"production workshop\" for GAN models. As the name suggests, this refers to its ability to construct many different forms of GAN models following a fixed procedure. A few days ago, I saw a new paper on arXiv titled \"Designing GANs: A Likelihood Ratio Approach\" (hereafter referred to as Designing GANs or the original paper). I found it to be doing the same thing as f-GAN, but taking a completely different path (though ultimately arriving at the same destination). The entire paper is quite interesting, so I've decided to share it here. f-GAN Recap From \"An Introduction to f-GAN: A Production Workshop for GAN Models,\" we know that the first step of f-GAN is to find a function $f$ that satisfies the following conditions: 1. $f$ is a mapping from non-negative real numbers to real numbers ($R^* \\to R$); 2. $f(1) = 0$; 3. $f$ is a convex function. Once such a function is found, a probability f-divergence can be constructed. Then, a technique called \"convex conjugation\" is used to "}, {"file": "translation_7213.html", "title": "Already used CRF? Why not learn about the faster MEMM?", "content": "← Back to Index Already used CRF? Why not learn about the faster MEMM? By 苏剑林 | February 24, 2020 HMM, MEMM, and CRF are known as the three classic probabilistic graphical models. In the era of machine learning before deep learning, they were widely used in various sequence labeling-related tasks. An interesting phenomenon is that in the deep learning era, HMM and MEMM seem to have \"declined,\" leaving only CRF on the stage. I believe readers working in NLP have likely heard of, if not personally implemented, BiLSTM+CRF for tasks like Chinese word segmentation and named entity recognition, but almost never hear of BiLSTM+HMM or BiLSTM+MEMM. Why is that? Today, let's study MEMM and, through a comparison with CRF, gain a deeper understanding of the principles and design of probabilistic graphical models. Model Derivation MEMM stands for Maximum Entropy Markov Model. It must be said that this name might intimidate 80% of beginners: \"I don't even understand Maximum Entropy, and I don't know Markov—combined, aren't they just gibberish?\" In fact, whether it's MEMM or CRF, their models are much simpler than their names suggest. Their concepts and designs are very simple and natural, and no"}, {"file": "translation_7234.html", "title": "A Brief Discussion on Adversarial Training: Significance, Methods, and Thoughts (with Keras Implementation)", "content": "← Back to Index A Brief Discussion on Adversarial Training: Significance, Methods, and Thoughts (with Keras Implementation) By 苏剑林 | March 01, 2020 Currently, when talking about \"adversarial\" in deep learning, there are generally two meanings: one is Generative Adversarial Networks (GANs), representing a major class of advanced generative models; the other is the field related to adversarial attacks and adversarial samples. This latter field is related to GANs but quite different, primarily concerning the robustness of models under small perturbations. The adversarial topics previously covered in this blog have all been about the former; today, let's discuss \"adversarial training\" within the context of the latter. This article includes the following content: 1. Introduction to basic concepts like adversarial samples and adversarial training; 2. Introduction to adversarial training based on Fast Gradient Method (FGM) and its application in NLP; 3. A Keras implementation of adversarial training (invoked with a single line of code); 4. Discussion on the equivalence between adversarial training and gradient penalty; 5. An intuitive geometric understanding of adversarial training based "}, {"file": "translation_7259.html", "title": "Brief Analysis and Countermeasures for Exposure Bias in Seq2Seq", "content": "← Back to Index Brief Analysis and Countermeasures for Exposure Bias in Seq2Seq By 苏剑林 | March 09, 2020 A few days ago, I wrote \"Now that you've used CRF, why not learn about the faster MEMM?\" , which mentioned the advantages and disadvantages of MEMM's local normalization versus CRF's global normalization. At the same time, I thought of the Seq2Seq model, because the typical training scheme for Seq2Seq, Teacher Forcing, is a local normalization model, and therefore it also suffers from the problems brought by local normalization—what we often call \"Exposure Bias.\" With this thought in mind, I continued to reflect on the matter and recorded my final thoughts in this article. The classic Seq2Seq model illustration This article is an advanced piece, suitable for readers who already have a certain understanding of Seq2Seq models and wish to further enhance their understanding or model performance. For introductory articles on Seq2Seq, you can read previous works \"Playing with Keras: Seq2Seq for Automatic Title Generation\" and \"From Language Models to Seq2Seq: Transformer is all about the Mask.\" The content of this article is roughly: 1. Analysis of the causes of Exposure Bias and exam"}, {"file": "translation_7292.html", "title": "Now You Can Play with Chinese GPT2 Using Keras (GPT2_ML)", "content": "← Back to Index Now You Can Play with Chinese GPT2 Using Keras (GPT2_ML) By 苏剑林 | March 16, 2020 A while ago, I noticed that an expert open-sourced a Chinese GPT2 model. It is the largest version with 1.5 billion parameters. Looking at the demo provided by the author, the generation effect is quite impressive. I thought about loading it into my bert4keras to play with it. However, the overall architecture of early bert4keras was written quite \"rigidly,\" making it very inconvenient to integrate multiple different models. Two weeks ago, I finally couldn't stand it anymore and rewrote the overall structure of bert4keras. Now, bert4keras can be considered relatively flexible for writing various Transformer-based models, such as GPT2 and T5 , which have already been integrated. GPT2 Brief Introduction GPT is something many readers have likely heard of. Simply put, it is a language model based on the Transformer structure, originating from the paper \"GPT: Improving Language Understanding by Generative Pre-Training\" . However, it wasn't created just to be a language model; it uses the language model to pre-train itself and then fine-tunes on downstream tasks to improve performance. It is "}, {"file": "translation_7302.html", "title": "Analysis of the AdaFactor Optimizer (with Open Source Implementation)", "content": "← Back to Index Analysis of the AdaFactor Optimizer (with Open Source Implementation) By 苏剑林 | March 23, 2020 Since pre-trained models like GPT and BERT became popular, one obvious trend is that models are getting larger, because larger models combined with more thorough pre-training usually tend to top the leaderboards more effectively. However, while ideals can be infinitely far-reaching, reality is often quite constrained. Sometimes the model is so large that even if you have GPUs or even TPUs with huge memory, you still feel a sense of despair. For example, the largest version of GPT-2 has 1.5 billion parameters, and the largest version of the T5 model even reached 11 billion parameters. Models of this scale can hardly run with a large batch size even on TPU clusters. At this point, one usually needs to focus on the optimization process—for example, using mixed-precision training (on TensorFlow, you can also use a new floating-point format called bfloat16), which saves memory and accelerates training; or using more memory-efficient optimizers, such as RMSProp, which is more memory-efficient than Adam. This article introduces AdaFactor , a new optimizer proposed by Google, first"}, {"file": "translation_7309.html", "title": "How the Two Elementary Function Approximations of GELU Came to Be", "content": "← Back to Index How the Two Elementary Function Approximations of GELU Came to Be By 苏剑林 | March 26, 2020 GELU, which stands for Gaussian Error Linear Unit, is a variant of the ReLU activation function and is expressed in a non-elementary form. It was introduced in the paper \"Gaussian Error Linear Units (GELUs)\" , used later in GPT, then in BERT, and subsequently adopted by many later pre-trained language models. With the rise of BERT and other pre-trained models, GELU has surged in popularity, becoming a trendy activation function almost overnight. In the original GELU paper, the authors proposed not only the exact form of GELU but also provided two elementary function approximations. This article discusses how those approximations were derived. The GELU Function The form of the GELU function is: \\begin{equation}\\text{GELU}(x)=x \\Phi(x)\\end{equation} where $\\Phi(x)$ is the cumulative distribution function of the standard normal distribution, i.e., \\begin{equation}\\Phi(x)=\\int_{-\\infty}^x \\frac{e^{-t^2/2}}{\\sqrt{2\\pi}}dt=\\frac{1}{2}\\left[1 + \\text{erf}\\left(\\frac{x}{\\sqrt{2}}\\right)\\right]\\end{equation} Here $\\text{erf}(x)=\\frac{2}{\\sqrt{\\pi}}\\int_0^x e^{-t^2}dt$. The original pape"}, {"file": "translation_7321.html", "title": "bert4keras in hand, I have the baseline: Baidu LIC2020", "content": "← Back to Index bert4keras in hand, I have the baseline: Baidu LIC2020 By 苏剑林 | April 02, 2020 Baidu's \" 2020 Language and Intelligence Challenge \" has begun. This year features five tracks: Machine Reading Comprehension, Recommendation-based Dialogue, Semantic Parsing, Relation Extraction, and Event Extraction. For each track, the organizers have provided baseline models based on PaddlePaddle. Here, I am providing my personal baselines for three of these tracks based on bert4keras . From these, we can see how quick, convenient, and concise it is to build baseline models using bert4keras. Address: https://github.com/bojone/lic2020_baselines Brief Analysis of Ideas Here is a brief analysis of the task characteristics of these three tracks and the design of the corresponding baselines. Reading Comprehension Sample example: There isn't much to say about this baseline; it essentially follows BERT with two Dense layers + Softmax to predict the start and end of the answer respectively. Some training samples are labeled with multiple answers, but since only one answer needs to be predicted during inference, one answer is randomly selected for training during each step of the training phas"}, {"file": "translation_7325.html", "title": "Breaking Through the Bottleneck: Building a Stronger Transformer", "content": "← Back to Index Breaking Through the Bottleneck: Building a Stronger Transformer By 苏剑林 | April 13, 2020 Since the release of \"Attention is All You Need\" , Transformer models based on Multi-Head Attention have become popular. The BERT model released last year pushed the popularity of the Transformer model to a new peak. Of course, the exploration of technology is endless, and incremental improvements have emerged: some improve pre-training tasks, such as XLNet's PLM and ALBERT's SOP; some improve normalization, such as the shift from Post-Norm to Pre-Norm and the removal of the beta parameter in Layer Norm in T5; some improve the model structure, such as Transformer-XL; some improve training methods, such as ALBERT's parameter sharing; and so on. The changes above all take place outside of the Attention mechanism. In other words, they default to the rationality of the Attention mechanism itself and do not modify it. In this article, we introduce two new results: they target potential modeling bottlenecks in Multi-Head Attention and propose different solutions to improve Multi-Head Attention. Both papers come from Google and include extensive experiments, making the results quite pe"}, {"file": "translation_7343.html", "title": "EAE: Autoencoder + BN + Maximum Entropy = Generative Model", "content": "← Back to Index EAE: Autoencoder + BN + Maximum Entropy = Generative Model By 苏剑林 | April 20, 2020 Generative models have always been a theme of great interest to me, whether in the context of NLP or CV. In this article, we introduce a novel generative model from the paper \"Batch norm with entropic regularization turns deterministic autoencoders into generative models,\" which the authors call EAE (Entropic AutoEncoder). What it aims to achieve is fundamentally consistent with Variational Autoencoders (VAEs), and the final results are also similar (slightly superior). Its novelty lies not so much in how well it generates, but in its unique and elegant line of thought. Furthermore, we will take this opportunity to learn a method for estimating statistics—the k-nearest neighbor (k-NN) method—which is a very useful non-parametric estimation technique. Autoencoders vs. Generative Models A standard autoencoder is a \"coding-decoding\" reconstruction process, as shown in the diagram below: Its loss is generally given by: \\begin{equation}L_{AE} = \\mathbb{E}_{x\\sim \\tilde{p}(x)}\\left[\\left\\Vert x - \\hat{x}\\right\\Vert^2\\right] = \\mathbb{E}_{x\\sim \\tilde{p}(x)}\\left[\\left\\Vert x - D(E(x))\\right"}, {"file": "translation_7359.html", "title": "Extending \"Softmax + Cross Entropy\" to Multi-label Classification", "content": "← Back to Index Extending \"Softmax + Cross Entropy\" to Multi-label Classification By 苏剑林 | April 25, 2020 (Note: The relevant content of this article has been organized into a paper \"ZLPR: A Novel Loss for Multi-label Classification\" . If you need to cite this, you can cite the English paper directly. Thank you.) Generally speaking, when dealing with conventional multi-class classification problems, we use a fully connected layer at the end of the model to output scores for each class, then use softmax activation and cross-entropy as the loss function. In this article, we attempt to extend the \"Softmax + Cross Entropy\" scheme to multi-label classification scenarios, hoping to obtain a loss function for multi-label classification tasks that does not require special adjustment of class weights or thresholds. Category Imbalance: From Single-label to Multi-label Generally, multi-class classification refers to single-label classification, i.e., selecting $1$ target category from $n$ candidate categories. Assuming the scores for each class are $s_1, s_2, \\dots, s_n$ and the target class is $t \\in \\{1, 2, \\dots, n\\}$, the loss used is: \\begin{equation}-\\log \\frac{e^{s_t}}{\\sum\\limits_{i=1"}, {"file": "translation_7367.html", "title": "The Memory-Saving Recomputation Technique Now Has a Keras Version", "content": "← Back to Index The Memory-Saving Recomputation Technique Now Has a Keras Version By 苏剑林 | April 29, 2020 Many readers recently might have noticed the official account article \"BERT Recomputation: Saving 5x memory overhead with 22.5% training time (including code)\" . It introduced a technique called \"recomputation\" (gradient checkpointing), which is essentially a method to save memory. It allows for a several-fold increase in batch_size at the cost of a slightly slower average training speed. This technique was first proposed in the paper \"Training Deep Nets with Sublinear Memory Cost\" back in 2016, though it doesn't seem to have become particularly popular until now. Exploration The aforementioned article mentioned that this technique has native implementations in PyTorch and PaddlePaddle, but not yet in TensorFlow. However, in reality, since TensorFlow 1.8, TensorFlow has included this functionality. At that time, it was placed in the tf.contrib sub-library. Starting from TensorFlow 1.15, it became a built-in core function of TensorFlow: tf.recompute_grad . After finding tf.recompute_grad , I looked into its usage. After some tinkering, I actually succeeded in using it, successfu"}, {"file": "translation_7381.html", "title": "Variational Autoencoders (Part 5): VAE + BN = Better VAE", "content": "← Back to Index Variational Autoencoders (Part 5): VAE + BN = Better VAE By 苏剑林 | May 06, 2020 In this article, we continue our previous Variational Autoencoder series and analyze how to prevent the \"KL Vanishing\" phenomenon in NLP VAE models. This article is inspired by the ACL 2020 paper \"A Batch Normalized Inference Network Keeps the KL Vanishing Away\" , with further refinements added by the author. It is worth mentioning that the final solution derived in this article is quite concise— simply adding BN (Batch Normalization) to the encoder output followed by a simple scale —but it is indeed very effective and worth a try for readers researching related issues. At the same time, the conclusions are also applicable to general VAE models (including those for CV); in my view, it could even be considered a \"standard configuration\" for VAE models. Finally, a reminder to readers that this is an advanced VAE paper, so please ensure you have a certain understanding of VAEs before reading further. A Simple Review of VAE Here we briefly review the VAE model and discuss the difficulties VAE faces in NLP. For a more detailed introduction to VAE, please refer to the author's previous works: \""}, {"file": "translation_7387.html", "title": "Analysis of the AdaX Optimizer (with Open-Source Implementation)", "content": "← Back to Index Analysis of the AdaX Optimizer (with Open-Source Implementation) By 苏剑林 | May 11, 2020 This article briefly introduces an optimizer called AdaX, from the paper \"AdaX: Adaptive Gradient Descent with Exponential Long Term Memory\" . The reason for introducing this optimizer is that it once again confirms a conclusion mentioned in my previous post \"Analysis of the AdaFactor Optimizer (with Open-Source Implementation)\" ; the two articles can be read in comparison. Adam & AdaX The update format of AdaX is: \\begin{equation}\\left\\{\\begin{aligned}&g_t = \\nabla_{\\theta} L(\\theta_t)\\\\\n&m_t = \\beta_1 m_{t-1} + \\left(1 - \\beta_1\\right) g_t\\\\\n&v_t = (1 + \\beta_2) v_{t-1} + \\beta_2 g_t^2\\\\\n&\\hat{v}_t = v_t\\left/\\left(\\left(1 + \\beta_2\\right)^t - 1\\right)\\right.\\\\\n&\\theta_t = \\theta_{t-1} - \\alpha_t m_t\\left/\\sqrt{\\hat{v}_t + \\epsilon}\\right.\n\\end{aligned}\\right.\\end{equation} Where the default value of $\\beta_2$ is $0.0001$. By the way, here is my Keras implementation: https://github.com/bojone/adax For comparison, the update format of Adam is: \\begin{equation}\\left\\{\\begin{aligned}&g_t = \\nabla_{\\theta} L(\\theta_t)\\\\\n&m_t = \\beta_1 m_{t-1} + \\left(1 - \\beta_1\\right) g_t\\\\\n&v_t = "}, {"file": "translation_7388.html", "title": "From EMD and WMD to WRD: Similarity Calculation of Text Vector Sequences", "content": "← Back to Index From EMD and WMD to WRD: Similarity Calculation of Text Vector Sequences By 苏剑林 | May 13, 2020 In NLP, we often need to compare the similarity of two sentences. The standard method is to find a way to encode the sentences into fixed-size vectors and then use a geometric distance (Euclidean distance, $\\cos$ distance, etc.) as the similarity. This scheme is relatively simple and allows for fast retrieval, satisfying engineering requirements to a certain extent. Additionally, one can directly compare the differences between two variable-length sequences, such as using Edit Distance, which finds the optimal mapping between two strings through dynamic programming and then calculates the degree of mismatch. Nowadays, we also have tools like Word2Vec and BERT, which can convert text sequences into corresponding vector sequences. Therefore, it is also possible to directly compare the differences between these two vector sequences, rather than first condensing the vector sequence into a single vector. The latter scheme is relatively slower but allows for a finer comparison and is theoretically more elegant, so it has certain application scenarios. This article briefly introd"}, {"file": "translation_7427.html", "title": "Have Your Cake and Eat It Too: The SimBERT Model for Joint Retrieval and Generation", "content": "← Back to Index Have Your Cake and Eat It Too: The SimBERT Model for Joint Retrieval and Generation By 苏剑林 | May 18, 2020 Some time ago, we released the model weights for a project called SimBERT . It is based on Google's open-source BERT model and uses Microsoft's UniLM ideology to design a task that integrates retrieval and generation. After further fine-tuning, the resulting model possesses both the ability to generate similar questions and retrieve similar sentences. At the time of the release, we only provided a weight file and an example script without further explaining the model's principles and training process. In this article, we will supplement that information. Open Source Address: https://github.com/ZhuiyiTechnology/simbert UniLM UniLM (Unified Language Model) is a Transformer model that fuses NLU (Natural Language Understanding) and NLG (Natural Language Generation) capabilities. It was proposed by Microsoft in May last year, and in February this year, it was upgraded to v2 . Our previous article \"From Language Models to Seq2Seq: Transformer is All About the Mask\" briefly introduced UniLM, and it has already been integrated into bert4keras . The core of UniLM is the "}, {"file": "translation_7430.html", "title": "Google's New Work Synthesizer: We Still Don't Understand Self-Attention Well Enough", "content": "← Back to Index Google's New Work Synthesizer: We Still Don't Understand Self-Attention Well Enough By 苏剑林 | May 25, 2020 The box of deep learning is far blacker than we imagine. Writing at the Start The physicist Richard Feynman is credited with saying [ Source ]: \"If you think you understand quantum mechanics, you don't understand quantum mechanics.\" I increasingly feel that the \"quantum mechanics\" in this sentence could be replaced with \"deep learning.\" Although deep learning has proven its effectiveness in more and more fields, our explainability of it remains quite weak. Of course, in recent years, many efforts have been committed to opening this black box of deep learning, but unfortunately, these works are basically \"hindsight\" explanations—proposing explanations that barely convince oneself based on existing experimental results, failing to construct and understand the model's principles from the top down, let alone making forward-looking predictions. This article focuses on the self-attention mechanism. Intuitively, self-attention is considered one of the more interpretable models; it automatically captures correlations between tokens through self-with-self attention. In f"}, {"file": "translation_7466.html", "title": "Generalization Ramblings: From Random Noise and Gradient Penalty to Virtual Adversarial Training", "content": "← Back to Index Generalization Ramblings: From Random Noise and Gradient Penalty to Virtual Adversarial Training By 苏剑林 | June 1, 2020 Improving the generalization performance of a model is one of the primary goals of machine learning. Common methods to improve generalization mainly fall into two categories: the first is adding noise, such as adding Gaussian noise to the input, using Dropout in intermediate layers, and the currently popular adversarial training; data augmentation techniques like random translation and scaling of images also belong to this category in a sense. The second is adding regularization terms to the loss, such as $L_1, L_2$ penalties, gradient penalties, etc. This article attempts to explore the connections between several common means of improving generalization performance. Random Noise Let the model be $f(x)$, $\\mathcal{D}$ be the training dataset, and $l(f(x), y)$ be the loss for a single sample. Our optimization objective is: \\begin{equation}\\mathop{\\text{argmin}}_{\\theta} L(\\theta)=\\mathbb{E}_{(x,y)\\sim \\mathcal{D}}[l(f(x), y)]\\end{equation} where $\\theta$ represents the trainable parameters within $f(x)$. If we add noise $\\varepsilon$ to the model in"}, {"file": "translation_7469.html", "title": "Why Gradient Clipping Accelerates Training: A Concise Analysis", "content": "← Back to Index Why Gradient Clipping Accelerates Training: A Concise Analysis By 苏剑林 | June 05, 2020 This article introduces a perfect-score paper from MIT presented at ICLR 2020 titled \"Why gradient clipping accelerates training: A theoretical justification for adaptivity\" . As the name suggests, this paper analyzes why gradient clipping can accelerate the training process in deep learning. The original paper is very long, filled with formulas, and contains many concepts from complexity research. To be honest, I was also confused by much of its content, but I was able to capture its core idea: it introduces a more relaxed constraint than the commonly used L-constraint and demonstrates the necessity of gradient clipping based on these new conditions. This article aims to provide a brief analysis of this process for the readers' reference. Gradient Clipping Assume the function to be minimized is $f(\\theta)$, where $\\theta$ represents the optimization parameters. The update formula for gradient descent is: \\begin{equation}\\theta \\leftarrow \\theta-\\eta \\nabla_{\\theta} f(\\theta)\\end{equation} where $\\eta$ is the learning rate. So-called gradient clipping involves scaling the update am"}, {"file": "translation_7476.html", "title": "Unsupervised Word Segmentation and Syntax Parsing! It turns out BERT can be used this way", "content": "← Back to Index Unsupervised Word Segmentation and Syntax Parsing! It turns out BERT can be used this way By 苏剑林 | June 10, 2020 The standard usage of BERT is to load its pre-trained weights, attach a small number of new layers, and then fine-tune it on downstream tasks. In other words, its general usage is typically supervised training. Based on this workflow, we can perform Chinese word segmentation, NER, and even syntax parsing—tasks that most people have heard of, even if they haven't implemented them. However, it might be surprising and interesting to learn that one can perform word segmentation and even extract syntactic structures directly from pre-trained BERT without any fine-tuning. This article introduces the ACL 2020 paper \"Perturbed Masking: Parameter-free Probing for Analyzing and Interpreting BERT\" . It provides a method for analyzing and interpreting BERT by directly utilizing the Masked Language Model (MLM). Using this idea, we can achieve unsupervised word segmentation and syntax parsing. Correlation Matrix This article is recommended to be read in conjunction with the following posts: \"[Chinese Word Segmentation Series] 2. New Word Discovery Based on Segmentation"}, {"file": "translation_7500.html", "title": "How to Deal with the \"Unending Generation\" Problem in Seq2Seq?", "content": "← Back to Index How to Deal with the \"Unending Generation\" Problem in Seq2Seq? By 苏剑林 | June 16, 2020 In the decoding process of Seq2Seq, we generate tokens recursively one by one until the <eos> tag appears; this is the so-called \"autoregressive\" generation model. However, readers who have studied Seq2Seq may have noticed that this autoregressive decoding occasionally exhibits a phenomenon where it \"simply cannot stop.\" This is mainly characterized by the repeated appearance of a certain fragment, such as \"The weather today is great great great great great...\" or \"Don't you think what I said is right right right right right...\", but it stubbornness refuses to produce the <eos> tag. The ICML 2020 paper \"Consistency of a Recurrent Language Model With Respect to Incomplete Decoding\" systematically discusses this phenomenon and proposes some countermeasures. This article will briefly introduce the main content of that paper. Decoding Algorithms For autoregressive models, we build a conditional language model as follows:\n    \\begin{equation}p(y_t|y_{\\lt t}, x)\\label{eq:p}\\end{equation}\n    The decoding algorithm then outputs the corresponding $y=(y_1,y_2,\\dots,y_T)$ for a given $x$ bas"}, {"file": "translation_7515.html", "title": "Record of the Solar Eclipse", "content": "← Back to Index Record of the Solar Eclipse By 苏剑林 | June 21, 2020 A simple and successful solar eclipse observation (June 21, 2020 16:02 Shajing, Bao'an, Shenzhen) Citation This is a machine translation of the original Chinese article: https://kexue.fm/archives/7515 Original author: 苏剑林 (Su Jianlin) Original publication: 科学空间 (Scientific Spaces) Translated using Gemini 3 Flash. Please refer to the original for authoritative content."}, {"file": "translation_7521.html", "title": "Optimization via Sampling: A Unified Perspective on Differentiable and Non-Differentiable Optimization", "content": "← Back to Index Optimization via Sampling: A Unified Perspective on Differentiable and Non-Differentiable Optimization By 苏剑林 | June 23, 2020 Many readers are likely aware that the inconsistency between loss functions and evaluation metrics is a classic phenomenon in machine learning. For instance, in classification problems, the loss function uses cross-entropy while the evaluation metric is accuracy or F1 score. Similarly, in text generation, the loss function is cross-entropy in a teacher-forcing format, while the evaluation metrics are BLEU, ROUGE, etc. Ideally, we would want to optimize whatever metric we are evaluating. However, evaluation metrics are usually non-differentiable, whereas most of our optimizers are gradient-based, which requires the target to be differentiable. This is the source of the inconsistency. A few days ago, I came across a paper on arXiv titled \"MLE-guided parameter search for task loss minimization in neural sequence modeling.\" As the name suggests, it investigates how to directly optimize evaluation metrics for text generation. Upon reading it, I found this paper highly valuable. In fact, it provides a new way of thinking about optimizing evaluation"}, {"file": "translation_7533.html", "title": "Integrated Gradients: A Novel Neural Network Visualization Method", "content": "← Back to Index Integrated Gradients: A Novel Neural Network Visualization Method By 苏剑林 | June 28, 2020 This article introduces a neural network visualization method: Integrated Gradients. It was first proposed in the paper \"Gradients of Counterfactuals\" and subsequently introduced again in \"Axiomatic Attribution for Deep Networks\" . Both papers share the same authors and largely the same content, though the latter is relatively easier to understand; if you plan to read the original research, I recommend the latter. Of course, this work dates back to 2016–2017. Calling it \"novel\" refers to its innovative and interesting approach, rather than its recentness. Neural network visualization, simply put, means identifying which components of an input $x$ significantly influence the decision of a model $F(x)$, or ranking the importance of the individual components of $x$—a process professionally known as \"attribution.\" A naive approach is to use the gradient $\\nabla_x F(x)$ directly as an indicator of importance for each component of $x$. Integrated Gradients is an improvement upon this idea. However, I believe many articles explaining Integrated Gradients (including the original papers)"}, {"file": "translation_7546.html", "title": "Exploration of Linear Attention: Does Attention Need a Softmax?", "content": "← Back to Index Exploration of Linear Attention: Does Attention Need a Softmax? By 苏剑林 | July 04, 2020 As is well known, although Transformer-based models using the Attention mechanism have excellent parallel performance, their spatial and temporal complexity are both $\\mathcal{O}(n^2)$, where $n$ is the sequence length. Therefore, when $n$ is large, the computational cost of Transformer models becomes unbearable. Recently, many works have been dedicated to reducing the computational volume of Transformer models, such as model pruning, quantization, distillation, and other compression techniques, or modifying the Attention structure to reduce its complexity to $\\mathcal{O}(n \\log n)$ or even $\\mathcal{O}(n)$. A few days ago, I read the paper \"Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention\" and learned about the exploration of Linear Attention. Subsequently, I read some related literature and gained some good insights. Finally, I have summarized my understanding of Linear Attention in this article. Attention The most popular Attention mechanism today is Scaled-Dot Attention , in the form:\n\\begin{equation}Attention(\\boldsymbol{Q},\\boldsymbol{K},\\boldsym"}, {"file": "translation_7574.html", "title": "Powerful NVAE: You Can No Longer Say VAE Generated Images are Blurry", "content": "← Back to Index Powerful NVAE: You Can No Longer Say VAE Generated Images are Blurry By 苏剑林 | July 10, 2020 Yesterday morning, while doing my daily scroll through arXiv, I was shocked by a newly released paper! The paper is titled \"NVAE: A Deep Hierarchical Variational Autoencoder\" . As the name suggests, it is an improvement on VAE, proposing a new model called NVAE. To be honest, when I clicked on it, I didn't have high hopes because I consider myself to have some understanding of VAEs and felt that their generative capabilities were ultimately limited. However, when the paper opened, the visuals looked like this: NVAE face generation results And my first reaction was: W!T!F! Is this really the result of a VAE? Is this still the VAE I know? It seems my understanding of VAE was too shallow; I can never say VAE-generated images are blurry again... But after looking at the author's affiliation—NVIDIA—it became somewhat understandable. In recent years, you might have noticed that NVIDIA usually releases a breakthrough in generative models towards the end of the year: 2017 was PGGAN , 2018 was StyleGAN , and 2019 was StyleGAN2 . This year seems a bit earlier and more prolific, as they"}, {"file": "translation_7575.html", "title": "BERT-of-Theseus: A Model Compression Method Based on Module Replacement", "content": "← Back to Index BERT-of-Theseus: A Model Compression Method Based on Module Replacement By 苏剑林 | July 17, 2020 Recently, I learned about a BERT model compression method called \"BERT-of-Theseus,\" from the paper \"BERT-of-Theseus: Compressing BERT by Progressive Module Replacing\" . This is a model compression scheme built around the starting point of \"replaceability.\" Compared to conventional means such as pruning and distillation, the entire process is more elegant and concise. In this article, I will give a brief introduction to the method, provide an implementation based on bert4keras , and verify its effectiveness. BERT-of-Theseus, original illustration Model Compression First, let's briefly introduce model compression. However, since I am not a specialist in model compression and haven't conducted a particularly systematic investigation, this introduction might appear unprofessional; I hope readers will understand. Basic Concepts Simply put, model compression is \"simplifying a large model to obtain a small model with faster inference speed.\" Of course, model compression generally involves certain sacrifices; most notably, the final evaluation metrics will drop to some extent. Aft"}, {"file": "translation_7611.html", "title": "A Few More Words on the \"China Adolescents Science & Technology Innovation Contest\"", "content": "← Back to Index A Few More Words on the \"China Adolescents Science & Technology Innovation Contest\" By 苏剑林 | July 18, 2020 By 苏剑林 | July 18, 2020 Recently, the \"China Adolescents Science & Technology Innovation Contest\" (CASTIC) has gone viral. The reason is simple: almost every published winning entry is at the level of a Master's or even a Doctoral thesis, and many are no less impressive than articles published in well-known journals. Yet, the authors of these works are mere middle school or even elementary school students. They have crossed all sorts of \"natural chasms\" to complete scientific research projects that would be considered \"celestial scripts\" to many people, including professional Master's and PhD students. This list of winners has stirred up a massive outcry on the internet, making us \"melon-eating masses\" feel the overwhelming power of the hòulàng (\"after-waves\" / younger generation). The situation is still developing; gradually, investigation teams are being formed, statements are being issued, apologies are being made for \"over-participation,\" and some are strictly insisting they \"did not participate.\" Onlookers are enjoying the unfolding drama immensely. In the "}, {"file": "translation_7615.html", "title": "Mitigating Class Imbalance via Mutual Information Thinking", "content": "← Back to Index Mitigating Class Imbalance via Mutual Information Thinking By 苏剑林 | July 19, 2020 Class imbalance, also known as the \"long-tail problem,\" is one of the common challenges faced in machine learning, especially with datasets derived from real-world scenarios, which are almost always imbalanced. About two years ago, I also thought about this problem. At the time, I happened to have some insights into \"mutual information,\" so I conceived a solution based on that idea. However, upon further reflection, the logic seemed too commonplace, so I didn't pursue it further. Yet, a few days ago, I came across an article from Google on arXiv titled \"Long-tail learning via logit adjustment\" and was surprised to find that it contained almost exactly the same method I had originally conceived. This made me realize that my initial idea could actually achieve SOTA (state-of-the-art) performance. Therefore, combining that paper with my original thought process, I have organized the following content, hoping readers won't dismiss it as \"Monday morning quarterbacking.\" Problem Description The primary concern here is the single-label multi-classification problem. Suppose there are $K$ candi"}, {"file": "translation_7630.html", "title": "BERT That Learns to Ask: End-to-End Construction of Q&A Pairs from Passages", "content": "← Back to Index BERT That Learns to Ask: End-to-End Construction of Q&A Pairs from Passages By 苏剑林 | July 25, 2020 Machine Reading Comprehension (MRC) tasks are likely familiar to many readers. Simply put, it involves finding the answer to a given question from a given passage—a process of \"Passage + Question → Answer.\" I have previously written articles on reading comprehension, such as \"CNN-based Machine Reading Comprehension Question Answering Model: DGCNN\" . Constructing Q&A pairs is essentially the inverse task of reading comprehension: the process of \"Passage → Answer + Question.\" In academia, this is generally referred to as \"Question Generation (QG).\" Since answers can often be selected through relatively regular random selection in most cases, many papers focus only on the \"Passage + Answer → Question\" step. This article presents a fully end-to-end practice of \"Passage → Answer + Question,\" including the model introduction and implementation code based on bert4keras . Readers are welcome to try it out. First, See the Results Input Passage : Mount K2 is the second highest peak in the world, located in China. Generated Q&A : What is the name of the second highest mountain in"}, {"file": "translation_7643.html", "title": "Do We Really Need to Reduce Training Set Loss to Zero?", "content": "← Back to Index Do We Really Need to Reduce Training Set Loss to Zero? By 苏剑林 | July 31, 2020 When training a model, do we need the loss function to be trained all the way down to 0? Obviously not. Generally speaking, we use a training set to train the model, but what we hope for is that the loss on the validation set is as small as possible. Normally, after the training set loss drops to a certain value, the validation set loss will start to rise. Therefore, there is no need to reduce the training set loss to 0. That being the case, after a certain threshold has already been reached, can we do something else to improve model performance? The ICML 2020 paper \"Do We Need Zero Training Loss After Achieving Zero Training Error?\" answers this question. However, the paper's answer is limited only to the level of \"what it is\" and does not describe \"why\" very well. Additionally, after reading the interpretation by the expert kid丶 on Zhihu, I still didn't find the answer I wanted. Therefore, I analyzed it myself and recorded it here. Description of Ideas The solution provided by the paper is very simple. Suppose the original loss function is $\\mathcal{L}(\\theta)$, it is now modified to $\\t"}, {"file": "translation_7661.html", "title": "Modifying Transformer Architecture to Design a Faster and Better MLM Model", "content": "← Back to Index Modifying Transformer Architecture to Design a Faster and Better MLM Model By 苏剑林 | August 07, 2020 As is well-known, MLM (Masked Language Model) is the pre-training method for BERT and RoBERTa. As the name suggests, it involves masking some tokens from the original sequence and then letting the model predict these masked tokens. As research has deepened, it has been discovered that MLM is not only valuable as a pre-training method but also has a wealth of practical applications. For instance, I previously found that directly loading the MLM weights of BERT allows it to be used as a UniLM for Seq2Seq tasks (refer here ). Another example is the paper published in ACL 2020, \"Spelling Error Correction with Soft-Masked BERT\" , which applies the MLM model to text error correction. However, anyone who has read the BERT paper carefully or tried it personally knows that the training efficiency of the original MLM is relatively low. This is because only a small fraction of tokens are masked in each pass. The ACL 2020 paper \"Fast and Accurate Deep Bidirectional Language Representations for Unsupervised Learning\" also addresses this issue and proposes a new MLM model design th"}, {"file": "translation_7681.html", "title": "L2 Regularization is Not as Good as Imagined? It Might Be the Fault of \"Weight Scale Shifting\"", "content": "← Back to Index L2 Regularization is Not as Good as Imagined? It Might Be the Fault of \"Weight Scale Shifting\" By 苏剑林 | August 14, 2020 By Su Jianlin | August 14, 2020 L2 regularization is a commonly used method in machine learning to prevent overfitting (and is likely a frequent interview question). Simply put, it aims to keep the magnitude of weights as small as possible so that the model can withstand more perturbations, ultimately improving its generalization performance. However, readers may also find that the performance of L2 regularization is often not as good as theory suggests; in many cases, adding it might even have a negative effect. A recent paper, \"Improve Generalization and Robustness of Neural Networks via Weight Scale Shifting Invariant Regularizations\" , analyzes the drawbacks of L2 regularization from the perspective of \"Weight Scale Shifting\" and proposes a new WEISSI regularization term. The entire analysis process is quite interesting, and I would like to share it with you here. Related Content In this section, we first briefly review L2 regularization, then introduce its connection to weight decay and the related AdamW optimizer. Understanding L2 Regularizat"}, {"file": "translation_7695.html", "title": "The Principle of Minimum Entropy (Part 6): How Should We Choose Word Embedding Dimensions?", "content": "← Back to Index The Principle of Minimum Entropy (Part 6): How Should We Choose Word Embedding Dimensions? By 苏剑林 | August 20, 2020 With the development of NLP, word embedding models like Word2Vec and GloVe are gradually being replaced by Transformer-based models such as BERT. However, classics remain classics; word embedding models still shine in many scenarios and there is still much worth studying. In this article, we address a common point of confusion regarding word embedding models: what dimension is roughly sufficient for word vectors? First, let's give the conclusion—the estimated result provided by the author is: \\begin{equation}n > 8.33\\log N\\label{eq:final}\\end{equation} To be even more concise, one can simply remember $n > 8\\log N$, where $N$ is the vocabulary size and $n$ is the word vector dimension, with $\\log$ being the natural logarithm. When $n$ exceeds this threshold, it indicates that the model has sufficient capacity to accommodate these $N$ words (of course, a larger $n$ also increases the risk of overfitting). Thus, when $N=100,000$, the resulting $n$ is approximately 96; so for a word embedding model with 100,000 words, a dimension of 96 is sufficient. If yo"}, {"file": "translation_7708.html", "title": "Revisiting Class Imbalance: Comparison and Connection between Weight Adjustment and Custom Loss Functions", "content": "← Back to Index Revisiting Class Imbalance: Comparison and Connection between Weight Adjustment and Custom Loss Functions By 苏剑林 | August 31, 2020 The class imbalance problem, also known as the long-tail distribution problem, has been discussed several times on this blog in posts such as \"From Hard Truncation and Softening of Loss to Focal Loss\" , \"Generalizing 'Softmax + Cross Entropy' to Multi-label Classification\" , and \"Mitigating Class Imbalance through Mutual Information Ideology\" . For alleviating class imbalance, the most basic method is adjusting sample weights, while more \"high-end\" approaches involve various modifications to the loss function (such as Focal Loss, Dice Loss, Logits Adjustment, etc.). This article aims to systematically understand the connections between them. Long-tail distribution: A few categories have a very large number of samples, while most categories have a very small number of samples. From Smooth Accuracy to Cross-Entropy The analysis here is primarily based on binary classification with sigmoid, but most conclusions can be generalized to multi-class classification with softmax. Let $x$ be the input, $y \\in \\{0,1\\}$ be the target, and $p_{\\theta}"}, {"file": "translation_7718.html", "title": "Let's Build a DialoGPT: A Generative Multi-turn Dialogue Model Based on Language Models", "content": "← Back to Index Let's Build a DialoGPT: A Generative Multi-turn Dialogue Model Based on Language Models By 苏剑林 | September 07, 2020 A while ago, while browsing Arxiv, I noticed that Tsinghua University open-sourced a large-scale cleaned Chinese conversation corpus called LCCC ( paper link , project address ). Based on the open-sourced files, this might be the largest and highest-quality open-source chitchat corpus currently available, and it even includes some multi-turn conversations. Overall, it's quite playable. I was drawn to it and tried using it to train a chitchat dialogue model. The results look pretty good, so I’m sharing my experience here. Corpus Introduction Here’s a brief introduction to the LCCC dataset (Large-scale Cleaned Chinese Conversation). For specific details, you can visit the GitHub page; the download links are also there. LCCC is divided into \"base\" and \"large\" versions. The \"base\" version mainly originates from Weibo conversations, while \"large\" integrates other open-source dialogue corpora on top of \"base.\" According to the authors, LCCC underwent a rigorous cleaning process, so the overall quality appears to be very high. \\[\\begin{array}{c|c|c}\n    \\hlin"}, {"file": "translation_7725.html", "title": "Variational Autoencoder (VI): An Attempt to Understand VAE from a Geometric Perspective", "content": "← Back to Index Variational Autoencoder (VI): An Attempt to Understand VAE from a Geometric Perspective By 苏剑林 | September 10, 2020 Some time ago, during a technical sharing session at the company, it was the author's turn to present. Everyone hoped I would talk about VAE. Given that I have previously written a Variational Autoencoder series , I thought it wouldn't be particularly difficult, so I agreed. However, on second thought, I found myself in a dilemma: how should I present it? Regarding VAE, I previously wrote two systematic introductions: \"Variational Autoencoder (I): So That's How It Is\" and \"Variational Autoencoder (II): From a Bayesian Perspective\" . The latter is pure probabilistic derivation, which might not be meaningful or easily understood by those not doing theoretical research. While the former is simpler, it is also somewhat inadequate because it explains things from the perspective of generative models without clearly explaining \"why VAE is needed\" (to put it plainly, VAE can result in a generative model, but VAE is not necessarily just for generative models), and the overall style is not particularly friendly. I thought about it, and for most readers who don't"}, {"file": "translation_7737.html", "title": "Policy Gradient and Zeroth-Order Optimization: Different Paths to the Same Destination", "content": "← Back to Index Policy Gradient and Zeroth-Order Optimization: Different Paths to the Same Destination By 苏剑林 | September 15, 2020 A huge reason for the immense success of deep learning is that gradient-based optimization algorithms (SGD, Adam, etc.) can effectively solve most neural network models. However, since they are based on gradients, they require the model to be differentiable. As research progresses, we often find ourselves needing to solve non-differentiable models. Typical examples include directly optimizing metrics like accuracy, F1, or BLEU, or incorporating non-differentiable modules (such as \"skip-reading\" operations) within a neural network. This article will briefly introduce two effective methods for solving non-differentiable models: Policy Gradient, one of the most important methods in reinforcement learning, and Zeroth-Order Optimization, which requires no gradients at all. On the surface, these two optimization methods seem to have completely different approaches, but this article will further demonstrate that for a large class of optimization problems, the two are essentially equivalent. Formal Description First, let us formally define the problem we need t"}, {"file": "translation_7758.html", "title": "Faster and Just as Good: Word-Based Chinese WoBERT", "content": "← Back to Index Faster and Just as Good: Word-Based Chinese WoBERT By 苏剑林 | September 18, 2020 Currently, most Chinese pre-trained models use characters as the basic unit, meaning that Chinese sentences are split into individual characters. There are also some multi-granularity Chinese language models, such as Innovation Works' ZEN and ByteDance's AMBERT , but the basic unit of these models is still characters, with mechanisms added to fuse word information. Currently, there are very few Chinese pre-trained models purely based on words; as far as the author knows, only Tencent UER has open-sourced a word-granularity BERT model , but its practical performance was not ideal. So, how effective is a purely word-based Chinese pre-trained model? Does it have any value? Recently, we pre-trained and open-sourced a word-based Chinese BERT model, which we call WoBERT (Word-based BERT, or \"My BERT!\" in Chinese), and experiments show that word-based WoBERT has unique advantages in many tasks, such as significant speed improvements, while performance remains roughly the same or even improves. Here is a summary of our work. Characters or Words? Is \"character\" or \"word\" better? This is a frustrat"}, {"file": "translation_7764.html", "title": "Must it be GPT3? No, BERT's MLM model can also do few-shot learning", "content": "← Back to Index Must it be GPT3? No, BERT's MLM model can also do few-shot learning By 苏剑林 | September 27, 2020 Everyone knows that GPT3 is currently in the spotlight. However, with GPT3 being promoted everywhere, do readers remember the actual title of the GPT3 paper? In fact, the GPT3 paper is titled \"Language Models are Few-Shot Learners.\" The title no longer contains the letters G, P, or T, but because it is a direct descendant of the original GPT, it is still referred to as GPT3. As the name suggests, GPT3 focuses on Few-Shot Learning. Additionally, another characteristic of GPT3 is its size—the largest version has 175 billion parameters, which is more than a thousand times the size of BERT Base. Because of this, a paper titled \"It's Not Just Size That Matters: Small Language Models Are Also Few-Shot Learners\" appeared on Arxiv recently and caught my attention. Translated, it means \"Who says it has to be big? Small models can also do few-shot learning.\" Obviously, this title is aimed directly at GPT3. I was curious to see who had the courage to challenge GPT3 and what kind of small model could do it. After reading it, I realized the authors proposed that with proper constructi"}, {"file": "translation_7782.html", "title": "The 1,000th Article", "content": "← Back to Index The 1,000th Article By 苏剑林 | September 29, 2020 The backend indicates that this is the 1,000th article on Scientific Spaces. I originally intended to write the next serious article, but seeing this prompt, I’ll first write a casual \"filler\" post to commemorate it. They say as people get older, they like to get sentimental over everything, and it seems that’s true. Seeing others write reflections on the college entrance exam, writing a reflection for a blog's tenth anniversary, and now writing one for the 1,000th article—it’s as if I’m always looking for a reason to be nostalgic. So, what can I ramble about today? First, a bit of narcissism. One thousand articles—if they were printed out, even at one page per article, that would be a 1,000-page book. I believe many people have never even held a 1,000-page book (though I actually have read one, as evidenced in my post: \"Haha, my 'Bible' has arrived\" ). To think I’ve actually written 1,000 posts—I’m quite impressed with myself. Of course, some of the early articles were reposts and not entirely my own writing, but I’ve stuck with a lot of original content. Even the reposted ones were edited and organized by me rather t"}, {"file": "translation_7787.html", "title": "Optimization Algorithms from a Dynamical Perspective (V): Why the Learning Rate Should Not Be Too Small?", "content": "← Back to Index Optimization Algorithms from a Dynamical Perspective (V): Why the Learning Rate Should Not Be Too Small? By 苏剑林 | October 10, 2020 The theme of this article is \"why we need a finite learning rate.\" By \"finite,\" we mean the rate should be neither too large nor too small, but just right. It is easy to understand that a learning rate that is too large can lead to the divergence of the algorithm. But why is a learning rate that is too small also undesirable? An easy-to-understand answer is that a learning rate that is too small requires an excessive number of iterations, which is a waste of time and computational power. Therefore, from the perspective of \"efficiency\" and \"acceleration,\" we avoid using excessively small learning rates. However, if we set aside the factors of computational power and time, would an extremely small learning rate be desirable? A recent paper published by Google on arXiv, \"Implicit Gradient Regularization,\" attempts to answer this question. It points out that a finite learning rate implicitly introduces a gradient penalty term into the optimization process, which is beneficial for improving generalization performance. Therefore, even without "}, {"file": "translation_7805.html", "title": "How to split a validation set that is closer to the test set?", "content": "← Back to Index How to split a validation set that is closer to the test set? By 苏剑林 | Oct 16, 2020 Whether in competitions, experiments, or engineering, we often encounter situations where the distribution of the training set and the test set is inconsistent. Generally, we split a validation set from the training set to tune hyperparameters (refer to \"The Significance of Training, Validation, and Test Sets\" ), such as controlling the number of training epochs to prevent overfitting. However, if the validation set itself is quite different from the test set, a model that performs well on the validation set does not necessarily perform well on the test set. Therefore, how to make the distribution of the split validation set closer to the test set is a topic worth researching. Two Cases First, clarify that what this article considers is a scenario where we can obtain the test set data itself but do not know the test set labels. If it is a scenario where models are submitted for closed evaluation and we cannot see the test set at all, there is not much we can do. Why does the phenomenon of inconsistent training and test set distributions occur? There are mainly two situations. The fir"}, {"file": "translation_7809.html", "title": "How Many Grades Can BERT Attend? A \"Hardline\" Seq2Seq Approach to Primary School Math Word Problems", "content": "← Back to Index How Many Grades Can BERT Attend? A \"Hardline\" Seq2Seq Approach to Primary School Math Word Problems By 苏剑林 | October 19, 2020 Those years of \"Chickens and Rabbits in a Cage,\" \"Surplus and Deficiency Problems,\" \"Age Problems,\" \"Tree Planting Problems,\" \"Cattle Eating Grass Problems,\" \"Profit Problems\"... During your primary school years, were you ever tortured by all kinds of fancy mathematical word problems? It's okay; now machine learning models can help us solve word problems too. Let's see what grade level it can reach! This article will provide a baseline for solving primary school Math Word Problems (MWP), trained on the ape210k dataset, directly using a Seq2Seq model to generate executable mathematical expressions. Ultimately, the Large version of the model achieved an accuracy of 75%, significantly higher than the results reported in the ape210k paper. The so-called \"hardline\" approach refers to directly generating human-readable expressions similar to how a person would solve them, without special expression transformations or template processing. Data Processing First, let's observe the situation of the ape210k dataset: {\n \"id\": \"254761\",\n \"segmented_text\":"}, {"file": "translation_7818.html", "title": "TeaForN: Making Teacher Forcing a Bit More \"Farsighted\"", "content": "← Back to Index TeaForN: Making Teacher Forcing a Bit More \"Farsighted\" By 苏剑林 | October 27, 2020 Teacher Forcing is the classic training method for Seq2Seq models, and Exposure Bias is the classic deficiency of Teacher Forcing. This is a well-known fact for students working on text generation. I have previously written a blog post \"A Brief Analysis and Countermeasures for the Exposure Bias Phenomenon in Seq2Seq\" , which initially analyzed the Exposure Bias problem. This article introduces a scheme proposed by Google called \" TeaForN \" to alleviate the Exposure Bias phenomenon, from the paper \"TeaForN: Teacher-Forcing with N-grams\" . Through a nested iterative approach, it allows the model to predict the next $N$ tokens in advance (rather than just the current token to be predicted). The logic behind its approach is quite remarkable and worth learning. (Note: To maintain consistency with previous articles on this blog, the notation in this article differs from that in the original paper. Please focus on understanding the meaning of the symbols rather than memorizing their specific forms.) Teacher Forcing The article \"A Brief Analysis and Countermeasures for the Exposure Bias Phenom"}, {"file": "translation_7846.html", "title": "Before using ALBERT and ELECTRA, make sure you really understand them", "content": "← Back to Index Before using ALBERT and ELECTRA, make sure you really understand them By 苏剑林 | October 29, 2020 In the world of pre-trained language models, ALBERT and ELECTRA can be considered two \"rising stars\" that followed BERT. They improved upon BERT from different perspectives and ultimately enhanced performance (at least on many public benchmark datasets), thus earning a certain reputation. However, in daily exchanges and learning, I have found that many friends have misunderstandings about these two models, leading to unnecessary time wasted during use. Here, I attempt to summarize some key points of these two models for your reference, hoping you can avoid detours when using them. ALBERT and ELECTRA (Note: In this article, the word \"BERT\" refers to both the initially released BERT model and its subsequent improved version, RoBERTa. We can think of BERT as an insufficiently trained RoBERTa, and RoBERTa as a more fully trained BERT. This article focuses on their comparison with ALBERT and ELECTRA, so it does not distinguish between BERT and RoBERTa.) ALBERT ALBERT comes from the paper \"ALBERT: A Lite BERT for Self-supervised Learning of Language Representations.\" As the nam"}, {"file": "translation_7867.html", "title": "The T5 Model That Dominated the Leaderboards Can Now Be Used in Chinese", "content": "← Back to Index The T5 Model That Dominated the Leaderboards Can Now Be Used in Chinese By 苏剑林 | November 06, 2020 I wonder if everyone still has an impression of Google's chart-topping work T5 from last year? That's the model that, under the banner of \"Everything is Seq2Seq,\" scaled up to 11 billion parameters and swept multiple NLP leaderboards like GLUE and SuperGLUE. Even after a year, T5 still holds the top spot on the SuperGLUE leaderboard, currently maintaining a steady 2% lead over second place. However, for friends in the Chinese NLP community, T5 might not have had much presence for a simple reason: there was no Chinese version of T5 available. But this situation is about to change, because Google recently released the multilingual version of T5 (mT5), which naturally includes Chinese. Although it’s not a \"pure\" Chinese version, it’s good enough to make do with. \"Everything is Seq2Seq\" T5 This article will provide a brief review and introduction to the T5 model, and then explain how to call the mT5 model in bert4keras for Chinese tasks. As a native Seq2Seq pre-trained model, mT5 performs quite well on text generation tasks and is well worth a try. T5 Like BERT, T5 is also"}, {"file": "translation_7877.html", "title": "When GPT Meets Chinese Chess: Written Articles, Solved Problems, How About a Game of Chess?", "content": "← Back to Index When GPT Meets Chinese Chess: Written Articles, Solved Problems, How About a Game of Chess? By 苏剑林 | Nov 11, 2020 I wonder if readers have seen the article by QuantumBit at the beginning of the year titled \"The Strongest Writing AI Actually Learned Chess and Composition, Language Model Cross-Border Operations Spark Heated Discussion, Seeking Opponents Online\" . It mentioned that a netizen trained a GPT-2 model to play International Chess. I have always been thinking, how can such an interesting thing not have a Chinese version? For International Chess, the Chinese counterpart is naturally Chinese Chess (Xiangqi). So, I have been wanting to replicate its results on Chinese Chess. After dragging it out for more than half a year, I finally completed it in the last few days. I would like to share it with everyone here. Chess Formula: The General never leaves the Nine Palaces; the Advisors remain by his side and never exit the palace. The Bishops fly across the four directions to the four corners; the Knights move in one step and one diagonal jump. The Cannon must skip over one piece to hit another; the Rook moves in straight lines as it pleases. Only the Pawns move just"}, {"file": "translation_7888.html", "title": "Also Discussing RNN's Gradient Vanishing/Exploding Problem", "content": "← Back to Index Also Discussing RNN's Gradient Vanishing/Exploding Problem By 苏剑林 | November 13, 2020 Although Transformer-based models have occupied most areas of NLP, RNN models such as LSTM and GRU still have their unique value in certain scenarios. Therefore, RNNs are still models worth studying thoroughly. Analyzing RNN gradients is an excellent example of thinking about and analyzing models from an optimization perspective, which is worth careful consideration. You may have noticed that questions like \"Why can LSTM solve gradient vanishing/exploding?\" remain among the most popular interview questions today... Classic LSTM Many netizens have provided answers to such questions. However, after searching through several articles (including some answers on Zhihu, columns, and classic English blogs), the author found that it is difficult to find a truly satisfactory answer: some derivation notations are inherently confusing, some narration processes fail to highlight the key points, and overall, they lack clarity and self-consistency. Therefore, I will attempt to provide my own understanding for your reference. RNN and Its Gradient The unified definition of an RNN is: where $h_t$ i"}, {"file": "translation_7912.html", "title": "Playing with the Currently Largest Chinese GPT-2 Model (bert4keras)", "content": "← Back to Index Playing with the Currently Largest Chinese GPT-2 Model (bert4keras) By 苏剑林 | November 20, 2020 I believe many readers have seen the \"Qingyuan Plan\" launched by Tsinghua University and the Beijing Academy of Artificial Intelligence (BAAI) over the past few days (related link: \"Chinese version of GPT-3? BAAI Releases Qingyuan CPM — A Large-scale Pre-trained Model Centered on Chinese\" ). It open-sourced CPM-LM (2.6 billion parameters), currently the largest Chinese GPT-2 model. It is said that in the future, they will open-source models with 20 billion or even 100 billion parameters to create a \"Chinese version of GPT-3.\" Official Few Shot effect demonstration of CPM-LM We know that GPT-3 can achieve Few Shot learning without fine-tuning. In the current demonstration examples of CPM-LM, the Few Shot performance is also quite impressive, which makes one eager to try it out. Naturally, I wanted to adapt it to my bert4keras to make it easier to use. Thus, the adaptation work began. I originally thought it would be a simple task, but I ended up stumbling into pitfalls for nearly three days before getting it right. Here, I'm recording the process of troubleshooting and test"}, {"file": "translation_7919.html", "title": "The Even-Order Taylor Expansion of exp(x) at x=0 is Always Positive", "content": "← Back to Index The Even-Order Taylor Expansion of exp(x) at x=0 is Always Positive By 苏剑林 | November 24, 2020 I recently came across an interesting conclusion: For any real number $x$ and any even number $n$, it always holds that $\\sum\\limits_{k=0}^n \\frac{x^k}{k!} > 0$. That is, the even-order Taylor expansion of $e^x$ at $x=0$ is always positive. Below, we will look at the proof of this conclusion and its application in finding alternatives to the softmax function. Proof Process This appears to be a very strong result. Will the proof be complicated? In fact, the proof is very simple. Let's denote\n    \\begin{equation}f_n(x) = \\sum\\limits_{k=0}^n \\frac{x^k}{k!}\\end{equation}\n    When $n$ is an even number, we have $\\lim\\limits_{x\\to\\pm\\infty} f_n(x)=+\\infty$, which means the function's global shape is opening upwards. Therefore, we only need to prove that its minimum value is greater than 0. Furthermore, since it is a smooth and continuous polynomial function, its global minimum must occur at one of its local minima. Looking at it from another perspective, we only need to prove that the function values at all its stationary points (whether they are local maxima or minima) are grea"}, {"file": "translation_7921.html", "title": "Performer: Linearizing Attention Complexity with Random Projections", "content": "← Back to Index Performer: Linearizing Attention Complexity with Random Projections By 苏剑林 | December 01, 2020 The $\\mathcal{O}(n^2)$ complexity of the Attention mechanism has been a long-standing issue. There are two main approaches to changing this complexity: one is the sparsification route, such as the Sparse Attention we introduced previously and Big Bird released by Google a few months ago; the other is the linearization route, parts of which we summarized in \"Exploring Linear Attention: Does Attention Need a Softmax?\" . This article introduces a new piece of improvement work, Performer, from Google's paper \"Rethinking Attention with Performers.\" Its goal is quite ambitious: to linearize the complexity of Attention through random projections without losing accuracy. To put it bluntly, in an ideal scenario, we wouldn't need to retrain the model, and the output results wouldn't change significantly, but the complexity would drop to $\\mathcal{O}(n)$! It sounds like a \"pie in the sky\" improvement. Is it really that beautiful? Attention We know that the general definition of Attention is:\n\\begin{equation}Attention(\\boldsymbol{Q},\\boldsymbol{K},\\boldsymbol{V})_i = \\frac{\\sum\\limits"}, {"file": "translation_7947.html", "title": "Hierarchical Decomposition Position Encoding: Enabling BERT to Handle Ultra-Long Text", "content": "← Back to Index Hierarchical Decomposition Position Encoding: Enabling BERT to Handle Ultra-Long Text By 苏剑林 | December 04, 2020 As we all know, current mainstream BERT models can handle a maximum of 512 tokens. The root cause of this bottleneck is that BERT uses absolute position encodings trained from random initialization. Usually, the maximum position is set to 512, thus it can only handle 512 tokens at most—any additional parts have no position encodings available. Of course, another important reason is the $\\mathcal{O}(n^2)$ complexity of Attention, which significantly increases VRAM usage for long sequences, making fine-tuning impossible on typical GPUs. This article primarily addresses the former cause. Assuming one has sufficient VRAM, how can we simply modify a BERT model with an existing maximum length of 512 so that it can directly process longer texts? The main idea is to hierarchically decompose the pre-trained absolute position encodings, allowing them to extend to longer positions. Position Encoding BERT uses learned absolute position encodings. This encoding method is simple and effective, but since each position vector is learned by the model itself, we cannot inf"}, {"file": "translation_7961.html", "title": "[Turtle/Fish Diary] Full Ceramsite Same-Path Bottom Filter Ecological Tank", "content": "← Back to Index [Turtle/Fish Diary] Full Ceramsite Same-Path Bottom Filter Ecological Tank By 苏剑林 | December 07, 2020 Recently, I’ve fallen into the rabbit hole of fishkeeping and set up a 60cm × 40cm ultra-white tank, primarily for co-habitating turtles and fish. I personally pursue a natural and biomimetic style, so I hope to establish a relatively stable ecological environment within the tank. Of course, those are actually just excuses; the real reason is that I’m too lazy to change the water and don’t want to wash filter sponges. Therefore, I thought about relying on the ecosystem's own self-purification capabilities to extend intervals between water changes. To this end, I referenced online materials to set up a Same-Path Bottom Filter and made some modifications based on my own experience. Same-Path Bottom Filter Simply put, a Same-Path Bottom Filter consists of pipes buried under the substrate. Its characteristic is having one outlet and multiple inlets, where the distance from each inlet to the outlet is equal (\"same-path\"), and these inlets are distributed as evenly as possible. By burying such pipes under the substrate and using a water pump to draw water from the outlet,"}, {"file": "translation_7980.html", "title": "Optimization Algorithms from a Dynamic Perspective (VI): Why Doesn't SimSiam Collapse?", "content": "← Back to Index Optimization Algorithms from a Dynamic Perspective (VI): Why Doesn't SimSiam Collapse? By 苏剑林 | December 11, 2020 Since SimCLR , work on unsupervised feature learning in CV (Computer Vision) has emerged endlessly, making it hard to keep up. Most of these works are based on contrastive learning, which involves categorical learning by constructing positive and negative samples in appropriate ways. However, among many similar works, there are always some unique ones, such as Google's BYOL and the more recent SimSiam . They propose schemes that can complete feature learning relying only on positive samples, which feels refreshing. But without the support of negative samples, why doesn't the model collapse to a meaningless constant model? This is the most thought-provoking question in these two papers. SimSiam provides an answer that many have praised, but I feel that SimSiam only changes the way the question is phrased without truly solving it. I believe the success of models like SimSiam and GAN is largely due to the use of gradient-based optimizers (rather than other stronger or weaker optimizers). Therefore, any answer that doesn't integrate optimization dynamics is "}, {"file": "translation_7991.html", "title": "Mitchell Approximation: Turning Multiplication into Addition, with Error no more than 1/9", "content": "← Back to Index Mitchell Approximation: Turning Multiplication into Addition, with Error no more than 1/9 By 苏剑林 | December 14, 2020 Today, I would like to introduce a very interesting paper from 1962 titled \"Computer Multiplication and Division Using Binary Logarithms\" , authored by John N. Mitchell. In it, he proposed a quite fascinating algorithm: in binary, multiplication of two numbers can be approximated entirely through addition, with a maximum relative error of no more than 1/9. The entire algorithm is quite ingenious, and even more interestingly, it has a very concise programming implementation that is simply marvelous. However, I found that there are almost no webpages introducing this algorithm online, so I am presenting it here. Do you think this is just obsolete stuff? You would be wrong. Not long ago, someone used it to publish a paper at NeurIPS 2020! So, are you sure you don't want to learn about it? Fast Logarithms and Exponents When talking about turning multiplication into addition, one naturally thinks of logarithms and exponents, namely: \\begin{equation}pq = a^s, \\quad s = \\log_a p + \\log_a q\\end{equation} This article is based on binary, so $a=2$. The problem "}, {"file": "translation_8009.html", "title": "Optimization Algorithms from a Dynamical Perspective (VII): SGD ≈ SVM?", "content": "← Back to Index Optimization Algorithms from a Dynamical Perspective (VII): SGD ≈ SVM? By 苏剑林 | Dec 21, 2020 As is well known, before the era of deep learning, machine learning was dominated by SVM (Support Vector Machine). It was once the center of attention in the machine learning world, captivating countless researchers. Even today, \"hand-deriving SVM\" remains one of the popular interview questions at major tech companies. However, times have changed. When deep learning became popular, the first thing it \"revolutionized\" was SVM. Now, the presence of SVM is mostly found in specific scenarios where efficiency is paramount or in the aforementioned interview questions. In a surprising turn of events, a recent paper on Arxiv, \"Every Model Learned by Gradient Descent Is Approximately a Kernel Machine\" , made a very \"bold\" declaration: Any model learned by a gradient descent algorithm can be approximately viewed as an SVM! This conclusion is truly \"bold\" because it isn't just aimed at deep learning; it suggests that as long as you optimize using gradient descent, the resulting model is nothing more than an (approximation of an) SVM. I have reviewed the analysis in the original paper a"}, {"file": "translation_8027.html", "title": "RealFormer: Moving Residuals to the Attention Matrix", "content": "← Back to Index RealFormer: Moving Residuals to the Attention Matrix By 苏剑林 | December 24, 2020 As is well known, Layer Normalization is one of the crucial components of the Transformer model. Its usage generally falls into two categories: PostLN and PreLN. The paper \"On Layer Normalization in the Transformer Architecture\" provides a detailed analysis of both. Simply put, PreLN is more friendly to gradient descent, converges faster, and is more robust to training hyperparameters like learning rate. In almost every respect, it seems superior, except for one significant drawback: the performance of PreLN always seems slightly worse than PostLN. Recently, a paper from Google titled \"RealFormer: Transformer Likes Residual Attention\" proposed the RealFormer design, successfully bridging this gap. This allows the model to possess the optimization friendliness of PreLN while achieving better performance than PostLN, truly offering the best of both worlds. Form RealFormer stands for \" Re sidual Attention L ayer Trans former .\" As the name suggests, it places the residual connection inside the Attention mechanism. Regarding the name, there is a small anecdote. When this blog post was first "}, {"file": "translation_8046.html", "title": "SPACES: \"Extractive-Abstractive\" Long-Text Summarization (Summary of \"CAIL\")", "content": "← Back to Index SPACES: \"Extractive-Abstractive\" Long-Text Summarization (Summary of \"CAIL\") By 苏剑林 | Jan 01, 2021 The \"CAIL\" (China AI+Law Challenge) has become one of the most well-known NLP competitions in recent years. This year marks the third edition, featuring four tracks, among which the \"Judicial Summarization\" track caught our interest. After learning more, we found it focuses on long-text summarization generation for judicial judgment documents in the legal field. This is likely the first public long-text generation task and dataset in China. Over the past year or so, we have continuously invested in and explored text generation, so we decided to choose this track as a \"touchstone\" to verify our research results. Fortunately, we ultimately won first place in this track with a slim margin. Here, we provide a summary and sharing of our competition model. Screenshot of the competition ranking In this competition, we stepped away from pure \"alchemy\" (trial-and-error hyperparameter tuning) and improved model performance through versatile new methods such as a novel Copy mechanism and Sparse Softmax. Overall, our model is relatively concise, effective, and capable of end-to-en"}, {"file": "translation_8062.html", "title": "Searching for Text: (1) From Text Generation to Search Sampling", "content": "← Back to Index Searching for Text: (1) From Text Generation to Search Sampling By 苏剑林 | January 07, 2021 Recently, I have entered a new niche: performing text generation tasks based on the idea of discrete optimization. Simply put, we quantitatively write down the target for the text we want to generate, construct a distribution, and then search for the maximum point of this distribution or sample from it. This process usually does not require training on labeled data . Since language is discrete, continuous function optimization methods like gradient descent are unavailable. Furthermore, because this distribution usually does not have a form that is easy to sample from, direct sampling is also unfeasible. Therefore, specially designed sampling algorithms are required, such as Rejection Sampling, MCMC (Markov Chain Monte Carlo), MH sampling (Metropolis-Hastings Sampling), Gibbs Sampling, and so on. Some readers might find this familiar; does it feel like returning to those head-spinning years of learning LDA (Latent Dirichlet Allocation)? That's right—the aforementioned sampling algorithms are also the essential foundation for understanding the LDA model. In this article, we will "}, {"file": "translation_8069.html", "title": "You Might Not Need BERT-flow: A Linear Transformation Comparable to BERT-flow", "content": "← Back to Index You Might Not Need BERT-flow: A Linear Transformation Comparable to BERT-flow By 苏剑林 | January 11, 2021 BERT-flow originates from the paper \"On the Sentence Embeddings from Pre-trained Language Models\" , which was accepted to EMNLP 2020. It primarily uses a flow model to calibrate the distribution of sentenece embeddings produced by BERT, thereby making the calculated cosine similarity more reasonable. Due to my habit of regularly browsing Arxiv, I saw the paper when it was first uploaded, but I didn't find it particularly interesting at the time. Unexpectedly, it gained significant popularity recently, with numerous interpretations appearing in public accounts and Zhihu within a short period. I believe many readers have probably seen it in their feeds. From the experimental results, BERT-flow indeed achieves a new SOTA. However, regarding this result, my first impression was: something is not quite right! Of course, I'm not saying there's an issue with the results, but based on my understanding, it is unlikely that the flow model itself is playing the critical role. Following this intuition, I performed some analysis, and as expected, I found that while the logic b"}, {"file": "translation_8084.html", "title": "[Searching for Text] Part 2: From MCMC to Simulated Annealing", "content": "← Back to Index [Searching for Text] Part 2: From MCMC to Simulated Annealing By 苏剑林 | January 14, 2021 [Text from Searching] · (2) From MCMC to Simulated Annealing In the previous article, we introduced the concept of \"constrained text generation,\" noting that certain conditional text generation tasks can be completed unsupervised by quantifying targets and sampling from them. At the same time, the previous article introduced \"Importance Sampling\" and \"Rejection Sampling\" methods, pointing out that for high-dimensional spaces, the easy-to-sample distributions they rely on are often difficult to design, making it hard for them to meet our sampling needs. At this point, we need to introduce one of the most important algorithms in the sampling world: the \"Markov Chain Monte Carlo (MCMC)\" method. It combines Markov chains and Monte Carlo methods, making it possible (at least in theory) to sample from many high-dimensional distributions. It is also one of the fundamental algorithms for the constrained text generation applications we will introduce later. This article attempts to provide a basic introduction to it. Markov Chain A Markov chain is essentially a \"memoryless\" random walk pr"}, {"file": "translation_8119.html", "title": "[Searching for Text] · (III) Text Sampling Based on BERT", "content": "← Back to Index [Searching for Text] · (III) Text Sampling Based on BERT By 苏剑林 | Jan 22, 2021 Starting from this article, we will apply the sampling algorithms introduced earlier to specific text generation examples. As the first example, we will introduce how to use BERT for random text sampling. The so-called random text sampling is the process of randomly generating natural language sentences from a model. The common view is that such random sampling is a unique function of unidirectional autoregressive language models like GPT-2 and GPT-3, while bidirectional Masked Language Models (MLM) like BERT cannot do it. Is that really the case? Of course not. Utilizing BERT's MLM model can also achieve text sampling; in fact, it is exactly the Gibbs sampling introduced in the previous article. This fact was first clearly pointed out in the paper \"BERT has a Mouth, and It Must Speak: BERT as a Markov Random Field Language Model.\" The title of the paper is quite interesting: \"BERT has a mouth, so it must say something.\" Now let's see what BERT can actually say~ Sampling Process First, let's review the Gibbs sampling process introduced in the previous article: Gibbs Sampling The initial s"}, {"file": "translation_8128.html", "title": "A Theoretical Analysis Attempt of the Repetition Problem in Seq2Seq", "content": "← Back to Index A Theoretical Analysis Attempt of the Repetition Problem in Seq2Seq By 苏剑林 | January 26, 2021 Last year, the author wrote a blog post \"How to Deal with the 'Never-Ending' Problem in Seq2Seq?\" , which introduced a paper's strategy for handling the phenomenon where Seq2Seq decoding fails to stop, and pointed out that the paper only provided strategies without providing a theoretical understanding of the problem. Recently, the author read a paper titled \"A Theoretical Analysis of the Repetition Problem in Text Generation\" from AAAI 2021 on arXiv, which analyzes the repetition decoding phenomenon in Seq2Seq from a theoretical perspective. In essence, repetition decoding and the \"never-ending\" decoding problem are cut from the same cloth, so this new paper can be seen as filling the gap left by the previous one. Upon studying it, the author found that the paper indeed has many commendable points worth reading. The author has refined, corrected, and generalized the analysis process from the original paper and recorded the results in this article for reference. Furthermore, putting the problem background aside, readers can also treat this article as a matrix analysis exerc"}, {"file": "translation_8130.html", "title": "Transformer Position Encodings That Make Researchers Rack Their Brains", "content": "← Back to Index Transformer Position Encodings That Make Researchers Rack Their Brains By 苏剑林 | February 03, 2021 Unlike RNN or CNN models, for the Transformer model, the addition of position encoding is essential. This is because the pure Attention module is unable to capture the input order; that is, it cannot distinguish between tokens at different positions. For this reason, we generally have two choices: 1. Find a way to incorporate position information into the input, which constitutes the general approach for absolute position encoding; 2. Find a way to fine-tune the Attention structure so that it has the ability to distinguish tokens at different positions, which constitutes the general approach for relative position encoding. Although it is said that there are mainly two categories—absolute position encoding and relative position encoding—each category can actually spawn various variants. To this end, researchers have exerted a great deal of effort and racked their brains. Additionally, there are some position encodings that do not follow conventional patterns. In this article, let us appreciate the \"Eight Immortals crossing the sea, each showing their own prowess\" style e"}, {"file": "translation_8159.html", "title": "How Did a Binarized Word Vector Model Get Involved with Fruit Flies?", "content": "← Back to Index How Did a Binarized Word Vector Model Get Involved with Fruit Flies? By 苏剑林 | Feb 09, 2021 Fruit Fly (Image from Google Search) Some readers might have noticed the ICLR 2021 paper 《Can a Fruit Fly Learn Word Embeddings?》 recently. The paper states it is a binarized word vector model based on biomimetic ideas (the olfactory circuit of the fruit fly). Actually, the algorithmic part of the paper isn't that difficult to read. The main confusion after reading might be, \"What does this have to do with fruit flies?\" or \"Was the author really inspired by fruit flies?\" and so on. In this article, let's trace the origins of the algorithm and try to answer how this word vector model got involved with fruit flies. BioWord The original paper did not give this word vector model a specific name. For convenience, I will take the liberty of calling it \"BioWord.\" In general, the content of the paper consists of three parts: 1. Constructing a bag-of-words representation vector for each n-gram; 2. Applying the BioHash algorithm to these n-gram vectors to obtain so-called (binarized) static/dynamic word vectors; 3. \"Struggling\" to tell a story. We will introduce BioHash later, but in sh"}, {"file": "translation_8180.html", "title": "Nyströmformer: A Linearized Attention Scheme Based on Matrix Decomposition", "content": "← Back to Index Nyströmformer: A Linearized Attention Scheme Based on Matrix Decomposition By 苏剑林 | Feb 16, 2021 The $\\mathcal{O}(n^2)$ complexity of standard Attention is truly a headache for researchers. Recently, in the blog post \"Performer: Linearizing Attention Complexity with Random Projections\" , we introduced Google's Performer model, which transforms standard Attention into linear Attention via random projections. Coincidentally, a paper titled \"Nyströmformer: A Nyström-Based Algorithm for Approximating Self-Attention\" was released on Arxiv for AAAI 2021, proposing another scheme to linearize standard Attention from a different perspective. This scheme is \"Nyström-Based,\" which, as the name suggests, utilizes the Nyström method to approximate standard Attention. However, frankly speaking, before seeing this paper, I had never heard of the Nyström method. Looking through the entire paper, it is filled with matrix decomposition derivations that seemed somewhat confusing at first glance. Interestingly, although the author's derivation is complex, I found that the final result can be understood through a relatively simpler approach. I have organized my understanding of Nyström"}, {"file": "translation_8194.html", "title": "【Search-based Text】 (4) Constructing Sentences through Addition, Deletion, and Modification", "content": "← Back to Index 【Search-based Text】 (4) Constructing Sentences through Addition, Deletion, and Modification By 苏剑林 | Feb 25, 2021 \"Constructing sentences with given words\" is a classic task in elementary school to help us understand and apply vocabulary. From the perspective of Natural Language Processing, it is a task of sentence expansion or sentence completion, which essentially requires the ability to perform non-directional text generation. However, current mainstream language models are unidirectional (mostly forward, i.e., left-to-right; a few are backward, i.e., right-to-left). Since the provided words in a sentence construction task do not necessarily appear at the beginning or the end of the sentence, language models cannot be directly used to complete such tasks. In this article, we will introduce the paper \"CGMH: Constrained Sentence Generation by Metropolis-Hastings Sampling\" . It utilizes MCMC sampling to enable unidirectional language models to perform non-directional generation. By simulating the human process of writing and polishing through addition, deletion, and modification operations, it can unsupervisedly complete various text generation tasks, such as senten"}, {"file": "translation_8209.html", "title": "T5 PEGASUS: Open-sourcing a Chinese Generative Pre-trained Model", "content": "← Back to Index T5 PEGASUS: Open-sourcing a Chinese Generative Pre-trained Model By 苏剑林 | March 03, 2021 Last year, in the article \"That leaderboard-topping T5 model is now playable in Chinese,\" we introduced Google's multilingual version of the T5 model (mT5) and provided examples of using mT5 for Chinese text generation tasks. While mT5 is a viable solution for Chinese generation, the lack of a model trained entirely on Chinese corpora felt somewhat unsatisfactory, so I decided to develop one. After repeated consideration and testing, we decided to use the mT5 architecture and initial weights as a base, first improving the Tokenizer based on Chinese characteristics, and then mimicking PEGASUS to construct pre-training tasks. This resulted in a new version of the T5 model, which we are open-sourcing today as T5 PEGASUS. Tokenizer First, let's look at our improvements to the Tokenizer. The Tokenizer used by mT5 is sentencepiece , a subword tokenization library written in C++. It is efficient and lightweight, but unfortunately, it is not particularly friendly to Chinese, mainly manifesting in: 1. sentencepiece forcibly converts certain full-width symbols into half-width symbols, whi"}, {"file": "translation_8213.html", "title": "Short-Text Matching Baseline: An Attempt at Using Pre-trained Models on Anonymized Data", "content": "← Back to Index Short-Text Matching Baseline: An Attempt at Using Pre-trained Models on Anonymized Data By 苏剑林 | March 05, 2021 Recently, I decided to join the fun and participate in the \" Xiao Bu Assistant Dialogue Short-Text Semantic Matching \" track of the Global AI Technology Innovation Competition. The task is a standard short-text sentence pair binary classification task. In this era where various pre-trained Transformers are \"running wild,\" this task doesn't pose much of a special challenge. However, what makes it interesting is that the data for this competition is anonymized—meaning every character has been mapped to a numeric ID, so we cannot access the original text. Under these circumstances, can we still use pre-trained models like BERT? Certainly, they can be used, but it requires some techniques, and you might need to perform further pre-training. This article shares a baseline that combines classification, pre-training, and semi-supervised learning, which can be applied to anonymized data tasks. Model Overview The core idea of the entire model is actually a variant of PET (Pattern-Exploiting Training) introduced in the previous article \"Is GPT3 Necessary? No, BERT's"}, {"file": "translation_8231.html", "title": "Transformer Upgrade Road: 1. Tracing the Origins of Sinusoidal Position Encoding", "content": "← Back to Index Transformer Upgrade Road: 1. Tracing the Origins of Sinusoidal Position Encoding By 苏剑林 | March 08, 2021 Recently, the author has made some attempts to understand and improve the Transformer, gaining some experiences and conclusions that seem valuable. Consequently, I am starting a special series to summarize these findings, named \"Transformer Upgrade Road,\" representing both a deepening of understanding and improvements in results. As the first article in this series, I will introduce a new understanding of the Sinusoidal position encoding proposed by Google in \"Attention is All You Need\" : \\begin{equation}\\left\\{\\begin{aligned}&\\boldsymbol{p}_{k,2i}=\\sin\\Big(k/10000^{2i/d}\\Big)\\\\\n&\\boldsymbol{p}_{k, 2i+1}=\\cos\\Big(k/10000^{2i/d}\\Big)\n\\end{aligned}\\right.\\label{eq:sin}\\end{equation} where $\\boldsymbol{p}_{k,2i}, \\boldsymbol{p}_{k,2i+1}$ are the $2i$-th and $(2i+1)$-th components of the encoding vector for position $k$, and $d$ is the vector dimension. As an explicit solution for position encoding, Google's description of it in the original paper was sparse, merely mentioning that it could express relative position information. Later, various interpretations appeare"}, {"file": "translation_8244.html", "title": "The Success of WGAN Might Have Nothing to Do with Wasserstein Distance", "content": "← Back to Index The Success of WGAN Might Have Nothing to Do with Wasserstein Distance By 苏剑林 | March 15, 2021 WGAN, or Wasserstein GAN, is considered a major theoretical breakthrough in the history of GANs. It transformed the measure of the two probability distributions in GANs from f-divergence to Wasserstein distance, making the training process of WGAN more stable and generally resulting in better generation quality. Wasserstein distance is related to Optimal Transport and belongs to the class of Integral Probability Metrics (IPMs). These probability measures usually possess superior theoretical properties, which is why the emergence of WGAN attracted many researchers to understand and study GAN models through the lens of Optimal Transport and IPMs. However, a recent paper on arXiv, \"Wasserstein GANs Work Because They Fail (to Approximate the Wasserstein Distance)\" , points out that although WGAN was derived from the theory of Wasserstein distance, successful WGANs today do not actually approximate the Wasserstein distance well. On the contrary, if we perform a better approximation of the Wasserstein distance, the performance actually gets worse. In fact, I have long had this d"}, {"file": "translation_8265.html", "title": "Transformer Upgrade Path: 2. Rotary Position Embedding (RoPE) Borrowing Strengths from Many", "content": "← Back to Index Transformer Upgrade Path: 2. Rotary Position Embedding (RoPE) Borrowing Strengths from Many By 苏剑林 | March 23, 2021 In the previous article , we provided a detailed derivation and understanding of the original Sinusoidal position encoding. The general feeling is that Sinusoidal position encoding is an \"absolute position encoding that wants to be a relative position encoding.\" Generally speaking, absolute position encoding has the advantages of simple implementation and fast calculation, while relative position encoding directly reflects relative position signals, which aligns with our intuitive understanding and often yields better actual performance. It follows that if one can implement relative position encoding via an absolute position encoding method, it would be a \"best of both worlds\" design. Sinusoidal position encoding vaguely achieves this, but not well enough. This article will introduce our self-developed Rotary Transformer (RoFormer) model. Its main modification is the application of \"Rotary Position Embedding (RoPE),\" which I conceived. This design allows the Attention mechanism to achieve \"relative position encoding via an absolute position encoding me"}, {"file": "translation_8295.html", "title": "P-tuning: Automatically Constructing Templates to Release the Potential of Language Models", "content": "← Back to Index P-tuning: Automatically Constructing Templates to Release the Potential of Language Models By 苏剑林 | April 03, 2021 In a previous article, \"Is GPT-3 Necessary? No, BERT's MLM Model Can Also Do Small-Shot Learning,\" we introduced a method called Pattern-Exploiting Training (PET). By combining manually constructed templates with BERT's MLM (Masked Language Model), it can achieve excellent results in zero-shot, small-shot, and even semi-supervised learning. This approach is elegant because it unifies the pre-training task with downstream tasks. However, manually constructing these templates can sometimes be difficult, and the effectiveness of different templates varies greatly. If templates could be automatically constructed using a small number of samples, it would be highly valuable. A recent paper on Arxiv, \"GPT Understands, Too,\" proposed a method called P-tuning, which successfully realizes the automatic construction of templates. Not only that, but with the help of P-tuning, GPT's performance on SuperGLUE exceeded that of BERT models of the same class for the first time. This overturns the long-held conclusion that \"GPT is not good at NLU\" (Natural Language Unders"}, {"file": "translation_8321.html", "title": "Which Unsupervised Semantic Similarity Method is the Strongest? We Conducted a Comprehensive Evaluation", "content": "← Back to Index Which Unsupervised Semantic Similarity Method is the Strongest? We Conducted a Comprehensive Evaluation By 苏剑林 | April 11, 2021 In January, I wrote \"You Might Not Need BERT-flow: A Simple Linear Transformation Comparable to BERT-flow\" , pointing out that the state-of-the-art (SOTA) unsupervised semantic similarity model, BERT-flow, can actually be matched by a simple linear transformation (whitening operation, or BERT-whitening). Subsequently, we further refined our experimental results and wrote a paper titled \"Whitening Sentence Representations for Better Semantics and Faster Retrieval\" . This blog post will summarize the content of that paper and provide supplementary evaluations on five Chinese semantic similarity tasks, involving over 600 experimental results. Github Link: https://github.com/bojone/BERT-whitening Method Overview The logic behind BERT-whitening is simple: after obtaining the sentence vectors $\\{x_i\\}_{i=1}^N$ for each sentence, a whitening operation (essentially PCA) is applied to these vectors so that the mean of each dimension is 0 and the covariance matrix is the identity matrix. Finally, the top $k$ principal components are retained. The pro"}, {"file": "translation_8337.html", "title": "Sohu Text Matching: A Multi-Task Baseline Based on Conditional LayerNorm", "content": "← Back to Index Sohu Text Matching: A Multi-Task Baseline Based on Conditional LayerNorm By 苏剑林 | April 16, 2021 Not long ago, I saw the \" 2021 Sohu Campus Text Matching Algorithm Competition \" and found the problem quite interesting, so I gave it a try. However, since the competition itself is only for students, I cannot participate as an official contestant. Therefore, I am open-sourcing my approach as a baseline for everyone's reference. Github Link: https://github.com/bojone/sohu2021-baseline Task Introduction As the name suggests, the task is text matching—determining whether two texts are similar. This is normally a conventional task, but what's interesting here is that it is divided into multiple subtasks. Specifically, it is divided into two major categories, A and B. Category A has more relaxed matching standards, while Category B has stricter standards. Each major category is further divided into three sub-categories: \"Short-Short Matching,\" \"Short-Long Matching,\" and \"Long-Long Matching.\" Therefore, although the task type is the same, strictly speaking, there are six different subtasks. Generally speaking, completing this task would require at least two models, as the cl"}, {"file": "translation_8338.html", "title": "The Road to Transformer Upgrade: 3. From Performer to Linear Attention", "content": "← Back to Index The Road to Transformer Upgrade: 3. From Performer to Linear Attention By 苏剑林 | April 22, 2021 Readers of my previous articles \"Exploration of Linear Attention: Does Attention Need a Softmax?\" and \"Performer: Linearizing Attention Complexity with Random Projections\" might find the title of this post a bit unnatural. Since Linear Attention came before the Performer, the relationship is typically framed as \"the Performer is an implementation of Linear Attention that approximates standard Attention while maintaining linear complexity.\" Thus, normally, it should be \"From Linear Attention to Performer.\" However, this post does not intend to trace the historical development of Linear Attention, but rather to reflect on the insights Performer brings to Linear Attention. Hence, \"From Performer to Linear Attention.\" Activation Functions The common form of Linear Attention is: \\begin{equation}Attention(\\boldsymbol{Q},\\boldsymbol{K},\\boldsymbol{V})_i = \\frac{\\sum\\limits_{j=1}^n \\text{sim}(\\boldsymbol{q}_i, \\boldsymbol{k}_j)\\boldsymbol{v}_j}{\\sum\\limits_{j=1}^n \\text{sim}(\\boldsymbol{q}_i, \\boldsymbol{k}_j)} = \\frac{\\sum\\limits_{j=1}^n \\phi(\\boldsymbol{q}_i)^{\\top} \\varphi(\\bol"}, {"file": "translation_8348.html", "title": "Is it Still SOTA for Chinese Tasks? We Supplemented SimCSE with Some Experiments", "content": "← Back to Index Is it Still SOTA for Chinese Tasks? We Supplemented SimCSE with Some Experiments By 苏剑林 | April 26, 2021 At the beginning of this year, inspired by BERT-flow, I conceived the \"BERT-whitening\" method, which temporarily became the new SOTA for semantic similarity (see \"You Might Not Need BERT-flow: A Simple Linear Transformation Competes with BERT-flow\" ; the paper is \"Whitening Sentence Representations for Better Semantics and Faster Retrieval\" ). However, the \"good times did not last long.\" Shortly after BERT-whitening was submitted to Arxiv, at least two new papers appeared on Arxiv with results significantly superior to BERT-whitening. The first is \"Generating Datasets with Pretrained Language Models\" , which uses templates to unsupervisedly construct data pairs from GPT2_XL to train similarity models. While I find it clarifying and effective, the reproduction cost and variance are somewhat large. The other is the protagonist of this article, \"SimCSE: Simple Contrastive Learning of Sentence Embeddings\" . The proposed SimCSE significantly outperforms BERT-flow and BERT-whitening on English data, and the method is remarkably simple. So, is SimCSE equally effective i"}, {"file": "translation_8373.html", "title": "GlobalPointer: A Unified Manner to Handle Nested and Non-nested NER", "content": "← Back to Index GlobalPointer: A Unified Manner to Handle Nested and Non-nested NER By 苏剑林 | May 01, 2021 (Note: The relevant content of this article has been organized into the paper 《Global Pointer: Novel Efficient Span-based Approach for Named Entity Recognition》 . If you need to cite it, you can cite the English paper directly. Thank you.) This article will introduce a design called GlobalPointer, which uses the idea of global normalization to perform Named Entity Recognition (NER). It can identify both nested and non-nested entities without distinction. In the case of non-nested (Flat NER), it achieves results comparable to CRF, while in nested (Nested NER) scenarios, it also performs well. Furthermore, theoretically, the design philosophy of GlobalPointer is more reasonable than CRF; and in practice, it does not require recursive denominator calculation during training like CRF, nor does it require dynamic programming during prediction—it is completely parallel. In ideal conditions, the time complexity is $\\mathcal{O}(1)$! In short: more elegant, faster, and more powerful! Is there really such a good design? Let's take a look. GlobalPointer multi-head recognition of nested en"}, {"file": "translation_8397.html", "title": "The Road to Transformer Upgrade: 4. Rotary Position Embedding for 2D Positions", "content": "← Back to Index The Road to Transformer Upgrade: 4. Rotary Position Embedding for 2D Positions By 苏剑林 | May 10, 2021 In the previous article \"The Road to Transformer Upgrade: 2. Rotary Position Embedding (RoPE) Drawing from Various Strengths,\" we proposed the Rotary Position Embedding (RoPE) and the corresponding Transformer model, RoFormer. Since the author's primary research area is NLP, this seemed like the end of the story. However, recently, Transformer models have also exploded in the field of Computer Vision (CV), with various Vision Transformers (ViT) emerging one after another. This led to a natural question: What should 2D RoPE look like? At first glance, it might seem like a simple generalization of the 1D case, but the derivation and understanding involved are far more complex than imagined. This article provides an analysis of it to deepen our understanding of RoPE. 2D RoPE What is a 2D position? What does the corresponding 2D RoPE look like? Where is the difficulty? In this section, we will briefly introduce 2D positions, then directly provide the results and derivation logic for 2D RoPE, followed by a detailed derivation in subsequent sections. 2D Position In NLP, th"}, {"file": "translation_8404.html", "title": "Variational Autoencoder (VII): VAE on the Sphere (vMF-VAE)", "content": "← Back to Index Variational Autoencoder (VII): VAE on the Sphere (vMF-VAE) By 苏剑林 | May 17, 2021 In \"Variational Autoencoder (V): VAE + BN = Better VAE\" , we discussed the common phenomenon of KL divergence vanishing when training VAEs in NLP and mentioned using Batch Normalization (BN) to give the KL divergence term a positive lower bound, thereby ensuring it does not vanish. In fact, back in 2018, work based on similar ideas was proposed. These approaches involve using new prior and posterior distributions in the VAE to ensure the KL divergence term has a fixed positive lower bound. This idea appeared in two similar papers in 2018, namely \"Hyperspherical Variational Auto-Encoders\" and \"Spherical Latent Spaces for Stable Variational Autoencoders\" . Both use the von Mises–Fisher (vMF) distribution, defined on a hypersphere, to construct the prior and posterior distributions. In some ways, this distribution is even simpler and more interesting than the Gaussian distribution we commonly use! KL Divergence Vanishing We know that the training objective of a VAE is: \\begin{equation}\\mathcal{L} = \\mathbb{E}_{x\\sim \\tilde{p}(x)} \\Big[\\mathbb{E}_{z\\sim p(z|x)}\\big[-\\log q(x|z)\\big]+KL\\big("}, {"file": "translation_8431.html", "title": "Reviewing Some Recent Non-Transformer Works", "content": "← Back to Index Reviewing Some Recent Non-Transformer Works By 苏剑林 | May 24, 2021 Everyone has likely been bombarded lately by various MLP-related works. Led by Google, several research institutions have been \"pulling out all the stops,\" attempting to \"strike\" the Transformer model from multiple dimensions. Among these, the most aggressive are a series of models claiming to be \"pure MLP,\" creating a sensation as if the era of \"MLP is all you need\" has arrived. Behind this dazzling array of operations, is it a \"return to simplicity\" under the principle of the Great Way, or is it merely \"reheating cold leftovers\" after running out of creative talent? Let's follow this trend and review some of the recent related works. A Very Busy May Strange things happen every day, but May has been particularly unusual. Since the beginning of this month, major institutions seem to have made a pact, as various non-Transformer works have made their debut, like \"a sudden spring breeze that brings thousands of pear trees into bloom.\" Just among the papers I've come across on arXiv, there have been as many as seven (and the month isn't even over yet, with seven papers sharing extremely consistent directi"}, {"file": "translation_8444.html", "title": "Can we losslessly enlarge a Transformer model? (Part 1)", "content": "← Back to Index Can we losslessly enlarge a Transformer model? (Part 1) By 苏剑林 | June 02, 2021 Looking at the title, readers might find it strange: shouldn't everyone be thinking about how to shrink large models? Why are you thinking about enlarging a small one? The background is this: Generally speaking, larger models with more data do indeed yield better results. However, when computing power is limited, the time cost of pre-training a large model from scratch is too high. If you need to tune hyper-parameters a few times, months might pass by. This is where \"poor man's logic\" comes in (those with infinite resources can ignore this): Can we first train a small model with the same number of layers, then enlarge it and continue training? In this way, the weights of the pre-trained small model, after being enlarged, serve as a very high starting point for the large model's initialization. Consequently, the number of training steps for the large model stage can be reduced, thereby shortening the overall training time. So, can a small model be losslessly enlarged into a large model? This article will analyze this problem from a theoretical perspective. Meaning Some readers might think:"}, {"file": "translation_8453.html", "title": "Orthogonal Matrix for Transforming One Unit Vector to Another", "content": "← Back to Index Orthogonal Matrix for Transforming One Unit Vector to Another By 苏剑林 | June 05, 2021 In this article, we discuss a practical linear algebra problem: Given two $d$-dimensional unit (column) vectors $\\boldsymbol{a}, \\boldsymbol{b}$, find an orthogonal matrix $\\boldsymbol{T}$ such that $\\boldsymbol{b} = \\boldsymbol{T}\\boldsymbol{a}$. Since the two vectors have the same magnitude, it is clear that such an orthogonal matrix must exist. So, how do we find it? Two Dimensions It is not hard to imagine that this is essentially a problem of vector transformation (such as rotation or reflection) within the two-dimensional sub-plane formed by $\\boldsymbol{a}$ and $\\boldsymbol{b}$. Therefore, let us first consider the case where $d=2$. Schematic of orthogonal decomposition As shown in the figure above, through orthogonal decomposition, we can obtain a vector $\\boldsymbol{b} - \\boldsymbol{a}\\cos\\theta$ which is perpendicular to $\\boldsymbol{a}$. After normalization, we can obtain an orthonormal basis: \\begin{equation}\\boldsymbol{Q} = \\begin{pmatrix}\\boldsymbol{a} & \\frac{\\boldsymbol{b} - \\boldsymbol{a}\\cos\\theta}{\\Vert \\boldsymbol{b} - \\boldsymbol{a}\\cos\\theta\\Vert}\\end{pmatrix}\\"}, {"file": "translation_8454.html", "title": "SimBERTv2 Is Here! The RoFormer-Sim Model Integrating Retrieval and Generation", "content": "← Back to Index SimBERTv2 Is Here! The RoFormer-Sim Model Integrating Retrieval and Generation By 苏剑林 | June 11, 2021 Last year, we released the SimBERT model, which has been one of our most successful open-source models, earning recognition from many readers. Simply put, SimBERT is a model that integrates generation and retrieval. it can be used as a relatively high baseline for sentence vectors and can also be used to automatically generate similar questions, serving as an auxiliary tool for data augmentation—a feature that was quite pioneering. Recently, using RoFormer as the foundation, we further integrated and optimized SimBERT-related technologies and finally released the upgraded RoFormer-Sim model. Introduction RoFormer-Sim is the upgraded version of SimBERT; we can also colloquially refer to it as \"SimBERTv2,\" while \"SimBERT\" by default refers to the old version. Externally, aside from the base architecture switching to RoFormer, there is no obvious difference between RoFormer-Sim and SimBERT. In fact, their primary differences lie in the training details, which can be compared using two formulas: $$ \\text{SimBERT} = \\text{BERT} + \\text{UniLM} + \\text{Contrastive Learning"}, {"file": "translation_8471.html", "title": "Can Contrastive Learning Use Gradient Accumulation?", "content": "← Back to Index Can Contrastive Learning Use Gradient Accumulation? By 苏剑林 | June 17, 2021 In the previous article, \"Trading Time for Performance: Keras Gradient Accumulation Optimizer,\" we introduced \"gradient accumulation,\" a technique for achieving the effect of a large batch size under limited GPU memory. Generally speaking, gradient accumulation is suitable for scenarios where the loss is independent and identically distributed (i.i.d.). In other words, the loss for each sample is calculated independently, and the total loss is the average or sum of all individual losses. However, not all tasks satisfy this condition. For instance, in the recently popular contrastive learning, the loss for each sample also depends on other samples. So, in the context of contrastive learning, can we still use gradient accumulation to achieve the effect of a large batch size? This article analyzes this question. Introduction In general, the loss for contrastive learning can be written as: \\begin{equation}\\mathcal{L}=-\\sum_{i,j=1}^b t_{i,j}\\log p_{i,j} = -\\sum_{i,j=1}^b t_{i,j}\\log \\frac{e^{s_{i,j}}}{\\sum\\limits_j e^{s_{i,j}}}=-\\sum_{i,j=1}^b t_{i,j}s_{i,j} + \\sum_{i=1}^b \\log\\sum_{j=1}^b e^{s_{i"}, {"file": "translation_8475.html", "title": "UniVAE: A Transformer-based Single-Model Multi-Scale VAE Model", "content": "← Back to Index UniVAE: A Transformer-based Single-Model Multi-Scale VAE Model By 苏剑林 | June 29, 2021 As everyone knows, the $\\mathcal{O}(n^2)$ complexity of the Transformer is one of its \"fatal flaws.\" However, every coin has two sides. The $\\mathcal{O}(n^2)$ complexity also provides a lot of room for maneuvering. We can flexibly customize different attention masks to design Transformer models for various purposes, such as UniLM and K-BERT . This article introduces a \"UniVAE\" model for text conceived by the author. It follows an idea similar to UniLM, integrating a VAE into a single Transformer model while also incorporating multi-scale characteristics. UniAE Variational Autoencoder (VAE) will not be introduced here from scratch; this site already has several articles on it, and you can search for them yourself. A VAE can be understood as an AE (Autoencoder) with a regularization term. In general, the Encoder is responsible for encoding the input into a vector that satisfies a certain distribution, while the Decoder is responsible for reconstructing the input from that encoding vector. Therefore, to implement UniVAE, we must first implement the corresponding UniAE. In \"From Langua"}, {"file": "translation_8496.html", "title": "Dropout Twice Again! This Time It Achieved SOTA on Supervised Tasks", "content": "← Back to Index Dropout Twice Again! This Time It Achieved SOTA on Supervised Tasks By 苏剑林 | July 01, 2021 Readers who follow new developments in NLP are likely impressed by SimCSE released in April. By simply \"applying Dropout twice\" to construct positive samples for contrastive learning, it achieved across-the-board SOTA results in unsupervised semantic similarity tasks. Coincidentally, the recent paper \"R-Drop: Regularized Dropout for Neural Networks\" proposed R-Drop, which applies the idea of \"Dropout twice\" to supervised tasks. Almost every experimental result showed significant improvements. Furthermore, the author found in his own experiments that it also performs impressively in semi-supervised tasks. The simple act of \"Dropout twice\" surprisingly yields a \"decathlon-like\" performance. This article introduces R-Drop and shares the author's thoughts on the underlying principles. SimCSE In \"Are Chinese Tasks Still SOTA? We Added Some Experiments to SimCSE\" , we already introduced SimCSE. Briefly, SimCSE is a contrastive learning scheme for NLP. The standard contrastive learning workflow involves passing the same sample through different data augmentation methods to obtain a p"}, {"file": "translation_8512.html", "title": "KL Divergence, Bhattacharyya Distance, and Wasserstein Distance between two Multivariate Normal Distributions", "content": "← Back to Index KL Divergence, Bhattacharyya Distance, and Wasserstein Distance between two Multivariate Normal Distributions By 苏剑林 | July 08, 2021 The normal distribution is one of the most common continuous probability distributions. It is the maximum entropy distribution given a mean and covariance (refer to \"Entropy: From Entropy, Maximum Entropy Principle to Maximum Entropy Models (Part II)\" ). It can also be viewed as a second-order approximation of any continuous distribution, occupying a status equivalent to linear approximation for general functions. From this perspective, the normal distribution can be considered the simplest continuous distribution. Precisely because of its simplicity, analytical solutions can be derived for many estimators. This article primarily calculates several metrics between two multivariate normal distributions, including KL divergence, Bhattacharyya distance, and Wasserstein distance, all of which have explicit analytical solutions. Normal Distribution Here we briefly review some basic knowledge of the normal distribution. Note that this is only a review and is not intended as an introductory tutorial on the subject. Probability Density The nor"}, {"file": "translation_8541.html", "title": "Enhancing RoFormer-Sim with Open-Source Human-Annotated Data", "content": "← Back to Index Enhancing RoFormer-Sim with Open-Source Human-Annotated Data By 苏剑林 | July 19, 2021 As many of you know, from SimBERT to SimBERTv2 (RoFormer-Sim) , we have established a fairly decent baseline model for Chinese text similarity tasks. However, both SimBERT and RoFormer-Sim are essentially \"weakly supervised\" models, similar to unsupervised ones; we cannot expect a purely weakly supervised model to achieve effects that perfectly align with human cognition. Therefore, to further improve the performance of RoFormer-Sim, we attempted to use some open-source annotated data to assist in training. This article introduces our exploration process. Some readers might think: Is there anything to talk about regarding supervised training? Isn't it just direct training? While that's easy to say, it's not actually that \"obvious and straightforward.\" There are still some \"minefields,\" so this article can be considered a simple \"mine-clearing guide.\" Previous Recollection I have found that since the release of SimBERT, the most frequent question from readers is probably: Why is the similarity between \"I like Beijing\" and \"I don't like Beijing\" so high? Aren't their meanings opposite?"}, {"file": "translation_8578.html", "title": "Linear Models from a Probabilistic Perspective: Does Logistic Regression Have an Analytical Solution?", "content": "← Back to Index Linear Models from a Probabilistic Perspective: Does Logistic Regression Have an Analytical Solution? By 苏剑林 | July 22, 2021 We know that linear regression is a relatively simple problem because it has an analytical solution. Its variant, Logistic Regression (LR), however, does not have an analytical solution, which is somewhat of a pity. Although it is called \"regression,\" it is actually used for classification problems, and for many readers, classification is more common than regression. To be precise, when we say logistic regression has no analytical solution, we are referring to the fact that \"logistic regression has no analytical solution under Maximum Likelihood Estimation (MLE).\" Does this mean that if we don't use Maximum Likelihood Estimation, we could find a usable analytical solution? This article will derive an analytical solution for logistic regression from a non-MLE perspective. Simple experiments show that its performance is not inferior to the MLE solution obtained via gradient descent. Furthermore, this analytical solution is easily generalized to the single-layer Softmax multi-class model. Linear Regression Let's first review linear regression. Su"}, {"file": "translation_8586.html", "title": "FlatNCE: Is the Reason for Poor Small-Batch Contrastive Learning Actually Floating-Point Error?", "content": "← Back to Index FlatNCE: Is the Reason for Poor Small-Batch Contrastive Learning Actually Floating-Point Error? By 苏剑林 | July 26, 2021 Since SimCLR brought unsupervised learning to the forefront of computer vision, contrastive learning has gradually become popular in CV and even NLP, with more and more related research and work appearing. A well-known drawback of standard contrastive learning is the requirement for a relatively large batch_size (SimCLR performs best when batch_size=4096); with small batch_sizes, performance drops significantly. Consequently, one of the improvement directions for subsequent work has been to reduce the reliance on large batch_sizes. So, a natural question arises: What exactly is the reason standard contrastive learning performs poorly at small batch_sizes? Recently, a paper titled \"Simpler, Faster, Stronger: Breaking The log-K Curse On Contrastive Learners With FlatNCE\" answered this question: floating-point error. It sounds almost unbelievable, but the paper's analysis is quite reasonable, and the proposed improvement, FlatNCE, indeed works better, making it hard to ignore. Subtleties Next, I will introduce the main content of the original paper acc"}, {"file": "translation_8601.html", "title": "Transformer Upgrade Journey: 5. Standard Attention as Infinite-Dimensional Linear Attention", "content": "← Back to Index Transformer Upgrade Journey: 5. Standard Attention as Infinite-Dimensional Linear Attention By 苏剑林 | August 06, 2021 In \"Performer: Linearizing Attention Complexity with Random Projections\" , we learned about the Performer model proposed by Google. It introduces a random projection scheme that can transform standard Attention into Linear Attention while maintaining a certain level of approximation. Theoretically, as long as the projection dimension is large enough, it can sufficiently approximate standard Attention. In other words, standard Attention can be viewed as an infinite-dimensional Linear Attention. This article will introduce two other ideas I conceived for converting standard Attention into infinite-dimensional Linear Attention. Unlike the random projection of Performer, these two schemes are deterministic and allow for a more intuitive perception of the degree of approximation. Brief Introduction Regarding standard Attention and Linear Attention, I won't go into too much detail here. Readers who are not yet familiar can refer to my previous articles \"Exploring Linear Attention: Does Attention Must Have a Softmax?\" and \"Transformer Upgrade Journey: 3. Fro"}, {"file": "translation_8610.html", "title": "Linear Transformer Should Not Be the Model You Are Waiting For", "content": "← Back to Index Linear Transformer Should Not Be the Model You Are Waiting For By 苏剑林 | August 09, 2021 In this blog, we have discussed the content related to Linear Attention several times. The logic for introducing Linear Attention is generally: standard Attention has a quadratic complexity of $\\mathcal{O}(n^2)$, which is one of its major \"pain points.\" Therefore, we introduce improved models with $\\mathcal{O}(n)$ linear complexity, known as Linear Attention. After seeing introductions to Linear Attention, some readers have been eagerly awaiting our release of pre-trained models based on Linear Attention, hoping to alleviate the \"life-and-death\" struggle caused by BERT's computational consumption. However, what this article aims to say is: readers holding onto this idea might be disappointed. The conversion from standard Attention to Linear Attention is likely to fall far short of your expectations, and the reason BERT is so slow is not actually because of the quadratic complexity of standard Attention. Reflections on BERT According to intuitive understanding, shouldn't replacing quadratic complexity with linear complexity lead to a \"massive leap\"? Why would it \"fall far short of"}, {"file": "translation_8620.html", "title": "A Brief Discussion on Initialization, Parameterization, and Normalization of Transformer", "content": "← Back to Index A Brief Discussion on Initialization, Parameterization, and Normalization of Transformer By 苏剑林 | August 17, 2021 A few days ago, while training a new Transformer model, I found that it wouldn't converge no matter what. After some debugging, I realized that I had forgotten to divide $\\boldsymbol{Q}\\boldsymbol{K}^{\\top}$ by $\\sqrt{d}$ during Self-Attention. This prompted me to revisit why dividing by $\\sqrt{d}$ is so important. Of course, Google's T5 does not divide by $\\sqrt{d}$, yet it still converges normally; that is because it adjusted its initialization strategy. Thus, this matter is closely related to initialization. Taking this opportunity, this article will summarize model initialization, parameterization, and normalization, with the discussion primarily centered around the Transformer. Sampling Distributions Initialization is naturally a process of random sampling, so we start by introducing commonly used sampling distributions. Generally, we sample from a random distribution with a specified mean and variance. Three random distributions are commonly used: Normal, Uniform, and Truncated Normal. Clearly, Normal and Uniform are very common. The Normal distrib"}, {"file": "translation_8634.html", "title": "Gradient Accumulation Hidden in Momentum: Updating Fewer Steps Might Lead to Better Results?", "content": "← Back to Index Gradient Accumulation Hidden in Momentum: Updating Fewer Steps Might Lead to Better Results? By 苏剑林 | August 24, 2021 As we know, gradient accumulation is a common technique for achieving large batch size training under limited GPU memory. In a previous article, \"Trading Time for Quality: Keras Gradient Accumulation Optimizer\" (Chinese), we briefly introduced the implementation of gradient accumulation. The general idea is to add a new set of parameters to cache gradients and then use the cached gradients to update the model. Unfortunately, adding a new set of parameters brings extra memory consumption. While thinking about optimizers these past few days, I suddenly realized: gradient accumulation can actually be built directly into optimizers with momentum! Based on this idea, I performed some derivations and experiments on the optimizers, eventually reaching an interesting but somewhat counter-intuitive conclusion: by updating parameters fewer times, the model's final performance might actually improve! SGDM Before the formal discussion, we define the function \\begin{equation}\\chi_{t/k} = \\left\\{ \\begin{aligned}&1,\\quad t \\equiv 0\\,(\\text{mod}\\, k) \\\\ \n&0,\\quad t "}, {"file": "translation_8656.html", "title": "From Triangle Inequality to Margin Softmax", "content": "← Back to Index From Triangle Inequality to Margin Softmax By 苏剑林 | September 01, 2021 In \"A Sentence Similarity Model Based on GRU and AM-Softmax,\" we introduced AM-Softmax, a version of Softmax with a margin, which is typically used in scenarios where classification is used for retrieval. At that time, we briefly mentioned through illustrations that the introduction of a margin is due to the \"inequivalence between classification and ranking,\" but we did not provide a relatively quantitative explanation for the source of this inequivalence. In this article, we revisit this topic to derive and understand the necessity of a margin from the perspective of the triangle inequality for distances. Triangle Inequality Usually, when we speak of distance, we generally refer to the intuitive \"Euclidean distance.\" However, in mathematics, distance is also called a \"metric,\" and it has a refined axiomatic definition. It is defined as a binary function $d(x,y)$ on a set that satisfies: 1. Non-negativity: $d(x,y) \\geq 0$; 2. Identity: $d(x,y)=0 \\Leftrightarrow x = y$; 3. Symmetry: $d(x,y)=d(y,x)$; 4. Triangle Inequality: $d(x,y) \\leq d(x,z) + d(z,y)$. As the name suggests, distance is used to me"}, {"file": "translation_8662.html", "title": "Globally Shuffle Hundreds of GBs of Files with Limited Memory (Python)", "content": "← Back to Index Globally Shuffle Hundreds of GBs of Files with Limited Memory (Python) By 苏剑林 | September 08, 2021 In this article, we will tackle a programming problem: How to globally shuffle hundreds of gigabytes of text files randomly under limited memory? The background of the problem is quite clear: modern pre-trained models often use dozens or even hundreds of gigabytes of corpora. To enable the model to pre-train better, it is necessary to perform a global random shuffle of the training data. However, for many people, a few hundred GBs of data is often larger than their available RAM. Therefore, how to achieve a global random shuffle within limited memory is a problem worth studying. Existing Tools Assuming our file is stored line-by-line, meaning one line represents one sample, our goal is to randomly shuffle the file by lines. If we only have one file and its size is significantly smaller than the memory, we can use the shuf command that comes with Linux: shuf input.txt -o output.txt The reason for emphasizing that the file size must be significantly smaller than memory is that shuf loads the entire file into memory before shuffling, which requires us to have enough RAM. "}, {"file": "translation_8671.html", "title": "The Once-Dismissed Pre-training Task NSP Delivers Excellent Zero-Shot Results", "content": "← Back to Index The Once-Dismissed Pre-training Task NSP Delivers Excellent Zero-Shot Results By 苏剑林 | September 10, 2021 Among the wide variety of pre-training task designs, NSP (Next Sentence Prediction) is generally considered to be one of the poorer ones. This is because it is relatively easy, and adding it to pre-training has not shown significant benefits for downstream fine-tuning. In fact, the RoBERTa paper showed that it can even have a negative impact. Consequently, subsequent pre-training efforts have generally taken one of two paths: either discarding the NSP task entirely, as RoBERTa did, or finding ways to increase its difficulty, as ALBERT did. In other words, NSP has long been something of an \"outcast.\" However, the tables have turned, and NSP may be about to make a \"comeback.\" A recent paper, \"NSP-BERT: A Prompt-based Zero-Shot Learner Through an Original Pre-training Task--Next Sentence Prediction\" (hereafter referred to as NSP-BERT), shows that NSP can actually achieve very impressive Zero-Shot results! This is another classic case of Prompt-based Few/Zero-Shot learning, but this time, NSP is the protagonist. Background Recap We used to believe that pre-training "}, {"file": "translation_8679.html", "title": "The Amazing Johnson-Lindenstrauss Lemma: Theoretical Edition", "content": "← Back to Index The Amazing Johnson-Lindenstrauss Lemma: Theoretical Edition By 苏剑林 | September 17, 2021 Today we are going to learn about the Johnson-Lindenstrauss Lemma. Since the name is quite long, we will refer to it as the \"JL Lemma\" hereafter. In my personal opinion, the JL Lemma is one of the miraculous conclusions that every computer science student must understand. It is a famous result regarding dimensionality reduction and serves as one of the classic examples of the counter-intuitive \"curse of dimensionality\" phenomena in high-dimensional spaces. It can be said that the JL Lemma is the theoretical foundation for various dimensionality reduction and hashing techniques in machine learning. Furthermore, in modern machine learning, the JL Lemma provides important theoretical support for our understanding and tuning of model hyperparameters such as dimensions. Logarithmic Dimensions The JL Lemma can be expressed very popularly as: Popular Version of the JL Lemma : To accommodate $N$ vectors, you only need $\\mathcal{O}(\\log N)$ dimensional space. Specifically, the JL Lemma states that regardless of the original dimensionality of these $N$ vectors, we can reduce them to $\\mat"}, {"file": "translation_8706.html", "title": "The Amazing Johnson-Lindenstrauss Lemma: Applications", "content": "← Back to Index The Amazing Johnson-Lindenstrauss Lemma: Applications By 苏剑林 | September 24, 2021 In the previous article \"The Amazing Johnson-Lindenstrauss Lemma: Theoretical Edition\" , we introduced the theoretical derivation of the Johnson-Lindenstrauss Lemma (JL Lemma) in detail. In this piece, we will focus on its applications. As a conclusion inherently related to dimensionality reduction, the most basic application of the JL Lemma is, naturally, as a method for reducing dimensions. However, beyond this direct application, many seemingly unrelated algorithms, such as Locality-Sensitive Hashing (LSH) and Randomized SVD, essentially rely on the JL Lemma. Furthermore, for machine learning models, the JL Lemma often provides a theoretical explanation for our choice of dimensions. Tools for Dimensionality Reduction The JL Lemma provides a very simple and direct \"Random Projection\" approach to dimensionality reduction: Given $N$ vectors $v_1, v_2, \\cdots, v_N \\in \\mathbb{R}^m$, if you want to reduce them to $n$ dimensions, you only need to sample an $n \\times m$ matrix $A$ from $\\mathcal{N}(0, 1/n)$. Then $Av_1, Av_2, \\cdots, Av_N$ are the results after dimensionality reduction. Th"}, {"file": "translation_8711.html", "title": "Analysis of the Usability of the Dimension Formula \"n > 8.33 log N\"", "content": "← Back to Index Analysis of the Usability of the Dimension Formula \"n > 8.33 log N\" By 苏剑林 | September 27, 2021 In the previous article \"Minimum Entropy Principle (VI): How to choose the dimension of word vectors?\" , we derived a word vector dimension formula \"$n > 8.33 \\log N$\" based on minimum entropy principles. Then, in \"The Amazing Johnson-Lindenstrauss Lemma: Application Edition\" , we further pointed out that this result is consistent with the $\\mathcal{O}(\\log N)$ provided by the JL lemma. Since it looks perfect in theory, readers naturally ask: what about the experimental results? Is the coefficient 8.33 optimal? This article provides a brief summary of relevant content regarding this problem. Word Vectors First, we can directly calculate that when $N$ is 100,000, $8.33 \\log N \\approx 96$, and when $N$ is 5 million, $8.33 \\log N \\approx 128$. This indicates that, at least in terms of order of magnitude, the results given by the formula are very much in line with the dimensions we actually use, because in the era of word vectors, the dimensions of the word vectors we trained ourselves were around 100 dimensions. Some readers might question that most open-source word vectors "}, {"file": "translation_8715.html", "title": "Doubts and Communication Regarding the Originality of WhiteningBERT", "content": "← Back to Index Doubts and Communication Regarding the Originality of WhiteningBERT By 苏剑林 | October 09, 2021 In the article \"You Might Not Need BERT-flow: A Linear Transformation Comparable to BERT-flow\" , inspired by BERT-flow, I proposed an alternative solution called BERT-whitening . It is simpler than BERT-flow and achieves comparable or even better results on most datasets. Furthermore, it can be used for dimensionality reduction of sentence vectors to improve retrieval speed. Later, together with several collaborators, I supplemented the experiments for BERT-whitening and wrote it as an English paper, \"Whitening Sentence Representations for Better Semantics and Faster Retrieval\" , which was posted on ArXiv on March 29 this year. However, about a week later, a paper titled \"WhiteningBERT: An Easy Unsupervised Sentence Embedding Approach\" (hereafter referred to as WhiteningBERT ) appeared on ArXiv. Its content heavily overlaps with BERT-whitening. Some readers noticed this and gave me feedback, suggesting that WhiteningBERT might have plagiarized BERT-whitening. This post is to report to concerned readers the results of my communication with the authors of WhiteningBERT. Timel"}, {"file": "translation_8718.html", "title": "Using Dirac Functions to Construct Smooth Approximations of Non-smooth Functions", "content": "← Back to Index Using Dirac Functions to Construct Smooth Approximations of Non-smooth Functions By 苏剑林 | October 10, 2021 In machine learning, we often encounter non-smooth functions. However, our optimization methods are usually gradient-based, which means that smooth models are generally more conducive to optimization (as the gradients are continuous). Consequently, there is a demand for finding smooth approximations of non-smooth functions. In fact, this blog has discussed related topics multiple times, such as \"Seeking a Smooth Maximum Function\" and \"On Function Smoothing: Differentiable Approximations of Non-differentiable Functions\" . However, the previous discussions lacked generality in their methodologies. Recently, I learned a relatively general approach from the paper \"SAU: Smooth activation function using convolution with approximate identities\" : using Dirac functions to construct smooth approximations. How general is it? In theory, any function with a countable number of discontinuities can be smoothly approximated using this method! I find this quite interesting. The Dirac Function In a very early article, \"The Eerie Dirac Function\" , we introduced the Dirac functio"}, {"file": "translation_8725.html", "title": "Thoughts on Dimension Averaging Strategies for Non-Square Matrices in Initialization Methods", "content": "← Back to Index Thoughts on Dimension Averaging Strategies for Non-Square Matrices in Initialization Methods By 苏剑林 | Oct 18, 2021 In articles such as \"Understanding Model Parameter Initialization Strategies from a Geometric Perspective\" and \"A Brief Discussion on Transformer Initialization, Parameterization, and Normalization\" , we discussed initialization strategies for models. The general idea is: if an $n \\times n$ square matrix is initialized with independent and identically distributed (i.i.d.) values with a mean of 0 and a variance of $1/n$, it approximates an orthogonal matrix, allowing the second moment (or variance) of the data to remain roughly constant during propagation. But what if it is an $m \\times n$ non-square matrix? The common approach (Xavier initialization) considers both forward and backward propagation together, thus using an i.i.d. initialization with mean 0 and variance $2/(m+n)$. However, this \"averaging\" is somewhat arbitrary. This article explores whether there might be better averaging schemes. Basic Review Xavier initialization considers a fully connected layer as follows (assuming $m$ input nodes and $n$ output nodes): \\begin{equation} y_j = b_j + \\s"}, {"file": "translation_8728.html", "title": "CAN: A Simple Post-Processing Trick to Improve Classification Performance Using Prior Distribution", "content": "← Back to Index CAN: A Simple Post-Processing Trick to Improve Classification Performance Using Prior Distribution By 苏剑林 | Oct 22, 2021 As the name suggests, this article introduces a post-processing technique for classification problems—CAN (Classification with Alternating Normalization), from the paper \"When in Doubt: Improving Classification Performance with Alternating Normalization\" . According to my tests, CAN indeed improves performance in most multi-class classification scenarios, and it adds almost no predictive cost, as it involves simple re-normalization operations on the prediction results. Interestingly, the core idea behind CAN is very rustic—so rustic that everyone probably uses similar logic in daily life. However, the original CAN paper did not explain this idea very clearly, instead focusing on formal introductions and experiments. In this post, I will try to explain the algorithmic logic clearly. Conceptual Example Suppose we have a binary classification problem. For input $a$, the model's prediction is $p^{(a)} = [0.05, 0.95]$, so we predict class $1$. Next, for input $b$, the model gives $p^{(b)} = [0.5, 0.5]$. At this point, the model is in a state of maximum"}, {"file": "translation_8739.html", "title": "bert4keras in Hand, Baseline I Have: CLUE Benchmark Code", "content": "← Back to Index bert4keras in Hand, Baseline I Have: CLUE Benchmark Code By 苏剑林 | October 31, 2021 CLUE (Chinese GLUE) is an evaluation benchmark for Chinese Natural Language Processing, and it has already gained recognition from many teams. The official CLUE GitHub provides baselines for TensorFlow and PyTorch, but they are not very readable or easy to debug. In fact, whether it's TensorFlow or PyTorch, CLUE or GLUE, the author believes that most available baseline codes are far from user-friendly, and trying to understand them can be quite a painful experience. Therefore, I decided to implement a set of CLUE baselines based on bert4keras . After a period of testing, I have basically reproduced the official benchmark results, and even outperformed them on some tasks. Most importantly, all the code is kept as clear and readable as possible, truly living up to the motto \"Deep Learning for Humans.\" Code Link: https://github.com/bojone/CLUE-bert4keras Code Introduction Below is a brief introduction to the construction ideas for each task baseline in this code. Before reading the article and code, readers are encouraged to observe the data format of each task themselves, as I will not "}, {"file": "translation_8747.html", "title": "Discussion on Model Optimization: Why is BERT's Initial Standard Deviation 0.02?", "content": "← Back to Index Discussion on Model Optimization: Why is BERT's Initial Standard Deviation 0.02? By 苏剑林 | November 08, 2021 A few days ago in a group discussion, the question \"How does the Transformer solve gradient vanishing?\" came up. Answers mentioned residuals, and others mentioned LN (Layer Norm). Are these the correct answers? In fact, this is a very interesting and comprehensive question that relates to many model details, such as \"Why does BERT need warmup?\", \"Why is BERT's initial standard deviation 0.02?\", \"Why add an extra Dense layer before MLM prediction?\", and so on. This article aims to focus on discussing these issues. What Does \"Gradient Vanishing\" Mean? In the article \"Also Talking About the RNN Gradient Vanishing/Explosion Problem\" , we discussed the gradient vanishing problem in RNNs. In fact, the phenomenon of gradient vanishing in general models is similar: it refers to the fact that (mainly in the initial stage of the model) the closer a layer is to the input, the smaller its gradient becomes, tending toward zero or even equalling zero. Since we mainly use gradient-based optimizers, gradient vanishing means we lack a good signal to adjust and optimize the ear"}, {"file": "translation_8757.html", "title": "A New WGAN Scheme: Implementing Lipschitz Constraints through Gradient Normalization", "content": "← Back to Index A New WGAN Scheme: Implementing Lipschitz Constraints through Gradient Normalization By 苏剑林 | November 15, 2021 Currently, the mainstream methods for implementing WGAN include Weight Clipping, Spectral Normalization, and Gradient Penalty. This article introduces a new implementation scheme: Gradient Normalization (GN). This scheme originates from two interesting papers, namely \"Gradient Normalization for Generative Adversarial Networks\" and \"GraN-GAN: Piecewise Gradient Normalization for Generative Adversarial Networks\" . What makes them interesting? As you can see from the titles, these two papers should be highly overlapping, perhaps even by the same authors. However, they are actually papers from two different teams at roughly the same time—one published in ICCV and the other in WACV. They derived nearly identical solutions based on the same assumptions. The degree of overlap in content is so high that I kept thinking they were the same paper. Truly, coincidences are everywhere~ Basic Review We have introduced WGAN many times before, such as in \"The Art of Mutual Confrontation: From Zero to WGAN-GP\" and \"From Wasserstein Distance and Duality Theory to WGAN\" , so "}, {"file": "translation_8764.html", "title": "ChildTuning: Trying to add Dropout to Gradients?", "content": "← Back to Index ChildTuning: Trying to add Dropout to Gradients? By 苏剑林 | Nov 22, 2021 Dropout is a classic approach to prevent overfitting, and most readers are likely familiar with it. Interestingly, Dropout has recently seen something of a \"second spring,\" with new and intriguing variations emerging. For instance, SimCSE and R-Drop have recently sparked heated discussions. In the article \"Dropout Twice Again! This Time It Achieved SOTA in Supervised Tasks\" , we discovered that simple R-Drop can even rival adversarial training, which is quite surprising. Generally, Dropout is added to the output of each layer or to the model parameters—these are the two classic uses of Dropout. However, I recently learned about a novel use from the paper \"Raise a Child in Large Language Model: Towards Effective and Generalizable Fine-tuning\" : adding it to the gradients. Adding Dropout to gradients? Most readers have probably never heard of this. So, how effective is it? Let's take a closer look. General Idea In short, this paper proposes an approach called \"ChildTuning\" to improve the performance of pre-trained models during fine-tuning. \"Child\" stands for \"Children Network,\" referring to select"}, {"file": "translation_8770.html", "title": "MLM and MAE from a Dropout Perspective: Some New Insights", "content": "← Back to Index MLM and MAE from a Dropout Perspective: Some New Insights By 苏剑林 | November 29, 2021 As everyone knows, the inconsistency of BERT's MLM (Masked Language Model) task between pre-training and fine-tuning—specifically, the appearance of [MASK] during pre-training and its absence during downstream fine-tuning—is a frequently criticized issue. Many works believe this is a significant factor affecting BERT's fine-tuning performance and have proposed improvements such as XL-NET , ELECTRA , and MacBERT . In this article, we will analyze this inconsistency of MLM from the perspective of Dropout and propose a simple operation to correct it. A similar analysis can be applied to the MAE (Masked Autoencoder) model recently proposed by Kaiming He. The results show that MAE indeed possesses better consistency compared to MLM, from which we can derive a regularization technique that may accelerate training speed. Dropout First, let's revisit Dropout. From a mathematical standpoint, Dropout is an operation that introduces random noise into a model via a Bernoulli distribution. Let's briefly review the Bernoulli distribution. Bernoulli Distribution The Bernoulli distribution is perha"}, {"file": "translation_8783.html", "title": "Start with Nonsense, Fabricate the Data? Truly Outraged by a \"Divine Paper\"", "content": "← Back to Index Start with Nonsense, Fabricate the Data? Truly Outraged by a \"Divine Paper\" By 苏剑林 | December 04, 2021 This article discusses my experience of being outraged by a \"divine paper\" that came out yesterday. This \"divine paper\" is 《How not to Lie with a Benchmark: Rearranging NLP Leaderboards》 . The general content of the paper argues that many current leaderboards use the arithmetic mean for averaging, while it contends that the geometric mean and harmonic mean are more reasonable. Most critically, it re-calculates the rankings for models on leaderboards like GLUE and SuperGLUE using geometric and harmonic means, claiming to find that models which previously surpassed humans no longer do so under the new averaging schemes. Doesn't that sound quite interesting? I thought so too, which is why I planned to write a blog post to introduce it. However, as I was finishing the post and cross-checking the data, I discovered that the data in the tables was completely fabricated!!! The actual results do not support its conclusions at all!!! Consequently, this blog post has turned from a \"commendation ceremony\" into a \"criticism session\"... Talking Nonsense First, let's look at the"}, {"file": "translation_8791.html", "title": "Variational Autoencoder (8): Estimating Sample Probability Density", "content": "← Back to Index Variational Autoencoder (8): Estimating Sample Probability Density By 苏剑林 | December 09, 2021 In the previous articles of this series, we have understood VAE from multiple perspectives. Generally, VAE is used to obtain a generative model or a better encoding model, which are the standard applications of VAE. However, besides these common uses, there are some \"niche requirements,\" such as estimating the probability density of $x$, which is often used in tasks like compression. This article will explore and derive the VAE model from the perspective of estimating probability density. Two Problems The estimation of probability density involves taking known samples $x_1, x_2, \\dots, x_N \\sim \\tilde{p}(x)$ and using a parametric family of probability densities $q_{\\theta}(x)$ to fit these samples. The general goal is to minimize the negative log-likelihood: \\begin{equation}\\mathbb{E}_{x\\sim \\tilde{p}(x)}[-\\log q_{\\theta}(x)] = -\\frac{1}{N}\\sum_{i=1}^N \\log q_{\\theta}(x_i)\\label{eq:mle}\\end{equation} But this is purely a theoretical form, and several problems remain to be solved, primarily categorized into two major questions: 1. What kind of $q_{\\theta}(x)$ should be used"}, {"file": "translation_8796.html", "title": "An Inequality Between Input Gradient Penalty and Parameter Gradient Penalty", "content": "← Back to Index An Inequality Between Input Gradient Penalty and Parameter Gradient Penalty By 苏剑林 | December 11, 2021 In this blog, we have discussed gradient penalty-related content multiple times. In terms of form, gradient penalty terms are divided into two types. One is the gradient penalty with respect to the input $\\Vert\\nabla_{\\boldsymbol{x}} f(\\boldsymbol{x};\\boldsymbol{\\theta})\\Vert^2$, which we discussed in articles such as \"A Brief Talk on Adversarial Training: Meaning, Methods, and Reflections (with Keras Implementation)\" and \"Random Thoughts on Generalization: From Random Noise and Gradient Penalty to Virtual Adversarial Training\" . The other type is the gradient penalty with respect to parameters $\\Vert\\nabla_{\\boldsymbol{\\theta}} f(\\boldsymbol{x};\\boldsymbol{\\theta})\\Vert^2$, which we discussed in articles like \"Optimization Algorithms from a Dynamical Perspective (V): Why the Learning Rate Should Not Be Too Small?\" and \"Do We Really Need to Reduce Training Set Loss to Zero?\" . In these related articles, both types of gradient penalties are claimed to have the ability to improve the generalization performance of the model. So, is there any connection between the two"}, {"file": "translation_8802.html", "title": "Seq2Seq+Prefix Tree: A New Paradigm for Retrieval Tasks (Taking KgCLUE as an Example)", "content": "← Back to Index Seq2Seq+Prefix Tree: A New Paradigm for Retrieval Tasks (Taking KgCLUE as an Example) By 苏剑林 | Dec 17, 2021 Two years ago, in \"Universal Seq2Seq: Reading Comprehension Question Answering Based on Seq2Seq\" and \"'Non-Autoregressive' Is Not Bad Either: MLM-Based Reading Comprehension Question Answering\" , we explored using \"Seq2Seq+Prefix Tree\" and \"MLM+Prefix Tree\" formats for extractive reading comprehension tasks and achieved good results. At ICLR 2021 last year, Facebook's paper \"Autoregressive Entity Retrieval\" also utilized the \"Seq2Seq+Prefix Tree\" combination, achieving a \"win-win\" in both effectiveness and efficiency for entity linking and document retrieval. In fact, the \"Seq2Seq+Prefix Tree\" combination can theoretically be applied to any retrieval-type task, making it a \"new paradigm\" for retrieval tasks. This article will once again review the ideas behind \"Seq2Seq+Prefix Tree\" and use it to implement a baseline for the recently released KgCLUE (Knowledge Graph Question Answering) benchmark . Retrieval Tasks Speaking of retrieval tasks, everyone is likely familiar with them. Besides conventional similar question retrieval, many tasks in NLP can be viewed a"}, {"file": "translation_8823.html", "title": "Understanding Attention Scaling from the Perspective of Entropy Invariance", "content": "← Back to Index Understanding Attention Scaling from the Perspective of Entropy Invariance By 苏剑林 | December 21, 2021 The most widely used attention mechanism in current Transformer architectures is the \"Scaled Dot-Product Attention.\" The term \"Scaled\" refers to the fact that after the transpose multiplication of $Q$ and $K$, the result is divided by $\\sqrt{d}$ before applying the Softmax (without loss of generality, we assume $Q,K,V\\in\\mathbb{R}^{n\\times d}$): \\begin{equation}Attention(Q,K,V) = softmax\\left(\\frac{QK^{\\top}}{\\sqrt{d}}\\right)V\\label{eq:std}\\end{equation} In \"Brief Discussion on Initialization, Parameterization, and Standardization of Transformer,\" we previously explained the reasoning behind dividing by $\\sqrt{d}$. In this article, the author will understand this scaling operation from the perspective of \"entropy invariance\" and derive a new scaling factor. Experiments in MLM (Masked Language Model) show that the new scaling factor possesses better length extrapolation performance. Entropy Invariance We rewrite the general Scaled Dot-Product Attention as: \\begin{equation}\\boldsymbol{o}_i = \\sum_{j=1}^n a_{i,j}\\boldsymbol{v}_j,\\quad a_{i,j}=\\frac{e^{\\lambda \\boldsymb"}, {"file": "translation_8829.html", "title": "Entropy Normalization of Probability Distributions", "content": "← Back to Index Entropy Normalization of Probability Distributions By 苏剑林 | December 24, 2021 In the previous article \"From Entropy Invariance to Attention Scaling\" , we derived a new Attention Scale from the perspective of entropy invariance. Experiments showed that this new scale, which maintains entropy invariance, indeed results in better extrapolation performance for Attention. At that point, a natural question occurred to me: Is there an operation similar to L2 Normalization that can directly transform a probability distribution such that it maintains its original distribution characteristics while setting its entropy to a specified value? I searched around but found no similar research. So, I attempted to derive it myself and obtained a basically satisfactory result, which I refer to as \"Entropy Normalization.\" I am recording it here for readers who may find it useful. Power Transformation First, assume an $n$-dimensional distribution $(p_1, p_2, \\cdots, p_n)$. Its entropy is defined as: \\begin{equation}\\mathcal{H} = -\\sum_i p_i \\log p_i = \\mathbb{E}[-\\log p_i]\\end{equation} Since $p_i \\in [0,1]$, we have $-p_i \\log p_i \\geq 0$, and thus $\\mathcal{H} \\geq 0$. When one $p_i$ "}, {"file": "translation_8833.html", "title": "SquarePlus: Possibly the Simplest Algebraic Smooth Approximation of ReLU", "content": "← Back to Index SquarePlus: Possibly the Simplest Algebraic Smooth Approximation of ReLU By 苏剑林 | December 29, 2021 By Jerry Su | 2021-12-29 The ReLU function, namely $\\max(x,0)$, is one of the most common activation functions. However, its non-differentiability at $x=0$ is often regarded as a \"drawback.\" Consequently, various smooth approximations have been proposed, such as SoftPlus, GeLU, Swish, etc. However, all of these smooth approximations use at least one exponential operation $e^x$ (SoftPlus even uses a logarithm). From an \"efficiency-conscious\" perspective, the computational cost is not negligible (although with modern GPU acceleration, we rarely perceive this overhead). Recently, a paper titled \"Squareplus: A Softplus-Like Algebraic Rectifier\" proposed a simpler approximation called SquarePlus. We will discuss it here. It should be pointed out beforehand that I do not recommend spending too much time on the selection and design of activation functions. Therefore, while I am sharing this paper, it is mainly to provide a reference result and serve as an exercise for everyone to \"practice.\" Definition The form of SquarePlus is very simple, using only addition, multiplicatio"}, {"file": "translation_8847.html", "title": "CoSENT (1): A More Effective Sentence Vector Scheme than Sentence-BERT", "content": "← Back to Index CoSENT (1): A More Effective Sentence Vector Scheme than Sentence-BERT By 苏剑林 | January 06, 2022 Approaches for learning sentence vectors can generally be divided into two categories: unsupervised and supervised. Among supervised sentence vector schemes, the mainstream approach was \"InferSent\" proposed by Facebook, followed by \"Sentence-BERT,\" which further confirmed its effectiveness on top of BERT. However, both InferSent and Sentence-BERT remain theoretically somewhat confusing. Although they are effective, they suffer from an inconsistency between training and prediction; furthermore, if one directly optimizes the prediction target (the cosine value), the performance is usually particularly poor. Recently, I reconsidered this issue. After nearly a week of analysis and experimentation, I have roughly determined the reasons why InferSent is effective and why direct optimization of the cosine value fails. I have proposed a new scheme for optimizing cosine values called CoSENT ( Co sine SENT ence). Experiments show that CoSENT generally outperforms both InferSent and Sentence-BERT in terms of convergence speed and final performance. Naive Idea The scenario in this a"}, {"file": "translation_8860.html", "title": "CoSENT (2): How Big is the Gap Between Representation-Based and Interaction-Based Matching?", "content": "← Back to Index CoSENT (2): How Big is the Gap Between Representation-Based and Interaction-Based Matching? By 苏剑林 | January 12, 2022 Generally speaking, text matching has two implementation schemes: interaction-based and representation-based. Interaction-based refers to concatenating two texts together and treating them as a single text for classification. Representation-based refers to two sentences being separately encoded by an encoder into sentence vectors, followed by simple fusion processing (calculating cosine similarity or passing through a shallow network). The usual conclusion is that interaction-based methods typically achieve better accuracy because they allow for thorough comparison between the two texts, but they have the clear disadvantage of poor efficiency in retrieval scenarios. Representation-based methods, on the other hand, can pre-calculate and cache sentence vectors, offering high efficiency, but since the level of interaction between sentences is shallow, their performance is usually inferior to interaction-based methods. In the previous article, I introduced CoSENT, which is essentially a representation-based scheme and shows improved performance over prev"}, {"file": "translation_8870.html", "title": "Chatting about Multi-Task Learning (I): In the Name of Loss", "content": "← Back to Index Chatting about Multi-Task Learning (I): In the Name of Loss By 苏剑林 | January 18, 2022 There are many methods to improve model performance, and Multi-Task Learning (MTL) is one of them. Simply put, MTL aims to train multiple related tasks together, hoping that the tasks can complement and promote each other to achieve better results (accuracy, robustness, etc.) than a single task. However, MTL is not as simple as stacking all tasks together; how to balance the training of each task so that each task obtains a beneficial improvement remains a subject worthy of research. Recently, by coincidence, I have also made some attempts at multi-task learning and took the opportunity to learn about it. I have selected some results to exchange and discuss with everyone here. Weighted Sum From the perspective of the loss function, multi-task learning involves multiple loss functions $\\mathcal{L}_1,\\mathcal{L}_2,\\cdots,\\mathcal{L}_n$. Generally, they have a large number of shared parameters and a small number of independent parameters. Our goal is to make each loss function as small as possible. To this end, we introduce weights $\\alpha_1,\\alpha_2,\\cdots,\\alpha_n\\geq 0$ and convert"}, {"file": "translation_8877.html", "title": "Efficient GlobalPointer: Fewer Parameters, Better Results", "content": "← Back to Index Efficient GlobalPointer: Fewer Parameters, Better Results By 苏剑林 | January 25, 2022 In \"GlobalPointer: A Unified Way to Handle Nested and Non-Nested NER\" , we proposed a token-pair recognition module named \"GlobalPointer.\" When applied to NER, it can handle both nested and non-nested tasks in a unified manner. In non-nested scenarios, it offers faster speeds than CRF and performance that is not inferior to CRF. In other words, based on current experimental results, at least for NER scenarios, we can safely replace CRF with GlobalPointer without worrying about losses in performance or speed. In this article, we propose an improved version of GlobalPointer—Efficient GlobalPointer. It primarily targets the issue of low parameter utilization in the original GlobalPointer, significantly reducing the number of parameters. More interestingly, experimental results from multiple tasks show that Efficient GlobalPointer, despite having fewer parameters, actually achieves better results. Massive Parameters Let's briefly review GlobalPointer; for a detailed introduction, readers are referred to \"GlobalPointer: A Unified Way to Handle Nested and Non-Nested NER\" . Simply put, Glob"}, {"file": "translation_8888.html", "title": "GPLinker: Entity-Relation Joint Extraction based on GlobalPointer", "content": "← Back to Index GPLinker: Entity-Relation Joint Extraction based on GlobalPointer By 苏剑林 | January 30, 2022 Nearly three years ago, during Baidu's \"2019 Language and Intelligence Technology Competition\" (hereafter referred to as LIC2019), I proposed a new relation extraction model (refer to \"A Lightweight Information Extraction Model Based on DGCNN and Probabilistic Graphs\" ), which was later further published and named \" CasRel ,\" and was considered the SOTA for relation extraction at the time. However, when CasRel was proposed, I was actually new to the field, so in hindsight, CasRel still had many imperfections. I had thought about further improving it later, but hadn't come up with a particularly good design. Later, I proposed GlobalPointer and the recent Efficient GlobalPointer , feeling that I now had sufficient \"materials\" to construct a new relation extraction model. Starting from the probabilistic graph perspective and referencing some SOTA designs following CasRel, I finally arrived at a model version similar to TPLinker. Basic Idea At first glance, relation extraction is the extraction of triplets $(s, p, o)$ (i.e., subject, predicate, object). However, in specific imple"}, {"file": "translation_8896.html", "title": "Multitask Learning Chat (Part 2): Acting via Gradients", "content": "← Back to Index Multitask Learning Chat (Part 2): Acting via Gradients By 苏剑林 | February 8, 2022 In \"Multitask Learning Chat (Part 1): In the Name of Loss\" , we initially explored the multitask learning problem from the perspective of loss functions. We ultimately found that if we want the results to possess both scaling invariance and translation invariance, choosing the reciprocal of the gradient norm as the task weight is a relatively simple choice. We further analyzed that this design is equivalent to normalizing each task's gradient individually and then summing them. This means the \"battlefield\" of multitask learning has shifted from the loss function to the gradient: what appears to be designing the loss function is, in fact, designing better gradients—what we call \"In the name of loss, acting via gradients.\" So, what are the standards for \"better\" gradients? How can we design them? In this article, we will understand multitask learning from the perspective of gradients, attempting to build multitask learning algorithms directly from the idea of designing gradients. Overall Idea We know that for single-task learning, the commonly used optimization method is gradient descent."}, {"file": "translation_8907.html", "title": "Multi-Task Learning (Part 3): The Order of Primary and Secondary", "content": "← Back to Index Multi-Task Learning (Part 3): The Order of Primary and Secondary By 苏剑林 | February 14, 2022 Multi-task learning (MTL) is a very broad topic, and the objectives of MTL vary in different scenarios. In \"Multi-Task Learning (Part 1): In the Name of Loss\" and \"Multi-Task Learning (Part 2): By Way of Gradient\" , we understood the goal of MTL as \"doing every task well,\" which translates to \"treating every task as equally as possible.\" We can call this \"Parallel Multi-Task Learning.\" However, not all MTL objectives are like this. In many scenarios, our primary goal is to learn a specific main task well, while the other tasks serve only as auxiliary tasks, added in the hope of improving the performance of the main task. We can call this type of scenario \"Primary-Secondary Multi-Task Learning.\" In this context, if we continue to use the learning scheme of Parallel MTL aimed at \"doing every task well,\" it may significantly degrade the performance of the main task. Therefore, this article continues the idea of \"acting by gradient\" to explore training schemes for Primary-Secondary MTL. Objective Form In this article, we assume the reader has already read and basically understood"}, {"file": "translation_8926.html", "title": "GPLinker: Event Joint Extraction based on GlobalPointer", "content": "← Back to Index GPLinker: Event Joint Extraction based on GlobalPointer By Su Jianlin | Feb 21, 2022 | Information Age About two years ago, I first encountered the event extraction task in Baidu's \" 2020 Language and Intelligent Technology Competition .\" In the article \"bert4keras in hand, I have the baseline: Baidu LIC2020,\" I shared a simple baseline that converted it into NER using BERT+CRF. However, that baseline was more of a placeholder—a semi-finished product—and could not be called a complete event extraction model. Over the past two years, while relation extraction models have emerged one after another with SOTA results constantly being updated, event extraction has not seen many particularly brilliant designs. Recently, I revisited the event extraction task. Building upon my previous relation extraction model GPLinker , and combining it with complete subgraph searching, I designed a relatively simple but comprehensive joint event extraction model, which I still call GPLinker. I invite everyone to comment on it. Task Introduction Event extraction is a comprehensive task. A standard event extraction sample is as follows: Standard event extraction sample (Image from Baidu Du"}, {"file": "translation_8934.html", "title": "FLASH: Probably the Most Interesting Efficient Transformer Design Recently", "content": "← Back to Index FLASH: Probably the Most Interesting Efficient Transformer Design Recently By 苏剑林 | February 25, 2022 Efficient Transformers, which generally refer to works that improve the efficiency of Transformer models, are something I have been following for a long time. My earliest blog post on this topic dates back to 2019, \"Born for Economy: From Standard Attention to Sparse Attention\" , at a time when there were very few works in this area. Later, such works gradually increased, and I followed several of them, such as Linear Attention , Performer , and Nyströmformer , and even performed some explorations myself, such as the \"Transformer Upgrade Path\" . Subsequently, as related works became more numerous and often quite repetitive, I stopped paying close attention. Feeling like \"welcome rain after a long drought,\" a very interesting work on efficient Transformers has recently appeared—Google's \"Transformer Quality in Linear Time\" . After reading it carefully, I believe it is truly full of surprises. What’s the Surprise? What kind of results deserve the description \"surprising\"? Is it an exaggeration? Let's first look at what the paper achieves: 1. It proposes a new Transfor"}, {"file": "translation_8968.html", "title": "Exponentiated Gradient Descent + Meta-Learning = Adaptive Learning Rate", "content": "← Back to Index Exponentiated Gradient Descent + Meta-Learning = Adaptive Learning Rate By 苏剑林 | March 03, 2022 A couple of days ago, I came across a paper from Google titled \"Step-size Adaptation Using Exponentiated Gradient Updates\" , where I learned some new concepts, so I am documenting and sharing them here. There are two main pieces of content: one is Exponentiated Gradient Descent for non-negative optimization, and the other is a learning rate adjustment algorithm based on the idea of meta-learning. Both are quite interesting, and readers who are interested can also take a look. Exponentiated Gradient Descent You have likely heard much about gradient descent, which refers to the minimization of an unconstrained function $\\mathcal{L}(\\boldsymbol{\\theta})$ using the following update format: \\begin{equation}\\boldsymbol{\\theta}_{t+1} = \\boldsymbol{\\theta}_t - \\eta\\nabla_{\\boldsymbol{\\theta}}\\mathcal{L}(\\boldsymbol{\\theta}_t)\\end{equation} where $\\eta$ is the learning rate. However, many tasks are not always unconstrained. For the simplest non-negative constraints, we can instead use the following update format: \\begin{equation}\\boldsymbol{\\theta}_{t+1} = \\boldsymbol{\\theta}_t \\o"}, {"file": "translation_8978.html", "title": "What are the difficulties in training a 1000-layer Transformer?", "content": "← Back to Index What are the difficulties in training a 1000-layer Transformer? By 苏剑林 | March 09, 2022 As is well known, current Transformers are growing larger, but this \"largeness\" is usually in \"width\" rather than \"depth.\" For example, although GPT-3 has hundreds of billions of parameters, it is only a 96-layer Transformer, which is far from the depth we can imagine. What limits the development of Transformers toward greater \"depth\"? Some readers might think it's computational power, but a \"wide and shallow\" model doesn't require significantly less computational power than a \"narrow and deep\" model. Therefore, computational power is not the main constraint; ultimately, it boils down to the inherent training difficulties of Transformers. The general view is that the training difficulty of deep models stems from gradient vanishing or exploding. However, practice shows that even when gradients are improved through various means, deep models are still not easy to train. Recent work (such as Admin ) points out that the fundamental difficulty in training deep models lies in \"increment explosion\"—the deeper the model, the greater the perturbation to the output. Last week's paper, \"Dee"}, {"file": "translation_8990.html", "title": "Does the Gated Attention Unit (GAU) Still Need Warmup?", "content": "← Back to Index Does the Gated Attention Unit (GAU) Still Need Warmup? By 苏剑林 | March 11, 2022 After the article \"What are the difficulties in training a 1000-layer Transformer?\" was published, a reader soon asked: what would the results be if that logic were applied to the \"Gated Attention Unit (GAU)\" from \"FLASH: Probably the Most Interesting Efficient Transformer Design Recently\" ? How does it differ from the standard Transformer results? This article discusses that question. Conclusion First In fact, GAU is a very easy-to-train model. Even if we directly use \"Post Norm + Xavier initialization\" without any adjustments, we can easily train a GAU with dozens of layers without needing Warmup. Therefore, many training techniques for standard Transformers may find no use here in GAU. Why can GAU achieve this? Quite simply, because under default settings, theoretically $\\text{GAU}(\\boldsymbol{x}_l)$ is nearly two orders of magnitude smaller than $\\boldsymbol{x}_l$, so: \\begin{equation}\\boldsymbol{x}_{l+1} = \\text{LN}(\\boldsymbol{x}_l + \\text{GAU}(\\boldsymbol{x}_l))\\approx \\boldsymbol{x}_l\\end{equation} Consequently, when paired with residual connections, GAU is already very close to a"}, {"file": "translation_8994.html", "title": "Why Are Residuals Needed? A Perspective from DeepNet", "content": "← Back to Index Why Are Residuals Needed? A Perspective from DeepNet By 苏剑林 | March 19, 2022 In \"What Are the Difficulties in Training a 1000-Layer Transformer?\" , we introduced the DeepNet technology proposed by Microsoft, which enables training a 1000-layer Transformer. Readers generally have two reactions to DeepNet: one is to be amazed and give it a thumbs up, while the other is to feel it’s just \"old wine in a new bottle\" and uninteresting. The latter reaction often stems from the fact that the two improvements proposed by DeepNet—increasing the weight of the identity path and reducing the residual branch initialization—are quite common, and similar conclusions have appeared in other works. Therefore, it is hard to find anything particularly novel in them. Admittedly, from the perspective of conclusions alone, DeepNet is indeed not that interesting. However, I believe that the process of DeepNet is far more important than the conclusions. Its interesting aspect lies in providing a concise and effective gradient magnitude analysis approach that can be used to analyze many related issues. For example, the question we will discuss in this article—\"Why are residuals needed?\"—can b"}, {"file": "translation_8998.html", "title": "RoFormerV2: Exploration of the Limits of Natural Language Understanding", "content": "← Back to Index RoFormerV2: Exploration of the Limits of Natural Language Understanding By 苏剑林 | March 21, 2022 About a year ago, we proposed Rotary Position Embedding (RoPE) and released the corresponding pre-trained model RoFormer . Over time, RoFormer has fortunately received increasing attention and recognition. For example, EleutherAI's newly released 6 billion and 20 billion parameter GPT models utilize RoPE position embeddings. Furthermore, Google's newly proposed FLASH model paper explicitly points out that RoPE has a significant positive effect on Transformer performance. At the same time, we have been continuously trying to further strengthen the RoFormer model, attempting to take RoFormer's performance \"to the next level.\" After nearly half a year of effort, we believe we have achieved quite good results, which we are now officially releasing as \"RoFormerV2\": Github: https://github.com/ZhuiyiTechnology/roformer-v2 Exploration of the Limits Since the rise of pre-trained models, many researchers have been quite interested in one question: Where is the limit of pre-trained models? Of course, the word \"limit\" has many meanings. A series of works represented by GPT-3 aims to "}, {"file": "translation_9009.html", "title": "Why is Pre Norm Less Effective Than Post Norm?", "content": "← Back to Index Why is Pre Norm Less Effective Than Post Norm? By 苏剑林 | March 29, 2022 The comparison between Pre Norm and Post Norm is a \"long-standing\" topic. This blog has discussed this issue multiple times, such as in the articles \"A Brief Discussion on Initialization, Parameterization, and Standardization of Transformers\" and \"Model Optimization Talk: Why is the Initial Standard Deviation of BERT 0.02?\" . Currently, the most established conclusion is that under the same settings, the Pre Norm structure is often easier to train, but its final performance is usually not as good as Post Norm. It is easy to understand why Pre Norm is easier to train because its identity path is more prominent, but why is its performance not as good? I didn't have a good answer for this until recently when I saw a reply from @Tang Xianghao on Zhihu, which gave me a \"sudden realization.\" It turns out there is a very intuitive understanding of this problem! Let's learn about it together in this article. Basic Conclusion The formulas for Pre Norm and Post Norm are as follows: \\begin{align}\n\\text{Pre Norm: } \\quad \\boldsymbol{x}_{t+1} = \\boldsymbol{x}_t + F_t(\\text{Norm}(\\boldsymbol{x}_t))\\\\\n\\text{Pos"}, {"file": "translation_9019.html", "title": "It Is Said That Attention and Softmax Go Better Together", "content": "← Back to Index It Is Said That Attention and Softmax Go Better Together By 苏剑林 | April 07, 2022 I don't know if you've noticed a detail: the current mainstream NLP pre-training mode is carried out on a fixed length (such as 512), and then the pre-trained model is directly applied to tasks of different lengths. It seems that no one has ever doubted this mode, as if it's \"taken for granted\" that the model can automatically generalize to different lengths. Of course, I hadn't questioned this either until a few days ago when I conducted Base-version GAU experiments. I found that the length generalization ability of GAU was not as good as imagined. After further analysis, I finally understood that this length generalization ability is not \"naturally occurring\"... Model Review In \"FLASH: Perhaps the Most Interesting Efficient Transformer Design Recently,\" we introduced the \"Gated Attention Unit (GAU),\" which is a new design that integrates GLU and Attention. Aside from its performance, GAU brought us two main design shocks: first, it showed that single-head attention is not necessarily inferior to multi-head attention, which established its status as being \"fast\" and \"efficient\"; second"}, {"file": "translation_9034.html", "title": "A Quick Derivation of Entropy-Invariant Softmax", "content": "← Back to Index A Quick Derivation of Entropy-Invariant Softmax By 苏剑林 | April 11, 2022 In the article \"From Entropy Invariance to the Scale Operation in Attention\" , we derived a version of the attention mechanism with entropy-invariant properties: \\begin{equation}Attention(Q,K,V) = softmax\\left(\\frac{\\kappa \\log n}{d}QK^{\\top}\\right)V\\label{eq:a}\\end{equation} It can be observed that this is mainly achieved by introducing a length-related scaling factor $\\log n$ into the Softmax. The original derivation was quite cumbersome and relied on several assumptions, which were not conducive to an intuitive understanding. This article provides a relatively concise and rapid derivation as a supplement. Derivation Process We can set aside the background of the attention mechanism and directly assume $s_1, s_2, \\dots, s_n \\in \\mathbb{R}$, defining: $$p_i = \\frac{e^{\\lambda s_i}}{\\sum_{i=1}^n e^{\\lambda s_i}}$$ Obviously, this is the result of applying Softmax to $s_1, s_2, \\dots, s_n$ after multiplying them by a scaling factor $\\lambda$. Now let us calculate its entropy: \\begin{equation}\\begin{aligned}H =&\\, -\\sum_{i=1}^n p_i \\log p_i = \\log\\sum_{i=1}^n e^{\\lambda s_i} - \\lambda\\sum_{i=1}^n "}, {"file": "translation_9039.html", "title": "What Should \"KL Divergence\" Look Like Under GlobalPointer?", "content": "← Back to Index What Should \"KL Divergence\" Look Like Under GlobalPointer? By 苏剑林 | April 15, 2022 Recently, a reader mentioned wanting to test the combined effect of GlobalPointer and R-Drop , but was unsure how to calculate the KL divergence under GlobalPointer. Regularization techniques like R-Drop or Virtual Adversarial Training require calculating the KL divergence between probability distributions. However, since the output of GlobalPointer is not a probability distribution, it cannot be calculated directly. After some exploration, I have identified a usable form and verified its feasibility through simple experiments. I will introduce my analysis process here. Symmetric Divergence KL divergence is a function of two probability distributions. It is asymmetric, meaning $KL(p\\Vert q)$ is generally not equal to $KL(q\\Vert p)$. In practical applications, we usually use the symmetrized KL divergence: \\begin{equation}D(p,q) = KL(p\\Vert q) + KL(q\\Vert p)\\end{equation} Substituting the definition of KL divergence $KL(p\\Vert q)=\\sum\\limits_i p_i\\log\\frac{p_i}{q_i}$, we can simplify it to obtain: \\begin{equation}D(p,q) = \\sum_i (p_i - q_i)(\\log p_i - \\log q_i)\\end{equation} Considering"}, {"file": "translation_9046.html", "title": "Does Your Language Model Have \"Unpredictable Words\"?", "content": "← Back to Index Does Your Language Model Have \"Unpredictable Words\"? By 苏剑林 | April 20, 2022 As is common knowledge, classification models usually obtain an encoding vector first, then connect it to a Dense layer to predict the probability of each category, and the output during prediction is the category with the highest probability. But have you ever wondered about this possibility: a trained classification model might have \"unpredictable categories\"? That is, no matter what the input is, it is impossible to predict a certain category $k$; category $k$ can never become the one with the highest probability. Of course, this situation generally only occurs in scenarios where the number of categories far exceeds the dimension of the encoding vector; conventional classification problems are rarely so extreme. However, we know that a language model is essentially a classification model, and its number of categories—the total size of the vocabulary—often far exceeds the vector dimension. So, does our language model have \"unpredictable words\"? (Considering only Greedy decoding). Do they exist? The ACL 2022 paper \"Low-Rank Softmax Can Have Unargmaxable Classes in Theory but Rarely in Prac"}, {"file": "translation_9052.html", "title": "GAU-α: Experiencing the \"Faster, Better, and More Efficient\" Next-Generation Attention", "content": "← Back to Index GAU-α: Experiencing the \"Faster, Better, and More Efficient\" Next-Generation Attention By 苏剑林 | April 22, 2022 In \"FLASH: Probably the Most Interesting Efficient Transformer Design Recently\" , we introduced the GAU (Gated Attention Unit). I am personally inclined to call it the \"most promising next-generation Attention design\" because it truly achieves the characteristics of being \"faster (speed), better (performance), and more efficient (memory).\" However, some readers have obtained opposite results in their own tests, such as slower convergence or worse performance, which differs greatly from my own testing results. This article aims to share my own training experience and release a \"taster\" version, \"GAU-α,\" for everyone to test. Open Source Address: https://github.com/ZhuiyiTechnology/GAU-alpha GAU-α First, let's introduce the scorecard of the open-sourced \"GAU-α\" on CLUE tasks: \\[\\small{\\begin{array}{c|ccccccccccc}\n  \\hline\n  & \\text{iflytek} & \\text{tnews} & \\text{afqmc} & \\text{cmnli} & \\text{ocnli} & \\text{wsc} & \\text{csl} & \\text{cmrc2018} & \\text{c3} & \\text{chid} & \\text{cluener}\\\\\n  \\hline\n  \\text{BERT} & 60.06 & 56.80 & 72.41 & 79.56 & 73.93 & 78.62 & "}, {"file": "translation_9059.html", "title": "Using Mixed Precision and XLA to Accelerate Training in bert4keras", "content": "← Back to Index Using Mixed Precision and XLA to Accelerate Training in bert4keras By 苏剑林 | April 28, 2022 Previously, I have always focused on model conception and implementation, rarely paying attention to model training acceleration. Although I had heard of technologies like mixed precision and XLA, I had never truly put them into practice. Over the past two days, after some tinkering, I successfully used mixed precision and XLA to accelerate training in bert4keras. Here is a brief summary for your reference. Most of the empirical conclusions in this article are not limited to use within bert4keras. The reason bert4keras is emphasized in the title is simply that the model implementations in bert4keras are relatively structured, making the modifications required to enable these acceleration techniques relatively minimal. Experimental Environment The graphics card used for the experiments in this article is an RTX 3090, and the Docker image used is nvcr.io/nvidia/tensorflow:21.09-tf1-py3 , which comes with TensorFlow version 1.15.5. Additionally, the version of bert4keras used in the experiments is 0.11.3. Other environments can also be set up by referring to this, but be sure to "}, {"file": "translation_9064.html", "title": "ZLPR: A Novel Loss for Multi-label Classification", "content": "← Back to Index ZLPR: A Novel Loss for Multi-label Classification By 苏剑林 | May 07, 2022 (Note: The relevant content of this article has been compiled into the paper \"ZLPR: A Novel Loss for Multi-label Classification\" . If you need to cite it, you can cite the English paper directly, thank you.) In \"Promoting 'Softmax + Cross-Entropy' to Multi-Label Classification\" , we proposed a loss function for multi-label classification: \\begin{equation}\\log \\left(1 + \\sum\\limits_{i\\in\\Omega_{neg}} e^{s_i}\\right) + \\log \\left(1 + \\sum\\limits_{j\\in\\Omega_{pos}} e^{-s_j}\\right)\\label{eq:original}\\end{equation} This loss function possesses the advantages of \"Softmax + Cross-Entropy\" in single-label classification and works effectively even when the positive and negative classes are imbalanced. However, as can be seen from its form, it is only applicable to \"hard labels,\" which means techniques like label smoothing and mixup cannot be used. This article attempts to solve this problem by proposing a soft label version of the aforementioned loss function. A Clever Connection The classic approach to multi-label classification is to transform it into multiple binary classification problems, where each "}, {"file": "translation_9070.html", "title": "A Few Inequalities for the logsumexp Operation", "content": "← Back to Index A Few Inequalities for the logsumexp Operation By 苏剑林 | May 10, 2022 $\\text{logsumexp}$ is an operation frequently encountered in machine learning, especially in the implementation and derivation of cross-entropy. At the same time, it is a smooth approximation of the $\\max$ function (refer to \"Seeking a Smooth Maximum Function\" ). Let $x=(x_1, x_2, \\cdots, x_n)$; $\\text{logsumexp}$ is defined as: \\begin{equation}\\text{logsumexp}(x) = \\log \\sum_{i=1}^n e^{x_i}\\end{equation} This article introduces several inequalities related to $\\text{logsumexp}$ that may be useful in theoretical derivations. Basic Bounds Let $x_{\\max} = \\max(x_1, x_2, \\cdots, x_n)$. Then it is obvious that: \\begin{equation}e^{x_{\\max}} < \\sum_{i=1}^n e^{x_i} \\leq \\sum_{i=1}^n e^{x_{\\max}} = ne^{x_{\\max}}\\end{equation} Taking the logarithm of each part gives: \\begin{equation}x_{\\max} < \\text{logsumexp}(x) \\leq x_{\\max} + \\log n\\end{equation} This is the most basic result regarding the upper and lower bounds of $\\text{logsumexp}$. It indicates that the approximation error of $\\text{logsumexp}$ relative to $\\max$ does not exceed $\\log n$. Note that this error is independent of $x$ itself. Thus, we hav"}, {"file": "translation_9079.html", "title": "When BERT-whitening Introduces Hyperparameters: There Is Always One That Suits You", "content": "← Back to Index When BERT-whitening Introduces Hyperparameters: There Is Always One That Suits You By 苏剑林 | May 18, 2022 In \"You Might Not Need BERT-flow: A Linear Transformation Comparable to BERT-flow\" , I proposed BERT-whitening, verifying that a simple linear transformation could rival the SOTA method at the time, BERT-flow. Additionally, BERT-whitening can reduce the dimensionality of sentence vectors, resulting in lower memory usage and faster retrieval speeds. However, in \"Which Unsupervised Semantic Similarity Method is Stronger? A Comprehensive Evaluation\" , we also found that the whitening operation does not always bring improvements. Some models are inherently well-suited to the task (such as the supervised SimBERT), and in these cases, additional whitening often degrades performance. To address this deficiency, this article proposes introducing two hyperparameters $\\beta$ and $\\gamma$ into BERT-whitening. By adjusting these two hyperparameters, we can almost always obtain results that achieve \"dimensionality reduction without performance loss.\" In other words, even for tasks where adding whitening originally caused a drop in performance, there is now a chance to maintai"}, {"file": "translation_9085.html", "title": "Constructing Discrete Probability Distributions from a Reparameterization Perspective", "content": "← Back to Index Constructing Discrete Probability Distributions from a Reparameterization Perspective By 苏剑林 | May 25, 2022 Generally, neural network outputs are unconstrained, meaning their range is $\\mathbb{R}$. To obtain constrained outputs, activation functions are typically employed. For instance, if we want to output a probability distribution representing the probability of each category, we usually add Softmax as the activation function at the end. A subsequent question arises: besides Softmax, are there any other operations that can generate a probability distribution? In \"A Discussion on Reparameterization: From Normal Distribution to Gumbel Softmax\" , we introduced the reparameterization operation of Softmax. This article reverses that process—that is, we first define a reparameterization operation and then derive the corresponding probability distribution, thereby obtaining a new perspective on constructing probability distributions. Problem Definition Assume the output vector of the model is $\\boldsymbol{\\mu}=[\\mu_1,\\cdots,\\mu_n]\\in\\mathbb{R}^n$. Without loss of generality, we assume that the $\\mu_i$ are distinct. We hope to transform $\\boldsymbol{\\mu}$ into an $n$-dim"}, {"file": "translation_9098.html", "title": "How to Train Your Accuracy?", "content": "← Back to Index How to Train Your Accuracy? By 苏剑林 | June 01, 2022 Recently, a paper on Arxiv titled \"EXACT: How to Train Your Accuracy\" caught my interest. As the name suggests, it introduces how to directly use accuracy as the training objective to train a model. Since I have previously done some analysis on this topic, such as in \"Random Talk on Function Smoothing: Differentiable Approximations of Non-differentiable Functions\" and \"Revisiting the Class Imbalance Problem: Comparison and Connection Between Adjusting Weights and Modifying Loss\" , I quickly finished reading the paper with my prior research experience and wrote this summary, along with some recent new thoughts on this subject. An Unrealistic Example The paper points out at the beginning that the classification loss functions we usually use, such as Cross-Entropy or Hinge Loss in SVM, do not fit the final evaluation metric—accuracy—very well. To illustrate this, the paper gives a very simple example: suppose the data consists of only three points $\\{(-0.25,-1),(0,-1),(0.25,1)\\}$, where $-1$ and $1$ represent the negative and positive classes respectively. The model to be fitted is $f(x)=x-b$, where $b$ is a parameter,"}, {"file": "translation_9105.html", "title": "A Theoretical Defect and Countermeasure of Relative Position Encoding Transformer", "content": "← Back to Index A Theoretical Defect and Countermeasure of Relative Position Encoding Transformer By 苏剑林 | June 07, 2022 Position encoding is a crucial part of the Transformer. In \"Transformer Position Encodings That Make Researchers Rack Their Brains\" , we summarized several common designs for position encoding. Broadly speaking, we categorize Transformer position encodings into two types: \"absolute position encoding\" and \"relative position encoding,\" with the latter generally performing better in many NLP/CV experiments. However, it is observable that currently, almost all relative position encodings operate on the Attention matrix before the Softmax. This method of application actually possesses a theoretical defect that prevents the Transformer from becoming a \"universal approximator.\" This article will analyze this problem and explore some solutions. Simple Probe As the name suggests, position encoding is used to supplement the model with positional information. So, how do we determine if a model has sufficient capability to recognize positions? I previously conceived a simple probe experiment: For a model with the capability to identify positions, it should be able to accurat"}, {"file": "translation_9119.html", "title": "Generative Diffusion Model Talks (1): DDPM = Demolition + Construction", "content": "← Back to Index Generative Diffusion Model Talks (1): DDPM = Demolition + Construction By 苏剑林 | June 13, 2022 When it comes to generative models, VAE and GAN are certainly \"household names,\" and this site has featured them many times. In addition, there are some niche choices such as flow models and VQ-VAE , which are also quite popular—especially VQ-VAE and its variant VQ-GAN , which have recently evolved into a \"Tokenizer for images\" used to directly invoke various NLP pre-training methods. Besides these, there is another choice that was originally even more niche—Diffusion Models—which is currently \"rising sharply\" in the field of generative models. The two most advanced text-to-image models at present—OpenAI's DALL·E 2 and Google's Imagen —are both completed based on diffusion models. Some examples of \"text-to-image\" from Imagen Starting from this article, we will open a new series to gradually introduce some progress in generative diffusion models over the past two years. Generating diffusion models are rumored to be famous for their mathematical complexity and seem much harder to understand than VAEs or GANs. Is this really the case? Can't diffusion models be understood in \"p"}, {"file": "translation_9138.html", "title": "Ladder Side-Tuning: The \"Wall-Climbing Ladder\" for Pre-trained Models", "content": "← Back to Index Ladder Side-Tuning: The \"Wall-Climbing Ladder\" for Pre-trained Models By 苏剑林 | June 20, 2022 If large-scale pre-trained models are the \"brilliant strategies\" (Zhang Liang's strategies) of natural language processing, then what is the corresponding \"wall-climbing ladder\" (counter-strategy)? In the author's opinion, it is the various techniques used to efficiently fine-tune these large models for specific tasks. Besides directly fine-tuning all parameters, there are many parameter-efficient fine-tuning techniques such as Adapter and P-Tuning . These methods achieve performance close to full parameter fine-tuning by updating only a small number of parameters. However, these techniques are usually only \"parameter-efficient\" and not necessarily \"training-efficient.\" This is because they still require backpropagation through the entire model to obtain gradients for the few trainable parameters. In other words, while the number of trainable parameters is significantly reduced, the training speed does not see a marked improvement. A recent paper, \"LST: Ladder Side-Tuning for Parameter and Memory Efficient Transfer Learning,\" proposes a new training technique called \"Ladder "}, {"file": "translation_9147.html", "title": "A Brief Analysis of the \"Hubness Phenomenon\" in the Curse of Dimensionality", "content": "← Back to Index A Brief Analysis of the \"Hubness Phenomenon\" in the Curse of Dimensionality By 苏剑林 | June 28, 2022 In the past few days, I came across the paper \"Exploring and Exploiting Hubness Priors for High-Quality GAN Latent Sampling\" and learned a new term: the \"Hubness phenomenon.\" It refers to an aggregation effect in high-dimensional space, which is essentially one of the manifestations of the \"curse of dimensionality.\" Using the concept of Hubness, the paper derives a scheme to improve the generation quality of GAN models, which seems quite interesting. Therefore, I took the opportunity to learn more about the Hubness phenomenon and recorded my findings here for your reference. The Collapsing Sphere \"Curse of dimensionality\" is a very broad concept. Any conclusion in high-dimensional space that differs significantly from its 2D or 3D counterparts can be called a \"curse of dimensionality.\" For example, as introduced in \"Distribution of the Angle Between Two Random Vectors in n-Dimensional Space,\" \"any two vectors in a high-dimensional space are almost always orthogonal.\" Many manifestations of the curse of dimensionality share the same source: \"The ratio of the volume of a"}, {"file": "translation_9152.html", "title": "Generative Diffusion Model Talk (2): DDPM = Autoregressive VAE", "content": "← Back to Index Generative Diffusion Model Talk (2): DDPM = Autoregressive VAE By 苏剑林 | July 06, 2022 In the article \"Generative Diffusion Model Talk (1): DDPM = Tearing Down + Building Up\" , we constructed a popular analogy of \"tearing down a building and rebuilding it\" for the generative diffusion model DDPM, and used this analogy to completely derive the theoretical form of the DDPM model. In that article, we also pointed out that DDPM is essentially no longer a traditional diffusion model; it is more of a Variational Autoencoder (VAE). In fact, the original DDPM paper also derived it following the VAE line of thought. Therefore, this article introduces DDPM again from the perspective of a VAE, while sharing my Keras implementation code and practical experience. Multi-step Breakthrough In traditional VAEs, both the encoding process and the generation process are done in one step: \\begin{equation}\\text{Encoding:}\\,\\,x\\to z\\,,\\quad \\text{Generation:}\\,\\,z\\to x\\end{equation} This approach involves only three distributions: the encoding distribution $p(z|x)$, the generation distribution $q(x|z)$, and the prior distribution $q(z)$. The advantage is that the form is relatively simple,"}, {"file": "translation_9158.html", "title": "An Unsuccessful Attempt: Generalizing Multi-label Cross-Entropy to \"n-way m-class\" Classification", "content": "← Back to Index An Unsuccessful Attempt: Generalizing Multi-label Cross-Entropy to \"n-way m-class\" Classification By 苏剑林 | July 15, 2022 Some readers might have noticed that this update has been delayed for quite a while. In fact, I started preparing this article last weekend. However, I underestimated the difficulty of this problem; I spent nearly a whole week on derivations and still haven't reached a perfect result. What I am posting now is still a failed attempt, and I hope experienced readers can provide some guidance. In the article \"Generalizing 'Softmax + Cross-Entropy' to Multi-label Classification\" , we proposed a multi-label classification loss function that automatically adjusts for the imbalance between positive and negative classes. Later, in \"The Soft Label Version of Multi-label 'Softmax + Cross-Entropy'\" , we further derived its \"soft label\" version. Essentially, multi-label classification is an \"n-way 2-class\" problem. Consequently, what would the loss function for an \"n-way m-class\" problem look like? This is the question that this article intends to explore. Analogy Attempt In the soft label generalization article \"The Soft Label Version of Multi-label 'Softmax "}, {"file": "translation_9164.html", "title": "Talk on Generative Diffusion Models (Part 3): DDPM = Bayes + Denoising", "content": "← Back to Index Talk on Generative Diffusion Models (Part 3): DDPM = Bayes + Denoising By 苏剑林 | July 19, 2022 So far, I have provided two derivations for the generative diffusion model DDPM: the common analogy scheme in \"Talk on Generative Diffusion Models (Part 1): DDPM = Demolishing + Building\" and the Variational Autoencoder (VAE) scheme in \"Talk on Generative Diffusion Models (Part 2): DDPM = Autoregressive VAE\" . These two schemes have their own characteristics; the former is more straightforward and easy to understand but cannot provide much theoretical extension or quantitative understanding, while the latter is more theoretically complete but slightly formal and lacks heuristic insight. In this article, we will share another derivation of DDPM, which primarily utilizes Bayes' theorem to simplify calculations. The entire process has a strong sense of \"deliberation\" and is quite illuminating. Moreover, it is closely related to the DDIM model that we will introduce later. Model Landscape To recap, DDPM models the following transformation process: \\begin{equation}\\boldsymbol{x} = \\boldsymbol{x}_0 \\rightleftharpoons \\boldsymbol{x}_1 \\rightleftharpoons \\boldsymbol{x}_2 \\rightleft"}, {"file": "translation_9181.html", "title": "Diffusion Models Part 4: DDIM = DDPM from a High-Level Perspective", "content": "← Back to Index Diffusion Models Part 4: DDIM = DDPM from a High-Level Perspective By 苏剑林 | July 27, 2022 I believe many readers have heard of, or even read, Felix Klein's series of books Elementary Mathematics from an Advanced Standpoint . As the name suggests, this involves re-examining elementary mathematics learned in the past from a higher perspective after studying more in-depth and complete mathematical knowledge, in order to gain a more comprehensive understanding and even achieve the effect of \"reviewing the old to know the new.\" There are many similar books, such as Calculus Revisited and Visual Complex Analysis . Returning to diffusion models, we have already interpreted DDPM from different perspectives through three articles. Does there also exist a \"higher\" perspective for understanding it that allows us to gain new insights? Certainly there is; the DDIM model introduced in Denoising Diffusion Implicit Models is a classic case. Let us appreciate it in this article. Thinking Analysis In Diffusion Models Part 3: DDPM = Bayesian + Denoising , we mentioned that the derivation introduced in that article is closely related to DDIM. Specifically, the derivation route of that "}, {"file": "translation_9209.html", "title": "Conversations on Generative Diffusion Models (5): General Framework - SDE Edition", "content": "← Back to Index Conversations on Generative Diffusion Models (5): General Framework - SDE Edition By 苏剑林 | August 03, 2022 When I wrote the first article on generative diffusion models , a reader in the comments recommended Dr. Yang Song's paper \"Score-Based Generative Modeling through Stochastic Differential Equations\" . It can be said that this paper constructs a fairly generalized theoretical framework for generative diffusion models, linking results from DDPM, SDE, ODE, and many more. Admittedly, this is an excellent paper, but it is not suitable for beginners. It directly utilizes results from Stochastic Differential Equations (SDE), the Fokker-Planck equation, score matching, and other advanced topics, making the barrier to entry quite high. However, after accumulating knowledge through the first four articles, we can now attempt to study this paper. In the following sections, I will try to replicate the derivation results of the original paper starting from as few theoretical foundations as possible. Stochastic Differentiation In DDPM, the diffusion process is divided into a fixed $T$ steps. Using the analogy from \"Conversations on Generative Diffusion Models (1): DDPM = Dem"}, {"file": "translation_9228.html", "title": "Conversations on Generative Diffusion Models (6): ODE Perspective of the General Framework", "content": "← Back to Index Conversations on Generative Diffusion Models (6): ODE Perspective of the General Framework By 苏剑林 | August 8, 2022 In the previous article \"Conversations on Generative Diffusion Models (5): SDE Perspective of the General Framework\" , we provided a basic introduction and derivation of Yang Song's paper \"Score-Based Generative Modeling through Stochastic Differential Equations\" . However, as the name suggests, the previous article primarily covered the SDE-related parts of the original paper, leaving out the section referred to as \"Probability flow ODE.\" This article serves as a supplementary share on that topic. In fact, while this legacy content only occupies a small section in the main text of the original paper, it warrants a dedicated article for introduction. After much consideration, I found that the derivation of this result cannot bypass the Fokker-Planck equation. Therefore, we need to spend some space introducing the Fokker-Planck equation before the main protagonist, the ODE, can take the stage. Rethinking Again Let's roughly summarize the content of the previous article: First, we defined a forward process (\"tearing down the building\") through an SDE: \\be"}, {"file": "translation_9245.html", "title": "Diffusion Model Notes (7): Optimal Diffusion Variance Estimation (Part 1)", "content": "← Back to Index Diffusion Model Notes (7): Optimal Diffusion Variance Estimation (Part 1) By 苏剑林 | August 12, 2022 For generative diffusion models, a crucial question is how to choose the variance of the generation process, as different variances significantly impact the generation quality. In \"Diffusion Model Notes (2): DDPM = Autoregressive VAE\" , we mentioned that DDPM derived two usable results by assuming the data followed two special distributions respectively. In \"Diffusion Model Notes (4): DDIM = High-Perspective DDPM\" , DDIM adjusted the generation process, making the variance a hyperparameter and even allowing for zero-variance generation; however, the generation quality of zero-variance DDIM is generally worse than non-zero variance DDPM. Furthermore, \"Diffusion Model Notes (5): SDE Perspective of the General Framework\" showed that the variance of the forward and reverse SDEs should be consistent, but this principle theoretically only holds when $\\Delta t \\to 0$. \"Improved Denoising Diffusion Probabilistic Models\" proposed treating it as a trainable parameter to be learned, though this increases training difficulty. So, how exactly should the variance of the generation p"}, {"file": "translation_9246.html", "title": "Talk on Generative Diffusion Models (8): Optimal Diffusion Variance Estimation (Part 2)", "content": "← Back to Index Talk on Generative Diffusion Models (8): Optimal Diffusion Variance Estimation (Part 2) By 苏剑林 | August 18, 2022 In the previous article \"Talk on Generative Diffusion Models (7): Optimal Diffusion Variance Estimation (Part 1)\" , we introduced and derived the optimal variance estimation results in Analytic-DPM. It provides an analytical estimate of the optimal variance for a pre-trained generative diffusion model, and experiments showed that this estimation effectively improves generation quality. In this article, we continue to introduce the upgrade of Analytic-DPM, from the same author team's paper \"Estimating the Optimal Covariance with Imperfect Mean in Diffusion Probabilistic Models\" , referred to as \"Extended-Analytic-DPM\" in their official GitHub. Below, we will also use this name. Review of Results The previous article derived, based on DDIM, that the optimal variance for the DDIM generation process should be\n\\[\\sigma_t^2 + \\gamma_t^2\\bar{\\sigma}_t^2\\]\nwhere $\\bar{\\sigma}_t^2$ is the variance of the distribution $p(\\boldsymbol{x}_0|\\boldsymbol{x}_t)$, which has the following estimation result (using the result of \" Variance Estimation 2 \"):\n\\begin{equation}\\b"}, {"file": "translation_9257.html", "title": "Talk on Generative Diffusion Models (9): Conditional Generation Control", "content": "← Back to Index Talk on Generative Diffusion Models (9): Conditional Generation Control By 苏剑林 | August 30, 2022 The previous articles have mostly focused on theoretical results. In this article, we will discuss a topic of significant practical value: conditional controlled generation. As generative models, the development of diffusion models closely mirrors that of VAEs, GANs, and flow models: unconditional generation appeared first, followed closely by conditional generation. Unconditional generation is often used to explore the upper bounds of performance, while conditional generation is more focused on application, as it allows us to control the output according to our intentions. Since the advent of DDPM, many works on conditional diffusion models have emerged; one could even say it was conditional diffusion models—such as the famous text-to-image models DALL·E 2 and Imagen —that truly made diffusion models popular. In this article, we will briefly study and summarize the theoretical foundations of conditional diffusion models. Technical Analysis Methodologically, there are two primary ways to achieve conditional generation: post-hoc modification (Classifier-Guidance) and pre-"}, {"file": "translation_9262.html", "title": "Generative Diffusion Model Discussion (10): Unified Diffusion Model (Theory)", "content": "← Back to Index Generative Diffusion Model Discussion (10): Unified Diffusion Model (Theory) By 苏剑林 | September 14, 2022 Old readers may notice that, compared to previous update frequencies, this article is indeed \"long overdue\" because this article involves \"thinking too much.\" Through the previous nine articles, we have provided a relatively comprehensive introduction to generative diffusion models. Although the theoretical content is extensive, we can see that the diffusion models introduced earlier all deal with continuous objects and are constructed based on normal noise for the forward process. This article, \"thinking too much,\" hopes to construct a Unified Diffusion Model (UDM) framework that can break through the above limitations: 1. No restriction on object types (can be continuous $\\boldsymbol{x}$ or discrete $\\boldsymbol{x}$); 2. No restriction on the forward process (the forward process can be constructed using various transformations like adding noise, blurring, masking, or deletion); 3. No restriction on time types (can be discrete $t$ or continuous $t$); 4. Includes existing results (capable of deriving previous results like DDPM, DDIM, SDE, ODE, etc.). Is this too "}, {"file": "translation_9271.html", "title": "Generating Diffusion Models (11): Unified Diffusion Model (Application)", "content": "← Back to Index Generating Diffusion Models (11): Unified Diffusion Model (Application) By 苏剑林 | September 21, 2022 In \"Generating Diffusion Models (10): Unified Diffusion Model (Theory)\" , the author claimed to have constructed a Unified Diffusion Model (UDM) framework that allows for more general diffusion methods and data types. Can the UDM framework actually achieve its intended purpose? This article demonstrates its generality through several specific examples. Framework Review First, UDM constructs the forward process by choosing a noise distribution $q(\\boldsymbol{\\varepsilon})$ and a transformation $\\boldsymbol{\\mathcal{F}}$: \\begin{equation}\\boldsymbol{x}_t = \\boldsymbol{\\mathcal{F}}_t(\\boldsymbol{x}_0,\\boldsymbol{\\varepsilon}),\\quad \\boldsymbol{\\varepsilon}\\sim q(\\boldsymbol{\\varepsilon})\\end{equation} Then, the reverse process $\\boldsymbol{x}_{t-1}\\sim p(\\boldsymbol{x}_{t-1}|\\boldsymbol{x}_t)$ is sampled via the following decomposition: \\begin{equation}\\hat{\\boldsymbol{x}}_0\\sim p(\\boldsymbol{x}_0|\\boldsymbol{x}_t)\\quad \\& \\quad \\boldsymbol{x}_{t-1}\\sim p(\\boldsymbol{x}_{t-1}|\\boldsymbol{x}_t, \\boldsymbol{x}_0=\\hat{\\boldsymbol{x}}_0)\\end{equation} Where $p(\\boldsymbol{x}"}, {"file": "translation_9280.html", "title": "Let's Talk About Generative Diffusion Models (12): \"Hard-core\" Diffusion ODE", "content": "← Back to Index Let's Talk About Generative Diffusion Models (12): \"Hard-core\" Diffusion ODE By 苏剑林 | September 28, 2022 In \"Let's Talk About Generative Diffusion Models (5): SDE in General Framework\" , we understood the generative diffusion model from the perspective of SDEs. Then, in \"Let's Talk About Generative Diffusion Models (6): ODE in General Framework\" , we learned that an ODE model is actually implicit within the diffusion model corresponding to the SDE. Coincidentally, in \"Let's Talk About Generative Diffusion Models (4): DDIM = High-level Perspective of DDPM\" , we also saw that the original stochastic sampling DDPM model implies a deterministic sampling process, DDIM, whose continuous limit is also an ODE. Thinking carefully about the above processes, we can find that whether it is \"DDPM → DDIM\" or \"SDE → ODE\", it is a transition from a stochastic sampling model to a deterministic one. If our initial goal was an ODE, this process might seem a bit \"roundabout.\" In this article, the author attempts to provide a direct derivation of the ODE diffusion model and reveals its connections with the Jacobian determinant, the heat equation, and other related concepts. Differential"}, {"file": "translation_9291.html", "title": "A Brief Trial on the \"Cross\" Combinatorial Counting Problem", "content": "← Back to Index A Brief Trial on the \"Cross\" Combinatorial Counting Problem By 苏剑林 | October 09, 2022 Yesterday, I saw a \"cross\" combinatorial counting problem in a WeChat public account article that is said to have a controversial answer: In a square, if two of the four sides are of color $i$ and the other two are of two different other colors, then the square is called \"$i$-color dominant.\" Consider the following \"cross\" figure composed of 16 line segments and 5 squares. Each segment is colored with one of three colors: red, yellow, or blue, such that the dominant colors of the three squares in the horizontal and vertical directions are all different. How many different coloring methods are there? Schematic of the \"Cross\" The linked article provides two answers: 54,432 by Teacher Wu Kang and 27,216 by Teacher Wang Huixing. This post first confirms through programming that Teacher Wang Huixing's 27,216 is the correct answer, and then provides my own theoretical analysis process. Programming Verification For such combinatorial problems with a relatively small number of permutations, the simplest and most direct way to verify the correctness of the answer is, naturally, programming "}, {"file": "translation_9305.html", "title": "Generative Diffusion Model Ramblings (13): From Universal Gravitation to Diffusion Models", "content": "← Back to Index Generative Diffusion Model Ramblings (13): From Universal Gravitation to Diffusion Models By 苏剑林 | October 18, 2022 For many readers, the generative diffusion model may be the first model they have encountered that applies so many mathematical tools to deep learning. In this series of articles, we have demonstrated the profound connections between diffusion models and mathematical analysis, probability and statistics, ordinary differential equations (ODEs), stochastic differential equations (SDEs), and even partial differential equations (PDEs). It can be said that even students doing pure theoretical research in mathematical physics equations are likely to find a use for their skills within diffusion models. In this article, we introduce another diffusion model that also has deep connections with mathematical physics—an ODE-based diffusion model inspired by the \"Law of Universal Gravitation,\" from the paper \"Poisson Flow Generative Models\" (abbreviated as PFGM). It provides an entirely new perspective on constructing ODE-based diffusion models. Universal Gravitation In middle school, we learned the Law of Universal Gravitation, which is roughly described as: The gr"}, {"file": "translation_9324.html", "title": "Probability of $n$ Random Points in a Circle Lying Within a Sector of Central Angle $\\theta$", "content": "← Back to Index Probability of $n$ Random Points in a Circle Lying Within a Sector of Central Angle $\\theta$ By 苏剑林 | October 25, 2022 In recent days, the \"four ducks in a semicircle\" problem has been circulating widely on the internet: Four ducks in a semicircle problem Many readers may have tried to solve it upon seeing it, and even Teacher Li Yongle dedicated a lesson to this problem (refer to \"Four ducks in a circular pond, what is the probability they are in the same semicircle?\" ). As for the problem itself, the answer is not particularly difficult and can be calculated using many methods. What is slightly more challenging is its generalized version—as described in the title of this article—where the number of ducks is generalized to $n$ and the semicircle is generalized to a sector with a central angle of $\\theta$. Even more interesting is that when $\\theta \\leq \\pi$, there are still relatively elementary solutions, but as soon as $\\theta > \\pi$, the complexity begins to \"increase dramatically\"... Problem Transformation First, it should be noted that we are treating the ducks as abstract points sampled uniformly at random within the circle. We will not consider generalizatio"}, {"file": "translation_9336.html", "title": "Accelerating Retrieval of Interaction-based Similarity Models Using CUR Decomposition", "content": "← Back to Index Accelerating Retrieval of Interaction-based Similarity Models Using CUR Decomposition By 苏剑林 | November 02, 2022 In the field of text similarity, there are two common approaches: \"interaction-based\" and \"representation-based.\" Many readers are likely familiar with these. I previously wrote an article, \"CoSENT (Part 2): How Large Is the Gap Between Representation-based and Interaction-based Matching?\" , comparing their performance. In general, interaction-based similarity models usually deliver better results, but using them directly for large-scale retrieval is impractical. On the other hand, representation-based similarity models offer faster retrieval speeds but slightly inferior performance. Therefore, how to improve the retrieval speed of interaction-based similarity while maintaining its performance is a subject that the academic community has been studying for a long time. Recently, the paper \"Efficient Nearest Neighbor Search for Cross-Encoder Models using Matrix Factorization\" proposed a new solution: CUR decomposition. Problem Analysis In a retrieval scenario, we generally have a massive candidate set $\\mathcal{K}$. Without loss of generality, we can assume"}, {"file": "translation_9341.html", "title": "CoSENT (3): As a Loss Function for Interactive Similarity Models", "content": "← Back to Index CoSENT (3): As a Loss Function for Interactive Similarity Models By 苏剑林 | November 09, 2022 In \"CoSENT (1): A More Effective Sentence Vector Scheme than Sentence-BERT\" , the author proposed a supervised sentence vector scheme named \"CoSENT.\" Since it directly trains the cosine similarity, which is more relevant to the evaluation target, it usually achieves better performance and faster convergence than Sentence-BERT. In \"CoSENT (2): How Big is the Gap Between Feature-based Matching and Interactive Matching?\" , we compared the differences between it and interactive similarity models, showing that its performance on certain tasks can even approach that of interactive similarity models. However, at that time, the author was focused on finding a Sentence-BERT replacement that was closer to the evaluation target, so the results were oriented towards supervised sentence vectors, i.e., feature-based similarity models. Recently, it occurred to me that CoSENT can actually also serve as a loss function for interactive similarity models. So, how does it compare to the standard choice of cross-entropy? This article supplements that part of the experiments. Basic Review When CoS"}, {"file": "translation_9344.html", "title": "Some \"Alchemy Strategies\" Derived from the Amos Optimizer Ideas", "content": "← Back to Index Some \"Alchemy Strategies\" Derived from the Amos Optimizer Ideas By 苏剑林 | November 22, 2022 If training a model is compared to \"alchemy,\" then the \"alchemical furnace\" is clearly the optimizer. It is rumored that the AdamW optimizer is currently the fastest solution for training neural networks. I haven't compared this one by one, so I don't know the specific details, but it is true that most pre-training currently uses AdamW or its variant LAMB. However, just as having an alchemical furnace doesn't guarantee a good pill, even if we decide on the AdamW optimizer, there are still many questions without definitive answers, such as: 1. How should the learning rate adapt to different initializations and parameterizations? 2. How should the weight decay rate be tuned? 3. What strategy should be used for learning rate scheduling? 4. Can we reduce the memory usage of the optimizer? Although in practical applications we can often directly apply parameters and strategies tuned by predecessors, the lack of systematic parameter-tuning guidance always leaves us feeling uncertain when \"alchemy\" is involved. In this article, based on the ideas of the Amos optimizer recently propos"}, {"file": "translation_9359.html", "title": "Using the Heat Equation to Guide Self-Supervised Learning", "content": "← Back to Index Using the Heat Equation to Guide Self-Supervised Learning By 苏剑林 | November 30, 2022 Leveraging theoretical physics to push machine learning is no longer a new phenomenon. For example, the article introduced last month, \"General Discourse on Generative Diffusion Models (13): From Universal Gravitation to Diffusion Models,\" is a classic case. Recently, a new paper titled \"Self-Supervised Learning based on Heat Equation\" has piqued my interest. As the name suggests, it uses the heat conduction equation for self-supervised learning in the field of computer vision. How do these physical equations play a role in machine learning? Can the same ideas be transferred to NLP? Let's read the paper together. Basic Equations # As shown in the figure below, on the left is the solution to the heat conduction equation in physics, and on the right is the attribution heatmap obtained through saliency methods such as CAM and Integrated Gradients. It can be seen that there is a certain similarity between the two. Consequently, the authors believe that the heat conduction equation can serve as a good prior for visual features. Heatmap of the heat equation (left) and heatmap of a vision "}, {"file": "translation_9365.html", "title": "Smart Home: A Simple Solution for Controlling XGIMI Projectors with XiaoAI", "content": "← Back to Index Smart Home: A Simple Solution for Controlling XGIMI Projectors with XiaoAI By 苏剑林 | December 05, 2022 Some time ago, I bought an XGIMI projector, only to realize after starting to tinker with it that XGIMI has almost nothing to do with Xiaomi—it simply cannot interact with XiaoAI. Among the many brands with \"Mi\" in their names, XGIMI is one of the few that cannot be integrated into the Mi Home ecosystem. I suspect many users were misled by the name XGIMI at first; the irony is that XGIMI projectors are even sold on the Xiaomi Mall (facepalms). Since I already bought it and the seven-day no-reason return period has passed, I can't return it. I decided to try and \"brute-force\" some interaction to see if I could make it work. Existing Solutions First, I searched online. The reference solutions provided by netizens generally fall into several categories: one is using a \"Mi Home Smart Plug + Power-on scheduled startup\" to control the power (in fact, the main automation needed is just switching it on and off); another is connecting it to Home Assistant and controlling it via ADB; and a third is modifying the remote control by adding an infrared (IR) module to it, then usi"}, {"file": "translation_9368.html", "title": "From Local to Global: Geodesic Distance for Semantic Similarity", "content": "← Back to Index From Local to Global: Geodesic Distance for Semantic Similarity By 苏剑林 | December 07, 2022 Recently, I learned a new concept called \"Geodesic Distance\" from a recent paper 《Unsupervised Opinion Summarization Using Approximate Geodesics》 , which I found quite interesting and would like to share with you. To me, what is \"new\" is not the concept of geodesic distance itself (which I encountered back when I studied Riemannian geometry), but rather how the field of semantic similarity can cleverly construct geodesic distances and apply them in certain scenarios. If we want to be fancy, we could even call it \"semantic similarity on a manifold\"—doesn't that sound much more advanced instantly? Paper Outline First, let's briefly summarize the main content of the original paper. As the title suggests, the theme of the paper is summarization. Typically, unsupervised summarization is done like this: assume an article consists of $n$ sentences $t_1, t_2, \\cdots, t_n$, and design a scoring function $s(t_i)$ for each sentence (classically tf-idf and its variants), then pick several sentences with the highest scores as the summary. Of course, the paper does not just do simple summar"}, {"file": "translation_9370.html", "title": "Generative Diffusion Models (14): General Steps for Constructing ODEs (Part 1)", "content": "← Back to Index Generative Diffusion Models (14): General Steps for Constructing ODEs (Part 1) By 苏剑林 | December 15, 2022 Continuing from where we left off, in \"Generative Diffusion Models (13): From Universal Gravitation to Diffusion Models\" , we introduced an ODE-style generative diffusion model inspired by universal gravitation with a very clear geometric meaning. Some readers wondered: it seems \"universal gravitation\" is not the only choice; could other forms of force be used to build diffusion models from the same physical picture? Furthermore, while the model is physically intuitive, it still lacks a mathematical proof that it can indeed learn the data distribution. This article attempts to answer the question \"What kind of force field is suitable for constructing an ODE-style generative diffusion model?\" from a more precise mathematical perspective. Basic Conclusions To answer this question, we need to use a conclusion regarding the distribution changes corresponding to ordinary differential equations, which we derived in \"Generative Diffusion Models (12): Tackling Diffusion ODEs Head-on\" . Consider a system of first-order (ordinary) differential equations for $\\boldsymbol{x"}, {"file": "translation_9379.html", "title": "Diffusion Model Ramblings (Part 14): General Steps for Constructing ODEs (Part 2)", "content": "← Back to Index Diffusion Model Ramblings (Part 14): General Steps for Constructing ODEs (Part 2) By 苏剑林 | December 22, 2022 Last week, I wrote \"Generating Diffusion Models Chat (14): General Steps for Constructing ODEs (Part 1)\" (at that time, it didn't have the \"Part 1\" suffix). I thought I had glimpsed the general principles of constructing ODE diffusion models. However, shortly after, a prominent figure in the comments section, @gaohuazuo, provided a more efficient and intuitive scheme for constructing Green's functions, which made me feel humbled. Recalling that this expert previously provided a brilliant description of diffusion ODEs in \"Generating Diffusion Models Chat (12): 'Hard-Core' Diffusion ODE\" (which indirectly inspired the results of my previous post), their insight is truly admirable. After much discussion and reflection, I realized that their approach is essentially the method of characteristics for first-order partial differential equations. By constructing a specific vector field to guarantee the initial value conditions and then solving the differential equation to satisfy the terminal value conditions, both conditions are met—truly ingenious! Finally, I have s"}, {"file": "translation_9403.html", "title": "Transformer Upgrade Road: 6. Completeness Analysis of Rotary Positional Embeddings", "content": "← Back to Index Transformer Upgrade Road: 6. Completeness Analysis of Rotary Positional Embeddings By 苏剑林 | December 28, 2022 In last year's article \"Transformer Upgrade Road: 2. Rotary Positional Embeddings (RoPE) that Incorporate Various Strengths,\" I proposed Rotary Positional Embedding (RoPE). At that time, the starting point was simply feeling that implementing relative positions through absolute positions was a \"very fun thing to do,\" and I did not expect that its actual performance would be quite good and widely accepted. This was truly a pleasant surprise. Later, in \"Transformer Upgrade Road: 4. Rotary Positional Embeddings for 2D Positions,\" I discussed the 2D form of RoPE and studied the general solution of RoPE expressed using matrix exponentials. Since we have a general solution, a natural question arises: the RoPE we commonly use is just a block-diagonal matrix with 2D rotation matrices as the basic units. If we replaced it with the general solution, would the performance be theoretically better? This article aims to answer this question. Exponential General Solution In \"Transformer Upgrade Road: 4. Rotary Positional Embeddings for 2D Positions,\" we abstractly defined "}, {"file": "translation_9405.html", "title": "Analysis of the Technical Principles of \"Zero Cold Water\" for Smart Home Water Heaters", "content": "← Back to Index Analysis of the Technical Principles of \"Zero Cold Water\" for Smart Home Water Heaters By 苏剑林 | January 04, 2023 If a household uses a single centralized water heater for hot water, we often need to let the cold water run for a while before hot water arrives. If the wait time is long, it can significantly impact the user experience. So-called \"Zero Cold Water\" (ZCW) is actually a method to pre-drain the cold water from the hot water pipes to achieve a (nearly) instant hot water effect. In fact, ZCW is not a high-end technology, but it hasn't become universal in households yet, possibly due to outdated concepts or misunderstandings. However, as people's demands for quality of life increase, ZCW is indeed becoming more popular. This article provides a simple analysis of the implementation principles of Zero Cold Water technology, including the pros and cons of various solutions and reference ideas for DIY implementation. Before We Begin To start the article, it's necessary to correct a common misconception: Zero Cold Water is not intended to save money; it is intended to improve the quality of life. If your mindset is primarily focused on saving money, then the follow"}, {"file": "translation_9431.html", "title": "Road to Transformer Upgrade: 7. Length Extrapolation and Local Attention", "content": "← Back to Index Road to Transformer Upgrade: 7. Length Extrapolation and Local Attention By 苏剑林 | January 12, 2023 For Transformer models, length extrapolation is a desirable property we have been pursuing. It refers to the ability of a model trained on short sequences to be applied to long sequences without fine-tuning while maintaining good performance. The pursuit of length extrapolation stems from two sides: on one hand, theoretical completeness, as it is felt to be a property an ideal model should possess; on the other hand, training practicality, as it allows us to train a model usable for long sequences at a lower cost (on shorter sequences). Below, we analyze the key ideas for strengthening the length extrapolation of Transformers, propose a \"super baseline\" solution, and then use this \"super baseline\" to evaluate some related research work. Misconceptions The first work to explicitly study the length extrapolation of Transformers was likely ALIBI , published in mid-2021, which isn't very long ago. Why did it take so long (compared to the Transformer's first implementation in 2017) for someone to specialize in this topic? It's likely because we long assumed that length extr"}, {"file": "translation_9444.html", "title": "Transformer Upgrade Journey: 8. Length Extrapolation and Positional Robustness", "content": "← Back to Index Transformer Upgrade Journey: 8. Length Extrapolation and Positional Robustness By 苏剑林 | January 31, 2023 In the previous article \"Transformer Upgrade Journey: 7. Length Extrapolation and Local Attention,\" we discussed the length extrapolation of Transformers. We concluded that length extrapolation is an issue of inconsistency between training and prediction, and the main idea to solve this inconsistency is to localize attention. Many improvements with good extrapolation performance are, in a sense, variants of local attention. Admittedly, while many current indicators in language models suggest that local attention can indeed solve the length extrapolation problem, this \"forced truncation\" approach might not satisfy some readers' aesthetic preferences because the traces of manual craftsmanship are too strong, lacking a sense of naturalness. It also raises questions about their effectiveness in non-language modeling tasks. In this article, we revisit the problem of length extrapolation from the perspective of the model's robustness to positional encodings. This approach can improve the length extrapolation effect of Transformers without modifying the attention mechan"}, {"file": "translation_9461.html", "title": "Deriving the Continuity Equation and Fokker-Planck Equation using the Test Function Method", "content": "← Back to Index Deriving the Continuity Equation and Fokker-Planck Equation using the Test Function Method By 苏剑林 | February 11, 2023 In the article \"Discussion on Generative Diffusion Models (6): ODEs in the General Framework\" , we derived the Fokker-Planck equation for SDEs; and in \"Discussion on Generative Diffusion Models (12): 'Hardcore' Diffusion ODEs\" , we independently derived the continuity equation for ODEs. Both are equations describing the distribution change of random variables evolving along SDEs/ODEs, where the continuity equation is a special case of the Fokker-Planck equation. When deriving the Fokker-Planck equation, we relied on a somewhat crude application of Taylor expansion to the Dirac delta function; while the result was correct, the method was a bit unconventional. When deriving the continuity equation, we combined the Jacobian determinant with Taylor expansion, which is a standard approach but difficult to generalize to the Fokker-Planck equation. In this article, we introduce the \"Test Function Method.\" It is one of the standard methods for deriving continuity equations and Fokker-Planck equations. Its analytical process is more formal and its application"}, {"file": "translation_9467.html", "title": "Generative Diffusion Models (Part 16): W-Distance ≤ Score Matching", "content": "← Back to Index Generative Diffusion Models (Part 16): W-Distance ≤ Score Matching By 苏剑林 | February 14, 2023 The Wasserstein distance (hereinafter referred to as the \"W-distance\") is a distance function based on the concept of optimal transport to measure the degree of difference between two probability distributions. I have previously introduced it in blog posts such as \"From Wasserstein Distance and Duality Theory to WGAN\" . For many readers, the first time they heard of the W-distance was because of WGAN , which was released in 2017. It pioneered a new branch of understanding GANs from the perspective of optimal transport and elevated the status of optimal transport theory in machine learning. For a long time, GANs were the \"main force\" in the field of generative models until diffusion models emerged suddenly in the past two years. While the popularity of GANs has declined somewhat, they remain a powerful generative model in their own right. Formally, diffusion models and GANs differ significantly, so their research has remained relatively independent. However, a paper late last year, \"Score-based Generative Modeling Secretly Minimizes the Wasserstein Distance\" , broke this bar"}, {"file": "translation_9473.html", "title": "Google's New Discovered Optimizer Lion: A \"Training Lion\" with Both Efficiency and Effectiveness", "content": "← Back to Index Google's New Discovered Optimizer Lion: A \"Training Lion\" with Both Efficiency and Effectiveness By 苏剑林 | February 16, 2023 Yesterday, I discovered a new paper from Google on arXiv titled \"Symbolic Discovery of Optimization Algorithms\" . It focuses on the automatic search for optimization algorithms. At first glance, it didn't seem particularly interesting, as there have been many similar works, and most of their results are somewhat uninspiring. However, a closer look revealed something Remarkable. The authors used thousands of TPU hours of compute combined with human intervention to discover a faster and more memory-efficient optimizer called Lion ( EvoLved Sign Momentum —I have to admit, the backronym is a bit forced). They conducted extensive experiments on various tasks, including image classification, image-text matching, diffusion models, and language model pre-training and fine-tuning. In most tasks, Lion demonstrated better performance than the currently mainstream AdamW and other optimizers. Saving VRAM while achieving better results—truly having one's cake and eating it too. What kind of optimizer can possess such powerful performance? Let's take a look a"}, {"file": "translation_9497.html", "title": "Discussion on Generative Diffusion Models (17): General Steps for Constructing ODE (Part 3)", "content": "← Back to Index Discussion on Generative Diffusion Models (17): General Steps for Constructing ODE (Part 3) By 苏剑林 | February 23, 2023 History is always strikingly similar. When I originally wrote \"Discussion on Generative Diffusion Models (14): General Steps for Constructing ODE (Part 1)\" (which didn't have the \"Part 1\" suffix at the time), I thought I had clarified the general steps for constructing ODE-based diffusion. However, reader @gaohuazuo provided a new intuitive and effective scheme, which directly led to the subsequent \"Discussion on Generative Diffusion Models (14): General Steps for Constructing ODE (Part 2)\" (initially suffixed as \"Part 2\"). Just as I thought the matter was settled, I discovered the ICLR 2023 paper \"Flow Straight and Fast: Learning to Generate and Transfer Data with Rectified Flow\" , which introduces a new scheme for constructing ODE-based diffusion models. Its simplicity and intuitiveness are almost unprecedented and simply breathtaking. Therefore, I have quietly changed the suffix of the previous article to \"Part 2\" and written this \"Part 3\" to share this new result. Intuitive Result We know that a diffusion model is an evolutionary process $\\bolds"}, {"file": "translation_9509.html", "title": "Diffusion Model Musings (18): Score Matching = Conditional Score Matching", "content": "← Back to Index Diffusion Model Musings (18): Score Matching = Conditional Score Matching By 苏剑林 | February 28, 2023 In previous introductions, we have repeatedly mentioned \"Score Matching\" and \"Conditional Score Matching.\" These are concepts that frequently appear in diffusion models, energy-based models, and the like. In particular, many articles directly state that the training objective of diffusion models is \"Score Matching,\" but in fact, the training objective of current mainstream diffusion models such as DDPM is actually \"Conditional Score Matching.\" So, what is the specific relationship between \"Score Matching\" and \"Conditional Score Matching\"? Are they equivalent? This article discusses this question in detail. Score Matching First, Score Matching refers to the training objective: \\begin{equation}\\mathbb{E}_{\\boldsymbol{x}_t\\sim p_t(\\boldsymbol{x}_t)}\\left[\\left\\Vert\\nabla_{\\boldsymbol{x}_t}\\log p_t(\\boldsymbol{x}_t) - \\boldsymbol{s}_{\\boldsymbol{\\theta}}(\\boldsymbol{x}_t,t)\\right\\Vert^2\\right]\\label{eq:sm}\\end{equation} where $\\boldsymbol{\\theta}$ represents the trainable parameters. Obviously, Score Matching aims to learn a model $\\boldsymbol{s}_{\\boldsymbol{\\theta}}(\\b"}, {"file": "translation_9512.html", "title": "Tiger: An \"Ultra-Stingy\" Optimizer", "content": "← Back to Index Tiger: An \"Ultra-Stingy\" Optimizer By 苏剑林 | March 07, 2023 Recently, I have been experimenting with the Lion optimizer introduced in \"Google's Newly Discovered Lion Optimizer: A 'Training Lion' that Achieves Both Efficiency and Effectiveness.\" My interest in Lion stems from the fact that it aligns with some of my previous thoughts regarding an ideal optimizer. While I was unable to tune it to achieve good results at the time, Lion has succeeded. Compared to the standard Lion, I am more interested in its special case where $\\beta_1 = \\beta_2$, which I refer to here as \" Tiger .\" Tiger uses only momentum to construct the update. According to the conclusions in \"Hidden Gradient Accumulation in Momentum: Better Results with Fewer Updates?\" , we can implement gradient accumulation \"seamlessly\" without adding an extra set of parameters! This means that when there is a need for gradient accumulation, Tiger reaches the theoretical lower bound for memory usage, which is the origin of the name \"Tiger\" ( Tig ht-fisted Optimiz er , a stingy optimizer unwilling to spend a single extra bit of VRAM). In addition, Tiger incorporates some of our hyperparameter tuning experiences and"}, {"file": "translation_9526.html", "title": "A Concise Solution to Mitigate Cross-Entropy Overconfidence", "content": "← Back to Index A Concise Solution to Mitigate Cross-Entropy Overconfidence By 苏剑林 | March 14, 2023 As is well known, the standard evaluation metric for classification problems is accuracy, while the standard loss function is cross-entropy. Cross-entropy has the advantage of fast convergence, but it is not a smooth approximation of accuracy, which leads to an inconsistency between training and prediction. On the other hand, when the predicted probability of a training sample is very low, cross-entropy yields a very large loss (tending towards $-\\log 0^{+}=\\infty$). This means that cross-entropy pays excessive attention to samples with low predicted probabilities—even if those samples might be \"noisy data.\" Consequently, models trained with cross-entropy often exhibit an overconfidence phenomenon, where the model assigns high predicted probabilities to every sample. This brings two side effects: first, a decrease in performance due to overfitting on noisy data; and second, the inability to use predicted probabilities as reliable indicators of uncertainty. Regarding improvements to cross-entropy, the academic community has continuously produced research, yet this field remains in a s"}, {"file": "translation_9529.html", "title": "Why are current LLMs all using a Decoder-only architecture?", "content": "← Back to Index Why are current LLMs all using a Decoder-only architecture? By 苏剑林 | March 17, 2023 LLM is an abbreviation for \"Large Language Model,\" which currently generally refers to language models with more than 10 billion parameters, primarily oriented towards text generation tasks. Unlike the \"hundred flowers blooming\" of small-scale models (around 1 billion or fewer), the current status of LLMs is that most research focuses on the Decoder-only architecture. Aside from OpenAI, which has always insisted on the Decoder-only GPT series, even companies like Google, which haven't placed all their bets on Decoder-only, have invested significant energy into researching Decoder-only models, such as PaLM. So, why has the Decoder-only architecture become the mainstream choice for LLMs? There is a similar question on Zhihu: \"Why are current LLMs all using a Decoder-only architecture?\" Most answers there focus on the advantages of Decoder-only in terms of training efficiency and engineering implementation. But does it have theoretical advantages? This article attempts to provide a simple analysis from that perspective. Unified Perspective It should be noted that the largest models I ha"}, {"file": "translation_9547.html", "title": "FAQ for \"Why are modern LLMs all Decoder-only architectures?\"", "content": "← Back to Index FAQ for \"Why are modern LLMs all Decoder-only architectures?\" By 苏剑林 | March 20, 2023 Last week, I wrote \"Why are modern LLMs all Decoder-only architectures?\" , summarizing some of my experimental conclusions and conjectures on this issue. As expected, hot topics attract significant traffic; the repost by PaperWeekly reached over 10,000 views in a short time, and it received many likes on Zhihu. Across several platforms, I received various suggestions and questions from readers. I have summarized some of the most representative questions into this FAQ, hoping to further help everyone resolve their doubts. Review In \"Why are modern LLMs all Decoder-only architectures?\" , I compared GPT and UniLM architectures through experiments and, combined with previous research experience, conjectured the following conclusions: Changing the attention of the input part to bidirectional does not bring benefits; the advantage of the Encoder-Decoder architecture is likely merely derived from doubling the parameters. The reason bidirectional attention fails to provide benefits may be due to the low-rank problem of bidirectional attention leading to performance degradation. Therefore, "}, {"file": "translation_9554.html", "title": "Google's New Work Attempts to \"Resurrect\" RNN: Can RNN Shine Again?", "content": "← Back to Index Google's New Work Attempts to \"Resurrect\" RNN: Can RNN Shine Again? By 苏剑林 | March 28, 2023 Currently, Large Language Models (LLMs) like ChatGPT are \"sweeping the globe.\" Some readers have noticed that almost all LLMs still use the original Multi-Head Scaled-Dot Attention . In recent years, a large amount of work on Efficient Transformers, such as Linear Attention and FLASH , has not been adopted. Is it because their performance is too poor, or is there simply no need to consider efficiency? Actually, I analyzed the answer in \"Linear Transformer is Probably Not the Model You Are Waiting For\" . Standard Attention only exhibits quadratic complexity when the sequence length significantly exceeds the hidden size; before that, it remains close to linear. Its speed is faster than many efficient improvements. Since GPT-3 uses hidden sizes in the tens of thousands, it means that as long as your LLM is not targeting text generation with lengths in the tens of thousands, efficient improvements are unnecessary. Often, speed is not improved, while performance decreases. So, when there is a real demand for processing sequences with lengths of tens or even hundreds of thousands, "}, {"file": "translation_9577.html", "title": "The Amazing Effect of the Bias Term: RoPE + Bias = Better Length Extrapolation", "content": "← Back to Index The Amazing Effect of the Bias Term: RoPE + Bias = Better Length Extrapolation By 苏剑林 | April 03, 2023 I never expected that the Bias term could be linked to the length extrapolation of Transformers! Length extrapolation is an ideal property we hope Transformers possess. I have systematically introduced this issue in \"Transformer Upgrade Road: 7. Length Extrapolation and Local Attention\" and \"Transformer Upgrade Road: 8. Length Extrapolation and Positional Robustness\" . As for the Bias term (offset term), the current mainstream view is that when the model is large enough, the Bias term does not play a special role. Therefore, many models choose to remove the Bias term, with representative examples being Google's T5 and PaLM . Our subsequent RoFormerV2 and GAU-α also followed this practice. So, how are these two seemingly \"completely unrelated\" things connected? Can the Bias term really enhance the length extrapolation of Transformers? Let's dive in. Hidden Easter Egg First, why think about exploring the connection between the Bias term and length extrapolation? This is because while I was revisiting the GAU paper \"Transformer Quality in Linear Time\" a few days ago, "}, {"file": "translation_9588.html", "title": "Entropy-Invariant Attention from the Perspective of the JL Lemma", "content": "← Back to Index Entropy-Invariant Attention from the Perspective of the JL Lemma By 苏剑林 | April 10, 2023 In the articles \"Entropy Invariance: Looking at the Scaling Operation of Attention\" and \"A Fast Derivation of Entropy-Invariant Softmax,\" the author proposed Entropy-Invariant Softmax. Simply put, this involves multiplying the Attention matrix before Softmax by an additional factor of $\\log n$, which theoretically helps improve length extrapolation, where $n$ is the sequence length. This $\\log n$ factor reminded me of the JL Lemma ( Johnson-Lindenstrauss Lemma ). Since the JL Lemma tells us that encoding $n$ vectors only requires a dimension of $\\mathcal{O}(\\log n)$, and both involve $\\log n$, is there a connection between the two? Entropy Invariance We know that entropy is a measure of uncertainty. In the attention mechanism, we use it as the \"degree of focused attention.\" So-called entropy invariance means that regardless of the sequence length $n$, we want the attention to remain concentrated on a few key tokens rather than becoming too dispersed. To this end, the proposed form of Entropy-Invariant Attention is: \\begin{equation}Attention(Q,K,V) = softmax\\left(\\frac{\\log_{512}"}, {"file": "translation_9590.html", "title": "LoRA from a Gradient Perspective: Introduction, Analysis, Conjectures, and Extensions", "content": "← Back to Index LoRA from a Gradient Perspective: Introduction, Analysis, Conjectures, and Extensions By 苏剑林 | April 17, 2023 With the fervor surrounding ChatGPT and its open-source alternatives, various parameter-efficient fine-tuning methods have also risen in popularity. One of the most prominent among these is LoRA , the subject of this article, which originates from the paper \"LoRA: Low-Rank Adaptation of Large Language Models\" . LoRA's methodology is relatively simple and direct, and there are many existing implementations. Therefore, understanding and using it is quite straightforward, and there might not seem to be much left to narrate. However, directly implementing LoRA requires modifying the network structure, which can be slightly cumbersome. At the same time, LoRA feels very similar to the previous optimizer AdaFactor . Thus, my question is: Can we analyze and implement LoRA from an optimizer perspective? This article revolves around this theme. Method Introduction Previous results (such as \"Exploring Universal Intrinsic Task Subspace via Prompt Tuning\" ) have shown that although pre-trained models have a huge number of parameters, the \"intrinsic dimension\" correspondi"}, {"file": "translation_9593.html", "title": "Two Interesting Discoveries about Attention and Softmax: Robustness and Information Volume", "content": "← Back to Index Two Interesting Discoveries about Attention and Softmax: Robustness and Information Volume By 苏剑林 | April 25, 2023 In recent weeks, I have been constantly thinking about the properties of the attention mechanism. During this process, I have gained a deeper understanding of attention and Softmax. In this article, I will briefly share two of those points: 1. Softmax attention can naturally resist certain noise perturbations; 2. Initialization problems can be intuitively understood from the perspective of information entropy. Robustness The attention mechanism based on Softmax normalization can be written as: \\[o = \\frac{\\sum_{i=1}^n e^{s_i} v_i}{\\sum_{i=1}^n e^{s_i}}\\] One day, I suddenly thought of a question: What if independent and identically distributed (i.i.d.) noise is added to $s_i$? To this end, we consider: \\[\\tilde{o} = \\frac{\\sum_{i=1}^n e^{s_i + \\epsilon_i} v_i}{\\sum_{i=1}^n e^{s_i + \\epsilon_i}}\\] where $\\epsilon_i$ is i.i.d. noise. However, after a simple analysis, I found the conclusion to be \"not much of anything\"—the attention mechanism naturally resists this type of noise, i.e., $\\tilde{o} \\approx o$. To understand this point, one only needs to real"}, {"file": "translation_9595.html", "title": "How to Measure Data Sparsity?", "content": "← Back to Index How to Measure Data Sparsity? By 苏剑林 | May 05, 2023 In machine learning, we often talk about sparsity; for example, we frequently say that attention matrices are usually very sparse. However, have you noticed that we never seem to provide a standard method for measuring the degree of sparsity? In other words, our previous discussions on sparsity have remained at an intuitive level without quantitative analysis. So the question arises: is there a standard method for measuring sparsity? After some searching, I found that there are indeed several available indicators, such as $l_1/l_2$, entropy, etc. However, due to different perspectives of focus, there is no single \"standard answer\" for measuring sparsity. This article briefly records my findings. Basic Results In a narrow sense, \"sparsity\" refers to the presence of a large number of zeros in the data. Therefore, the simplest sparsity indicator is the proportion of zeros. But if we only use this definition, attention matrices could hardly be considered sparse because the results from a softmax are always positive. Thus, it is necessary to generalize the concept of sparsity. A naive idea is to count the proportion of "}, {"file": "translation_9603.html", "title": "Transformer Upgrade Road: 9. A New Idea for Global Length Extrapolation", "content": "← Back to Index Transformer Upgrade Road: 9. A New Idea for Global Length Extrapolation By 苏剑林 | May 12, 2023 When discussing the reasons why Transformers cannot handle ultra-long sequences, the first thing that usually comes to mind is the quadratic complexity of Self Attention. However, even if we ignore computational power limitations, conventional Transformers still cannot handle ultra-long sequences because their Length Extrapolation is not good. Specifically, when the input sequence significantly exceeds the training length, the model's performance usually drops severely. Although there has been some related work, the problem of length extrapolation is still far from being practically solved. This article introduces a reference scheme conceived by the author, which might be the only current method suitable for generative models that possesses global dependency capabilities for length extrapolation. Method Review Length extrapolation, also known as length generalization, has been partially covered in our previous posts: \"Transformer Upgrade Road: 7. Length Extrapolation and Local Attention\" and \"Transformer Upgrade Road: 8. Length Extrapolation and Position Robustness\" . Howev"}, {"file": "translation_9607.html", "title": "Deriving Model Scaling Laws Based on the Quantization Hypothesis", "content": "← Back to Index Deriving Model Scaling Laws Based on the Quantization Hypothesis By 苏剑林 | May 18, 2023 Scaling Law refers to the asymptotic relationship between a model's performance and its scale. Specifically, model performance can be simplified as the model's loss function, while model scale can refer to the number of parameters, the amount of training data, or the number of training steps. Research on Scaling Laws investigates the general relationship between the loss function and variables such as parameters, data volume, and training steps. Experimental results from works like \"Scaling Laws for Neural Language Models\" and \"Training Compute-Optimal Large Language Models\" show that the Scaling Laws for neural networks mostly take the form of a \"Power Law.\" Why do they follow a power law? Can it be explained theoretically? The paper \"The Quantization Model of Neural Scaling\" provides a very interesting derivation based on a \"Quantization\" hypothesis. Let's explore it in this article. Derivation Hypotheses First, we assume that for a specific task, there exists a \"perfect model,\" and the models we train are approximations of this \"perfect model.\" Furthermore, we assume that the \""}, {"file": "translation_9617.html", "title": "NBCE: Using Naive Bayes to Extend LLM Context Handling Length", "content": "← Back to Index NBCE: Using Naive Bayes to Extend LLM Context Handling Length By 苏剑林 | May 23, 2023 Playing with Naive Bayes in the era of LLMs? This might be the first thought for many readers upon seeing the title. Indeed, when ancient Naive Bayes meets cutting-edge LLMs, it produces surprising results—we can directly extend the context handling length of existing LLM models without fine-tuning, regardless of model architecture, with linear efficiency, and results that look quite good. This is the NBCE ( N aive B ayes-based C ontext E xtension) method proposed in this article. Crossing the River by Feeling the Stones Assume $T$ is the token sequence to be generated, and $S_1, S_2, \\cdots, S_n$ are several given, relatively independent Context sets (e.g., $n$ different paragraphs, at least not a single sentence split into two fragments). Suppose their total length has exceeded the training length, while a single $S_k$ plus $T$ remains within the training length. We need to generate $T$ based on $S_1, S_2, \\cdots, S_n$, which means estimating $p(T|S_1, S_2, \\cdots, S_n)$. Simply put, Naive Bayes is \"Bayes' theorem + independence assumption.\" According to Bayes' theorem: \\begin{equa"}, {"file": "translation_9632.html", "title": "Some Supplementary Explanations and Analyses of the NBCE Method", "content": "← Back to Index Some Supplementary Explanations and Analyses of the NBCE Method By 苏剑林 | May 31, 2023 Last week, in \"NBCE: Expanding the Context Length of LLMs using Naive Bayes\" , we introduced a scheme called NBCE (Naive Bayes-based Context Extension) to extend the context length of LLMs based on Naive Bayes. Because it has advantages such as being plug-and-play, model-independent, and requiring no fine-tuning, it has received recognition from several readers. Overall, the testing feedback so far has been quite positive. Of course, some readers raised questions while using it. This article combines readers' queries and the author's subsequent reflections to provide some supplementary explanations and analyses of the NBCE method. Method Review Suppose $T$ is the token sequence to be generated, and $S_1, S_2, \\cdots, S_n$ are several given contexts. We need to generate $T$ based on $S_1, S_2, \\cdots, S_n$, which requires estimating $p(T|S_1, S_2, \\cdots, S_n)$. According to the Naive Bayes principle, we obtain: \\begin{equation}\\log p(T|S_1, S_2,\\cdots,S_n) = \\color{red}{(\\beta + 1)\\overline{\\log p(T|S)}} - \\color{green}{\\beta\\log p(T)} + \\color{skyblue}{\\text{constant}}\\label{eq:nb"}, {"file": "translation_9648.html", "title": "Naive Bayes is all you need ?", "content": "← Back to Index Naive Bayes is all you need ? By 苏剑林 | June 08, 2023 I apologize for choosing such a clickbait-style title. After writing \"NBCE: Using Naive Bayes to Extend the Context Length of LLMs\" , I felt that the Naive Bayes mechanism shares many characteristics with the Attention mechanism. Later, after doing some derivation, I discovered that the Attention mechanism can actually be viewed as a generalized, parameterized version of Naive Bayes. If that is the case, doesn't \" Attention is All You Need \" also imply that \"Naive Bayes is all you need\"? This is the reason behind the title of this article. Next, I will introduce my thought process and analyze how to understand the Attention mechanism from the perspective of Naive Bayes. Naive Bayes This article primarily considers language models, which aim to model $p(x_t|x_1,\\cdots,x_{t-1})$. According to Bayes' theorem, we have: \\begin{equation}p(x_t|x_1,\\cdots,x_{t-1}) = \\frac{p(x_1,\\cdots,x_{t-1}|x_t)p(x_t)}{p(x_1,\\cdots,x_{t-1})}\\propto p(x_1,\\cdots,x_{t-1}|x_t)p(x_t)\\end{equation} Based on the independence assumption $p(x_1,\\cdots,x_{t-1}|x_t) = \\prod\\limits_{j=1}^{t-1} p(x_j|x_t)$, we get: \\begin{equation}p(x_t|x_1,\\cdots,"}, {"file": "translation_9660.html", "title": "Gradient Flow: Exploring the Path to the Minimum", "content": "← Back to Index Gradient Flow: Exploring the Path to the Minimum By 苏剑林 | June 16, 2023 In this article, we will explore a concept known as \"Gradient Flow.\" Put simply, gradient flow connects the various points we encounter during the process of finding a minimum using gradient descent, forming a trajectory that changes over (virtual) time. This trajectory is what we call \"gradient flow.\" In the second half of the article, we will focus on how to extend the concept of gradient flow to probability spaces, resulting in \"Wasserstein Gradient Flow,\" which provides a new perspective for understanding the continuity equation, the Fokker-Planck equation, and other related topics. Gradient Descent Suppose we want to search for the minimum of a smooth function $f(\\boldsymbol{x})$. A common approach is Gradient Descent, which iterates according to the following format:\n\\begin{equation}\\boldsymbol{x}_{t+1} = \\boldsymbol{x}_t -\\alpha \\nabla_{\\boldsymbol{x}_t}f(\\boldsymbol{x}_t)\\label{eq:gd-d}\\end{equation}\nIf $f(\\boldsymbol{x})$ is convex with respect to $\\boldsymbol{x}$, gradient descent can usually find the minimum point; conversely, it typically only converges to a \"stationary point\"—a poin"}, {"file": "translation_9662.html", "title": "Generative Diffusion Models (19): GAN as a Diffusion ODE", "content": "← Back to Index Generative Diffusion Models (19): GAN as a Diffusion ODE By 苏剑林 | June 24, 2023 In the article \"Generative Diffusion Models (16): W-Distance ≤ Score Matching\" , we derived an inequality between the Wasserstein distance and the diffusion model's score matching loss, suggesting that the optimization objectives of diffusion models and WGAN share a certain degree of similarity. In this article, we will explore the findings in \"MonoFlow: Rethinking Divergence GANs via the Perspective of Wasserstein Gradient Flows\" , which further demonstrate the connection between GANs and diffusion models: a GAN can actually be viewed as a diffusion ODE in another time dimension! These findings suggest that although GANs and diffusion models appear to be two completely different types of generative models on the surface, they actually share many similarities and can provide mutual insights in many aspects. Introduction to the Idea We know that the generator trained by a GAN is a direct, deterministic transformation $\\boldsymbol{g}_{\\boldsymbol{\\theta}}(\\boldsymbol{z})$ from noise $\\boldsymbol{z}$ to a real sample. In contrast, a defining characteristic of diffusion models is \"progressiv"}, {"file": "translation_9668.html", "title": "Generative Diffusion Model Ramblings (20): From ReFlow to WGAN-GP", "content": "← Back to Index Generative Diffusion Model Ramblings (20): From ReFlow to WGAN-GP By 苏剑林 | June 28, 2023 In the previous article \"Generative Diffusion Model Ramblings (19): GAN as Diffusion ODE\" , we introduced how to understand GAN as a diffusion ODE in another time dimension. In short, GAN essentially transforms the movement of samples in a diffusion model into the movement of generator parameters! However, the derivation process in that article relied on relatively complex and independent content like Wasserstein gradient flow, making it difficult to connect well with the previous articles in the diffusion series, resulting in a technical \"gap.\" In my view, the ReFlow introduced in \"Generative Diffusion Model Ramblings (17): General Steps for Constructing ODE (Part 2)\" is the most intuitive approach for understanding Diffusion ODEs. Since GAN can be understood from a Diffusion ODE perspective, there must exist a perspective to understand GAN via ReFlow. After some experimentation, I have successfully derived results similar to WGAN-GP from ReFlow. Theory Review The reason I say \"ReFlow is the most intuitive approach for understanding Diffusion ODEs\" is that it is highly flexible"}, {"file": "translation_9675.html", "title": "Transformer Path to Upgrade: 10. RoPE is a $\beta$-base Encoding", "content": "← Back to Index Transformer Path to Upgrade: 10. RoPE is a $\beta$-base Encoding By 苏剑林 | July 6, 2023 For readers concerned with how to extend the Context length of LLMs (Large Language Models), last week was undoubtedly an exciting week, with the open-source community continuously producing heartening results. First, the user @kaiokendev experimented with a \"Positional Linear Interpolation\" scheme in his project SuperHOT , showing that with very little fine-tuning on long texts, existing LLMs can handle a Long Context. Almost simultaneously, Meta proposed the same idea and published it with rich experimental results in the paper \"Extending Context Window of Large Language Models via Positional Interpolation\" . The surprises did not stop there—subsequently, the user @bloc97 proposed NTK-aware Scaled RoPE , achieving the effect of extending the Context length without any fine-tuning! All these developments, especially NTK-aware Scaled RoPE, have forced me to re-think the meaning of RoPE . After analysis, I discovered that the construction of RoPE can be viewed as a $\\beta$-base encoding. From this perspective, these recent advances in the open-source community can be understood as d"}, {"file": "translation_9687.html", "title": "When Generative Models Run Amok: Will the Internet Suffer from \"Mad Cow Disease\"?", "content": "← Back to Index When Generative Models Run Amok: Will the Internet Suffer from \"Mad Cow Disease\"? By 苏剑林 | July 14, 2023 As is well known, whether in the realm of text or vision, various generative models are \"running amok\" on the internet with an unstoppable momentum. Although everyone understands that there is still a long way to go to achieve true Artificial General Intelligence (AGI), this does not prevent people from increasingly frequent use of generative models to create and share content. You see, many online articles are already paired with illustrations generated by Stable Diffusion models; you see, the style of many news reports is increasingly showing the shadow of ChatGPT. This seemingly harmless trend is quietly raising a question: Should we remain vigilant about the proliferation of generative model data on the internet? A recently published paper, \"Self-Consuming Generative Models Go MAD\" , reveals a worrying possibility: the unchecked expansion of generative models on the internet might lead to a digital version of a \"Mad Cow Disease\" epidemic. In this article, we will study this paper together and explore its potential impacts. \"Eating Oneself\" On one hand, the in"}, {"file": "translation_9698.html", "title": "Re-exploring Shared Embeddings at the Output of Language Models", "content": "← Back to Index Re-exploring Shared Embeddings at the Output of Language Models By 苏剑林 | July 20, 2023 In the early days of pre-training, it was common practice to reuse Embedding weights at the output of a language model. For instance, BERT, the first version of T5, and early versions of GPT all employed this operation. This was because when the model backbone is small and the vocabulary is large, the number of parameters in the Embedding layer is quite significant. Adding an independent weight matrix of the same size at the output would cause a sharp increase in VRAM consumption. However, as model parameter scales have grown, the Proportion of the Embedding layer has become relatively smaller. Furthermore, studies like \"Rethinking embedding coupling in pre-trained language models\" have suggested that sharing Embeddings might have some negative impacts. Consequently, the practice of sharing Embeddings has become increasingly rare. The purpose of this article is to analyze the problems that may arise when sharing Embedding weights and to explore how to perform initialization and parameterization more effectively. Although shared Embeddings may seem \"outdated,\" it remains an interes"}, {"file": "translation_9706.html", "title": "Transformer Upgrade Path: 11. Taking the β-base Position Encoding to the End", "content": "← Back to Index Transformer Upgrade Path: 11. Taking the β-base Position Encoding to the End By 苏剑林 | July 31, 2023 In the article \"Transformer Upgrade Path: 10. RoPE is a β-base Encoding\" , we provided a $\\beta$-base interpretation of RoPE and derived NTK-aware Scaled RoPE , which can extend Context length without fine-tuning, based on the idea of base conversion. It must be said that understanding position encoding through the analogy of $\\beta$-base representation is a very beautiful and inspiring perspective. Every time I think deeply about it, I seem to gain new insights. This article will revisit the $\\beta$-base interpretation of RoPE and attempt to generalize the existing NTK-aware Scaled RoPE, with the aim of finding an optimal strategy for extending the Context length of LLMs without fine-tuning. Base Analogy We know that the parameterization of RoPE follows the form of Sinusoidal position encoding . Whether by coincidence or design, the Sinusoidal position encoding of an integer $n$ has many similarities with its $\\beta$-base encoding. Specifically, the $m$-th digit (counting from right to left) of the $\\beta$-base representation of an integer $n$ is:\n    \\begin{equation"}, {"file": "translation_9708.html", "title": "Transformer Upgrade Road: 12, ReRoPE for Infinite Extrapolation?", "content": "← Back to Index Transformer Upgrade Road: 12, ReRoPE for Infinite Extrapolation? By 苏剑林 | August 07, 2023 Since introducing the idea of mixed-base positional bases in \"Transformer Upgrade Road: 11, Thinking the Base-β Positional System Through\" to further generalize NTK-aware Scaled RoPE, I felt that the effectiveness of similar approaches had reached its upper limit. To achieve more significant improvements, one must find another way. At this point, I recalled a concept I had previously conceived. It was shelved because its complexity was relatively high, but since I have hit a bottleneck, \"the only way is the best way,\" so I decided to revisit it. Unexpectedly, although this method increases some inference complexity, its experimental results are surprisingly good—it even hints at having infinite length extrapolation capabilities! Therefore, I am eager to share this method in this article. Due to its formal similarity to the ReLU activation function, I have named this method \"ReRoPE (Rectified Rotary Position Embeddings).\" Review We know that RoPE is formally an absolute position encoding, but in reality, it brings relative position information to the Attention mechanism, specifi"}, {"file": "translation_9728.html", "title": "Transformer Upgrade Road: 13. Inverse Leaky ReRoPE", "content": "← Back to Index Transformer Upgrade Road: 13. Inverse Leaky ReRoPE By 苏剑林 | August 14, 2023 Last week in \"Transformer Upgrade Road: 12. ReRoPE for Infinite Extrapolation?\" , I proposed ReRoPE and Leaky ReRoPE. Numerous experimental results indicate that they can extend the Context length of LLMs without fine-tuning and with almost no loss in training performance, achieving the ideal characteristic of \"longer context, lower loss.\" Furthermore, unlike NTK-aware Scaled RoPE, ReRoPE seems to exhibit infinite Context processing capabilities. In short, ReRoPE appears quite satisfactory, but the flies in the ointment are the increased inference costs. Specifically, the first step of inference requires calculating Attention twice, and each subsequent step requires recomputing position embeddings. This article attempts to resolve this issue by \"inversely\" using Leaky ReRoPE during training. Review Let us tirelessly revisit: RoPE is formally an absolute position encoding, but the effect it actually achieves is relative position encoding. The corresponding relative position matrix is: \\begin{equation}\\begin{pmatrix}0 & \\\\\n1 & 0 & \\\\\n2 & 1 & 0 &\\\\\n3 & 2 & 1 & 0 & \\\\\n\\ddots & 3 & 2 & 1 & 0 & \\\\"}, {"file": "translation_9731.html", "title": "The Road to Transformer Upgrade: 14. When HWFA Meets ReRoPE", "content": "← Back to Index The Road to Transformer Upgrade: 14. When HWFA Meets ReRoPE By 苏剑林 | August 24, 2023 In the previous article \"The Road to Transformer Upgrade: 13. Reversing Leaky ReRoPE\" , I attempted to use the idea of reversing Leaky ReRoPE during the training phase so that the position encoding during the inference phase becomes normal RoPE. This aimed to achieve length extrapolation while solving the drawback of slow inference in ReRoPE. Unfortunately, experimental results showed that the \"Leaky ReRoPE → RoPE\" effect was not as good as \"RoPE → ReRoPE/Leaky ReRoPE,\" so this problem hasn't been fully resolved yet. At this point, I remembered the HWFA proposed in the earlier article \"The Road to Transformer Upgrade: 9. A New Idea for Global Length Extrapolation\" . HWFA itself possesses a certain degree of length extrapolation capability. If combined with ReRoPE in a \"powerful alliance,\" would it yield better results? More importantly, the addition of HWFA can significantly reduce inference costs, thereby compensating for the shortcomings of ReRoPE! Reviewing the Old First, as a \"routine,\" let's review HWFA. HWFA (Hybrid Window-Full Attention) is not a specific model but an Attenti"}, {"file": "translation_9736.html", "title": "Abnormalities and Countermeasures for Embeddings under Lion/Tiger Optimizer Training", "content": "← Back to Index Abnormalities and Countermeasures for Embeddings under Lion/Tiger Optimizer Training By 苏剑林 | August 28, 2023 Ever since I proposed the Tiger optimizer in \"Tiger: An Extremely Frugal Optimizer\" , Tiger has become my \"standard\" optimizer for training models. Recently, I attempted to apply Tiger to the pre-training of a model with 7 billion parameters. The initial results looked promising, preliminary suggesting that Tiger is capable of scaling up. However, upon inspecting the weights of the trained model, I discovered some abnormalities in the Embeddings; certain components of the Embedding reached the magnitude of \\(\\pm 100\\). After analysis, I found that similar phenomena do not occur with Adam. This is a specific issue for optimizers like Tiger or Lion that use the \\(\\text{sign}\\) function. This article provides two reference solutions for this issue at the end. I will record the analysis process here for everyone's reference. Phenomenon Next, we will use the Tiger optimizer as an example for our analysis, but the process and conclusions apply equally to Lion. First, the phenomena I observed were as follows: 1. Some Embedding components for certain tokens became \\"}, {"file": "translation_9752.html", "title": "BytePiece: A Purer, Higher Compression Ratio Tokenizer", "content": "← Back to Index BytePiece: A Purer, Higher Compression Ratio Tokenizer By 苏剑林 | September 07, 2023 Currently, the most popular Tokenizer in LLMs is likely Google's SentencePiece . It aligns with several ideal properties for a tokenizer, such as being language-agnostic and data-driven. Because it is written in C++, its realization (tokenization) speed is very fast, making it highly suitable for efficiency-critical scenarios. However, it also has some obvious drawbacks, such as slow training speeds (for the BPE algorithm) and high memory consumption. Furthermore, because it is written in C++, it often acts as a \"black box\" for most users, making it difficult to study or perform secondary development. In fact, training a tokenizer is equivalent to the traditional task of \"new word discovery.\" Given the author's prior work on Chinese word segmentation and the Minimum Entropy series, I have accumulated considerable experience in new word discovery. Consequently, I have long wanted to write my own version of a tokenizer. These past few days, I finally found the time to complete the initial version. Emulating SentencePiece, I have named it \"BytePiece.\" Ideal Characteristics Since we are r"}, {"file": "translation_9762.html", "title": "A Problem and Countermeasure for Large-Vocabulary Language Models in Text Continuation Tasks", "content": "← Back to Index A Problem and Countermeasure for Large-Vocabulary Language Models in Text Continuation Tasks By 苏剑林 | September 13, 2023 For LLMs, increasing the vocabulary size of the Tokenizer to improve the compression rate, thereby shortening sequence lengths and reducing decoding costs, is a development that everyone welcomes. After all, increasing the vocabulary only requires expanding the Embedding layer and the output Dense layer, parts where the added computational overhead is nearly imperceptible. However, the performance boost in decoding speed brought by shortening the sequence length is very tangible. Of course, increasing the vocabulary size may also have some negative impacts on model performance, so it cannot be increased without restraint. This article analyzes a problem that arises in language models on text continuation tasks after increasing the vocabulary and proposes a reference solution. Pros and Cons Analysis The benefits of increasing the vocabulary size are obvious. On one hand, since LLMs are autoregressive, decoding becomes progressively slower. \"Increasing vocabulary → improving compression rate → shortening sequence length\" means that the number of tok"}, {"file": "translation_9768.html", "title": "A Brief Exploration of Stochastic Tokenization: From Viterbi Decoding to Viterbi Sampling", "content": "← Back to Index A Brief Exploration of Stochastic Tokenization: From Viterbi Decoding to Viterbi Sampling By 苏剑林 | September 16, 2023 After the previous article \"A Problem and Countermeasure for Large Vocabulary Language Models in Continuation Tasks\" was published, a reader quickly pointed out that introducing stochastic tokenization during the training phase can solve the same problem, and there are already papers and implementations for it. After further research and learning, I discovered that this technique is called Subword Regularization , which was first applied in NMT (Neural Machine Translation), and SentencePiece already has a corresponding implementation. It seems this technique can indeed alleviate the aforementioned problem and may even help enhance the fault tolerance of language models, so I had the idea to include it in BytePiece . So the question arises: how to change deterministic tokenization into stochastic tokenization? BytePiece is based on the Unigram model, which uses the Viterbi algorithm to find the tokenization scheme with the maximum probability. Since there are probabilities involved, can we naturally derive a stochastic sampling method? This article di"}, {"file": "translation_9775.html", "title": "The Minimum Value of $a+b+c$ When $N=ab+c$ in the Set of Natural Numbers", "content": "← Back to Index The Minimum Value of $a+b+c$ When $N=ab+c$ in the Set of Natural Numbers By 苏剑林 | September 20, 2023 The night before last, a member in a WeChat group posed a question: For an arbitrary integer $N > 100$, find an approximate algorithm to find $N=a \\times b+c$ (where $a, b, c$ are non-negative integers) such that $a+b+c$ is as small as possible. At first glance, my initial reaction was, \"Is an algorithm even necessary for this?\" It seemed like there was so much freedom that an analytical solution should exist. After a brief analysis, I offered an \"answer,\" but a group member quickly provided a counterexample. It was then that I realized the problem was not so trivial. After formally deriving it, I finally arrived at a viable algorithm. Just as I thought the problem was settled, a member of another mathematics group elegantly constructed a new parameterization, proving that the algorithm's complexity could be further reduced! The entire process was full of twists and turns, yielding significant insights, which I have recorded here to share with everyone. A Careless Mistake Setting aside the constraint $N > 100$, the original problem can be equivalently transformed int"}, {"file": "translation_9783.html", "title": "Mind-Bending: Nonlinear RNNs Can Actually Be Computed in Parallel?", "content": "← Back to Index Mind-Bending: Nonlinear RNNs Can Actually Be Computed in Parallel? By 苏剑林 | September 26, 2023 In recent years, linear RNNs have attracted significant attention from researchers (such as in my previous post \"Google's New Work Attempts to 'Resurrect' RNN: Can RNNs Shine Again?\" ) due to their ability to be trained in parallel and their constant inference cost. This gives RNNs \"a foothold\" even in the current trend where Transformers are blooming everywhere. However, at present, it seems this \"foothold\" belongs only to linear RNNs, as nonlinear RNNs cannot be trained efficiently in parallel, making them \"unable to keep up\" in the architecture wars. However, a paper titled \"Parallelizing Non-Linear Sequential Models over the Sequence Length\" takes a different view. It proposes an iterative algorithm that claims to achieve parallel training for nonlinear RNNs! Is it really that magical? Let's take a closer look. Finding Fixed Points The original paper presents its method in a very general way, focusing primarily on PDEs and ODEs. Here, we will start directly with RNNs. Consider a common simple nonlinear RNN: \\begin{equation}x_t = \\tanh(Ax_{t-1} + u_t)\\label{eq:rnn}\\end{"}, {"file": "translation_9787.html", "title": "With a Little Pre-training, Transformer's Long-Sequence Performance Can Still Rise Significantly!", "content": "← Back to Index With a Little Pre-training, Transformer's Long-Sequence Performance Can Still Rise Significantly! By 苏剑林 | October 8, 2023 As the mainstream model architecture for LLMs, Transformers perform excellently across all types of tasks. In most cases, the criticism of Transformers is focused on their quadratic complexity rather than their effectiveness—except for a benchmark called Long Range Arena (hereafter referred to as LRA). For a long time, LRA has been the \"home court\" for linear RNN-based models; compared to them, Transformers have shown a clear performance gap, leading some to question whether this is an inherent flaw of the architecture. However, the recent paper \"Never Train from Scratch: Fair Comparison of Long-Sequence Models Requires Data-Driven Priors\" has finally supplied the \"missing link.\" The paper points out that a lack of pre-training is the primary reason why Transformers perform poorly on LRA, and while all architectures can benefit from pre-training, the improvements for Transformers are significantly more pronounced. Old Background Long Range Arena (LRA) is a benchmark for long-sequence modeling, introduced in the paper \"Long Range Arena: A Benchma"}, {"file": "translation_9797.html", "title": "EMO: A Classification Loss Function Designed Based on Optimal Transport", "content": "← Back to Index EMO: A Classification Loss Function Designed Based on Optimal Transport By 苏剑林 | October 13, 2023 As is well-known, the standard loss for classification tasks is Cross Entropy (equivalent to Maximum Likelihood Estimation, or MLE). It is characterized by its simplicity and efficiency, but in certain scenarios, it reveals issues such as deviation from evaluation metrics and overconfidence. Correspondingly, there have been many improvement efforts. We have previously introduced some, such as \"Revisiting the Class Imbalance Problem: Comparison and Connection between Weight Adjustment and Modified Loss\" , \"How to Train Your Accuracy?\" , and \"A Simple Solution to Mitigate Overconfidence in Cross Entropy\" . Since the training of Large Language Models (LLMs) can also be understood as a token-by-token classification task with cross-entropy as the default loss, these improvement works remain valuable in today's LLM-dominated era. In this article, we introduce a work titled \"EMO: Earth Mover Distance Optimization for Auto-Regressive Language Modeling\" . It proposes a new improved loss function, EMO, based on the principle of Optimal Transport, claiming to significantly improve"}, {"file": "translation_9811.html", "title": "Revisiting Random Tokenization: From Viterbi Sampling to Perfect Sampling Algorithms", "content": "← Back to Index Revisiting Random Tokenization: From Viterbi Sampling to Perfect Sampling Algorithms By 苏剑林 | October 16, 2023 In the article \"An Exploration of Random Tokenization: From Viterbi Decoding to Viterbi Sampling,\" I proposed a random tokenization algorithm called \"Viterbi Sampling.\" It is a minor modification of the Viterbi Decoding algorithm, which seeks the optimal solution. It retains the simple and fast characteristics of the Viterbi algorithm and is significantly more efficient than existing methods like Subword Regularization . However, a reader on Zhihu, @鶴舞 , pointed out that the current sampling algorithm might \"dilute\" the occurrence probability of certain schemes through multiple rounds of 1-on-1 \"challenges.\" The direct consequence is that the segmentation with the highest original score might not appear with the highest probability. After careful consideration, I realized that this issue does indeed exist. At the time, in my eagerness to derive a new sampling algorithm, my thinking and handling of the details were indeed somewhat coarse. Therefore, this article will further refine the Viterbi Sampling algorithm and prove that the perfected algorithm is equi"}, {"file": "translation_9812.html", "title": "Looking at the Scale Operation of Attention from the Perspective of Gradient Maximization", "content": "← Back to Index Looking at the Scale Operation of Attention from the Perspective of Gradient Maximization By 苏剑林 | October 22, 2023 We know that the Scale factor of Scaled Dot-Product Attention is $\\frac{1}{\\sqrt{d}}$, where $d$ is the dimension of $\\boldsymbol{q}, \\boldsymbol{k}$. The general explanation for this Scale factor is: if not divided by $\\sqrt{d}$, the initial Attention would be very close to a one-hot distribution, which causes gradient vanishing and prevents the model from being trained. However, it can be proven that when the Scale equals 0, there is also a gradient vanishing problem, which means the Scale being either too large or too small is problematic. So, how large should the Scale be? Is $\\frac{1}{\\sqrt{d}}$ the optimal Scale? This article attempts to answer this question from the perspective of gradients. Existing Results In \"Briefly Discussion on Transformer Initialization, Parameterization, and Standardization\" , we derived the standard Scale factor $\\frac{1}{\\sqrt{d}}$. The derivation logic is simple: assuming that in the initial stage, $\\boldsymbol{q}, \\boldsymbol{k} \\in \\mathbb{R}^d$ are sampled from a distribution with \"mean 0 and variance 1\", we can ca"}, {"file": "translation_9826.html", "title": "Embarrassingly Simple FSQ: \"Rounding\" Surpasses VQ-VAE", "content": "← Back to Index Embarrassingly Simple FSQ: \"Rounding\" Surpasses VQ-VAE By 苏剑林 | October 31, 2023 Just as \"XXX is all you need\" has become a cliché, many papers are titled \"An Embarrassingly Simple XXX.\" In my view, most of these papers are more gimmick than substance. However, I recently read a paper that truly makes one exclaim, \"That is embarrassingly simple!\" The paper is titled \"Finite Scalar Quantization: VQ-VAE Made Simple.\" As the name suggests, this work aims to simplify VQ-VAE using FSQ (Finite Scalar Quantization). With the growing popularity of generative models and multimodal LLMs, VQ-VAE and its successors have risen in prominence as \"Tokenizers for images.\" However, the training of VQ-VAE itself presents several challenges. The FSQ paper claims that through a simpler \"rounding\" operation, one can achieve the same goals with better results, faster convergence, and more stable training. Is FSQ really that magical? Let's dive in and learn about it. VQ First, let's understand \"VQ.\" VQ stands for \"Vector Quantize,\" which refers to the technique of mapping infinite, continuous encoding vectors to a finite, discrete set of integer numbers. If we apply VQ to the bottleneck la"}, {"file": "translation_9844.html", "title": "VQ the Key, and Transformer Complexity Becomes Linear", "content": "← Back to Index VQ the Key, and Transformer Complexity Becomes Linear By 苏剑林 | November 09, 2023 Efficient Transformer refers generally to all work dedicated to reducing the quadratic complexity of the Transformer. Initially, this specifically targeted improvements to Attention, but later, more general ideas such as Fourier transforms and linear RNNs were also included in this category. It must be said that in order to reduce the quadratic complexity of the Transformer, various experts have truly \"shown their special prowess like the Eight Immortals crossing the sea,\" and various magical ideas have \"bloomed in abundance,\" from which I have learned a lot of theoretical knowledge. However, although Efficient Transformers are brilliant in theory, in practice, the field has remained somewhat lukewarm, with no exceptionally performing models. In today's LLM-dominated era, they have even gradually faded from public view and from my own personal interests. However, a recent paper titled \"Transformer-VQ: Linear-Time Transformers via Vector Quantization\" has truly impressed me. The authors insightfully observed that by simply applying VQ (Vector Quantization) to the Keys of standard Attenti"}, {"file": "translation_9855.html", "title": "[Life Notes] The Ultimate Destination of Frying Pans is the Iron Wok", "content": "← Back to Index [Life Notes] The Ultimate Destination of Frying Pans is the Iron Wok By 苏剑林 | November 13, 2023 Iron wok (Internet image) Many students who know how to cook have probably struggled with one thing: choosing a frying pan. The dilemma regarding frying pans ultimately boils down to the balance between non-stick performance and convenience. The simplest non-stick pan is, of course, the coated one. If your only heat source at home is an induction cooker and your cooking habits are relatively gentle, then a coated non-stick pan is often the best choice. However, once you have a gas stove with an open flame, or if you prefer high-heat stir-frying (bao chao), a coated pan may no longer be suitable. After all, there is always a risk of the coating peeling off at high temperatures. In such cases, one generally considers uncoated non-stick pans. There are many varieties of uncoated non-stick pans, such as simple iron woks, stainless steel pans with honeycomb patterns, titanium pans, pure titanium pans, etc., and their prices generally increase accordingly. However, in the end, I believe most people will return to the humble iron wok. The iron wok was the original choice to begi"}, {"file": "translation_9859.html", "title": "Transformer Upgrade Path: 15. Key Normalization Helps Length Extrapolation", "content": "← Back to Index Transformer Upgrade Path: 15. Key Normalization Helps Length Extrapolation By 苏剑林 | November 20, 2023 Broadly speaking, current Transformer length extrapolation techniques can be classified into two categories: one is post-hoc modification, such as NTK-RoPE , YaRN , and ReRoPE . These methods are characterized by directly modifying the inference model, achieving a certain degree of length extrapolation without fine-tuning. However, the downside is that they cannot maintain identity in the model's performance within the original training length. The other category is pre-training modification, such as ALIBI , KERPLE , XPOS , and HWFA . These can achieve length extrapolation without further modification, but the corresponding changes must be introduced before training. Consequently, they cannot be used on existing models without fine-tuning, and whether such methods can effectively scale up has not yet been widely recognized. In this article, I will introduce an unexpectedly discovered length extrapolation scheme—\"KeyNorm\"—which involves applying $L_2$ Normalization to the Key sequence in Attention. It clearly belongs to the pre-training modification category, but the"}, {"file": "translation_9862.html", "title": "I Found Traces of Transformer-VQ in Performer", "content": "← Back to Index I Found Traces of Transformer-VQ in Performer By 苏剑林 | November 29, 2023 A few days ago, in \"VQ the Key, and Transformer's Complexity Becomes Linear,\" we introduced \"Transformer-VQ,\" a scheme that achieves linear attention complexity by applying a Vector Quantization (VQ) transformation to the Key sequence. Admittedly, Transformer-VQ provides a beautiful transition from standard Attention to linear Attention, embodying a sense of \"simplicity as the ultimate sophistication.\" However, readers familiar with VQ might feel that as the codebook size or model parameters increase, VQ could become a performance bottleneck. This is because the gradients estimated via the Straight-Through Estimator (STE) are likely suboptimal (experimental results from FSQ also provide support for this). Furthermore, the gradient truncation used by Transformer-VQ to make training efficiency linear might also become a future performance bottleneck. To this end, I spent some time thinking about linearization ideas that could replace VQ. From the $\\exp(QC^\\top)$ form in Transformer-VQ, I was reminded of Performer , and by \"following the trail,\" I discovered that Performer can be regarded as a \"so"}, {"file": "translation_9881.html", "title": "Generative Diffusion Model Talk (21): Accelerated ODE Sampling via the Mean Value Theorem", "content": "← Back to Index Generative Diffusion Model Talk (21): Accelerated ODE Sampling via the Mean Value Theorem By 苏剑林 | December 07, 2023 In the history of generative diffusion models, DDIM and Song Yang's concurrent work on Diffusion SDEs are both considered milestones. This is because they established a close connection between diffusion models and the mathematical fields of Stochastic Differential Equations (SDE) and Ordinary Differential Equations (ODE). This allows us to utilize various existing mathematical tools from SDEs and ODEs to analyze, solve, and extend diffusion models. For instance, a vast amount of subsequent work on accelerated sampling is based on this foundation, effectively opening a completely new perspective on generative diffusion models. In this article, we focus on ODEs. In our previous blogs—Part (6) , (12) , (14) , (15) , and (17) —we already derived the relationship between ODEs and diffusion models. This article provides a brief introduction to sampling acceleration for diffusion ODEs and highlights a clever new acceleration scheme called \"AMED,\" which utilizes the spirit of the \"Mean Value Theorem.\" Euler's Method As mentioned, since we have already derive"}, {"file": "translation_9889.html", "title": "Can Attention Mechanisms Really \"Focus Attention\"?", "content": "← Back to Index Can Attention Mechanisms Really \"Focus Attention\"? By 苏剑林 | December 12, 2023 Previously, in articles like \"Pathways to Transformer Upgrades: 3. From Performer to Linear Attention\" and \"Why are Current LLMs All Decoder-only Architectures?\" , we explored the Attention mechanism from the perspective of the \"rank\" of the Attention matrix. We once judged that the critical reason why Linear Attention is inferior to Standard Attention is the \"low-rank bottleneck.\" However, while this explanation may hold for bidirectional Encoder models, it is difficult to apply to unidirectional Decoder models. This is because the upper triangular part of the Decoder's Attention matrix is masked, and the remaining lower triangular matrix is necessarily full-rank. Since it is already full-rank, the low-rank bottleneck problem seemingly no longer exists. Therefore, the \"low-rank bottleneck\" cannot completely explain the performance deficiencies of Linear Attention. In this article, I attempt to seek an explanation from another angle. Simply put, compared to Standard Attention, Linear Attention has a harder time \"focusing attention,\" making it difficult to accurately locate key tokens. This"}, {"file": "translation_9902.html", "title": "Making Alchemy More Scientific (I): Average Loss Convergence of SGD", "content": "← Back to Index Making Alchemy More Scientific (I): Average Loss Convergence of SGD By 苏剑林 | December 19, 2023 Many times, we jokingly refer to the training process of deep learning models as \"alchemy,\" because, like the alchemy of ancient times, the process seems to have a certain scientific basis but overall feels \"mysterious and profound.\" Although this site has previously covered some work related to optimizers and even written a series titled \"Optimization Algorithms from a Dynamical Perspective,\" those were relatively superficial introductions that did not delve into deeper theoretical results. To make future alchemy more scientific, I have decided to brush up on some theoretical results related to optimization, striving to provide more theoretical support for the path of alchemy. In this article, we will learn a very fundamental convergence result for Stochastic Gradient Descent (SGD). Although this result may now seem crude and impractical, it was a very important attempt at proving optimizer convergence, especially since it considered the characteristic that we actually use Stochastic Gradient Descent (SGD) rather than Full Gradient Descent (GD), making the conclusion sign"}, {"file": "translation_9907.html", "title": "Wrote a Helper Website for Browsing Papers: Cool Papers", "content": "← Back to Index Wrote a Helper Website for Browsing Papers: Cool Papers By 苏剑林 | December 25, 2023 Writing at the Beginning # For a long time, I have had the habit of scanning Arxiv daily to keep up with the latest results in the field and to remind myself that \"not to advance is to go back.\" Many readers have previously asked me how I browse Arxiv and what helper tools I use. In fact, for a long time, I directly browsed the Arxiv official website without using any algorithmic filtering—I went through them one by one myself. This process is tedious, but not unacceptable. The reason for not using algorithms for initial screening was mainly the concern about missed recalls; after all, \"scanning\" is for chasing the new, and once an algorithm misses a recall, one \"loses the first-mover advantage.\" Since the release of Kimi Chat, I had been planning to write a helper website combined with Kimi to accelerate the process of scanning papers. Having been a bit more free in the last few weeks, with the help of GPT-4 and Kimi, I have preliminarily completed this website. After several days of testing and optimization, it has gradually become stable, so I am formally inviting readers to try it"}, {"file": "translation_9920.html", "title": "Happy New Year! My Development Experience with Cool Papers", "content": "← Back to Index Happy New Year! My Development Experience with Cool Papers By 苏剑林 | January 01, 2024 Last week, in “I Wrote a Research Paper Assistant Website: Cool Papers,” I shared a paper-skimming website I developed, which has since gained some recognition from users. However, as the saying goes, “the more people use it, the more problems are exposed.” Once the user volume increased, I realized how unrefined my original code was. Consequently, I spent the entire past week constantly fixing bugs, even dealing with one as late as this afternoon. This article briefly summarizes my thoughts during the development and bug-fixing process. Cool Papers: https://papers.cool Technology In fact, the domain “papers.cool” has been registered for over four years. This shows that I had planned to create a website like Cool Papers a long time ago and had even built some prototypes. However, the reason it took four years to officially launch can be boiled down to one thing: my technical skills weren't up to par. On one hand, my web development skills were lacking; I don't really know how to build websites, and at most, I could only make simple patches here and there. Although this blog, “Scient"}, {"file": "translation_9931.html", "title": "If Local Cosine Similarity is High, Does the Global Cosine Similarity Have to Be High as Well?", "content": "← Back to Index If Local Cosine Similarity is High, Does the Global Cosine Similarity Have to Be High as Well? By 苏剑林 | January 09, 2024 When analyzing model parameters, there are scenarios where we treat all parameters of a model as a single holistic vector, while in other cases, we analyze different sets of parameters separately. For instance, in a 7B-parameter LLaMA model, we might sometimes view it as \"one 7-billion-dimensional vector.\" At other times, following the implementation of the model, we may view it as \"it several hundred vectors of varying dimensions.\" In the most extreme case, we could even view it as \"7 billion 1-dimensional vectors.\" Since there are different ways to view these parameters, different methods exist for calculating statistical indicators—namely, local calculation and global calculation. This raises the question: what is the relationship between indicators computed locally and those computed globally? In this article, we focus on the cosine similarity of two vectors. If the dimensions of two large vectors are partitioned into several groups, and the cosine similarities of the sub-vectors corresponding to the same groups are all very high, does the cos"}, {"file": "translation_9938.html", "title": "Side Paths: How to Implement Retries More Elegantly in Python", "content": "← Back to Index Side Paths: How to Implement Retries More Elegantly in Python By 苏剑林 | January 14, 2024 In this article, we discuss a programming topic: how to implement retries in Python more elegantly. In the post \"Happy New Year! Recording the Development Experience of Cool Papers,\" I shared some experiences from developing Cool Papers, specifically mentioning the network communication steps required. Whenever network communication is involved, there is a risk of failure (no one can guarantee the network won't occasionally act up), so retrying is a fundamental operation in network communication. Furthermore, when dealing with multi-processing, databases, hardware interactions, etc., a retry mechanism is usually necessary. Implementing retries in Python is not difficult, but there are certain techniques to doing it more simply without losing readability. Next, I will share my own attempts. Looping Retries A complete retry process generally includes parts like loop iteration, exception handling, delay waiting, and follow-up operations. Its standard implementation uses a for loop with try ... except ... to catch exceptions. A reference code snippet is as follows: import time\nfrom r"}, {"file": "translation_9948.html", "title": "Transformer Upgrade Path: 16. \"Reviewing\" Length Extrapolation Techniques", "content": "← Back to Index Transformer Upgrade Path: 16. \"Reviewing\" Length Extrapolation Techniques By 苏剑林 | January 26, 2024 Looking back, I realized that starting from the 7th post, \"Transformer Upgrade Path: 7. Length Extrapolation and Local Attention,\" this series has been \"stuck\" on length extrapolation, with nine consecutive articles (excluding this one) revolving around it. Today, exactly one year and a bit has passed since that 7th article. During this year, the open-source community has made significant progress in length extrapolation research, and I have gradually developed some of my own understandings. For instance, the problem is far less simple than initially imagined, and many previous works based on local attention are not always effective. This suggests that several older analyses failed to touch the core of the issue. In this article, I attempt to combine my findings and insights to \"review\" mainstream length extrapolation results and try to discover the key to training-free length extrapolation. Problem Definition As the name suggests, training-free length extrapolation means that there is no need to perform additional training on long sequence data. By training the model"}, {"file": "translation_9969.html", "title": "Idempotent Generative Network IGN: A GAN Attempting to Unify Discrimination and Generation", "content": "← Back to Index Idempotent Generative Network IGN: A GAN Attempting to Unify Discrimination and Generation By 苏剑林 | January 31, 2024 A while ago, a generative model named \" Idempotent Generative Network (IGN) \" caught some attention. It claimed to be a new type of generative model independent of existing VAE, GAN, flow, and Diffusion models, and it features single-step sampling. Perhaps because everyone has grown weary of the multi-step sampling process of current mainstream diffusion models, any \"rustle in the grass\" claiming single-step sampling easily attracts attention. Furthermore, the word \"idempotent\" in the name IGN added a sense of mystery, further heightening expectations and successfully piquing my interest. However, I was busy with other things before, so I didn't have time to read the model details carefully. Recently, having some free time, I remembered IGN and pulled up the paper. But after reading it, I was quite puzzled: how is this a new model? Isn't it just a variant of GAN? The difference from a regular GAN is that it merges the generator and discriminator into one. Does this \"merging into one\" offer any special benefits, such as better training stability? Perso"}, {"file": "translation_9978.html", "title": "A More Convenient Way to Open Cool Papers: Chrome Redirect Extension", "content": "← Back to Index A More Convenient Way to Open Cool Papers: Chrome Redirect Extension By 苏剑林 | February 02, 2024 Some Background Since the launch of Cool Papers, many users have suggested adding a search function. Later, a simple page-level search was indeed implemented in the frontend using JavaScript, which solved the needs of some users. However, some readers still hope for a more complete global search. To be honest, I understand that this demand exists, but the data on Cool Papers is accumulated day by day. Since it has only been online for a month, the number of papers is not yet large, so building a comprehensive search engine wouldn't be very meaningful. Furthermore, building search is not my forte, and I haven't found a particularly good way to utilize LLMs to optimize search yet, among other reasons. All in all, for now, the conditions are not right to implement a comprehensive and unique search, so it’s better not to do it at all (though everyone is welcome to brainstorm in the comments section). Later, after discussions with colleagues, I came up with a \"borrowing a gift to present to Buddha\" approach—writing a Chrome redirect extension that can redirect from any page to"}, {"file": "translation_9984.html", "title": "Reflections on \"Building Behind Closed Doors\": Shallow Thoughts on Multimodal Approaches (1): Lossless Input", "content": "← Back to Index Reflections on \"Building Behind Closed Doors\": Shallow Thoughts on Multimodal Approaches (1): Lossless Input By 苏剑林 | February 21, 2024 In this article, I want to share some of my \"behind closed doors\" thoughts—or rather, some conjectures—regarding multimodal model architectures. The recent releases of Google's Gemini 1.5 and OpenAI's Sora have once again ignited many people's passion for multimodality. The sparse technical reports have also led to intense speculation about the architectures behind them. However, this article is not being published just to join the hype; in fact, some of these ideas have been brewing for a long time and have only recently been organized into a coherent form. It just so happens that their release coincided with these thoughts. A disclaimer beforehand: the term \"building behind closed doors\" is not false modesty. My experience with large models is \"unremarkable,\" and my practice in multimodality is almost a \"total blank.\" This article is indeed just a \"subjective conjecture\" based on previous experiences in text and image generation. Problem Background First, let's simplify the problem. The \"multimodality\" discussed in this article pr"}];
