## SEARCH

## MENU

- [打赏](https://kexue.fm/reward.html)
- [公式](https://kexue.fm/latex.html)
- [天象](https://kexue.fm/ac.html)
- [链接](https://kexue.fm/links.html)
- [时光](https://kexue.fm/me.html)
- [博览](https://kexue.fm/science.html)
- [归档](https://kexue.fm/content.html)

## CATEGORIES

- [千奇百怪](https://kexue.fm/category/Everything)
- [天文探索](https://kexue.fm/category/Astronomy)
- [数学研究](https://kexue.fm/category/Mathematics)
- [物理化学](https://kexue.fm/category/Phy-chem)
- [信息时代](https://kexue.fm/category/Big-Data)
- [生物自然](https://kexue.fm/category/Biology)
- [图片摄影](https://kexue.fm/category/Photograph)
- [问题百科](https://kexue.fm/category/Questions)
- [生活/情感](https://kexue.fm/category/Life-Feeling)
- [资源共享](https://kexue.fm/category/Resources)

## NEWPOSTS

- [msign的导数](https://kexue.fm/archives/11025)
- [通过msign来计算mclip（奇...](https://kexue.fm/archives/11006)
- [msign算子的Newton-Sc...](https://kexue.fm/archives/10996)
- [等值振荡定理：最优多项式逼近的充要条件](https://kexue.fm/archives/10972)
- [生成扩散模型漫谈（三十）：从瞬时速...](https://kexue.fm/archives/10958)
- [MoE环游记：5、均匀分布的反思](https://kexue.fm/archives/10945)
- [msign算子的Newton-Sc...](https://kexue.fm/archives/10922)
- [Transformer升级之路：2...](https://kexue.fm/archives/10907)
- [一道概率不等式：盯着它到显然成立为止！](https://kexue.fm/archives/10902)
- [SVD的导数](https://kexue.fm/archives/10878)

## COMMENTS

- [苏剑林: 那就没问题了？就是$\\mcsgn$的相似不变性。](https://kexue.fm/archives/11025/comment-page-1#comment-27877)
- [苏剑林: 写漏了，是$f^∗\_t(x)$在 $x=0$ 处的斜率大于零，...](https://kexue.fm/archives/10996/comment-page-1#comment-27876)
- [苏剑林: 认真！感谢指出，更正过来了](https://kexue.fm/archives/4/comment-page-1#comment-27875)
- [苏剑林: 我不大了解JVP形式的ODE Solver，另外就是我不大确定...](https://kexue.fm/archives/10958/comment-page-1#comment-27874)
- [苏剑林: 谢谢，更正过来了。如果你说双时间参数，那么跟CTM和Short...](https://kexue.fm/archives/10958/comment-page-1#comment-27873)
- [苏剑林: 不少见了，还有一个叫Based的模型。不过简单看了看，这篇论文...](https://kexue.fm/archives/8601/comment-page-1#comment-27872)
- [苏剑林: 更正过来了，谢谢。](https://kexue.fm/archives/10352/comment-page-2#comment-27871)
- [tesslqy: 说的是绝对收敛，和非负没关系，从非负可测函数到构造负的可测函数...](https://kexue.fm/archives/4083/comment-page-1#comment-27870)
- [苏剑林: 我的测试结果是mha-128跟mla-192大致上持平。要说潜...](https://kexue.fm/archives/10907/comment-page-1#comment-27869)
- [xumingyu: 不好意思...我刚刚把csgn和msign看岔了.](https://kexue.fm/archives/11025/comment-page-1#comment-27868)

## USERLOGIN

- [登录](https://kexue.fm/admin/login.php)

[科学空间\|Scientific Spaces](https://kexue.fm)

- [登录](https://kexue.fm/admin/login.php)
- [打赏](https://kexue.fm/reward.html)
- [公式](https://kexue.fm/latex.html)
- [天象](https://kexue.fm/ac.html)
- [链接](https://kexue.fm/links.html)
- [时光](https://kexue.fm/me.html)
- [博览](https://kexue.fm/science.html)
- [归档](https://kexue.fm/content.html)

渴望成为一个小飞侠

- [欢迎订阅](https://kexue.fm/feed)
- [个性邮箱](https://kexue.fm/archives/119)
- [天象信息](https://kexue.fm/ac.html)
- [观测ISS](https://kexue.fm/archives/41)
- [LaTeX](https://kexue.fm/latex.html)
- [关于博主](https://kexue.fm/me.html)

欢迎访问“科学空间”，这里将与您共同探讨自然科学，回味人生百态；也期待大家的分享～

- [**千奇百怪** Everything](https://kexue.fm/category/Everything)
- [**天文探索** Astronomy](https://kexue.fm/category/Astronomy)
- [**数学研究** Mathematics](https://kexue.fm/category/Mathematics)
- [**物理化学** Phy-chem](https://kexue.fm/category/Phy-chem)
- [**信息时代** Big-Data](https://kexue.fm/category/Big-Data)
- [**生物自然** Biology](https://kexue.fm/category/Biology)
- [**图片摄影** Photograph](https://kexue.fm/category/Photograph)
- [**问题百科** Questions](https://kexue.fm/category/Questions)
- [**生活/情感** Life-Feeling](https://kexue.fm/category/Life-Feeling)
- [**资源共享** Resources](https://kexue.fm/category/Resources)

- [**千奇百怪**](https://kexue.fm/category/Everything)
- [**天文探索**](https://kexue.fm/category/Astronomy)
- [**数学研究**](https://kexue.fm/category/Mathematics)
- [**物理化学**](https://kexue.fm/category/Phy-chem)
- [**信息时代**](https://kexue.fm/category/Big-Data)
- [**生物自然**](https://kexue.fm/category/Biology)
- [**图片摄影**](https://kexue.fm/category/Photograph)
- [**问题百科**](https://kexue.fm/category/Questions)
- [**生活/情感**](https://kexue.fm/category/Life-Feeling)
- [**资源共享**](https://kexue.fm/category/Resources)

[首页](https://kexue.fm) [信息时代](https://kexue.fm/category/Big-Data) 线性Transformer应该不是你要等的那个模型

9Aug

# [线性Transformer应该不是你要等的那个模型](https://kexue.fm/archives/8610)

By 苏剑林 \|
2021-08-09 \|
129561位读者\|

在本博客中，我们已经多次讨论过线性Attention的相关内容。介绍线性Attention的逻辑大体上都是：标准Attention具有$\\mathcal{O}(n^2)$的平方复杂度，是其主要的“硬伤”之一，于是我们$\\mathcal{O}(n)$复杂度的改进模型，也就是线性Attention。有些读者看到线性Attention的介绍后，就一直很期待我们发布基于线性Attention的预训练模型，以缓解他们被BERT的算力消耗所折腾的“死去活来”之苦。

然而，本文要说的是：抱有这种念头的读者可能要失望了，标准Attention到线性Attention的转换应该远远达不到你的预期，而BERT那么慢的原因也并不是因为标准Attention的平方复杂度。

## BERT之反思 [\#](https://kexue.fm/archives/8610\#BERT%E4%B9%8B%E5%8F%8D%E6%80%9D)

按照直观理解，平方复杂度换成线性复杂度不应该要“突飞猛进”才对嘛？怎么反而“远远达不到预期”？出现这个疑惑的主要原因，是我们一直以来都没有仔细评估一下常规的Transformer模型（如BERT）的整体计算量。

很多读者都已经知道，Transformer的结构大体上是Embedding层加若干个Transformer层，Embedding层的计算量很少，我们主要关心Transformer层。忽略残差、Layer Normalization等计算量比较小的层不计，每个Transformer层主要组成就是两个子层：Self Attention（简称SA）和FeedForward Network（简称FFN）。虽然Transformer的开山之作声称“ [Attention is all you need](https://papers.cool/arxiv/1706.03762)”，但是也有不少工作论证了残差、FFN等模块的必要性了，比如 [《Attention is Not All You Need: Pure Attention Loses Rank Doubly Exponentially with Depth》](https://papers.cool/arxiv/2103.03404)。

现在问大家一个问题：

> 你觉得是SA计算量大还是FFN计算量大？

## 评估计算量 [\#](https://kexue.fm/archives/8610\#%E8%AF%84%E4%BC%B0%E8%AE%A1%E7%AE%97%E9%87%8F)

毋庸置疑，SA的复杂度是$\\mathcal{O}(n^2)$，而FFN的复杂度则是$\\mathcal{O}(n)$，如果你直接凭此就想当然地说SA计算量比FFN大，那就错了！

我们知道加法比乘法快很多，所以在估计计算量的时候我们主要计算要做多少次乘法，神经网络里边，主要的运算是矩阵相乘，不难估计按照定义一个$a\\times b$的矩阵乘以一个$b\\times c$的矩阵要做$abc$次乘法，所以$abc$就是两个矩阵相乘的复杂度了，这是我们估算Transformer复杂度的依据。

设$n$为序列长度，$d$为head\_size（base版是64），$h$为head的数目（base版是12），那么$hd$就是我们通常说的“hidden\_size”（base版是768）。对于SA来说，一开始是$Q,K,V$的投影变换，即$n\\times hd$的矩阵乘以$hd\\times hd$的矩阵做3次，因此计算量是$3n(hd)^2$；然后是$h$个Attention头的运算，每个头先是$n\\times d$的$Q$与$d\\times n$的$K^{\\top}$相乘得到$n\\times n$的Attention矩阵（softmax和归一化的计算量暂且忽略），然后$n\\times n$的矩阵与$n\\times d$的$V$相乘得到$n\\times d$的矩阵，这两步的计算量都是$n^2 d$，所以总计算量是$h(n^2 d + n^2 d)$；最后的输出还有一个投影变换，也是$n\\times hd$的矩阵乘以$hd\\times hd$的矩阵，计算量是$n(hd)^2$。所以，SA的总计算量是
\\begin{equation}3n(hd)^2 + h(n^2 d + n^2 d) + n(hd)^2 = 4nh^2 d^2 + 2n^2 hd\\end{equation}
至于FFN就比较简单了，它就是两个全连接层，也就是两个矩阵变换（激活函数的计算量也忽略不计），一般的参数设置是：第一层是$n\\times hd$的矩阵乘以$hd\\times 4hd$的矩阵，第二层就是$n\\times 4hd$的矩阵乘以$4hd\\times hd$的矩阵。所以总计算量是
\\begin{equation}n\\times hd\\times 4hd + n\\times 4hd\\times hd = 8nh^2 d^2\\end{equation}
这样一来，如果SA的计算量比FFN大，就意味着
\\begin{equation}4nh^2 d^2 + 2n^2 hd > 8nh^2 d^2\\quad\\Leftrightarrow\\quad n > 2hd\\end{equation}
对于base版来说，这意味着$n > 1536$！也就是说，只有当序列长度超过1536时，SA的计算量才大于FFN，在这之前，都是线性复杂度的FFN占主导！

这还不止，由上面的结果我们可以得到Transformer层总的计算量为
\\begin{equation}4nh^2 d^2 + 2n^2 hd + 8nh^2 d^2 = 12nh^2 d^2 + 2n^2 hd\\end{equation}
它是关于$n$的一次项和二次项的求和，当$n$足够大时，复杂度自然是$\\mathcal{O}(n^2)$，然而二次项占主导的条件是
\\begin{equation}2n^2 hd > 12nh^2 d^2\\quad\\Leftrightarrow\\quad n > 6hd\\end{equation}
对于base版来说，这意味着$n > 4608$！也就是说，当序列长度接近5000时，Transformer的复杂度才真正体现出二次性！

## 综合的结论 [\#](https://kexue.fm/archives/8610\#%E7%BB%BC%E5%90%88%E7%9A%84%E7%BB%93%E8%AE%BA)

综合上述结果，我们可以得到结论：对于base版来说，当序列长度不超过1536时，Transformer的复杂度都是近乎线性的；当序列长度超过1536时，Transformer的计算量逐渐以Attention为主，复杂度慢慢趋于二次方，直到长度超过4608，才真正以二次项为主。当然这个边界只是一个估计，实际情况可能有所偏差，大家就此感知一下范围和数量级就好。

笔者以前也建议过很多读者，对于不超过2000长度的“长文本”任务，直接用 [NEZHA](https://github.com/huawei-noah/Pretrained-Language-Model/tree/master/NEZHA-TensorFlow) 或者 [RoFormer](https://kexue.fm/archives/8265) 这种不限长度的模型试试，不要想太多的技巧，原因也是如此。你想再多技巧，也顶多是降到线性复杂度，而在这个长度范围内模型本身就是近乎线性的，各种技巧也省不了多少。

对于老老实实用BERT base的读者来说，maxlen一般不超过512，远低于上述界限，因此就不要再说Attention的平方复杂度费硬件之类的吐槽了，因为事实是：

> BERT之所以慢，主要是因为它真的大，而不是因为Attention的平方复杂度。

## “线性”含义 [\#](https://kexue.fm/archives/8610\#%E2%80%9C%E7%BA%BF%E6%80%A7%E2%80%9D%E5%90%AB%E4%B9%89)

至于对线性Attention“远远达不到预期”而感到疑惑的另一个原因，则是没有从实际情况分析线性Attention的计算量，以至于对线性Attention期待过高。

线性Attention的介绍可以参考 [《线性Attention的探索：Attention必须有个Softmax吗？》](https://kexue.fm/archives/7546)，这里不做重复。简单来说，线性Attention就是按照$Q(K^{\\top} V)$的顺序算注意力。所以按照前面的估算方法，线性Attention每个头运算的计算量就是$2nd^2$，而标准Attention则是$2n^2 d$，因此如果$n > d$，那么线性Attention是比标准Attention要省计算量的。（注：实现线性效率的Attention也不止这一种思路，但总的而言复杂度是相似的，因此下面的结论也有代表性。）

对于base版来说，那就是$n > 64$，这个界还是很容易达到的，所以有些读者可能会想“能省一点是一点”、“不用白不用”。然而，这是假设了标准Attention与线性Attention都用同一个$d$的前提下得出的结果。而认真琢磨过 [《Performer：用随机投影将Attention的复杂度线性化》](https://kexue.fm/archives/7921)、 [《Transformer升级之路：3、从Performer到线性Attention》](https://kexue.fm/archives/8338) 的读者都知道，线性Attention有着比标准Attention更严重的“低秩瓶颈”，所以如果切换为线性Attention后还用同一个$d$，那么线性Attention的效果将会明显下降，而如果要保留大致相同的效果，那么线性Attention要用更大的$d$（一般是原来的4倍左右）。

这样一来，线性Attention的计算量应该是$2n(4d)^2$，如果线性Attention要比标准Attention快，那么就要$n > 16d$，对于base版来说，就是$n > 1024$，这也超出了一般读者所能用到的范围了。况且换成线性Attention后，前面关于SA和FFN的计算量结论依然存在，即大部分序列长度下占主导计算量的还是FFN等线性运算，换了线性Attention后也无法感觉到明显的速度提升。所以，总的来说

> 你要不是成千上万的序列长度，就不要想着换线性Attention了。

## 再翻翻论文 [\#](https://kexue.fm/archives/8610\#%E5%86%8D%E7%BF%BB%E7%BF%BB%E8%AE%BA%E6%96%87)

事实上，就算不进行上述分析，只要认真读过关于Attention效率改进相关工作的读者，从论文中的某些图片就可以得到类似的结论：所谓更“高效”的Attention，一般都只适用于成千上万的序列长度，只有在这个场景下性能才有明显提升。

比如较早的工作 [Sparse Transformers](https://papers.cool/arxiv/1904.10509)，里边有一张图显示出处理的序列长度都是3000+的：

Sparse Transformer处理的长度都是3000+

比如大名鼎鼎的 [Reformer](https://papers.cool/arxiv/2001.04451)，演示性能的序列长度都是以K为单位的：

Reformer演示性能的序列长度都是以K为单位的

大家颇多好评的 [Longformer](https://papers.cool/arxiv/2004.05150) 也是如此：

Longformer演示性能的序列长度都是几千甚至上万

还有Google关于线性Attention的经典之作 [Performer](https://papers.cool/arxiv/2009.14794)，显示出哪怕序列长度是$2^{12}=4096$，Performer与Transformer的差距也不能说特别显著：

Performer的性能曲线

最后是比较新的工作 [Luna](https://papers.cool/arxiv/2106.01540)，提供了一个比较综合的对比表格，同样支持我们的结论：

Luna里边关于各个改进版Attention机制的性能对比

从已有的各个高效Attention的工作中，我们可以得出结论：这些改进工作所关心的序列长度主要都是以千为单位的，有明显计算效率提升的序列长度基本上都要好几千；当然，我们前面的讨论主要针对的还是时间复杂度，对于空间复杂度，也就是显存占用量，降低的幅度一般要比时间复杂度提升的幅度的要大，但总体而言都是长序列才有价值。

## 换个期待吧 [\#](https://kexue.fm/archives/8610\#%E6%8D%A2%E4%B8%AA%E6%9C%9F%E5%BE%85%E5%90%A7)

所以，如果你的序列长度还只是一两百，那么就完全不要期望Attention本身的改进了，老老实实换个小点的模型就好。你可以期望未来会有更小的模型能达到同样好的效果，但是不要期望同样大的模型通过修改Attention来提升效率，因为说白了，就算把Attention完全去掉，也提升不了多少性能。

_**转载到请包括本文地址：** [https://kexue.fm/archives/8610](https://kexue.fm/archives/8610)_

_**更详细的转载事宜请参考：**_ [《科学空间FAQ》](https://kexue.fm/archives/6508#%E6%96%87%E7%AB%A0%E5%A6%82%E4%BD%95%E8%BD%AC%E8%BD%BD/%E5%BC%95%E7%94%A8)

**如果您还有什么疑惑或建议，欢迎在下方评论区继续讨论。**

**如果您觉得本文还不错，欢迎 [分享](https://kexue.fm/archives/8610#share)/ [打赏](https://kexue.fm/archives/8610#pay) 本文。打赏并非要从中获得收益，而是希望知道科学空间获得了多少读者的真心关注。当然，如果你无视它，也不会影响你的阅读。再次表示欢迎和感谢！**

打赏

微信打赏

支付宝打赏

因为网站后台对打赏并无记录，因此欢迎在打赏时候备注留言。你还可以 [**点击这里**](http://mail.qq.com/cgi-bin/qm_share?t=qm_mailme&email=tN7d1drY3drrx8H0xcWa19vZ) 或在下方评论区留言来告知你的建议或需求。

**如果您需要引用本文，请参考：**

苏剑林. (Aug. 09, 2021). 《线性Transformer应该不是你要等的那个模型 》\[Blog post\]. Retrieved from [https://kexue.fm/archives/8610](https://kexue.fm/archives/8610)

@online{kexuefm-8610,
        title={线性Transformer应该不是你要等的那个模型},
        author={苏剑林},
        year={2021},
        month={Aug},
        url={\\url{https://kexue.fm/archives/8610}},
}

分类： [信息时代](https://kexue.fm/category/Big-Data)    标签： [模型](https://kexue.fm/tag/%E6%A8%A1%E5%9E%8B/), [矩阵](https://kexue.fm/tag/%E7%9F%A9%E9%98%B5/), [attention](https://kexue.fm/tag/attention/)[41 评论](https://kexue.fm/archives/8610#comments)

< [Transformer升级之路：5、作为无限维的线性Attention](https://kexue.fm/archives/8601) \| [浅谈Transformer的初始化、参数化与标准化](https://kexue.fm/archives/8620) >

### 你也许还对下面的内容感兴趣

- [msign的导数](https://kexue.fm/archives/11025)
- [通过msign来计算mclip（奇异值裁剪）](https://kexue.fm/archives/11006)
- [Transformer升级之路：20、MLA究竟好在哪里？](https://kexue.fm/archives/10907)
- [SVD的导数](https://kexue.fm/archives/10878)
- [Transformer升级之路：19、第二类旋转位置编码](https://kexue.fm/archives/10862)
- [矩阵的有效秩（Effective Rank）](https://kexue.fm/archives/10847)
- [Muon续集：为什么我们选择尝试Muon？](https://kexue.fm/archives/10739)
- [MoE环游记：1、从几何意义出发](https://kexue.fm/archives/10699)
- [细水长flow之TARFLOW：流模型满血归来？](https://kexue.fm/archives/10667)
- [低秩近似之路（五）：CUR](https://kexue.fm/archives/10662)

[发表你的看法](https://kexue.fm/archives/8610#comment_form)

1. [«](https://kexue.fm/archives/8610/comment-page-1#comments)
2. [1](https://kexue.fm/archives/8610/comment-page-1#comments)
3. [2](https://kexue.fm/archives/8610/comment-page-2#comments)

11层

July 7th, 2023

https://arxiv.org/abs/2307.02486 LongNet: Scaling Transformers to 1,000,000,000 Tokens
微软新出炉的作品, 把序列长度扩充到了10亿.

[回复评论](https://kexue.fm/archives/8610/comment-page-2?replyTo=22174#respond-post-8610)

winston m

October 22nd, 2023

请教大神，我目前的需求是处理生物数据，也是一个embedding序列，类似于nlp，序列长度差不多在1500左右正太分布，90%的序列长度小于3072,现在是用talking head attention + rope进行处理，不过在A100 80G上，超过1500左右的序列长度时batch size就变成2或者1了。

您的这篇文章主要进行的是运算时间的分析，但是我想的是，如果换作linear transformer可以省点gpu vram，让我的batch size可以变大一点吗？

或者说，从空间上分析，linear transformer有好一些吗

[回复评论](https://kexue.fm/archives/8610/comment-page-2?replyTo=22932#respond-post-8610)

[苏剑林](https://kexue.fm) 发表于
November 1st, 2023

短时间内，可能去掉talking head，换回标准attention，然后上flash attention收益最高～

[回复评论](https://kexue.fm/archives/8610/comment-page-2?replyTo=22969#respond-post-8610)

winston m 发表于
November 5th, 2023

感谢。。。

[回复评论](https://kexue.fm/archives/8610/comment-page-2?replyTo=23004#respond-post-8610)

[Gary Liu](http://github.com/ahuizxc) 发表于
April 14th, 2025

个人认为生物领域需要先提点而不是加速，涉及到DNA序列、癌症相关，准确才是人们需要的东西，当然不否定又快又好

[回复评论](https://kexue.fm/archives/8610/comment-page-2?replyTo=27383#respond-post-8610)

inse7en

December 19th, 2023

线性attention省得不是矩阵计算？是softmax mask？

[回复评论](https://kexue.fm/archives/8610/comment-page-2?replyTo=23315#respond-post-8610)

[苏剑林](https://kexue.fm) 发表于
December 19th, 2023

是省矩阵计算量

[回复评论](https://kexue.fm/archives/8610/comment-page-2?replyTo=23324#respond-post-8610)

Kiren

December 26th, 2023

受益匪浅！苏神如何看待今年清华在linear attention方面的新工作《FLatten Transformer: Vision Transformer using Focused Linear Attention》呢？

[回复评论](https://kexue.fm/archives/8610/comment-page-2?replyTo=23362#respond-post-8610)

[苏剑林](https://kexue.fm) 发表于
December 30th, 2023

CV的效果尚可，但NLP未知，而且不是做Causal的，没多大意思呀。

[回复评论](https://kexue.fm/archives/8610/comment-page-2?replyTo=23402#respond-post-8610)

yuanyuan zhang

May 22nd, 2024

“投影变换，即n×hd的矩阵乘以hd×hd的矩阵做3次”后面那个应该是hd\*d呀

[回复评论](https://kexue.fm/archives/8610/comment-page-2?replyTo=24400#respond-post-8610)

yuanyuan zhang 发表于
May 22nd, 2024

忽略

[回复评论](https://kexue.fm/archives/8610/comment-page-2?replyTo=24401#respond-post-8610)

1. [«](https://kexue.fm/archives/8610/comment-page-1#comments)
2. [1](https://kexue.fm/archives/8610/comment-page-1#comments)
3. [2](https://kexue.fm/archives/8610/comment-page-2#comments)

[取消回复](https://kexue.fm/archives/8610#respond-post-8610)

你的大名

电子邮箱

个人网站（选填）

1\. 可以使用LaTeX代码，点击“预览效果”可查看效果；2. 可以通过点击评论楼层编号来引用该楼层；3. 网站可能会有点卡，如非确认评论失败，请 **不要重复点击提交**。

### 内容速览

[BERT之反思](https://kexue.fm/archives/8610#BERT%E4%B9%8B%E5%8F%8D%E6%80%9D)
[评估计算量](https://kexue.fm/archives/8610#%E8%AF%84%E4%BC%B0%E8%AE%A1%E7%AE%97%E9%87%8F)
[综合的结论](https://kexue.fm/archives/8610#%E7%BB%BC%E5%90%88%E7%9A%84%E7%BB%93%E8%AE%BA)
[“线性”含义](https://kexue.fm/archives/8610#%E2%80%9C%E7%BA%BF%E6%80%A7%E2%80%9D%E5%90%AB%E4%B9%89)
[再翻翻论文](https://kexue.fm/archives/8610#%E5%86%8D%E7%BF%BB%E7%BF%BB%E8%AE%BA%E6%96%87)
[换个期待吧](https://kexue.fm/archives/8610#%E6%8D%A2%E4%B8%AA%E6%9C%9F%E5%BE%85%E5%90%A7)

### 智能搜索

支持整句搜索！网站自动使用 [结巴分词](https://github.com/fxsjy/jieba) 进行分词，并结合ngrams排序算法给出合理的搜索结果。

### 热门标签

[生成模型](https://kexue.fm/tag/%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B/) [attention](https://kexue.fm/tag/attention/) [优化](https://kexue.fm/tag/%E4%BC%98%E5%8C%96/) [语言模型](https://kexue.fm/tag/%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/) [模型](https://kexue.fm/tag/%E6%A8%A1%E5%9E%8B/) [网站](https://kexue.fm/tag/%E7%BD%91%E7%AB%99/) [概率](https://kexue.fm/tag/%E6%A6%82%E7%8E%87/) [梯度](https://kexue.fm/tag/%E6%A2%AF%E5%BA%A6/) [转载](https://kexue.fm/tag/%E8%BD%AC%E8%BD%BD/) [微分方程](https://kexue.fm/tag/%E5%BE%AE%E5%88%86%E6%96%B9%E7%A8%8B/) [矩阵](https://kexue.fm/tag/%E7%9F%A9%E9%98%B5/) [天象](https://kexue.fm/tag/%E5%A4%A9%E8%B1%A1/) [分析](https://kexue.fm/tag/%E5%88%86%E6%9E%90/) [深度学习](https://kexue.fm/tag/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/) [积分](https://kexue.fm/tag/%E7%A7%AF%E5%88%86/) [python](https://kexue.fm/tag/python/) [优化器](https://kexue.fm/tag/%E4%BC%98%E5%8C%96%E5%99%A8/) [力学](https://kexue.fm/tag/%E5%8A%9B%E5%AD%A6/) [无监督](https://kexue.fm/tag/%E6%97%A0%E7%9B%91%E7%9D%A3/) [扩散](https://kexue.fm/tag/%E6%89%A9%E6%95%A3/) [几何](https://kexue.fm/tag/%E5%87%A0%E4%BD%95/) [节日](https://kexue.fm/tag/%E8%8A%82%E6%97%A5/) [生活](https://kexue.fm/tag/%E7%94%9F%E6%B4%BB/) [文本生成](https://kexue.fm/tag/%E6%96%87%E6%9C%AC%E7%94%9F%E6%88%90/) [数论](https://kexue.fm/tag/%E6%95%B0%E8%AE%BA/)

### 随机文章

- [缓解交叉熵过度自信的一个简明方案](https://kexue.fm/archives/9526)
- [天文马拉松：观测国际空间站](https://kexue.fm/archives/14)
- [用复数化简二次曲线的尝试](https://kexue.fm/archives/1851)
- [《新理解矩阵1》：矩阵是什么？](https://kexue.fm/archives/1765)
- [太阳系是稳定的吗？](https://kexue.fm/archives/1115)
- [百度实体链接比赛后记：行为建模和实体链接](https://kexue.fm/archives/6919)
- [科学空间相册上线,与你分享科学图片](https://kexue.fm/archives/275)
- [【理解黎曼几何】6\. 曲率的计数与计算(Python)](https://kexue.fm/archives/4026)
- [AdaFactor优化器浅析（附开源实现）](https://kexue.fm/archives/7302)
- [脑洞大开：非线性RNN居然也可以并行计算？](https://kexue.fm/archives/9783)

### 最近评论

- [苏剑林](https://kexue.fm/archives/11025/comment-page-1#comment-27877): 那就没问题了？就是$\\mcsgn$的相似不变性。
- [苏剑林](https://kexue.fm/archives/10996/comment-page-1#comment-27876): 写漏了，是$f^∗\_t(x)$在 $x=0$ 处的斜率大于零，如果$l\_t$是最大值点，那么它...
- [苏剑林](https://kexue.fm/archives/4/comment-page-1#comment-27875): 认真！感谢指出，更正过来了
- [苏剑林](https://kexue.fm/archives/10958/comment-page-1#comment-27874): 我不大了解JVP形式的ODE Solver，另外就是我不大确定你这里的“似乎等价于”想要表达的...
- [苏剑林](https://kexue.fm/archives/10958/comment-page-1#comment-27873): 谢谢，更正过来了。如果你说双时间参数，那么跟CTM和Shortcut都相似，区别在于训练目标。
- [苏剑林](https://kexue.fm/archives/8601/comment-page-1#comment-27872): 不少见了，还有一个叫Based的模型。不过简单看了看，这篇论文的改动还是蛮少的，只能说大部份C...
- [苏剑林](https://kexue.fm/archives/10352/comment-page-2#comment-27871): 更正过来了，谢谢。
- [tesslqy](https://kexue.fm/archives/4083/comment-page-1#comment-27870): 说的是绝对收敛，和非负没关系，从非负可测函数到构造负的可测函数可以看看stein的经典四部曲（...
- [苏剑林](https://kexue.fm/archives/10907/comment-page-1#comment-27869): 我的测试结果是mha-128跟mla-192大致上持平。要说潜力可能是mha-128高点，但m...
- [xumingyu](https://kexue.fm/archives/11025/comment-page-1#comment-27868): 不好意思...我刚刚把csgn和msign看岔了.

### 友情链接

- [Cool Papers](https://papers.cool)
- [数学研发](https://bbs.emath.ac.cn)
- [Seatop](http://www.seatop.com.cn/)
- [Xiaoxia](https://xiaoxia.org/)
- [积分表-网络版](https://kexue.fm/sci/integral/index.html)
- [丝路博傲](http://blog.dvxj.com/)
- [ph4ntasy 饭特稀](http://www.ph4ntasy.com/)
- [数学之家](http://www.2math.cn/)
- [有趣天文奇观](http://interesting-sky.china-vo.org/)
- [TwistedW](http://www.twistedwg.com/)
- [godweiyang](https://godweiyang.com/)
- [AI柠檬](https://blog.ailemon.net/)
- [王登科-DK博客](https://greatdk.com)
- [ESON](https://blog.eson.org/)
- [枫之羽](https://fzhiy.net/)
- [Mathor's blog](https://wmathor.com/)
- [coding-zuo](https://coding-zuo.github.io/)
- [博科园](https://www.bokeyuan.net/)
- [孔皮皮的博客](https://www.kppkkp.top/)
- [运鹏的博客](https://yunpengtai.top/)
- [jiming.site](https://jiming.site/)
- [OmegaXYZ](https://www.omegaxyz.com/)
- [Blog by Eacls](https://www.eacls.top/)
- [EAI猩球](https://www.robotech.ink/)
- [文举的博客](https://liwenju0.com/)
- [用代码打点酱油](https://bruceyuan.com/)
- [申请链接](https://kexue.fm/links.html)

本站采用创作共用版权协议，要求署名、非商业用途和保持一致。转载本站内容必须也遵循“ [署名-非商业用途-保持一致](http://creativecommons.org/licenses/by-nc-nd/2.5/cn/)”的创作共用协议。
© 2009-2025 Scientific Spaces. All rights reserved. Theme by [laogui](http://www.laogui.com). Powered by [Typecho](http://typecho.org). 备案号: [粤ICP备09093259号-1/2](https://beian.miit.gov.cn/)。