## SEARCH

## MENU

- [打赏](https://kexue.fm/reward.html)
- [公式](https://kexue.fm/latex.html)
- [天象](https://kexue.fm/ac.html)
- [链接](https://kexue.fm/links.html)
- [时光](https://kexue.fm/me.html)
- [博览](https://kexue.fm/science.html)
- [归档](https://kexue.fm/content.html)

## CATEGORIES

- [千奇百怪](https://kexue.fm/category/Everything)
- [天文探索](https://kexue.fm/category/Astronomy)
- [数学研究](https://kexue.fm/category/Mathematics)
- [物理化学](https://kexue.fm/category/Phy-chem)
- [信息时代](https://kexue.fm/category/Big-Data)
- [生物自然](https://kexue.fm/category/Biology)
- [图片摄影](https://kexue.fm/category/Photograph)
- [问题百科](https://kexue.fm/category/Questions)
- [生活/情感](https://kexue.fm/category/Life-Feeling)
- [资源共享](https://kexue.fm/category/Resources)

## NEWPOSTS

- [生成扩散模型漫谈（三十一）：预测数...](https://kexue.fm/archives/11428)
- [Muon优化器指南：快速上手与关键细节](https://kexue.fm/archives/11416)
- [AdamW的Weight RMS的...](https://kexue.fm/archives/11404)
- [n个正态随机数的最大值的渐近估计](https://kexue.fm/archives/11390)
- [流形上的最速下降：5\. 对偶梯度下降](https://kexue.fm/archives/11388)
- [低精度Attention可能存在有...](https://kexue.fm/archives/11371)
- [MuP之上：1. 好模型的三个特征](https://kexue.fm/archives/11340)
- [随机矩阵的谱范数的快速估计](https://kexue.fm/archives/11335)
- [DiVeQ：一种非常简洁的VQ训练方案](https://kexue.fm/archives/11328)
- [为什么线性注意力要加Short C...](https://kexue.fm/archives/11320)

## COMMENTS

- [王勇顺: 2015年？开始的太早了吧！](https://kexue.fm/archives/3331/comment-page-2#comment-28937)
- [Mornmirror: 苏神你好，请教一个小问题我们内部在实验muon（代码参考了mo...](https://kexue.fm/archives/11416/comment-page-1#comment-28936)
- [Lu: 哥们太牛了](https://kexue.fm/archives/5253/comment-page-18#comment-28935)
- [qsh: 用muon的时候weights initialization有...](https://kexue.fm/archives/11416/comment-page-1#comment-28933)
- [夺宇: 苏老师的解释和推导好自然啊，比原论文更容易看懂](https://kexue.fm/archives/9152/comment-page-3#comment-28932)
- [夺宇: 苏老师，(17)式中的噪声项可以直接在(8)式中直接添加吗？(...](https://kexue.fm/archives/9119/comment-page-13#comment-28931)
- [Xiaozhi Zhu: 我觉得这个work摆脱了two stages，真正做到E2E，...](https://kexue.fm/archives/11428/comment-page-1#comment-28930)
- [wednesday: 想问问苏老师的数据挖掘学习思路或者学习路径是怎样的](https://kexue.fm/archives/3319/comment-page-1#comment-28929)
- [wednesday: 因为我们只对p(Y\|X)建模，因此$p\_{\\theta}(X)...](https://kexue.fm/archives/5239/comment-page-3#comment-28928)
- [ykwen: 不动点迭代的时候 有没有可能迭代到0附近呢？](https://kexue.fm/archives/10592/comment-page-3#comment-28927)

## USERLOGIN

- [登录](https://kexue.fm/admin/login.php)

[科学空间\|Scientific Spaces](https://kexue.fm)

- [登录](https://kexue.fm/admin/login.php)
- [打赏](https://kexue.fm/reward.html)
- [公式](https://kexue.fm/latex.html)
- [天象](https://kexue.fm/ac.html)
- [链接](https://kexue.fm/links.html)
- [时光](https://kexue.fm/me.html)
- [博览](https://kexue.fm/science.html)
- [归档](https://kexue.fm/content.html)

渴望成为一个小飞侠

- [欢迎订阅](https://kexue.fm/feed)
- [个性邮箱](https://kexue.fm/archives/119)
- [天象信息](https://kexue.fm/ac.html)
- [观测ISS](https://kexue.fm/archives/41)
- [LaTeX](https://kexue.fm/latex.html)
- [关于博主](https://kexue.fm/me.html)

欢迎访问“科学空间”，这里将与您共同探讨自然科学，回味人生百态；也期待大家的分享～

- [**千奇百怪** Everything](https://kexue.fm/category/Everything)
- [**天文探索** Astronomy](https://kexue.fm/category/Astronomy)
- [**数学研究** Mathematics](https://kexue.fm/category/Mathematics)
- [**物理化学** Phy-chem](https://kexue.fm/category/Phy-chem)
- [**信息时代** Big-Data](https://kexue.fm/category/Big-Data)
- [**生物自然** Biology](https://kexue.fm/category/Biology)
- [**图片摄影** Photograph](https://kexue.fm/category/Photograph)
- [**问题百科** Questions](https://kexue.fm/category/Questions)
- [**生活/情感** Life-Feeling](https://kexue.fm/category/Life-Feeling)
- [**资源共享** Resources](https://kexue.fm/category/Resources)

- [**千奇百怪**](https://kexue.fm/category/Everything)
- [**天文探索**](https://kexue.fm/category/Astronomy)
- [**数学研究**](https://kexue.fm/category/Mathematics)
- [**物理化学**](https://kexue.fm/category/Phy-chem)
- [**信息时代**](https://kexue.fm/category/Big-Data)
- [**生物自然**](https://kexue.fm/category/Biology)
- [**图片摄影**](https://kexue.fm/category/Photograph)
- [**问题百科**](https://kexue.fm/category/Questions)
- [**生活/情感**](https://kexue.fm/category/Life-Feeling)
- [**资源共享**](https://kexue.fm/category/Resources)

[首页](https://kexue.fm) [信息时代](https://kexue.fm/category/Big-Data) 也来盘点一些最近的非Transformer工作

24May

# [也来盘点一些最近的非Transformer工作](https://kexue.fm/archives/8431)

By 苏剑林 \|
2021-05-24 \|
82965位读者\|

大家最近应该多多少少都被各种MLP相关的工作“席卷眼球”了。以Google为主的多个研究机构“奇招频出”，试图从多个维度“打击”Transformer模型，其中势头最猛的就是号称是纯MLP的一系列模型了，让人似乎有种“MLP is all you need”时代到来的感觉。

这一顿顿让人眼花缭乱的操作背后，究竟是大道至简下的“返璞归真”，还是江郎才尽后的“冷饭重炒”？让我们也来跟着这股热潮，一起盘点一些最近的相关工作。

## 五月人倍忙 [\#](https://kexue.fm/kexue.fm\#%E4%BA%94%E6%9C%88%E4%BA%BA%E5%80%8D%E5%BF%99)

怪事天天有，五月特别多。这个月以来，各大机构似乎相约好了一样，各种非Transformer的工作纷纷亮相，仿佛“忽如一夜春风来，千树万树梨花开”。单就笔者在Arxiv上刷到的相关论文，就已经多达七篇（一个月还没过完，七篇方向极其一致的论文），涵盖了NLP和CV等多个任务，真的让人应接不暇：

> [《MLP-Mixer: An all-MLP Architecture for Vision》](https://papers.cool/arxiv/2105.01601) \- Google Research
>
> [《Beyond Self-attention: External Attention using Two Linear Layers for Visual Tasks》](https://papers.cool/arxiv/2105.02358) \- 清华大学
>
> [《Do You Even Need Attention? A Stack of Feed-Forward Layers Does Surprisingly Well on ImageNet》](https://papers.cool/arxiv/2105.02723) \- 牛津大学
>
> [《Are Pre-trained Convolutions Better than Pre-trained Transformers?》](https://papers.cool/arxiv/2105.03322) \- Google Research
>
> [《ResMLP: Feedforward networks for image classification with data-efficient training》](https://papers.cool/arxiv/2105.03404) \- Facebook AI
>
> [《FNet: Mixing Tokens with Fourier Transforms》](https://papers.cool/arxiv/2105.03824) \- Google Research
>
> [《Pay Attention to MLPs》](https://papers.cool/arxiv/2105.08050) \- Google Research

以上论文是按照出现在arixv上的时间排序的。可以看到主力军依旧是Google大佬。想当年一手促成了“Attention is all you need”趋势的也是Google，现在“重拳出击”Transformer的还是Google，Google大佬真可谓一直挖坑不断啊。

## 把酒话桑麻 [\#](https://kexue.fm/kexue.fm\#%E6%8A%8A%E9%85%92%E8%AF%9D%E6%A1%91%E9%BA%BB)

那么这系列工作究竟能带来什么启发呢？我们要不要赶紧跟上这系列工作呢？在这部分内容中，我们就来简要地梳理一下上述几篇论文，看看它们是何方神圣，是否有可能造成新一股模型潮流？

### Synthesizer [\#](https://kexue.fm/kexue.fm\#Synthesizer)

要解读上述MLP相关的工作，就不得不提到去年五月Google发表在 [《Synthesizer: Rethinking Self-Attention in Transformer Models》](https://papers.cool/arxiv/2005.00743) 的Synthesizer。而事实上，如果你已经了解了Synthesizer，那么上面列表中的好几篇论文都可以一笔带过了。

在之前的博客 [《Google新作Synthesizer：我们还不够了解自注意力》](https://kexue.fm/archives/7430) 中，我们已经对Synthesizer做了简单的解读。撇开缩放因子不说，那么Attention的运算可以分解为
\\begin{equation}\\boldsymbol{O}=\\boldsymbol{A}\\boldsymbol{V},\\quad \\boldsymbol{A}=softmax(\\boldsymbol{B}),\\quad \\boldsymbol{B}=\\boldsymbol{Q}\\boldsymbol{K}^{\\top}\\end{equation}
其中$\\boldsymbol{Q},\\boldsymbol{K},\\boldsymbol{V}$是输入序列的变换，这个了解Self Attention的读者应该都清楚，不再详写。Synthesizer则是对几种$\\boldsymbol{B}$的新算法做了实验，其中最让人深刻的一种名为Random，就是将整个$\\boldsymbol{B}$当作一个参数矩阵（随机初始化后更新或者不更新）。

Synthesizer的“预训练+微调”实验结果。实验的baseline是T5，其中“R”即为Random模式，相当于MLP。

在Random的情况下，Attention矩阵不再是随样本变化的了，也就是所有样本公用同一个Attention矩阵，但是它依然能取得不错的效果，这在当时确实对大家对Attention的固有理解造成了强烈冲击。Synthesizer的实验相当丰富，包括“机器翻译”、“自动摘要”、“对话生成”、“预训练+微调”等，可以说，上面列罗的多数论文，实验都没有Synthesizer丰富。

### MLP-Mixer [\#](https://kexue.fm/kexue.fm\#MLP-Mixer)

Synthesizer也许没想到，一年之后，它换了个名字，然后火起来了。

论文 [《MLP-Mixer: An all-MLP Architecture for Vision》](https://papers.cool/arxiv/2105.01601) 所提出来的MLP-Mxier，其实就是Synthesizer的Random模式并去掉了softmax激活，也就是说，它将$\\boldsymbol{B}$设为可训练的参数矩阵，然后直接让$\\boldsymbol{A}=\\boldsymbol{B}$。模型就这样已经介绍完了，除此之外的区别就是MLP-Mxier做CV任务而Synthesizer做NLP任务而已。

MLP-Mixer的部分实验结果

对了，为啥这模型叫MLP-Mxier呢，因为作者把这种直接可训练的Attention模式起了个名字叫做“token-mixing MLP”，把原来的FFN改叫做“channel-mixing MLP”（以前叫做Position-wise FC），不管叫啥，反正就是号称只是MLP，所以模型也叫做MLP-Mxier。

而事实上，笔者认为这个更标准的叫法是窗口为1的一维卷积，但不管是这篇论文还是之前的 [《Attention Is All You Need》](https://papers.cool/arxiv/1706.03762)，都是宁愿把这些常规操作自己另起个名字，也要选择性地减少甚至无视与卷积的联系，可谓是为了“A Good Title Is All You Need”而煞费苦心了。

其实这一点也遭到了LeCun的批评，如果真的是标准的MLP，那应该要将输入展平为一个一维向量，然后再接变换矩阵～

### External Attention [\#](https://kexue.fm/kexue.fm\#External%20Attention)

从类比的角度看，Synthesizer的Random模式或者MLP-Mxier，相当于将Attention中的$\\boldsymbol{Q}$和$\\boldsymbol{K}$都设为参数矩阵了，而 [《Beyond Self-attention: External Attention using Two Linear Layers for Visual Tasks》](https://papers.cool/arxiv/2105.02358) 所提出的External Attention，则是把$\\boldsymbol{K}$和$\\boldsymbol{V}$设为（固定大小的）参数矩阵了，实验任务同样是CV的。

本来这也没什么，毕竟深度学习就是效果为王，效果好了就能成文。但是个人认为External Attention很多说法就禁不住推敲的。

首先，它把自己称为“两个线性层”，刻意淡化它跟Attention的联系（说出它是Attention的特例很丢人？）；然后它又说“通过引入两个外部记忆单元（也就是设为参数的$\\boldsymbol{K}$和$\\boldsymbol{V}$），隐式地学习了整个数据集的特征”，这种说法也不能算错，然而其实任意模型的任意参数都可以这样解释，这并不是External Attention的特性；还有它说能实现线性的复杂度，那得固定$\\boldsymbol{K},\\boldsymbol{V}$的长度，这种情况下其实应该跟也同样是线性复杂的LinFormer比比才更有说服力（论文比了Performer，但是Performer的降低复杂度思路是不一样的，LinFormer更有可比性）。

抛开这些文字上的不说，External Attention的工作机制似乎有点迷。不难想到External Attention对每个特征的编码是孤立的，如果换到NLP来说，那就是说每个词都独立编码的，根本不与上下文产生联系，所以肯定是不work的，那为什么在CV中会work呢？

### Stack of FFN [\#](https://kexue.fm/kexue.fm\#Stack%20of%20FFN)

至于论文 [《Do You Even Need Attention? A Stack of Feed-Forward Layers Does Surprisingly Well on ImageNet》](https://papers.cool/arxiv/2105.02723)，其实跟MLP-Mixer是高度重合的，不过它写起来就实在多了。它就是将输入过一个常规的FFN，然后将输出转置，再过一个FFN，最后转置回来，这样如果本身就熟悉Transformer的话，我们很快就清楚它做了啥。

这篇论文本身就很短，一共只有4页，还包括1页代码和半页参考文献，正文其实就只有2.5页，更像是一个简报。也许作者本身也想在这个方面深挖一下，结果Google的MLP-Mixer先出来了，那么做下去也没意思了，遂草草完事发出。（这部分故事纯粹是笔者自己的猜测。）

### Pre-trained CNN [\#](https://kexue.fm/kexue.fm\#Pre-trained%20CNN)

事实上，CNN才是最早尝试（在Seq2Seq任务中）取代RNN的模型，Facebook的 [《Convolutional Sequence to Sequence Learning》](https://papers.cool/arxiv/1705.03122) 其实更早发表，只不过很快就被Google的 [《Attention Is All You Need》](https://papers.cool/arxiv/1706.03762) 抢了风头，后来GPT、BERT等模型发布之后，Transformer类模型就成了当前主流，CNN很少被深入研究了。

论文 [《Are Pre-trained Convolutions Better than Pre-trained Transformers?》](https://papers.cool/arxiv/2105.03322) 则帮助我们验证了“CNN+预训练”的有效性。论文结果显示，不管是直接用下游数据监督训练，还是先预训练然后微调，基于膨胀卷积或动态卷积的CNN模型都略优于Transformer模型，并且在速度上CNN模型还更加快。对了，这篇论文已经中了ACL2021，所以这篇论文的成文其实更早，只不过这个月才放出来而已。

不管有无预训练，CNN都体现出了自己的优势

这篇论文给我们的主要启发是：预训练改进与模型改进不应该混为一谈，预训练技术本身往往能给各种模型都带来提升，不应该一提到预训练就想到Transformer，也不应该只把预训练跟Transformer结合。事实上，笔者之前也比较喜欢CNN，曾通过“膨胀门卷积（ [DGCNN](https://kexue.fm/search/DGCNN/)）”的设计在多个任务上取得不错的效果，而这篇论文则再次肯定了CNN的价值。不过尽管如此，笔者可能依然不会投入主要精力转向CNN的研究。

首先，理论上来说，CNN就无法捕捉足够远的长程依赖，这是根本缺陷，虽然通过膨胀卷积等方式，可以快速增大CNN的感受野，但也只是比较大，不是Transformer理论上的一步到位；其次，如果单纯看提高效率角度，Transformer本身也有很多优化空间，如果只是为了执行效率而转向CNN，那这个理由似乎不那么有说服力；还有，Transformer的$\\mathcal{O}(n^2)$的复杂度本身也带来更多的折腾空间（比如像UniLM），可以玩出更多的花样（比如像K-BERT）。

总的来说，我们不能否定CNN的价值，但如果当前已经比较专注Transformer了，那么就没必要分出太多精力去转向CNN了。

### ResMLP [\#](https://kexue.fm/kexue.fm\#ResMLP)

至于Facebook在 [《ResMLP: Feedforward networks for image classification with data-efficient training》](https://papers.cool/arxiv/2105.03404) 提出的ResMLP，跟前述的MLP-Mixer和Stack of FFN也没有本质区别，其文字描述也跟Stack of FFN很相似，忽略细微的细节差异，甚至可以认为它们三个就是同一个模型。最后，ResMLP的实验任务同样是CV的。

### FNet [\#](https://kexue.fm/kexue.fm\#FNet)

就笔者看来， [《FNet: Mixing Tokens with Fourier Transforms》](https://papers.cool/arxiv/2105.03824) 所提出的FNet，是列表的七篇论文中最有意思的一篇。某种意义上来说，FNet也是MLP-Mixer的一个特例，但它是一个非常有意思的特例：MLP-Mixer的注意力矩阵是直接参数优化而来的，FNet的参数矩阵是直接通过傅里叶变换得到的！所以，FNet的“注意力层”是没有任何优化参数的！

其实我们也可以从注意力的角度来理解FNet。抛开归一化因子不看，那么注意力运算大致可以写为：
\\begin{equation}\\boldsymbol{O}=\\boldsymbol{A}\\boldsymbol{V},\\quad \\boldsymbol{A}=\\exp(\\boldsymbol{B}),\\quad \\boldsymbol{B}=\\boldsymbol{Q}\\boldsymbol{K}^{\\top}\\end{equation}
这里的$\\boldsymbol{Q},\\boldsymbol{K}$本来是$n\\times d$的矩阵，FNet说：$\\boldsymbol{Q},\\boldsymbol{K}$可以换成$n\\times 1$矩阵：
\\begin{equation}\\boldsymbol{Q}=\\boldsymbol{K}=\\begin{pmatrix}0 \\\ 1 \\\ 2 \\\ \\vdots \\\ n - 1\\end{pmatrix}\\end{equation}
是的，你没看错，它就是要将它粗暴地换成$0\\sim n-1$组成的$n\\times 1$矩阵。当然，这样一来越到后面$\\exp(\\boldsymbol{B})$就指数爆炸了。为了避免这种情况，FNet就改为
\\begin{equation}\\boldsymbol{A}=\\exp(\\text{i}\\boldsymbol{B})\\end{equation}
也就是搞成虚指数就不会爆炸了！就这么粗暴，这就得到了基于傅里叶变换的FNet。原论文对序列长度和特征维度两个方向都做了傅里叶变换，然后只保留实数部分，就用这个运算取代了自注意力。对于傅里叶变换的实现，我们有称之为“快速傅里叶变换（FFT）”的算法，效率是$\\mathcal{O}(n\\log n)$，所以FNet也能有效处理长序列。

FNet的部分效果如下表。其实从预训练和下游任务的效果上来看，FNet并没有什么优势，不过它在 [Long-Range Arena](https://papers.cool/arxiv/2011.04006)（一个测试模型长程能力的评测榜单）上的效果倒是不错。

FNet的“预训练+微调”实验结果

FNet的Long-Range Arena实验结果

当然，FNet这么粗暴的做法能行本来就已经是个奇迹了，它给我们带来的最大冲击无疑是：就这样都行？傅里叶变换为什么能行？笔者也不知道答案。网上有些评论说，这说明了注意力机制其实就是一种坐标基的变换，而傅里叶变换也是一种基的变换，两者的作用是类似的。这个说法确实有点本质的感觉，在ICLR2021中也有篇论文《Is Attention Better Than Matrix Decomposition?》用SVD代替Attention也能取得不错的效果，这说明基变换的说法确实存在（SVD也是一种基变换），但是基变换的同时如何保持时序性、哪种基变换更适合，这些问题完全没有头绪。

### gMLP / aMLP [\#](https://kexue.fm/kexue.fm\#gMLP%20/%20aMLP)

最后 [《Pay Attention to MLPs》](https://papers.cool/arxiv/2105.08050) 所给我们带来的gMLP、aMLP是比较常规的新结构探索工作，算是MLP-Mixer的增强版。gMLP的g是“gate”的意思，简单来说gMLP就是将MLP-Mixer跟门控机制结合起来，而aMLP的a是“attention”的意思，它将attention与gMLP结合起来。

具体来说，gMLP大致是如下运算：
\\begin{equation}\\begin{aligned}
&\[\\boldsymbol{X}\_1, \\boldsymbol{X}\_2\] = \\boldsymbol{X} \\\
&\\boldsymbol{Y} = \\boldsymbol{W}\\boldsymbol{X}\_2 + \\boldsymbol{b} \\\
&\\boldsymbol{O} = \\boldsymbol{X}\_1 \\otimes \\boldsymbol{Y}
\\end{aligned}\\end{equation}
简单来说，就是将输入沿着特征维度分为两半，然后将其中一半传入MLP-Mixer，作为另一半的gate。而aMLP则是将MLP-Mixer和一个简单的单头Self Attention结合来作为gate：
\\begin{equation}\\begin{aligned}
&\[\\boldsymbol{X}\_1, \\boldsymbol{X}\_2\] = \\boldsymbol{X} \\\
&\\boldsymbol{Y}\_1 = \\boldsymbol{W}\\boldsymbol{X}\_2 + \\boldsymbol{b} \\\
&\\boldsymbol{Y}\_2 = SelfAttention(\\boldsymbol{X}) \\\
&\\boldsymbol{O} = \\boldsymbol{X}\_1 \\otimes (\\boldsymbol{Y}\_1 + \\boldsymbol{Y}\_2)
\\end{aligned}\\end{equation}

论文做的实验比较全面，包括CV和NLP的。从论文所报告的效果来看，gMLP略差于标准的Self Attention，而aMLP则是普遍优于Self Attention，这进一步肯定了门控机制的价值。只不过不管是gMLP还是aMLP，人工堆砌的味道太重了，要水一篇paper还可以，但个人认为没有给模型的发展方向带来什么新的启发。

gMLP,aMLP的NLP部分实验结果

## 前路在何方 [\#](https://kexue.fm/kexue.fm\#%E5%89%8D%E8%B7%AF%E5%9C%A8%E4%BD%95%E6%96%B9)

通过以上阅读，我们可以知道，MLP-Mixer、Stack of FFN、ResMLP这三个模型，事实上可以看成是去年的Synthesizer的一个特例，甚至从技术上来说，它们还不如Synthesizer的内容丰富，因此真算不上什么有意思的工作；至于它的改进版gMLP / aMLP，则是非常常规的结构炼丹工作，只要算力足够我们都可以去做，所以也确实没什么意思；External Attention号称两个线性层，事实上就是Attention的变式，其生效机制和实验对比也不够明朗；比较有意思的就是CNN预训练和FNet这两个工作了，一个让我们解耦了“预训练改进”和“模型改进”两个概念，一个提出的傅里叶变换也有效给我们带来了较大的思想冲击。

整体而言，这些工作离成熟还远得很，最多是初步验证了有效性，连优雅也说不上。比如，除了FNet，这些所谓的“all in MLP”的模型，都没有办法比较优雅处理变长输入，像MLP-Mixer、Stack of FFN、ResMLP纯粹在（固定大小的）图像上实验，所以不用考虑这个问题，像Synthesizer / gMLP / aMLP虽然做了NLP的实验，但看上去都是强行截断的，算不上漂亮。所以，这系列工作一定程度上是开拓了新的思路，但其实带来了更多有待解答的问题。

那么我们要不要跟呢？个人认为没必要投入多少精力进去，平时大致关注一下就行了。抛开前面说的优雅性问题不说，这些工作的实用性本身就值得商榷。像将Attention换成MLP的改进，最大的优点无非就是提速，没错，是会快一点，但理论复杂度还是$\\mathcal{O}(n^2)$，这说明其实没有本质改进，况且提速的同时通常还会降低一点性能。如果单从“提速并降低一点性能”的追求来看，Transformer可做的工作也非常多（最直接的就是减少一两层），没必要换成MLP，而换成MLP探索自由度降低了不少。当然，从“拓荒”的学术角度来看，多角度尝试各种新模型是有意义的，但这也不宜掺入过多的人造因素在里边，不然就变成了一个在结构上过拟合任务的过程了，难登大雅之堂。

此外，对于NLP来说，我们可能比较关心的是“预训练+微调”这一块的性能，而很遗憾，从Synthesizer开始的一系列NLP实验表明，将Attention换成MLP后的模型也许在某个任务上能取得有竞争性的结果，但是其迁移性往往不好，也就是说可能单看预训练效果还不错，但是“预训练+微调”多数就比不上Transformer了。这也不难理解，因为它们把Attention矩阵参数化，那么该矩阵更有可能跟具体任务强相关了，不像Transformer那样自适应生成的Attention矩阵那样具有更好的适应能力。

## 曲终人散时 [\#](https://kexue.fm/kexue.fm\#%E6%9B%B2%E7%BB%88%E4%BA%BA%E6%95%A3%E6%97%B6)

本文盘点了最近的一些“非主流”工作，主要是通过以MLP为主的非Transformer结构来取代Transformer并获得了有竞争力的结果。总的来说，这些工作看起来形形色色，但都有迹可循，有“新瓶装旧酒”之感，能给人新启示的并不多。

全文仅乃笔者的闭门造车之言，仅代表笔者的个人观点，如有不当之处，还请读者海涵斧正。

_**转载到请包括本文地址：** [https://kexue.fm/archives/8431](https://kexue.fm/archives/8431)_

_**更详细的转载事宜请参考：**_ [《科学空间FAQ》](https://kexue.fm/archives/6508#%E6%96%87%E7%AB%A0%E5%A6%82%E4%BD%95%E8%BD%AC%E8%BD%BD/%E5%BC%95%E7%94%A8)

**如果您还有什么疑惑或建议，欢迎在下方评论区继续讨论。**

**如果您觉得本文还不错，欢迎 [分享](https://kexue.fm/kexue.fm#share)/ [打赏](https://kexue.fm/kexue.fm#pay) 本文。打赏并非要从中获得收益，而是希望知道科学空间获得了多少读者的真心关注。当然，如果你无视它，也不会影响你的阅读。再次表示欢迎和感谢！**

打赏

微信打赏

支付宝打赏

因为网站后台对打赏并无记录，因此欢迎在打赏时候备注留言。你还可以 [**点击这里**](http://mail.qq.com/cgi-bin/qm_share?t=qm_mailme&email=tN7d1drY3drrx8H0xcWa19vZ) 或在下方评论区留言来告知你的建议或需求。

**如果您需要引用本文，请参考：**

苏剑林. (May. 24, 2021). 《 也来盘点一些最近的非Transformer工作 》\[Blog post\]. Retrieved from [https://kexue.fm/archives/8431](https://kexue.fm/archives/8431)

@online{kexuefm-8431,
        title={ 也来盘点一些最近的非Transformer工作},
        author={苏剑林},
        year={2021},
        month={May},
        url={\\url{https://kexue.fm/archives/8431}},
}

分类： [信息时代](https://kexue.fm/category/Big-Data)    标签： [模型](https://kexue.fm/tag/%E6%A8%A1%E5%9E%8B/), [优化](https://kexue.fm/tag/%E4%BC%98%E5%8C%96/), [语言模型](https://kexue.fm/tag/%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/), [attention](https://kexue.fm/tag/attention/)[17 评论](https://kexue.fm/archives/8431#comments)

< [变分自编码器（七）：球面上的VAE（vMF-VAE）](https://kexue.fm/archives/8404) \| [我们可以无损放大一个Transformer模型吗（一）](https://kexue.fm/archives/8444) >

### 你也许还对下面的内容感兴趣

- [Muon优化器指南：快速上手与关键细节](https://kexue.fm/archives/11416)
- [低精度Attention可能存在有偏的舍入误差](https://kexue.fm/archives/11371)
- [MuP之上：1. 好模型的三个特征](https://kexue.fm/archives/11340)
- [为什么线性注意力要加Short Conv？](https://kexue.fm/archives/11320)
- [QK-Clip：让Muon在Scaleup之路上更进一步](https://kexue.fm/archives/11126)
- [Transformer升级之路：21、MLA好在哪里?（下）](https://kexue.fm/archives/11111)
- [“对角+低秩”三角阵的高效求逆方法](https://kexue.fm/archives/11072)
- [线性注意力简史：从模仿、创新到反哺](https://kexue.fm/archives/11033)
- [MoE环游记：5、均匀分布的反思](https://kexue.fm/archives/10945)
- [Transformer升级之路：20、MLA好在哪里?（上）](https://kexue.fm/archives/10907)

[发表你的看法](https://kexue.fm/kexue.fm#comment_form)

1. [«](https://kexue.fm/archives/8431/comment-page-1#comments)
2. [1](https://kexue.fm/archives/8431/comment-page-1#comments)
3. [2](https://kexue.fm/archives/8431/comment-page-2#comments)

gali

July 28th, 2021

能展开 FNet 的解释吗？原文中这样描述：

\> Essentially, we replace the self-attention sublayer of each Transformer encoder layer with a Fourier sublayer, which applies a 2D DFT to its (sequence length, hidden dimension) embedding input – one 1D DFT along
the sequence dimension, $F\_{seq}$, and one 1D DFT along the hidden dimension, $F\_h$:

$y = \\mathcal{R} (F\_{seq} (F\_h(x)))$

用本文中的 $Q, K, V, O$ notation，设 $F\_m \\in \\mathbb{R}^{m \\times m}$ 为对长度为 $m$ 序列的 1DFT的矩阵表示，Fnet 把 $O = attention(V, Q=V, K=V)$ 替换成了 $O = Real(F\_n V F\_d^T)$。这个并不能写成 $O = AV$的形式吧？

[回复评论](https://kexue.fm/archives/8431/comment-page-2?replyTo=17031#respond-post-8431)

[苏剑林](https://kexue.fm) 发表于
July 28th, 2021

先不看取实部这个操作，你的$F\_n$不就相当于$A$了么？所以形式不就是$AV$？

[回复评论](https://kexue.fm/archives/8431/comment-page-2?replyTo=17032#respond-post-8431)

gali 发表于
July 29th, 2021

$F\_d$怎么解释呢？不能移到 $V$ 前面变成 $AV$ 的形式。

$AV$ 本质上是对 V 的 column vector 做线性变换。
$VA$ 则是对 V 的 row vector 做线性变换。

按原文中的描述，这个要对 both column and row vector 做线性变化。我的理解是不能写成 $O=AV$ 的形式。

[回复评论](https://kexue.fm/archives/8431/comment-page-2?replyTo=17035#respond-post-8431)

[苏剑林](https://kexue.fm) 发表于
July 29th, 2021

那是你根本没好好了解Attention。当前主流的Attention机制，在$AV$之后都会对channel维度做一个全连接变换，也就是$AVW$的形式，只不过这一步太平凡了，一般不做特别讨论罢了。

[回复评论](https://kexue.fm/archives/8431/comment-page-2?replyTo=17036#respond-post-8431)

nanfangalan

August 29th, 2022

现在有更多MLP文章出来了，苏神有关注吗？

[回复评论](https://kexue.fm/archives/8431/comment-page-2?replyTo=19684#respond-post-8431)

[苏剑林](https://kexue.fm) 发表于
August 30th, 2022

不关注。

[回复评论](https://kexue.fm/archives/8431/comment-page-2?replyTo=19692#respond-post-8431)

1. [«](https://kexue.fm/archives/8431/comment-page-1#comments)
2. [1](https://kexue.fm/archives/8431/comment-page-1#comments)
3. [2](https://kexue.fm/archives/8431/comment-page-2#comments)

[取消回复](https://kexue.fm/archives/8431#respond-post-8431)

你的大名

电子邮箱

个人网站（选填）

1\. 可以使用LaTeX代码，点击“预览效果”可查看效果；2. 可以通过点击评论楼层编号来引用该楼层；3. 网站可能会有点卡，如非确认评论失败，请 **不要重复点击提交**。

### 内容速览

[五月人倍忙](https://kexue.fm/kexue.fm#%E4%BA%94%E6%9C%88%E4%BA%BA%E5%80%8D%E5%BF%99)
[把酒话桑麻](https://kexue.fm/kexue.fm#%E6%8A%8A%E9%85%92%E8%AF%9D%E6%A1%91%E9%BA%BB)
[Synthesizer](https://kexue.fm/kexue.fm#Synthesizer)
[MLP-Mixer](https://kexue.fm/kexue.fm#MLP-Mixer)
[External Attention](https://kexue.fm/kexue.fm#External%20Attention)
[Stack of FFN](https://kexue.fm/kexue.fm#Stack%20of%20FFN)
[Pre-trained CNN](https://kexue.fm/kexue.fm#Pre-trained%20CNN)
[ResMLP](https://kexue.fm/kexue.fm#ResMLP)
[FNet](https://kexue.fm/kexue.fm#FNet)
[gMLP / aMLP](https://kexue.fm/kexue.fm#gMLP%20/%20aMLP)
[前路在何方](https://kexue.fm/kexue.fm#%E5%89%8D%E8%B7%AF%E5%9C%A8%E4%BD%95%E6%96%B9)
[曲终人散时](https://kexue.fm/kexue.fm#%E6%9B%B2%E7%BB%88%E4%BA%BA%E6%95%A3%E6%97%B6)

### 智能搜索

支持整句搜索！网站自动使用 [结巴分词](https://github.com/fxsjy/jieba) 进行分词，并结合ngrams排序算法给出合理的搜索结果。

### 热门标签

[生成模型](https://kexue.fm/tag/%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B/) [attention](https://kexue.fm/tag/attention/) [优化](https://kexue.fm/tag/%E4%BC%98%E5%8C%96/) [语言模型](https://kexue.fm/tag/%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/) [模型](https://kexue.fm/tag/%E6%A8%A1%E5%9E%8B/) [网站](https://kexue.fm/tag/%E7%BD%91%E7%AB%99/) [概率](https://kexue.fm/tag/%E6%A6%82%E7%8E%87/) [梯度](https://kexue.fm/tag/%E6%A2%AF%E5%BA%A6/) [矩阵](https://kexue.fm/tag/%E7%9F%A9%E9%98%B5/) [转载](https://kexue.fm/tag/%E8%BD%AC%E8%BD%BD/) [优化器](https://kexue.fm/tag/%E4%BC%98%E5%8C%96%E5%99%A8/) [微分方程](https://kexue.fm/tag/%E5%BE%AE%E5%88%86%E6%96%B9%E7%A8%8B/) [分析](https://kexue.fm/tag/%E5%88%86%E6%9E%90/) [天象](https://kexue.fm/tag/%E5%A4%A9%E8%B1%A1/) [深度学习](https://kexue.fm/tag/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/) [积分](https://kexue.fm/tag/%E7%A7%AF%E5%88%86/) [python](https://kexue.fm/tag/python/) [扩散](https://kexue.fm/tag/%E6%89%A9%E6%95%A3/) [力学](https://kexue.fm/tag/%E5%8A%9B%E5%AD%A6/) [无监督](https://kexue.fm/tag/%E6%97%A0%E7%9B%91%E7%9D%A3/) [几何](https://kexue.fm/tag/%E5%87%A0%E4%BD%95/) [节日](https://kexue.fm/tag/%E8%8A%82%E6%97%A5/) [生活](https://kexue.fm/tag/%E7%94%9F%E6%B4%BB/) [文本生成](https://kexue.fm/tag/%E6%96%87%E6%9C%AC%E7%94%9F%E6%88%90/) [数论](https://kexue.fm/tag/%E6%95%B0%E8%AE%BA/)

### 随机文章

- [科学空间：2010年7月重要天象](https://kexue.fm/archives/704)
- [首次报名参加天文竞赛，期待中...](https://kexue.fm/archives/277)
- [【中文分词系列】 5\. 基于语言模型的无监督分词](https://kexue.fm/archives/3956)
- [让MathJax的数学公式随窗口大小自动缩放](https://kexue.fm/archives/10474)
- [从最大似然到EM算法：一致的理解方式](https://kexue.fm/archives/5239)
- [最小熵原理（一）：无监督学习的原理](https://kexue.fm/archives/5448)
- [果壳中的条件随机场(CRF In A Nutshell)](https://kexue.fm/archives/4695)
- [日出东方,重逢,最美的风采](https://kexue.fm/archives/815)
- [【语料】2500万中文三元组！](https://kexue.fm/archives/4359)
- [随机分词再探：从Viterbi Sampling到完美采样算法](https://kexue.fm/archives/9811)

### 最近评论

- [王勇顺](https://kexue.fm/archives/3331/comment-page-2#comment-28937): 2015年？开始的太早了吧！
- [Mornmirror](https://kexue.fm/archives/11416/comment-page-1#comment-28936): 苏神你好，请教一个小问题我们内部在实验muon（代码参考了moonlight的实现）的时候，做...
- [Lu](https://kexue.fm/archives/5253/comment-page-18#comment-28935): 哥们太牛了
- [qsh](https://kexue.fm/archives/11416/comment-page-1#comment-28933): 用muon的时候weights initialization有什么讲究吗？还是直接pytorc...
- [夺宇](https://kexue.fm/archives/9152/comment-page-3#comment-28932): 苏老师的解释和推导好自然啊，比原论文更容易看懂
- [夺宇](https://kexue.fm/archives/9119/comment-page-13#comment-28931): 苏老师，(17)式中的噪声项可以直接在(8)式中直接添加吗？(8)式中添加一个噪声项似乎对后续...
- [Xiaozhi Zhu](https://kexue.fm/archives/11428/comment-page-1#comment-28930): 我觉得这个work摆脱了two stages，真正做到E2E，让feature extract...
- [wednesday](https://kexue.fm/archives/3319/comment-page-1#comment-28929): 想问问苏老师的数据挖掘学习思路或者学习路径是怎样的
- [wednesday](https://kexue.fm/archives/5239/comment-page-3#comment-28928): 因为我们只对p(Y\|X)建模，因此$p\_{\\theta}(X)$我们认为就是$\\tilde{p...
- [ykwen](https://kexue.fm/archives/10592/comment-page-3#comment-28927): 不动点迭代的时候 有没有可能迭代到0附近呢？

### 友情链接

- [Cool Papers](https://papers.cool)
- [数学研发](https://bbs.emath.ac.cn)
- [Seatop](http://www.seatop.com.cn/)
- [Xiaoxia](https://xiaoxia.org/)
- [积分表-网络版](https://kexue.fm/sci/integral/index.html)
- [丝路博傲](http://blog.dvxj.com/)
- [数学之家](http://www.2math.cn/)
- [有趣天文奇观](http://interesting-sky.china-vo.org/)
- [TwistedW](http://www.twistedwg.com/)
- [godweiyang](https://godweiyang.com/)
- [AI柠檬](https://blog.ailemon.net/)
- [王登科-DK博客](https://greatdk.com)
- [ESON](https://blog.eson.org/)
- [枫之羽](https://fzhiy.net/)
- [Mathor's blog](https://wmathor.com/)
- [coding-zuo](https://coding-zuo.github.io/)
- [博科园](https://www.bokeyuan.net/)
- [孔皮皮的博客](https://www.kppkkp.top/)
- [运鹏的博客](https://yunpengtai.top/)
- [jiming.site](https://jiming.site/)
- [OmegaXYZ](https://www.omegaxyz.com/)
- [EAI猩球](https://www.robotech.ink/)
- [文举的博客](https://liwenju0.com/)
- [申请链接](https://kexue.fm/links.html)

本站采用创作共用版权协议，要求署名、非商业用途和保持一致。转载本站内容必须也遵循“ [署名-非商业用途-保持一致](http://creativecommons.org/licenses/by-nc-nd/2.5/cn/)”的创作共用协议。
© 2009-2025 Scientific Spaces. All rights reserved. Theme by [laogui](http://www.laogui.com). Powered by [Typecho](http://typecho.org). 备案号: [粤ICP备09093259号-1/2](https://beian.miit.gov.cn/)。