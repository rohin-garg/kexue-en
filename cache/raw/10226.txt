## SEARCH

## MENU

- [打赏](https://kexue.fm/reward.html)
- [公式](https://kexue.fm/latex.html)
- [天象](https://kexue.fm/ac.html)
- [链接](https://kexue.fm/links.html)
- [时光](https://kexue.fm/me.html)
- [博览](https://kexue.fm/science.html)
- [归档](https://kexue.fm/content.html)

## CATEGORIES

- [千奇百怪](https://kexue.fm/category/Everything)
- [天文探索](https://kexue.fm/category/Astronomy)
- [数学研究](https://kexue.fm/category/Mathematics)
- [物理化学](https://kexue.fm/category/Phy-chem)
- [信息时代](https://kexue.fm/category/Big-Data)
- [生物自然](https://kexue.fm/category/Biology)
- [图片摄影](https://kexue.fm/category/Photograph)
- [问题百科](https://kexue.fm/category/Questions)
- [生活/情感](https://kexue.fm/category/Life-Feeling)
- [资源共享](https://kexue.fm/category/Resources)

## NEWPOSTS

- [通过msign来计算mclip（奇...](https://kexue.fm/archives/11006)
- [msign算子的Newton-Sc...](https://kexue.fm/archives/10996)
- [等值振荡定理：最优多项式逼近的充要条件](https://kexue.fm/archives/10972)
- [生成扩散模型漫谈（三十）：从瞬时速...](https://kexue.fm/archives/10958)
- [MoE环游记：5、均匀分布的反思](https://kexue.fm/archives/10945)
- [msign算子的Newton-Sc...](https://kexue.fm/archives/10922)
- [Transformer升级之路：2...](https://kexue.fm/archives/10907)
- [一道概率不等式：盯着它到显然成立为止！](https://kexue.fm/archives/10902)
- [SVD的导数](https://kexue.fm/archives/10878)
- [智能家居之手搓一套能接入米家的零冷水装置](https://kexue.fm/archives/10869)

## COMMENTS

- [Chenguang Wang: 苏神好，我有个疑问，既然瞬时速度与时间t无关，那么对于样本$x...](https://kexue.fm/archives/10958/comment-page-1#comment-27821)
- [陈少龙: 苏神，您好！看了您的分享受益匪浅。但是我有个担心和疑问，您这里...](https://kexue.fm/archives/10907/comment-page-2#comment-27813)
- [PengchengMa: 牛啊](https://kexue.fm/archives/10996/comment-page-1#comment-27811)
- [xczh: 已使用mean flow policy，一步推理效果确实惊人，...](https://kexue.fm/archives/10958/comment-page-1#comment-27810)
- [Cosine: 是不是因为shared experts每次都激活，而route...](https://kexue.fm/archives/10945/comment-page-1#comment-27809)
- [rpsun: 这样似乎与传统的经验正交函数之类的有相似之处。把样本的平均值减...](https://kexue.fm/archives/10699/comment-page-1#comment-27808)
- [贵阳机场接机: 怎么不更新啦](https://kexue.fm/archives/1490/comment-page-1#comment-27807)
- [czvzb: 具身智能模型目前主流也是在使用扩散和流匹配这类方法来预测动作。...](https://kexue.fm/archives/10958/comment-page-1#comment-27806)
- [Shawn\_yang: 苏神，关于您所说的：“推理阶段可以事先预估Routed Exp...](https://kexue.fm/archives/10945/comment-page-1#comment-27802)
- [OceanYU: 您好，关于由式（7）推导出高斯分布，我这里有一点问题，式（7）...](https://kexue.fm/archives/9164/comment-page-4#comment-27801)

## USERLOGIN

- [登录](https://kexue.fm/admin/login.php)

[科学空间\|Scientific Spaces](https://kexue.fm)

- [登录](https://kexue.fm/admin/login.php)
- [打赏](https://kexue.fm/reward.html)
- [公式](https://kexue.fm/latex.html)
- [天象](https://kexue.fm/ac.html)
- [链接](https://kexue.fm/links.html)
- [时光](https://kexue.fm/me.html)
- [博览](https://kexue.fm/science.html)
- [归档](https://kexue.fm/content.html)

渴望成为一个小飞侠

- [欢迎订阅](https://kexue.fm/feed)
- [个性邮箱](https://kexue.fm/archives/119)
- [天象信息](https://kexue.fm/ac.html)
- [观测ISS](https://kexue.fm/archives/41)
- [LaTeX](https://kexue.fm/latex.html)
- [关于博主](https://kexue.fm/me.html)

欢迎访问“科学空间”，这里将与您共同探讨自然科学，回味人生百态；也期待大家的分享～

- [**千奇百怪** Everything](https://kexue.fm/category/Everything)
- [**天文探索** Astronomy](https://kexue.fm/category/Astronomy)
- [**数学研究** Mathematics](https://kexue.fm/category/Mathematics)
- [**物理化学** Phy-chem](https://kexue.fm/category/Phy-chem)
- [**信息时代** Big-Data](https://kexue.fm/category/Big-Data)
- [**生物自然** Biology](https://kexue.fm/category/Biology)
- [**图片摄影** Photograph](https://kexue.fm/category/Photograph)
- [**问题百科** Questions](https://kexue.fm/category/Questions)
- [**生活/情感** Life-Feeling](https://kexue.fm/category/Life-Feeling)
- [**资源共享** Resources](https://kexue.fm/category/Resources)

- [**千奇百怪**](https://kexue.fm/category/Everything)
- [**天文探索**](https://kexue.fm/category/Astronomy)
- [**数学研究**](https://kexue.fm/category/Mathematics)
- [**物理化学**](https://kexue.fm/category/Phy-chem)
- [**信息时代**](https://kexue.fm/category/Big-Data)
- [**生物自然**](https://kexue.fm/category/Biology)
- [**图片摄影**](https://kexue.fm/category/Photograph)
- [**问题百科**](https://kexue.fm/category/Questions)
- [**生活/情感**](https://kexue.fm/category/Life-Feeling)
- [**资源共享**](https://kexue.fm/category/Resources)

[首页](https://kexue.fm) [数学研究](https://kexue.fm/category/Mathematics) [信息时代](https://kexue.fm/category/Big-Data) 对齐全量微调！这是我看过最精彩的LoRA改进（一）

12Jul

# [对齐全量微调！这是我看过最精彩的LoRA改进（一）](https://kexue.fm/archives/10226)

By 苏剑林 \|
2024-07-12 \|
81155位读者\|

众所周知，LoRA是一种常见的参数高效的微调方法，我们在 [《梯度视角下的LoRA：简介、分析、猜测及推广》](https://kexue.fm/archives/9590) 做过简单介绍。LoRA利用低秩分解来降低微调参数量，节省微调显存，同时训练好的权重可以合并到原始权重上，推理架构不需要作出改变，是一种训练和推理都比较友好的微调方案。此外，我们在 [《配置不同的学习率，LoRA还能再涨一点？》](https://kexue.fm/archives/10001) 还讨论过LoRA的不对称性，指出给$A,B$设置不同的学习率能取得更好的效果，该结论被称为“LoRA+”。

为了进一步提升效果，研究人员还提出了不少其他LoRA变体，如 [AdaLoRA](https://papers.cool/arxiv/2303.10512)、 [rsLoRA](https://papers.cool/arxiv/2312.03732)、 [DoRA](https://papers.cool/arxiv/2402.09353)、 [PiSSA](https://papers.cool/arxiv/2404.02948) 等，这些改动都有一定道理，但没有特别让人深刻的地方觉。然而，前两天的 [《LoRA-GA: Low-Rank Adaptation with Gradient Approximation》](https://papers.cool/arxiv/2407.05000)，却让笔者眼前一亮，仅扫了摘要就有种必然有效的感觉，仔细阅读后更觉得它是至今最精彩的LoRA改进。

究竟怎么个精彩法？LoRA-GA的实际含金量如何？我们一起来学习一下。

## 基础回顾 [\#](https://kexue.fm/archives/10226\#%E5%9F%BA%E7%A1%80%E5%9B%9E%E9%A1%BE)

首先我们再来温习一下LoRA。假设预训练参数为$W\_0 \\in \\mathbb{R}^{n\\times m}$，那么全量微调时的更新量自然也是一个$n\\times m$矩阵，LoRA将更新量约束为低秩矩阵来降低训练时的参数量，即设$W=W\_0 + AB$，其中$A\\in\\mathbb{R}^{n\\times r},B\\in\\mathbb{R}^{r\\times m}$以及$r\\ll \\min(n,m)$，用新的$W$替换模型原参数，并固定$W\_0$不变，只训练$A,B$，如下图所示：
$$\\style{display: inline-block; width: 24ex; padding: 10ex 0; border: 1px solid #6C8EBF; background-color: #DAE8FC}{W\_0\\in\\mathbb{R}^{n\\times m}} \\quad + \\quad \\style{display: inline-block; width: 8ex; padding: 10ex 0; border: 1px solid #D79B00; background-color: #FFE6CC}{A\\in\\mathbb{R}^{n\\times r}}\\quad\\times\\quad \\style{display: inline-block; width: 24ex; padding: 3ex 0; border: 1px solid #D79B00; background-color: #FFE6CC}{B\\in\\mathbb{R}^{r\\times m}}$$

为了使得LoRA的初始状态跟预训练模型一致，我们通常会将$A,B$之一全零初始化，这样可以得到$A\_0 B\_0=0$，那么初始的$W$就是$W\_0$。但这并不是必须的，如果$A,B$都是非全零初始化，那么我们只需要将$W$设置为
\\begin{equation}W = (W\_0 - A\_0 B\_0) + AB\\end{equation}
也就是说将固定不变的权重从$W\_0$换为$W\_0 - A\_0 B\_0$，同样可以满足**初始$W$等于$W\_0$**这一条件。

需要指出的是，LoRA往往只是显存不足的无奈之选，因为一般情况下全量微调的效果都会优于LoRA，所以如果算力足够并且要追求效果最佳时，请优先选择全量微调。这也是LoRA-GA的假设之一，因为它的改进方向就是向全量微调对齐。使用LoRA的另一个场景是有大量的微型定制化需求，我们要存下非常多的微调结果，此时使用LoRA能减少储存成本。

## 对齐全量 [\#](https://kexue.fm/archives/10226\#%E5%AF%B9%E9%BD%90%E5%85%A8%E9%87%8F)

LoRA-GA提出了一个非常深刻的优化点：通过$W=(W\_0 - A\_0 B\_0) + AB$我们可以保证$W$的初始值等于$W\_0$，即初始状态的LoRA与全量微调是等价的，那么我们是否还可以调整$A\_0$和$B\_0$，使得LoRA和全量微调在后续训练中也尽可能近似？比如最简单地，让经过第一步优化后的$W\_1$尽可能相等？

越仔细回味，我们会越发现这个优化点是如此“直击本质”——LoRA的目标不就是“以小搏大”，希望能接近全量微调的效果吗？既然如此，尽可能对齐全量微调的后续更新结果，不就是最正确的改进方向？从逼近的角度来看，“$W$的初始值等于$W\_0$”相当于全量微调的零阶近似，保持后面的$W\_1,W\_2,\\cdots$接近，则相当于是更高阶的近似，是合情合理的选择，所以笔者看完摘要后就有种“就是它了”的强烈感觉。

具体来说，假设我们的优化器是SGD，那么对于全量微调，我们有
\\begin{equation} W\_1 = W\_0 - \\eta \\frac{\\partial \\mathcal{L}}{\\partial W\_0}\\end{equation}
其中$\\mathcal{L}$是损失函数，$\\eta$是学习率。如果是LoRA的话，那么有
\\begin{equation}\\begin{gathered}
A\_1 = A\_0 - \\eta \\frac{\\partial \\mathcal{L}}{\\partial A\_0} = A\_0 - \\eta \\frac{\\partial \\mathcal{L}}{\\partial W\_0} B\_0^{\\top},\\quad B\_1 = B\_0 - \\eta \\frac{\\partial \\mathcal{L}}{\\partial B\_0} = B\_0 - \\eta A\_0^{\\top}\\frac{\\partial \\mathcal{L}}{\\partial W\_0} \\\\[8pt\]
W\_1 = W\_0 - A\_0 B\_0 + A\_1 B\_1 \\approx W\_0 - \\eta\\left(A\_0 A\_0^{\\top}\\frac{\\partial \\mathcal{L}}{\\partial W\_0} + \\frac{\\partial \\mathcal{L}}{\\partial W\_0}B\_0^{\\top} B\_0\\right)
\\end{gathered}\\end{equation}
最后的近似省略了$\\eta$的二阶项。现在两个$W\_1$具有相似的形式，为了让它们尽可能近似，我们可以考虑最小化
\\begin{equation}\\mathop{\\text{argmin}}\_{A\_0,B\_0}\\left\\Vert A\_0 A\_0^{\\top}\\frac{\\partial \\mathcal{L}}{\\partial W\_0} + \\frac{\\partial \\mathcal{L}}{\\partial W\_0}B\_0^{\\top} B\_0 - \\frac{\\partial \\mathcal{L}}{\\partial W\_0}\\right\\Vert\_F^2 \\label{eq:loss-0}\\end{equation}
其中$\\Vert\\cdot\\Vert\_F^2$是矩阵的 [Frobenius范数](https://en.wikipedia.org/wiki/Matrix_norm#Frobenius_norm) 的平方，即矩阵每个元素的平方和。

## 求解过程 [\#](https://kexue.fm/archives/10226\#%E6%B1%82%E8%A7%A3%E8%BF%87%E7%A8%8B)

简单起见，我们记$G\_0=\\frac{\\partial \\mathcal{L}}{\\partial W\_0}$，那么目标$\\eqref{eq:loss-0}$可以简写成
\\begin{equation}\\mathop{\\text{argmin}}\_{A\_0,B\_0}\\left\\Vert A\_0 A\_0^{\\top}G\_0 + G\_0 B\_0^{\\top} B\_0 - G\_0\\right\\Vert\_F^2 \\label{eq:loss-1}\\end{equation}
注意$A\_0 A\_0^{\\top}G\_0$、$G\_0 B\_0^{\\top} B\_0$的秩顶多为$r$，它们相加后的秩顶多为$2r$，我们假设$2r < \\min(n,m)$，所以上述目标相当于寻找$G\_0$的一个秩不超过$2r$的最优近似。

我们先考虑$G\_0$是非负对角阵的情形，并且对角线元素已经按照从大到小的顺序排列。这个例子很简单，它的秩不超过$2r$的最优近似就是只保留对角线前$2r$个元素的新对角矩阵，这个结论叫做“ [Eckart-Young-Mirsky定理](https://en.wikipedia.org/wiki/Low-rank_approximation)”，而能让$A\_0 A\_0^{\\top}G\_0 + G\_0 B\_0^{\\top} B\_0$只保留$G\_0$的前$2r$个对角线元素的$A\_0,B\_0$可以是（分块矩阵）：
\\begin{equation}A\_0 = (I\_n)\_{\[:, :r\]}, \\quad B\_0 = (I\_m)\_{\[r:2r, :\]}\\end{equation}
其中$I\_n,I\_m$分别是$n,m$阶单位阵，${}\_{\[:, :r\]}$和${}\_{\[r:2r, :\]}$就是像Python切片那样，取前$r$列和第$r+1\\sim 2r$行。注意我们说的是“可以是”，也就是说解并不唯一，说白了就是要把$G\_0$的前$2r$个对角线元素挑出来，$A\_0 A\_0^{\\top}G\_0$和 $G\_0 B\_0^{\\top} B\_0$各挑一半，至于怎么分配就无所谓了。上面给出的解，对应的是$A\_0 A\_0^{\\top}G\_0$挑出前$r$个，$G\_0 B\_0^{\\top} B\_0$挑出第$r+1\\sim 2r$个。

当$G\_0$不是对角阵时，我们将它SVD为$U\\Sigma V$，其中$U\\in\\mathbb{R}^{n\\times n},V\\in\\mathbb{R}^{m\\times m}$为正交矩阵，$\\Sigma\\in\\mathbb{R}^{n\\times m}$为对角矩阵，对角线元素非负且从大到小排列。代入式$\\eqref{eq:loss-1}$后得到
\\begin{equation}\\begin{aligned}
&\\,\\left\\Vert A\_0 A\_0^{\\top}G\_0 + G\_0 B\_0^{\\top} B\_0 - G\_0\\right\\Vert\_F^2 \\\
=&\\, \\left\\Vert A\_0 A\_0^{\\top}U\\Sigma V + U\\Sigma V B\_0^{\\top} B\_0 - U\\Sigma V\\right\\Vert\_F^2 \\\
=&\\, \\left\\Vert U\\left\[(U^{\\top}A\_0) (U^{\\top}A\_0)^{\\top}\\Sigma + \\Sigma (B\_0 V^{\\top})^{\\top} (B\_0 V^{\\top}) - \\Sigma \\right\]V\\right\\Vert\_F^2 \\\
=&\\, \\left\\Vert (U^{\\top}A\_0) (U^{\\top}A\_0)^{\\top}\\Sigma + \\Sigma (B\_0 V^{\\top})^{\\top} (B\_0 V^{\\top}) - \\Sigma\\right\\Vert\_F^2 \\\
\\end{aligned}\\end{equation}
前两个等号都是简单的代换，第三个等号是因为正交变换不改变Frobenius范数（请读者自行证明一下）。经过这样的转换，我们发现逼近的对象重新转变为对角阵$\\Sigma$，自变量则变成了$U^{\\top}A\_0$、$B\_0 V^{\\top}$，那么按照$G\_0$是对角矩阵时所给出的解，我们得到
\\begin{equation}A\_0 = U(I\_n)\_{\[:, :r\]} = U\_{\[:, :r\]},\\quad B\_0 = (I\_m)\_{\[r:2r, :\]} V = V\_{\[r:2r, :\]}\\end{equation}

## 一般结果 [\#](https://kexue.fm/archives/10226\#%E4%B8%80%E8%88%AC%E7%BB%93%E6%9E%9C)

现在我们就得到了LoRA的一种初始化方法：

> **LoRA-GA** 选取一批样本，计算初始梯度$G\_0 = \\nabla\_{W\_0}\\mathcal{L}$，对梯度SVD为$G\_0 = U\\Sigma V$，取$U$的前$r$列初始化$A$，取$V$的第$r+1\\sim 2r$行初始化$B$。

这样LoRA + SGD得到的$W\_1$就跟全量微调的$W\_1$尽可能相近了。此外，梯度最重要的是方向，其模长不大重要，所以初始化结果我们还可以乘以个scale，LoRA本身也可以乘以个scale，即$W = (W\_0 - \\lambda A\_0 B\_0) + \\lambda AB$，这些都是LoRA常见的超参数，这里就不展开讨论了。顺便提一下，形式上跟LoRA-GA比较相似的是 [PiSSA](https://papers.cool/arxiv/2404.02948)，它是对$W\_0$做SVD来初始化$A,B$，这在理论支持上就不如LoRA-GA了，是一个纯粹的经验选择。

当然，可能有读者会发现目前的推导都是基于SGD优化器的假设，那么对于我们更常用的Adam优化器，结论是否要做出改变呢？理论上是要的。我们在 [《配置不同的学习率，LoRA还能再涨一点？》](https://kexue.fm/archives/10001) 讨论过，对于Adam来说，第一步优化结果是$W\_1 = W\_0 - \\eta\\, \\text{sign}(G\_0)$而不是$W\_1 = W\_0 - \\eta G\_0$，这样重复前面的推导，我们可以得到优化目标为
\\begin{equation}\\mathop{\\text{argmin}}\_{A\_0,B\_0}\\left\\Vert A\_0 \\text{sign}(A\_0^{\\top}G\_0) + \\text{sign}(G\_0 B\_0^{\\top}) B\_0 - \\text{sign}(G\_0)\\right\\Vert\_F^2 \\label{eq:loss-adam}\\end{equation}
由于符号函数$\\text{sign}$的存在，我们没法求出它的解析解，所以针对Adam的理论分析就只能止步于此了。

在这个背景下，对于Adam优化器，我们有三个选择：

> 1、 **信仰**：直接引用SGD的结果，相信它也可以在Adam中发挥同样的效果；
>
> 2、 **硬刚**：用优化器直接去最小化目标$\\eqref{eq:loss-adam}$，由于目标比较简单，计算量尚能接受；
>
> 3、 **投机**：直觉上将$G\_0$换成$\\text{sign}(G\_0)$，然后代入SGD的结论，可能更贴合Adam。

看起来原论文选择的是第1个方案，论文的实验结果确实也支持这一选择。

## 实验效果 [\#](https://kexue.fm/archives/10226\#%E5%AE%9E%E9%AA%8C%E6%95%88%E6%9E%9C)

论文的实验结果还是比较惊艳的，尤其是在GLUE上取得了最接近全量微调的效果：

LoRA-GA + T5-Base 在GLUE上的表现

平均来说，训练数据量越少，相对提升的幅度越大，这表明LoRA-GA对齐全量微调的策略，不仅有助于提高最终效果，还能提高训练效率，即可以用更少的训练步数就能达到更优的效果。

在LLAMA2-7b上的表现也可圈可点：

LoRA-GA + LLAMA2-7b 在几个Benchmark的表现

注意使用LoRA的主要场景是显存不足，但LoRA的初始化需要求出所有训练参数的完整梯度，这可能会由于显存不足而无法实现。为此，原论文提出的技巧是我们可以一个个参数串行地求梯度，而不是同时求所有训练参数的梯度，这样就可以把单步计算的显存降下来。串行求梯度虽然会降低效率，但初始化本身是一次性工作，因此稍慢点也无妨。至于怎么实现这个操作，不同框架有不同方法，这里也不展开讨论了。

## 文章小结 [\#](https://kexue.fm/archives/10226\#%E6%96%87%E7%AB%A0%E5%B0%8F%E7%BB%93)

本文介绍了LoRA的一个新改进LoRA-GA。虽然LoRA的各种变体并不鲜见，但LoRA-GA以非常直观的理论指导折服了笔者，其改进思路给人一种“确认过眼神，它就是对的论文”的感觉，再配上可圈可点的实验结果，整个过程如行云流水，让人赏心悦目。

_**转载到请包括本文地址：** [https://kexue.fm/archives/10226](https://kexue.fm/archives/10226)_

_**更详细的转载事宜请参考：**_ [《科学空间FAQ》](https://kexue.fm/archives/6508#%E6%96%87%E7%AB%A0%E5%A6%82%E4%BD%95%E8%BD%AC%E8%BD%BD/%E5%BC%95%E7%94%A8)

**如果您还有什么疑惑或建议，欢迎在下方评论区继续讨论。**

**如果您觉得本文还不错，欢迎 [分享](https://kexue.fm/archives/10226#share)/ [打赏](https://kexue.fm/archives/10226#pay) 本文。打赏并非要从中获得收益，而是希望知道科学空间获得了多少读者的真心关注。当然，如果你无视它，也不会影响你的阅读。再次表示欢迎和感谢！**

打赏

微信打赏

支付宝打赏

因为网站后台对打赏并无记录，因此欢迎在打赏时候备注留言。你还可以 [**点击这里**](http://mail.qq.com/cgi-bin/qm_share?t=qm_mailme&email=tN7d1drY3drrx8H0xcWa19vZ) 或在下方评论区留言来告知你的建议或需求。

**如果您需要引用本文，请参考：**

苏剑林. (Jul. 12, 2024). 《对齐全量微调！这是我看过最精彩的LoRA改进（一） 》\[Blog post\]. Retrieved from [https://kexue.fm/archives/10226](https://kexue.fm/archives/10226)

@online{kexuefm-10226,
        title={对齐全量微调！这是我看过最精彩的LoRA改进（一）},
        author={苏剑林},
        year={2024},
        month={Jul},
        url={\\url{https://kexue.fm/archives/10226}},
}

分类： [数学研究](https://kexue.fm/category/Mathematics), [信息时代](https://kexue.fm/category/Big-Data)    标签： [梯度](https://kexue.fm/tag/%E6%A2%AF%E5%BA%A6/), [优化器](https://kexue.fm/tag/%E4%BC%98%E5%8C%96%E5%99%A8/), [低秩](https://kexue.fm/tag/%E4%BD%8E%E7%A7%A9/), [lora](https://kexue.fm/tag/lora/)[38 评论](https://kexue.fm/archives/10226#comments)

< [“闭门造车”之多模态思路浅谈（二）：自回归](https://kexue.fm/archives/10197) \| [【生活杂记】用电饭锅来煮米汤](https://kexue.fm/archives/10240) >

### 你也许还对下面的内容感兴趣

- [msign算子的Newton-Schulz迭代（下）](https://kexue.fm/archives/10996)
- [msign算子的Newton-Schulz迭代（上）](https://kexue.fm/archives/10922)
- [SVD的导数](https://kexue.fm/archives/10878)
- [矩阵的有效秩（Effective Rank）](https://kexue.fm/archives/10847)
- [通过梯度近似寻找Normalization的替代品](https://kexue.fm/archives/10831)
- [MoE环游记：4、难处应当多投入](https://kexue.fm/archives/10815)
- [高阶muP：更简明但更高明的谱条件缩放](https://kexue.fm/archives/10795)
- [初探muP：超参数的跨模型尺度迁移规律](https://kexue.fm/archives/10770)
- [MoE环游记：3、换个思路来分配](https://kexue.fm/archives/10757)
- [Muon续集：为什么我们选择尝试Muon？](https://kexue.fm/archives/10739)

[发表你的看法](https://kexue.fm/archives/10226#comment_form)

1. [«](https://kexue.fm/archives/10226/comment-page-1#comments)
2. [1](https://kexue.fm/archives/10226/comment-page-1#comments)
3. [2](https://kexue.fm/archives/10226/comment-page-2#comments)

[Halo Master](https://jieyibu.net/)

August 12th, 2024

LoRA的初衷是为了在有限资源下实现高效微调,而LoRA-Pro却通过引入复杂的矩阵计算,大大增加了训练开销。

[回复评论](https://kexue.fm/archives/10226/comment-page-2?replyTo=25000#respond-post-10226)

[苏剑林](https://kexue.fm) 发表于
August 14th, 2024

LoRA-Pro相比全量微调的成本还是更低的，相比常规LoRA确实增加了不少。不过作者表示确实在想办法优化这一点。

[回复评论](https://kexue.fm/archives/10226/comment-page-2?replyTo=25023#respond-post-10226)

KLxz

August 26th, 2024

请问为什么 $\\mathbf{A}\_0 \\mathbf{A}\_0^\\top \\mathbf{G}\_0$的秩顶多为r 呀？

[回复评论](https://kexue.fm/archives/10226/comment-page-2?replyTo=25110#respond-post-10226)

KLxz

August 26th, 2024

[@KLxz\|comment-25110](https://kexue.fm/archives/10226/comment-page-2#comment-25110)

我知道了，线代基本忘光现在。。

[回复评论](https://kexue.fm/archives/10226/comment-page-2?replyTo=25111#respond-post-10226)

[Lora变式学习 R11; ConneRの博客](http://conner.rovn.ink/index.php/2024/09/06/lora/)

September 6th, 2024

\[...\]第三篇，对齐全量微调！这是我看过最精彩的LoRA改进（二）。\[...\]

[回复评论](https://kexue.fm/archives/10226/comment-page-2?replyTo=25153#respond-post-10226)

sqle

November 29th, 2024

苏神，我有点关于lora微调技术的想法，想听听你的看法。我觉得多次lora可以完全取代全参数微调，甚至直接取代全参数训练，下面链接是一点很简单的证明：

https://zhuanlan.zhihu.com/p/9760853103

目前还没什么条件进行实验验证，但我觉得理论上来说是可行的。

[回复评论](https://kexue.fm/archives/10226/comment-page-2?replyTo=25862#respond-post-10226)

[苏剑林](https://kexue.fm) 发表于
December 1st, 2024

full rank矩阵等于多个rank-1矩阵的叠加，这个结果只要对矩阵乘法熟悉一点，就是显然成立的。问题单凭这一点，不足以推出多次lora可以完全取代全参数微调，你得证明对于每个lora周期内，lora跟full finetune能力是对等的。

另外，lora省训练成本的最大原因，是它只选择了一部份的的参数进行更新，如果全体参数量都更新，那么lora能省的东西其实有限，所以即便“多次lora可以完全取代全参数微调“成立，意义也不算特别重大。

[回复评论](https://kexue.fm/archives/10226/comment-page-2?replyTo=25885#respond-post-10226)

sqle 发表于
December 3rd, 2024

嗯嗯，只是用在微调上，确实意义不是很重大，但如果可以用在训练上，感觉就提升很大了。而虽然全参数都会在一段时间后固定更新，但需要计算梯度的只有低秩矩阵，应该还是能省不少的吧？

[回复评论](https://kexue.fm/archives/10226/comment-page-2?replyTo=25894#respond-post-10226)

[苏剑林](https://kexue.fm) 发表于
December 10th, 2024

你推一下低秩矩阵的梯度是怎么算出来的（链式法则）就明白了。

[回复评论](https://kexue.fm/archives/10226/comment-page-2?replyTo=25936#respond-post-10226)

leopold1423

February 10th, 2025

复现了LoRA-GA，感觉性能提升很高是因为没有调基线学习率。

[回复评论](https://kexue.fm/archives/10226/comment-page-2?replyTo=26552#respond-post-10226)

1. [«](https://kexue.fm/archives/10226/comment-page-1#comments)
2. [1](https://kexue.fm/archives/10226/comment-page-1#comments)
3. [2](https://kexue.fm/archives/10226/comment-page-2#comments)

[取消回复](https://kexue.fm/archives/10226#respond-post-10226)

你的大名

电子邮箱

个人网站（选填）

1\. 可以使用LaTeX代码，点击“预览效果”可查看效果；2. 可以通过点击评论楼层编号来引用该楼层；3. 网站可能会有点卡，如非确认评论失败，请 **不要重复点击提交**。

### 内容速览

[基础回顾](https://kexue.fm/archives/10226#%E5%9F%BA%E7%A1%80%E5%9B%9E%E9%A1%BE)
[对齐全量](https://kexue.fm/archives/10226#%E5%AF%B9%E9%BD%90%E5%85%A8%E9%87%8F)
[求解过程](https://kexue.fm/archives/10226#%E6%B1%82%E8%A7%A3%E8%BF%87%E7%A8%8B)
[一般结果](https://kexue.fm/archives/10226#%E4%B8%80%E8%88%AC%E7%BB%93%E6%9E%9C)
[实验效果](https://kexue.fm/archives/10226#%E5%AE%9E%E9%AA%8C%E6%95%88%E6%9E%9C)
[文章小结](https://kexue.fm/archives/10226#%E6%96%87%E7%AB%A0%E5%B0%8F%E7%BB%93)

### 智能搜索

支持整句搜索！网站自动使用 [结巴分词](https://github.com/fxsjy/jieba) 进行分词，并结合ngrams排序算法给出合理的搜索结果。

### 热门标签

[生成模型](https://kexue.fm/tag/%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B/) [attention](https://kexue.fm/tag/attention/) [优化](https://kexue.fm/tag/%E4%BC%98%E5%8C%96/) [语言模型](https://kexue.fm/tag/%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/) [模型](https://kexue.fm/tag/%E6%A8%A1%E5%9E%8B/) [网站](https://kexue.fm/tag/%E7%BD%91%E7%AB%99/) [概率](https://kexue.fm/tag/%E6%A6%82%E7%8E%87/) [梯度](https://kexue.fm/tag/%E6%A2%AF%E5%BA%A6/) [转载](https://kexue.fm/tag/%E8%BD%AC%E8%BD%BD/) [微分方程](https://kexue.fm/tag/%E5%BE%AE%E5%88%86%E6%96%B9%E7%A8%8B/) [矩阵](https://kexue.fm/tag/%E7%9F%A9%E9%98%B5/) [天象](https://kexue.fm/tag/%E5%A4%A9%E8%B1%A1/) [分析](https://kexue.fm/tag/%E5%88%86%E6%9E%90/) [深度学习](https://kexue.fm/tag/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/) [积分](https://kexue.fm/tag/%E7%A7%AF%E5%88%86/) [python](https://kexue.fm/tag/python/) [优化器](https://kexue.fm/tag/%E4%BC%98%E5%8C%96%E5%99%A8/) [力学](https://kexue.fm/tag/%E5%8A%9B%E5%AD%A6/) [无监督](https://kexue.fm/tag/%E6%97%A0%E7%9B%91%E7%9D%A3/) [扩散](https://kexue.fm/tag/%E6%89%A9%E6%95%A3/) [几何](https://kexue.fm/tag/%E5%87%A0%E4%BD%95/) [节日](https://kexue.fm/tag/%E8%8A%82%E6%97%A5/) [生活](https://kexue.fm/tag/%E7%94%9F%E6%B4%BB/) [文本生成](https://kexue.fm/tag/%E6%96%87%E6%9C%AC%E7%94%9F%E6%88%90/) [数论](https://kexue.fm/tag/%E6%95%B0%E8%AE%BA/)

### 随机文章

- [最小熵原理（四）：“物以类聚”之从图书馆到词向量](https://kexue.fm/archives/6191)
- [大自然的隐身术——保护色](https://kexue.fm/archives/471)
- [CRF用过了，不妨再了解下更快的MEMM？](https://kexue.fm/archives/7213)
- [路径积分系列：4.随机微分方程](https://kexue.fm/archives/3762)
- [路径积分系列：3.路径积分](https://kexue.fm/archives/3757)
- [两百万素数之和与“电脑病”](https://kexue.fm/archives/2991)
- [【中文分词系列】 7\. 深度学习分词？只需一个词典！](https://kexue.fm/archives/4245)
- [“十字架”组合计数问题浅试](https://kexue.fm/archives/9291)
- [上学了，更新放缓......](https://kexue.fm/archives/125)
- [宇宙驿站定于本周五更换服务器](https://kexue.fm/archives/728)

### 最近评论

- [Chenguang Wang](https://kexue.fm/archives/10958/comment-page-1#comment-27821): 苏神好，我有个疑问，既然瞬时速度与时间t无关，那么对于样本$x\_0$和$x\_1$之间任意时刻的...
- [陈少龙](https://kexue.fm/archives/10907/comment-page-2#comment-27813): 苏神，您好！看了您的分享受益匪浅。但是我有个担心和疑问，您这里的每个实验表里不同对比组的los...
- [PengchengMa](https://kexue.fm/archives/10996/comment-page-1#comment-27811): 牛啊
- [xczh](https://kexue.fm/archives/10958/comment-page-1#comment-27810): 已使用mean flow policy，一步推理效果确实惊人，性能跟多步推理的diffusio...
- [Cosine](https://kexue.fm/archives/10945/comment-page-1#comment-27809): 是不是因为shared experts每次都激活，而routed experts是依概率被选中...
- [rpsun](https://kexue.fm/archives/10699/comment-page-1#comment-27808): 这样似乎与传统的经验正交函数之类的有相似之处。把样本的平均值减掉之后做正交分解。那么如果单纯地...
- [贵阳机场接机](https://kexue.fm/archives/1490/comment-page-1#comment-27807): 怎么不更新啦
- [czvzb](https://kexue.fm/archives/10958/comment-page-1#comment-27806): 具身智能模型目前主流也是在使用扩散和流匹配这类方法来预测动作。
苏神推荐你看这几篇文章：
1....
- [Shawn\_yang](https://kexue.fm/archives/10945/comment-page-1#comment-27802): 苏神，关于您所说的：“推理阶段可以事先预估Routed Expert的实际分布，只要细致地进行...
- [OceanYU](https://kexue.fm/archives/9164/comment-page-4#comment-27801): 您好，关于由式（7）推导出高斯分布，我这里有一点问题，式（7）只能保证关于x\_t-1是二次函数...

### 友情链接

- [Cool Papers](https://papers.cool)
- [数学研发](https://bbs.emath.ac.cn)
- [Seatop](http://www.seatop.com.cn/)
- [Xiaoxia](https://xiaoxia.org/)
- [积分表-网络版](https://kexue.fm/sci/integral/index.html)
- [丝路博傲](http://blog.dvxj.com/)
- [ph4ntasy 饭特稀](http://www.ph4ntasy.com/)
- [数学之家](http://www.2math.cn/)
- [有趣天文奇观](http://interesting-sky.china-vo.org/)
- [TwistedW](http://www.twistedwg.com/)
- [godweiyang](https://godweiyang.com/)
- [AI柠檬](https://blog.ailemon.net/)
- [王登科-DK博客](https://greatdk.com)
- [ESON](https://blog.eson.org/)
- [枫之羽](https://fzhiy.net/)
- [Mathor's blog](https://wmathor.com/)
- [coding-zuo](https://coding-zuo.github.io/)
- [博科园](https://www.bokeyuan.net/)
- [孔皮皮的博客](https://www.kppkkp.top/)
- [运鹏的博客](https://yunpengtai.top/)
- [jiming.site](https://jiming.site/)
- [OmegaXYZ](https://www.omegaxyz.com/)
- [Blog by Eacls](https://www.eacls.top/)
- [EAI猩球](https://www.robotech.ink/)
- [文举的博客](https://liwenju0.com/)
- [用代码打点酱油](https://bruceyuan.com/)
- [申请链接](https://kexue.fm/links.html)

本站采用创作共用版权协议，要求署名、非商业用途和保持一致。转载本站内容必须也遵循“ [署名-非商业用途-保持一致](http://creativecommons.org/licenses/by-nc-nd/2.5/cn/)”的创作共用协议。
© 2009-2025 Scientific Spaces. All rights reserved. Theme by [laogui](http://www.laogui.com). Powered by [Typecho](http://typecho.org). 备案号: [粤ICP备09093259号-1/2](https://beian.miit.gov.cn/)。