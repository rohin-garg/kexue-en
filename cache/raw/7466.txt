## SEARCH

## MENU

- [打赏](https://kexue.fm/reward.html)
- [公式](https://kexue.fm/latex.html)
- [天象](https://kexue.fm/ac.html)
- [链接](https://kexue.fm/links.html)
- [时光](https://kexue.fm/me.html)
- [博览](https://kexue.fm/science.html)
- [归档](https://kexue.fm/content.html)

## CATEGORIES

- [千奇百怪](https://kexue.fm/category/Everything)
- [天文探索](https://kexue.fm/category/Astronomy)
- [数学研究](https://kexue.fm/category/Mathematics)
- [物理化学](https://kexue.fm/category/Phy-chem)
- [信息时代](https://kexue.fm/category/Big-Data)
- [生物自然](https://kexue.fm/category/Biology)
- [图片摄影](https://kexue.fm/category/Photograph)
- [问题百科](https://kexue.fm/category/Questions)
- [生活/情感](https://kexue.fm/category/Life-Feeling)
- [资源共享](https://kexue.fm/category/Resources)

## NEWPOSTS

- [线性注意力简史：从模仿、创新到反哺](https://kexue.fm/archives/11033)
- [msign的导数](https://kexue.fm/archives/11025)
- [通过msign来计算mclip（奇...](https://kexue.fm/archives/11006)
- [msign算子的Newton-Sc...](https://kexue.fm/archives/10996)
- [等值振荡定理：最优多项式逼近的充要条件](https://kexue.fm/archives/10972)
- [生成扩散模型漫谈（三十）：从瞬时速...](https://kexue.fm/archives/10958)
- [MoE环游记：5、均匀分布的反思](https://kexue.fm/archives/10945)
- [msign算子的Newton-Sc...](https://kexue.fm/archives/10922)
- [Transformer升级之路：2...](https://kexue.fm/archives/10907)
- [一道概率不等式：盯着它到显然成立为止！](https://kexue.fm/archives/10902)

## COMMENTS

- [宋佳铭: 对，个人感觉mean flow就是continuous tim...](https://kexue.fm/archives/10958/comment-page-1#comment-27947)
- [宋佳铭: 的确，对sg这个事情我感觉如果是用‘归纳’法做是不太能避免的，...](https://kexue.fm/archives/10958/comment-page-1#comment-27946)
- [MoFHeka: 苏老师您好，请问一下这套结论在稀疏参数上应该如何应用？比如大规...](https://kexue.fm/archives/10542/comment-page-1#comment-27945)
- [苏剑林: Temp LoRA倒是有印象，其实思想是一样的，如果我单独开一...](https://kexue.fm/archives/11033/comment-page-1#comment-27944)
- [苏剑林: 你搜搜mamba、rwkv甚至rnn做vision的工作，其实...](https://kexue.fm/archives/11033/comment-page-1#comment-27943)
- [苏剑林: 问题1可以看看 https://kexue.fm/archiv...](https://kexue.fm/archives/9379/comment-page-1#comment-27942)
- [苏剑林: 你的“信息量”怎么定义？直观来说，reflow训练的是切线模型...](https://kexue.fm/archives/10958/comment-page-2#comment-27941)
- [苏剑林: 加大codebook也不行吗？主要是我也不大了解SDXL有什么...](https://kexue.fm/archives/10711/comment-page-2#comment-27940)
- [苏剑林: 如果考虑额外的scale，那情况肯定不一样了，甚至手工scal...](https://kexue.fm/archives/9009/comment-page-2#comment-27939)
- [苏剑林: 最后的问题，可能要监控一下每个参数的grad norm，看是哪...](https://kexue.fm/archives/10592/comment-page-2#comment-27938)

## USERLOGIN

- [登录](https://kexue.fm/admin/login.php)

[科学空间\|Scientific Spaces](https://kexue.fm)

- [登录](https://kexue.fm/admin/login.php)
- [打赏](https://kexue.fm/reward.html)
- [公式](https://kexue.fm/latex.html)
- [天象](https://kexue.fm/ac.html)
- [链接](https://kexue.fm/links.html)
- [时光](https://kexue.fm/me.html)
- [博览](https://kexue.fm/science.html)
- [归档](https://kexue.fm/content.html)

渴望成为一个小飞侠

- [欢迎订阅](https://kexue.fm/feed)
- [个性邮箱](https://kexue.fm/archives/119)
- [天象信息](https://kexue.fm/ac.html)
- [观测ISS](https://kexue.fm/archives/41)
- [LaTeX](https://kexue.fm/latex.html)
- [关于博主](https://kexue.fm/me.html)

欢迎访问“科学空间”，这里将与您共同探讨自然科学，回味人生百态；也期待大家的分享～

- [**千奇百怪** Everything](https://kexue.fm/category/Everything)
- [**天文探索** Astronomy](https://kexue.fm/category/Astronomy)
- [**数学研究** Mathematics](https://kexue.fm/category/Mathematics)
- [**物理化学** Phy-chem](https://kexue.fm/category/Phy-chem)
- [**信息时代** Big-Data](https://kexue.fm/category/Big-Data)
- [**生物自然** Biology](https://kexue.fm/category/Biology)
- [**图片摄影** Photograph](https://kexue.fm/category/Photograph)
- [**问题百科** Questions](https://kexue.fm/category/Questions)
- [**生活/情感** Life-Feeling](https://kexue.fm/category/Life-Feeling)
- [**资源共享** Resources](https://kexue.fm/category/Resources)

- [**千奇百怪**](https://kexue.fm/category/Everything)
- [**天文探索**](https://kexue.fm/category/Astronomy)
- [**数学研究**](https://kexue.fm/category/Mathematics)
- [**物理化学**](https://kexue.fm/category/Phy-chem)
- [**信息时代**](https://kexue.fm/category/Big-Data)
- [**生物自然**](https://kexue.fm/category/Biology)
- [**图片摄影**](https://kexue.fm/category/Photograph)
- [**问题百科**](https://kexue.fm/category/Questions)
- [**生活/情感**](https://kexue.fm/category/Life-Feeling)
- [**资源共享**](https://kexue.fm/category/Resources)

[首页](https://kexue.fm) [信息时代](https://kexue.fm/category/Big-Data) 泛化性乱弹：从随机噪声、梯度惩罚到虚拟对抗训练

1Jun

# [泛化性乱弹：从随机噪声、梯度惩罚到虚拟对抗训练](https://kexue.fm/archives/7466)

By 苏剑林 \|
2020-06-01 \|
116231位读者\|

提高模型的泛化性能是机器学习致力追求的目标之一。常见的提高泛化性的方法主要有两种：第一种是添加噪声，比如往输入添加高斯噪声、中间层增加Dropout以及进来比较热门的对抗训练等，对图像进行随机平移缩放等数据扩增手段某种意义上也属于此列；第二种是往loss里边添加正则项，比如$L\_1, L\_2$惩罚、梯度惩罚等。本文试图探索几种常见的提高泛化性能的手段的关联。

## 随机噪声 [\#](https://kexue.fm/archives/7466\#%E9%9A%8F%E6%9C%BA%E5%99%AA%E5%A3%B0)

我们记模型为$f(x)$，$\\mathcal{D}$为训练数据集合，$l(f(x), y)$为单个样本的loss，那么我们的优化目标是
\\begin{equation}\\mathop{\\text{argmin}}\_{\\theta} L(\\theta)=\\mathbb{E}\_{(x,y)\\sim \\mathcal{D}}\[l(f(x), y)\]\\end{equation}
$\\theta$是$f(x)$里边的可训练参数。假如往模型输入添加噪声$\\varepsilon$，其分布为$q(\\varepsilon)$，那么优化目标就变为
\\begin{equation}\\mathop{\\text{argmin}}\_{\\theta} L\_{\\varepsilon}(\\theta)=\\mathbb{E}\_{(x,y)\\sim \\mathcal{D}, \\varepsilon\\sim q(\\varepsilon)}\[l(f(x + \\varepsilon), y)\]\\end{equation}
当然，可以添加噪声的地方不仅仅是输入，也可以是中间层，也可以是权重$\\theta$，甚至可以是输出$y$（等价于标签平滑），噪声也不一定是加上去的，比如Dropout是乘上去的。对于加性噪声来说，$q(\\varepsilon)$的常见选择是均值为0、方差固定的高斯分布；而对于乘性噪声来说，常见选择是均匀分布$U(\[0,1\])$或者是伯努利分布。

添加随机噪声的目的很直观，就是希望模型能学会抵御一些随机扰动，从而降低对输入或者参数的敏感性，而降低了这种敏感性，通常意味着所得到的模型不再那么依赖训练集，所以有助于提高模型泛化性能。

## 提高效率 [\#](https://kexue.fm/archives/7466\#%E6%8F%90%E9%AB%98%E6%95%88%E7%8E%87)

添加随机噪声的方式容易实现，而且在不少情况下确实也很有效，但它有一个明显的缺点：不够“特异性”。噪声$\\varepsilon$是随机的，而不是针对$x$构建的，这意味着多数情况下$x + \\varepsilon$可能只是一个平凡样本，也就是没有对原模型造成比较明显的扰动，所以对泛化性能的提高帮助有限。

### 增加采样 [\#](https://kexue.fm/archives/7466\#%E5%A2%9E%E5%8A%A0%E9%87%87%E6%A0%B7)

从理论上来看，加入随机噪声后，单个样本的loss变为
\\begin{equation}\\tilde{l}(x,y)=\\mathbb{E}\_{\\varepsilon\\sim q(\\varepsilon)}\[l(f(x+\\varepsilon),y)\]=\\int q(\\varepsilon) l(f(x+\\varepsilon),y) d\\varepsilon\\label{eq:noisy-loss}\\end{equation}
但实践上，对于每个特定的样本$(x,y)$，我们一般只采样一个噪声，所以并没有很好地近似上式。当然，我们可以采样多个噪声$\\varepsilon\_1,\\varepsilon\_2,\\cdots,\\varepsilon\_k\\sim q(\\varepsilon)$，然后更好地近似
\\begin{equation}\\tilde{l}(x,y)\\approx \\frac{1}{k}\\sum\_{i=1}^k l(f(x+\\varepsilon\_i),y)\\end{equation}
但这样相当于batch\_size扩大为原来的$k$倍，增大了计算成本，并不是那么友好。

### 近似展开 [\#](https://kexue.fm/archives/7466\#%E8%BF%91%E4%BC%BC%E5%B1%95%E5%BC%80)

一个直接的想法是，如果能事先把式$\\eqref{eq:noisy-loss}$中的积分算出来，那就用不着低效率地采样了（或者相当于一次性采样无限多的噪声）。我们就往这个方向走一下试试。当然，精确的显式积分基本上是做不到的，我们可以做一下近似展开：
\\begin{equation}l(f(x+\\varepsilon),y)\\approx l(f(x),y)+(\\varepsilon \\cdot \\nabla\_x) l(f(x),y)+\\frac{1}{2}(\\varepsilon \\cdot \\nabla\_x)^2 l(f(x),y)\\end{equation}
然后两端乘以$q(\\varepsilon)$积分，这里假设$\\varepsilon$的各个分量是独立同分布的，并且均值为0、方差为$\\sigma^2$，那么积分结果就是
\\begin{equation}\\int q(\\varepsilon)l(f(x+\\varepsilon),y)d\\varepsilon \\approx l(f(x),y)+\\frac{1}{2}\\sigma^2 \\Delta l(f(x),y)\\end{equation}
这里的$\\Delta$是拉普拉斯算子，即$\\Delta f = \\sum\\limits\_i \\frac{\\partial^2}{\\partial x\_i^2} f$。这个结果在形式上很简单，就是相当于往loss里边加入正则项$\\frac{1}{2}\\sigma^2 \\Delta l(f(x),y)$，然而实践上却相当困难，因为这意味着要算$l$的二阶导数，再加上梯度下降，那么就一共要算三阶导数，这是现有深度学习框架难以高效实现的。

## 转移目标 [\#](https://kexue.fm/archives/7466\#%E8%BD%AC%E7%A7%BB%E7%9B%AE%E6%A0%87)

直接化简$l(f(x+\\varepsilon),y)$的积分是行不通了，但我们还可以试试将优化目标换成
\\begin{equation}l(f(x+\\varepsilon),f(x)) + l(f(x),y)\\label{eq:loss-2}\\end{equation}
也就是变成同时缩小$f(x),y$、$f(x+\\varepsilon),f(x)$的差距，两者双管齐下，一定程度上也能达到缩小$f(x+\\varepsilon),y$差距的目标。关键的是，这个目标能得到更有意思的结果。

### 思路解析 [\#](https://kexue.fm/archives/7466\#%E6%80%9D%E8%B7%AF%E8%A7%A3%E6%9E%90)

用数学的话来讲，如果$l$是某种形式的距离度量，那么根据三角不等式就有
\\begin{equation}l(f(x+\\varepsilon),y) \\leq l(f(x+\\varepsilon),f(x)) + l(f(x),y)\\end{equation}
如果$l$不是度量，那么通常根据詹森不等式也能得到一个类似的结果，比如$l(f(x+\\varepsilon),y)=\\Vert f(x+\\varepsilon) - y\\Vert^2$，那么我们有
\\begin{equation}\\begin{aligned}
\\Vert f(x+\\varepsilon) - f(x) + f(x) - y\\Vert^2 =& \\left\\Vert \\frac{1}{2}\\times 2\[f(x+\\varepsilon) - f(x)\] + \\frac{1}{2}\\times 2\[f(x) - y\]\\right\\Vert^2\\\
\\leq& \\frac{1}{2} \\Vert 2\[f(x+\\varepsilon) - f(x)\]\\Vert^2 + \\frac{1}{2} \\Vert 2\[f(x) - y\]\\Vert^2\\\
=& 2\\big(\\Vert f(x+\\varepsilon) - f(x)\\Vert^2 + \\Vert f(x) - y\\Vert^2\\big)
\\end{aligned}\\end{equation}
这也就是说，目标$\\eqref{eq:loss-2}$（的若干倍）可以认为是$l(f(x+\\varepsilon),y)$的上界，原始目标不大好优化，所以我们改为优化它的上界。

注意到，目标$\\eqref{eq:loss-2}$的两项之中，$l(f(x+\\varepsilon),f(x))$衡量了模型本身的平滑程度，跟标签没关系，用无标签数据也可以对它进行优化，这意味着它可以跟带标签的数据一起，构成一个 **半监督学习** 流程。

### 勇敢地算 [\#](https://kexue.fm/archives/7466\#%E5%8B%87%E6%95%A2%E5%9C%B0%E7%AE%97)

对于目标$\\eqref{eq:loss-2}$来说，它的积分结果是：
\\begin{equation}\\int q(\\varepsilon) \\big\[l(f(x+\\varepsilon),f(x)) + l(f(x),y)\\big\]d\\varepsilon = l(f(x),y) + \\int q(\\varepsilon) l(f(x+\\varepsilon),f(x)) d\\varepsilon\\end{equation}
还是老路子，近似展开$\\varepsilon$：
\\begin{equation}\\begin{aligned}l(f(x+\\varepsilon),f(x))\\approx &\\, l(f(x),f(x)) + \\left.\\sum\_{i,j} \\frac{\\partial l(F(x),f(x))}{\\partial F\_i(x)}\\frac{\\partial f\_i(x)}{\\partial x\_j}\\varepsilon\_j\\right\|\_{F(x)=f(x)}\\\
&\\, + \\frac{1}{2}\\left.\\sum\_{i,j,k} \\frac{\\partial l(F(x),f(x))}{\\partial F\_i(x)}\\frac{\\partial^2 f\_i(x)}{\\partial x\_j \\partial x\_k}\\varepsilon\_j \\varepsilon\_k\\right\|\_{F(x)=f(x)}\\\
&\\, + \\frac{1}{2}\\left.\\sum\_{i,i',j,k} \\frac{\\partial^2 l(F(x),f(x))}{\\partial F\_i(x) \\partial F\_{i'}(x)}\\frac{\\partial f\_i(x)}{\\partial x\_j}\\frac{\\partial f\_{i'}(x)}{\\partial x\_k}\\varepsilon\_j \\varepsilon\_k\\right\|\_{F(x)=f(x)}
\\end{aligned}\\label{eq:kongbu}\\end{equation}
很恐怖？不着急，我们回顾一下，作为loss函数的$l$，它一般会有如下几个特点：

> 1、$l$是光滑的；
>
> 2、$l(x, x)=0$；
>
> 3、$\\left.\\frac{\\partial}{\\partial x} l(x,y)\\right\|\_{x=y}=0,\\left.\\frac{\\partial}{\\partial y} l(x,y)\\right\|\_{y=x}=0$。

这其实就是说$l$是光滑的，并且在$x=y$的时候取到极（小）值，且极（小）值为0，这几个特点几乎是所有loss的共性了。基于这几个特点，恐怖的$\\eqref{eq:kongbu}$式的前三项就直接为0了，所以最后的积分结果是：
\\begin{equation}\\int q(\\varepsilon) l(f(x+\\varepsilon),f(x)) d\\varepsilon \\approx \\frac{1}{2}\\sigma^2\\left.\\sum\_{i,i',j} \\frac{\\partial^2 l(F(x),f(x))}{\\partial F\_i(x) \\partial F\_{i'}(x)}\\frac{\\partial f\_i(x)}{\\partial x\_j}\\frac{\\partial f\_{i'}(x)}{\\partial x\_j}\\right\|\_{F(x)=f(x)}
\\end{equation}

### 梯度惩罚 [\#](https://kexue.fm/archives/7466\#%E6%A2%AF%E5%BA%A6%E6%83%A9%E7%BD%9A)

看上去依然让人有些心悸，但总比$\\eqref{eq:kongbu}$好多了。上式也是一个正则项，其特点是只包含一阶梯度项，而对于特定的损失函数，$\\left.\\frac{\\partial^2 l(F(x),f(x))}{\\partial F\_i(x) \\partial F\_{i'}(x)}\\right\|\_{F(x)=f(x)}$可以提前算出来，特别地，对于常见的几个损失函数，当$i\\neq i'$时$\\left.\\frac{\\partial^2 l(F(x),f(x))}{\\partial F\_i(x) \\partial F\_{i'}(x)}\\right\|\_{F(x)=f(x)}=0$，所以仅需计算$i=i'$的分量，我们记它为$\\lambda\_{i}(x)$，那么
\\begin{equation}\\int q(\\varepsilon) l(f(x+\\varepsilon),f(x)) d\\varepsilon \\approx \\frac{1}{2}\\sigma^2 \\sum\_i \\lambda\_i(x)\\Vert \\nabla\_x f\_i(x)\\Vert^2\\label{eq:gp}\\end{equation}
可见，形式上就是对每个$f(x)$的每个分量都算一个梯度惩罚项$\\Vert \\nabla\_x f\_i(x)\\Vert^2$，然后按$\\lambda\_i(x)$加权求和。

例如，对于MSE来说，$l(f(x),y)=\\Vert f(x) - y\\Vert^2$，这时候可以算得$\\lambda\_i(x)\\equiv 2$，所以对应的正则项为$\\sum\\limits\_i\\Vert \\nabla\_x f\_i(x)\\Vert^2$；对于KL散度来说，$l(f(x),y)=\\sum\\limits\_i y\_i \\log \\frac{y\_i}{f\_i(x)}$，这时候$\\lambda\_i(x)=\\frac{1}{f\_i(x)}$，那么对应的正则项为$\\sum\\limits\_i f\_i(x) \\Vert \\nabla\_x \\log f\_i(x)\\Vert^2$。这些结果大家多多少少可以从著名的“花书” [《深度学习》](https://book.douban.com/subject/27087503/) 中找到类似的，并非新的结果。类似的推导还可以参考文献 [《Training with noise is equivalent to Tikhonov regularization》](https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/bishop-tikhonov-nc-95.pdf)。

### 采样近似 [\#](https://kexue.fm/archives/7466\#%E9%87%87%E6%A0%B7%E8%BF%91%E4%BC%BC)

当然，虽然能求出只带有一阶梯度的正则项$\\sum\\limits\_i \\lambda\_i(x)\\Vert \\nabla\_x f\_i(x)\\Vert^2$，但事实上这个计算量也不低，因为需要对每个$f\_i(x)$都要求梯度，如果输出的分量数太大，这个计算量依然难以承受。

这时候可以考虑的方案是通过采样近似计算：假设$q(\\eta)$是均值为0、方差为1的分布，那么我们有
\\begin{equation}\\sum\\limits\_i \\Vert \\nabla\_x f\_i(x)\\Vert^2=\\sum\\limits\_i \\left\\Vert \\nabla\_x f\_i(x)\\right\\Vert^2=\\mathbb{E}\_{\\eta\_i\\sim q(\\eta)}\\left\[\\left\\Vert\\sum\_i \\eta\_i \\nabla\_x f\_i(x)\\right\\Vert^2\\right\]\\end{equation}
这样一来，每步我们只需要算$\\sum\\limits\_i \\eta\_i f\_i(x)$的梯度，不需要算多次梯度。$q(\\eta)$的一个最简单的取法是空间为$\\{-1,1\\}$的均匀分布，也就是$\\eta\_i$等概率地从$\\{-1,1\\}$中选取一个。

## 对抗训练 [\#](https://kexue.fm/archives/7466\#%E5%AF%B9%E6%8A%97%E8%AE%AD%E7%BB%83)

回顾前面的流程，我们先是介绍了添加随机噪声这一增强泛化性能的手段，然后指出随机加噪声可能太没特异性，所以想着先把积分算出来，才有了后面推导的关于近似展开与梯度惩罚的一些结果。那么换个角度来想，如果我们能想办法更特异性地构造噪声信号，那么也能提高训练效率，增强泛化性能了。

### 监督对抗 [\#](https://kexue.fm/archives/7466\#%E7%9B%91%E7%9D%A3%E5%AF%B9%E6%8A%97)

有监督的对抗训练，关注的是原始目标$\\eqref{eq:noisy-loss}$，优化的目标是让loss尽可能小，所以如果我们要选择更有代表性的噪声，那么应该选择能让loss变得更大的噪声，而
\\begin{equation}l(f(x + \\varepsilon), y) \\approx l(f(x), y) + \\varepsilon \\cdot \\nabla\_x l(f(x), y)\\end{equation}
所以让$l(f(x + \\varepsilon), y)$尽可能大就意味着$\\varepsilon$要跟$\\nabla\_x l(f(x), y)$同向，换言之扰动要往梯度上升方向走，即
\\begin{equation}\\varepsilon \\sim \\nabla\_x l(f(x), y)\\end{equation}
这便构成了对抗训练中的FGM方法，之前在 [《对抗训练浅谈：意义、方法和思考（附Keras实现）》](https://kexue.fm/archives/7234) 就已经介绍过了。

值得注意的是，在 [《对抗训练浅谈：意义、方法和思考（附Keras实现）》](https://kexue.fm/archives/7234) 一文中我们也推导过，对抗训练在一定程度上也等价于往loss里边加入梯度惩罚项$\\left\\Vert\\nabla\_x l(f(x), y)\\right\\Vert^2$，这又跟前一节的关于噪声积分的结果类似。这表明梯度惩罚应该是通用的能提高模型性能的手段之一。

### 虚拟对抗 [\#](https://kexue.fm/archives/7466\#%E8%99%9A%E6%8B%9F%E5%AF%B9%E6%8A%97)

在前面我们提到，$l(f(x+\\varepsilon),f(x))$这一项不需要标签信号，因此可以用来做无监督学习，并且关于它的展开高斯积分我们得到了梯度惩罚$\\eqref{eq:gp}$。如果沿着对抗训练的思想，我们不去计算积分，而是去寻找让$l(f(x+\\varepsilon),f(x))$尽可能大的扰动噪声，这就构成了“虚拟对抗训练（VAT）”，首次出现在文章 [《Virtual Adversarial Training: A Regularization Method for Supervised and Semi-Supervised Learning》](https://papers.cool/arxiv/1704.03976) 中。

基于前面对损失函数$l$的性质的讨论，我们知道$l(f(x+\\varepsilon),f(x))$关于$\\varepsilon$的一阶梯度为0，所以要算对抗扰动，还必须将它展开到二阶：
\\begin{equation}\\begin{aligned}
l(f(x+\\varepsilon),f(x))\\approx&\\, l(f(x),f(x)) + \\varepsilon^{\\top} \\nabla\_x l(f(x),f\_{ng}(x)) + \\frac{1}{2}\\varepsilon^{\\top}\\nabla\_x^2 l(f(x),f\_{ng}(x)) \\varepsilon\\\
=&\\, \\frac{1}{2}\\varepsilon^{\\top}\\nabla\_x^2 l(f(x),f\_{ng}(x)) \\varepsilon\\end{aligned}\\end{equation}
这里用$f\_{ng}(x)$表示不需要对里边的$x$求梯度。这样一来，我们需要解决两个问题：1、如何高效计算Hessian矩阵$\\mathcal{H}=\\nabla\_x^2 l(f(x),f\_{ng}(x))$；2、如何求单位向量$u$使得$u^{\\top}\\mathcal{H}u$最大？

事实上，不难证明$u$的最优解实际上就是“$\\mathcal{H}$的最大特征根对应的特征向量”，也称为“$\\mathcal{H}$的主特征向量”，而要近似求主特征向量，一个行之有效的方法就是“ [幂迭代法](https://en.wikipedia.org/wiki/Power_iteration)”：从一个随机向量$u\_0$出发，迭代执行$u\_{i+1}=\\frac{\\mathcal{H}u\_i}{\\Vert\\mathcal{H}u\_i\\Vert}$。相关推导可以参考 [《深度学习中的Lipschitz约束：泛化与生成模型》](https://kexue.fm/archives/6051#%E4%B8%BB%E7%89%B9%E5%BE%81%E6%A0%B9) 的“主特征根”和“幂迭代”两节。

在幂迭代中，我们发现并不需要知道$\\mathcal{H}$具体值，只需要知道$\\mathcal{H}u$的值，这可以通过差分来近似计算：
\\begin{equation}\\begin{aligned}\\mathcal{H}u =&\\, \\nabla\_x^2 l(f(x),f\_{ng}(x)) u\\\
=&\\, \\nabla\_x \\big(u\\cdot\\nabla\_x l(f(x),f\_{ng}(x))\\big)\\\
\\approx&\\, \\nabla\_x \\left(\\frac{l(f(x + \\xi u),f\_{ng}(x)) - l(f(x),f\_{ng}(x))}{\\xi}\\right)\\\
=&\\, \\frac{1}{\\xi}\\nabla\_x l(f(x + \\xi u),f\_{ng}(x))\\end{aligned}\\end{equation}
其中$\\xi$是一个标量常数。根据这个近似结果，我们就可以得到如下的VAT流程：

> 初始化向量$u\\sim \\mathcal{N}(0,1)$、标量$\\epsilon$和$\\xi$；
> 迭代$r$次：
>    $u \\leftarrow \\frac{u}{\\Vert u\\Vert}$；
>    $u \\leftarrow \\nabla\_x l(f(x+\\xi u), f\_{ng}(x))$
> $u \\leftarrow \\frac{u}{\\Vert u\\Vert}$；
> 用$l(f(x+\\epsilon u), f\_{ng}(x))$作为loss执行常规梯度下降。

实验表明一般迭代1次就不错了，而如果迭代0次，那么就是本文开头提到的添加高斯噪声。这表明虚拟对抗训练就是通过$\\nabla\_x l(f(x+\\xi u), f\_{ng}(x))$来提高噪声的“特异性”的。

### 参考实现 [\#](https://kexue.fm/archives/7466\#%E5%8F%82%E8%80%83%E5%AE%9E%E7%8E%B0)

关于对抗训练的Keras实现，在 [《对抗训练浅谈：意义、方法和思考（附Keras实现）》](https://kexue.fm/archives/7234) 一文中已经给出过，这里笔者给出Keras下虚拟对抗训练的参考实现：

```
def virtual_adversarial_training(
 model, embedding_name, epsilon=1, xi=10, iters=1
):
 """给模型添加虚拟对抗训练
 其中model是需要添加对抗训练的keras模型，embedding_name
 则是model里边Embedding层的名字。要在模型compile之后使用。
 """
 if model.train_function is None: # 如果还没有训练函数
 model._make_train_function() # 手动make
 old_train_function = model.train_function # 备份旧的训练函数

 # 查找Embedding层
 for output in model.outputs:
 embedding_layer = search_layer(output, embedding_name)
 if embedding_layer is not None:
 break
 if embedding_layer is None:
 raise Exception('Embedding layer not found')

 # 求Embedding梯度
 embeddings = embedding_layer.embeddings # Embedding矩阵
 gradients = K.gradients(model.total_loss, [embeddings]) # Embedding梯度
 gradients = K.zeros_like(embeddings) + gradients[0] # 转为dense tensor

 # 封装为函数
 inputs = (
 model._feed_inputs + model._feed_targets + model._feed_sample_weights
 ) # 所有输入层
 model_outputs = K.function(
 inputs=inputs,
 outputs=model.outputs,
 name='model_outputs',
 ) # 模型输出函数
 embedding_gradients = K.function(
 inputs=inputs,
 outputs=[gradients],
 name='embedding_gradients',
 ) # 模型梯度函数

 def l2_normalize(x):
 return x / (np.sqrt((x**2).sum()) + 1e-8)

 def train_function(inputs): # 重新定义训练函数
 outputs = model_outputs(inputs)
 inputs = inputs[:2] + outputs + inputs[3:]
 delta1, delta2 = 0.0, np.random.randn(*K.int_shape(embeddings))
 for _ in range(iters): # 迭代求扰动
 delta2 = xi * l2_normalize(delta2)
 K.set_value(embeddings, K.eval(embeddings) - delta1 + delta2)
 delta1 = delta2
 delta2 = embedding_gradients(inputs)[0] # Embedding梯度
 delta2 = epsilon * l2_normalize(delta2)
 K.set_value(embeddings, K.eval(embeddings) - delta1 + delta2)
 outputs = old_train_function(inputs) # 梯度下降
 K.set_value(embeddings, K.eval(embeddings) - delta2) # 删除扰动
 return outputs

 model.train_function = train_function # 覆盖原训练函数

# 写好函数后，启用虚拟对抗训练只需要一行代码
virtual_adversarial_training(model_vat, 'Embedding-Token')
```

完整的使用脚本请参考： [task\_sentiment\_virtual\_adversarial\_training.py](https://github.com/bojone/bert4keras/blob/master/examples/task_sentiment_virtual_adversarial_training.py)。大概是将模型建立两次，一个模型通过标注数据正常训练，一个模型通过无标注数据虚拟对抗训练，两者交替执行，请读懂源码后再使用，不要乱套代码。实验任务为情况分类，大约有2万的标注数据，取前200个作为标注样本，剩下的作为无标注数据，VAT和非VAT的表现对比如下（每个实验都重复了三次，取平均）：
\\begin{array}{c\|cc}
\\hline
& \\text{验证集} & \\text{测试集}\\\
\\hline
\\text{非VAT} & 88.93\\% & 89.34\\%\\\
\\text{VAT} & 89.83\\% & 90.37\\%\\\
\\hline
\\end{array}

> **说明：** 前面提到$f\_{ng}(x)$表示不对$x$求梯度，不过$f$自身的参数$\\theta$的梯度还是需要求的。但是读懂了上述代码的读者会发现，上述实现中相当于把$f\_{ng}(x)$中$x,\\theta$的梯度都去掉了，理论上不完全等价于标准的VAT。问题是在Keras中实现标准的VAT有点麻烦，而且计算量会加大，此外实验发现上述“山寨”版本也已经能带来提升了，标准的VAT相对它而言差别是二阶小量的，所以差别不大，上述代码基本满足需求了。

## 文章小结 [\#](https://kexue.fm/archives/7466\#%E6%96%87%E7%AB%A0%E5%B0%8F%E7%BB%93)

本文先介绍了添加随机噪声这一常规的正则化手段，然后通过近似展开与积分的过程，推导了它与梯度惩罚之间的联系，并从中引出了可以用于半监督训练的模型平滑损失，接着进一步联系到了监督式的对抗训练和半监督的虚拟对抗训练，最后给出了Keras下虚拟对抗训练的实现和例子。

_**转载到请包括本文地址：** [https://kexue.fm/archives/7466](https://kexue.fm/archives/7466)_

_**更详细的转载事宜请参考：**_ [《科学空间FAQ》](https://kexue.fm/archives/6508#%E6%96%87%E7%AB%A0%E5%A6%82%E4%BD%95%E8%BD%AC%E8%BD%BD/%E5%BC%95%E7%94%A8)

**如果您还有什么疑惑或建议，欢迎在下方评论区继续讨论。**

**如果您觉得本文还不错，欢迎 [分享](https://kexue.fm/archives/7466#share)/ [打赏](https://kexue.fm/archives/7466#pay) 本文。打赏并非要从中获得收益，而是希望知道科学空间获得了多少读者的真心关注。当然，如果你无视它，也不会影响你的阅读。再次表示欢迎和感谢！**

打赏

微信打赏

支付宝打赏

因为网站后台对打赏并无记录，因此欢迎在打赏时候备注留言。你还可以 [**点击这里**](http://mail.qq.com/cgi-bin/qm_share?t=qm_mailme&email=tN7d1drY3drrx8H0xcWa19vZ) 或在下方评论区留言来告知你的建议或需求。

**如果您需要引用本文，请参考：**

苏剑林. (Jun. 01, 2020). 《泛化性乱弹：从随机噪声、梯度惩罚到虚拟对抗训练 》\[Blog post\]. Retrieved from [https://kexue.fm/archives/7466](https://kexue.fm/archives/7466)

@online{kexuefm-7466,
        title={泛化性乱弹：从随机噪声、梯度惩罚到虚拟对抗训练},
        author={苏剑林},
        year={2020},
        month={Jun},
        url={\\url{https://kexue.fm/archives/7466}},
}

分类： [信息时代](https://kexue.fm/category/Big-Data)    标签： [概率](https://kexue.fm/tag/%E6%A6%82%E7%8E%87/), [GAN](https://kexue.fm/tag/GAN/), [对抗训练](https://kexue.fm/tag/%E5%AF%B9%E6%8A%97%E8%AE%AD%E7%BB%83/), [泛化](https://kexue.fm/tag/%E6%B3%9B%E5%8C%96/)[32 评论](https://kexue.fm/archives/7466#comments)

< [Google新作Synthesizer：我们还不够了解自注意力](https://kexue.fm/archives/7430) \| [为什么梯度裁剪能加速训练过程？一个简明的分析](https://kexue.fm/archives/7469) >

### 你也许还对下面的内容感兴趣

- [一道概率不等式：盯着它到显然成立为止！](https://kexue.fm/archives/10902)
- [Softmax后传：寻找Top-K的光滑近似](https://kexue.fm/archives/10373)
- [通向最优分布之路：概率空间的最小化](https://kexue.fm/archives/10289)
- [通向概率分布之路：盘点Softmax及其替代品](https://kexue.fm/archives/10145)
- [用傅里叶级数拟合一维概率密度函数](https://kexue.fm/archives/10007)
- [幂等生成网络IGN：试图将判别和生成合二为一的GAN](https://kexue.fm/archives/9969)
- [Transformer升级之路：16、“复盘”长度外推技术](https://kexue.fm/archives/9948)
- [Transformer升级之路：15、Key归一化助力长度外推](https://kexue.fm/archives/9859)
- [随机分词再探：从Viterbi Sampling到完美采样算法](https://kexue.fm/archives/9811)
- [EMO：基于最优传输思想设计的分类损失函数](https://kexue.fm/archives/9797)

[发表你的看法](https://kexue.fm/archives/7466#comment_form)

1. [«](https://kexue.fm/archives/7466/comment-page-1#comments)
2. [1](https://kexue.fm/archives/7466/comment-page-1#comments)
3. [2](https://kexue.fm/archives/7466/comment-page-2#comments)

Shuan

December 6th, 2021

想请问一下：(14)式的最右侧部分的范数符号是否应写在求和符号之内呢？

[回复评论](https://kexue.fm/archives/7466/comment-page-2?replyTo=17968#respond-post-7466)

Shuan

December 6th, 2021

Sorry. Get了

[回复评论](https://kexue.fm/archives/7466/comment-page-2?replyTo=17969#respond-post-7466)

mlb

March 8th, 2025

xi=10，对这个参数其实对应公式里的ξ，论文中取得是1e-6，这个差距为什么这么大？

[回复评论](https://kexue.fm/archives/7466/comment-page-2?replyTo=27052#respond-post-7466)

[苏剑林](https://kexue.fm) 发表于
March 9th, 2025

忘了...不过我看了一下代码，这里是将全体Embedding当成一个向量来看待的，这样一来对delta2进行l2\_normalize后数值上就会变得非常小，因此xi取10其实也不大（相对于Embedding本身的数值来说），毕竟xi \* delta2要对Embedding进行适量的扰动，太小了扰动就不明显了。

[回复评论](https://kexue.fm/archives/7466/comment-page-2?replyTo=27056#respond-post-7466)

1. [«](https://kexue.fm/archives/7466/comment-page-1#comments)
2. [1](https://kexue.fm/archives/7466/comment-page-1#comments)
3. [2](https://kexue.fm/archives/7466/comment-page-2#comments)

[取消回复](https://kexue.fm/archives/7466#respond-post-7466)

你的大名

电子邮箱

个人网站（选填）

1\. 可以使用LaTeX代码，点击“预览效果”可查看效果；2. 可以通过点击评论楼层编号来引用该楼层；3. 网站可能会有点卡，如非确认评论失败，请 **不要重复点击提交**。

### 内容速览

[随机噪声](https://kexue.fm/archives/7466#%E9%9A%8F%E6%9C%BA%E5%99%AA%E5%A3%B0)
[提高效率](https://kexue.fm/archives/7466#%E6%8F%90%E9%AB%98%E6%95%88%E7%8E%87)
[增加采样](https://kexue.fm/archives/7466#%E5%A2%9E%E5%8A%A0%E9%87%87%E6%A0%B7)
[近似展开](https://kexue.fm/archives/7466#%E8%BF%91%E4%BC%BC%E5%B1%95%E5%BC%80)
[转移目标](https://kexue.fm/archives/7466#%E8%BD%AC%E7%A7%BB%E7%9B%AE%E6%A0%87)
[思路解析](https://kexue.fm/archives/7466#%E6%80%9D%E8%B7%AF%E8%A7%A3%E6%9E%90)
[勇敢地算](https://kexue.fm/archives/7466#%E5%8B%87%E6%95%A2%E5%9C%B0%E7%AE%97)
[梯度惩罚](https://kexue.fm/archives/7466#%E6%A2%AF%E5%BA%A6%E6%83%A9%E7%BD%9A)
[采样近似](https://kexue.fm/archives/7466#%E9%87%87%E6%A0%B7%E8%BF%91%E4%BC%BC)
[对抗训练](https://kexue.fm/archives/7466#%E5%AF%B9%E6%8A%97%E8%AE%AD%E7%BB%83)
[监督对抗](https://kexue.fm/archives/7466#%E7%9B%91%E7%9D%A3%E5%AF%B9%E6%8A%97)
[虚拟对抗](https://kexue.fm/archives/7466#%E8%99%9A%E6%8B%9F%E5%AF%B9%E6%8A%97)
[参考实现](https://kexue.fm/archives/7466#%E5%8F%82%E8%80%83%E5%AE%9E%E7%8E%B0)
[文章小结](https://kexue.fm/archives/7466#%E6%96%87%E7%AB%A0%E5%B0%8F%E7%BB%93)

### 智能搜索

支持整句搜索！网站自动使用 [结巴分词](https://github.com/fxsjy/jieba) 进行分词，并结合ngrams排序算法给出合理的搜索结果。

### 热门标签

[生成模型](https://kexue.fm/tag/%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B/) [attention](https://kexue.fm/tag/attention/) [优化](https://kexue.fm/tag/%E4%BC%98%E5%8C%96/) [语言模型](https://kexue.fm/tag/%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/) [模型](https://kexue.fm/tag/%E6%A8%A1%E5%9E%8B/) [网站](https://kexue.fm/tag/%E7%BD%91%E7%AB%99/) [概率](https://kexue.fm/tag/%E6%A6%82%E7%8E%87/) [梯度](https://kexue.fm/tag/%E6%A2%AF%E5%BA%A6/) [转载](https://kexue.fm/tag/%E8%BD%AC%E8%BD%BD/) [微分方程](https://kexue.fm/tag/%E5%BE%AE%E5%88%86%E6%96%B9%E7%A8%8B/) [矩阵](https://kexue.fm/tag/%E7%9F%A9%E9%98%B5/) [天象](https://kexue.fm/tag/%E5%A4%A9%E8%B1%A1/) [分析](https://kexue.fm/tag/%E5%88%86%E6%9E%90/) [深度学习](https://kexue.fm/tag/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/) [积分](https://kexue.fm/tag/%E7%A7%AF%E5%88%86/) [python](https://kexue.fm/tag/python/) [优化器](https://kexue.fm/tag/%E4%BC%98%E5%8C%96%E5%99%A8/) [力学](https://kexue.fm/tag/%E5%8A%9B%E5%AD%A6/) [无监督](https://kexue.fm/tag/%E6%97%A0%E7%9B%91%E7%9D%A3/) [扩散](https://kexue.fm/tag/%E6%89%A9%E6%95%A3/) [几何](https://kexue.fm/tag/%E5%87%A0%E4%BD%95/) [节日](https://kexue.fm/tag/%E8%8A%82%E6%97%A5/) [生活](https://kexue.fm/tag/%E7%94%9F%E6%B4%BB/) [文本生成](https://kexue.fm/tag/%E6%96%87%E6%9C%AC%E7%94%9F%E6%88%90/) [数论](https://kexue.fm/tag/%E6%95%B0%E8%AE%BA/)

### 随机文章

- [美绘制太空引力高速路帮飞船穿越太阳系(图)](https://kexue.fm/archives/121)
- [深度学习中的Lipschitz约束：泛化与生成模型](https://kexue.fm/archives/6051)
- [科学空间：2010年7月重要天象](https://kexue.fm/archives/704)
- [“n次方程有n个根”的证明](https://kexue.fm/archives/481)
- [百科翻译：臭氧的性质](https://kexue.fm/archives/8)
- [鬼斧神工：求n维球的体积](https://kexue.fm/archives/3154)
- [只有两个四阶群和六阶群](https://kexue.fm/archives/3036)
- [生成扩散模型漫谈（七）：最优扩散方差估计（上）](https://kexue.fm/archives/9245)
- [变分自编码器（二）：从贝叶斯观点出发](https://kexue.fm/archives/5343)
- [抛物线内一根定长的弦](https://kexue.fm/archives/1639)

### 最近评论

- [宋佳铭](https://kexue.fm/archives/10958/comment-page-1#comment-27947): 对，个人感觉mean flow就是continuous time CTM
- [宋佳铭](https://kexue.fm/archives/10958/comment-page-1#comment-27946): 的确，对sg这个事情我感觉如果是用‘归纳’法做是不太能避免的，因为毕竟是用步长短的模型去约束步...
- [MoFHeka](https://kexue.fm/archives/10542/comment-page-1#comment-27945): 苏老师您好，请问一下这套结论在稀疏参数上应该如何应用？比如大规模稀疏Embedding，每个B...
- [苏剑林](https://kexue.fm/archives/11033/comment-page-1#comment-27944): Temp LoRA倒是有印象，其实思想是一样的，如果我单独开一篇文章介绍TTT的话，应该会提到...
- [苏剑林](https://kexue.fm/archives/11033/comment-page-1#comment-27943): 你搜搜mamba、rwkv甚至rnn做vision的工作，其实不少。不过多数确实像你说的，正反...
- [苏剑林](https://kexue.fm/archives/9379/comment-page-1#comment-27942): 问题1可以看看 https://kexue.fm/archives/4718 ，简单来说就是点...
- [苏剑林](https://kexue.fm/archives/10958/comment-page-2#comment-27941): 你的“信息量”怎么定义？直观来说，reflow训练的是切线模型，而一步生成需要的是割线模型，m...
- [苏剑林](https://kexue.fm/archives/10711/comment-page-2#comment-27940): 加大codebook也不行吗？主要是我也不大了解SDXL有什么特别之处～
- [苏剑林](https://kexue.fm/archives/9009/comment-page-2#comment-27939): 如果考虑额外的scale，那情况肯定不一样了，甚至手工scale得好的话，layernorm都...
- [苏剑林](https://kexue.fm/archives/10592/comment-page-2#comment-27938): 最后的问题，可能要监控一下每个参数的grad norm，看是哪里比较异常？一般情况下，Muon...

### 友情链接

- [Cool Papers](https://papers.cool)
- [数学研发](https://bbs.emath.ac.cn)
- [Seatop](http://www.seatop.com.cn/)
- [Xiaoxia](https://xiaoxia.org/)
- [积分表-网络版](https://kexue.fm/sci/integral/index.html)
- [丝路博傲](http://blog.dvxj.com/)
- [ph4ntasy 饭特稀](http://www.ph4ntasy.com/)
- [数学之家](http://www.2math.cn/)
- [有趣天文奇观](http://interesting-sky.china-vo.org/)
- [TwistedW](http://www.twistedwg.com/)
- [godweiyang](https://godweiyang.com/)
- [AI柠檬](https://blog.ailemon.net/)
- [王登科-DK博客](https://greatdk.com)
- [ESON](https://blog.eson.org/)
- [枫之羽](https://fzhiy.net/)
- [Mathor's blog](https://wmathor.com/)
- [coding-zuo](https://coding-zuo.github.io/)
- [博科园](https://www.bokeyuan.net/)
- [孔皮皮的博客](https://www.kppkkp.top/)
- [运鹏的博客](https://yunpengtai.top/)
- [jiming.site](https://jiming.site/)
- [OmegaXYZ](https://www.omegaxyz.com/)
- [Blog by Eacls](https://www.eacls.top/)
- [EAI猩球](https://www.robotech.ink/)
- [文举的博客](https://liwenju0.com/)
- [用代码打点酱油](https://bruceyuan.com/)
- [申请链接](https://kexue.fm/links.html)

本站采用创作共用版权协议，要求署名、非商业用途和保持一致。转载本站内容必须也遵循“ [署名-非商业用途-保持一致](http://creativecommons.org/licenses/by-nc-nd/2.5/cn/)”的创作共用协议。
© 2009-2025 Scientific Spaces. All rights reserved. Theme by [laogui](http://www.laogui.com). Powered by [Typecho](http://typecho.org). 备案号: [粤ICP备09093259号-1/2](https://beian.miit.gov.cn/)。