## SEARCH

## MENU

- [打赏](https://kexue.fm/reward.html)
- [公式](https://kexue.fm/latex.html)
- [天象](https://kexue.fm/ac.html)
- [链接](https://kexue.fm/links.html)
- [时光](https://kexue.fm/me.html)
- [博览](https://kexue.fm/science.html)
- [归档](https://kexue.fm/content.html)

## CATEGORIES

- [千奇百怪](https://kexue.fm/category/Everything)
- [天文探索](https://kexue.fm/category/Astronomy)
- [数学研究](https://kexue.fm/category/Mathematics)
- [物理化学](https://kexue.fm/category/Phy-chem)
- [信息时代](https://kexue.fm/category/Big-Data)
- [生物自然](https://kexue.fm/category/Biology)
- [图片摄影](https://kexue.fm/category/Photograph)
- [问题百科](https://kexue.fm/category/Questions)
- [生活/情感](https://kexue.fm/category/Life-Feeling)
- [资源共享](https://kexue.fm/category/Resources)

## NEWPOSTS

- [通过msign来计算奇异值裁剪mc...](https://kexue.fm/archives/11059)
- [矩阵符号函数mcsgn能计算什么？](https://kexue.fm/archives/11056)
- [线性注意力简史：从模仿、创新到反哺](https://kexue.fm/archives/11033)
- [msign的导数](https://kexue.fm/archives/11025)
- [通过msign来计算奇异值裁剪mc...](https://kexue.fm/archives/11006)
- [msign算子的Newton-Sc...](https://kexue.fm/archives/10996)
- [等值振荡定理：最优多项式逼近的充要条件](https://kexue.fm/archives/10972)
- [生成扩散模型漫谈（三十）：从瞬时速...](https://kexue.fm/archives/10958)
- [MoE环游记：5、均匀分布的反思](https://kexue.fm/archives/10945)
- [msign算子的Newton-Sc...](https://kexue.fm/archives/10922)

## COMMENTS

- [Leco: 请问LoRA的A,B矩阵初始化时，一个高斯随机一个全零还是只能...](https://kexue.fm/archives/9590/comment-page-2#comment-27984)
- [苏剑林: 如果你把你这里提到的数学都学通透了，数学基础基本上可以胜任95...](https://kexue.fm/archives/9119/comment-page-13#comment-27983)
- [苏剑林: 我跑过这个项目，效果是能复现的。“在 CIFAR-10 上效果...](https://kexue.fm/archives/10958/comment-page-2#comment-27982)
- [Henry Zha: 苏神你好，我是一名管理科学与工程专业的博士生，研究方向是结合人...](https://kexue.fm/archives/9119/comment-page-13#comment-27981)
- [SunlightZero: 我根据 https://github.com/haidog-y...](https://kexue.fm/archives/10958/comment-page-2#comment-27980)
- [苏剑林: 噢，是笔误，更正了，感谢指出。](https://kexue.fm/archives/11025/comment-page-1#comment-27979)
- [苏剑林: 这里有很多因素。如果推理数据跟训练数据同分布，那么理论上就是均...](https://kexue.fm/archives/10945/comment-page-1#comment-27978)
- [苏剑林: 目前看来给O加rmsnorm挺稳的，效果甚至还好点。我其实是直...](https://kexue.fm/archives/11033/comment-page-1#comment-27977)
- [苏剑林: 你应该说的是$\\exp(\\boldsymbol{Q}\\bold...](https://kexue.fm/archives/11033/comment-page-1#comment-27976)
- [苏剑林: 可以这么说吧，通过某种分母归一化的操作，导数格式都类似，毕竟公...](https://kexue.fm/archives/10831/comment-page-1#comment-27975)

## USERLOGIN

- [登录](https://kexue.fm/admin/login.php)

[科学空间\|Scientific Spaces](https://kexue.fm)

- [登录](https://kexue.fm/admin/login.php)
- [打赏](https://kexue.fm/reward.html)
- [公式](https://kexue.fm/latex.html)
- [天象](https://kexue.fm/ac.html)
- [链接](https://kexue.fm/links.html)
- [时光](https://kexue.fm/me.html)
- [博览](https://kexue.fm/science.html)
- [归档](https://kexue.fm/content.html)

渴望成为一个小飞侠

- [欢迎订阅](https://kexue.fm/feed)
- [个性邮箱](https://kexue.fm/archives/119)
- [天象信息](https://kexue.fm/ac.html)
- [观测ISS](https://kexue.fm/archives/41)
- [LaTeX](https://kexue.fm/latex.html)
- [关于博主](https://kexue.fm/me.html)

欢迎访问“科学空间”，这里将与您共同探讨自然科学，回味人生百态；也期待大家的分享～

- [**千奇百怪** Everything](https://kexue.fm/category/Everything)
- [**天文探索** Astronomy](https://kexue.fm/category/Astronomy)
- [**数学研究** Mathematics](https://kexue.fm/category/Mathematics)
- [**物理化学** Phy-chem](https://kexue.fm/category/Phy-chem)
- [**信息时代** Big-Data](https://kexue.fm/category/Big-Data)
- [**生物自然** Biology](https://kexue.fm/category/Biology)
- [**图片摄影** Photograph](https://kexue.fm/category/Photograph)
- [**问题百科** Questions](https://kexue.fm/category/Questions)
- [**生活/情感** Life-Feeling](https://kexue.fm/category/Life-Feeling)
- [**资源共享** Resources](https://kexue.fm/category/Resources)

- [**千奇百怪**](https://kexue.fm/category/Everything)
- [**天文探索**](https://kexue.fm/category/Astronomy)
- [**数学研究**](https://kexue.fm/category/Mathematics)
- [**物理化学**](https://kexue.fm/category/Phy-chem)
- [**信息时代**](https://kexue.fm/category/Big-Data)
- [**生物自然**](https://kexue.fm/category/Biology)
- [**图片摄影**](https://kexue.fm/category/Photograph)
- [**问题百科**](https://kexue.fm/category/Questions)
- [**生活/情感**](https://kexue.fm/category/Life-Feeling)
- [**资源共享**](https://kexue.fm/category/Resources)

[首页](https://kexue.fm) [数学研究](https://kexue.fm/category/Mathematics) 时空之章：将Attention视为平方复杂度的RNN

18Mar

# [时空之章：将Attention视为平方复杂度的RNN](https://kexue.fm/archives/10017)

By 苏剑林 \|
2024-03-18 \|
64121位读者\|

近年来，RNN由于其线性的训练和推理效率，重新吸引了不少研究人员和用户的兴趣，隐约有“文艺复兴”之势，其代表作有 [RWKV](https://papers.cool/arxiv/2305.13048)、 [RetNet](https://papers.cool/arxiv/2307.08621)、 [Mamba](https://papers.cool/arxiv/2312.00752) 等。当将RNN用于语言模型时，其典型特点就是每步生成都是常数的空间复杂度和时间复杂度，从整个序列看来就是常数的空间复杂度和线性的时间复杂度。当然，任何事情都有两面性，相比于Attention动态增长的KV Cache，RNN的常数空间复杂度通常也让人怀疑记忆容量有限，在Long Context上的效果很难比得上Attention。

在这篇文章中，我们表明Causal Attention可以重写成RNN的形式，并且它的每一步生成理论上也能够以$\\mathcal{O}(1)$的空间复杂度进行（代价是时间复杂度非常高，远超平方级）。这表明Attention的优势（如果有的话）是靠计算堆出来的，而不是直觉上的堆内存，它跟RNN一样本质上都是常数量级的记忆容量（记忆瓶颈）。

## 超越线性的RNN [\#](https://kexue.fm/archives/10017\#%E8%B6%85%E8%B6%8A%E7%BA%BF%E6%80%A7%E7%9A%84RNN)

RNN的支持者通常会给出一个看上去让人难以反驳的观点：想想你的大脑是RNN还是Attention？

直觉来想，RNN推理的空间复杂度是常数，而Attention的KV cache是动态增长的，再考虑到人的脑容量是有限的，从这一点来看不得不说确实RNN更接近人脑。然而，即便可以合理地认为脑容量限制了人每步推理的空间复杂度是常数，但它并没有限制每步的时间复杂度是常数，又或者换个说法，即便人的每步时间复杂度是常数，但人处理长度为$L$的序列时未必只扫描一遍序列（比如“翻书”），所以总的推理步数可能明显超出$L$，从而导致了非线性的时间复杂度。

考虑到这一点，笔者“突发奇想”：是否可以一般化地考虑常数空间复杂度、非线性时间复杂度的RNN模型，来补足主流RNN的所没有的能力（比如上面说的翻书）？对于语言模型任务，假设样本是a b c d e，那么训练任务就是输入a b c d，预测b c d e，常见的RNN如下图：

图一：常见RNN

这种RNN的问题就是没有翻书能力，每个输入读完就丢了。而Attention的特点就是每读一个token，就完整地翻一遍历史，虽然这个做法可能存在效率问题，但它无疑是引入翻书能力的最简单粗暴的方式。而为了给RNN补上翻书能力，我们完全可以模仿Attention的做法来使用RNN：

图二：不断“翻书”的RNN

跟Attention一样，每读一个新的token，就翻一遍完整的历史。当然，也可以说这其实没有设计一种新的RNN，只是RNN的一种新用法，单纯修改了输入，不管是RWKV还是Mamba都可以套上去。在这种用法之下，解码依旧可以在常数空间复杂度内完成，但每一步推理的时间复杂度在线性增长，从而总的时间成本是$\\mathcal{O}(L^2)$。

## 注意力也是RNN [\#](https://kexue.fm/archives/10017\#%E6%B3%A8%E6%84%8F%E5%8A%9B%E4%B9%9F%E6%98%AFRNN)

事实上，图二所代表的模型非常广泛，甚至于Attention也只不过是它的一个特例，如下图所示：

图三：Causal Attention对应的RNN

跟图二相比，图三有几个箭头虚化了，代表这几处位置实际上是断开的，所以说Attention只不过是图二的一个特例。具体来说，Attention的计算公式为：
\\begin{equation}o\_i = \\sum\_{j=1}^i a\_{i,j}v\_j = \\frac{\\sum\_{j=1}^i e^{q\_i\\cdot k\_j} v\_j}{\\sum\_{j=1}^i e^{q\_i\\cdot k\_j}}\\end{equation}
很明显，分子分母的求和都可以写成递归的形式：
\\begin{equation}
\\begin{pmatrix} y\_i^{(t)} \\\ z\_i^{(t)} \\end{pmatrix} = \\begin{pmatrix} y\_i^{(t-1)} \\\ z\_i^{(t-1)} \\end{pmatrix} + e^{q\_i\\cdot k\_{i-t+1}}\\begin{pmatrix} v\_{i-t+1} \\\ 1 \\end{pmatrix}\\quad,\\quad o\_i = \\frac{y\_i^{(i)}}{z\_i^{(i)}}
\\end{equation}
根据笔者所阅读的文献，最早提出上式并用它来优化Attention计算的文献是 [《Self-attention Does Not Need O(n^2) Memory》](https://papers.cool/arxiv/2112.05682)，上式的分块矩阵版本正是当前主流的加速技术Flash Attention的理论基础。由于在Self Attention中，Q、K、V都是由同一个输入通过token-wise的运算得到，所以上述递归形式正好就可以表示为图三。

当然，图三只画出了一层Attention，多层自然也可以画出来，但连接看起来会有点复杂，比如两层的情况如下图所示：

图四：两层Attention对应的RNN

## 常数空间复杂度 [\#](https://kexue.fm/archives/10017\#%E5%B8%B8%E6%95%B0%E7%A9%BA%E9%97%B4%E5%A4%8D%E6%9D%82%E5%BA%A6)

本文开头已经说了，RNN的常见优点是可以常数空间复杂度、线性时间复杂度进行推理，既然Attention也可以写成RNN，那么自然的问题是在这种写法下它也有这两个优点吗？

很明显，由于Attention对应的RNN是一个序列长度增加到了$\\mathcal{O}(L^2)$的RNN，所以线性时间复杂度那是不用想了，唯一值得思考的是能不能做到常数空间复杂度？大家的第一反应也许是不能，因为众所周知Attention解码有一个动态线性增长的KV cache。但这只是通常情况下比较高效率的实现，如果我们不计成本地用时间换空间，那么空间复杂度可以进一步降低到多少呢？

答案可能让人意外： **如果真的将时间换空间做到极致，那么确实可以将空间复杂度降低到$\\mathcal{O}(1)$！**

其实这个结论并不难想象。首先，图三所示的单层Attention，形式跟普通的单层RNN没什么两样，因此显然是可以用固定大小的储存空间就可以完成推理；接着，我们来看图四所示的多层Attention，它的层与层之间的连接比较复杂，所以通常需要将历史K、V缓存起来才能比较高效地计算，但如果我们坚决不存KV cache，那么每一层、每一步推理所输入的K、V，完全从最原始输入进行重新计算得到（重计算），这会导致非常多的重复计算，所以总的时间复杂度会远超平方复杂度，非常不环保，但空间复杂度确实可以保持在$\\mathcal{O}(1)$。

以两层Attention为例，第二层Attention用到了第一层Attention的输出作为输入，而第一层Attention的每个输出都可以在$\\mathcal{O}(1)$空间内计算得到，所以只要我们愿意牺牲效率去重计算，第二层Attention也只需要在$\\mathcal{O}(1)$空间就可以完成。依此类推，第三层Attention用到了第二层Attention的输出作为输入，第$N$层Attention用到了第$N-1$层Attention的输出作为输入，由于上一层都可以通过重计算在$\\mathcal{O}(1)$空间就可以完成，所以每一层乃至整个模型都可以在$\\mathcal{O}(1)$空间完成计算。

这就再次回到了文章开头的观点：如果Attention相比RNN真的存在什么优势，那也只是靠更多的计算达到的，直觉上的扩大了“内存”，只是用空间换时间的表象，它跟RNN一样本质上都具有常数容量的记忆瓶颈。

当然，也许有读者觉得：用时间换空间不是很常见的做法吗？这看上去并不是什么有价值的结论？的确，时间换空间确实很常见，但并非总是能做到的。换句话说，并不是所有问题都可以通过时间换空间来将空间复杂度降低到$\\mathcal{O}(1)$的，这是一个常见但非平凡的特性。

## 模型能力的思考 [\#](https://kexue.fm/archives/10017\#%E6%A8%A1%E5%9E%8B%E8%83%BD%E5%8A%9B%E7%9A%84%E6%80%9D%E8%80%83)

之所以指出Attention的这一特性，并不是真的要用这个特性去推理，而是通过它来帮助我们进一步思考Attention的能力瓶颈。

首先，真的要抠细节的话，$\\mathcal{O}(1)$其实是不对的，更严格来说应该是$\\mathcal{O}(L)$，因为平方复杂度的RNN需要反复扫描历史序列，这至少需要把原始输入和生成过程的输出都存下来，即至少需要存$L$个整数token id，这个所需要的空间是$\\mathcal{O}(L)$的，如果$L$足够大，那么$\\mathcal{O}(L)$将会比$\\mathcal{O}(1)$更大。然而，这里的$\\mathcal{O}(1)$主要说的是LLM中间的计算层所需要的最少空间，相当于作为RNN时的hidden\_state，至少有(hidden\_size \* num\_layers \* 2)个分量，而$\\mathcal{O}(L)$的空间则体现在输入和输出。一个直观的类比是将Attention当作一台具有无限硬盘、固定内存的计算机，它不断从硬盘中读取数据，然后在内存中进行计算，同时把结果写进硬盘中。

我们知道，如果内存本身很大而处理的数据不大时，那么我们自己在编程时通常都会更加“任性”一点，甚至可能将所有数据加载到内存，中间计算过程完全不依赖于硬盘的读写。同样，在“大模型、短序列”背景之下训练出来的LLM，会更倾向于使用模型scale带来$\\mathcal{O}(1)$级别的固定“内存”，而不是由序列长度带来的动态“硬盘”，因为在当前LLM的scale之下前者会足够大，SGD会“偷懒”将模型当成一个具有无限静态内存的机器来训练（因为对短序列来说内存总是足够），但实际上模型的静态内存是有限的，因此对于那些不可能在$\\mathcal{O}(1)$空间完成的任务，基于Attention的模型也不能够泛化到任意长度的输入。

举个例子，我们要计算$2^x$的十进制表示$y$，用Attention进行条件建模$p(y\|x)$，训练语料就是$\\{x,\\color{red}{\[sep\]},y\\}$拼接，只算$y$的loss。注意这里的$y$可以由输入$x$唯一确定，那么理论上应该可以学出100%的准确率。但如果没有思维链（CoT）来动态增加序列长度，模型只能将所有计算过程隐式地放到“内存”中，这对于短输入总是有效的。但事实上，内存是有限的，而计算$2^x$所需要的空间则随着$x$的增加而增加，所以必然存在一个足够大的$x$，使得$p(y\|x)$的准确率无法做到100%（哪怕是训练准确率）。这跟 [《Transformer升级之路：16、“复盘”长度外推技术》](htthttps://kexue.fm/archives/9948p://) 所讨论的长度外推问题不一样，它不是由位置编码的OOD导致的，而是没有足够CoT引导时“大模型、短序列”的训练所带来的的能力缺陷。

那为什么当前主流的scale up方向依然是增大LLM的内存，即增加模型的hidden\_size和num\_layers，而不是去研究诸如CoT等增加seq\_len的方案呢？后者当然也是主流研究之一，但核心问题是如果内存成为瓶颈，会降低模型的学习效率和普适性。就好比内存不大而数据量很大时，我们就需要及时保存结果到硬盘中并清空内存，这意味着算法上要更加精巧、难写，而且有可能还要根据具体的任务来定制算法细节。那什么情况下会出现内存瓶颈呢？以LLAMA2-70B为例，它的num\_layers为80、hidden\_size为8192，两者相乘是640K，再乘个2刚好是1M左右。换句话说，当输入长度达到1M tokens的这个级别，那么LLAMA2-70B的“内存”就可能成为瓶颈。尽管目前训练1M tokens级别的LLM依然不容易，但已经不再是遥不可及，比如Kimi就已经上线了1M级别的模型内测。

所以，不断增加模型的context length（硬盘），以容纳更多的输入和CoT，同时提高模型本身的scale，使得“内存”不至于是瓶颈，就成为了当前LLM的主旋律。

同时，这还否定了笔者之前的一个想法：是否可以通过缩小模型规模、增加seq\_len来达到跟大模型一样的效果？答案大概是不行，因为小模型存在内存瓶颈，要靠seq\_len带来的硬盘来补足的话，需要给每个样本都设置足够长的CoT才行，这难度比直接训练大模型更加大，如果只是通过repeat等简单方案来增加seq\_len，由于没有带来额外信息，那么是没有有实质收益的。不过，如果增加seq\_len是通过prefix tuning的方式来实现的，那么是有可能补足空间复杂度上的差距的，因为prefix的参数并非由输入序列计算出来，而是单独训练的，这就相当于额外插了一系列“内存条”，从而增大了模型的内存。

## 最后再来个小结 [\#](https://kexue.fm/archives/10017\#%E6%9C%80%E5%90%8E%E5%86%8D%E6%9D%A5%E4%B8%AA%E5%B0%8F%E7%BB%93)

在这篇文章中，我们从平方复杂度RNN的角度审视了Attention，并发现了它具有常数空间复杂度的瓶颈，这表明Attention相比RNN本质上并没有增加“内存”，而只是增加了非常多的计算量。这个瓶颈的存在，表明Attention对某些任务的长度泛化可能存在理论上的困难（内存不足），如何引导模型更好地利用seq\_len维度所带来的动态“硬盘”，也许是解决这个困难的关键之处。

_**转载到请包括本文地址：** [https://kexue.fm/archives/10017](https://kexue.fm/archives/10017)_

_**更详细的转载事宜请参考：**_ [《科学空间FAQ》](https://kexue.fm/archives/6508#%E6%96%87%E7%AB%A0%E5%A6%82%E4%BD%95%E8%BD%AC%E8%BD%BD/%E5%BC%95%E7%94%A8)

**如果您还有什么疑惑或建议，欢迎在下方评论区继续讨论。**

**如果您觉得本文还不错，欢迎 [分享](https://kexue.fm/archives/10017#share)/ [打赏](https://kexue.fm/archives/10017#pay) 本文。打赏并非要从中获得收益，而是希望知道科学空间获得了多少读者的真心关注。当然，如果你无视它，也不会影响你的阅读。再次表示欢迎和感谢！**

打赏

微信打赏

支付宝打赏

因为网站后台对打赏并无记录，因此欢迎在打赏时候备注留言。你还可以 [**点击这里**](http://mail.qq.com/cgi-bin/qm_share?t=qm_mailme&email=tN7d1drY3drrx8H0xcWa19vZ) 或在下方评论区留言来告知你的建议或需求。

**如果您需要引用本文，请参考：**

苏剑林. (Mar. 18, 2024). 《时空之章：将Attention视为平方复杂度的RNN 》\[Blog post\]. Retrieved from [https://kexue.fm/archives/10017](https://kexue.fm/archives/10017)

@online{kexuefm-10017,
        title={时空之章：将Attention视为平方复杂度的RNN},
        author={苏剑林},
        year={2024},
        month={Mar},
        url={\\url{https://kexue.fm/archives/10017}},
}

分类： [数学研究](https://kexue.fm/category/Mathematics)    标签： [语言模型](https://kexue.fm/tag/%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/), [RNN](https://kexue.fm/tag/RNN/), [attention](https://kexue.fm/tag/attention/), [复杂度](https://kexue.fm/tag/%E5%A4%8D%E6%9D%82%E5%BA%A6/)[38 评论](https://kexue.fm/archives/10017#comments)

< [用傅里叶级数拟合一维概率密度函数](https://kexue.fm/archives/10007) \| [Transformer升级之路：17、多模态位置编码的简单思考](https://kexue.fm/archives/10040) >

### 你也许还对下面的内容感兴趣

- [线性注意力简史：从模仿、创新到反哺](https://kexue.fm/archives/11033)
- [Transformer升级之路：20、MLA好在哪里?（上）](https://kexue.fm/archives/10907)
- [Transformer升级之路：19、第二类旋转位置编码](https://kexue.fm/archives/10862)
- [细水长flow之TARFLOW：流模型满血归来？](https://kexue.fm/archives/10667)
- [“闭门造车”之多模态思路浅谈（三）：位置编码](https://kexue.fm/archives/10352)
- [Decoder-only的LLM为什么需要位置编码？](https://kexue.fm/archives/10347)
- [Monarch矩阵：计算高效的稀疏型矩阵分解](https://kexue.fm/archives/10249)
- [重温SSM（四）：有理生成函数的新视角](https://kexue.fm/archives/10180)
- [重温SSM（三）：HiPPO的高效计算（S4）](https://kexue.fm/archives/10162)
- [重温SSM（二）：HiPPO的一些遗留问题](https://kexue.fm/archives/10137)

[发表你的看法](https://kexue.fm/archives/10017#comment_form)

1. [«](https://kexue.fm/archives/10017/comment-page-1#comments)
2. [1](https://kexue.fm/archives/10017/comment-page-1#comments)
3. [2](https://kexue.fm/archives/10017/comment-page-2#comments)

Lei

May 7th, 2024

苏神，想请教一下为什么说CoT能动态地增加序列长度？

[回复评论](https://kexue.fm/archives/10017/comment-page-2?replyTo=24276#respond-post-10017)

[苏剑林](https://kexue.fm) 发表于
May 13th, 2024

这里指的是模型自己生成CoT，模型可以把所需要的内容逐步写入到生成结果中，自然是增加了长度啊。

[回复评论](https://kexue.fm/archives/10017/comment-page-2?replyTo=24297#respond-post-10017)

[苏剑林](https://kexue.fm)

July 14th, 2024

接 [@allen7575\|comment-24813](https://kexue.fm/archives/10017/comment-page-1#comment-24813)：

1、TTT是利用SGD来构造新的RNN，不管怎么构造，结果始终还是RNN；
2、用SGD来构造的RNN，有一个新的、比较有趣诠释，但它依旧是一个RNN；
3、抛开现象看本质，TTT就是将RNN的state看成是模型的参数，RNN的递归看成是SGD；
4、更本质一点，TTT就是将模型能Cache的部份看成是模型的参数；
5、RNN能Cache的就是state，Attention也有能Cache的地方，就是K、V；
6、所以，你觉得TTT是一个能够边预测边更新参数的模型，Attention也是；
7、特别地，TTT论文里还（强行）证明了Attention就是TTT的一个case；
8、你可以吐槽KV Cache的动态增长效率差/速度慢，但它能力更强几乎是毋庸置疑的。

总的来说，不要“神化”TTT，它就是一种具有特殊诠释的新RNN，简单来说就是——TTT是RNN。

[回复评论](https://kexue.fm/archives/10017/comment-page-2?replyTo=24818#respond-post-10017)

杨博文 发表于
May 21st, 2025

按照这篇文章的证明说Attention是TTT的变种确实没错，只不过把序列长度变成了L^2

[回复评论](https://kexue.fm/archives/10017/comment-page-2?replyTo=27651#respond-post-10017)

[苏剑林](https://kexue.fm) 发表于
May 28th, 2025

倒是可以这样理解。不过TTT论文中是直接用Attention视为序列的非参数核估计器，而RNN则是SGD求解回归问题。

[回复评论](https://kexue.fm/archives/10017/comment-page-2?replyTo=27704#respond-post-10017)

lines

December 24th, 2024

苏神，想请教一下为什么说“以LLAMA2-70B为例，它的num\_layers为80、hidden\_size为8192，两者相乘是640K，再乘个2刚好是1M左右。换句话说，当输入长度达到1M tokens的这个级别，那么LLAMA2-70B的“内存”就可能成为瓶颈”。我没明白的是，为什么中间计算层所需的最小空间能直接对应于seq长度。望解答

[回复评论](https://kexue.fm/archives/10017/comment-page-2?replyTo=26075#respond-post-10017)

[苏剑林](https://kexue.fm) 发表于
December 26th, 2024

模型运算的过程，假设所有context都要用上，那么可以直观地理解为是将输入“搬到”输出（这个“搬”是广义的，指“读取并运算”），中间层的所有hidden states是内存，就是这个搬运过程的瓶颈。

[回复评论](https://kexue.fm/archives/10017/comment-page-2?replyTo=26097#respond-post-10017)

1. [«](https://kexue.fm/archives/10017/comment-page-1#comments)
2. [1](https://kexue.fm/archives/10017/comment-page-1#comments)
3. [2](https://kexue.fm/archives/10017/comment-page-2#comments)

[取消回复](https://kexue.fm/archives/10017#respond-post-10017)

你的大名

电子邮箱

个人网站（选填）

1\. 可以使用LaTeX代码，点击“预览效果”可查看效果；2. 可以通过点击评论楼层编号来引用该楼层；3. 网站可能会有点卡，如非确认评论失败，请 **不要重复点击提交**。

### 内容速览

[超越线性的RNN](https://kexue.fm/archives/10017#%E8%B6%85%E8%B6%8A%E7%BA%BF%E6%80%A7%E7%9A%84RNN)
[注意力也是RNN](https://kexue.fm/archives/10017#%E6%B3%A8%E6%84%8F%E5%8A%9B%E4%B9%9F%E6%98%AFRNN)
[常数空间复杂度](https://kexue.fm/archives/10017#%E5%B8%B8%E6%95%B0%E7%A9%BA%E9%97%B4%E5%A4%8D%E6%9D%82%E5%BA%A6)
[模型能力的思考](https://kexue.fm/archives/10017#%E6%A8%A1%E5%9E%8B%E8%83%BD%E5%8A%9B%E7%9A%84%E6%80%9D%E8%80%83)
[最后再来个小结](https://kexue.fm/archives/10017#%E6%9C%80%E5%90%8E%E5%86%8D%E6%9D%A5%E4%B8%AA%E5%B0%8F%E7%BB%93)

### 智能搜索

支持整句搜索！网站自动使用 [结巴分词](https://github.com/fxsjy/jieba) 进行分词，并结合ngrams排序算法给出合理的搜索结果。

### 热门标签

[生成模型](https://kexue.fm/tag/%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B/) [attention](https://kexue.fm/tag/attention/) [优化](https://kexue.fm/tag/%E4%BC%98%E5%8C%96/) [语言模型](https://kexue.fm/tag/%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/) [模型](https://kexue.fm/tag/%E6%A8%A1%E5%9E%8B/) [网站](https://kexue.fm/tag/%E7%BD%91%E7%AB%99/) [概率](https://kexue.fm/tag/%E6%A6%82%E7%8E%87/) [梯度](https://kexue.fm/tag/%E6%A2%AF%E5%BA%A6/) [转载](https://kexue.fm/tag/%E8%BD%AC%E8%BD%BD/) [微分方程](https://kexue.fm/tag/%E5%BE%AE%E5%88%86%E6%96%B9%E7%A8%8B/) [矩阵](https://kexue.fm/tag/%E7%9F%A9%E9%98%B5/) [天象](https://kexue.fm/tag/%E5%A4%A9%E8%B1%A1/) [分析](https://kexue.fm/tag/%E5%88%86%E6%9E%90/) [深度学习](https://kexue.fm/tag/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/) [积分](https://kexue.fm/tag/%E7%A7%AF%E5%88%86/) [python](https://kexue.fm/tag/python/) [优化器](https://kexue.fm/tag/%E4%BC%98%E5%8C%96%E5%99%A8/) [力学](https://kexue.fm/tag/%E5%8A%9B%E5%AD%A6/) [无监督](https://kexue.fm/tag/%E6%97%A0%E7%9B%91%E7%9D%A3/) [扩散](https://kexue.fm/tag/%E6%89%A9%E6%95%A3/) [几何](https://kexue.fm/tag/%E5%87%A0%E4%BD%95/) [节日](https://kexue.fm/tag/%E8%8A%82%E6%97%A5/) [生活](https://kexue.fm/tag/%E7%94%9F%E6%B4%BB/) [文本生成](https://kexue.fm/tag/%E6%96%87%E6%9C%AC%E7%94%9F%E6%88%90/) [数论](https://kexue.fm/tag/%E6%95%B0%E8%AE%BA/)

### 随机文章

- [如何看费曼的讲义和朗道的教程？](https://kexue.fm/archives/2498)
- [揭开迷雾，来一顿美味的Capsule盛宴](https://kexue.fm/archives/4819)
- [数学基本技艺之23、24（下）](https://kexue.fm/archives/2096)
- [差分方程的摄动法](https://kexue.fm/archives/3889)
- [【问答】为什么绿色星星非常罕见呢?](https://kexue.fm/archives/501)
- [线性微分方程组：已知特解求通解](https://kexue.fm/archives/2644)
- [神奇的麦田圈坐标图片之谜](https://kexue.fm/archives/124)
- [今日晒书](https://kexue.fm/archives/1423)
- [基于fine tune的图像分类（百度分狗竞赛）](https://kexue.fm/archives/4611)
- [又是Dropout两次！这次它做到了有监督任务的SOTA](https://kexue.fm/archives/8496)

### 最近评论

- [Leco](https://kexue.fm/archives/9590/comment-page-2#comment-27984): 请问LoRA的A,B矩阵初始化时，一个高斯随机一个全零还是只能A高斯，B全零呢？
- [苏剑林](https://kexue.fm/archives/9119/comment-page-13#comment-27983): 如果你把你这里提到的数学都学通透了，数学基础基本上可以胜任95%以上的场景了吧？至于“直觉”这...
- [苏剑林](https://kexue.fm/archives/10958/comment-page-2#comment-27982): 我跑过这个项目，效果是能复现的。“在 CIFAR-10 上效果非常差，生成的图片都是模糊的”是...
- [Henry Zha](https://kexue.fm/archives/9119/comment-page-13#comment-27981): 苏神你好，我是一名管理科学与工程专业的博士生，研究方向是结合人工智能模型建模用户行为之类的管理...
- [SunlightZero](https://kexue.fm/archives/10958/comment-page-2#comment-27980): 我根据 https://github.com/haidog-yaqub/MeanFlow 尝试...
- [苏剑林](https://kexue.fm/archives/11025/comment-page-1#comment-27979): 噢，是笔误，更正了，感谢指出。
- [苏剑林](https://kexue.fm/archives/10945/comment-page-1#comment-27978): 这里有很多因素。如果推理数据跟训练数据同分布，那么理论上就是均匀分布，但实际上同分布假设不一定...
- [苏剑林](https://kexue.fm/archives/11033/comment-page-1#comment-27977): 目前看来给O加rmsnorm挺稳的，效果甚至还好点。我其实是直接在flash attentio...
- [苏剑林](https://kexue.fm/archives/11033/comment-page-1#comment-27976): 你应该说的是$\\exp(\\boldsymbol{Q}\\boldsymbol{K}^{\\top}...
- [苏剑林](https://kexue.fm/archives/10831/comment-page-1#comment-27975): 可以这么说吧，通过某种分母归一化的操作，导数格式都类似，毕竟公式$(f/g)'=f'/g-fg...

### 友情链接

- [Cool Papers](https://papers.cool)
- [数学研发](https://bbs.emath.ac.cn)
- [Seatop](http://www.seatop.com.cn/)
- [Xiaoxia](https://xiaoxia.org/)
- [积分表-网络版](https://kexue.fm/sci/integral/index.html)
- [丝路博傲](http://blog.dvxj.com/)
- [ph4ntasy 饭特稀](http://www.ph4ntasy.com/)
- [数学之家](http://www.2math.cn/)
- [有趣天文奇观](http://interesting-sky.china-vo.org/)
- [TwistedW](http://www.twistedwg.com/)
- [godweiyang](https://godweiyang.com/)
- [AI柠檬](https://blog.ailemon.net/)
- [王登科-DK博客](https://greatdk.com)
- [ESON](https://blog.eson.org/)
- [枫之羽](https://fzhiy.net/)
- [Mathor's blog](https://wmathor.com/)
- [coding-zuo](https://coding-zuo.github.io/)
- [博科园](https://www.bokeyuan.net/)
- [孔皮皮的博客](https://www.kppkkp.top/)
- [运鹏的博客](https://yunpengtai.top/)
- [jiming.site](https://jiming.site/)
- [OmegaXYZ](https://www.omegaxyz.com/)
- [Blog by Eacls](https://www.eacls.top/)
- [EAI猩球](https://www.robotech.ink/)
- [文举的博客](https://liwenju0.com/)
- [用代码打点酱油](https://bruceyuan.com/)
- [申请链接](https://kexue.fm/links.html)

本站采用创作共用版权协议，要求署名、非商业用途和保持一致。转载本站内容必须也遵循“ [署名-非商业用途-保持一致](http://creativecommons.org/licenses/by-nc-nd/2.5/cn/)”的创作共用协议。
© 2009-2025 Scientific Spaces. All rights reserved. Theme by [laogui](http://www.laogui.com). Powered by [Typecho](http://typecho.org). 备案号: [粤ICP备09093259号-1/2](https://beian.miit.gov.cn/)。