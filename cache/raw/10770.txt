## SEARCH

## MENU

- [打赏](https://kexue.fm/reward.html)
- [公式](https://kexue.fm/latex.html)
- [天象](https://kexue.fm/ac.html)
- [链接](https://kexue.fm/links.html)
- [时光](https://kexue.fm/me.html)
- [博览](https://kexue.fm/science.html)
- [归档](https://kexue.fm/content.html)

## CATEGORIES

- [千奇百怪](https://kexue.fm/category/Everything)
- [天文探索](https://kexue.fm/category/Astronomy)
- [数学研究](https://kexue.fm/category/Mathematics)
- [物理化学](https://kexue.fm/category/Phy-chem)
- [信息时代](https://kexue.fm/category/Big-Data)
- [生物自然](https://kexue.fm/category/Biology)
- [图片摄影](https://kexue.fm/category/Photograph)
- [问题百科](https://kexue.fm/category/Questions)
- [生活/情感](https://kexue.fm/category/Life-Feeling)
- [资源共享](https://kexue.fm/category/Resources)

## NEWPOSTS

- [线性注意力简史：从模仿、创新到反哺](https://kexue.fm/archives/11033)
- [msign的导数](https://kexue.fm/archives/11025)
- [通过msign来计算mclip（奇...](https://kexue.fm/archives/11006)
- [msign算子的Newton-Sc...](https://kexue.fm/archives/10996)
- [等值振荡定理：最优多项式逼近的充要条件](https://kexue.fm/archives/10972)
- [生成扩散模型漫谈（三十）：从瞬时速...](https://kexue.fm/archives/10958)
- [MoE环游记：5、均匀分布的反思](https://kexue.fm/archives/10945)
- [msign算子的Newton-Sc...](https://kexue.fm/archives/10922)
- [Transformer升级之路：2...](https://kexue.fm/archives/10907)
- [一道概率不等式：盯着它到显然成立为止！](https://kexue.fm/archives/10902)

## COMMENTS

- [忍者猫: 这优化器的作者真的应该给你打钱](https://kexue.fm/archives/10592/comment-page-2#comment-27952)
- [Chaofa Yuan: 写得太好了](https://kexue.fm/archives/11033/comment-page-1#comment-27951)
- [Skyler Lin: respect苏神！](https://kexue.fm/archives/11033/comment-page-1#comment-27949)
- [宋佳铭: 对，个人感觉mean flow就是continuous tim...](https://kexue.fm/archives/10958/comment-page-1#comment-27947)
- [宋佳铭: 的确，对sg这个事情我感觉如果是用‘归纳’法做是不太能避免的，...](https://kexue.fm/archives/10958/comment-page-1#comment-27946)
- [MoFHeka: 苏老师您好，请问一下这套结论在稀疏参数上应该如何应用？比如大规...](https://kexue.fm/archives/10542/comment-page-1#comment-27945)
- [苏剑林: Temp LoRA倒是有印象，其实思想是一样的，如果我单独开一...](https://kexue.fm/archives/11033/comment-page-1#comment-27944)
- [苏剑林: 你搜搜mamba、rwkv甚至rnn做vision的工作，其实...](https://kexue.fm/archives/11033/comment-page-1#comment-27943)
- [苏剑林: 问题1可以看看 https://kexue.fm/archiv...](https://kexue.fm/archives/9379/comment-page-1#comment-27942)
- [苏剑林: 你的“信息量”怎么定义？直观来说，reflow训练的是切线模型...](https://kexue.fm/archives/10958/comment-page-2#comment-27941)

## USERLOGIN

- [登录](https://kexue.fm/admin/login.php)

[科学空间\|Scientific Spaces](https://kexue.fm)

- [登录](https://kexue.fm/admin/login.php)
- [打赏](https://kexue.fm/reward.html)
- [公式](https://kexue.fm/latex.html)
- [天象](https://kexue.fm/ac.html)
- [链接](https://kexue.fm/links.html)
- [时光](https://kexue.fm/me.html)
- [博览](https://kexue.fm/science.html)
- [归档](https://kexue.fm/content.html)

渴望成为一个小飞侠

- [欢迎订阅](https://kexue.fm/feed)
- [个性邮箱](https://kexue.fm/archives/119)
- [天象信息](https://kexue.fm/ac.html)
- [观测ISS](https://kexue.fm/archives/41)
- [LaTeX](https://kexue.fm/latex.html)
- [关于博主](https://kexue.fm/me.html)

欢迎访问“科学空间”，这里将与您共同探讨自然科学，回味人生百态；也期待大家的分享～

- [**千奇百怪** Everything](https://kexue.fm/category/Everything)
- [**天文探索** Astronomy](https://kexue.fm/category/Astronomy)
- [**数学研究** Mathematics](https://kexue.fm/category/Mathematics)
- [**物理化学** Phy-chem](https://kexue.fm/category/Phy-chem)
- [**信息时代** Big-Data](https://kexue.fm/category/Big-Data)
- [**生物自然** Biology](https://kexue.fm/category/Biology)
- [**图片摄影** Photograph](https://kexue.fm/category/Photograph)
- [**问题百科** Questions](https://kexue.fm/category/Questions)
- [**生活/情感** Life-Feeling](https://kexue.fm/category/Life-Feeling)
- [**资源共享** Resources](https://kexue.fm/category/Resources)

- [**千奇百怪**](https://kexue.fm/category/Everything)
- [**天文探索**](https://kexue.fm/category/Astronomy)
- [**数学研究**](https://kexue.fm/category/Mathematics)
- [**物理化学**](https://kexue.fm/category/Phy-chem)
- [**信息时代**](https://kexue.fm/category/Big-Data)
- [**生物自然**](https://kexue.fm/category/Biology)
- [**图片摄影**](https://kexue.fm/category/Photograph)
- [**问题百科**](https://kexue.fm/category/Questions)
- [**生活/情感**](https://kexue.fm/category/Life-Feeling)
- [**资源共享**](https://kexue.fm/category/Resources)

[首页](https://kexue.fm) [数学研究](https://kexue.fm/category/Mathematics) 初探muP：超参数的跨模型尺度迁移规律

13Mar

# [初探muP：超参数的跨模型尺度迁移规律](https://kexue.fm/archives/10770)

By 苏剑林 \|
2025-03-13 \|
26480位读者\|

众所周知，完整训练一次大型LLM的成本是昂贵的，这就决定了我们不可能直接在大型LLM上反复测试超参数。一个很自然的想法是希望可以在同结构的小模型上仔细搜索超参数，找到最优组合后直接迁移到大模型上。尽管这个想法很朴素，但要实现它并不平凡，它需要我们了解常见的超参数与模型尺度之间的缩放规律，而muP正是这个想法的一个实践。

muP，有时也写$\\mu P$，全名是Maximal Update Parametrization，出自论文 [《Tensor Programs V: Tuning Large Neural Networks via Zero-Shot Hyperparameter Transfer》](https://papers.cool/arxiv/2203.03466)，随着LLM训练的普及，它逐渐已经成为了科学炼丹的事实标配之一。

## 方法大意 [\#](https://kexue.fm/archives/10770\#%E6%96%B9%E6%B3%95%E5%A4%A7%E6%84%8F)

在接入主题之前，必须先吐槽一下muP原论文写得实在太过晦涩，并且结论的表达也不够清晰，平白增加了不少理解难度，所以接下来笔者尽量以一种（自认为）简明扼要的方式来复现muP的结论。

先说结论，muP主要研究超参数跨模型尺度的迁移规律。这里有几个关键词：

> 1、超参数，目前主要指 **学习率**；
>
> 2、模型尺度，目前主要是模型 **宽度**；
>
> 3、这里的核心是“ **迁移**”。

请注意，muP不研究什么是最优的超参数，只研究最优超参数随着模型尺度的变化规律，所以我们需要在某个小模型上搜索最优的超参数组合，然后迁移到大模型上，这就是muP的使用场景和使用方法。

推导muP的原理是让模型的前向传播、反向传播、损失增量和特征变化都不随模型尺度的变化而发生明显变化：

> 1、具体做法是分析初始化的数量级，然后认为结论可以代表后续优化的规律；
>
> 2、说白了就是假设做好初始化，后面就会自动沿着正确的轨迹走（好的开始是成功的一大半？）;
>
> 3、当然也可以给这个假设讲 **大数定律** 或 **中心极限定理** 的故事，但个人认为非必须。

## 前向传播 [\#](https://kexue.fm/archives/10770\#%E5%89%8D%E5%90%91%E4%BC%A0%E6%92%AD)

我们从前向传播开始讨论，因为这是相对简单且成熟的部分。首先，考虑线性层$\\boldsymbol{Y}=\\boldsymbol{X}\\boldsymbol{W}$，其中$\\boldsymbol{X}\\in\\mathbb{R}^{b\\times d\_{in}},\\boldsymbol{W}\\in\\mathbb{R}^{d\_{in}\\times d\_{out}}$。我们用RMS（Root Mean Square）来作为矩阵尺度的指标，例如
\\begin{equation}\\text{RMS}(\\boldsymbol{W}) = \\sqrt{\\frac{1}{d\_{in} d\_{out}}\\sum\_{i=1}^{d\_{in}} \\sum\_{j=1}^{d\_{out}} W\_{i,j}^2}\\end{equation}

我们知道，要让初始化阶段$\\boldsymbol{X}$的RMS跟$\\boldsymbol{Y}$的RMS大致相等（简称“ **稳定**”），那么$\\boldsymbol{W}$要用：

> **LeCun初始化**：“均值为0、方差为$1/d\_{in}$”的随机初始化。

这已经算是深度学习的基础结论之一，所以不再展开推导，还不大了解的读者可以参考以往的 [《从几何视角来理解模型参数的初始化策略》](https://kexue.fm/archives/7180)、 [《浅谈Transformer的初始化、参数化与标准化》](https://kexue.fm/archives/8620) 等博文。

接着，我们考虑非线性层$\\boldsymbol{Y}=\\phi(\\boldsymbol{X}\\boldsymbol{W})$，其中$\\phi$是Element-wise的激活函数。如果还是要维持$\\boldsymbol{X}$的RMS跟$\\boldsymbol{Y}$的RMS近似相等，那么结果会稍有不同，比如$\\text{relu}$激活时我们得到

> **Kaiming初始化**：“均值为0、方差为$2/d\_{in}$”的随机初始化。

容易看出， **Kaiming初始化** 跟 **LeCun初始化** 相比，只是方差相差一个（跟模型尺度无关的）常数2，可以证明其他激活函数的结果也类似。所以我们可以下一个结论：

> **fan\_in初始化**：要保证前向传播的稳定性，那么应该要用“均值为0、方差正比于$1/d\_{in}$”的随机初始化。

这个结论也可以理解为“激活函数的影响是模型尺度无关的”，所以如果我们只想分析模型尺度的效应，那么可以忽略（Element-wise的）激活函数的存在，由LeCun初始化直接得到缩放规律$\\propto 1/d\_{in}$。

## 反向传播 [\#](https://kexue.fm/archives/10770\#%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD)

现在我们继续分析反向传播（梯度），注意这里约定变量及其梯度具有相同的shape，那么可以算得
\\begin{align}
\\frac{\\partial\\mathcal{L}}{\\partial \\boldsymbol{W}} =&\\, \\boldsymbol{X}^{\\top}\\left(\\frac{\\partial\\mathcal{L}}{\\partial \\boldsymbol{Y}}\\otimes \\phi'(\\boldsymbol{X}\\boldsymbol{W})\\right) \\\\[5pt\]
\\frac{\\partial\\mathcal{L}}{\\partial \\boldsymbol{X}} =&\\, \\left(\\frac{\\partial\\mathcal{L}}{\\partial \\boldsymbol{Y}}\\otimes \\phi'(\\boldsymbol{X}\\boldsymbol{W})\\right)\\boldsymbol{W}^{\\top}
\\end{align}
第一个公式是当前层内参数的梯度，第二个公式则是该层往前传播的梯度，$\\otimes$是Hadamard积，$\\phi'$是$\\phi$的导函数。

注意到一个事实：我们常用的激活函数，其导数都可以被一个（尺度无关的）常数给Bound住，所以至少在数量级上我们可以写出
\\begin{align}
\\frac{\\partial\\mathcal{L}}{\\partial \\boldsymbol{W}} =&\\, \\boldsymbol{X}^{\\top}\\left(\\frac{\\partial\\mathcal{L}}{\\partial \\boldsymbol{Y}}\\otimes \\phi'(\\boldsymbol{X}\\boldsymbol{W})\\right) \\sim \\boldsymbol{X}^{\\top}\\frac{\\partial\\mathcal{L}}{\\partial \\boldsymbol{Y}} \\label{eq:grad-w}\\\\[5pt\]
\\frac{\\partial\\mathcal{L}}{\\partial \\boldsymbol{X}} =&\\, \\left(\\frac{\\partial\\mathcal{L}}{\\partial \\boldsymbol{Y}}\\otimes \\phi'(\\boldsymbol{X}\\boldsymbol{W})\\right)\\boldsymbol{W}^{\\top}\\sim \\frac{\\partial\\mathcal{L}}{\\partial \\boldsymbol{Y}}\\boldsymbol{W}^{\\top}\\label{eq:grad-x}
\\end{align}
我们先来看第二个公式，跟$\\boldsymbol{Y}=\\boldsymbol{X}\\boldsymbol{W}$相比，它右端乘的矩阵变成了$\\boldsymbol{W}^{\\top}$，那么按照上一节的结论，如果要保持反向传播的RMS稳定性，那么$\\boldsymbol{W}$的初始化就应该是：

> **fan\_out初始化**：“均值为0、方差为$1/d\_{out}$”的随机初始化。

当$d\_{in}\\neq d\_{out}$时，前向传播和反向传播的要求就出现冲突，这时候有人提了一个折中策略：

> **Xavier初始化**：“均值为0、方差为$2/(d\_{in} + d\_{out})$”的随机初始化。

这也叫“ **fan\_avg初始化**”，因为就是将$d\_{in}$和$d\_{out}$简单代数平均了一下，其他平均方式也可以考虑，参考 [《初始化方法中非方阵的维度平均策略思考》](https://kexue.fm/archives/8725)。Xavier初始化看上去同时兼顾了前向和反向，但也可以说两者都没兼顾，更好的办法是设计模型让大部分参数都是方阵，如后面讨论的模型簇$\\eqref{eq:model}$。

## 损失增量 [\#](https://kexue.fm/archives/10770\#%E6%8D%9F%E5%A4%B1%E5%A2%9E%E9%87%8F)

有了前向传播和反向传播的铺垫，我们就可以尝试分析损失函数的增量了。考虑$\\boldsymbol{W}\\to \\boldsymbol{W} + \\Delta\\boldsymbol{W}$时损失函数的变化量
\\begin{equation}\\Delta \\mathcal{L} = \\mathcal{L}(\\boldsymbol{W} + \\Delta\\boldsymbol{W}) - \\mathcal{L}(\\boldsymbol{W})\\approx \\left\\langle\\frac{\\partial\\mathcal{L}}{\\partial \\boldsymbol{W}}, \\Delta\\boldsymbol{W}\\right\\rangle\_F\\end{equation}
这里的$\\langle\\cdot,\\cdot\\rangle\_F$是Frobenius内积，即把矩阵展平成向量后算向量内积。考虑梯度下降$\\Delta\\boldsymbol{W} = -\\eta \\frac{\\partial\\mathcal{L}}{\\partial \\boldsymbol{W}}$，这里$\\eta$自然是学习率，结合式$\\eqref{eq:grad-w}$，我们有
$$\\begin{equation}\\Delta \\mathcal{L}\\approx -\\eta\\left\\Vert\\frac{\\partial\\mathcal{L}}{\\partial \\boldsymbol{W}}\\right\\Vert\_F^2\\sim -\\eta \\left\\Vert\\boldsymbol{X}^{\\top}\\frac{\\partial\\mathcal{L}}{\\partial \\boldsymbol{Y}}\\right\\Vert\_F^2\\end{equation}$$
事实上，这个式子已经告诉了我们同一个学习率$\\eta$不能跨模型尺度使用的原因：

> 1、$\\boldsymbol{X}^{\\top}\\frac{\\partial\\mathcal{L}}{\\partial \\boldsymbol{Y}}$是一个$d\_{in}\\times d\_{out}$的矩阵；
>
> 2、$\\left\\Vert\\boldsymbol{X}^{\\top}\\frac{\\partial\\mathcal{L}}{\\partial \\boldsymbol{Y}}\\right\\Vert\_F^2$是$d\_{in}\\times d\_{out}$个数的平方和；
>
> 3、$\\boldsymbol{X}^{\\top}\\frac{\\partial\\mathcal{L}}{\\partial \\boldsymbol{Y}}$正好是前向和反向的乘积；
>
> 4、如果前向和反向都稳定，那么$\\boldsymbol{X}^{\\top}\\frac{\\partial\\mathcal{L}}{\\partial \\boldsymbol{Y}}$每个元素都是$\\mathcal{O}(1)$；
>
> 5、所以$\\left\\Vert\\boldsymbol{X}^{\\top}\\frac{\\partial\\mathcal{L}}{\\partial \\boldsymbol{Y}}\\right\\Vert\_F^2$就是$\\mathcal{O}(d\_{in} d\_{out})$。

第4点可能要多加评述一下。$\\boldsymbol{X}^{\\top}$是一个$d\_{in}\\times b$矩阵，$\\frac{\\partial\\mathcal{L}}{\\partial \\boldsymbol{Y}}$是一个$b\\times d\_{out}$矩阵，两者相乘就是$d\_{in} d\_{out}$个$b$维向量对做内积，内积是$b$项求和，而损失$\\mathcal{L}$通常是对样本求平均（即包含了除以$b$操作），所以如果$\\boldsymbol{X}^{\\top}$和$\\frac{\\partial\\mathcal{L}}{\\partial \\boldsymbol{Y}}$都是尺度无关的，那么它们乘起来基本也是尺度无关的【即RMS都是$\\mathcal{O}(1)$】。

最后的结论表明，如果我们直接将小模型的学习率用于大模型，那么对于足够大的模型，它的每一步损失增量就会随着参数尺度（即$d\_{in} d\_{out}$）的变大而**爆炸**，这意味着没法复制小模型的收敛过程，甚至可能因为步子迈得太大导致无法收敛。

此时大家可能想到的一个做法是让$\\eta\\propto 1/(d\_{in} d\_{out})$来缩放$\\Delta\\mathcal{L}$，事实上这个想法已经跟上了muP的思路，但实际场景中由于前面说的前向和反向的不兼容性，导致第4点“如果前向和反向都稳定，那么$\\boldsymbol{X}^{\\top}\\frac{\\partial\\mathcal{L}}{\\partial \\boldsymbol{Y}}$每个元素就是$\\mathcal{O}(1)$”不能总是成立，所以实际情况更为复杂一些，

## 模型假设 [\#](https://kexue.fm/archives/10770\#%E6%A8%A1%E5%9E%8B%E5%81%87%E8%AE%BE)

现在让我们考虑一个更接近实践的场景。我们的任务是训练一个$\\mathbb{R}^{d\_{in}}\\mapsto \\mathbb{R}^{d\_{out}}$的模型，其中$d\_{in},d\_{out}$是数据决定的，不可改变。开头我们就说了，muP旨在研究超参数随着模型尺度的缩放规律，所以一切固定不变的量，都相当于是常数或者说$\\mathcal{O}(1)$，比如初始化方差为$1/d\_{in}$，等价于说初始化方差为$\\mathcal{O}(1)$。

我们可以改变的是模型的架构、参数量等部分，但muP主要考虑宽度的规律，所以我们把模型的架构定一下。这里主要考虑的模型簇是：
\\begin{equation}\\begin{gathered}
\\boldsymbol{Y}\_{in} = \\boldsymbol{X} \\boldsymbol{W}\_{in} \\\\[5pt\]
\\boldsymbol{Y}\_{out} = \\text{NN}(\\boldsymbol{Y}\_{in},\\boldsymbol{\\Theta}) \\\\[5pt\]
\\boldsymbol{Z} = \\boldsymbol{Y}\_{out} \\boldsymbol{W}\_{out}
\\end{gathered}\\label{eq:model}\\end{equation}

其中：

> 1、$\\boldsymbol{X}\\in\\mathbb{R}^{b\\times d\_{in}}$（带上了batch size）；
>
> 2、$\\boldsymbol{W}\_{in} \\in \\mathbb{R}^{d\_{in}\\times d}, \\boldsymbol{W}\_{out} \\in \\mathbb{R}^{d\\times d\_{out}}$；
>
> 3、$\\text{NN}$是任意$\\mathbb{R}^d\\mapsto \\mathbb{R}^d$的神经网络；
>
> 4、这里$d$其实就是我们常说的hidden size；
>
> 5、我们可以随意调大$d$，来提升模型的参数量和潜力；
>
> 6、muP就是想研究超参数关于$d$的变化规律。

更具体一点，这里我们考虑的$\\text{NN}$是$K$层MLP：
\\begin{equation}\\begin{aligned}
\\boldsymbol{Y}\_0 =&\\, Y\_{in} \\\\[5pt\]
\\boldsymbol{Y}\_{k+1} =&\\, \\phi(\\boldsymbol{Y}\_k \\boldsymbol{W}\_{k+1}) \\\\[5pt\]
\\boldsymbol{Y}\_{out} =&\\, \\boldsymbol{Y}\_K
\\end{aligned}\\end{equation}
这里$\\boldsymbol{\\Theta}=\\{\\boldsymbol{W}\_1,\\boldsymbol{W}\_2,\\cdots,\\boldsymbol{W}\_K\\}$，$\\boldsymbol{W}\_k\\in\\mathbb{R}^{d\\times d}$，即都是$d\\times d$的 **方阵**，全都用 **fan\_in初始化**（等价地，也是 **fan\_out初始化**）。

补充一下，这里约定所有参数矩阵都是$d\\times d$方阵，纯粹是为了简化分析，并不是强制要求。因为这里真正的目的是假设$\\text{NN}$的参数里没有尺度无关的形状，比如不允许$d\\times 64$这样的形状，因为$64$是一个常数，但$d\\times 4d$这样的形状是允许的，因为你不管fan\_in、fan\_out或fan\_avg初始化，方差都是正比于$1/d$。

## 组装起来 [\#](https://kexue.fm/archives/10770\#%E7%BB%84%E8%A3%85%E8%B5%B7%E6%9D%A5)

确立后具体模型后，我们就可以把前面的结论都组装起来了。要更新的参数分为$\\boldsymbol{W}\_{in},\\boldsymbol{\\Theta},\\boldsymbol{W}\_{out}$三部分，分别求梯度：
\\begin{align}
\\frac{\\partial\\mathcal{L}}{\\partial \\boldsymbol{W}\_{out}} =&\\, \\boldsymbol{Y}\_{out}^{\\top}\\frac{\\partial\\mathcal{L}}{\\partial \\boldsymbol{Z}} \\\\[6pt\]
\\frac{\\partial\\mathcal{L}}{\\partial \\boldsymbol{W}\_k} =&\\, \\frac{\\partial \\boldsymbol{Y}\_{out}}{\\partial \\boldsymbol{W}\_k} \\cdot\\frac{\\partial\\mathcal{L}}{\\partial \\boldsymbol{Y}\_{out}} = \\frac{\\partial \\boldsymbol{Y}\_{out}}{\\partial \\boldsymbol{W}\_k} \\cdot\\left(\\frac{\\partial\\mathcal{L}}{\\partial \\boldsymbol{Z}}\\boldsymbol{W}\_{out}^{\\top}\\right) \\\\[6pt\]
\\frac{\\partial\\mathcal{L}}{\\partial \\boldsymbol{W}\_{in}} =&\\, \\boldsymbol{X}^{\\top} \\frac{\\partial\\mathcal{L}}{\\partial \\boldsymbol{Y}\_{in}} = \\boldsymbol{X}^{\\top} \\left(\\frac{\\partial\\boldsymbol{Y}\_{out}}{\\partial \\boldsymbol{Y}\_{in}}\\cdot\\frac{\\partial\\mathcal{L}}{\\partial \\boldsymbol{Y}\_{out}}\\right) = \\boldsymbol{X}^{\\top} \\left(\\frac{\\partial\\boldsymbol{Y}\_{out}}{\\partial \\boldsymbol{Y}\_{in}}\\cdot\\left(\\frac{\\partial\\mathcal{L}}{\\partial \\boldsymbol{Z}}\\boldsymbol{W}\_{out}^{\\top}\\right)\\right) \\\\[6pt\]
\\end{align}

这里的$\\cdot$运算需要稍微解释一下：$\\boldsymbol{Y}\_{in},\\boldsymbol{Y}\_{out}$都是一个矩阵，所以$\\frac{\\partial\\boldsymbol{Y}\_{out}}{\\partial \\boldsymbol{Y}\_{in}}$原则上是一个四阶张量，链式法则$\\frac{\\partial\\boldsymbol{Y}\_{out}}{\\partial \\boldsymbol{Y}\_{in}}\\cdot\\frac{\\partial\\mathcal{L}}{\\partial \\boldsymbol{Y}\_{out}}$实际是高阶张量的乘法，但这里不打算展开介绍了，所以简单用一个$\\cdot$代替，读者只需要知道它是矩阵乘法的一般推广就行。

现在来观察规律：

> 1、三个式子都有$\\frac{\\partial\\mathcal{L}}{\\partial \\boldsymbol{Z}}$；
>
> 2、后两式都有$\\boldsymbol{W}\_{out}^{\\top}$；
>
> 3、$\\boldsymbol{W}\_k$里都是方阵，$\\frac{\\partial\\boldsymbol{Y}\_{out}}{\\partial \\boldsymbol{Y}\_{in}}$和$\\frac{\\partial \\boldsymbol{Y}\_{out}}{\\partial \\boldsymbol{W}\_k}$都是稳定的【RMS是$\\mathcal{O}(1)$】；
>
> 4、如果$\\boldsymbol{W}\_{in}$也用fan\_in初始化，那么$\\boldsymbol{Y}\_{out}$也是稳定的；
>
> 5、要想$\\frac{\\partial\\mathcal{L}}{\\partial \\boldsymbol{Z}}\\boldsymbol{W}\_{out}^{\\top}$稳定，那么初始化方差是$1/d\_{out}$，但$d\_{out}$是尺度无关的，相当于常数。

这样一来：

> 1、$\\frac{\\partial\\mathcal{L}}{\\partial \\boldsymbol{W}\_{out}}$的RMS是$\\mathcal{O}(1)$，$\\left\\Vert\\frac{\\partial\\mathcal{L}}{\\partial \\boldsymbol{W}\_{out}}\\right\\Vert\_F^2$是$d\\times d\_{out}$个数平方和，所以大小是$\\mathcal{O}(d\\times d\_{out})$，别忘了$d\_{out}$是常数，所以实际上就是$\\mathcal{O}(d)$，于是为了得到$\\mathcal{O}(1)$的$\\Delta\\mathcal{L}$，它的学习率要满足$\\eta\_{out}\\propto 1/d$；
>
> 2、$\\left\\Vert\\frac{\\partial\\mathcal{L}}{\\partial \\boldsymbol{W}\_k}\\right\\Vert\_F^2$是$d^2$个数求和，$\\frac{\\partial \\boldsymbol{Y}\_{out}}{\\partial \\boldsymbol{W}\_k}$和$\\frac{\\partial\\mathcal{L}}{\\partial \\boldsymbol{Z}}$的RMS都是$\\mathcal{O}(1)$，我们直接将$\\boldsymbol{W}\_{out}$的初始化方差设为$\\propto 1/d^2$，那么$\\frac{\\partial\\mathcal{L}}{\\partial \\boldsymbol{W}\_k}$的RMS就是$\\mathcal{O}(1/d)$，平方求和后就正好是$\\mathcal{O}(1)$，因此学习率不用变化；
>
> 3、此时$\\frac{\\partial\\mathcal{L}}{\\partial \\boldsymbol{W}\_{in}}$的RMS也是$\\mathcal{O}(1/d)$，但$\\left\\Vert\\frac{\\partial\\mathcal{L}}{\\partial \\boldsymbol{W}\_{in}}\\right\\Vert\_F^2$只是$d\_{in}\\times d$个数平方和，所以结果是$\\mathcal{O}(1/d)$的，为了得到$\\mathcal{O}(1)$的$\\Delta\\mathcal{L}$，学习率反而需要放大$d$倍来抵消这个影响，即$\\eta\_{in}\\propto d$。

## 特征变化 [\#](https://kexue.fm/archives/10770\#%E7%89%B9%E5%BE%81%E5%8F%98%E5%8C%96)

以上结果是没有问题的，但仔细思考我们会发现推导过程的一个问题：上面的第2、3点，都建立在“我们直接将$\\boldsymbol{W}\_{out}$的初始化方差设为$\\propto 1/d^2$”这个设置上，然而这个设置目前来说并没有直接的依据。如果不对此进一步解释，那么推导过程还是不够完备的。

事实上，单看$\\Delta \\mathcal{L}=\\mathcal{O}(1)$这个要求的话，确实是无法排除其他选择的可能性的，比如$\\boldsymbol{W}\_{out}$的初始化方差设为$\\propto 1/d$，此时$\\frac{\\partial\\mathcal{L}}{\\partial \\boldsymbol{W}\_k}$的RMS是$\\mathcal{O}(1/\\sqrt{d})$，平方求和后是$\\mathcal{O}(d)$，那么只要学习率$\\eta\\propto 1/d$同样可以实现$\\Delta \\mathcal{L}=\\mathcal{O}(1)$。因此，为了解释“$\\boldsymbol{W}\_{out}$的初始化方差设为$\\propto 1/d^2$”的必要性，那么就需要引入新的条件。

损失函数$\\mathcal{L}$是模型的一个宏观指标，或者说外部指标，单看它的变化已经不足以解释全部结果了，那么就需要细化到模型内部了。具体来说，我们希望模型每一层的输出（通常也称为特征，有时也称激活值）变化量也具有尺度不变性。比如线性层$\\boldsymbol{Y}\_k = \\boldsymbol{Y}\_{k-1} \\boldsymbol{W}\_k$，参数$\\boldsymbol{W}\_k\\to \\boldsymbol{W}\_k + \\Delta \\boldsymbol{W}\_k$带来的输出变化是
\\begin{equation}\\Delta\\boldsymbol{Y}\_k = \\boldsymbol{Y}\_{k-1} (\\boldsymbol{W}\_k + \\Delta \\boldsymbol{W}\_k) - \\boldsymbol{Y}\_{k-1} \\boldsymbol{W}\_k = \\boldsymbol{Y}\_{k-1} \\Delta\\boldsymbol{W}\_k\\end{equation}
注意$\\boldsymbol{Y}\_{k-1}\\in\\mathbb{R}^{b\\times d},\\Delta\\boldsymbol{W}\_k\\in\\mathbb{R}^{d\\times d}$，所以$\\boldsymbol{Y}\_{k-1} \\Delta\\boldsymbol{W}\_k$就是$b\\times d$个$d$维向量对的内积。注意这里$\\Delta\\boldsymbol{W}\_k$是精心设计的更新量，它不大可能跟初始化那样跟$\\boldsymbol{Y}\_{k-1}$是独立的，所以“$d$维向量对的内积”更有可能是$\\mathcal{O}(d)$（$d$维内积共有$d$项求和），因此如果$\\Delta\\boldsymbol{Y}\_{k-1}$的RMS是$\\mathcal{O}(1)$，那么可以认为$\\Delta\\boldsymbol{Y}\_k$的RMS将是$\\mathcal{O}(d\\times \\text{RMS}(\\Delta \\boldsymbol{W}\_k))$。

于是，为了让$\\Delta\\boldsymbol{Y}\_k$的RMS是$\\mathcal{O}(1)$，我们得到了对$\\Delta \\boldsymbol{W}\_k$的一个额外要求：
\\begin{equation}\\text{RMS}(\\Delta \\boldsymbol{W}\_k) = \\mathcal{O}(1 / d)\\label{eq:dw-rms}\\end{equation}

结合$\\Delta \\boldsymbol{W}\_k = -\\eta\\frac{\\partial\\mathcal{L}}{\\partial \\boldsymbol{W}\_k}$和$\\Delta\\mathcal{L}=\\mathcal{O}(1)$，我们就可以得到“$\\boldsymbol{W}\_{out}$的初始化方差设为$\\propto 1/d^2$”的结果。

（注：这一节依赖于 [@Chenyu Zheng](https://kexue.fm/archives/10770/comment-page-1#comment-27212) 的指点，非常感谢！）

## Adam版本 [\#](https://kexue.fm/archives/10770\#Adam%E7%89%88%E6%9C%AC)

以上就是SGD的muP，对于Adam，我们通常用SignSGD近似做数量级分析：

> 1、$\\Delta \\boldsymbol{W} = -\\eta \\mathop{\\text{sign}}\\left(\\frac{\\partial\\mathcal{L}}{\\partial \\boldsymbol{W}}\\right)$；
>
> 2、$\\Delta \\mathcal{L} \\approx -\\eta \\left\|\\frac{\\partial\\mathcal{L}}{\\partial \\boldsymbol{W}}\\right\|\_1$；
>
> 3、这里的$\|\\cdot\|\_1$指每个元素取绝对值然后求和。

关于SignSGD近似本身，读者还可以参考 [《当Batch Size增大时，学习率该如何随之变化？》](https://kexue.fm/archives/10542)、 [《Adam的epsilon如何影响学习率的Scaling Law？》](https://kexue.fm/archives/10563) 等文章，这里也不展开讨论了。总而言之，SignSGD是分析Adam相关缩放规律时一个常用的近似方式。

现在可以模仿SGD的过程进行分析：

> 1、$\\frac{\\partial\\mathcal{L}}{\\partial \\boldsymbol{W}\_{out}}$的RMS是$\\mathcal{O}(1)$，$\\left\|\\frac{\\partial\\mathcal{L}}{\\partial \\boldsymbol{W}\_{out}}\\right\|\_1$是$d\\times d\_{out}$个数求和，大小是$\\mathcal{O}(d\\times d\_{out}) = \\mathcal{O}(d)$，所以它的学习率要满足$\\eta\_{out}\\propto 1/d$来抵消尺度影响；
>
> 2、$\\left\|\\frac{\\partial\\mathcal{L}}{\\partial \\boldsymbol{W}\_k}\\right\|\_1$是$d^2$个数求和，$\\frac{\\partial \\boldsymbol{Y}\_{out}}{\\partial \\boldsymbol{W}\_k}$和$\\frac{\\partial\\mathcal{L}}{\\partial \\boldsymbol{Z}}$的RMS都是$\\mathcal{O}(1)$，我们将$\\boldsymbol{W}\_{out}$的初始方差设为$\\propto 1/d^2$，那么$\\frac{\\partial\\mathcal{L}}{\\partial \\boldsymbol{W}\_k}$的RMS就是$\\mathcal{O}(1/d)$，$d^2$个数求和后是$\\mathcal{O}(d)$，所以学习率按照$\\eta\_k\\propto 1/d$变换来抵消尺度影响；
>
> 3、此时$\\frac{\\partial\\mathcal{L}}{\\partial \\boldsymbol{W}\_{in}}$的RMS也是$\\mathcal{O}(1/d)$，但$\\left\|\\frac{\\partial\\mathcal{L}}{\\partial \\boldsymbol{W}\_{in}}\\right\|\_1$只是$d\_{in}\\times d$个数求和，所以它已经是$\\mathcal{O}(1)$，从而学习率不用随尺度改变。

（注：读者可以自行检查一下式$\\eqref{eq:dw-rms}$是满足的。）

## Muon版本 [\#](https://kexue.fm/archives/10770\#Muon%E7%89%88%E6%9C%AC)

接下来自然少不了Muon的分析。对于Muon本身，我们已经在 [《Muon优化器赏析：从向量到矩阵的本质跨越》](https://kexue.fm/archives/10592)、 [《Muon续集：为什么我们选择尝试Muon？》](https://kexue.fm/archives/10739) 做了详细介绍，这里不再重复。跟Adam用SignSGD类似，我们用MSignSGD来近似Muon：

> 1、$\\Delta \\boldsymbol{W} = -\\eta \\mathop{\\text{msign}}\\left(\\frac{\\partial\\mathcal{L}}{\\partial \\boldsymbol{W}}\\right)$；
>
> 2、$\\Delta \\mathcal{L} \\approx -\\eta \\left\\Vert\\frac{\\partial\\mathcal{L}}{\\partial \\boldsymbol{W}}\\right\\Vert\_\*$（证明见 [《Muon优化器赏析：从向量到矩阵的本质跨越》](https://kexue.fm/archives/10592)）；
>
> 3、这里的$\\Vert\\cdot\\Vert\_\*$指 [Nuclear范数](https://en.wikipedia.org/wiki/Nuclear_norm)，是矩阵的所有奇异值之和；
>
> 4、Nuclear范数并不好算，但$F$范数好算，它等于矩阵的所有奇异值的平方和的平方根；
>
> 5、我们用$F$范数作为Nuclear范数近似，因此$\\Delta \\mathcal{L} \\approx -\\eta \\left\\Vert\\frac{\\partial\\mathcal{L}}{\\partial \\boldsymbol{W}}\\right\\Vert\_\*\\approx -\\eta \\left\\Vert\\frac{\\partial\\mathcal{L}}{\\partial \\boldsymbol{W}}\\right\\Vert\_F$；
>
> 6、$F$范数又等于矩阵的所有元素的平方和的平方根。

那么可以开始分析过程：

> 1、$\\frac{\\partial\\mathcal{L}}{\\partial \\boldsymbol{W}\_{out}}$的RMS是$\\mathcal{O}(1)$，所以$\\left\\Vert\\frac{\\partial\\mathcal{L}}{\\partial \\boldsymbol{W}\_{out}}\\right\\Vert\_\*$大小是$\\mathcal{O}(\\sqrt{d\\times d\_{out}}) = \\mathcal{O}(\\sqrt{d})$，要消除尺度的影响，那么它的学习率要满足$\\eta\_{out}\\propto 1/\\sqrt{d}$；
>
> 2、$\\left\\Vert\\frac{\\partial\\mathcal{L}}{\\partial \\boldsymbol{W}\_k}\\right\\Vert\_F$是$d^2$个数的平方和的平方根，$\\frac{\\partial \\boldsymbol{Y}\_{out}}{\\partial \\boldsymbol{W}\_k}$和$\\frac{\\partial\\mathcal{L}}{\\partial \\boldsymbol{Z}}$的RMS都是$\\mathcal{O}(1)$，我们将$\\boldsymbol{W}\_{out}$的初始方差设为$\\propto 1/d^2$，那么$\\frac{\\partial\\mathcal{L}}{\\partial \\boldsymbol{W}\_k}$的RMS就是$\\mathcal{O}(1/d)$，平方和后再平方根，结果是$\\mathcal{O}(1)$，所以学习率不用变；
>
> 3、此时$\\frac{\\partial\\mathcal{L}}{\\partial \\boldsymbol{W}\_{in}}$的RMS也是$\\mathcal{O}(1/d)$，但$\\left\\Vert\\frac{\\partial\\mathcal{L}}{\\partial \\boldsymbol{W}\_{in}}\\right\\Vert\_F$只是$d\_{in}\\times d$个数的平方和平方根，所以它是$\\mathcal{O}(1/\\sqrt{d})$的，学习率反而需要放大$\\sqrt{d}$倍来抵消这个影响，即$\\eta\_{in}\\propto \\sqrt{d}$。

（注：这里Muon的结论是对的，但它不满足条件$\\eqref{eq:dw-rms}$，因为式$\\eqref{eq:dw-rms}$要细说的话还依赖于一个更新量是Element-wise的假设，而Muon不符合这个假设，所以实际上不可用。这里没有仔细展开相关讨论，而是直接沿用了“$\\boldsymbol{W}\_{out}$的初始化方差设为$\\propto 1/d^2$”的结论，回避了式$\\eqref{eq:dw-rms}$。）

## 结论汇总 [\#](https://kexue.fm/archives/10770\#%E7%BB%93%E8%AE%BA%E6%B1%87%E6%80%BB)

将上述结论汇总在一起是：
\\begin{array}{c\|c\|c\|c\|c\|c\|c}
\\hline
& \\boldsymbol{W}\_{in}\\text{方差} & \\boldsymbol{W}\_{in}\\text{学习率} & \\boldsymbol{W}\_k\\text{方差} & \\boldsymbol{W}\_k\\text{学习率} & \\boldsymbol{W}\_{out}\\text{方差} & \\boldsymbol{W}\_{out}\\text{学习率} \\\
\\hline
\\text{SGD} & 1/d\_{in} & d & 1 / d & 1 & 1/d^2 & 1 / d\\\
\\text{Adam} & 1/d\_{in} & 1 & 1 / d & 1 / d & 1/d^2 & 1 / d\\\
\\text{Muon} & 1/d\_{in} & \\sqrt{d} & 1 / d & 1 & 1/d^2 & 1 / \\sqrt{d} \\\
\\hline
\\end{array}

这里的$\\boldsymbol{W}\_k$指的是除$\\boldsymbol{W}\_{in},\\boldsymbol{W}\_{out}$外的所有参数，还有要强调的是，这里的关系都是“正比于”而不是“等于”。另外实践中可以根据具体需求稍作变化，比如实际我们用Muon时，$\\boldsymbol{W}\_{in}$和$\\boldsymbol{W}\_{out}$的优化通常不用Muon而是用Adam，这将导致两个变化：

> 1、$\\eta\_{out}\\propto 1/d$；
>
> 2、$\\eta\_{in}$不变。

如果结合我们在 [《Muon is Scalable for LLM Training》](https://papers.cool/arxiv/2502.16982) 所提的Adujst LR的话，那么学习率要多乘一个$\\sqrt{\\max(n, m)}$，$n\\times m$是参数矩阵的形状，我们已经假设了$\\text{NN}$部分的参数总等比例缩放，所以$\\sqrt{\\max(n, m)}\\propto \\sqrt{d}$。因此，如果要抵消Adujst LR带来的尺度影响，那么就需要

> 3、$\\eta\_k\\propto 1/\\sqrt{d}$ 。

## 文章小结 [\#](https://kexue.fm/archives/10770\#%E6%96%87%E7%AB%A0%E5%B0%8F%E7%BB%93)

本文以尽可能简明清晰的方式介绍了muP（Maximal Update Parametrization），这是旨在研究超参数跨模型尺度的迁移规律的工作。基于muP，我们可以在小模型上以相对较小的成本仔细搜索超参数（这里主要是学习率和初始化），然后迁移到大模型上，降低大模型的炼丹成本。

客观来讲，这里的介绍和分析还比较初步，比如没有考虑Bias项、没有评估结论在MLP以外架构的通用性、也没有仔细考虑Normalization和残差的作用等。没有考虑Bias项这个单纯是偷懒，权当留给读者的习题了；至于不同架构下的muP，一般分析起来比较麻烦，但由于神经网络的相似性，结论大致上是相同的，我们可以不加证明地用着。个人认为比较关键的改进点是Normalization和残差的影响，尤其是Normalization，它使得不依赖特殊的初始化就可以稳定前向传播，带来了更大的自由度和可能性。

当然，这些都留给后续分析了。

_**转载到请包括本文地址：** [https://kexue.fm/archives/10770](https://kexue.fm/archives/10770)_

_**更详细的转载事宜请参考：**_ [《科学空间FAQ》](https://kexue.fm/archives/6508#%E6%96%87%E7%AB%A0%E5%A6%82%E4%BD%95%E8%BD%AC%E8%BD%BD/%E5%BC%95%E7%94%A8)

**如果您还有什么疑惑或建议，欢迎在下方评论区继续讨论。**

**如果您觉得本文还不错，欢迎 [分享](https://kexue.fm/archives/10770#share)/ [打赏](https://kexue.fm/archives/10770#pay) 本文。打赏并非要从中获得收益，而是希望知道科学空间获得了多少读者的真心关注。当然，如果你无视它，也不会影响你的阅读。再次表示欢迎和感谢！**

打赏

微信打赏

支付宝打赏

因为网站后台对打赏并无记录，因此欢迎在打赏时候备注留言。你还可以 [**点击这里**](http://mail.qq.com/cgi-bin/qm_share?t=qm_mailme&email=tN7d1drY3drrx8H0xcWa19vZ) 或在下方评论区留言来告知你的建议或需求。

**如果您需要引用本文，请参考：**

苏剑林. (Mar. 13, 2025). 《初探muP：超参数的跨模型尺度迁移规律 》\[Blog post\]. Retrieved from [https://kexue.fm/archives/10770](https://kexue.fm/archives/10770)

@online{kexuefm-10770,
        title={初探muP：超参数的跨模型尺度迁移规律},
        author={苏剑林},
        year={2025},
        month={Mar},
        url={\\url{https://kexue.fm/archives/10770}},
}

分类： [数学研究](https://kexue.fm/category/Mathematics)    标签： [梯度](https://kexue.fm/tag/%E6%A2%AF%E5%BA%A6/), [学习率](https://kexue.fm/tag/%E5%AD%A6%E4%B9%A0%E7%8E%87/), [优化器](https://kexue.fm/tag/%E4%BC%98%E5%8C%96%E5%99%A8/), [尺度定律](https://kexue.fm/tag/%E5%B0%BA%E5%BA%A6%E5%AE%9A%E5%BE%8B/)[11 评论](https://kexue.fm/archives/10770#comments)

< [MoE环游记：3、换个思路来分配](https://kexue.fm/archives/10757) \| [高阶muP：更简明但更高明的谱条件缩放](https://kexue.fm/archives/10795) >

### 你也许还对下面的内容感兴趣

- [msign的导数](https://kexue.fm/archives/11025)
- [msign算子的Newton-Schulz迭代（下）](https://kexue.fm/archives/10996)
- [msign算子的Newton-Schulz迭代（上）](https://kexue.fm/archives/10922)
- [SVD的导数](https://kexue.fm/archives/10878)
- [通过梯度近似寻找Normalization的替代品](https://kexue.fm/archives/10831)
- [MoE环游记：4、难处应当多投入](https://kexue.fm/archives/10815)
- [高阶muP：更简明但更高明的谱条件缩放](https://kexue.fm/archives/10795)
- [MoE环游记：3、换个思路来分配](https://kexue.fm/archives/10757)
- [Muon续集：为什么我们选择尝试Muon？](https://kexue.fm/archives/10739)
- [MoE环游记：2、不患寡而患不均](https://kexue.fm/archives/10735)

[发表你的看法](https://kexue.fm/archives/10770#comment_form)

plc

March 14th, 2025

建议看Tensor Programs IVb （arxiv:2308.01814），里面有提到关于优化器的问题。
需要特别注意：优化器默认为element-wise，但Muon很明显不满足这个条件，不能按照此文章的理论来分析。
（这篇文章的核心其实在脚注里，如果认为Adam eps为0，可以极大简化这一理论）
Muon需要按照Spectral Condition（arxiv:2310.17813）来分析，原版Muon在此分析下学习率在扩宽模型时保持不变。

[回复评论](https://kexue.fm/archives/10770/comment-page-1?replyTo=27116#respond-post-10770)

[苏剑林](https://kexue.fm) 发表于
March 14th, 2025

感谢推荐。确实我没有完整读过Tensor Programs系列文章，因为对我的学习背景来说读起来还是太困难了，所以想了另外一个稍微不同的理解路线（即从$\\Delta\\mathcal{L}$入手）。

在个人的推导方式下，能够成功复现SGD和Adam的muP结论，并没有遇到“优化器需要是Element-wise”的障碍，所以也一并推到了Muon上。如果您说原本的muP有这个假设（我不确定），那可能是因为它是从$\\Delta\\boldsymbol{y}$入手分析造成的困难？毕竟$\\mathcal{L}$只是一个标量，$\\boldsymbol{y}$至少是一个向量，分析后者可能需要更多的简化假设。

[回复评论](https://kexue.fm/archives/10770/comment-page-1?replyTo=27119#respond-post-10770)

plc 发表于
March 14th, 2025

https://arxiv.org/pdf/2308.01814, Section 2.1，第12页。确实是这个假设。

[回复评论](https://kexue.fm/archives/10770/comment-page-1?replyTo=27121#respond-post-10770)

Chenyu Zheng 发表于
March 16th, 2025

我觉得greg yang在TP4b里的假设和苏神这里的推导没有冲突。TP4b中element-wise update的假设是至关重要的，因为如果超出这个范畴，网络的前传和反传过程就很可能不能被TP理论限定的三个算子表示，这会导致后续的所有的理论结果都不再可靠。

但苏神的推导并不依赖TP的语言（即基于TP的算子来表征分析优化过程，并且推导无穷宽极限），所以自然也不需要这个假设。苏神相当于用更简单的一套语言分析了mup的条件，至少目前和element-wise没有关系。

当然，这种分析方式是否对任意架构和优化器都成立，还是不清楚的。

[回复评论](https://kexue.fm/archives/10770/comment-page-1?replyTo=27142#respond-post-10770)

[苏剑林](https://kexue.fm) 发表于
March 16th, 2025

感谢两位 [@plc\|comment-27121](https://kexue.fm/archives/10770/comment-page-1#comment-27121) [@Chenyu Zheng\|comment-27142](https://kexue.fm/archives/10770/comment-page-1#comment-27142)

事实上，我几乎没读过Tensor Programs系列其他篇文章，所以不大了解Tensor Programs一贯下来的假设和推导。不过后面我还是打算从Grey Yang最新的 https://arxiv.org/abs/2310.17813 的谱条件缩放入手了，毕竟这样仔细算梯度还是太麻烦了（已经逐渐崩溃～

[回复评论](https://kexue.fm/archives/10770/comment-page-1?replyTo=27152#respond-post-10770)

Samuel66666666

March 20th, 2025

苏老师，我还是没看明白SGD的$\\frac{\\partial Y\_{out}}{\\partial Y\_{in}}$和$\\frac{\\partial Y\_{out}}{\\partial W\_{k}}$为什么它们的RMS是$O(1)$??如何估算出来的？能否大概写一下证明的思路？

[回复评论](https://kexue.fm/archives/10770/comment-page-1?replyTo=27181#respond-post-10770)

[苏剑林](https://kexue.fm) 发表于
March 23rd, 2025

前向和反向传播的稳定性。非要逐步证明的话，那就用链式法则写出每一步的梯度，然后证明每一步往回传都是稳定的（尺度无关的）。

[回复评论](https://kexue.fm/archives/10770/comment-page-1?replyTo=27226#respond-post-10770)

Chenyu Zheng

March 23rd, 2025

苏神，我这几天仔细拜读了一下，收获很大。但我也发现一个地方有问题。主要是类似“我们直接将$W\_{out}$的初始化方差设为$1/d^2$”的语句，其实是恰好设对的，而不是从本文的三个条件推出来的。

我们以最简单的SGD中的$W\_k$为例子。为了使$\\Delta L = O(1)$，我们在保证前传反传稳定的情况下，完全可以设置其它的$W\_{out}$方差和学习率。比如，$W\_{out}$的初始化方差设为$1/d$（前传和反传稳定），此时$\\frac{\\partial L}{\\partial W\_k}$的RMS为$1/\\sqrt{d}$，平方求和后是$d$，所以对应地我们把学习率设为$1/d$。

在这个时候，我们来简单观察一下第$k$层feature的RMS的变化。
$$
\\Delta W\_k Y\_{k-1} = -\\eta \\frac{\\partial L}{\\partial W\_k} Y\_k = O(d \\times d^{-1} d^{-0.5}) = O(1/\\sqrt{d})。
$$
当宽度很大的时候，我们发现中间层的feature不再变化，这正是mup（所有层feature最大更新）所不期望的。作为对比，我们可以看看本文$W\_{out}$的初始化方差设为$1/d^2$为什么合理：
$$
\\Delta W\_k Y\_{k-1} = -\\eta \\frac{\\partial L}{\\partial W\_k} Y\_k = O(d \\times 1 d^{-1}) = O(1)。
$$
我们可以看到，此时所有隐藏层可以进行最大更新，这正是mup的要求。

所以，本文的$\\Delta L = O(\\Delta Z)$条件只是确保了output能够最大更新且不爆炸，但是这个条件不能够保证中间层的feature也得到最大更新。只有将中间层的feature更新$\\Delta W\_k Y\_{k-1}$加入作为第4个条件，才能正确地推导出$W\_{out}$的初始化方差设为$1/d^2$是唯一正确的。

不过还是感谢苏神，之前看TP4原文虽然都把证明过了一遍，但一直没能尝试去建立起非常直观的、直觉的、简单的推导过程。最近几天对照苏神好好探究了一下，感觉自己终于发现了梯度传播视角下，mup的简单原子条件了。

[回复评论](https://kexue.fm/archives/10770/comment-page-1?replyTo=27212#respond-post-10770)

[苏剑林](https://kexue.fm) 发表于
March 24th, 2025

感谢指点！事实上我在写博客的时候已经意识到你说的问题了，你补全了本文的不足。我参考你的意见，把它补充到正文里边了。再次感谢！

不过顺便说，我已经“叛逃”到 https://arxiv.org/abs/2310.17813 这套方法了hhh，因为即便是本文这套简化版思路，对于一些复杂case也太难算了。

[回复评论](https://kexue.fm/archives/10770/comment-page-1?replyTo=27235#respond-post-10770)

Chenyu Zheng

March 24th, 2025

确实太复杂了，我之后也学一下谱的视角

[回复评论](https://kexue.fm/archives/10770/comment-page-1?replyTo=27238#respond-post-10770)

Kuy

March 27th, 2025

每次看到苏神的文章都要仔细揣摩一遍，然后一脸蒙蔽地离开。

[回复评论](https://kexue.fm/archives/10770/comment-page-1?replyTo=27254#respond-post-10770)

[取消回复](https://kexue.fm/archives/10770#respond-post-10770)

你的大名

电子邮箱

个人网站（选填）

1\. 可以使用LaTeX代码，点击“预览效果”可查看效果；2. 可以通过点击评论楼层编号来引用该楼层；3. 网站可能会有点卡，如非确认评论失败，请 **不要重复点击提交**。

### 内容速览

[方法大意](https://kexue.fm/archives/10770#%E6%96%B9%E6%B3%95%E5%A4%A7%E6%84%8F)
[前向传播](https://kexue.fm/archives/10770#%E5%89%8D%E5%90%91%E4%BC%A0%E6%92%AD)
[反向传播](https://kexue.fm/archives/10770#%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD)
[损失增量](https://kexue.fm/archives/10770#%E6%8D%9F%E5%A4%B1%E5%A2%9E%E9%87%8F)
[模型假设](https://kexue.fm/archives/10770#%E6%A8%A1%E5%9E%8B%E5%81%87%E8%AE%BE)
[组装起来](https://kexue.fm/archives/10770#%E7%BB%84%E8%A3%85%E8%B5%B7%E6%9D%A5)
[特征变化](https://kexue.fm/archives/10770#%E7%89%B9%E5%BE%81%E5%8F%98%E5%8C%96)
[Adam版本](https://kexue.fm/archives/10770#Adam%E7%89%88%E6%9C%AC)
[Muon版本](https://kexue.fm/archives/10770#Muon%E7%89%88%E6%9C%AC)
[结论汇总](https://kexue.fm/archives/10770#%E7%BB%93%E8%AE%BA%E6%B1%87%E6%80%BB)
[文章小结](https://kexue.fm/archives/10770#%E6%96%87%E7%AB%A0%E5%B0%8F%E7%BB%93)

### 智能搜索

支持整句搜索！网站自动使用 [结巴分词](https://github.com/fxsjy/jieba) 进行分词，并结合ngrams排序算法给出合理的搜索结果。

### 热门标签

[生成模型](https://kexue.fm/tag/%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B/) [attention](https://kexue.fm/tag/attention/) [优化](https://kexue.fm/tag/%E4%BC%98%E5%8C%96/) [语言模型](https://kexue.fm/tag/%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/) [模型](https://kexue.fm/tag/%E6%A8%A1%E5%9E%8B/) [网站](https://kexue.fm/tag/%E7%BD%91%E7%AB%99/) [概率](https://kexue.fm/tag/%E6%A6%82%E7%8E%87/) [梯度](https://kexue.fm/tag/%E6%A2%AF%E5%BA%A6/) [转载](https://kexue.fm/tag/%E8%BD%AC%E8%BD%BD/) [微分方程](https://kexue.fm/tag/%E5%BE%AE%E5%88%86%E6%96%B9%E7%A8%8B/) [矩阵](https://kexue.fm/tag/%E7%9F%A9%E9%98%B5/) [天象](https://kexue.fm/tag/%E5%A4%A9%E8%B1%A1/) [分析](https://kexue.fm/tag/%E5%88%86%E6%9E%90/) [深度学习](https://kexue.fm/tag/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/) [积分](https://kexue.fm/tag/%E7%A7%AF%E5%88%86/) [python](https://kexue.fm/tag/python/) [优化器](https://kexue.fm/tag/%E4%BC%98%E5%8C%96%E5%99%A8/) [力学](https://kexue.fm/tag/%E5%8A%9B%E5%AD%A6/) [无监督](https://kexue.fm/tag/%E6%97%A0%E7%9B%91%E7%9D%A3/) [扩散](https://kexue.fm/tag/%E6%89%A9%E6%95%A3/) [几何](https://kexue.fm/tag/%E5%87%A0%E4%BD%95/) [节日](https://kexue.fm/tag/%E8%8A%82%E6%97%A5/) [生活](https://kexue.fm/tag/%E7%94%9F%E6%B4%BB/) [文本生成](https://kexue.fm/tag/%E6%96%87%E6%9C%AC%E7%94%9F%E6%88%90/) [数论](https://kexue.fm/tag/%E6%95%B0%E8%AE%BA/)

### 随机文章

- [《新理解矩阵6》：为什么只有方阵有行列式？](https://kexue.fm/archives/2757)
- [《环球科学》:超越费曼图](https://kexue.fm/archives/1790)
- [校准你的钟表（时间科普网站）](https://kexue.fm/archives/79)
- [路径积分系列：1.我的毕业论文](https://kexue.fm/archives/3749)
- [EAE：自编码器 + BN + 最大熵 = 生成模型](https://kexue.fm/archives/7343)
- [11月03日美国“发现号”航天飞机“绝唱”](https://kexue.fm/archives/1034)
- [有限内存下全局打乱几百G文件（Python）](https://kexue.fm/archives/8662)
- [几何的数与数的几何：超复数的浅探究](https://kexue.fm/archives/2291)
- [当生成模型肆虐：互联网将有“疯牛病”之忧？](https://kexue.fm/archives/9687)
- [【NASA每日一图】GigaGalaxy Zoom-礁湖星云](https://kexue.fm/archives/158)

### 最近评论

- [忍者猫](https://kexue.fm/archives/10592/comment-page-2#comment-27952): 这优化器的作者真的应该给你打钱
- [Chaofa Yuan](https://kexue.fm/archives/11033/comment-page-1#comment-27951): 写得太好了
- [Skyler Lin](https://kexue.fm/archives/11033/comment-page-1#comment-27949): respect苏神！
- [宋佳铭](https://kexue.fm/archives/10958/comment-page-1#comment-27947): 对，个人感觉mean flow就是continuous time CTM
- [宋佳铭](https://kexue.fm/archives/10958/comment-page-1#comment-27946): 的确，对sg这个事情我感觉如果是用‘归纳’法做是不太能避免的，因为毕竟是用步长短的模型去约束步...
- [MoFHeka](https://kexue.fm/archives/10542/comment-page-1#comment-27945): 苏老师您好，请问一下这套结论在稀疏参数上应该如何应用？比如大规模稀疏Embedding，每个B...
- [苏剑林](https://kexue.fm/archives/11033/comment-page-1#comment-27944): Temp LoRA倒是有印象，其实思想是一样的，如果我单独开一篇文章介绍TTT的话，应该会提到...
- [苏剑林](https://kexue.fm/archives/11033/comment-page-1#comment-27943): 你搜搜mamba、rwkv甚至rnn做vision的工作，其实不少。不过多数确实像你说的，正反...
- [苏剑林](https://kexue.fm/archives/9379/comment-page-1#comment-27942): 问题1可以看看 https://kexue.fm/archives/4718 ，简单来说就是点...
- [苏剑林](https://kexue.fm/archives/10958/comment-page-2#comment-27941): 你的“信息量”怎么定义？直观来说，reflow训练的是切线模型，而一步生成需要的是割线模型，m...

### 友情链接

- [Cool Papers](https://papers.cool)
- [数学研发](https://bbs.emath.ac.cn)
- [Seatop](http://www.seatop.com.cn/)
- [Xiaoxia](https://xiaoxia.org/)
- [积分表-网络版](https://kexue.fm/sci/integral/index.html)
- [丝路博傲](http://blog.dvxj.com/)
- [ph4ntasy 饭特稀](http://www.ph4ntasy.com/)
- [数学之家](http://www.2math.cn/)
- [有趣天文奇观](http://interesting-sky.china-vo.org/)
- [TwistedW](http://www.twistedwg.com/)
- [godweiyang](https://godweiyang.com/)
- [AI柠檬](https://blog.ailemon.net/)
- [王登科-DK博客](https://greatdk.com)
- [ESON](https://blog.eson.org/)
- [枫之羽](https://fzhiy.net/)
- [Mathor's blog](https://wmathor.com/)
- [coding-zuo](https://coding-zuo.github.io/)
- [博科园](https://www.bokeyuan.net/)
- [孔皮皮的博客](https://www.kppkkp.top/)
- [运鹏的博客](https://yunpengtai.top/)
- [jiming.site](https://jiming.site/)
- [OmegaXYZ](https://www.omegaxyz.com/)
- [Blog by Eacls](https://www.eacls.top/)
- [EAI猩球](https://www.robotech.ink/)
- [文举的博客](https://liwenju0.com/)
- [用代码打点酱油](https://bruceyuan.com/)
- [申请链接](https://kexue.fm/links.html)

本站采用创作共用版权协议，要求署名、非商业用途和保持一致。转载本站内容必须也遵循“ [署名-非商业用途-保持一致](http://creativecommons.org/licenses/by-nc-nd/2.5/cn/)”的创作共用协议。
© 2009-2025 Scientific Spaces. All rights reserved. Theme by [laogui](http://www.laogui.com). Powered by [Typecho](http://typecho.org). 备案号: [粤ICP备09093259号-1/2](https://beian.miit.gov.cn/)。