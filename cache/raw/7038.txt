Processing math: 0%

![MobileSideBar](https://kexue.fm/usr/themes/geekg/images/slide-button.png)

## SEARCH

## MENU

- [打赏](https://kexue.fm/reward.html)
- [公式](https://kexue.fm/latex.html)
- [天象](https://kexue.fm/ac.html)
- [链接](https://kexue.fm/links.html)
- [时光](https://kexue.fm/me.html)
- [博览](https://kexue.fm/science.html)
- [归档](https://kexue.fm/content.html)

## CATEGORIES

- [千奇百怪](https://kexue.fm/category/Everything)
- [天文探索](https://kexue.fm/category/Astronomy)
- [数学研究](https://kexue.fm/category/Mathematics)
- [物理化学](https://kexue.fm/category/Phy-chem)
- [信息时代](https://kexue.fm/category/Big-Data)
- [生物自然](https://kexue.fm/category/Biology)
- [图片摄影](https://kexue.fm/category/Photograph)
- [问题百科](https://kexue.fm/category/Questions)
- [生活/情感](https://kexue.fm/category/Life-Feeling)
- [资源共享](https://kexue.fm/category/Resources)

## NEWPOSTS

- [让炼丹更科学一些（五）：基于梯度精...](https://kexue.fm/archives/11530)
- [让炼丹更科学一些（四）：新恒等式，...](https://kexue.fm/archives/11494)
- [为什么DeltaNet要加L2 N...](https://kexue.fm/archives/11486)
- [让炼丹更科学一些（三）：SGD的终...](https://kexue.fm/archives/11480)
- [让炼丹更科学一些（二）：将结论推广...](https://kexue.fm/archives/11469)
- [滑动平均视角下的权重衰减和学习率](https://kexue.fm/archives/11459)
- [生成扩散模型漫谈（三十一）：预测数...](https://kexue.fm/archives/11428)
- [Muon优化器指南：快速上手与关键细节](https://kexue.fm/archives/11416)
- [AdamW的Weight RMS的...](https://kexue.fm/archives/11404)
- [n个正态随机数的最大值的渐近估计](https://kexue.fm/archives/11390)

## COMMENTS

- [Bin: 今天偶然从某个论坛看到有人推荐您的博客，定睛一看竟然是华师同院...](https://kexue.fm/archives/1990/comment-page-2#comment-29105)
- [Rapture D: 我有一个问题，为什么不考虑亥姆霍兹定理和斯托克斯公式。](https://kexue.fm/archives/11530/comment-page-1#comment-29104)
- [mofheka: 苏神是还在用jax是么？最近在做基于Google Pathwa...](https://kexue.fm/archives/11390/comment-page-1#comment-29103)
- [长琴: 看懂这篇博客也不是一件容易的事情。](https://kexue.fm/archives/11530/comment-page-1#comment-29102)
- [AlexLi: 苏老师，请教一下(7)式中将 \\mu(x\_t) 传给 $p...](https://kexue.fm/archives/9257/comment-page-4#comment-29101)
- [tyler\_zxc: "Performer的思想是将标准的Attention线性化，...](https://kexue.fm/archives/7921/comment-page-2#comment-29100)
- [我: 似乎并非mHC提出矩阵的思想？之前hyper connecti...](https://kexue.fm/archives/11494/comment-page-1#comment-29099)
- [winter: 苏神您好，假如对于比较均匀的attention weightP...](https://kexue.fm/archives/10847/comment-page-1#comment-29098)
- [苏剑林: KL散度、JS散度、W距离啥的，都行啊，看你喜欢哪个](https://kexue.fm/archives/8512/comment-page-2#comment-29097)
- [苏剑林: 没有绝对公平的对比方法，主要看你关心什么。比如，如果只关心推理...](https://kexue.fm/archives/9119/comment-page-14#comment-29096)

## USERLOGIN

- [登录](https://kexue.fm/admin/login.php)

[科学空间\|Scientific Spaces](https://kexue.fm/)

- [登录](https://kexue.fm/admin/login.php)
- [打赏](https://kexue.fm/reward.html)
- [公式](https://kexue.fm/latex.html)
- [天象](https://kexue.fm/ac.html)
- [链接](https://kexue.fm/links.html)
- [时光](https://kexue.fm/me.html)
- [博览](https://kexue.fm/science.html)
- [归档](https://kexue.fm/content.html)

渴望成为一个小飞侠

- [![](https://kexue.fm/usr/themes/geekg/images/rss.png)\\
\\
欢迎订阅](https://kexue.fm/feed)
- [![](https://kexue.fm/usr/themes/geekg/images/mail.png)\\
\\
个性邮箱](https://kexue.fm/archives/119)
- [![](https://kexue.fm/usr/themes/geekg/images/Saturn.png)\\
\\
天象信息](https://kexue.fm/ac.html)
- [![](https://kexue.fm/usr/themes/geekg/images/iss.png)\\
\\
观测ISS](https://kexue.fm/archives/41)
- [![](https://kexue.fm/usr/themes/geekg/images/pi.png)\\
\\
LaTeX](https://kexue.fm/latex.html)
- [![](https://kexue.fm/usr/themes/geekg/images/mlogo.png)\\
\\
关于博主](https://kexue.fm/me.html)

欢迎访问“科学空间”，这里将与您共同探讨自然科学，回味人生百态；也期待大家的分享～

- [**千奇百怪** Everything](https://kexue.fm/category/Everything)
- [**天文探索** Astronomy](https://kexue.fm/category/Astronomy)
- [**数学研究** Mathematics](https://kexue.fm/category/Mathematics)
- [**物理化学** Phy-chem](https://kexue.fm/category/Phy-chem)
- [**信息时代** Big-Data](https://kexue.fm/category/Big-Data)
- [**生物自然** Biology](https://kexue.fm/category/Biology)
- [**图片摄影** Photograph](https://kexue.fm/category/Photograph)
- [**问题百科** Questions](https://kexue.fm/category/Questions)
- [**生活/情感** Life-Feeling](https://kexue.fm/category/Life-Feeling)
- [**资源共享** Resources](https://kexue.fm/category/Resources)

- [**千奇百怪**](https://kexue.fm/category/Everything)
- [**天文探索**](https://kexue.fm/category/Astronomy)
- [**数学研究**](https://kexue.fm/category/Mathematics)
- [**物理化学**](https://kexue.fm/category/Phy-chem)
- [**信息时代**](https://kexue.fm/category/Big-Data)
- [**生物自然**](https://kexue.fm/category/Biology)
- [**图片摄影**](https://kexue.fm/category/Photograph)
- [**问题百科**](https://kexue.fm/category/Questions)
- [**生活/情感**](https://kexue.fm/category/Life-Feeling)
- [**资源共享**](https://kexue.fm/category/Resources)

[首页](https://kexue.fm/) [信息时代](https://kexue.fm/category/Big-Data) 从去噪自编码器到生成模型

31Oct

# [从去噪自编码器到生成模型](https://kexue.fm/archives/7038)

By 苏剑林 \|
2019-10-31 \|
154337位读者 \|

在我看来，几大顶会之中，ICLR的论文通常是最有意思的，因为它们的选题和风格基本上都比较轻松活泼、天马行空，让人有脑洞大开之感。所以，ICLR 2020的投稿论文列表出来之后，我也抽时间粗略过了一下这些论文，确实发现了不少有意思的工作。

其中，我发现了两篇利用去噪自编码器的思想做生成模型的论文，分别是 [《Learning Generative Models using Denoising Density Estimators》](https://openreview.net/forum?id=Skl1HCNKDr) 和 [《Annealed Denoising Score Matching: Learning Energy-Based Models in High-Dimensional Spaces》](https://openreview.net/forum?id=HJeFmkBtvB)。由于常规做生成模型的思路我基本都有所了解，所以这种“别具一格”的思路就引起了我的兴趣。细读之下，发现两者的出发点是一致的，但是具体做法又有所不同，最终的落脚点又是一样的，颇有“一题多解”的美妙，遂将这两篇论文放在一起，对比分析一翻。

## 去噪自编码 [\#](https://kexue.fm/archives/7038\#%E5%8E%BB%E5%99%AA%E8%87%AA%E7%BC%96%E7%A0%81)

两篇论文的根本出发点都是去噪自编码器，更准确地说，它利用了去噪自编码器的最优解

> **基本结果：** 若x,\\varepsilon\\in \\mathbb{R}^d，并且x\\sim p(x),\\varepsilon\\sim u(\\varepsilon)，这里u(\\varepsilon)=\\mathcal{N}(0,\\sigma^2 I\_d)，那么
> \\begin{equation}\\begin{aligned}r(x)=&\\,\\mathop{\\text{argmin}}\_{r}\\mathbb{E}\_{x\\sim p(x),\\varepsilon\\sim \\mathcal{N}(0,\\sigma^2 I\_d)}\\left\[\\Vert r(x + \\varepsilon) - x\\Vert^2\\right\] \\\
> =&\\,x + \\sigma^2 \\nabla\_x \\,\\log\\hat{p}(x)\\end{aligned}\\label{eq:denoise}\\end{equation}

其中\\hat{p}(x)=\[p\*u\](x)=\\int p(x-\\varepsilon)u(\\varepsilon) d\\varepsilon=\\int p(\\varepsilon)u(x-\\varepsilon) d\\varepsilon指的是分布p(x)和u(\\varepsilon)的卷积运算，具体含义是x+\\varepsilon的概率密度，换言之，如果p(x)代表真实图片的分布，那么如果我们能实现从\\hat{p}(x)中采样，那么得到的是一批带有高斯噪声的真实图片。

结果\\eqref{eq:denoise}也就是说加性高斯噪声的最优去噪自编码器是能显式地计算出来，并且结果跟分布的梯度有关。这个结果非常有意思，也非常深刻，值得我们多加回味。比如， **式\\eqref{eq:denoise}告诉我们r(x)-x实际上就是对（带噪声的）真实分布梯度的估计**，而有了真实分布的梯度，其实可以做很多事情，尤其是生成模型相关的事情。

> **证明：** 其实\\eqref{eq:denoise}的证明并不困难，变分目标得到
>
> \\begin{equation}\\begin{aligned}&\\delta \\iint p(x)u(\\varepsilon)\\left\\Vert r(x + \\varepsilon) - x\\right\\Vert\_2^2 dx d\\varepsilon\\\
> =&\\delta \\iint p(x)u(y-x)\\left\\Vert r(y) - x\\right\\Vert\_2^2 dx dy\\\
> =&2\\iint p(x)u(y-x)\\left\\langle r(y) - x, \\delta r(y)\\right\\rangle dx dy\\\
> \\end{aligned}\\end{equation}
>
> 所以\\int p(x)u(y-x)(r(y) - x)dx=0，即
>
> \\begin{equation}r(y) = \\frac{\\int p(x)u(y-x)x dx}{\\int p(x)u(y-x) dx}\\end{equation}
>
> 代入表达式u(\\varepsilon)=\\frac{1}{(2\\pi \\sigma^2)^{d/2}}\\exp\\left(-\\frac{\\left\\Vert\\varepsilon\\right\\Vert\_2^2}{2\\sigma^2}\\right)，即得
>
> \\begin{equation}r(y) = y + \\sigma^2\\nabla\_y \\log\\left\[p\*u\\right\](y)\\end{equation}

## 曲径通幽处 [\#](https://kexue.fm/archives/7038\#%E6%9B%B2%E5%BE%84%E9%80%9A%E5%B9%BD%E5%A4%84)

我们首先来介绍一下 [《Learning Generative Models using Denoising Density Estimators》](https://openreview.net/forum?id=Skl1HCNKDr) 的思路。按照GAN和VAE的通常习惯，我们是希望训练一个映射x=G(z)，使得从先验分布q(z)中采样出来的z都能被映射为一个真实样本，用概率的话说，那就是希望拉近p(x)和下述的q(x)的距离：

\\begin{equation}q(x) = \\int q(z)\\delta(x - G\_{\\theta}(z))dz\\end{equation}

为此，GAN常用的优化目标是最小化KL(q(x)\\Vert p(x))，这个观点可以参考 [《用变分推断统一理解生成模型（VAE、GAN、AAE、ALI）》](https://kexue.fm/archives/5716) 和 [《能量视角下的GAN模型（二）：GAN＝“分析”＋“采样”》](https://kexue.fm/archives/6331)。但是，由于前面估计的是\\hat{p}(x)的梯度，我们可以换个目标：最小化KL\\left(\\hat{q}(x)\\big\\Vert \\hat{p}(x)\\right)。

为了，我们可以进行演算：

\\begin{equation}\\begin{aligned}KL\\left(\\hat{q}(x)\\big\\Vert \\hat{p}(x)\\right)=&\\int \\hat{q}(x) \\log \\frac{\\hat{q}(x)}{\\hat{p}(x)}dx\\\
=&\\int q(x)u(\\varepsilon) \\log \\frac{\\hat{q}(x+\\varepsilon)}{\\hat{p}(x+\\varepsilon)}dx d\\varepsilon\\\
=&\\int q(z)\\delta(x-G\_{\\theta}(z))u(\\varepsilon) \\log \\frac{\\hat{q}(x+\\varepsilon)}{\\hat{p}(x+\\varepsilon)}dx d\\varepsilon dz\\\
=&\\int q(z)u(\\varepsilon) \\log \\frac{\\hat{q}(G\_{\\theta}(z)+\\varepsilon)}{\\hat{p}(G\_{\\theta}(z)+\\varepsilon)}d\\varepsilon dz\\\
=&\\,\\mathbb{E}\_{z\\sim q(z), \\varepsilon\\sim u(\\varepsilon)}\\big\[\\log \\hat{q}(G\_{\\theta}(z)+\\varepsilon) - \\log \\hat{p}(G\_{\\theta}(z)+\\varepsilon)\\big\]\\\
\\end{aligned}\\label{eq:dae-1}\\end{equation}

这个目标需要我们能得到\\log\\hat{p}(x)和\\log\\hat{q}(x)的估计。我们可以用神经网络构建两个\\mathbb{R}^d \\to \\mathbb{R}的模型E\_p(x)和E\_q(x)，然后分别去最小化

\\begin{equation}\\begin{aligned}\\mathop{\\text{argmin}}\_{E\_p}\\mathbb{E}\_{x\\sim p(x),\\varepsilon\\sim \\mathcal{N}(0,\\sigma^2 I\_d)}\\left\[\\Vert \\nabla\_x E\_p(x + \\varepsilon) + \\varepsilon\\Vert^2\\right\]\\\
\\mathop{\\text{argmin}}\_{E\_q}\\mathbb{E}\_{x\\sim q(x),\\varepsilon\\sim \\mathcal{N}(0,\\sigma^2 I\_d)}\\left\[\\Vert \\nabla\_x E\_q(x + \\varepsilon) + \\varepsilon\\Vert^2\\right\]
\\end{aligned}\\label{eq:e-grad}\\end{equation}

也就是用\\nabla\_x E\_p(x)+x和\\nabla\_x E\_q(x)+x作为去噪自编码器，根据结果\\eqref{eq:denoise}，我们就有

\\begin{equation}\\left\\{\\begin{aligned}\\nabla\_x E\_p(x)+x=x+\\sigma^2 \\nabla\_x \\log \\hat{p}(x)\\\
\\nabla\_x E\_q(x)+x=x+\\sigma^2 \\nabla\_x \\log \\hat{q}(x)\\end{aligned}\\right.
\\quad\\Rightarrow\\quad \\left\\{\\begin{aligned}E\_p(x) = \\sigma^2 \\log \\hat{p}(x) + C\_1\\\
E\_q(x) = \\sigma^2 \\log \\hat{q}(x) + C\_2\\end{aligned}\\right.\\end{equation}

也就是说在相差一个常数的情况下，E\_p(x)正比于\\log \\hat{p}(x)，E\_q(x)也正比于\\log \\hat{q}(x)，而常数不影响优化，所以我们可以将E\_p(x)和E\_q(x)替换到\\eqref{eq:dae-1}里边去，得到

\\begin{equation}KL\\left(\\hat{q}(x)\\big\\Vert \\hat{p}(x)\\right)\\sim\\,\\mathbb{E}\_{z\\sim q(z), \\varepsilon\\sim u(\\varepsilon)}\\big\[E\_q(G\_{\\theta}(z)+\\varepsilon) - E\_p(G\_{\\theta}(z)+\\varepsilon)\\big\]\\label{eq:dae-2}\\end{equation}

这就得到了一个生成模型的流程：

> 选定先验分布q(z)，初始化G\_{\\theta}(z)，事先求好E\_p(x)。循环执行下面的3步直到收敛：
>
> 1、选一批z\\sim q(z)，选一批噪声\\varepsilon\\sim\\mathcal{N}(0,\\sigma^2 I\_d)，合成一批带噪声的假样本x = G\_{\\theta}(z)+\\varepsilon；
>
> 2、利用这批带噪声的假样本训练E\_q(x)；
>
> 3、固定E\_p,E\_q，用梯度下降根据\\eqref{eq:dae-2}更新若干步G\_{\\theta}；

这篇论文的实验比较简单，只做了mnist和fashion mnist的实验，证明了它的可行性：

[![fashion mnist的生成效果](https://kexue.fm/usr/uploads/2019/10/3743468191.png)](https://kexue.fm/usr/uploads/2019/10/3743468191.png "点击查看原图")

fashion mnist的生成效果

## 峰回路转间 [\#](https://kexue.fm/archives/7038\#%E5%B3%B0%E5%9B%9E%E8%B7%AF%E8%BD%AC%E9%97%B4)

另外一篇论文 [《Annealed Denoising Score Matching: Learning Energy-Based Models in High-Dimensional Spaces》](https://openreview.net/forum?id=HJeFmkBtvB) 就更粗暴直接了，它相当于去噪自编码器跟 [《能量视角下的GAN模型（三）：生成模型=能量模型》](https://kexue.fm/archives/6612) 的结合。

因为\\eqref{eq:denoise}已经帮我们得到了\\nabla\_x\\log\\hat{p}(x)=(r(x)-x)/\\sigma^2了（当然这篇论文的实际做法也不是直接用神经网络拟合r(x)，而是像\\eqref{eq:e-grad}一样用神经网络拟合一个标量函数的，但这不影响思想），其实这就能够帮助我们从\\hat{p}(x)采样了。当然采样出来的图片是有噪声的，我们还需要它采样出来的结果传入r(x)去噪一下，即

p(x) = \\mathbb{E}\_{x\_{noise}\\sim \\hat{p}(x)} \\big\[\\delta(x - r(x\_{noise}))\\big\]

那具体来说怎么从\\hat{p}(x)采样呢？Langevin方程！因为已经知道了\\nabla\_x\\log\\hat{p}(x)，那么下述Langevin方程

\\begin{equation}x\_{t+1} = x\_t + \\frac{1}{2}\\varepsilon \\nabla\_x\\log\\hat{p}(x) + \\sqrt{\\varepsilon}\\alpha,\\quad \\alpha \\sim \\mathcal{N}(\\alpha;0,1)\\label{eq:sde}\\end{equation}

当\\varepsilon\\to 0且t\\to\\infty时，序列\\{x\_t\\}所服从的分布就是从\\hat{p}(x)，换句话说，\\hat{p}(x)是该Langevin方程的静态分布。

于是，从\\hat{p}(x)采样这个过程，就被 [《Annealed Denoising Score Matching: Learning Energy-Based Models in High-Dimensional Spaces》](https://openreview.net/forum?id=HJeFmkBtvB) 用这么一种粗暴直接（但我觉得不优雅）的方法解决了，所以训练完去噪自编码后，就自动地得到了一个生成模型了...

总的过程是：

> 1、训练去噪自编码器r(x)，得到\\nabla\_x\\hat{p}(x)；
>
> 2、用迭代过程\\eqref{eq:sde}采样，采样结果是一批带噪声的真实样本；
>
> 3、将第2步的采样结果传入r(x)去噪，得到无噪声的样本。

当然，论文还有很多细节，论文的核心技巧是用了退火技巧来稳定训练过程，提高生成质量，但笔者对这些并不是很感兴趣，因为我只是想学习一些新奇的生成模型思想，拓宽视野。不过不得不说，虽然做法有点粗暴，这篇论文的生成效果还是有一定的竞争力的，在fashion mnist、CelebA、cifar10都有相当不错的生成效果：

[![fashion mnist、CelebA、cifar10上的生成效果](https://kexue.fm/usr/uploads/2019/10/3190672949.png)](https://kexue.fm/usr/uploads/2019/10/3190672949.png "点击查看原图")

fashion mnist、CelebA、cifar10上的生成效果

## 曲终人散时 [\#](https://kexue.fm/archives/7038\#%E6%9B%B2%E7%BB%88%E4%BA%BA%E6%95%A3%E6%97%B6)

本文介绍了投稿ICLR 2020的两篇类似的论文，都是利用去噪自编码器来做生成模型的，因为之前我没了解过相关思路，所以就饶有兴致对比阅读了一番。

且不说生成效果如何，我觉得它们都是颇具启发性的，能引起我的一些思考（不仅是CV，还包括NLP方面的）。比如Bert的MLM预训练方式本质上也是一个去噪自编码器，那有没有类似\\eqref{eq:denoise}的结果？或者反过来，类似\\eqref{eq:denoise}的结果能不能启发我们构造一些新的预训练任务，又或者能不能借此说清楚pretrain + finetune这种流程的本质原理？

_**转载到请包括本文地址：** [https://kexue.fm/archives/7038](https://kexue.fm/archives/7038 "从去噪自编码器到生成模型")_

_**更详细的转载事宜请参考：**_ [《科学空间FAQ》](https://kexue.fm/archives/6508#%E6%96%87%E7%AB%A0%E5%A6%82%E4%BD%95%E8%BD%AC%E8%BD%BD/%E5%BC%95%E7%94%A8 "《科学空间FAQ》")

**如果您还有什么疑惑或建议，欢迎在下方评论区继续讨论。**

**如果您觉得本文还不错，欢迎 [分享](https://kexue.fm/archives/7038#share)/ [打赏](https://kexue.fm/archives/7038#pay) 本文。打赏并非要从中获得收益，而是希望知道科学空间获得了多少读者的真心关注。当然，如果你无视它，也不会影响你的阅读。再次表示欢迎和感谢！**

打赏

![科学空间](https://kexue.fm/usr/themes/geekg/payment/wx.png)

微信打赏

![科学空间](https://kexue.fm/usr/themes/geekg/payment/zfb.png)

支付宝打赏

因为网站后台对打赏并无记录，因此欢迎在打赏时候备注留言。

你还可以 [**点击这里**](http://mail.qq.com/cgi-bin/qm_share?t=qm_mailme&email=tN7d1drY3drrx8H0xcWa19vZ) 或在下方评论区留言来告知你的建议或需求。

**如果您需要引用本文，请参考：**

苏剑林. (Oct. 31, 2019). 《从去噪自编码器到生成模型 》\[Blog post\]. Retrieved from [https://kexue.fm/archives/7038](https://kexue.fm/archives/7038)

@online{kexuefm-7038,

         title={从去噪自编码器到生成模型},

         author={苏剑林},

         year={2019},

         month={Oct},

         url={\\url{https://kexue.fm/archives/7038}},

}


分类： [信息时代](https://kexue.fm/category/Big-Data)    标签： [生成模型](https://kexue.fm/tag/%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B/), [编码](https://kexue.fm/tag/%E7%BC%96%E7%A0%81/), [梯度](https://kexue.fm/tag/%E6%A2%AF%E5%BA%A6/), [去噪](https://kexue.fm/tag/%E5%8E%BB%E5%99%AA/)[38 评论](https://kexue.fm/archives/7038#comments)

< [什么时候多进程的加速比可以大于1？](https://kexue.fm/archives/7031 "什么时候多进程的加速比可以大于1？") \| [Keras：Tensorflow的黄金标准](https://kexue.fm/archives/7055 "Keras：Tensorflow的黄金标准") >

### 你也许还对下面的内容感兴趣

- [让炼丹更科学一些（五）：基于梯度精调学习率](https://kexue.fm/archives/11530 "让炼丹更科学一些（五）：基于梯度精调学习率")
- [滑动平均视角下的权重衰减和学习率](https://kexue.fm/archives/11459 "滑动平均视角下的权重衰减和学习率")
- [生成扩散模型漫谈（三十一）：预测数据而非噪声](https://kexue.fm/archives/11428 "生成扩散模型漫谈（三十一）：预测数据而非噪声")
- [AdamW的Weight RMS的渐近估计（下）](https://kexue.fm/archives/11404 "AdamW的Weight RMS的渐近估计（下）")
- [DiVeQ：一种非常简洁的VQ训练方案](https://kexue.fm/archives/11328 "DiVeQ：一种非常简洁的VQ训练方案")
- [为什么线性注意力要加Short Conv？](https://kexue.fm/archives/11320 "为什么线性注意力要加Short Conv？")
- [AdamW的Weight RMS的渐近估计（上）](https://kexue.fm/archives/11307 "AdamW的Weight RMS的渐近估计（上）")
- [为什么Adam的Update RMS是0.2？](https://kexue.fm/archives/11267 "为什么Adam的Update RMS是0.2？")
- [重新思考学习率与Batch Size（一）：现状](https://kexue.fm/archives/11260 "重新思考学习率与Batch Size（一）：现状")
- [Transformer升级之路：21、MLA好在哪里?（下）](https://kexue.fm/archives/11111 "Transformer升级之路：21、MLA好在哪里?（下）")

[发表你的看法](https://kexue.fm/archives/7038#comment_form)

1. [«](https://kexue.fm/archives/7038/comment-page-1#comments)
2. [1](https://kexue.fm/archives/7038/comment-page-1#comments)
3. [2](https://kexue.fm/archives/7038/comment-page-2#comments)

[shouldsee](http://www.catsmile.info/)

July 1st, 2022

这个模型好奇怪啊,看起来同时用了Fisher散度和KL散度的样子

[回复评论](https://kexue.fm/archives/7038/comment-page-2?replyTo=19408#respond-post-7038)

[shouldsee](http://www.catsmile.info/)

July 1st, 2022

看来MCMC采样过程应该是不如ScoreODE的

[回复评论](https://kexue.fm/archives/7038/comment-page-2?replyTo=19409#respond-post-7038)

[shouldsee](http://www.catsmile.info/) 发表于
July 1st, 2022

额还有点没看懂,这个 E\_p(x) 看起来是提前算好的能量函数了咯? 那这样E\_p(x) 其实已经包含了很多信息了. 后续又引入 E\_q(x) ,总感觉有一点奇怪

[回复评论](https://kexue.fm/archives/7038/comment-page-2?replyTo=19410#respond-post-7038)

[苏剑林](https://kexue.fm/) 发表于
July 4th, 2022

E\_p(x)是训练好的，E\_q(x)是用来训练生成模型的。因为直接通过E\_p(x)用Langevin方程采样的话会太慢，所以它想直接训练一个类似GAN的生成过程，这就用到了E\_q(x)。

[回复评论](https://kexue.fm/archives/7038/comment-page-2?replyTo=19417#respond-post-7038)

[shouldsee](http://www.catsmile.info/) 发表于
July 6th, 2022

恩恩好的,就是这个动机不太清楚.直接用 E\_p(x) 约束生成器,应该是会有些问题?

[回复评论](https://kexue.fm/archives/7038/comment-page-2?replyTo=19421#respond-post-7038)

[shouldsee](http://www.catsmile.info/) 发表于
July 6th, 2022

如果直接用 E\_p(x) 估计就跟GAN差不太多了?

[回复评论](https://kexue.fm/archives/7038/comment-page-2?replyTo=19422#respond-post-7038)

[苏剑林](https://kexue.fm/) 发表于
July 7th, 2022

只用E\_p(x)是指最大化E\_p(G(z))吧？要是G(z)只生成一张令E\_p(z)最大的图怎么办？

[回复评论](https://kexue.fm/archives/7038/comment-page-2?replyTo=19431#respond-post-7038)

灰羽

October 5th, 2023

请问一下（3）是怎么推到（4）的呀？算了很久也没有明白。毕竟不是数学专业出身

[回复评论](https://kexue.fm/archives/7038/comment-page-2?replyTo=22844#respond-post-7038)

[苏剑林](https://kexue.fm/) 发表于
October 6th, 2023

反过来推比较容易，直接推是比较难想。

[回复评论](https://kexue.fm/archives/7038/comment-page-2?replyTo=22854#respond-post-7038)

kingdeewang

May 16th, 2025

请教苏神，公式2 从第二步到第三步是怎么推导的？

[回复评论](https://kexue.fm/archives/7038/comment-page-2?replyTo=27610#respond-post-7038)

[苏剑林](https://kexue.fm/) 发表于
May 17th, 2025

这就跟对r求微分一样操作呀。

[回复评论](https://kexue.fm/archives/7038/comment-page-2?replyTo=27624#respond-post-7038)

X

May 19th, 2025

式\\left(1\\right)就是所谓的Tweedie's Formula吧，感觉和卡尔曼滤波器有点关系。

[回复评论](https://kexue.fm/archives/7038/comment-page-2?replyTo=27634#respond-post-7038)

1. [«](https://kexue.fm/archives/7038/comment-page-1#comments)
2. [1](https://kexue.fm/archives/7038/comment-page-1#comments)
3. [2](https://kexue.fm/archives/7038/comment-page-2#comments)

[取消回复](https://kexue.fm/archives/7038#respond-post-7038)

你的大名

电子邮箱

个人网站（选填）

1\. 可以使用LaTeX代码，点击“预览效果”可查看效果；

2\. 可以通过点击评论楼层编号来引用该楼层；

3\. 网站可能会有点卡，如非确认评论失败，请 **不要重复点击提交**。

### 内容速览

[去噪自编码](https://kexue.fm/archives/7038#%E5%8E%BB%E5%99%AA%E8%87%AA%E7%BC%96%E7%A0%81)
[曲径通幽处](https://kexue.fm/archives/7038#%E6%9B%B2%E5%BE%84%E9%80%9A%E5%B9%BD%E5%A4%84)
[峰回路转间](https://kexue.fm/archives/7038#%E5%B3%B0%E5%9B%9E%E8%B7%AF%E8%BD%AC%E9%97%B4)
[曲终人散时](https://kexue.fm/archives/7038#%E6%9B%B2%E7%BB%88%E4%BA%BA%E6%95%A3%E6%97%B6)

### 智能搜索

支持整句搜索！网站自动使用 [结巴分词](https://github.com/fxsjy/jieba) 进行分词，并结合ngrams排序算法给出合理的搜索结果。

### 热门标签

[生成模型](https://kexue.fm/tag/%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B/) [attention](https://kexue.fm/tag/attention/) [优化](https://kexue.fm/tag/%E4%BC%98%E5%8C%96/) [语言模型](https://kexue.fm/tag/%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/) [模型](https://kexue.fm/tag/%E6%A8%A1%E5%9E%8B/) [梯度](https://kexue.fm/tag/%E6%A2%AF%E5%BA%A6/) [网站](https://kexue.fm/tag/%E7%BD%91%E7%AB%99/) [概率](https://kexue.fm/tag/%E6%A6%82%E7%8E%87/) [矩阵](https://kexue.fm/tag/%E7%9F%A9%E9%98%B5/) [优化器](https://kexue.fm/tag/%E4%BC%98%E5%8C%96%E5%99%A8/) [转载](https://kexue.fm/tag/%E8%BD%AC%E8%BD%BD/) [微分方程](https://kexue.fm/tag/%E5%BE%AE%E5%88%86%E6%96%B9%E7%A8%8B/) [分析](https://kexue.fm/tag/%E5%88%86%E6%9E%90/) [天象](https://kexue.fm/tag/%E5%A4%A9%E8%B1%A1/) [深度学习](https://kexue.fm/tag/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/) [积分](https://kexue.fm/tag/%E7%A7%AF%E5%88%86/) [python](https://kexue.fm/tag/python/) [扩散](https://kexue.fm/tag/%E6%89%A9%E6%95%A3/) [力学](https://kexue.fm/tag/%E5%8A%9B%E5%AD%A6/) [无监督](https://kexue.fm/tag/%E6%97%A0%E7%9B%91%E7%9D%A3/) [几何](https://kexue.fm/tag/%E5%87%A0%E4%BD%95/) [节日](https://kexue.fm/tag/%E8%8A%82%E6%97%A5/) [生活](https://kexue.fm/tag/%E7%94%9F%E6%B4%BB/) [文本生成](https://kexue.fm/tag/%E6%96%87%E6%9C%AC%E7%94%9F%E6%88%90/) [数论](https://kexue.fm/tag/%E6%95%B0%E8%AE%BA/)

### 随机文章

- [O-GAN：简单修改，让GAN的判别器变成一个编码器！](https://kexue.fm/archives/6409)
- [更换了一个相册程序](https://kexue.fm/archives/1622)
- [CoSENT（三）：作为交互式相似度的损失函数](https://kexue.fm/archives/9341)
- [齐次多项式不等式的机器证明（差分代换）](https://kexue.fm/archives/2747)
- [最受尊崇的3位诺贝尔奖得主](https://kexue.fm/archives/154)
- [《为什么现在的LLM都是Decoder-only的架构？》FAQ](https://kexue.fm/archives/9547)
- [不可能事件——一道经典电磁感应题的错误](https://kexue.fm/archives/1170)
- [《自然极值》系列——5.最速降线的故事](https://kexue.fm/archives/1094)
- [数值方法解方程之终极算法](https://kexue.fm/archives/590)
- [有限Vs无限:无穷电荷板的场\|平行板电容](https://kexue.fm/archives/1271)

### 最近评论

- [Bin](https://kexue.fm/archives/1990/comment-page-2#comment-29105): 今天偶然从某个论坛看到有人推荐您的博客，定睛一看竟然是华师同院的往届师兄！看到这篇2013年的...
- [Rapture D](https://kexue.fm/archives/11530/comment-page-1#comment-29104): 我有一个问题，为什么不考虑亥姆霍兹定理和斯托克斯公式。
- [mofheka](https://kexue.fm/archives/11390/comment-page-1#comment-29103): 苏神是还在用jax是么？最近在做基于Google Pathway的理念做一个动态版的MPMD框...
- [长琴](https://kexue.fm/archives/11530/comment-page-1#comment-29102): 看懂这篇博客也不是一件容易的事情。
- [AlexLi](https://kexue.fm/archives/9257/comment-page-4#comment-29101): 苏老师，请教一下(7)式中将 \\mu(x\_t) 传给 p\_o 进行推理的操作。 $x\_...
- [tyler\_zxc](https://kexue.fm/archives/7921/comment-page-2#comment-29100): "Performer的思想是将标准的Attention线性化，所以为什么不干脆直接训练一个线性...
- [我](https://kexue.fm/archives/11494/comment-page-1#comment-29099): 似乎并非mHC提出矩阵的思想？之前hyper connection就是了
- [winter](https://kexue.fm/archives/10847/comment-page-1#comment-29098): 苏神您好，假如对于比较均匀的attention weightP，往往呈现long tail分布...
- [苏剑林](https://kexue.fm/archives/8512/comment-page-2#comment-29097): KL散度、JS散度、W距离啥的，都行啊，看你喜欢哪个
- [苏剑林](https://kexue.fm/archives/9119/comment-page-14#comment-29096): 没有绝对公平的对比方法，主要看你关心什么。比如，如果只关心推理成本和推理效果，那么有的方法可以...

### 友情链接

- [Cool Papers](https://papers.cool/)
- [数学研发](https://bbs.emath.ac.cn/)
- [Seatop](http://www.seatop.com.cn/)
- [Xiaoxia](https://xiaoxia.org/)
- [积分表-网络版](https://kexue.fm/sci/integral/index.html)
- [丝路博傲](http://blog.dvxj.com/)
- [数学之家](http://www.2math.cn/)
- [有趣天文奇观](http://interesting-sky.china-vo.org/)
- [TwistedW](http://www.twistedwg.com/)
- [godweiyang](https://godweiyang.com/)
- [AI柠檬](https://blog.ailemon.net/)
- [王登科-DK博客](https://greatdk.com/)
- [ESON](https://blog.eson.org/)
- [枫之羽](https://fzhiy.net/)
- [coding-zuo](https://coding-zuo.github.io/)
- [博科园](https://www.bokeyuan.net/)
- [孔皮皮的博客](https://www.kppkkp.top/)
- [运鹏的博客](https://yunpengtai.top/)
- [jiming.site](https://jiming.site/)
- [OmegaXYZ](https://www.omegaxyz.com/)
- [EAI猩球](https://www.robotech.ink/)
- [文举的博客](https://liwenju0.com/)
- [申请链接](https://kexue.fm/links.html)

[![署名-非商业用途-保持一致](https://kexue.fm/usr/themes/geekg/images/cc.gif)](http://creativecommons.org/licenses/by-nc-nd/2.5/cn/) 本站采用创作共用版权协议，要求署名、非商业用途和保持一致。转载本站内容必须也遵循“ [署名-非商业用途-保持一致](http://creativecommons.org/licenses/by-nc-nd/2.5/cn/)”的创作共用协议。



© 2009-2026 Scientific Spaces. All rights reserved. Theme by [laogui](http://www.laogui.com/). Powered by [Typecho](http://typecho.org/). 备案号: [粤ICP备09093259号-1/2](https://beian.miit.gov.cn/ "粤ICP备09093259号")。