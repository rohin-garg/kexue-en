## SEARCH

## MENU

- [打赏](https://kexue.fm/reward.html)
- [公式](https://kexue.fm/latex.html)
- [天象](https://kexue.fm/ac.html)
- [链接](https://kexue.fm/links.html)
- [时光](https://kexue.fm/me.html)
- [博览](https://kexue.fm/science.html)
- [归档](https://kexue.fm/content.html)

## CATEGORIES

- [千奇百怪](https://kexue.fm/category/Everything)
- [天文探索](https://kexue.fm/category/Astronomy)
- [数学研究](https://kexue.fm/category/Mathematics)
- [物理化学](https://kexue.fm/category/Phy-chem)
- [信息时代](https://kexue.fm/category/Big-Data)
- [生物自然](https://kexue.fm/category/Biology)
- [图片摄影](https://kexue.fm/category/Photograph)
- [问题百科](https://kexue.fm/category/Questions)
- [生活/情感](https://kexue.fm/category/Life-Feeling)
- [资源共享](https://kexue.fm/category/Resources)

## NEWPOSTS

- [线性注意力简史：从模仿、创新到反哺](https://kexue.fm/archives/11033)
- [msign的导数](https://kexue.fm/archives/11025)
- [通过msign来计算mclip（奇...](https://kexue.fm/archives/11006)
- [msign算子的Newton-Sc...](https://kexue.fm/archives/10996)
- [等值振荡定理：最优多项式逼近的充要条件](https://kexue.fm/archives/10972)
- [生成扩散模型漫谈（三十）：从瞬时速...](https://kexue.fm/archives/10958)
- [MoE环游记：5、均匀分布的反思](https://kexue.fm/archives/10945)
- [msign算子的Newton-Sc...](https://kexue.fm/archives/10922)
- [Transformer升级之路：2...](https://kexue.fm/archives/10907)
- [一道概率不等式：盯着它到显然成立为止！](https://kexue.fm/archives/10902)

## COMMENTS

- [musicfish1973: 好的设计都是相似的,haha](https://kexue.fm/archives/8009/comment-page-1#comment-27953)
- [忍者猫: 这优化器的作者真的应该给你打钱](https://kexue.fm/archives/10592/comment-page-2#comment-27952)
- [Chaofa Yuan: 写得太好了](https://kexue.fm/archives/11033/comment-page-1#comment-27951)
- [Skyler Lin: respect苏神！](https://kexue.fm/archives/11033/comment-page-1#comment-27949)
- [宋佳铭: 对，个人感觉mean flow就是continuous tim...](https://kexue.fm/archives/10958/comment-page-1#comment-27947)
- [宋佳铭: 的确，对sg这个事情我感觉如果是用‘归纳’法做是不太能避免的，...](https://kexue.fm/archives/10958/comment-page-1#comment-27946)
- [MoFHeka: 苏老师您好，请问一下这套结论在稀疏参数上应该如何应用？比如大规...](https://kexue.fm/archives/10542/comment-page-1#comment-27945)
- [苏剑林: Temp LoRA倒是有印象，其实思想是一样的，如果我单独开一...](https://kexue.fm/archives/11033/comment-page-1#comment-27944)
- [苏剑林: 你搜搜mamba、rwkv甚至rnn做vision的工作，其实...](https://kexue.fm/archives/11033/comment-page-1#comment-27943)
- [苏剑林: 问题1可以看看 https://kexue.fm/archiv...](https://kexue.fm/archives/9379/comment-page-1#comment-27942)

## USERLOGIN

- [登录](https://kexue.fm/admin/login.php)

[科学空间\|Scientific Spaces](https://kexue.fm)

- [登录](https://kexue.fm/admin/login.php)
- [打赏](https://kexue.fm/reward.html)
- [公式](https://kexue.fm/latex.html)
- [天象](https://kexue.fm/ac.html)
- [链接](https://kexue.fm/links.html)
- [时光](https://kexue.fm/me.html)
- [博览](https://kexue.fm/science.html)
- [归档](https://kexue.fm/content.html)

渴望成为一个小飞侠

- [欢迎订阅](https://kexue.fm/feed)
- [个性邮箱](https://kexue.fm/archives/119)
- [天象信息](https://kexue.fm/ac.html)
- [观测ISS](https://kexue.fm/archives/41)
- [LaTeX](https://kexue.fm/latex.html)
- [关于博主](https://kexue.fm/me.html)

欢迎访问“科学空间”，这里将与您共同探讨自然科学，回味人生百态；也期待大家的分享～

- [**千奇百怪** Everything](https://kexue.fm/category/Everything)
- [**天文探索** Astronomy](https://kexue.fm/category/Astronomy)
- [**数学研究** Mathematics](https://kexue.fm/category/Mathematics)
- [**物理化学** Phy-chem](https://kexue.fm/category/Phy-chem)
- [**信息时代** Big-Data](https://kexue.fm/category/Big-Data)
- [**生物自然** Biology](https://kexue.fm/category/Biology)
- [**图片摄影** Photograph](https://kexue.fm/category/Photograph)
- [**问题百科** Questions](https://kexue.fm/category/Questions)
- [**生活/情感** Life-Feeling](https://kexue.fm/category/Life-Feeling)
- [**资源共享** Resources](https://kexue.fm/category/Resources)

- [**千奇百怪**](https://kexue.fm/category/Everything)
- [**天文探索**](https://kexue.fm/category/Astronomy)
- [**数学研究**](https://kexue.fm/category/Mathematics)
- [**物理化学**](https://kexue.fm/category/Phy-chem)
- [**信息时代**](https://kexue.fm/category/Big-Data)
- [**生物自然**](https://kexue.fm/category/Biology)
- [**图片摄影**](https://kexue.fm/category/Photograph)
- [**问题百科**](https://kexue.fm/category/Questions)
- [**生活/情感**](https://kexue.fm/category/Life-Feeling)
- [**资源共享**](https://kexue.fm/category/Resources)

[首页](https://kexue.fm) [信息时代](https://kexue.fm/category/Big-Data) 预训练一下，Transformer的长序列成绩还能涨不少！

8Oct

# [预训练一下，Transformer的长序列成绩还能涨不少！](https://kexue.fm/archives/9787)

By 苏剑林 \|
2023-10-08 \|
43518位读者\|

作为LLM的主流模型架构，Transformer在各类任务上的总体表现都出色，大多数情况下，Transformer的槽点只是它的平方复杂度，而不是效果——除了一个名为Long Range Arena（下面简称LRA）的Benchmark。一直以来，LRA一直是线性RNN类模型的“主场”，与之相比Transformer在上面有明显的差距，以至于让人怀疑这是否就是Transformer的固有缺陷。

不过，近日论文 [《Never Train from Scratch: Fair Comparison of Long-Sequence Models Requires Data-Driven Priors》](https://papers.cool/arxiv/2310.02980) 将这“缺失的一环”给补齐了。论文指出，缺乏预训练是Transformer在LRA上效果较差的主要原因，而所有架构都可以通过预训练获得一定的提升，Transformer的提升则更为明显。

## 旧背景 [\#](https://kexue.fm/archives/9787\#%E6%97%A7%E8%83%8C%E6%99%AF)

Long Range Arena（LRA）是长序列建模的一个Benchmark，提出自论文 [《Long Range Arena: A Benchmark for Efficient Transformers》](https://papers.cool/arxiv/2011.04006)，从论文标题就可以看出，LRA是为了测试各种Efficient版的Transformer而构建的，里边包含了多种类型的数据，序列长度从1k到16k不等，此前不少Efficient Transformer的工作也都在LRA进行了测试。虽然在代表性方面有些争议，但LRA依然不失为一个测试Efficient Transformer的长序列能力的经典Benchmark。

MEGA论文中的LRA结果

可能会让部分读者意外的是，标准的Transformer（XFM）在这个Benchmark上的成绩并不出色，明显落后于一系列线性RNN类模型，比如经典的SSM（ [S4](https://papers.cool/arxiv/2111.00396)、 [S4D](https://papers.cool/arxiv/2203.14343)、 [S5](https://papers.cool/arxiv/2208.04933)）或者此前我们介绍过的 [LRU](https://kexue.fm/archives/9554)，甚至于此前的SOTA模型 [MEGA](https://papers.cool/arxiv/2209.10655)，也需要在 [GAU](https://kexue.fm/archives/8934) 的基础上装备线性RNN模块（论文里边称为EMA）。总而言之，此前LRA上的模型排行情况，强烈地透露着“Attention可以有，但RNN必不可少”的信号。

**（注：LRA的完整成绩排行可以在 [https://paperswithcode.com/sota/long-range-modeling-on-lra](https://paperswithcode.com/sota/long-range-modeling-on-lra) 查阅。）**

## 新结论 [\#](https://kexue.fm/archives/9787\#%E6%96%B0%E7%BB%93%E8%AE%BA)

很明显， [《Never Train from Scratch: Fair Comparison of Long-Sequence Models Requires Data-Driven Priors》](https://papers.cool/arxiv/2310.02980) 的出现打破了这一印象，它指出用训练集预训练就可以大大缩小两者的差距，并进一步提出“无预训练，不公平”的观点。

“Transformer+预训练”相比于Transformer及各种Effective版的提升

预训练的做法很简单，任务选择MLM或者GPT都可以，数据集则还是原本的训练集，这样一来除了增加了算力消耗外，并没有引入额外的知识来源，所以比较是公平的。事实上，不管是Transformer还是RNN，经过预训练之后都能获得明显的提升，只不过Transformer的提升更加明显：

“Transformer+预训练”与“S4+预训练”

与SOTA模型的对比

事后来看，论文的结论并不让人意外，甚至有点“显然成立”的感觉，但此前大家似乎都没往这个方向去想（或者是想到了但觉得不是关键？），所以作者们首先意识到并证明预训练在LRA的重要性，依然是非常值得称赞的。

预训练的重要性实际上表明了Inductive Bias在LRA上的重要性，因为LRA为了使得序列足够Long，它的token颗粒度是非常细的，比如文本任务是以字母为token的，图像任务是以像素为token并直接将二维图像展平为一维序列的，很明显这些任务既需要远程依赖，又有明显的局域性，线性RNN正好非常贴合它的特性。而Transformer相对来说没有那么明显的Inductive Bias，它还需要额外加位置编码才有位置信息，而即便加了也没有显著的局域性，因此更需要预训练来适应数据特性，或者说，通过预训练来补充Inductive Bias。

## 全剧终 [\#](https://kexue.fm/archives/9787\#%E5%85%A8%E5%89%A7%E7%BB%88)

本文跟大家快速分享了一个较新的实验结论，即预训练能有效提高各种模型在LRA上的成绩，尤其是Transformer经过预训练之后，效果基本上也能接近SOTA梯队，这打破了笔者一直以来LRA必须要加线性RNN的印象。

_**转载到请包括本文地址：** [https://kexue.fm/archives/9787](https://kexue.fm/archives/9787)_

_**更详细的转载事宜请参考：**_ [《科学空间FAQ》](https://kexue.fm/archives/6508#%E6%96%87%E7%AB%A0%E5%A6%82%E4%BD%95%E8%BD%AC%E8%BD%BD/%E5%BC%95%E7%94%A8)

**如果您还有什么疑惑或建议，欢迎在下方评论区继续讨论。**

**如果您觉得本文还不错，欢迎 [分享](https://kexue.fm/archives/9787#share)/ [打赏](https://kexue.fm/archives/9787#pay) 本文。打赏并非要从中获得收益，而是希望知道科学空间获得了多少读者的真心关注。当然，如果你无视它，也不会影响你的阅读。再次表示欢迎和感谢！**

打赏

微信打赏

支付宝打赏

因为网站后台对打赏并无记录，因此欢迎在打赏时候备注留言。你还可以 [**点击这里**](http://mail.qq.com/cgi-bin/qm_share?t=qm_mailme&email=tN7d1drY3drrx8H0xcWa19vZ) 或在下方评论区留言来告知你的建议或需求。

**如果您需要引用本文，请参考：**

苏剑林. (Oct. 08, 2023). 《预训练一下，Transformer的长序列成绩还能涨不少！ 》\[Blog post\]. Retrieved from [https://kexue.fm/archives/9787](https://kexue.fm/archives/9787)

@online{kexuefm-9787,
        title={预训练一下，Transformer的长序列成绩还能涨不少！},
        author={苏剑林},
        year={2023},
        month={Oct},
        url={\\url{https://kexue.fm/archives/9787}},
}

分类： [信息时代](https://kexue.fm/category/Big-Data)    标签： [语言模型](https://kexue.fm/tag/%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/), [attention](https://kexue.fm/tag/attention/)[4 评论](https://kexue.fm/archives/9787#comments)

< [脑洞大开：非线性RNN居然也可以并行计算？](https://kexue.fm/archives/9783) \| [EMO：基于最优传输思想设计的分类损失函数](https://kexue.fm/archives/9797) >

### 你也许还对下面的内容感兴趣

- [线性注意力简史：从模仿、创新到反哺](https://kexue.fm/archives/11033)
- [Transformer升级之路：20、MLA究竟好在哪里？](https://kexue.fm/archives/10907)
- [Transformer升级之路：19、第二类旋转位置编码](https://kexue.fm/archives/10862)
- [细水长flow之TARFLOW：流模型满血归来？](https://kexue.fm/archives/10667)
- [“闭门造车”之多模态思路浅谈（三）：位置编码](https://kexue.fm/archives/10352)
- [Decoder-only的LLM为什么需要位置编码？](https://kexue.fm/archives/10347)
- [Monarch矩阵：计算高效的稀疏型矩阵分解](https://kexue.fm/archives/10249)
- [Transformer升级之路：18、RoPE的底数选择原则](https://kexue.fm/archives/10122)
- [缓存与效果的极限拉扯：从MHA、MQA、GQA到MLA](https://kexue.fm/archives/10091)
- [Transformer升级之路：17、多模态位置编码的简单思考](https://kexue.fm/archives/10040)

[发表你的看法](https://kexue.fm/archives/9787#comment_form)

[预训练一下，Transformer的长序列成绩还能涨不少！ R11; AI 資訊](https://news.aitime.space/2023/10/59115/)

October 8th, 2023

\[...\]​Read More \[...\]

[回复评论](https://kexue.fm/archives/9787/comment-page-1?replyTo=22857#respond-post-9787)

shomy

October 9th, 2023

赞一个，之前也一直很有疑问，这个榜单的有效性，结论经常与实际中的任务效果相悖。

[回复评论](https://kexue.fm/archives/9787/comment-page-1?replyTo=22861#respond-post-9787)

hazdzz 发表于
May 16th, 2024

为什么这么说？你能举几个例子吗？

[回复评论](https://kexue.fm/archives/9787/comment-page-1?replyTo=24361#respond-post-9787)

[苏剑林](https://kexue.fm) 发表于
May 20th, 2024

原本这个榜单上transformer比各类线性RNN模型效果差一大截，实际的LLM则是transformer普遍比线性RNN模型效果要好，这不是经典的例子了嘛

[回复评论](https://kexue.fm/archives/9787/comment-page-1?replyTo=24377#respond-post-9787)

[取消回复](https://kexue.fm/archives/9787#respond-post-9787)

你的大名

电子邮箱

个人网站（选填）

1\. 可以使用LaTeX代码，点击“预览效果”可查看效果；2. 可以通过点击评论楼层编号来引用该楼层；3. 网站可能会有点卡，如非确认评论失败，请 **不要重复点击提交**。

### 内容速览

[旧背景](https://kexue.fm/archives/9787#%E6%97%A7%E8%83%8C%E6%99%AF)
[新结论](https://kexue.fm/archives/9787#%E6%96%B0%E7%BB%93%E8%AE%BA)
[全剧终](https://kexue.fm/archives/9787#%E5%85%A8%E5%89%A7%E7%BB%88)

### 智能搜索

支持整句搜索！网站自动使用 [结巴分词](https://github.com/fxsjy/jieba) 进行分词，并结合ngrams排序算法给出合理的搜索结果。

### 热门标签

[生成模型](https://kexue.fm/tag/%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B/) [attention](https://kexue.fm/tag/attention/) [优化](https://kexue.fm/tag/%E4%BC%98%E5%8C%96/) [语言模型](https://kexue.fm/tag/%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/) [模型](https://kexue.fm/tag/%E6%A8%A1%E5%9E%8B/) [网站](https://kexue.fm/tag/%E7%BD%91%E7%AB%99/) [概率](https://kexue.fm/tag/%E6%A6%82%E7%8E%87/) [梯度](https://kexue.fm/tag/%E6%A2%AF%E5%BA%A6/) [转载](https://kexue.fm/tag/%E8%BD%AC%E8%BD%BD/) [微分方程](https://kexue.fm/tag/%E5%BE%AE%E5%88%86%E6%96%B9%E7%A8%8B/) [矩阵](https://kexue.fm/tag/%E7%9F%A9%E9%98%B5/) [天象](https://kexue.fm/tag/%E5%A4%A9%E8%B1%A1/) [分析](https://kexue.fm/tag/%E5%88%86%E6%9E%90/) [深度学习](https://kexue.fm/tag/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/) [积分](https://kexue.fm/tag/%E7%A7%AF%E5%88%86/) [python](https://kexue.fm/tag/python/) [优化器](https://kexue.fm/tag/%E4%BC%98%E5%8C%96%E5%99%A8/) [力学](https://kexue.fm/tag/%E5%8A%9B%E5%AD%A6/) [无监督](https://kexue.fm/tag/%E6%97%A0%E7%9B%91%E7%9D%A3/) [扩散](https://kexue.fm/tag/%E6%89%A9%E6%95%A3/) [几何](https://kexue.fm/tag/%E5%87%A0%E4%BD%95/) [节日](https://kexue.fm/tag/%E8%8A%82%E6%97%A5/) [生活](https://kexue.fm/tag/%E7%94%9F%E6%B4%BB/) [文本生成](https://kexue.fm/tag/%E6%96%87%E6%9C%AC%E7%94%9F%E6%88%90/) [数论](https://kexue.fm/tag/%E6%95%B0%E8%AE%BA/)

### 随机文章

- [“末日”的快乐！](https://kexue.fm/archives/1840)
- [百度实体链接比赛后记：行为建模和实体链接](https://kexue.fm/archives/6919)
- [近乎完美地解决MathJax与Marked的冲突](https://kexue.fm/archives/10332)
- [生成扩散模型漫谈（十）：统一扩散模型（理论篇）](https://kexue.fm/archives/9262)
- [变分法的一个技巧及其“误用”](https://kexue.fm/archives/2040)
- [不求珍馐百味，但愿开水白菜](https://kexue.fm/archives/2445)
- [班门弄斧：Python的代码能有多简洁？](https://kexue.fm/archives/2971)
- [通过ssh动态端口转发共享校园资源（附带干货）](https://kexue.fm/archives/3651)
- [【NASA每日一图】宇宙中的鲸鱼和曲棍球棒](https://kexue.fm/archives/90)
- [让风筝飞](https://kexue.fm/archives/2891)

### 最近评论

- [musicfish1973](https://kexue.fm/archives/8009/comment-page-1#comment-27953): 好的设计都是相似的,haha
- [忍者猫](https://kexue.fm/archives/10592/comment-page-2#comment-27952): 这优化器的作者真的应该给你打钱
- [Chaofa Yuan](https://kexue.fm/archives/11033/comment-page-1#comment-27951): 写得太好了
- [Skyler Lin](https://kexue.fm/archives/11033/comment-page-1#comment-27949): respect苏神！
- [宋佳铭](https://kexue.fm/archives/10958/comment-page-1#comment-27947): 对，个人感觉mean flow就是continuous time CTM
- [宋佳铭](https://kexue.fm/archives/10958/comment-page-1#comment-27946): 的确，对sg这个事情我感觉如果是用‘归纳’法做是不太能避免的，因为毕竟是用步长短的模型去约束步...
- [MoFHeka](https://kexue.fm/archives/10542/comment-page-1#comment-27945): 苏老师您好，请问一下这套结论在稀疏参数上应该如何应用？比如大规模稀疏Embedding，每个B...
- [苏剑林](https://kexue.fm/archives/11033/comment-page-1#comment-27944): Temp LoRA倒是有印象，其实思想是一样的，如果我单独开一篇文章介绍TTT的话，应该会提到...
- [苏剑林](https://kexue.fm/archives/11033/comment-page-1#comment-27943): 你搜搜mamba、rwkv甚至rnn做vision的工作，其实不少。不过多数确实像你说的，正反...
- [苏剑林](https://kexue.fm/archives/9379/comment-page-1#comment-27942): 问题1可以看看 https://kexue.fm/archives/4718 ，简单来说就是点...

### 友情链接

- [Cool Papers](https://papers.cool)
- [数学研发](https://bbs.emath.ac.cn)
- [Seatop](http://www.seatop.com.cn/)
- [Xiaoxia](https://xiaoxia.org/)
- [积分表-网络版](https://kexue.fm/sci/integral/index.html)
- [丝路博傲](http://blog.dvxj.com/)
- [ph4ntasy 饭特稀](http://www.ph4ntasy.com/)
- [数学之家](http://www.2math.cn/)
- [有趣天文奇观](http://interesting-sky.china-vo.org/)
- [TwistedW](http://www.twistedwg.com/)
- [godweiyang](https://godweiyang.com/)
- [AI柠檬](https://blog.ailemon.net/)
- [王登科-DK博客](https://greatdk.com)
- [ESON](https://blog.eson.org/)
- [枫之羽](https://fzhiy.net/)
- [Mathor's blog](https://wmathor.com/)
- [coding-zuo](https://coding-zuo.github.io/)
- [博科园](https://www.bokeyuan.net/)
- [孔皮皮的博客](https://www.kppkkp.top/)
- [运鹏的博客](https://yunpengtai.top/)
- [jiming.site](https://jiming.site/)
- [OmegaXYZ](https://www.omegaxyz.com/)
- [Blog by Eacls](https://www.eacls.top/)
- [EAI猩球](https://www.robotech.ink/)
- [文举的博客](https://liwenju0.com/)
- [用代码打点酱油](https://bruceyuan.com/)
- [申请链接](https://kexue.fm/links.html)

本站采用创作共用版权协议，要求署名、非商业用途和保持一致。转载本站内容必须也遵循“ [署名-非商业用途-保持一致](http://creativecommons.org/licenses/by-nc-nd/2.5/cn/)”的创作共用协议。
© 2009-2025 Scientific Spaces. All rights reserved. Theme by [laogui](http://www.laogui.com). Powered by [Typecho](http://typecho.org). 备案号: [粤ICP备09093259号-1/2](https://beian.miit.gov.cn/)。