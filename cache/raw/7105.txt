级联抑制：提升GAN表现的一种简单有效的方法 - 科学空间|Scientific Spaces
![MobileSideBar](https://kexue.fm/usr/themes/geekg/images/slide-button.png "MobileSideBar")
## SEARCH
## MENU
* [打赏](https://kexue.fm/reward.html)
* [公式](https://kexue.fm/latex.html)
* [天象](https://kexue.fm/ac.html)
* [链接](https://kexue.fm/links.html)
* [时光](https://kexue.fm/me.html)
* [博览](https://kexue.fm/science.html)
* [归档](https://kexue.fm/content.html)
## CATEGORIES
* [千奇百怪](https://kexue.fm/category/Everything)
* [天文探索](https://kexue.fm/category/Astronomy)
* [数学研究](https://kexue.fm/category/Mathematics)
* [物理化学](https://kexue.fm/category/Phy-chem)
* [信息时代](https://kexue.fm/category/Big-Data)
* [生物自然](https://kexue.fm/category/Biology)
* [图片摄影](https://kexue.fm/category/Photograph)
* [问题百科](https://kexue.fm/category/Questions)
* [生活/情感](https://kexue.fm/category/Life-Feeling)
* [资源共享](https://kexue.fm/category/Resources)
## NEWPOSTS
* [让炼丹更科学一些（五）：基于梯度精...](https://kexue.fm/archives/11530)
* [让炼丹更科学一些（四）：新恒等式，...](https://kexue.fm/archives/11494)
* [为什么DeltaNet要加L2 N...](https://kexue.fm/archives/11486)
* [让炼丹更科学一些（三）：SGD的终...](https://kexue.fm/archives/11480)
* [让炼丹更科学一些（二）：将结论推广...](https://kexue.fm/archives/11469)
* [滑动平均视角下的权重衰减和学习率](https://kexue.fm/archives/11459)
* [生成扩散模型漫谈（三十一）：预测数...](https://kexue.fm/archives/11428)
* [Muon优化器指南：快速上手与关键细节](https://kexue.fm/archives/11416)
* [AdamW的Weight RMS的...](https://kexue.fm/archives/11404)
* [n个正态随机数的最大值的渐近估计](https://kexue.fm/archives/11390)
## COMMENTS
* [Rapture D: 我有一个问题，为什么不考虑亥姆霍兹定理和斯托克斯公式。](https://kexue.fm/archives/11530/comment-page-1#comment-29104)
* [mofheka: 苏神是还在用jax是么？最近在做基于Google Pathwa...](https://kexue.fm/archives/11390/comment-page-1#comment-29103)
* [长琴: 看懂这篇博客也不是一件容易的事情。](https://kexue.fm/archives/11530/comment-page-1#comment-29102)
* [AlexLi: 苏老师，请教一下(7)式中将 $\\mu(x\_t)$ 传给$p...](https://kexue.fm/archives/9257/comment-page-4#comment-29101)
* [tyler\_zxc: "Performer的思想是将标准的Attention线性化，...](https://kexue.fm/archives/7921/comment-page-2#comment-29100)
* [我: 似乎并非mHC提出矩阵的思想？之前hyper connecti...](https://kexue.fm/archives/11494/comment-page-1#comment-29099)
* [winter: 苏神您好，假如对于比较均匀的attention weightP...](https://kexue.fm/archives/10847/comment-page-1#comment-29098)
* [苏剑林: KL散度、JS散度、W距离啥的，都行啊，看你喜欢哪个](https://kexue.fm/archives/8512/comment-page-2#comment-29097)
* [苏剑林: 没有绝对公平的对比方法，主要看你关心什么。比如，如果只关心推理...](https://kexue.fm/archives/9119/comment-page-14#comment-29096)
* [苏剑林: 如果我有时间重新搭建博客，应该会用python自己写了，而不用...](https://kexue.fm/links.html/comment-page-6#comment-29095)
## USERLOGIN
* [登录](https://kexue.fm/admin/login.php)
[科学空间|Scientific Spaces](https://kexue.fm)
* [登录](https://kexue.fm/admin/login.php)
* [打赏](https://kexue.fm/reward.html)
* [公式](https://kexue.fm/latex.html)
* [天象](https://kexue.fm/ac.html)
* [链接](https://kexue.fm/links.html)
* [时光](https://kexue.fm/me.html)
* [博览](https://kexue.fm/science.html)
* [归档](https://kexue.fm/content.html)
渴望成为一个小飞侠* [![](https://kexue.fm/usr/themes/geekg/images/rss.png)
欢迎订阅](https://kexue.fm/feed)
* [![](https://kexue.fm/usr/themes/geekg/images/mail.png)
个性邮箱](https://kexue.fm/archives/119)
* [![](https://kexue.fm/usr/themes/geekg/images/Saturn.png)
天象信息](https://kexue.fm/ac.html)
* [![](https://kexue.fm/usr/themes/geekg/images/iss.png)
观测ISS](https://kexue.fm/archives/41)
* [![](https://kexue.fm/usr/themes/geekg/images/pi.png)
LaTeX](https://kexue.fm/latex.html)
* [![](https://kexue.fm/usr/themes/geekg/images/mlogo.png)
关于博主](https://kexue.fm/me.html)
欢迎访问“科学空间”，这里将与您共同探讨自然科学，回味人生百态；也期待大家的分享～* [**千奇百怪**Everything](https://kexue.fm/category/Everything)
* [**天文探索**Astronomy](https://kexue.fm/category/Astronomy)
* [**数学研究**Mathematics](https://kexue.fm/category/Mathematics)
* [**物理化学**Phy-chem](https://kexue.fm/category/Phy-chem)
* [**信息时代**Big-Data](https://kexue.fm/category/Big-Data)
* [**生物自然**Biology](https://kexue.fm/category/Biology)
* [**图片摄影**Photograph](https://kexue.fm/category/Photograph)
* [**问题百科**Questions](https://kexue.fm/category/Questions)
* [**生活/情感**Life-Feeling](https://kexue.fm/category/Life-Feeling)
* [**资源共享**Resources](https://kexue.fm/category/Resources)
* [**千奇百怪**](https://kexue.fm/category/Everything)
* [**天文探索**](https://kexue.fm/category/Astronomy)
* [**数学研究**](https://kexue.fm/category/Mathematics)
* [**物理化学**](https://kexue.fm/category/Phy-chem)
* [**信息时代**](https://kexue.fm/category/Big-Data)
* [**生物自然**](https://kexue.fm/category/Biology)
* [**图片摄影**](https://kexue.fm/category/Photograph)
* [**问题百科**](https://kexue.fm/category/Questions)
* [**生活/情感**](https://kexue.fm/category/Life-Feeling)
* [**资源共享**](https://kexue.fm/category/Resources)
[首页](https://kexue.fm)[信息时代](https://kexue.fm/category/Big-Data)级联抑制：提升GAN表现的一种简单有效的方法
1Dec
# [级联抑制：提升GAN表现的一种简单有效的方法](https://kexue.fm/archives/7105)
By苏剑林|2019-12-01|42523位读者|:
昨天刷arxiv时发现了一篇来自~~星星~~韩国的论文，名字很直白，就叫做[《A Simple yet Effective Way for Improving the Performance of GANs》](https://papers.cool/arxiv/1911.10979)。打开一看，发现内容也很简练，就是提出了一种加强GAN的判别器的方法，能让GAN的生成指标有一定的提升。
作者把这个方法叫做Cascading Rejection，我不知道咋翻译，扔到百度翻译里边显示“级联抑制”，想想看好像是有这么点味道，就暂时这样叫着了。介绍这个方法倒不是因为它有多强大，而是觉得它的几何意义很有趣，而且似乎有一定的启发性。
## 正交分解[#](#正交分解)
GAN的判别器一般是经过多层卷积后，通过flatten或pool得到一个固定长度的向量$\\boldsymbol{v}$，然后再与一个权重向量$\\boldsymbol{w}$做内积，得到一个标量打分（先不考虑偏置项和激活函数等末节）：
\\begin{equation}D(\\boldsymbol{x})=\\langle \\boldsymbol{v},\\boldsymbol{w}\\rangle\\end{equation}
也就是说，用$\\boldsymbol{v}$作为输入图片的表征，然后通过$\\boldsymbol{v}$和$\\boldsymbol{w}$的内积大小来判断出这个图片的“真”的程度。
然而，$\\langle \\boldsymbol{v},\\boldsymbol{w}\\rangle$只取决于$\\boldsymbol{v}$在$\\boldsymbol{w}$上的投影分量，换言之，固定$\\langle \\boldsymbol{v},\\boldsymbol{w}\\rangle$和$\\boldsymbol{w}$时，$\\boldsymbol{v}$仍然可以有很大的变动，如下面左图所示。
[![与w内积相等的v向量可以差异很大](https://kexue.fm/usr/uploads/2019/11/873030710.png)](https://kexue.fm/usr/uploads/2019/11/873030710.png)
与w内积相等的v向量可以差异很大
[![v的投影分量和垂直分量](https://kexue.fm/usr/uploads/2019/11/361353465.png)](https://kexue.fm/usr/uploads/2019/11/361353465.png)
v的投影分量和垂直分量
假如我们认为$\\langle \\boldsymbol{v},\\boldsymbol{w}\\rangle$等于某个值时图片就为真，问题是$\\boldsymbol{v}$变化那么大，难道每一个$\\boldsymbol{v}$都代表一张真实图片吗？显然不一定。这就反映了通过内积来打分的问题所在：它只考虑了在$\\boldsymbol{w}$上的投影分量，没有考虑垂直分量（如上面右图）：
\\begin{equation}\\boldsymbol{v}-\\Vert \\boldsymbol{v}\\Vert \\cos(\\boldsymbol{v},\\boldsymbol{w}) \\frac{\\boldsymbol{w}}{\\Vert \\boldsymbol{w}\\Vert}=\\boldsymbol{v}- \\frac{\\langle\\boldsymbol{v},\\boldsymbol{w}\\rangle}{\\Vert \\boldsymbol{w}\\Vert^2}\\boldsymbol{w}\\end{equation}
既然如此，一个很自然的想法是：能否用另一个参数向量来对这个垂直分量在做一次分类呢？显然是可以的，而且这个垂直分量的再次分类时也会导致一个新的垂直分量，因此这个过程可以迭代下去：\\begin{equation}\\left\\{\\begin{aligned}&\bolds&\boldsymbol{v}\_1=\\boldsymbol{v}\\\\
&D_1(\b&D_1(\boldsymbol{x})=\\langle \\boldsymbol{v}\_1,\\boldsymbol{w}\_1\\rangle\\\\
&\bolds&\boldsymbol{v}\_2 = \\boldsymbol{v}\_1- \\frac{\\langle\\boldsymbol{v}\_1,\\boldsymbol{w}\_1\\rangle}{\\Vert \\boldsymbol{w}\_1\\Vert^2}\\boldsymbol{w}\_1\\\\
&D_2(\b&D_2(\boldsymbol{x})=\\langle \\boldsymbol{v}\_2,\\boldsymbol{w}\_2\\rangle\\\\
&\bolds&\boldsymbol{v}\_3 = \\boldsymbol{v}\_2- \\frac{\\langle\\boldsymbol{v}\_2,\\boldsymbol{w}\_2\\rangle}{\\Vert \\boldsymbol{w}\_2\\Vert^2}\\boldsymbol{w}\_2\\\\
&D_3(\b&D_3(\boldsymbol{x})=\\langle \\boldsymbol{v}\_3,\\boldsymbol{w}\_3\\rangle\\\\
&\bolds&\boldsymbol{v}\_4 = \\boldsymbol{v}\_3- \\frac{\\langle\\boldsymbol{v}\_3,\\boldsymbol{w}\_3\\rangle}{\\Vert \\boldsymbol{w}\_3\\Vert^2}\\boldsymbol{w}\_3\\\\
&\qquad&\qquad\\vdots\\\\
&D_N(\b&D_N(\boldsymbol{x})=\\langle \\boldsymbol{v}\_N,\\boldsymbol{w}\_N\\rangle\\\\
\\end{aligned}\\right.\\end{equation}
## 分析思考[#](#分析思考)
其实写到这，原论文的思路基本上已经说完了，剩下的是一些细节上的操作。首先已经有了$N$个打分$D\_1(\\boldsymbol{x}),D\_2(\\boldsymbol{x}),\\dots,D\_N(\\boldsymbol{x})$，每个打分都可以应用判别器的loss（直接用hinge loss或者加sigmoid激活后用交叉熵），最后对这$N$个loss加权平均，作为最终的判别器loss，仅这样就能带来GAN的性能提升了。作者还将其进一步推广到CGAN中，也得到了不错的效果。
[![论文提出的GAN技巧的实验结果](https://kexue.fm/usr/uploads/2019/11/1864282304.png)](https://kexue.fm/usr/uploads/2019/11/1864282304.png)
论文提出的GAN技巧的实验结果
相比实验结果，笔者认为这个技巧更深层次的意义更值得关注。其实这个思路可以按理说可以用到一般的分类问题中而不单单是GAN。由于把垂直分量都迭代地加入了预测，我们可以认为参数$\\boldsymbol{w}\_1,\\boldsymbol{w}\_2,\\dots,\\boldsymbol{w}\_N$分别代表了$N$个不同的视角，而每一个分类相当于在不同的视角下进行分类判断。
想到这里，笔者想起了Hinton的[Capsule](https://kexue.fm/tag/Capsule/)。虽然形式上不大一样，但本意上似乎有相通之处，Capsule希望用一个向量而不是标量来表示一个实体，这里的“级联抑制”也是通过不断进行垂直分解来给出多个角度的分类结果，也就是说认定一个向量是不是属于一个类，必须给出多个打分而不单是一个，这也有“用向量而不是标量”的味道。
遗憾的是，笔者按上述思路简单实验了一下（cifar10），发现验证集的分类准确率下降了一点（注意这跟GAN的结果不矛盾，提升GAN的表现是因为加大了判别难度，但是有监督的分类模型不希望加大判别难度），但是好在过拟合程度也减少了（即训练集和验证集的准确率差距减少了），当然笔者的实验过于简陋，不能做到严谨地下结论。不过笔者依然觉得，由于其鲜明的几何意义，这个技巧仍然值得进一步思考。
## 文章小结[#](#文章小结)
本文介绍了一个具有鲜明几何意义的提升GAN表现的技巧，并且进一步讨论了它进一步的潜在价值。
***转载到请包括本文地址：** [https://kexue.fm/archives/7105](https://kexue.fm/archives/7105)*
***更详细的转载事宜请参考：*** [《科学空间FAQ》](https://kexue.fm/archives/6508#文章如何转载/引用)
**如果您还有什么疑惑或建议，欢迎在下方评论区继续讨论。**
**如果您觉得本文还不错，欢迎[分享](#share)/[打赏](#pay)本文。打赏并非要从中获得收益，而是希望知道科学空间获得了多少读者的真心关注。当然，如果你无视它，也不会影响你的阅读。再次表示欢迎和感谢！**
打赏![科学空间](https://kexue.fm/usr/themes/geekg/payment/wx.png)
微信打赏![科学空间](https://kexue.fm/usr/themes/geekg/payment/zfb.png)
支付宝打赏因为网站后台对打赏并无记录，因此欢迎在打赏时候备注留言。 你还可以[**点击这里**](http://mail.qq.com/cgi-bin/qm_share?t=qm_mailme&email=tN7d1drY3drrx8H0xcWa19vZ)或在下方评论区留言来告知你的建议或需求。
**如果您需要引用本文，请参考：**
苏剑林. (Dec. 01, 2019). 《级联抑制：提升GAN表现的一种简单有效的方法 》[Blog post]. Retrieved from[https://kexue.fm/archives/7105](https://kexue.fm/archives/7105)
@online{kexuefm-7105,
title={级联抑制：提升GAN表现的一种简单有效的方法},
author={苏剑林},
year={2019},
month={Dec},
url={\\url{https://kexue.fm/archives/7105}},
}
分类：[信息时代](https://kexue.fm/category/Big-Data) 标签：[几何](https://kexue.fm/tag/几何/),[GAN](https://kexue.fm/tag/GAN/),[生成模型](https://kexue.fm/tag/生成模型/)[4 评论](https://kexue.fm/archives/7105#comments)
&lt;[6个派生优化器的简单介绍及其实现](https://kexue.fm/archives/7094)|[万能的seq2seq：基于seq2seq的阅读理解问答](https://kexue.fm/archives/7115)&gt;
### 你也许还对下面的内容感兴趣* [生成扩散模型漫谈（三十一）：预测数据而非噪声](https://kexue.fm/archives/11428)
* [DiVeQ：一种非常简洁的VQ训练方案](https://kexue.fm/archives/11328)
* [为什么线性注意力要加Short Conv？](https://kexue.fm/archives/11320)
* [Transformer升级之路：21、MLA好在哪里?（下）](https://kexue.fm/archives/11111)
* [线性注意力简史：从模仿、创新到反哺](https://kexue.fm/archives/11033)
* [生成扩散模型漫谈（三十）：从瞬时速度到平均速度](https://kexue.fm/archives/10958)
* [Transformer升级之路：20、MLA好在哪里?（上）](https://kexue.fm/archives/10907)
* [生成扩散模型漫谈（二十九）：用DDPM来离散编码](https://kexue.fm/archives/10711)
* [MoE环游记：1、从几何意义出发](https://kexue.fm/archives/10699)
* [三个球的交点坐标（三球交会定位）](https://kexue.fm/archives/10684)
[发表你的看法](#comment_form)
Dam
December 12th, 2019
你好，想问一下可以给我发一份您复现这篇论文的代码，谢谢啦[回复评论](https://kexue.fm/archives/7105/comment-page-1?replyTo=12600#respond-post-7105)
[苏剑林](https://kexue.fm)发表于 December 12th, 2019
我没复现。[回复评论](https://kexue.fm/archives/7105/comment-page-1?replyTo=12609#respond-post-7105)
洪晨October 9th, 2020
苏老师，您实验了使用垂直分解进行实验，请问垂直分解的代码怎么实现呢？[回复评论](https://kexue.fm/archives/7105/comment-page-1?replyTo=14521#respond-post-7105)
[苏剑林](https://kexue.fm)发表于 October 9th, 2020
按照公式进行实现～[回复评论](https://kexue.fm/archives/7105/comment-page-1?replyTo=14525#respond-post-7105)
[取消回复](https://kexue.fm/archives/7105#respond-post-7105)
你的大名电子邮箱个人网站（选填）1. 可以使用LaTeX代码，点击“预览效果”可查看效果；
2. 可以通过点击评论楼层编号来引用该楼层；3. 网站可能会有点卡，如非确认评论失败，请**不要重复点击提交**。
********************
### 内容速览* [正交分解](#正交分解)
* [分析思考](#分析思考)
* [文章小结](#文章小结)
********************
### 智能搜索支持整句搜索！网站自动使用[结巴分词](https://github.com/fxsjy/jieba)进行分词，并结合ngrams排序算法给出合理的搜索结果。
********************
### 热门标签[生成模型](https://kexue.fm/tag/生成模型/)[attention](https://kexue.fm/tag/attention/)[优化](https://kexue.fm/tag/优化/)[语言模型](https://kexue.fm/tag/语言模型/)[模型](https://kexue.fm/tag/模型/)[梯度](https://kexue.fm/tag/梯度/)[网站](https://kexue.fm/tag/网站/)[概率](https://kexue.fm/tag/概率/)[矩阵](https://kexue.fm/tag/矩阵/)[优化器](https://kexue.fm/tag/优化器/)[转载](https://kexue.fm/tag/转载/)[微分方程](https://kexue.fm/tag/微分方程/)[分析](https://kexue.fm/tag/分析/)[天象](https://kexue.fm/tag/天象/)[深度学习](https://kexue.fm/tag/深度学习/)[积分](https://kexue.fm/tag/积分/)[python](https://kexue.fm/tag/python/)[扩散](https://kexue.fm/tag/扩散/)[力学](https://kexue.fm/tag/力学/)[无监督](https://kexue.fm/tag/无监督/)[几何](https://kexue.fm/tag/几何/)[节日](https://kexue.fm/tag/节日/)[生活](https://kexue.fm/tag/生活/)[文本生成](https://kexue.fm/tag/文本生成/)[数论](https://kexue.fm/tag/数论/)
********************
********************
### 随机文章* [路径积分系列：3.路径积分](https://kexue.fm/archives/3757)
* [2009.7.22日全食各地区模拟(Flash)](https://kexue.fm/archives/18)
* [微型博客在中国](https://kexue.fm/archives/189)
* [解答不等式的误区...](https://kexue.fm/archives/644)
* [科学空间：2010年4月重要天象](https://kexue.fm/archives/566)
* [宇宙驿站十岁啦](https://kexue.fm/archives/1557)
* [fashion mnist的一个baseline (MobileNet 95%)](https://kexue.fm/archives/4556)
* [我的写论文软件组合](https://kexue.fm/archives/3171)
* [流形上的最速下降：3. Muon + Stiefel](https://kexue.fm/archives/11221)
* [一个非线性差分方程的隐函数解](https://kexue.fm/archives/3696)
********************
********************
### 最近评论* [Rapture D](https://kexue.fm/archives/11530/comment-page-1#comment-29104): 我有一个问题，为什么不考虑亥姆霍兹定理和斯托克斯公式。* [mofheka](https://kexue.fm/archives/11390/comment-page-1#comment-29103): 苏神是还在用jax是么？最近在做基于Google Pathway的理念做一个动态版的MPMD框...
* [长琴](https://kexue.fm/archives/11530/comment-page-1#comment-29102): 看懂这篇博客也不是一件容易的事情。* [AlexLi](https://kexue.fm/archives/9257/comment-page-4#comment-29101): 苏老师，请教一下(7)式中将 $\\mu(x\_t)$ 传给$p\_o$ 进行推理的操作。$x\_...
* [tyler\_zxc](https://kexue.fm/archives/7921/comment-page-2#comment-29100): "Performer的思想是将标准的Attention线性化，所以为什么不干脆直接训练一个线性...
* [我](https://kexue.fm/archives/11494/comment-page-1#comment-29099): 似乎并非mHC提出矩阵的思想？之前hyper connection就是了
* [winter](https://kexue.fm/archives/10847/comment-page-1#comment-29098): 苏神您好，假如对于比较均匀的attention weightP，往往呈现long tail分布...
* [苏剑林](https://kexue.fm/archives/8512/comment-page-2#comment-29097): KL散度、JS散度、W距离啥的，都行啊，看你喜欢哪个
* [苏剑林](https://kexue.fm/archives/9119/comment-page-14#comment-29096): 没有绝对公平的对比方法，主要看你关心什么。比如，如果只关心推理成本和推理效果，那么有的方法可以...
* [苏剑林](https://kexue.fm/links.html/comment-page-6#comment-29095): 如果我有时间重新搭建博客，应该会用python自己写了，而不用第三方架构，这样可玩性好很多。事...
********************
********************
### 友情链接* [Cool Papers](https://papers.cool)
* [数学研发](https://bbs.emath.ac.cn)
* [Seatop](http://www.seatop.com.cn/)
* [Xiaoxia](https://xiaoxia.org/)
* [积分表-网络版](https://kexue.fm/sci/integral/index.html)
* [丝路博傲](http://blog.dvxj.com/)
* [数学之家](http://www.2math.cn/)
* [有趣天文奇观](http://interesting-sky.china-vo.org/)
* [TwistedW](http://www.twistedwg.com/)
* [godweiyang](https://godweiyang.com/)
* [AI柠檬](https://blog.ailemon.net/)
* [王登科-DK博客](https://greatdk.com)
* [ESON](https://blog.eson.org/)
* [枫之羽](https://fzhiy.net/)
* [coding-zuo](https://coding-zuo.github.io/)
* [博科园](https://www.bokeyuan.net/)
* [孔皮皮的博客](https://www.kppkkp.top/)
* [运鹏的博客](https://yunpengtai.top/)
* [jiming.site](https://jiming.site/)
* [OmegaXYZ](https://www.omegaxyz.com/)
* [EAI猩球](https://www.robotech.ink/)
* [文举的博客](https://liwenju0.com/)
* [申请链接](https://kexue.fm/links.html)
********************
[![署名-非商业用途-保持一致](https://kexue.fm/usr/themes/geekg/images/cc.gif)](http://creativecommons.org/licenses/by-nc-nd/2.5/cn/)本站采用创作共用版权协议，要求署名、非商业用途和保持一致。转载本站内容必须也遵循“[署名-非商业用途-保持一致](http://creativecommons.org/licenses/by-nc-nd/2.5/cn/)”的创作共用协议。
©2009-2026 Scientific Spaces. All rights reserved. Theme by[laogui](http://www.laogui.com). Powered by[Typecho](http://typecho.org). 备案号:[粤ICP备09093259号-1/2](https://beian.miit.gov.cn/)。