## SEARCH

## MENU

- [打赏](https://kexue.fm/reward.html)
- [公式](https://kexue.fm/latex.html)
- [天象](https://kexue.fm/ac.html)
- [链接](https://kexue.fm/links.html)
- [时光](https://kexue.fm/me.html)
- [博览](https://kexue.fm/science.html)
- [归档](https://kexue.fm/content.html)

## CATEGORIES

- [千奇百怪](https://kexue.fm/category/Everything)
- [天文探索](https://kexue.fm/category/Astronomy)
- [数学研究](https://kexue.fm/category/Mathematics)
- [物理化学](https://kexue.fm/category/Phy-chem)
- [信息时代](https://kexue.fm/category/Big-Data)
- [生物自然](https://kexue.fm/category/Biology)
- [图片摄影](https://kexue.fm/category/Photograph)
- [问题百科](https://kexue.fm/category/Questions)
- [生活/情感](https://kexue.fm/category/Life-Feeling)
- [资源共享](https://kexue.fm/category/Resources)

## NEWPOSTS

- [线性注意力简史：从模仿、创新到反哺](https://kexue.fm/archives/11033)
- [msign的导数](https://kexue.fm/archives/11025)
- [通过msign来计算mclip（奇...](https://kexue.fm/archives/11006)
- [msign算子的Newton-Sc...](https://kexue.fm/archives/10996)
- [等值振荡定理：最优多项式逼近的充要条件](https://kexue.fm/archives/10972)
- [生成扩散模型漫谈（三十）：从瞬时速...](https://kexue.fm/archives/10958)
- [MoE环游记：5、均匀分布的反思](https://kexue.fm/archives/10945)
- [msign算子的Newton-Sc...](https://kexue.fm/archives/10922)
- [Transformer升级之路：2...](https://kexue.fm/archives/10907)
- [一道概率不等式：盯着它到显然成立为止！](https://kexue.fm/archives/10902)

## COMMENTS

- [忍者猫: 这优化器的作者真的应该给你打钱](https://kexue.fm/archives/10592/comment-page-2#comment-27952)
- [Chaofa Yuan: 写得太好了](https://kexue.fm/archives/11033/comment-page-1#comment-27951)
- [Skyler Lin: respect苏神！](https://kexue.fm/archives/11033/comment-page-1#comment-27949)
- [宋佳铭: 对，个人感觉mean flow就是continuous tim...](https://kexue.fm/archives/10958/comment-page-1#comment-27947)
- [宋佳铭: 的确，对sg这个事情我感觉如果是用‘归纳’法做是不太能避免的，...](https://kexue.fm/archives/10958/comment-page-1#comment-27946)
- [MoFHeka: 苏老师您好，请问一下这套结论在稀疏参数上应该如何应用？比如大规...](https://kexue.fm/archives/10542/comment-page-1#comment-27945)
- [苏剑林: Temp LoRA倒是有印象，其实思想是一样的，如果我单独开一...](https://kexue.fm/archives/11033/comment-page-1#comment-27944)
- [苏剑林: 你搜搜mamba、rwkv甚至rnn做vision的工作，其实...](https://kexue.fm/archives/11033/comment-page-1#comment-27943)
- [苏剑林: 问题1可以看看 https://kexue.fm/archiv...](https://kexue.fm/archives/9379/comment-page-1#comment-27942)
- [苏剑林: 你的“信息量”怎么定义？直观来说，reflow训练的是切线模型...](https://kexue.fm/archives/10958/comment-page-2#comment-27941)

## USERLOGIN

- [登录](https://kexue.fm/admin/login.php)

[科学空间\|Scientific Spaces](https://kexue.fm)

- [登录](https://kexue.fm/admin/login.php)
- [打赏](https://kexue.fm/reward.html)
- [公式](https://kexue.fm/latex.html)
- [天象](https://kexue.fm/ac.html)
- [链接](https://kexue.fm/links.html)
- [时光](https://kexue.fm/me.html)
- [博览](https://kexue.fm/science.html)
- [归档](https://kexue.fm/content.html)

渴望成为一个小飞侠

- [欢迎订阅](https://kexue.fm/feed)
- [个性邮箱](https://kexue.fm/archives/119)
- [天象信息](https://kexue.fm/ac.html)
- [观测ISS](https://kexue.fm/archives/41)
- [LaTeX](https://kexue.fm/latex.html)
- [关于博主](https://kexue.fm/me.html)

欢迎访问“科学空间”，这里将与您共同探讨自然科学，回味人生百态；也期待大家的分享～

- [**千奇百怪** Everything](https://kexue.fm/category/Everything)
- [**天文探索** Astronomy](https://kexue.fm/category/Astronomy)
- [**数学研究** Mathematics](https://kexue.fm/category/Mathematics)
- [**物理化学** Phy-chem](https://kexue.fm/category/Phy-chem)
- [**信息时代** Big-Data](https://kexue.fm/category/Big-Data)
- [**生物自然** Biology](https://kexue.fm/category/Biology)
- [**图片摄影** Photograph](https://kexue.fm/category/Photograph)
- [**问题百科** Questions](https://kexue.fm/category/Questions)
- [**生活/情感** Life-Feeling](https://kexue.fm/category/Life-Feeling)
- [**资源共享** Resources](https://kexue.fm/category/Resources)

- [**千奇百怪**](https://kexue.fm/category/Everything)
- [**天文探索**](https://kexue.fm/category/Astronomy)
- [**数学研究**](https://kexue.fm/category/Mathematics)
- [**物理化学**](https://kexue.fm/category/Phy-chem)
- [**信息时代**](https://kexue.fm/category/Big-Data)
- [**生物自然**](https://kexue.fm/category/Biology)
- [**图片摄影**](https://kexue.fm/category/Photograph)
- [**问题百科**](https://kexue.fm/category/Questions)
- [**生活/情感**](https://kexue.fm/category/Life-Feeling)
- [**资源共享**](https://kexue.fm/category/Resources)

[首页](https://kexue.fm) [信息时代](https://kexue.fm/category/Big-Data) 模型优化漫谈：BERT的初始标准差为什么是0.02？

8Nov

# [模型优化漫谈：BERT的初始标准差为什么是0.02？](https://kexue.fm/archives/8747)

By 苏剑林 \|
2021-11-08 \|
109822位读者\|

前几天在群里大家讨论到了“Transformer如何解决梯度消失”这个问题，答案有提到残差的，也有提到LN（Layer Norm）的。这些是否都是正确答案呢？事实上这是一个非常有趣而综合的问题，它其实关联到挺多模型细节，比如“BERT为什么要warmup？”、“BERT的初始化标准差为什么是0.02？”、“BERT做MLM预测之前为什么还要多加一层Dense？”，等等。本文就来集中讨论一下这些问题。

## 梯度消失说的是什么意思？ [\#](https://kexue.fm/archives/8747\#%E6%A2%AF%E5%BA%A6%E6%B6%88%E5%A4%B1%E8%AF%B4%E7%9A%84%E6%98%AF%E4%BB%80%E4%B9%88%E6%84%8F%E6%80%9D%EF%BC%9F)

在文章 [《也来谈谈RNN的梯度消失/爆炸问题》](https://kexue.fm/archives/7888) 中，我们曾讨论过RNN的梯度消失问题。事实上，一般模型的梯度消失现象也是类似，它指的是（主要是在模型的初始阶段）越靠近输入的层梯度越小，趋于零甚至等于零，而我们主要用的是基于梯度的优化器，所以梯度消失意味着我们没有很好的信号去调整优化前面的层。

换句话说，前面的层也许几乎没有得到更新，一直保持随机初始化的状态；只有比较靠近输出的层才更新得比较好，但这些层的输入是前面没有更新好的层的输出，所以输入质量可能会很糟糕（因为经过了一个近乎随机的变换），因此哪怕后面的层更新好了，总体效果也不好。最终，我们会观察到很反直觉的现象：模型越深，效果越差，哪怕训练集都如此。

解决梯度消失的一个标准方法就是残差链接，正式提出于 [ResNet](https://papers.cool/arxiv/1512.03385) 中。残差的思想非常简单直接：你不是担心输入的梯度会消失吗？那我直接给它补上一个梯度为常数的项不就行了？最简单地，将模型变成
\\begin{equation}y = x + F(x)\\end{equation}
这样一来，由于多了一条“直通”路$x$，就算$F(x)$中的$x$梯度消失了，$x$的梯度基本上也能得以保留，从而使得深层模型得到有效的训练。

## LN真的能缓解梯度消失？ [\#](https://kexue.fm/archives/8747\#LN%E7%9C%9F%E7%9A%84%E8%83%BD%E7%BC%93%E8%A7%A3%E6%A2%AF%E5%BA%A6%E6%B6%88%E5%A4%B1%EF%BC%9F)

然而，在BERT和最初的Transformer里边，使用的是Post Norm设计，它把Norm操作加在了残差之后：
\\begin{equation}x\_{t+1} = \\text{Norm}(x\_t + F\_t(x\_t))\\end{equation}
其实具体的Norm方法不大重要，不管是Batch Norm还是Layer Norm，结论都类似。在文章 [《浅谈Transformer的初始化、参数化与标准化》](https://kexue.fm/archives/8620) 中，我们已经分析过这种Norm结构，这里再来重复一下。

在初始化阶段，由于所有参数都是随机初始化的，所以我们可以认为$x$与$F(x)$是两个相互独立的随机向量，如果假设它们各自的方差是1，那么$x+F(x)$的方差就是2，而$\\text{Norm}$操作负责将方差重新变为1，那么在初始化阶段，$\\text{Norm}$操作就相当于“除以$\\sqrt{2}$”：
\\begin{equation}x\_{t+1} = \\frac{x\_t + F\_t(x\_t)}{\\sqrt{2}}\\end{equation}
递归下去就是
\\begin{equation}\\begin{aligned}
x\_l =&\\, \\frac{x\_{l-1}}{\\sqrt{2}} + \\frac{F\_{l-1}(x\_{l-1})}{\\sqrt{2}} \\\
=&\\, \\frac{x\_{l-2}}{2} + \\frac{F\_{l-2}(x\_{l-2})}{2} + \\frac{F\_{l-1}(x\_{l-1})}{\\sqrt{2}} \\\
=&\\, \\cdots \\\
=&\\,\\frac{x\_0}{2^{l/2}} + \\frac{F\_0(x\_0)}{2^{l/2}} + \\frac{F\_1(x\_1)}{2^{(l-1)/2}} + \\frac{F\_2(x\_2)}{2^{(l-2)/2}} + \\cdots + \\frac{F\_{l-1}(x\_{l-1})}{2^{1/2}}
\\end{aligned}\\end{equation}
我们知道，残差有利于解决梯度消失，但是在Post Norm中，残差这条通道被严重削弱了，越靠近输入，削弱得越严重，残差“名存实亡”。所以说，在Post Norm的BERT模型中，LN不仅不能缓解梯度消失，它还是梯度消失的“元凶”之一。

## 那我们为什么还要加LN？ [\#](https://kexue.fm/archives/8747\#%E9%82%A3%E6%88%91%E4%BB%AC%E4%B8%BA%E4%BB%80%E4%B9%88%E8%BF%98%E8%A6%81%E5%8A%A0LN%EF%BC%9F)

那么，问题自然就来了：既然LN还加剧了梯度消失，那直接去掉它不好吗？

是可以去掉，但是前面说了，$x+F(x)$的方差就是2了，残差越多方差就越大了，所以还是要加一个Norm操作，我们可以把它加到每个模块的输入，即变为$x+F(\\text{Norm}(x))$，最后的总输出再加个$\\text{Norm}$就行，这就是Pre Norm结构，这时候每个残差分支是平权的，而不是像Post Norm那样有指数衰减趋势。当然，也有完全不加Norm的，但需要对$F(x)$进行特殊的初始化，让它初始输出更接近于0，比如ReZero、Skip Init、Fixup等，这些在 [《浅谈Transformer的初始化、参数化与标准化》](https://kexue.fm/archives/8620) 也都已经介绍过了。

但是，抛开这些改进不说，Post Norm就没有可取之处吗？难道Transformer和BERT开始就带了一个完全失败的设计？

显然不大可能。虽然Post Norm会带来一定的梯度消失问题，但其实它也有其他方面的好处。最明显的是，它稳定了前向传播的数值，并且保持了每个模块的一致性。比如BERT base，我们可以在最后一层接一个Dense来分类，也可以取第6层接一个Dense来分类；但如果你是Pre Norm的话，取出中间层之后，你需要自己接一个LN然后再接Dense，否则越靠后的层方差越大，不利于优化。

其次，梯度消失也不全是“坏处”，其实对于Finetune阶段来说，它反而是好处。在Finetune的时候，我们通常希望优先调整靠近输出层的参数，不要过度调整靠近输入层的参数，以免严重破坏预训练效果。而梯度消失意味着越靠近输入层，其结果对最终输出的影响越弱，这正好是Finetune时所希望的。所以，预训练好的Post Norm模型，往往比Pre Norm模型有更好的Finetune效果，这我们在 [《RealFormer：把残差转移到Attention矩阵上面去》](https://kexue.fm/archives/8027) 也提到过。

## 我们真的担心梯度消失吗？ [\#](https://kexue.fm/archives/8747\#%E6%88%91%E4%BB%AC%E7%9C%9F%E7%9A%84%E6%8B%85%E5%BF%83%E6%A2%AF%E5%BA%A6%E6%B6%88%E5%A4%B1%E5%90%97%EF%BC%9F)

其实，最关键的原因是，在当前的各种自适应优化技术下，我们已经不大担心梯度消失问题了。

这是因为，当前NLP中主流的优化器是Adam及其变种。对于Adam来说，由于包含了动量和二阶矩校正，所以近似来看，它的更新量大致上为
\\begin{equation}\\Delta \\theta = -\\eta\\frac{\\mathbb{E}\_t\[g\_t\]}{\\sqrt{\\mathbb{E}\_t\[g\_t^2\]}}\\end{equation}
可以看到，分子分母是都是同量纲的，因此分式结果其实就是$\\mathcal{O}(1)$的量级，而更新量就是$\\mathcal{O}(\\eta)$量级。也就是说，理论上只要梯度的绝对值大于随机误差，那么对应的参数都会有常数量级的更新量；这跟SGD不一样，SGD的更新量是正比于梯度的，只要梯度小，更新量也会很小，如果梯度过小，那么参数几乎会没被更新。

所以，Post Norm的残差虽然被严重削弱，但是在base、large级别的模型中，它还不至于削弱到小于随机误差的地步，因此配合Adam等优化器，它还是可以得到有效更新的，也就有可能成功训练了。当然，只是有可能，事实上越深的Post Norm模型确实越难训练，比如要仔细调节学习率和Warmup等。

## Warmup是怎样起作用的？ [\#](https://kexue.fm/archives/8747\#Warmup%E6%98%AF%E6%80%8E%E6%A0%B7%E8%B5%B7%E4%BD%9C%E7%94%A8%E7%9A%84%EF%BC%9F)

大家可能已经听说过，Warmup是Transformer训练的关键步骤，没有它可能不收敛，或者收敛到比较糟糕的位置。为什么会这样呢？不是说有了Adam就不怕梯度消失了吗？

要注意的是，Adam解决的是梯度消失带来的参数更新量过小问题，也就是说，不管梯度消失与否，更新量都不会过小。但对于Post Norm结构的模型来说，梯度消失依然存在，只不过它的意义变了。根据泰勒展开式：
\\begin{equation}f(x+\\Delta x) \\approx f(x) + \\langle\\nabla\_x f(x), \\Delta x\\rangle\\end{equation}
也就是说增量$f(x+\\Delta x) - f(x)$是正比于梯度的，换句话说，梯度衡量了输出对输入的依赖程度。如果梯度消失，那么意味着模型的输出对输入的依赖变弱了。

Warmup是在训练开始阶段，将学习率从0缓增到指定大小，而不是一开始从指定大小训练。如果不进行Wamrup，那么模型一开始就快速地学习，由于梯度消失，模型对越靠后的层越敏感，也就是越靠后的层学习得越快，然后后面的层是以前面的层的输出为输入的，前面的层根本就没学好，所以后面的层虽然学得快，但却是建立在糟糕的输入基础上的。

很快地，后面的层以糟糕的输入为基础到达了一个糟糕的局部最优点，此时它的学习开始放缓（因为已经到达了它认为的最优点附近），同时反向传播给前面层的梯度信号进一步变弱，这就导致了前面的层的梯度变得不准。但我们说过，Adam的更新量是常数量级的，梯度不准，但更新量依然是数量级，意味着可能就是一个常数量级的随机噪声了，于是学习方向开始不合理，前面的输出开始崩盘，导致后面的层也一并崩盘。

所以，如果Post Norm结构的模型不进行Wamrup，我们能观察到的现象往往是：loss快速收敛到一个常数附近，然后再训练一段时间，loss开始发散，直至NAN。如果进行Wamrup，那么留给模型足够多的时间进行“预热”，在这个过程中，主要是抑制了后面的层的学习速度，并且给了前面的层更多的优化时间，以促进每个层的同步优化。

这里的讨论前提是梯度消失，如果是Pre Norm之类的结果，没有明显的梯度消失现象，那么不加Warmup往往也可以成功训练。

## 初始标准差为什么是0.02？ [\#](https://kexue.fm/archives/8747\#%E5%88%9D%E5%A7%8B%E6%A0%87%E5%87%86%E5%B7%AE%E4%B8%BA%E4%BB%80%E4%B9%88%E6%98%AF0.02%EF%BC%9F)

喜欢扣细节的同学会留意到，BERT默认的初始化方法是标准差为0.02的截断正态分布，在 [《浅谈Transformer的初始化、参数化与标准化》](https://kexue.fm/archives/8620) 我们也提过，由于是截断正态分布，所以实际标准差会更小，大约是$0.02/1.1368472\\approx 0.0176$。这个标准差是大还是小呢？对于Xavier初始化来说，一个$n\\times n$的矩阵应该用$1/n$的方差初始化，而BERT base的$n$为768，算出来的标准差是$1/\\sqrt{768}\\approx 0.0361$。这就意味着，这个初始化标准差是明显偏小的，大约只有常见初始化标准差的一半。

为什么BERT要用偏小的标准差初始化呢？事实上，这还是跟Post Norm设计有关，偏小的标准差会导致函数的输出整体偏小，从而使得Post Norm设计在初始化阶段更接近于恒等函数，从而更利于优化。具体来说，按照前面的假设，如果$x$的方差是1，$F(x)$的方差是$\\sigma^2$，那么初始化阶段，$\\text{Norm}$操作就相当于除以$\\sqrt{1+\\sigma^2}$。如果$\\sigma$比较小，那么残差中的“直路”权重就越接近于1，那么模型初始阶段就越接近一个恒等函数，就越不容易梯度消失。

正所谓“我们不怕梯度消失，但我们也不希望梯度消失”，简单地将初始化标注差设小一点，就可以使得$\\sigma$变小一点，从而在保持Post Norm的同时缓解一下梯度消失，何乐而不为？那能不能设置得更小甚至全零？一般来说初始化过小会丧失多样性，缩小了模型的试错空间，也会带来负面效果。综合来看，缩小到标准的1/2，是一个比较靠谱的选择了。

当然，也确实有人喜欢挑战极限的，最近笔者也看到了一篇文章，试图让整个模型用几乎全零的初始化，还训练出了不错的效果，大家有兴趣可以读读，文章为 [《ZerO Initialization: Initializing Residual Networks with only Zeros and Ones》](https://papers.cool/arxiv/2110.12661)。

## 为什么MLM要多加Dense？ [\#](https://kexue.fm/archives/8747\#%E4%B8%BA%E4%BB%80%E4%B9%88MLM%E8%A6%81%E5%A4%9A%E5%8A%A0Dense%EF%BC%9F)

最后，是关于BERT的MLM模型的一个细节，就是BERT在做MLM的概率预测之前，还要多接一个Dense层和LN层，这是为什么呢？不接不行吗？

之前看到过的答案大致上是觉得，越靠近输出层的，越是依赖任务的（Task-Specified），我们多接一个Dense层，希望这个Dense层是MLM-Specified的，然后下游任务微调的时候就不是MLM-Specified的，所以把它去掉。这个解释看上去有点合理，但总感觉有点玄学，毕竟Task-Specified这种东西不大好定量分析。

这里笔者给出另外一个更具体的解释，事实上它还是跟BERT用了0.02的标准差初始化直接相关。刚才我们说了，这个初始化是偏小的，如果我们不额外加Dense就乘上Embedding预测概率分布，那么得到的分布就过于均匀了（Softmax之前，每个logit都接近于0），于是模型就想着要把数值放大。现在模型有两个选择：第一，放大Embedding层的数值，但是Embedding层的更新是稀疏的，一个个放大太麻烦；第二，就是放大输入，我们知道BERT编码器最后一层是LN，LN最后有个初始化为1的gamma参数，直接将那个参数放大就好。

模型优化使用的是梯度下降，我们知道它会选择最快的路径，显然是第二个选择更快，所以模型会优先走第二条路。这就导致了一个现象：最后一个LN层的gamma值会偏大。如果预测MLM概率分布之前不加一个Dense+LN，那么BERT编码器的最后一层的LN的gamma值会偏大，导致最后一层的方差会比其他层的明显大，显然不够优雅；而多加了一个Dense+LN后，偏大的gamma就转移到了新增的LN上去了，而编码器的每一层则保持了一致性。

事实上，读者可以自己去观察一下BERT每个LN层的gamma值，就会发现确实是最后一个LN层的gamma值是会明显偏大的，这就验证了我们的猜测～

## 希望大家多多海涵批评斧正 [\#](https://kexue.fm/archives/8747\#%E5%B8%8C%E6%9C%9B%E5%A4%A7%E5%AE%B6%E5%A4%9A%E5%A4%9A%E6%B5%B7%E6%B6%B5%E6%89%B9%E8%AF%84%E6%96%A7%E6%AD%A3)

本文试图回答了Transformer、BERT的模型优化相关的几个问题，有一些是笔者在自己的预训练工作中发现的结果，有一些则是结合自己的经验所做的直观想象。不管怎样，算是分享一个参考答案吧，如果有不当的地方，请大家海涵，也请各位批评斧正～

_**转载到请包括本文地址：** [https://kexue.fm/archives/8747](https://kexue.fm/archives/8747)_

_**更详细的转载事宜请参考：**_ [《科学空间FAQ》](https://kexue.fm/archives/6508#%E6%96%87%E7%AB%A0%E5%A6%82%E4%BD%95%E8%BD%AC%E8%BD%BD/%E5%BC%95%E7%94%A8)

**如果您还有什么疑惑或建议，欢迎在下方评论区继续讨论。**

**如果您觉得本文还不错，欢迎 [分享](https://kexue.fm/archives/8747#share)/ [打赏](https://kexue.fm/archives/8747#pay) 本文。打赏并非要从中获得收益，而是希望知道科学空间获得了多少读者的真心关注。当然，如果你无视它，也不会影响你的阅读。再次表示欢迎和感谢！**

打赏

微信打赏

支付宝打赏

因为网站后台对打赏并无记录，因此欢迎在打赏时候备注留言。你还可以 [**点击这里**](http://mail.qq.com/cgi-bin/qm_share?t=qm_mailme&email=tN7d1drY3drrx8H0xcWa19vZ) 或在下方评论区留言来告知你的建议或需求。

**如果您需要引用本文，请参考：**

苏剑林. (Nov. 08, 2021). 《模型优化漫谈：BERT的初始标准差为什么是0.02？ 》\[Blog post\]. Retrieved from [https://kexue.fm/archives/8747](https://kexue.fm/archives/8747)

@online{kexuefm-8747,
        title={模型优化漫谈：BERT的初始标准差为什么是0.02？},
        author={苏剑林},
        year={2021},
        month={Nov},
        url={\\url{https://kexue.fm/archives/8747}},
}

分类： [信息时代](https://kexue.fm/category/Big-Data)    标签： [模型](https://kexue.fm/tag/%E6%A8%A1%E5%9E%8B/), [分析](https://kexue.fm/tag/%E5%88%86%E6%9E%90/), [优化](https://kexue.fm/tag/%E4%BC%98%E5%8C%96/), [梯度](https://kexue.fm/tag/%E6%A2%AF%E5%BA%A6/)[33 评论](https://kexue.fm/archives/8747#comments)

< [bert4keras在手，baseline我有：CLUE基准代码](https://kexue.fm/archives/8739) \| [WGAN新方案：通过梯度归一化来实现L约束](https://kexue.fm/archives/8757) >

### 你也许还对下面的内容感兴趣

- [msign的导数](https://kexue.fm/archives/11025)
- [等值振荡定理：最优多项式逼近的充要条件](https://kexue.fm/archives/10972)
- [MoE环游记：5、均匀分布的反思](https://kexue.fm/archives/10945)
- [Transformer升级之路：20、MLA究竟好在哪里？](https://kexue.fm/archives/10907)
- [SVD的导数](https://kexue.fm/archives/10878)
- [通过梯度近似寻找Normalization的替代品](https://kexue.fm/archives/10831)
- [MoE环游记：4、难处应当多投入](https://kexue.fm/archives/10815)
- [高阶muP：更简明但更高明的谱条件缩放](https://kexue.fm/archives/10795)
- [初探muP：超参数的跨模型尺度迁移规律](https://kexue.fm/archives/10770)
- [MoE环游记：3、换个思路来分配](https://kexue.fm/archives/10757)

[发表你的看法](https://kexue.fm/archives/8747#comment_form)

1. [«](https://kexue.fm/archives/8747/comment-page-1#comments)
2. [1](https://kexue.fm/archives/8747/comment-page-1#comments)
3. [2](https://kexue.fm/archives/8747/comment-page-2#comments)

[BERT的初始标准差0.02以及Warmup、LN的作用 - 点击领取](https://www.dianjilingqu.com/11251.html)

January 8th, 2022

\[...\] [https://kexue.fm/archives/8747](https://kexue.fm/archives/8747)\[...\]

[回复评论](https://kexue.fm/archives/8747/comment-page-2?replyTo=18196#respond-post-8747)

Allen7575

January 23rd, 2024

關於多加一層 Dense，我嘗試提出另一個觀點：
當輸入為 \[MASK\] token 時，如果沒有引入任何隨機性，那麼它的 output embedding 幾乎是固定的，因此預測結果永遠都是同一個答案。但我們希望 \[MASK\] 能夠預測出被我們遮住的token，而遮住的token是隨機的，所以 MASK -> ground truth token 是一個一對多的關係。

為了讓 \[MASK\] 能夠產生多種預測結果，就必須在某個地方加上隨機性。而最簡單的做法就是 MLM 預測機率之前、transformer 最後一層輸出 embedding 之間加上一層 Dense+dropout，透過dropout 引入隨機性，可以產生不同的預測。如果沒有多這一層，droupout 只能直接加在 embedding上，這會使其他 Task 不能直接拿embedding出來用。透過將dropout施加在這額外一層Dense上，當 Task 不需要隨機性，只需要取出固定的embedding時，就只需要拿掉這層Dense即可。

經過實驗，透過加上這層額外的Dense+dropout後，可以避免訓練初期\[MASK\]不斷地預測同一個字，造成 80% 的 \[MASK\] 甚麼都學不到的情況發生。

[回复评论](https://kexue.fm/archives/8747/comment-page-2?replyTo=23565#respond-post-8747)

[苏剑林](https://kexue.fm) 发表于
January 23rd, 2024

这个角度有点偏了，并不大科学：

1、如果是原版BERT的话，dropout在每一层都加了，最后的一层dropout理论上是无关紧要的；

2、softmax建模的是离散分布，所以它本身能建立一个一对多的关系（以每个ground truth token可能出现的概率来表示），不需要dropout就能做到这样的（输出的时候则依概率采样，而不是argmax）

[回复评论](https://kexue.fm/archives/8747/comment-page-2?replyTo=23569#respond-post-8747)

Allen7575 发表于
January 23rd, 2024

你可以試試看，把所有的dropout關掉，看會發生甚麼事。

因為 neural network 所有的運算，加減乘除，都是確定性的，沒有任何隨機性，所以同一個輸入，就只能得到同一個output。如果把所有 dropout 關掉，\[MASK\] 就是一個固定的輸入，也就只能輸出一個固定的結果，所以就會發現MASK的地方怎麼樣都只會預測出同一個字，根本無法實現預測「被蓋掉的字」。這跟 softmax 沒有關係，而是因為當參數固定後，輸入固定，輸出就只能是一個確定性的結果。這有點類似Seq2Seq decode 的時候，需要調整 tempture，用 sampling 的方式來增加輸出的diversity，不然你就只會得到不斷重複的序列。

至於你說原版BERT中，每一層都加了dropout，所以最後一層無關緊要，你也可以試試把最後一層dropout關掉，看會發生甚麼。為了研究，我自己是嘗試手刻了一個BERT，發現最後一層如果不加 dropout，雖然還是可以訓練，但速度非常非常慢，MASK一開始幾乎都是輸出同一個字，要過很久才會出現一點點不同的字，loss也降得非常慢。

但只要最後一層加了dropout，預測的輸出就不再只是同一個字，loss也降得很快。可見最後一層dropout起到了類似seq2seq decode sampling 的作用，將dropout 的機率轉變成在 embeding space 附近做 sampling。雖然其他層也有dropout，但它的作用可能只是用來防止overfitting，輸出到了最後一層，所產生的隨機性也很低了，幾乎可視為固定輸出(你可驗證看看)，不足以產生夠大的變化來充當decode sampling。

[回复评论](https://kexue.fm/archives/8747/comment-page-2?replyTo=23575#respond-post-8747)

[苏剑林](https://kexue.fm) 发表于
January 24th, 2024

我已经试过很多次把全部层Dropout都关掉，比不关工作得更好、更快，另外当前的LLM我似乎没有看到一个是加Dropout的。

你后面的描述，表明你似乎并不理解建模（固定的）概率分布就是建模一对多现象最好的手段这个事实。假如答案有两个，A出现的概率是40%，B出现的概率是60%，那么我们只需要预测一个二元概率分布\[0.4, 0.6\]，就可以准确地描述这个一对二的现象，这个概率分布就是一个固定的、不带有任何随机性的输出，但它能描述随机性的现象。

同理，在语言模型中，描述一对多的现象不是靠Dropout的，而是靠最后建模的一个vocab\_size大小的分布向量，每个位置的数字代表着对应的token可能出现的概率，训练分布是最大似然。“怎么样都会只预测出同一个字”是因为你只用了argmax的方式做预测，想要随机出现不同的结果，只需要改为依概率采样。

至于无法建立精确分布的连续型变量，要想实现一对多输出，也不是简单地加个Dropout就行，而是要通过flow-based、GAN、diffusion等生成模型才能完成。

[回复评论](https://kexue.fm/archives/8747/comment-page-2?replyTo=23584#respond-post-8747)

Allen7575 发表于
January 24th, 2024

抱歉，是我疏忽了，沒確認清楚起作用的部分。
我後來試了 Dense+droupout+LN 的各種組合，確認是 Dense+LN 起作用，讓我的手刻 BERT 可以訓練下去。所以以上我所說的dropout理論就當作胡言亂語吧，不好意思。

[回复评论](https://kexue.fm/archives/8747/comment-page-2?replyTo=23592#respond-post-8747)

lzhlzh

March 20th, 2025

想问下为什么说warmup有助于前面层的训练，当学习率调小时，前面层参数的增量应该也会等比例地变得更小吧，如果是出于稳定前面层的原因，为什么不对不同层用不同的学习率呢

[回复评论](https://kexue.fm/archives/8747/comment-page-2?replyTo=27173#respond-post-8747)

[苏剑林](https://kexue.fm) 发表于
March 23rd, 2025

其实是Post Norm的设计导致初始阶段的正确探索空间在一个非常窄的区域内，所以用1/10的学习率去学习10步，优于用单倍的学习率去学习1步，因此Warmup有助于Post Norm跳出这个狭窄区域，走向康庄大道。

[回复评论](https://kexue.fm/archives/8747/comment-page-2?replyTo=27222#respond-post-8747)

1. [«](https://kexue.fm/archives/8747/comment-page-1#comments)
2. [1](https://kexue.fm/archives/8747/comment-page-1#comments)
3. [2](https://kexue.fm/archives/8747/comment-page-2#comments)

[取消回复](https://kexue.fm/archives/8747#respond-post-8747)

你的大名

电子邮箱

个人网站（选填）

1\. 可以使用LaTeX代码，点击“预览效果”可查看效果；2. 可以通过点击评论楼层编号来引用该楼层；3. 网站可能会有点卡，如非确认评论失败，请 **不要重复点击提交**。

### 内容速览

[梯度消失说的是什么意思？](https://kexue.fm/archives/8747#%E6%A2%AF%E5%BA%A6%E6%B6%88%E5%A4%B1%E8%AF%B4%E7%9A%84%E6%98%AF%E4%BB%80%E4%B9%88%E6%84%8F%E6%80%9D%EF%BC%9F)
[LN真的能缓解梯度消失？](https://kexue.fm/archives/8747#LN%E7%9C%9F%E7%9A%84%E8%83%BD%E7%BC%93%E8%A7%A3%E6%A2%AF%E5%BA%A6%E6%B6%88%E5%A4%B1%EF%BC%9F)
[那我们为什么还要加LN？](https://kexue.fm/archives/8747#%E9%82%A3%E6%88%91%E4%BB%AC%E4%B8%BA%E4%BB%80%E4%B9%88%E8%BF%98%E8%A6%81%E5%8A%A0LN%EF%BC%9F)
[我们真的担心梯度消失吗？](https://kexue.fm/archives/8747#%E6%88%91%E4%BB%AC%E7%9C%9F%E7%9A%84%E6%8B%85%E5%BF%83%E6%A2%AF%E5%BA%A6%E6%B6%88%E5%A4%B1%E5%90%97%EF%BC%9F)
[Warmup是怎样起作用的？](https://kexue.fm/archives/8747#Warmup%E6%98%AF%E6%80%8E%E6%A0%B7%E8%B5%B7%E4%BD%9C%E7%94%A8%E7%9A%84%EF%BC%9F)
[初始标准差为什么是0.02？](https://kexue.fm/archives/8747#%E5%88%9D%E5%A7%8B%E6%A0%87%E5%87%86%E5%B7%AE%E4%B8%BA%E4%BB%80%E4%B9%88%E6%98%AF0.02%EF%BC%9F)
[为什么MLM要多加Dense？](https://kexue.fm/archives/8747#%E4%B8%BA%E4%BB%80%E4%B9%88MLM%E8%A6%81%E5%A4%9A%E5%8A%A0Dense%EF%BC%9F)
[希望大家多多海涵批评斧正](https://kexue.fm/archives/8747#%E5%B8%8C%E6%9C%9B%E5%A4%A7%E5%AE%B6%E5%A4%9A%E5%A4%9A%E6%B5%B7%E6%B6%B5%E6%89%B9%E8%AF%84%E6%96%A7%E6%AD%A3)

### 智能搜索

支持整句搜索！网站自动使用 [结巴分词](https://github.com/fxsjy/jieba) 进行分词，并结合ngrams排序算法给出合理的搜索结果。

### 热门标签

[生成模型](https://kexue.fm/tag/%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B/) [attention](https://kexue.fm/tag/attention/) [优化](https://kexue.fm/tag/%E4%BC%98%E5%8C%96/) [语言模型](https://kexue.fm/tag/%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/) [模型](https://kexue.fm/tag/%E6%A8%A1%E5%9E%8B/) [网站](https://kexue.fm/tag/%E7%BD%91%E7%AB%99/) [概率](https://kexue.fm/tag/%E6%A6%82%E7%8E%87/) [梯度](https://kexue.fm/tag/%E6%A2%AF%E5%BA%A6/) [转载](https://kexue.fm/tag/%E8%BD%AC%E8%BD%BD/) [微分方程](https://kexue.fm/tag/%E5%BE%AE%E5%88%86%E6%96%B9%E7%A8%8B/) [矩阵](https://kexue.fm/tag/%E7%9F%A9%E9%98%B5/) [天象](https://kexue.fm/tag/%E5%A4%A9%E8%B1%A1/) [分析](https://kexue.fm/tag/%E5%88%86%E6%9E%90/) [深度学习](https://kexue.fm/tag/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/) [积分](https://kexue.fm/tag/%E7%A7%AF%E5%88%86/) [python](https://kexue.fm/tag/python/) [优化器](https://kexue.fm/tag/%E4%BC%98%E5%8C%96%E5%99%A8/) [力学](https://kexue.fm/tag/%E5%8A%9B%E5%AD%A6/) [无监督](https://kexue.fm/tag/%E6%97%A0%E7%9B%91%E7%9D%A3/) [扩散](https://kexue.fm/tag/%E6%89%A9%E6%95%A3/) [几何](https://kexue.fm/tag/%E5%87%A0%E4%BD%95/) [节日](https://kexue.fm/tag/%E8%8A%82%E6%97%A5/) [生活](https://kexue.fm/tag/%E7%94%9F%E6%B4%BB/) [文本生成](https://kexue.fm/tag/%E6%96%87%E6%9C%AC%E7%94%9F%E6%88%90/) [数论](https://kexue.fm/tag/%E6%95%B0%E8%AE%BA/)

### 随机文章

- [科学空间：2010年7月重要天象](https://kexue.fm/archives/704)
- [太空中的巨影——日食间的月球影子](https://kexue.fm/archives/32)
- [第114号化学元素再次被实验确认](https://kexue.fm/archives/692)
- [体积与阿达马不等式](https://kexue.fm/archives/2215)
- [《积分公式大全》电子书](https://kexue.fm/archives/354)
- [【NASA每日一图】IC 1396 星云](https://kexue.fm/archives/86)
- [【NASA每日一图】甘姆星云(Gum Nebula)](https://kexue.fm/archives/92)
- [如何应对Seq2Seq中的“根本停不下来”问题？](https://kexue.fm/archives/7500)
- [生成扩散模型漫谈（二十九）：用DDPM来离散编码](https://kexue.fm/archives/10711)
- [第100篇文章了](https://kexue.fm/archives/103)

### 最近评论

- [忍者猫](https://kexue.fm/archives/10592/comment-page-2#comment-27952): 这优化器的作者真的应该给你打钱
- [Chaofa Yuan](https://kexue.fm/archives/11033/comment-page-1#comment-27951): 写得太好了
- [Skyler Lin](https://kexue.fm/archives/11033/comment-page-1#comment-27949): respect苏神！
- [宋佳铭](https://kexue.fm/archives/10958/comment-page-1#comment-27947): 对，个人感觉mean flow就是continuous time CTM
- [宋佳铭](https://kexue.fm/archives/10958/comment-page-1#comment-27946): 的确，对sg这个事情我感觉如果是用‘归纳’法做是不太能避免的，因为毕竟是用步长短的模型去约束步...
- [MoFHeka](https://kexue.fm/archives/10542/comment-page-1#comment-27945): 苏老师您好，请问一下这套结论在稀疏参数上应该如何应用？比如大规模稀疏Embedding，每个B...
- [苏剑林](https://kexue.fm/archives/11033/comment-page-1#comment-27944): Temp LoRA倒是有印象，其实思想是一样的，如果我单独开一篇文章介绍TTT的话，应该会提到...
- [苏剑林](https://kexue.fm/archives/11033/comment-page-1#comment-27943): 你搜搜mamba、rwkv甚至rnn做vision的工作，其实不少。不过多数确实像你说的，正反...
- [苏剑林](https://kexue.fm/archives/9379/comment-page-1#comment-27942): 问题1可以看看 https://kexue.fm/archives/4718 ，简单来说就是点...
- [苏剑林](https://kexue.fm/archives/10958/comment-page-2#comment-27941): 你的“信息量”怎么定义？直观来说，reflow训练的是切线模型，而一步生成需要的是割线模型，m...

### 友情链接

- [Cool Papers](https://papers.cool)
- [数学研发](https://bbs.emath.ac.cn)
- [Seatop](http://www.seatop.com.cn/)
- [Xiaoxia](https://xiaoxia.org/)
- [积分表-网络版](https://kexue.fm/sci/integral/index.html)
- [丝路博傲](http://blog.dvxj.com/)
- [ph4ntasy 饭特稀](http://www.ph4ntasy.com/)
- [数学之家](http://www.2math.cn/)
- [有趣天文奇观](http://interesting-sky.china-vo.org/)
- [TwistedW](http://www.twistedwg.com/)
- [godweiyang](https://godweiyang.com/)
- [AI柠檬](https://blog.ailemon.net/)
- [王登科-DK博客](https://greatdk.com)
- [ESON](https://blog.eson.org/)
- [枫之羽](https://fzhiy.net/)
- [Mathor's blog](https://wmathor.com/)
- [coding-zuo](https://coding-zuo.github.io/)
- [博科园](https://www.bokeyuan.net/)
- [孔皮皮的博客](https://www.kppkkp.top/)
- [运鹏的博客](https://yunpengtai.top/)
- [jiming.site](https://jiming.site/)
- [OmegaXYZ](https://www.omegaxyz.com/)
- [Blog by Eacls](https://www.eacls.top/)
- [EAI猩球](https://www.robotech.ink/)
- [文举的博客](https://liwenju0.com/)
- [用代码打点酱油](https://bruceyuan.com/)
- [申请链接](https://kexue.fm/links.html)

本站采用创作共用版权协议，要求署名、非商业用途和保持一致。转载本站内容必须也遵循“ [署名-非商业用途-保持一致](http://creativecommons.org/licenses/by-nc-nd/2.5/cn/)”的创作共用协议。
© 2009-2025 Scientific Spaces. All rights reserved. Theme by [laogui](http://www.laogui.com). Powered by [Typecho](http://typecho.org). 备案号: [粤ICP备09093259号-1/2](https://beian.miit.gov.cn/)。