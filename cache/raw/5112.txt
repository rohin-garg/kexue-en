## SEARCH

## MENU

- [打赏](https://kexue.fm/reward.html)
- [公式](https://kexue.fm/latex.html)
- [天象](https://kexue.fm/ac.html)
- [链接](https://kexue.fm/links.html)
- [时光](https://kexue.fm/me.html)
- [博览](https://kexue.fm/science.html)
- [归档](https://kexue.fm/content.html)

## CATEGORIES

- [千奇百怪](https://kexue.fm/category/Everything)
- [天文探索](https://kexue.fm/category/Astronomy)
- [数学研究](https://kexue.fm/category/Mathematics)
- [物理化学](https://kexue.fm/category/Phy-chem)
- [信息时代](https://kexue.fm/category/Big-Data)
- [生物自然](https://kexue.fm/category/Biology)
- [图片摄影](https://kexue.fm/category/Photograph)
- [问题百科](https://kexue.fm/category/Questions)
- [生活/情感](https://kexue.fm/category/Life-Feeling)
- [资源共享](https://kexue.fm/category/Resources)

## NEWPOSTS

- [msign的导数](https://kexue.fm/archives/11025)
- [通过msign来计算mclip（奇...](https://kexue.fm/archives/11006)
- [msign算子的Newton-Sc...](https://kexue.fm/archives/10996)
- [等值振荡定理：最优多项式逼近的充要条件](https://kexue.fm/archives/10972)
- [生成扩散模型漫谈（三十）：从瞬时速...](https://kexue.fm/archives/10958)
- [MoE环游记：5、均匀分布的反思](https://kexue.fm/archives/10945)
- [msign算子的Newton-Sc...](https://kexue.fm/archives/10922)
- [Transformer升级之路：2...](https://kexue.fm/archives/10907)
- [一道概率不等式：盯着它到显然成立为止！](https://kexue.fm/archives/10902)
- [SVD的导数](https://kexue.fm/archives/10878)

## COMMENTS

- [rpsun: 老师您好，最近在自己的任务上尝试了muon，甚至只修改了学习率...](https://kexue.fm/archives/10592/comment-page-2#comment-27912)
- [盏一: 我之前做的笔记:Q: 公式 (14) 的理解.A: 首先基于 ...](https://kexue.fm/archives/8265/comment-page-8#comment-27911)
- [盏一: 哦哦哦 你是说 $\\exp nB$ 是正交矩阵! 并不是说 B.](https://kexue.fm/archives/8397/comment-page-3#comment-27910)
- [盏一: 呃, 是我脑子乱了... 忘了 $\\exp(0) = I$. ...](https://kexue.fm/archives/8397/comment-page-3#comment-27908)
- [盏一: 苏神, 请教一下\> 并且还可以证明它一定是正交矩阵是怎么证明的...](https://kexue.fm/archives/8397/comment-page-3#comment-27907)
- [sk: 请问公式14是怎么得出来的？](https://kexue.fm/archives/8265/comment-page-8#comment-27906)
- [tll1945tll1937: 真心实意的向大家请教问题：看了文章“对齐全量微调！这是我看过最...](https://kexue.fm/archives/10266/comment-page-1#comment-27901)
- [oYo\_logan: \[comment=27017\]苏剑林\[/comment\]苏神，...](https://kexue.fm/archives/10757/comment-page-1#comment-27897)
- [z123: 在参数矩阵较多的CNN小模型上，Muon会明显慢于Adam，这...](https://kexue.fm/archives/10592/comment-page-1#comment-27896)
- [dry: 苏神好，一直有个疑问，ReFlow构建的ODE是$dx\_t/d...](https://kexue.fm/archives/10958/comment-page-2#comment-27895)

## USERLOGIN

- [登录](https://kexue.fm/admin/login.php)

[科学空间\|Scientific Spaces](https://kexue.fm)

- [登录](https://kexue.fm/admin/login.php)
- [打赏](https://kexue.fm/reward.html)
- [公式](https://kexue.fm/latex.html)
- [天象](https://kexue.fm/ac.html)
- [链接](https://kexue.fm/links.html)
- [时光](https://kexue.fm/me.html)
- [博览](https://kexue.fm/science.html)
- [归档](https://kexue.fm/content.html)

渴望成为一个小飞侠

- [欢迎订阅](https://kexue.fm/feed)
- [个性邮箱](https://kexue.fm/archives/119)
- [天象信息](https://kexue.fm/ac.html)
- [观测ISS](https://kexue.fm/archives/41)
- [LaTeX](https://kexue.fm/latex.html)
- [关于博主](https://kexue.fm/me.html)

欢迎访问“科学空间”，这里将与您共同探讨自然科学，回味人生百态；也期待大家的分享～

- [**千奇百怪** Everything](https://kexue.fm/category/Everything)
- [**天文探索** Astronomy](https://kexue.fm/category/Astronomy)
- [**数学研究** Mathematics](https://kexue.fm/category/Mathematics)
- [**物理化学** Phy-chem](https://kexue.fm/category/Phy-chem)
- [**信息时代** Big-Data](https://kexue.fm/category/Big-Data)
- [**生物自然** Biology](https://kexue.fm/category/Biology)
- [**图片摄影** Photograph](https://kexue.fm/category/Photograph)
- [**问题百科** Questions](https://kexue.fm/category/Questions)
- [**生活/情感** Life-Feeling](https://kexue.fm/category/Life-Feeling)
- [**资源共享** Resources](https://kexue.fm/category/Resources)

- [**千奇百怪**](https://kexue.fm/category/Everything)
- [**天文探索**](https://kexue.fm/category/Astronomy)
- [**数学研究**](https://kexue.fm/category/Mathematics)
- [**物理化学**](https://kexue.fm/category/Phy-chem)
- [**信息时代**](https://kexue.fm/category/Big-Data)
- [**生物自然**](https://kexue.fm/category/Biology)
- [**图片摄影**](https://kexue.fm/category/Photograph)
- [**问题百科**](https://kexue.fm/category/Questions)
- [**生活/情感**](https://kexue.fm/category/Life-Feeling)
- [**资源共享**](https://kexue.fm/category/Resources)

[首页](https://kexue.fm) [信息时代](https://kexue.fm/category/Big-Data) 再来一顿贺岁宴：从K-Means到Capsule

12Feb

# [再来一顿贺岁宴：从K-Means到Capsule](https://kexue.fm/archives/5112)

By 苏剑林 \|
2018-02-12 \|
252258位读者\|

在本文中，我们再次对Capsule进行一次分析。

整体上来看，Capsule算法的细节不是很复杂，对照着它的流程把Capsule用框架实现它基本是没问题的。所以，困难的问题是理解Capsule究竟做了什么，以及为什么要这样做，尤其是Dynamic Routing那几步。

为什么我要反复对Capsule进行分析？这并非单纯的“炒冷饭”，而是为了得到对Capsule原理的理解。众所周知，Capsule给人的感觉就是“有太多人为约定的内容”，没有一种“虽然我不懂，但我相信应该就是这样”的直观感受。我希望尽可能将Capsule的来龙去脉思考清楚，使我们能觉得Capsule是一个自然、流畅的模型，甚至对它举一反三。

在 [《揭开迷雾，来一顿美味的Capsule盛宴》](https://kexue.fm/archives/4819/) 中，笔者先分析了动态路由的结果，然后指出输出是输入的某种聚类，这个“从结果到原因”的过程多多少少有些望文生义的猜测成分；这次则反过来，直接确认输出是输入的聚类，然后反推动态路由应该是怎样的，其中含糊的成分大大减少。两篇文章之间有一定的互补作用。

## Capsule框架 [\#](https://kexue.fm/archives/5112\#Capsule%E6%A1%86%E6%9E%B6)

图1：Capsule框架的简明示意图

与其说Capsule是一个具体的模型，倒不如说Capsule是一个建模的框架，而框架内每个步骤的内容，是可以自己灵活替换的，而Hinton所发表的论文，只是一个使用案例。

这是一个怎样的框架呢？

### 特征表达 [\#](https://kexue.fm/archives/5112\#%E7%89%B9%E5%BE%81%E8%A1%A8%E8%BE%BE)

Capsule模型中， **每个特征都是用一个向量（即Capsule，胶囊）来表示** 的。

图2：Capsule的每个特征都是向量，并且通过聚类来递进

当然，对于关注新闻的读者来说，这已经不是什么新消息。可能读者会有疑问：用向量来表示特征有什么稀奇的，本来神经网络的特征输入不就是一个向量吗？原来神经网络（MLP）的每一层输入是一个向量$\\boldsymbol{x}\\in\\mathbb{R}^n$，然后输出是$\\boldsymbol{y}=Activation(\\boldsymbol{W}\\boldsymbol{x}+\\boldsymbol{b})\\in \\mathbb{R}^k$，我们就将$\\boldsymbol{x}$的每一个分量都看成一个特征，那么每个特征都是标量了。而所谓的特征向量化后，那么每一层的输入变成了$\\boldsymbol{x}\\in\\mathbb{R}^{n\\times d\_x}$，然后输出是$\\boldsymbol{y}=Routing(\\boldsymbol{x})\\in \\mathbb{R}^{k\\times d\_y}$，这时候的输入$\\boldsymbol{x}$也看成是$n$个特征，但每个特征都是一个$d\_x$维向量；输出$\\boldsymbol{y}$则看成是$k$个特征，每个特征是一个$d\_y$维向量。换一个角度看，其实就是说MLP每一层的输入输出由单个的向量变成了向量的集合（矩阵）。

或者我们可以将它换一个名称，叫做“特征的分布式表示”。也许有读者看到了“分布式表示”，会想起NLP中的词向量。没错，词向量一开始确实叫做“分布式表示”（Distributed Representation），而笔者看到Capsule的这一特点，第一反应也就是词向量。我们可以用词向量代替one hot来表示一个词，这样表达的信息就更为丰富了，而且所有的词都位于同一向量空间，方便后续处理。

此外，事实上图像中早也有这样的例子，众所周知彩色图像一般有RGB三个通道，每个通道256个选择，所以一共可以表达$256^3=16777216$种颜色（约1700万），为什么不直接用1700万个数字来分别表示这1700种颜色，而要分开3组，每组256个数字呢？这其实也是一种分布式表示，这样可以更好地表达色彩的多样性（比如红色的相近颜色是什么色？也许有人说橙色，也有人说紫色，也有可能是粉红，单一一个数字难以表达多种的相似性，而分组后则可以。）。更进一步说，我们在对图像不断进行卷积操作时，所得结果的通道维度，其实就是图像特征的一种分布式表示了。

### 特征组合 [\#](https://kexue.fm/archives/5112\#%E7%89%B9%E5%BE%81%E7%BB%84%E5%90%88)

Capsule的第二个特点，是 **通过聚类来组合特征**。

#### 组合与表达 [\#](https://kexue.fm/archives/5112\#%E7%BB%84%E5%90%88%E4%B8%8E%E8%A1%A8%E8%BE%BE)

通过将底层特征组合为上层的特征，是跟我们的认知规律是相符的。在NLP中，我们有“字-->词-->句-->段”的层层组合；在图像中，我们也有“点-->线-->面-->体”的层层组合。面对新事物（上层特征），我们总会将它分解为我们熟悉的一些事物（底层特征），然后脑海里将这些事物映射到这个新事物（特征组合）。

对于我们来说，这个分解和组合的过程，不一定有什么目的，而只是为了用我们自己的方式去理解这个新事物（在大脑中形成良好的特征表达）。这也就能理解Hinton诟病深度学习、发展Capsule的原因之一了，因为他觉得现在深度学习的模型针对性太强（比如MNIST分类模型就只能做单个数字的识别，多个数字的识别就要重新构建数据集、重新设计并训练模型），而事实上，我们的根本目的并不是单纯地做任务，而是通过任务形成良好的、普适的特征表达，这样才有可能形成真正的人工智能。

#### 特征间聚类 [\#](https://kexue.fm/archives/5112\#%E7%89%B9%E5%BE%81%E9%97%B4%E8%81%9A%E7%B1%BB)

那么，怎么完成这个组合的过程呢？试想一下，两个字为什么能成为一个词，是因为这两个字经常“扎堆”出现，而且这个“堆”只有它们俩。这就告诉我们，特征的聚合是因为它们有聚类倾向，所以Capsule把聚类算法融入到模型中。

要注意，我们以前所说的聚类，都是指样本间的聚类，比如将MNIST的图片自动聚类成10个类别，或者将Word2Vec训练而来的词向量聚类成若干类，聚类的对象就是一个样本（输入）。而Capsule则设想将输入本身表示为若干个特征向量，然后对这些向量进行聚类（ **特征间的聚类**），得到若干中心向量，接着再对这些中心向量聚类，层层递进，从而完成层层抽象的过程。这是一种特征间的聚类。

现在问题就来了。既然是聚类，是按照什么方法来聚类的呢？然后又是怎么根据这个聚类方法来导出那个神奇的Dynamic Routing的呢？后面我们会从K-Means出发来寻根问底，现在让我们先把主要思路讲完。

#### 特征显著性 [\#](https://kexue.fm/archives/5112\#%E7%89%B9%E5%BE%81%E6%98%BE%E8%91%97%E6%80%A7)

通过特征的组合可以得到上层特征，那如何对比特征的强弱程度呢？ **Capsule的答案是：模长。** 这就好比在茫茫向量如何找出“突出”的那个？只需要看看谁更高就行了。因此通过特征向量的模长来衡量它自己的“突出程度”，显然也是比较自然的选择。此外，一个有界的度量也是我们希望的，因此我们对特征向量做一个压缩：
$$squash(\\boldsymbol{v})=\\frac{\\Vert\\boldsymbol{v}\\Vert^2}{1+\\Vert\\boldsymbol{v}\\Vert^2}\\frac{\\boldsymbol{v}}{\\Vert\\boldsymbol{v}\\Vert}\\tag{1}$$
压缩的方案并不唯一，这里就不展开了。不过我在实验过程中，发现将1替换为0.5能提升性能。

图3：Capsule通过特征向量的聚类，来刻画特征的组合特性

为了突出模长的这一含义，也需要在设计模型的时候有所配合。如图，尽管$\\boldsymbol{v}\_1$所代表的类所包含的特征向量$\\boldsymbol{u}\_1,\\boldsymbol{u}\_2,\\boldsymbol{u}\_4,\\boldsymbol{u}\_8$的模长均比较小，但因为成员多（“小弟多”），因此$\\boldsymbol{v}\_1$的模长也能占优（“势力大”）。这说明，一个类要突出，跟类内向量的数目、每个类内向量本身的模长都有关联。后面我们也会看到Capsule是如何体现这一点的。

## K-Means新探 [\#](https://kexue.fm/archives/5112\#K-Means%E6%96%B0%E6%8E%A2)

既然本文不断强调Capsule是通过聚类来抽象特征的，那么就有必要来细谈一下聚类算法了。Capsule所使用的聚类算法，其实是K-Means的变种。聚类算法有很多，理论上每种聚类算法都是可能的，然而要将聚类算法嵌入到Capsule中，还需要费上一点周折。

### 聚类目标 [\#](https://kexue.fm/archives/5112\#%E8%81%9A%E7%B1%BB%E7%9B%AE%E6%A0%87)

K-Means聚类本质上是一种“中心聚类方法”——聚类就是找类别中心。为了定义中心，我们就需要一个相近程度的度量，常用的是欧氏距离，但这并不是唯一的选择。所以这里我们干脆在一个更加 **一般的框架下介绍K-Means**：K-Means希望把已有的数据$\\boldsymbol{u}\_1, \\boldsymbol{u}\_2, \\dots, \\boldsymbol{u}\_n$无监督地划分为$k$类，聚类的方法是找出$k$个聚类中心$\\boldsymbol{v}\_1, \\boldsymbol{v}\_2, \\dots, \\boldsymbol{v}\_k$，使得类内间隔最小：
$$L=\\sum\_{i=1}^n \\min\_{j=1}^k d(\\boldsymbol{u}\_i, \\boldsymbol{v}\_j)\\tag{2}$$
这里$d$代表了相近程度的度量，所以这个式子的意思很简单，就是说每个$\\boldsymbol{u}\_i$只属于跟它最相近的那一类，然后将所有类内距离加起来，最小化这个类内距离：
$$(\\boldsymbol{v}\_1, \\dots, \\boldsymbol{v}\_k) = \\mathop{\\text{argmin}}\_{(\\boldsymbol{v}\_1, \\dots, \\boldsymbol{v}\_k)}L\\tag{3}$$

> 注：显然，聚类的结果依赖于$d$的具体形式，这其实就告诉我们：无监督学习和有监督学习的差别，在于我们跟模型“交流”的方法不同。有监督学习中，我们通过标注数据向模型传达我们的意愿；在无监督学习中，我们则通过设计适当的度量$d$来完成这个过程。

### 求解过程 [\#](https://kexue.fm/archives/5112\#%E6%B1%82%E8%A7%A3%E8%BF%87%E7%A8%8B)

怎么去最小化$L$来求出各个中心呢？如果读者不希望细细了解推导过程，可以跳过这一节，直接看下一节。

因为$L$中有$\\min$这个操作，所以直接求它的梯度会有困难（不是不能求，而是在临界点附近不好处理），事实上有很多类似的问题没能得到很好的解决，都是因为它们的loss中有$\\min$（日后有机会我们再谈这个事情。）。

然而，这里我们可以“软化”这个$L$，使得它可以求导。因为我们有一个很漂亮的公式（参考 [《寻求一个光滑的最大值函数》](https://kexue.fm/archives/3290)）
$$\\begin{aligned}\\max(\\lambda\_1,\\lambda\_2,\\dots,\\lambda\_n)=&\\lim\_{K\\to+\\infty}\\frac{1}{K}\\ln\\left(\\sum\_{i=1}^n e^{\\lambda\_i K}\\right)\\\
\\approx&\\frac{1}{K}\\ln\\left(\\sum\_{i=1}^n e^{\\lambda\_i K}\\right)\\end{aligned}\\tag{4}$$

> 注：如果取$K=1$，显然括号里边就是softmax的分母，这也就是softmax的由来了——它是“soft”加“max”——“软的最大值”。

而我们有
$$\\min(\\lambda\_1,\\lambda\_2,\\dots,\\lambda\_n)=-\\max(-\\lambda\_1,-\\lambda\_2,\\dots,-\\lambda\_n)\\tag{5}$$
因此我们就得到
$$L\\approx-\\frac{1}{K}\\sum\_{i=1}^n \\ln\\left(\\sum\_{j=1}^k e^{-K\\cdot d(\\boldsymbol{u}\_i, \\boldsymbol{v}\_j)}\\right)=-\\frac{1}{K}\\sum\_{i=1}^n\\ln Z\_i\\tag{6}$$
现在这个近似的loss在全局都光滑可导了。因此我们可以尝试求它的梯度
$$\\frac{\\partial L}{\\partial \\boldsymbol{v}\_j}\\approx\\sum\_{i=1}^n \\frac{e^{-K\\cdot d(\\boldsymbol{u}\_i, \\boldsymbol{v}\_j)}}{Z\_i} \\frac{\\partial d(\\boldsymbol{u}\_i, \\boldsymbol{v}\_j)}{\\partial \\boldsymbol{v}\_j}=\\sum\_{i=1}^n c\_{ij}\\frac{\\partial d(\\boldsymbol{u}\_i, \\boldsymbol{v}\_j)}{\\partial \\boldsymbol{v}\_j}\\tag{7}$$
这里
$$c\_{ij}=\\mathop{softmax}\\limits\_j\\Big(-K\\cdot d(\\boldsymbol{u}\_i, \\boldsymbol{v}\_j)\\Big)$$
我们已经指明了是对$j$所在的维度来归一化。为了求出一个极小值，我们希望让$\\partial L/\\partial \\boldsymbol{v}\_j=0$，但得到的方程并不是简单可解的。因此，可以引入一个迭代过程，假设$\\boldsymbol{v}^{(r)}\_j$是$\\boldsymbol{v}\_j$的第$r$次迭代的结果，那么我们可以让
$$0=\\sum\_{i=1}^n c\_{ij}^{(r)}\\frac{\\partial d(\\boldsymbol{u}\_i, \\boldsymbol{v}\_j^{(r+1)})}{\\partial \\boldsymbol{v}\_j^{(r+1)}}\\tag{8}$$
如果可以从上述方程解出$\\boldsymbol{v}\_j^{(r+1)}$，那么就可以从中得到一个迭代格式。

### 欧氏距离 [\#](https://kexue.fm/archives/5112\#%E6%AC%A7%E6%B0%8F%E8%B7%9D%E7%A6%BB)

现在就可以把我们选择的度量代入$(8)$式进行计算了。我们可以看一个最基本的例子：$d(\\boldsymbol{u}\_i, \\boldsymbol{v}\_j)=\\Vert\\boldsymbol{u}\_i - \\boldsymbol{v}\_j\\Vert^2$，这时候就有
$$\\frac{\\partial d(\\boldsymbol{u}\_i, \\boldsymbol{v}\_j)}{\\partial \\boldsymbol{v}\_j}=2(\\boldsymbol{v}\_j-\\boldsymbol{u}\_i)\\tag{9}$$
根据$(8)$式得到$0=2\\sum\\limits\_{i=1}^n c\_{ij}^{(r)}\\left(\\boldsymbol{v}\_j^{(r+1)}-\\boldsymbol{u}\_i\\right)$，从而我们可以解出
$$v\_{j}^{(r+1)}=\\frac{\\sum\\limits\_{i=1}^n c\_{ij}^{(r)}\\boldsymbol{u}\_i}{\\sum\\limits\_{i=1}^n c\_{ij}^{(r)}}\\tag{10}$$
如果取$K\\to+\\infty$，那么$c\_{ij}^{(r)}$非0即1，所以上式就是说（读者可以自己把证明补充完整）

> $\\boldsymbol{v}\_{j}^{(r+1)}$是距离$\\boldsymbol{v}\_{j}^{(r)}$最近的那些$\\boldsymbol{u}\_i$的平均值。

这就得到了我们平时说的K-Means聚类算法。

### 内积相似度 [\#](https://kexue.fm/archives/5112\#%E5%86%85%E7%A7%AF%E7%9B%B8%E4%BC%BC%E5%BA%A6)

欧氏距离并不适合用在Capsule中，这是因为欧氏距离得到的中心向量是类内的向量的平均，这样类内向量越多，也不会导致中心向量的模越长，这不满足我们前面说的“小弟越多，势力越大”的设计。

什么距离比较适合呢？在论文《Dynamic Routing Between Capsules》中有一段话：

> The initial coupling coefficients are then iteratively refined by measuring the agreement between the current output $\\boldsymbol{v}\_j$ of each capsule, $j$, in the layer above and the prediction $\\boldsymbol{\\hat{u}\_{j\|i}}$ made by capsule $i$.
>
> The agreement is simply the scalar product $a\_{ij} = \\boldsymbol{v}\_j \\cdot \\boldsymbol{\\hat{u}\_{j\|i}}$ ...

对应到本文，大概的意思是用内积$\\langle\\boldsymbol{u}\_i, \\boldsymbol{v}\_j\\rangle$作为相似度的度量，也就是说，$d(\\boldsymbol{u}\_i, \\boldsymbol{v}\_j)=-\\langle\\boldsymbol{u}\_i, \\boldsymbol{v}\_j\\rangle$。但仔细思考就会发现问题，因为这样的$d$是无下界的！无下界的函数我们不能用来做loss，所以我一直被这里困惑着。直到有一天，我觉得可以将$\\boldsymbol{v}\_j$先归一化，然后再算内积，这样一来实际上是：
$$d(\\boldsymbol{u}\_i, \\boldsymbol{v}\_j)=-\\left\\langle\\boldsymbol{u}\_i, \\frac{\\boldsymbol{v}\_j}{\\Vert\\boldsymbol{v}\_j\\Vert}\\right\\rangle\\tag{11}$$
现在对于固定的$\\boldsymbol{u}\_i$，不管$\\boldsymbol{v}\_j$怎么变，$d(\\boldsymbol{u}\_i, \\boldsymbol{v}\_j)$就有下界了。所以这样的$d$是可以用来作为loss的，代入$(8)$式算，最终得到的结果是
$$\\frac{\\boldsymbol{v}\_j^{(r+1)}}{\\left\\Vert\\boldsymbol{v}\_j^{(r+1)}\\right\\Vert}=\\frac{\\sum\\limits\_{i=1}^n c\_{ij}^{(r)}\\boldsymbol{u}\_i}{\\left\\Vert\\sum\\limits\_{i=1}^n c\_{ij}^{(r)}\\boldsymbol{u}\_i\\right\\Vert}\\tag{12}$$
也就是说$\\boldsymbol{v}\_j^{(r+1)}$和$\\sum\\limits\_{i=1}^n c\_{ij}^{(r)}\\boldsymbol{u}\_i$的方向是一样的，但这不能说明它们两个是相等的。然而，这也意味着我们确实可以简单地取
$$\\boldsymbol{v}\_j^{(r+1)}=\\sum\\limits\_{i=1}^n c\_{ij}^{(r)}\\boldsymbol{u}\_i\\tag{13}$$
如果取$K\\to +\\infty$的极限，那么就是说

> $\\boldsymbol{v}\_{j}^{(r+1)}$是距离$\\boldsymbol{v}\_{j}^{(r)}$最近的那些$\\boldsymbol{u}\_i$的和。

由于现在是求和，就可以体现出“小弟越多，势力越大”的特点了。（注意，这里和欧氏距离那都出现了“最近”，两个最近的含义并不一样，因为所选用的$d$不一样。）

> 注：$(12)$式的推导过程。
> $$\\begin{aligned}\\frac{\\partial\\left\\langle\\boldsymbol{u}\_i, \\frac{\\boldsymbol{v}\_j}{\\Vert\\boldsymbol{v}\_j\\Vert}\\right\\rangle}{\\partial \\boldsymbol{v}\_j}=&\\frac{\\partial \\left(\\boldsymbol{u}\_i \\cdot \\frac{\\boldsymbol{v}\_j}{\\Vert\\boldsymbol{v}\_j\\Vert}\\right)}{\\partial \\boldsymbol{v}\_j}\\\
> =&\\frac{\\boldsymbol{u}\_i}{\\Vert\\boldsymbol{v}\_j\\Vert}+(\\boldsymbol{u}\_i \\cdot \\boldsymbol{v}\_j)\\frac{\\partial}{\\partial \\boldsymbol{v}\_j}\\frac{1}{\\Vert\\boldsymbol{v}\_j\\Vert}\\\
> =&\\frac{\\boldsymbol{u}\_i}{\\Vert\\boldsymbol{v}\_j\\Vert}-(\\boldsymbol{u}\_i \\cdot \\boldsymbol{v}\_j)\\frac{\\boldsymbol{v}\_j}{\\Vert\\boldsymbol{v}\_j\\Vert^3}\\end{aligned}$$
> 然后根据$(8)$式，得到
> $$0=\\frac{\\sum\\limits\_{i=1}^n C\_{ij}^{(r)}\\boldsymbol{u}\_i}{\\left\\Vert\\boldsymbol{v}\_j^{(r+1)}\\right\\Vert}-\\left(\\sum\\limits\_{i=1}^n C\_{ij}^{(r)}\\boldsymbol{u}\_i \\cdot \\boldsymbol{v}\_j^{(r+1)}\\right)\\frac{\\boldsymbol{v}\_j^{(r+1)}}{\\left\\Vert\\boldsymbol{v}\_j^{(r+1)}\\right\\Vert^3}$$
> 整理得
> $$\\sum\\limits\_{i=1}^n C\_{ij}^{(r)}\\boldsymbol{u}\_i=\\left(\\sum\\limits\_{i=1}^n C\_{ij}^{(r)}\\boldsymbol{u}\_i \\cdot \\frac{\\boldsymbol{v}\_j^{(r+1)}}{\\left\\Vert\\boldsymbol{v}\_j^{(r+1)}\\right\\Vert}\\right)\\frac{\\boldsymbol{v}\_j^{(r+1)}}{\\left\\Vert\\boldsymbol{v}\_j^{(r+1)}\\right\\Vert}$$
> 两边取求模长
> $$\\left\\Vert\\sum\\limits\_{i=1}^n C\_{ij}^{(r)}\\boldsymbol{u}\_i\\right\\Vert=\\left\|\\sum\\limits\_{i=1}^n C\_{ij}^{(r)}\\boldsymbol{u}\_i \\cdot \\frac{\\boldsymbol{v}\_j^{(r+1)}}{\\left\\Vert\\boldsymbol{v}\_j^{(r+1)}\\right\\Vert}\\right\|=\\left\\Vert\\sum\\limits\_{i=1}^n C\_{ij}^{(r)}\\boldsymbol{u}\_i\\right\\Vert \\times \|\\cos\\theta\|$$
> 这里的$\\theta$是向量$\\sum\\limits\_{i=1}^n C\_{ij}^{(r)}\\boldsymbol{u}\_i$和向量$\\boldsymbol{v}\_j^{(r+1)}$的夹角，上式表明$\|\\cos\\theta\|=1$，因此$\\theta=0$或$\\pi$，$\\theta=\\pi$事实上是极大值点而不是极小值，所以$\\theta=0$，即它们方向一致，得到$(12)$式。

## 动态路由 [\#](https://kexue.fm/archives/5112\#%E5%8A%A8%E6%80%81%E8%B7%AF%E7%94%B1)

经过漫长的准备，Dynamic Routing算法已经呼之欲出了。

按照第一部分，我们说Capsule中每一层是通过特征间聚类来完成特征的组合与抽象，聚类需要反复迭代，是一个隐式的过程。我们需要为每一层找到光滑的、显式的表达式$$\\boldsymbol{v}\_j=\\boldsymbol{f}\_j(\\boldsymbol{u}\_1,\\dots,\\boldsymbol{u}\_n)\\tag{14}$$
才能完成模型的训练。 **动态路由就是通过迭代来写出这个（近似的）显式表达式的过程。**

### 基本步骤 [\#](https://kexue.fm/archives/5112\#%E5%9F%BA%E6%9C%AC%E6%AD%A5%E9%AA%A4)

假设Capsule的输入特征分别为$\\boldsymbol{u}\_1, \\boldsymbol{u}\_2, \\dots, \\boldsymbol{u}\_n$，然后下一层的特征向量就是$\\boldsymbol{v}\_1,\\boldsymbol{v}\_2,\\dots,\\boldsymbol{v}\_k$，它就是前一层$n$个向量聚为$k$类的聚类中心，聚类的度量是前面的归一化内积，于是我们就可以写出迭代过程：

> 初始化$\\boldsymbol{v}\_{j} \\leftarrow \\boldsymbol{v}\_{j}^{(0)}$
> 迭代$r$次：
>   $\\boldsymbol{v}\_{j} \\leftarrow \\boldsymbol{v}\_{j}/\\Vert\\boldsymbol{v}\_{j}\\Vert$；
>    如果$j=\\mathop{\\text{argmax}}\\limits\_{j=1,\\dots,k}\\langle\\boldsymbol{u}\_i,\\boldsymbol{v}\_j\\rangle$，那么$c\_{ij} \\leftarrow 1$，否则$c\_{ij} \\leftarrow 0$；
>   $\\boldsymbol{v}\_{j} \\leftarrow \\sum\\limits\_{i}c\_{ij}\\boldsymbol{u}\_{i}$；
> 返回$squash(\\boldsymbol{v}\_j)$。

这个版本是容易理解，但由于存在$\\text{argmax}$这个操作，我们用不了梯度下降，而梯度下降是目前求模型其他参数的唯一方法。为了解决这个问题，我们只好不取$K\\to+\\infty$的极限，取一个常数$K > 0$，然后将算法变为

> 初始化$\\boldsymbol{v}\_{j} \\leftarrow \\boldsymbol{v}\_{j}^{(0)}$
> 迭代$r$次：
>   $\\boldsymbol{v}\_{j} \\leftarrow \\boldsymbol{v}\_{j}/\\Vert\\boldsymbol{v}\_{j}\\Vert$；
>   $c\_{ij} \\leftarrow \\mathop{softmax}\\limits\_j \\Big(\\langle\\boldsymbol{u}\_i,K\\boldsymbol{v}\_j\\rangle\\Big)$；
>   $\\boldsymbol{v}\_{j} \\leftarrow \\sum\\limits\_{i}c\_{ij}\\boldsymbol{u}\_{i}$；
> 返回$squash(\\boldsymbol{v}\_j)$。

然而这样又新引入了一个参数$K$，咋看上去$K$太大了就梯度消失，$K$太小了就不够准确，很难确定。不过后面我们将会看到，直接让$K=1$即可，因为$K=1$的解空间已经包含了任意$K$的解。最终我们可以得到

> 初始化$\\boldsymbol{v}\_{j}=\\boldsymbol{v}\_{j}^{(0)}$
> 迭代$r$次：
>   $\\boldsymbol{v}\_{j} \\leftarrow \\boldsymbol{v}\_{j}/\\Vert\\boldsymbol{v}\_{j}\\Vert$；
>   $c\_{ij} \\leftarrow \\mathop{softmax}\\limits\_j \\Big(\\langle\\boldsymbol{u}\_i, \\boldsymbol{v}\_j\\rangle\\Big)$；
>   $\\boldsymbol{v}\_{j} \\leftarrow \\sum\\limits\_{i}c\_{ij}\\boldsymbol{u}\_{i}$；
> 返回$squash(\\boldsymbol{v}\_j)$。

有意思的是，最后导出的结果，不仅跟Hinton的原始论文《Dynamic Routing Between Capsules》有所出入，跟我前一篇介绍也有出入。 **其中，最明显的差别是在迭代过程中用$\\boldsymbol{v}\_{j}/\\Vert\\boldsymbol{v}\_{j}\\Vert$替换了$squash(\\boldsymbol{v}\_j)$，仅在最后输出时才进行squash。** 实验表明这有助于提升特征的表达能力，它在我的前一文的数字实验（单数字训练，双数字预测）中，能达到95%以上的准确率（原来是91%）。

### 三种症状 [\#](https://kexue.fm/archives/5112\#%E4%B8%89%E7%A7%8D%E7%97%87%E7%8A%B6)

这样就完了？远远还没有。我们还要解决好几个问题。

**1、如何做好类别初始化？** 因为聚类结果跟初始化有关，而且好的初始化往往是聚类成功的一大半。现在我们要将聚类这个过程嵌入到模型中，作为模型的一部分，那么各个$\\boldsymbol{v}\_{j}^{(0)}$应该怎么选取呢？如果同一初始化，那么无法完成聚类过程；如果随机初始化，那又不能得到确定的聚类结果（就算类中心向量不变，但是类的顺序也可能变化）。

**2、如何识别特征顺序？** 我们知道，聚类的结果跟样本的顺序是无关的，也就是说，如果将输入向量的顺序打乱，聚类的结果还是一样的。对于样本间的聚类，这是一个优点；然而如果是特征间的聚类，那么就有可能不妥了，因为不同顺序的特征组合可能代表不同的含义（就好比词序不同，句子含义也会不同），如果都给出一样的结果，那么就丧失了特征的序信息了；

**3、如何保证特征表达能力？** 动态路由将上层Capsule作为底层Capsule的聚类结果，每个类可能包含多个特征向量，但如果仅仅用类中心向量代表整个类的整体特征（上层特征），会不会降低了上层Capsule的特征表达能力？

### 一个对策 [\#](https://kexue.fm/archives/5112\#%E4%B8%80%E4%B8%AA%E5%AF%B9%E7%AD%96)

有意思的是，以上三个问题都可以由同一个方法解决： **加变换矩阵**。

首先，为了模型的简洁性，我们将所有$\\boldsymbol{u}\_i$的和平均分配到每个类中作为$\\boldsymbol{v}\_j^{(0)}$。那怎么分辨出各个不同的类呢？我们在输出到每个类之前，给每个类都配一个变换矩阵$\\boldsymbol{W}\_j$，用来分辨不同的类，这时候动态路由变成了：

> 初始化$\\boldsymbol{v}\_{j} \\leftarrow \\frac{1}{k}\\sum\\limits\_{i=1}^n\\boldsymbol{W}\_j\\boldsymbol{u}\_{i}$
> 迭代$r$次：
>   $\\boldsymbol{v}\_{j} \\leftarrow \\boldsymbol{v}\_{j}/\\Vert\\boldsymbol{v}\_{j}\\Vert$；
>   $c\_{ij} \\leftarrow \\mathop{softmax}\\limits\_j \\Big(\\langle\\boldsymbol{W}\_j\\boldsymbol{u}\_i, \\boldsymbol{v}\_j\\rangle\\Big)$；
>   $\\boldsymbol{v}\_{j} \\leftarrow \\sum\\limits\_{i}c\_{ij}\\boldsymbol{W}\_j\\boldsymbol{u}\_{i}$；
> 返回$squash(\\boldsymbol{v}\_j)$。

这就是我前一篇介绍中所说的 **共享权重版的Capsule**。细细斟酌就会发现，引入训练矩阵$\\boldsymbol{W}\_j$是个非常妙的招数，它不仅解决了聚类的初始化问题（同一初始化经过矩阵$\\boldsymbol{W}\_j$映射为不同初始化），而且通过$\\boldsymbol{W}\_j$可以改变$\\boldsymbol{u}\_i$的维度，从而也就改变了聚类后的中心向量的维度，这样也就能保证中心向量的特征表达能力（可以升高或降低维度）。还有，以前我们做分类，是用一个向量做内积然后softmax的方式，也就是用一个向量代表一个类，现在则相当于用一个矩阵来代表一个类，当然也就可以表达更丰富的类信息。此外还有一个好处，那就是我们有$\\langle\\boldsymbol{W}\_j\\boldsymbol{u}\_i, K\\boldsymbol{v}\_j\\rangle=\\big\\langle\\big(K\\boldsymbol{W}\_j\\big)\\boldsymbol{u}\_i, \\boldsymbol{v}\_j\\big\\rangle$，也就是说它相当于把前面的参数$K$也包含了，从而我们可以放心设$K=1$而不用担心准确性不够——如果有必要，模型会自己去调整$\\boldsymbol{W}\_j$达到调整$K$的效果！

现在只剩下最后一个问题了：识别输入特征的顺序。跟识别每一个类一样，我们也可以给每个输入都配一个变换矩阵$\\boldsymbol{\\tilde{W}}\_i$，用来分辨不同位置的输入，这样一来动态路由变为

> 初始化$\\boldsymbol{v}\_{j} \\leftarrow \\frac{1}{k}\\sum\\limits\_{i=1}^n\\boldsymbol{W}\_j\\boldsymbol{\\tilde{W}}\_i\\boldsymbol{u}\_{i}$
> 迭代$r$次：
>   $\\boldsymbol{v}\_{j} \\leftarrow \\boldsymbol{v}\_{j}/\\Vert\\boldsymbol{v}\_{j}\\Vert$；
>   $c\_{ij} \\leftarrow \\mathop{softmax}\\limits\_j \\Big(\\langle\\boldsymbol{W}\_j\\boldsymbol{\\tilde{W}}\_i\\boldsymbol{u}\_i, \\boldsymbol{v}\_j\\rangle\\Big)$；
>   $\\boldsymbol{v}\_{j} \\leftarrow \\sum\\limits\_{i}c\_{ij}\\boldsymbol{W}\_j\\boldsymbol{\\tilde{W}}\_i\\boldsymbol{u}\_{i}$；
> 返回$squash(\\boldsymbol{v}\_j)$。

如果觉得这样太累赘，那么可以把$\\boldsymbol{W}\_j\\boldsymbol{\\tilde{W}}\_i$替换成一个整体矩阵$\\boldsymbol{W}\_{ji}$，也就是对每对指标$(i,j)$都配上一个变换矩阵，这样的好处是整体更简单明了，缺点是矩阵数目从$n+k$个变成了$nk$个：

> 初始化$\\boldsymbol{v}\_{j} \\leftarrow \\frac{1}{k}\\sum\\limits\_{i=1}^n\\boldsymbol{W}\_{ji}\\boldsymbol{u}\_{i}$
> 迭代$r$次：
>   $\\boldsymbol{v}\_{j} \\leftarrow \\boldsymbol{v}\_{j}/\\Vert\\boldsymbol{v}\_{j}\\Vert$；
>   $c\_{ij} \\leftarrow \\mathop{softmax}\\limits\_j \\Big(\\langle\\boldsymbol{W}\_{ji}\\boldsymbol{u}\_i, \\boldsymbol{v}\_j\\rangle\\Big)$；
>   $\\boldsymbol{v}\_{j} \\leftarrow \\sum\\limits\_{i}c\_{ij}\\boldsymbol{W}\_{ji}\\boldsymbol{u}\_{i}$；
> 返回$squash(\\boldsymbol{v}\_j)$。

这便是 **全连接版的动态路由**。然而并不是每次我们都要分辨不同位置的输入，对于变长的输入，我们就很难给每个位置的输入都分配一个变换矩阵，这时候共享版的动态路由就能派上用场了。总的来说，全连接版和共享版动态路由都有其用武之地。

图4：Capsule的变换矩阵可能的位置

## 结语 [\#](https://kexue.fm/archives/5112\#%E7%BB%93%E8%AF%AD)

笔者通过这两篇“浩浩荡荡”（哆里哆嗦）的博文，来试图解读Hinton大力发展的Capsule模型，然而作者水平有限，其中不当之处，还请读者海涵。

个人认为，Capsule的确是新颖的、有前景的的研究内容。也许它不一定（但也是有可能的）是未来的发展方向，但细细品味它，仍足以让我们获益良多。

现在回顾文章开头的目标——企图让Capsule看起来更加自然一些，不知道读者现在的感受如何？个人感觉是，之前经过这样的解析，Capsule也不是那么超然物外，而是一个大胆的尝试——Hinton大胆地将聚类的迭代过程融入到神经网络中，因此诞生了Capsule。

那是不是说，可以考虑将其他比较直观的算法也融入到里边，从而造就其他有意思的玩意？让我们拭目以待。

_**转载到请包括本文地址：** [https://kexue.fm/archives/5112](https://kexue.fm/archives/5112)_

_**更详细的转载事宜请参考：**_ [《科学空间FAQ》](https://kexue.fm/archives/6508#%E6%96%87%E7%AB%A0%E5%A6%82%E4%BD%95%E8%BD%AC%E8%BD%BD/%E5%BC%95%E7%94%A8)

**如果您还有什么疑惑或建议，欢迎在下方评论区继续讨论。**

**如果您觉得本文还不错，欢迎 [分享](https://kexue.fm/archives/5112#share)/ [打赏](https://kexue.fm/archives/5112#pay) 本文。打赏并非要从中获得收益，而是希望知道科学空间获得了多少读者的真心关注。当然，如果你无视它，也不会影响你的阅读。再次表示欢迎和感谢！**

打赏

微信打赏

支付宝打赏

因为网站后台对打赏并无记录，因此欢迎在打赏时候备注留言。你还可以 [**点击这里**](http://mail.qq.com/cgi-bin/qm_share?t=qm_mailme&email=tN7d1drY3drrx8H0xcWa19vZ) 或在下方评论区留言来告知你的建议或需求。

**如果您需要引用本文，请参考：**

苏剑林. (Feb. 12, 2018). 《再来一顿贺岁宴：从K-Means到Capsule 》\[Blog post\]. Retrieved from [https://kexue.fm/archives/5112](https://kexue.fm/archives/5112)

@online{kexuefm-5112,
        title={再来一顿贺岁宴：从K-Means到Capsule},
        author={苏剑林},
        year={2018},
        month={Feb},
        url={\\url{https://kexue.fm/archives/5112}},
}

分类： [信息时代](https://kexue.fm/category/Big-Data)    标签： [深度学习](https://kexue.fm/tag/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/), [聚类](https://kexue.fm/tag/%E8%81%9A%E7%B1%BB/), [Capsule](https://kexue.fm/tag/Capsule/)[55 评论](https://kexue.fm/archives/5112#comments)

< [【分享】千万级百度知道语料](https://kexue.fm/archives/5067) \| [三味Capsule：矩阵Capsule与EM路由](https://kexue.fm/archives/5155) >

### 你也许还对下面的内容感兴趣

- [为什么需要残差？一个来自DeepNet的视角](https://kexue.fm/archives/8994)
- [多任务学习漫谈（三）：分主次之序](https://kexue.fm/archives/8907)
- [多任务学习漫谈（二）：行梯度之事](https://kexue.fm/archives/8896)
- [多任务学习漫谈（一）：以损失之名](https://kexue.fm/archives/8870)
- [也来谈谈RNN的梯度消失/爆炸问题](https://kexue.fm/archives/7888)
- [L2正则没有想象那么好？可能是“权重尺度偏移”惹的祸](https://kexue.fm/archives/7681)
- [我们真的需要把训练集的损失降低到零吗？](https://kexue.fm/archives/7643)
- [节省显存的重计算技巧也有了Keras版了](https://kexue.fm/archives/7367)
- [突破瓶颈，打造更强大的Transformer](https://kexue.fm/archives/7325)
- [最小熵原理（五）：“层层递进”之社区发现与聚类](https://kexue.fm/archives/7006)

[发表你的看法](https://kexue.fm/archives/5112#comment_form)

1. [«](https://kexue.fm/archives/5112/comment-page-3#comments)
2. [1](https://kexue.fm/archives/5112/comment-page-1#comments)
3. [2](https://kexue.fm/archives/5112/comment-page-2#comments)
4. [3](https://kexue.fm/archives/5112/comment-page-3#comments)
5. [4](https://kexue.fm/archives/5112/comment-page-4#comments)

JohnMayer

June 7th, 2021

博主你好，关于“但由于存在argmax这个操作，我们用不了梯度下降，而梯度下降是目前求模型其他参数的唯一方法”这一段，我有点疑问，我理解动态路由计算时，是一种启发式的参数学习方法，那为什么不可以用argmax呢？

[回复评论](https://kexue.fm/archives/5112/comment-page-4?replyTo=16593#respond-post-5112)

[苏剑林](https://kexue.fm) 发表于
June 8th, 2021

动态路由仅仅是一个没任何特别之处的前向传播计算，它的参数依然要靠梯度下降来学习。

[回复评论](https://kexue.fm/archives/5112/comment-page-4?replyTo=16601#respond-post-5112)

[用户多兴趣建模MIND－javascript－94007技术网](https://www.94007.cc/javascript/360813)

June 14th, 2023

\[...\]\[4\] 再来一顿贺岁宴：从K-Means到Capsule\[...\]

[回复评论](https://kexue.fm/archives/5112/comment-page-4?replyTo=21978#respond-post-5112)

[MIND模型：用户多兴趣建模网络原理解析 R11; AICMTY](https://www.aicmty.com/32974.html)

December 4th, 2023

\[...\]\[4\] 再来一顿贺岁宴：从K-Means到Capsule\[...\]

[回复评论](https://kexue.fm/archives/5112/comment-page-4?replyTo=23196#respond-post-5112)

1. [«](https://kexue.fm/archives/5112/comment-page-3#comments)
2. [1](https://kexue.fm/archives/5112/comment-page-1#comments)
3. [2](https://kexue.fm/archives/5112/comment-page-2#comments)
4. [3](https://kexue.fm/archives/5112/comment-page-3#comments)
5. [4](https://kexue.fm/archives/5112/comment-page-4#comments)

[取消回复](https://kexue.fm/archives/5112#respond-post-5112)

你的大名

电子邮箱

个人网站（选填）

1\. 可以使用LaTeX代码，点击“预览效果”可查看效果；2. 可以通过点击评论楼层编号来引用该楼层；3. 网站可能会有点卡，如非确认评论失败，请 **不要重复点击提交**。

### 内容速览

[Capsule框架](https://kexue.fm/archives/5112#Capsule%E6%A1%86%E6%9E%B6)
[特征表达](https://kexue.fm/archives/5112#%E7%89%B9%E5%BE%81%E8%A1%A8%E8%BE%BE)
[特征组合](https://kexue.fm/archives/5112#%E7%89%B9%E5%BE%81%E7%BB%84%E5%90%88)
[组合与表达](https://kexue.fm/archives/5112#%E7%BB%84%E5%90%88%E4%B8%8E%E8%A1%A8%E8%BE%BE)
[特征间聚类](https://kexue.fm/archives/5112#%E7%89%B9%E5%BE%81%E9%97%B4%E8%81%9A%E7%B1%BB)
[特征显著性](https://kexue.fm/archives/5112#%E7%89%B9%E5%BE%81%E6%98%BE%E8%91%97%E6%80%A7)
[K-Means新探](https://kexue.fm/archives/5112#K-Means%E6%96%B0%E6%8E%A2)
[聚类目标](https://kexue.fm/archives/5112#%E8%81%9A%E7%B1%BB%E7%9B%AE%E6%A0%87)
[求解过程](https://kexue.fm/archives/5112#%E6%B1%82%E8%A7%A3%E8%BF%87%E7%A8%8B)
[欧氏距离](https://kexue.fm/archives/5112#%E6%AC%A7%E6%B0%8F%E8%B7%9D%E7%A6%BB)
[内积相似度](https://kexue.fm/archives/5112#%E5%86%85%E7%A7%AF%E7%9B%B8%E4%BC%BC%E5%BA%A6)
[动态路由](https://kexue.fm/archives/5112#%E5%8A%A8%E6%80%81%E8%B7%AF%E7%94%B1)
[基本步骤](https://kexue.fm/archives/5112#%E5%9F%BA%E6%9C%AC%E6%AD%A5%E9%AA%A4)
[三种症状](https://kexue.fm/archives/5112#%E4%B8%89%E7%A7%8D%E7%97%87%E7%8A%B6)
[一个对策](https://kexue.fm/archives/5112#%E4%B8%80%E4%B8%AA%E5%AF%B9%E7%AD%96)
[结语](https://kexue.fm/archives/5112#%E7%BB%93%E8%AF%AD)

### 智能搜索

支持整句搜索！网站自动使用 [结巴分词](https://github.com/fxsjy/jieba) 进行分词，并结合ngrams排序算法给出合理的搜索结果。

### 热门标签

[生成模型](https://kexue.fm/tag/%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B/) [attention](https://kexue.fm/tag/attention/) [优化](https://kexue.fm/tag/%E4%BC%98%E5%8C%96/) [语言模型](https://kexue.fm/tag/%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/) [模型](https://kexue.fm/tag/%E6%A8%A1%E5%9E%8B/) [网站](https://kexue.fm/tag/%E7%BD%91%E7%AB%99/) [概率](https://kexue.fm/tag/%E6%A6%82%E7%8E%87/) [梯度](https://kexue.fm/tag/%E6%A2%AF%E5%BA%A6/) [转载](https://kexue.fm/tag/%E8%BD%AC%E8%BD%BD/) [微分方程](https://kexue.fm/tag/%E5%BE%AE%E5%88%86%E6%96%B9%E7%A8%8B/) [矩阵](https://kexue.fm/tag/%E7%9F%A9%E9%98%B5/) [天象](https://kexue.fm/tag/%E5%A4%A9%E8%B1%A1/) [分析](https://kexue.fm/tag/%E5%88%86%E6%9E%90/) [深度学习](https://kexue.fm/tag/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/) [积分](https://kexue.fm/tag/%E7%A7%AF%E5%88%86/) [python](https://kexue.fm/tag/python/) [优化器](https://kexue.fm/tag/%E4%BC%98%E5%8C%96%E5%99%A8/) [力学](https://kexue.fm/tag/%E5%8A%9B%E5%AD%A6/) [无监督](https://kexue.fm/tag/%E6%97%A0%E7%9B%91%E7%9D%A3/) [扩散](https://kexue.fm/tag/%E6%89%A9%E6%95%A3/) [几何](https://kexue.fm/tag/%E5%87%A0%E4%BD%95/) [节日](https://kexue.fm/tag/%E8%8A%82%E6%97%A5/) [生活](https://kexue.fm/tag/%E7%94%9F%E6%B4%BB/) [文本生成](https://kexue.fm/tag/%E6%96%87%E6%9C%AC%E7%94%9F%E6%88%90/) [数论](https://kexue.fm/tag/%E6%95%B0%E8%AE%BA/)

### 随机文章

- [解答不等式的误区...](https://kexue.fm/archives/644)
- [关于交错级数的审敛法则](https://kexue.fm/archives/159)
- [球壳内部的均匀力场](https://kexue.fm/archives/988)
- [椭圆内的一根定长弦（化圆法）](https://kexue.fm/archives/1654)
- [对齐全量微调！这是我看过最精彩的LoRA改进（二）](https://kexue.fm/archives/10266)
- [百科翻译：氢氧化钠（NaOH）的详细介绍](https://kexue.fm/archives/6)
- [变分自编码器（六）：从几何视角来理解VAE的尝试](https://kexue.fm/archives/7725)
- [果壳中的条件随机场(CRF In A Nutshell)](https://kexue.fm/archives/4695)
- [对抗训练浅谈：意义、方法和思考（附Keras实现）](https://kexue.fm/archives/7234)
- [Transformer升级之路：16、“复盘”长度外推技术](https://kexue.fm/archives/9948)

### 最近评论

- [rpsun](https://kexue.fm/archives/10592/comment-page-2#comment-27912): 老师您好，最近在自己的任务上尝试了muon，甚至只修改了学习率，同时不加区分地对所有二维以上的...
- [盏一](https://kexue.fm/archives/8265/comment-page-8#comment-27911): 我之前做的笔记:Q: 公式 (14) 的理解.A: 首先基于 \[定理 5\](https://b...
- [盏一](https://kexue.fm/archives/8397/comment-page-3#comment-27910): 哦哦哦 你是说 $\\exp nB$ 是正交矩阵! 并不是说 B.
- [盏一](https://kexue.fm/archives/8397/comment-page-3#comment-27908): 呃, 是我脑子乱了... 忘了 $\\exp(0) = I$. 所以只要 $\\Vert B^T+...
- [盏一](https://kexue.fm/archives/8397/comment-page-3#comment-27907): 苏神, 请教一下\> 并且还可以证明它一定是正交矩阵是怎么证明的. 我本来以为隐式利用了 $\\V...
- [sk](https://kexue.fm/archives/8265/comment-page-8#comment-27906): 请问公式14是怎么得出来的？
- [tll1945tll1937](https://kexue.fm/archives/10266/comment-page-1#comment-27901): 真心实意的向大家请教问题：看了文章“对齐全量微调！这是我看过最精彩的LoRA改进（二）”，我实...
- [oYo\_logan](https://kexue.fm/archives/10757/comment-page-1#comment-27897): \[comment=27017\]苏剑林\[/comment\]苏神，想请教一下，我理解在一个batc...
- [z123](https://kexue.fm/archives/10592/comment-page-1#comment-27896): 在参数矩阵较多的CNN小模型上，Muon会明显慢于Adam，这方面有什么优化提速的方案吗？
- [dry](https://kexue.fm/archives/10958/comment-page-2#comment-27895): 苏神好，一直有个疑问，ReFlow构建的ODE是$dx\_t/dt=x\_1-x\_0$，为什么这并...

### 友情链接

- [Cool Papers](https://papers.cool)
- [数学研发](https://bbs.emath.ac.cn)
- [Seatop](http://www.seatop.com.cn/)
- [Xiaoxia](https://xiaoxia.org/)
- [积分表-网络版](https://kexue.fm/sci/integral/index.html)
- [丝路博傲](http://blog.dvxj.com/)
- [ph4ntasy 饭特稀](http://www.ph4ntasy.com/)
- [数学之家](http://www.2math.cn/)
- [有趣天文奇观](http://interesting-sky.china-vo.org/)
- [TwistedW](http://www.twistedwg.com/)
- [godweiyang](https://godweiyang.com/)
- [AI柠檬](https://blog.ailemon.net/)
- [王登科-DK博客](https://greatdk.com)
- [ESON](https://blog.eson.org/)
- [枫之羽](https://fzhiy.net/)
- [Mathor's blog](https://wmathor.com/)
- [coding-zuo](https://coding-zuo.github.io/)
- [博科园](https://www.bokeyuan.net/)
- [孔皮皮的博客](https://www.kppkkp.top/)
- [运鹏的博客](https://yunpengtai.top/)
- [jiming.site](https://jiming.site/)
- [OmegaXYZ](https://www.omegaxyz.com/)
- [Blog by Eacls](https://www.eacls.top/)
- [EAI猩球](https://www.robotech.ink/)
- [文举的博客](https://liwenju0.com/)
- [用代码打点酱油](https://bruceyuan.com/)
- [申请链接](https://kexue.fm/links.html)

本站采用创作共用版权协议，要求署名、非商业用途和保持一致。转载本站内容必须也遵循“ [署名-非商业用途-保持一致](http://creativecommons.org/licenses/by-nc-nd/2.5/cn/)”的创作共用协议。
© 2009-2025 Scientific Spaces. All rights reserved. Theme by [laogui](http://www.laogui.com). Powered by [Typecho](http://typecho.org). 备案号: [粤ICP备09093259号-1/2](https://beian.miit.gov.cn/)。