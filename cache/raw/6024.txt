## SEARCH

## MENU

- [打赏](https://kexue.fm/reward.html)
- [公式](https://kexue.fm/latex.html)
- [天象](https://kexue.fm/ac.html)
- [链接](https://kexue.fm/links.html)
- [时光](https://kexue.fm/me.html)
- [博览](https://kexue.fm/science.html)
- [归档](https://kexue.fm/content.html)

## CATEGORIES

- [千奇百怪](https://kexue.fm/category/Everything)
- [天文探索](https://kexue.fm/category/Astronomy)
- [数学研究](https://kexue.fm/category/Mathematics)
- [物理化学](https://kexue.fm/category/Phy-chem)
- [信息时代](https://kexue.fm/category/Big-Data)
- [生物自然](https://kexue.fm/category/Biology)
- [图片摄影](https://kexue.fm/category/Photograph)
- [问题百科](https://kexue.fm/category/Questions)
- [生活/情感](https://kexue.fm/category/Life-Feeling)
- [资源共享](https://kexue.fm/category/Resources)

## NEWPOSTS

- [通过msign来计算mclip（奇...](https://kexue.fm/archives/11006)
- [msign算子的Newton-Sc...](https://kexue.fm/archives/10996)
- [等值振荡定理：最优多项式逼近的充要条件](https://kexue.fm/archives/10972)
- [生成扩散模型漫谈（三十）：从瞬时速...](https://kexue.fm/archives/10958)
- [MoE环游记：5、均匀分布的反思](https://kexue.fm/archives/10945)
- [msign算子的Newton-Sc...](https://kexue.fm/archives/10922)
- [Transformer升级之路：2...](https://kexue.fm/archives/10907)
- [一道概率不等式：盯着它到显然成立为止！](https://kexue.fm/archives/10902)
- [SVD的导数](https://kexue.fm/archives/10878)
- [智能家居之手搓一套能接入米家的零冷水装置](https://kexue.fm/archives/10869)

## COMMENTS

- [PengchengMa: 牛啊](https://kexue.fm/archives/10996/comment-page-1#comment-27811)
- [xczh: 已使用mean flow policy，一步推理效果确实惊人，...](https://kexue.fm/archives/10958/comment-page-1#comment-27810)
- [Cosine: 是不是因为shared experts每次都激活，而route...](https://kexue.fm/archives/10945/comment-page-1#comment-27809)
- [rpsun: 这样似乎与传统的经验正交函数之类的有相似之处。把样本的平均值减...](https://kexue.fm/archives/10699/comment-page-1#comment-27808)
- [贵阳机场接机: 怎么不更新啦](https://kexue.fm/archives/1490/comment-page-1#comment-27807)
- [czvzb: 具身智能模型目前主流也是在使用扩散和流匹配这类方法来预测动作。...](https://kexue.fm/archives/10958/comment-page-1#comment-27806)
- [Shawn\_yang: 苏神，关于您所说的：“推理阶段可以事先预估Routed Exp...](https://kexue.fm/archives/10945/comment-page-1#comment-27802)
- [OceanYU: 您好，关于由式（7）推导出高斯分布，我这里有一点问题，式（7）...](https://kexue.fm/archives/9164/comment-page-4#comment-27801)
- [jorjiang: 训练和prefill这个compute-bound阶段不做矩阵...](https://kexue.fm/archives/10907/comment-page-2#comment-27800)
- [amy: 苏老师，您有关注傅里叶旋转位置编码这篇工作吗，想知道您对这篇工...](https://kexue.fm/archives/10907/comment-page-2#comment-27799)

## USERLOGIN

- [登录](https://kexue.fm/admin/login.php)

[科学空间\|Scientific Spaces](https://kexue.fm)

- [登录](https://kexue.fm/admin/login.php)
- [打赏](https://kexue.fm/reward.html)
- [公式](https://kexue.fm/latex.html)
- [天象](https://kexue.fm/ac.html)
- [链接](https://kexue.fm/links.html)
- [时光](https://kexue.fm/me.html)
- [博览](https://kexue.fm/science.html)
- [归档](https://kexue.fm/content.html)

渴望成为一个小飞侠

- [欢迎订阅](https://kexue.fm/feed)
- [个性邮箱](https://kexue.fm/archives/119)
- [天象信息](https://kexue.fm/ac.html)
- [观测ISS](https://kexue.fm/archives/41)
- [LaTeX](https://kexue.fm/latex.html)
- [关于博主](https://kexue.fm/me.html)

欢迎访问“科学空间”，这里将与您共同探讨自然科学，回味人生百态；也期待大家的分享～

- [**千奇百怪** Everything](https://kexue.fm/category/Everything)
- [**天文探索** Astronomy](https://kexue.fm/category/Astronomy)
- [**数学研究** Mathematics](https://kexue.fm/category/Mathematics)
- [**物理化学** Phy-chem](https://kexue.fm/category/Phy-chem)
- [**信息时代** Big-Data](https://kexue.fm/category/Big-Data)
- [**生物自然** Biology](https://kexue.fm/category/Biology)
- [**图片摄影** Photograph](https://kexue.fm/category/Photograph)
- [**问题百科** Questions](https://kexue.fm/category/Questions)
- [**生活/情感** Life-Feeling](https://kexue.fm/category/Life-Feeling)
- [**资源共享** Resources](https://kexue.fm/category/Resources)

- [**千奇百怪**](https://kexue.fm/category/Everything)
- [**天文探索**](https://kexue.fm/category/Astronomy)
- [**数学研究**](https://kexue.fm/category/Mathematics)
- [**物理化学**](https://kexue.fm/category/Phy-chem)
- [**信息时代**](https://kexue.fm/category/Big-Data)
- [**生物自然**](https://kexue.fm/category/Biology)
- [**图片摄影**](https://kexue.fm/category/Photograph)
- [**问题百科**](https://kexue.fm/category/Questions)
- [**生活/情感**](https://kexue.fm/category/Life-Feeling)
- [**资源共享**](https://kexue.fm/category/Resources)

[首页](https://kexue.fm) [信息时代](https://kexue.fm/category/Big-Data) 深度学习的互信息：无监督提取特征

2Oct

# [深度学习的互信息：无监督提取特征](https://kexue.fm/archives/6024)

By 苏剑林 \|
2018-10-02 \|
320067位读者\|

随机采样的KNN样本

对于NLP来说，互信息是一个非常重要的指标，它衡量了两个东西的本质相关性。本博客中也多次讨论过互信息，而我也对各种利用互信息的文章颇感兴趣。前几天在机器之心上看到了最近提出来的 [Deep INFOMAX模型](https://papers.cool/arxiv/1808.06670)，用最大化互信息来对图像做无监督学习，自然也颇感兴趣，研读了一番，就得到了本文。

本文整体思路源于Deep INFOMAX的原始论文，但并没有照搬原始模型，而是按照这自己的想法改动了模型（主要是先验分布部分），并且会在相应的位置进行注明。

## 我们要做什么 [\#](https://kexue.fm/archives/6024\#%E6%88%91%E4%BB%AC%E8%A6%81%E5%81%9A%E4%BB%80%E4%B9%88)

### 自编码器 [\#](https://kexue.fm/archives/6024\#%E8%87%AA%E7%BC%96%E7%A0%81%E5%99%A8)

特征提取是无监督学习中很重要且很基本的一项任务，常见形式是训练一个编码器将原始数据集编码为一个固定长度的向量。自然地，我们对这个编码器的基本要求是： **保留原始数据的（尽可能多的）重要信息**。

我们怎么知道编码向量保留了重要信息呢？一个很自然的想法是这个编码向量应该也要能还原出原始图片出来，所以我们还训练一个解码器，试图重构原图片，最后的loss就是原始图片和重构图片的mse。这导致了标准的自编码器的设计。后来，我们还希望编码向量的分布尽量能接近高斯分布，这就导致了变分自编码器。

### 重构的思考 [\#](https://kexue.fm/archives/6024\#%E9%87%8D%E6%9E%84%E7%9A%84%E6%80%9D%E8%80%83)

然而，值得思考的是“重构”这个要求是否合理？

首先，我们可以发现通过低维编码重构原图的结果通常是很模糊的，这可以解释为损失函数mse要求“逐像素”重建过于苛刻。又或者可以理解为，对于图像重构事实上我们并没有非常适合的loss可以选用，最理想的方法是用对抗网络训练一个判别器出来，但是这会进一步增加任务难度。

其次，一个很有趣的事实是：我们大多数人能分辨出很多真假币，但如果要我们画一张百元大钞出来，我相信基本上画得一点都不像。这表明，对于真假币识别这个任务，可以设想我们有了一堆真假币供学习，我们能从中提取很丰富的特征，但是这些特征并不足以重构原图，它只能让我们分辨出这堆纸币的差异。也就是说，对于数据集和任务来说，合理的、充分的特征并不一定能完成图像重构。

## 最大化互信息 [\#](https://kexue.fm/archives/6024\#%E6%9C%80%E5%A4%A7%E5%8C%96%E4%BA%92%E4%BF%A1%E6%81%AF)

### 互信息 [\#](https://kexue.fm/archives/6024\#%E4%BA%92%E4%BF%A1%E6%81%AF)

上面的讨论表明，重构不是好特征的必要条件。好特征的基本原则应当是“能够从整个数据集中辨别出该样本出来”，也就是说，提取出该样本（最）独特的信息。如何衡量提取出来的信息是该样本独特的呢？我们用“互信息”来衡量。

让我们先引入一些记号，用$X$表示原始图像的集合，用$x\\in X$表示某一原始图像，$Z$表示编码向量的集合，$z\\in Z$表示某个编码向量，$p(z\|x)$表示$x$所产生的编码向量的分布，我们设它为高斯分布，或者简单理解它就是我们想要寻找的编码器。那么可以用互信息来表示$X,Z$的相关性
$$\\begin{equation}I(X,Z) = \\iint p(z\|x)\\tilde{p}(x)\\log \\frac{p(z\|x)}{p(z)}dxdz\\label{eq:mi}\\end{equation}$$
这里的$\\tilde{p}(x)$原始数据的分布，$p(z)$是在$p(z\|x)$给定之后整个$Z$的分布，即
$$\\begin{equation}p(z) = \\int p(z\|x)\\tilde{p}(x)dx\\end{equation}$$
那么一个好的特征编码器，应该要使得互信息尽量地大，即
$$\\begin{equation}p(z\|x) = \\mathop{\\text{argmax}}\_{p(z\|x)} I(X,Z) \\end{equation}$$
互信息越大意味着（大部分的）$\\log \\frac{p(z\|x)}{p(z)}$应当尽量大，这意味着$p(z\|x)$应当远大于$p(z)$，即对于每个$x$，编码器能找出专属于$x$的那个$z$，使得$p(z\|x)$的概率远大于随机的概率$p(z)$。这样一来，我们就有能力只通过$z$就从中分辨出原始样本来。

> 注意：$\\eqref{eq:mi}$的名称为互信息，而对数项$\\log \\frac{p(z\|x)}{p(z)}$我们称为“点互信息”，有时也直接称为互信息。两者的差别是：$\\eqref{eq:mi}$计算的是整体关系，比如回答“前后两个词有没有关系”的问题；$\\log \\frac{p(z\|x)}{p(z)}$计算的是局部关系，比如回答“‘忐’和‘忑’是否经常连在一起出现”的问题。

### 先验分布 [\#](https://kexue.fm/archives/6024\#%E5%85%88%E9%AA%8C%E5%88%86%E5%B8%83)

前面提到，相对于自编码器，变分自编码器同时还希望隐变量服从标准正态分布的先验分布，这有利于使得编码空间更加规整，甚至有利于解耦特征，便于后续学习。因此，在这里我们同样希望加上这个约束。

Deep INFOMAX论文中通过类似AAE的思路通过对抗来加入这个约束，但众所周知对抗是一个最大最小化过程，需要交替训练，不够稳定，也不够简明。这里提供另一种更加端到端的思路：设$q(z)$为标准正态分布，我们去最小化$p(z)$与先验分布$q(z)$的KL散度
$$\\begin{equation}\\label{eq:prior}KL(p(z)\\Vert q(z))=\\int p(z)\\log \\frac{p(z)}{q(z)}dz\\end{equation}$$
将$\\eqref{eq:mi}$与$\\eqref{eq:prior}$加权混合起来，我们可以得到最小化的总目标：
$$\\begin{equation}\\begin{aligned}p(z\|x) =& \\min\_{p(z\|x)} \\left\\{- I(X,Z) + \\lambda KL(p(z)\\Vert q(z))\\right\\}\\\
=&\\min\_{p(z\|x)}\\left\\{-\\iint p(z\|x)\\tilde{p}(x)\\log \\frac{p(z\|x)}{p(z)}dxdz + \\lambda\\int p(z)\\log \\frac{p(z)}{q(z)}dz\\right\\}\\end{aligned}\\label{eq:total-loss-1}\\end{equation}$$
看起来很清晰很美好，但是我们还不知道$p(z)$的表达式，也就没法算下去了，因此这事还没完。

## 逐个击破 [\#](https://kexue.fm/archives/6024\#%E9%80%90%E4%B8%AA%E5%87%BB%E7%A0%B4)

### 简化先验项 [\#](https://kexue.fm/archives/6024\#%E7%AE%80%E5%8C%96%E5%85%88%E9%AA%8C%E9%A1%B9)

有意思的是式$\\eqref{eq:total-loss-1}$的loss进行稍加变换得到：
$$\\begin{equation}p(z\|x) =\\min\_{p(z\|x)}\\left\\{\\iint p(z\|x)\\tilde{p}(x)\\left\[-(1+\\lambda)\\log \\frac{p(z\|x)}{p(z)} + \\lambda \\log \\frac{p(z\|x)}{q(z)}\\right\]dxdz\\right\\}\\end{equation}$$
注意上式正好是互信息与$\\mathbb{E}\_{x\\sim\\tilde{p}(x)}\[KL(p(z\|x)\\Vert q(z))\]$的加权求和，而$KL(p(z\|x)\\Vert q(z))$这一项是可以算出来的（正好是VAE的那一项KL散度），所以我们已经成功地解决了整个loss的一半，可以写为
$$\\begin{equation}p(z\|x) =\\min\_{p(z\|x)}\\left\\{-\\beta\\cdot I(X,Z)+\\gamma\\cdot \\mathbb{E}\_{x\\sim\\tilde{p}(x)}\[KL(p(z\|x)\\Vert q(z))\]\\right\\}\\label{eq:total-loss-2}\\end{equation}$$
下面我们主攻互信息这一项。

### 互信息本质 [\#](https://kexue.fm/archives/6024\#%E4%BA%92%E4%BF%A1%E6%81%AF%E6%9C%AC%E8%B4%A8)

现在只剩下了互信息这一项没解决了，怎么才能最大化互信息呢？我们把互信息的定义$\\eqref{eq:mi}$稍微变换一下：
$$\\begin{equation}\\begin{aligned}I(X,Z) =& \\iint p(z\|x)\\tilde{p}(x)\\log \\frac{p(z\|x)\\tilde{p}(x)}{p(z)\\tilde{p}(x)}dxdz\\\
=& KL(p(z\|x)\\tilde{p}(x)\\Vert p(z)\\tilde{p}(x))
\\end{aligned}\\end{equation}$$
这个形式揭示了互信息的本质含义：$p(z\|x)\\tilde{p}(x)$描述了两个变量$x,z$的联合分布，$p(z)\\tilde{p}(x)$则是随机抽取一个$x$和一个$z$时的分布（假设它们两个不相关时），而互信息则是这两个分布的KL散度。而所谓最大化互信息，就是要拉大$p(z\|x)\\tilde{p}(x)$与$p(z)\\tilde{p}(x)$之间的距离。

注意KL散度理论上是无上界的，我们要去最大化一个无上界的量，这件事情有点危险，很可能得到无穷大的结果。所以，为了更有效地优化，我们抓住“最大化互信息就是拉大$p(z\|x)\\tilde{p}(x)$与$p(z)\\tilde{p}(x)$之间的距离”这个特点，我们不用KL散度，而换一个有上界的度量：JS散度（当然理论上也可以换成Hellinger距离，请参考 [《f-GAN简介：GAN模型的生产车间》](https://kexue.fm/archives/6016)），它定义为
$$JS(P,Q) = \\frac{1}{2}KL\\left(P\\left\\Vert\\frac{P+Q}{2}\\right.\\right)+\\frac{1}{2}KL\\left(Q\\left\\Vert\\frac{P+Q}{2}\\right.\\right)$$
JS散度同样衡量了两个分布的距离，但是它有上界$\\frac{1}{2}\\log 2$，我们最大化它的时候，同样能起到类似最大化互信息的效果，但是又不用担心无穷大问题。于是我们用下面的目标取代式$\\eqref{eq:total-loss-2}$
$$\\begin{equation}p(z\|x) =\\min\_{p(z\|x)}\\left\\{-\\beta\\cdot JS\\big(p(z\|x)\\tilde{p}(x), p(z)\\tilde{p}(x)\\big)+\\gamma\\cdot \\mathbb{E}\_{x\\sim\\tilde{p}(x)}\[KL(p(z\|x)\\Vert q(z))\]\\right\\}\\label{eq:total-loss-3}\\end{equation}$$
当然，这并没有改变问题的本质和难度，JS散度也还是没有算出来。下面到了攻关的最后一步。

### 攻克互信息 [\#](https://kexue.fm/archives/6024\#%E6%94%BB%E5%85%8B%E4%BA%92%E4%BF%A1%E6%81%AF)

在文章 [《f-GAN简介：GAN模型的生产车间》](https://kexue.fm/archives/6016) 中，我们介绍了一般的$f$散度的局部变分推断（那篇文章的式$(13)$）
$$\\begin{equation}\\mathcal{D}\_f(P\\Vert Q) = \\max\_{T}\\Big(\\mathbb{E}\_{x\\sim p(x)}\[T(x)\]-\\mathbb{E}\_{x\\sim q(x)}\[g(T(x))\]\\Big)\\label{eq:f-div-e}\\end{equation}$$
对于JS散度，给出的结果是
$$\\begin{equation}JS(P,Q) = \\max\_{T}\\Big(\\mathbb{E}\_{x\\sim p(x)}\[\\log \\sigma(T(x))\] + \\mathbb{E}\_{x\\sim q(x)}\[\\log(1-\\sigma(T(x))\]\\Big)\\end{equation}$$
代入$p(z\|x)\\tilde{p}(x), p(z)\\tilde{p}(x)$就得到
$$\\begin{equation}\\begin{aligned}&JS\\big(p(z\|x)\\tilde{p}(x), p(z)\\tilde{p}(x)\\big)\\\=& \\max\_{T}\\Big(\\mathbb{E}\_{(x,z)\\sim p(z\|x)\\tilde{p}(x)}\[\\log \\sigma(T(x,z))\] + \\mathbb{E}\_{(x,z)\\sim p(z)\\tilde{p}(x)}\[\\log(1-\\sigma(T(x,z))\]\\Big)\\end{aligned}\\label{eq:f-div-e-js}\\end{equation}$$
你没看错，除去常数项不算，它就完全等价于deep INFOMAX论文中的式$(5)$。我很奇怪，为什么论文作者放着上面这个好看而直观的形式不用，非得故弄玄虚搞个让人茫然的形式。其实$\\eqref{eq:f-div-e-js}$式的含义非常简单，它就是“负采样估计”：引入一个判别网络$\\sigma(T(x,z))$，$x$及其对应的$z$视为一个正样本对，$x$及随机抽取的$z$则视为负样本，然后最大化似然函数，等价于最小化交叉熵。

这样一来，通过负采样的方式，我们就给出了估计JS散度的一种方案，从而也就给出了估计JS版互信息的一种方案，从而成功攻克了互信息。现在，对应式$\\eqref{eq:total-loss-3}$，具体的loss为
$$\\begin{equation}\\begin{aligned}&p(z\|x),T(x,z) \\\
=&\\min\_{p(z\|x),T(x,z)}\\Big\\{-\\beta\\cdot\\Big(\\mathbb{E}\_{(x,z)\\sim p(z\|x)\\tilde{p}(x)}\[\\log \\sigma(T(x,z))\] + \\mathbb{E}\_{(x,z)\\sim p(z)\\tilde{p}(x)}\[\\log(1-\\sigma(T(x,z))\]\\Big)\\\
&\\qquad\\qquad\\qquad+\\gamma\\cdot \\mathbb{E}\_{x\\sim\\tilde{p}(x)}\[KL(p(z\|x)\\Vert q(z))\]\\Big\\}\\end{aligned}\\label{eq:total-loss-4}\\end{equation}$$
现在，理论已经完备了，剩下的就是要付诸实践了。

## 从全局到局部 [\#](https://kexue.fm/archives/6024\#%E4%BB%8E%E5%85%A8%E5%B1%80%E5%88%B0%E5%B1%80%E9%83%A8)

### batch内打乱 [\#](https://kexue.fm/archives/6024\#batch%E5%86%85%E6%89%93%E4%B9%B1)

从实验上来看，式$\\eqref{eq:total-loss-4}$就是要怎么操作呢？先验分布的KL散度那一项不难办，照搬VAE的即可。而互信息那一项呢？

首先，我们随机选一张图片$x$，通过编码器就可以得到$z$的均值和方差，然后重参数就可以得到$z\_x$，这样的一个$(x, z\_x)$对构成一个正样本呢；负样本呢？为了减少计算量，我们直接在batch内对图片进行随机打乱，然后按照随机打乱的顺序作为选择负样本的依据，也就是说，如果$x$是原来batch内的第4张图片，将图片打乱后第4张图片是$\\hat{x}$，那么$(x,z\_x)$就是正样本，$(\\hat{x},z\_x)$就是负样本。

### 局部互信息 [\#](https://kexue.fm/archives/6024\#%E5%B1%80%E9%83%A8%E4%BA%92%E4%BF%A1%E6%81%AF)

上面的做法，实际上就是考虑了整张图片之间的关联，但是我们知道，图片的相关性更多体现在局部中（也就是因为这样所以我们才可以对图片使用CNN）。换言之，图片的识别、分类等应该是一个从局部到整体的过程。因此，有必要把“局部互信息”也考虑进来。

通过CNN进行编码的过程一般是：
$$\\text{原始图片}x\\xrightarrow{\\text{多个卷积层}} h\\times w\\times c\\text{的特征} \\xrightarrow{\\text{卷积和全局池化}} \\text{固定长度的向量}z$$
我们已经考虑了$x$和$z$的关联，那么中间层特征（feature map）和$z$的关联呢？我们记中间层特征为$\\{C\_{ij}(x)\|i=1,2,\\dots,h;j=1,2,\\dots,w\\}$也就是视为$h\\times w$个向量的集合，我们也去算这$h\\times w$个向量跟$z\_x$的互信息，称为“局部互信息”。

估算方法跟全局是一样的，将每一个$C\_{ij}(x)$与$z\_x$拼接起来得到$\[C\_{ij}(x), z\_x\]$，相当于得到了一个更大的feature map，然后对这个feature map用多个1x1的卷积层来作为局部互信息的估算网络$T\_{local}$。负样本的选取方法也是用在batch内随机打算的方案。

现在，加入局部互信息的总loss为
$$\\begin{equation}\\begin{aligned}&p(z\|x),T\_1(x,z),T\_2(C\_{ij}, z)=\\min\_{p(z\|x),T\_1,T\_2}\\Big\\{\\\
&\\quad-\\alpha\\cdot\\Big(\\mathbb{E}\_{(x,z)\\sim p(z\|x)\\tilde{p}(x)}\[\\log \\sigma(T\_1(x,z))\] + \\mathbb{E}\_{(x,z)\\sim p(z)\\tilde{p}(x)}\[\\log(1-\\sigma(T\_1(x,z))\]\\Big)\\\
&\\quad-\\frac{\\beta}{hw}\\sum\_{i,j}\\Big(\\mathbb{E}\_{(x,z)\\sim p(z\|x)\\tilde{p}(x)}\[\\log \\sigma(T\_2(C\_{ij},z))\] + \\mathbb{E}\_{(x,z)\\sim p(z)\\tilde{p}(x)}\[\\log(1-\\sigma(T\_2(C\_{ij},z))\]\\Big)\\\
&\\quad+\\gamma\\cdot \\mathbb{E}\_{x\\sim\\tilde{p}(x)}\[KL(p(z\|x)\\Vert q(z))\]\\Big\\}\\end{aligned}\\label{eq:total-loss-5}\\end{equation}$$

### 其他信息 [\#](https://kexue.fm/archives/6024\#%E5%85%B6%E4%BB%96%E4%BF%A1%E6%81%AF)

其实，还有很多其他的信息可以考虑进去。

比如我们已经考虑了$C\_{ij}$与$z$的互信息，还可以考虑的是$C\_{ij}$之间的互信息，即同一张图片之间的$C\_{ij}$应当是有关联的，它们的互信息应该尽可能大（正样本），而不同图片之间的$C\_{ij}$应当是没关联的，它们的互信息应该尽可能小。不过我实验过，这一项的提升不是特别明显。

还有多尺度信息，可以手动在输入图片那里做多尺度的数据扩增，又或者是在编码器这些引入多尺度结构、Attention结构。诸如此类的操作，都可以考虑引入到无监督学习中，提高编码质量。

### 类似的word2vec [\#](https://kexue.fm/archives/6024\#%E7%B1%BB%E4%BC%BC%E7%9A%84word2vec)

其实，熟悉NLP中的word2vec模型原理的读者应该会感觉到：这不就是图像中的word2vec吗？

没错，在原理和做法上deep INFOMAX跟word2vec大体都一样。在word2vec中，也是随机采集负样本，然后通过判别器来区分两者的过程。这个过程我们通常称为“噪声对比估计”，我们之前也提到过，word2vec的噪声对比估计过程（负采样）的实际优化目标就是互信息。（细节请参考 [《“噪声对比估计”杂谈：曲径通幽之妙》](https://kexue.fm/archives/5617)）

word2vec中，固定了一个窗口大小，然后在窗口内统计词的共现（正样本）。而deep INFOMAX呢？因为只有一张图片，没有其他“词”，所以它干脆把图片分割为一个个小块，然后把一张图片当作一个窗口，图片的每个小块就是一个个词了。当然，更准确地类比的话，deep INFOMAX更像类似word2vec的那个doc2vec模型。

换个角度来想，也可以这样理解：局部互信息的引入相当于将每个小局部也看成了一个样本，这样就相当于原来的1个样本变成了$1+hw$个样本，大大增加了样本量，所以能提升效果。同时这样做也保证了图片的每一个“角落”都被用上了，因为低维压缩编码，比如$32\\times 32\\times 3$编码到128维，很可能左上角的$8\\times 8\\times 3 > 128$的区域就已经能够唯一分辨出图片出来了，但这不能代表整张图片，因此要想办法让整张图片都用上。

## 开源和效果图 [\#](https://kexue.fm/archives/6024\#%E5%BC%80%E6%BA%90%E5%92%8C%E6%95%88%E6%9E%9C%E5%9B%BE)

### 参考代码 [\#](https://kexue.fm/archives/6024\#%E5%8F%82%E8%80%83%E4%BB%A3%E7%A0%81)

其实上述模型的实现代码应当说还是比较简单的（总比我复现glow模型容易几十倍～），不管用哪个框架都不困难，下面是用Keras实现的一个版本（Python 2.7 + Tensorflow 1.8 + Keras 2.2.4）：

Github： [https://github.com/bojone/infomax](https://github.com/bojone/infomax)

### 来，上图片 [\#](https://kexue.fm/archives/6024\#%E6%9D%A5%EF%BC%8C%E4%B8%8A%E5%9B%BE%E7%89%87)

无监督的算法好坏比较难定量判断，一般都是通过做很多下游任务看效果的。就好比当初词向量很火时，怎么定量衡量词向量的质量也是一个很头疼的问题。deep INFOMAX论文中做了很多相关实验，我这里也不重复了，只是看看它的KNN效果（通过一张图片查找最相近的k张图片）。

总的来说效果差强人意，我觉得精调之后做一个简单的以图搜图问题不大。原论文中的很多实验效果也都不错，进一步印证了该思路的威力～

#### cifar10 [\#](https://kexue.fm/archives/6024\#cifar10)

每一行的左边第一张是原始图片，右边9张是最邻近图片，用的是cos相似度。用欧氏距离的排序结果类似。

随机采样的KNN样本1

随机采样的KNN样本2

#### Tiny Imagenet [\#](https://kexue.fm/archives/6024\#Tiny%20Imagenet)

每一行的左边第一张是原始图片，右边9张是最邻近图片，用的是cos相似度。用欧氏距离的排序结果类似。

随机采样的KNN样本1

随机采样的KNN样本2

#### 全局 vs 局部 [\#](https://kexue.fm/archives/6024\#%E5%85%A8%E5%B1%80%20vs%20%E5%B1%80%E9%83%A8)

局部互信息的引入是很必要的，下面比较了只有全局互信息和只有局部互信息时的KNN的差异。

随机KNN样本（只有全局互信息）

随机KNN样本（只有局部互信息）

## 又到终点站 [\#](https://kexue.fm/archives/6024\#%E5%8F%88%E5%88%B0%E7%BB%88%E7%82%B9%E7%AB%99)

作为无监督学习的成功，将常见于NLP的互信息概念一般化、理论化，然后用到了图像中。当然，现在看来它也可以反过来用回NLP中，甚至用到其他领域，因为它已经被抽象化了，适用性很强。

deep INFOMAX整篇文章的风格我是很喜欢的：从一般化的理念（互信息最大化）到估算框架再到实际模型，思路清晰，论证完整，是我心中的理想文章的风格（除了它对先验分布的处理用了对抗网络，我认为这是没有必要的）。期待看到更多的这类文章。

_**转载到请包括本文地址：** [https://kexue.fm/archives/6024](https://kexue.fm/archives/6024)_

_**更详细的转载事宜请参考：**_ [《科学空间FAQ》](https://kexue.fm/archives/6508#%E6%96%87%E7%AB%A0%E5%A6%82%E4%BD%95%E8%BD%AC%E8%BD%BD/%E5%BC%95%E7%94%A8)

**如果您还有什么疑惑或建议，欢迎在下方评论区继续讨论。**

**如果您觉得本文还不错，欢迎 [分享](https://kexue.fm/archives/6024#share)/ [打赏](https://kexue.fm/archives/6024#pay) 本文。打赏并非要从中获得收益，而是希望知道科学空间获得了多少读者的真心关注。当然，如果你无视它，也不会影响你的阅读。再次表示欢迎和感谢！**

打赏

微信打赏

支付宝打赏

因为网站后台对打赏并无记录，因此欢迎在打赏时候备注留言。你还可以 [**点击这里**](http://mail.qq.com/cgi-bin/qm_share?t=qm_mailme&email=tN7d1drY3drrx8H0xcWa19vZ) 或在下方评论区留言来告知你的建议或需求。

**如果您需要引用本文，请参考：**

苏剑林. (Oct. 02, 2018). 《深度学习的互信息：无监督提取特征 》\[Blog post\]. Retrieved from [https://kexue.fm/archives/6024](https://kexue.fm/archives/6024)

@online{kexuefm-6024,
        title={深度学习的互信息：无监督提取特征},
        author={苏剑林},
        year={2018},
        month={Oct},
        url={\\url{https://kexue.fm/archives/6024}},
}

分类： [信息时代](https://kexue.fm/category/Big-Data)    标签： [最大熵](https://kexue.fm/tag/%E6%9C%80%E5%A4%A7%E7%86%B5/), [无监督](https://kexue.fm/tag/%E6%97%A0%E7%9B%91%E7%9D%A3/), [互信息](https://kexue.fm/tag/%E4%BA%92%E4%BF%A1%E6%81%AF/)[95 评论](https://kexue.fm/archives/6024#comments)

< [f-GAN简介：GAN模型的生产车间](https://kexue.fm/archives/6016) \| [深度学习中的Lipschitz约束：泛化与生成模型](https://kexue.fm/archives/6051) >

### 你也许还对下面的内容感兴趣

- [生成扩散模型漫谈（二十三）：信噪比与大图生成（下）](https://kexue.fm/archives/10055)
- [BytePiece：更纯粹、更高压缩率的Tokenizer](https://kexue.fm/archives/9752)
- [用热传导方程来指导自监督学习](https://kexue.fm/archives/9359)
- [WGAN新方案：通过梯度归一化来实现L约束](https://kexue.fm/archives/8757)
- [曾被嫌弃的预训练任务NSP，做出了优秀的Zero Shot效果](https://kexue.fm/archives/8671)
- [UniVAE：基于Transformer的单模型、多尺度的VAE模型](https://kexue.fm/archives/8475)
- [变分自编码器（七）：球面上的VAE（vMF-VAE）](https://kexue.fm/archives/8404)
- [从动力学角度看优化算法（六）：为什么SimSiam不退化？](https://kexue.fm/archives/7980)
- [必须要GPT3吗？不，BERT的MLM模型也能小样本学习](https://kexue.fm/archives/7764)
- [变分自编码器（六）：从几何视角来理解VAE的尝试](https://kexue.fm/archives/7725)

[发表你的看法](https://kexue.fm/archives/6024#comment_form)

1. [«](https://kexue.fm/archives/6024/comment-page-3#comments)
2. [1](https://kexue.fm/archives/6024/comment-page-1#comments)
3. [2](https://kexue.fm/archives/6024/comment-page-2#comments)
4. [3](https://kexue.fm/archives/6024/comment-page-3#comments)
5. [4](https://kexue.fm/archives/6024/comment-page-4#comments)

吴俊康

December 4th, 2020

苏神您好！
1.请问最小化互信息应该就添加一个负号就行吧？
2.请问如果对于需要最小化多个互信息有没有效率比较高的方式呀？比如minimize MI(x;y;z)这种情况呢？还是说需要两两分别进行最小化互信息？

[回复评论](https://kexue.fm/archives/6024/comment-page-4?replyTo=14962#respond-post-6024)

Alisa

February 13th, 2022

苏老师，您好，打扰了。1.想问一下第二个公式(2式)是什么($p(z)$为什么会等于右式)，有点懵；非常感谢老师的指导！

[回复评论](https://kexue.fm/archives/6024/comment-page-4?replyTo=18420#respond-post-6024)

Alisa 发表于
February 13th, 2022

老师，不好意思，刚刚脑子短路了，老师不用理会这个问题

[回复评论](https://kexue.fm/archives/6024/comment-page-4?replyTo=18421#respond-post-6024)

Guodeguo

December 26th, 2022

苏神你好，我想问下6式那个λ化简之后不是抵消前后为0了么

[回复评论](https://kexue.fm/archives/6024/comment-page-4?replyTo=20648#respond-post-6024)

[苏剑林](https://kexue.fm) 发表于
December 27th, 2022

一个是$p(z)$，一个是$q(z)$，怎么抵消？

[回复评论](https://kexue.fm/archives/6024/comment-page-4?replyTo=20650#respond-post-6024)

tianyi

January 11th, 2023

苏老师，想请教您一个问题，互信息可以在不同维度的变量之间计算吗？例如MNIST数据集，输入是28\*28的图片，而对应的标签只有1维，那么X是\[28\*28\]的向量，Y就是对应的标签。每次取样本的时候，采样得到的x有28\*28个值，而y只有一个值，这样两者如何计算互信息呀？

[回复评论](https://kexue.fm/archives/6024/comment-page-4?replyTo=20731#respond-post-6024)

[苏剑林](https://kexue.fm) 发表于
January 13th, 2023

点互信息是$\\frac{p(x,y)}{p(x)p(y)}$，互信息是$\\iint p(x,y)\\frac{p(x,y)}{p(x)p(y)}dxdy$，估计每一个亮，然后代入公式计算就行。

[回复评论](https://kexue.fm/archives/6024/comment-page-4?replyTo=20745#respond-post-6024)

Swaggyzhang

October 19th, 2023

苏神您好，您文章中(3)式的含义是不是“找到令$I(X;Z)$最大的$p(z\|x)$”呢？
如果是的话，是不是用$p(z\|x)=\\mathop{argmax}\\limits\_{p(z\|x)}\\{I(X;Z)\\}$更准确些呢？

[回复评论](https://kexue.fm/archives/6024/comment-page-4?replyTo=22909#respond-post-6024)

[苏剑林](https://kexue.fm) 发表于
October 21st, 2023

是的，感谢建议，已修改。

[回复评论](https://kexue.fm/archives/6024/comment-page-4?replyTo=22923#respond-post-6024)

小灰

July 17th, 2024

互信息越大意味着（大部分的）$log\\frac{p(z\|x)}{p(z)}$应当尽量大，这意味着p(z\|x)应当远大于p(z)，即对于每个x，编码器能找出专属于x的那个z，使得p(z\|x)的概率远大于随机的概率p(z)。这样一来，我们就有能力只通过z就从中分辨出原始样本来。
苏神,这段话不是很理解，p(z\|x)不是由x推测z的概率吗，而“只通过z就从中分辨出原始样本来。”不是指p(x\|z)吗？还有“随机的概率p(z)”，这句话怎么理解呢，为什么说是随机的。

[回复评论](https://kexue.fm/archives/6024/comment-page-4?replyTo=24856#respond-post-6024)

[苏剑林](https://kexue.fm) 发表于
July 18th, 2024

简单来说，$p(z)$是无条件分布，$p(z\|x)$是条件分布，如果$p(z\|x)\\gg p(z)$，意味着条件$x$跟$z$的关系更密切。

[回复评论](https://kexue.fm/archives/6024/comment-page-4?replyTo=24868#respond-post-6024)

1. [«](https://kexue.fm/archives/6024/comment-page-3#comments)
2. [1](https://kexue.fm/archives/6024/comment-page-1#comments)
3. [2](https://kexue.fm/archives/6024/comment-page-2#comments)
4. [3](https://kexue.fm/archives/6024/comment-page-3#comments)
5. [4](https://kexue.fm/archives/6024/comment-page-4#comments)

[取消回复](https://kexue.fm/archives/6024#respond-post-6024)

你的大名

电子邮箱

个人网站（选填）

1\. 可以使用LaTeX代码，点击“预览效果”可查看效果；2. 可以通过点击评论楼层编号来引用该楼层；3. 网站可能会有点卡，如非确认评论失败，请不要重复点击提交。

### 内容速览

[我们要做什么](https://kexue.fm/archives/6024#%E6%88%91%E4%BB%AC%E8%A6%81%E5%81%9A%E4%BB%80%E4%B9%88)
[自编码器](https://kexue.fm/archives/6024#%E8%87%AA%E7%BC%96%E7%A0%81%E5%99%A8)
[重构的思考](https://kexue.fm/archives/6024#%E9%87%8D%E6%9E%84%E7%9A%84%E6%80%9D%E8%80%83)
[最大化互信息](https://kexue.fm/archives/6024#%E6%9C%80%E5%A4%A7%E5%8C%96%E4%BA%92%E4%BF%A1%E6%81%AF)
[互信息](https://kexue.fm/archives/6024#%E4%BA%92%E4%BF%A1%E6%81%AF)
[先验分布](https://kexue.fm/archives/6024#%E5%85%88%E9%AA%8C%E5%88%86%E5%B8%83)
[逐个击破](https://kexue.fm/archives/6024#%E9%80%90%E4%B8%AA%E5%87%BB%E7%A0%B4)
[简化先验项](https://kexue.fm/archives/6024#%E7%AE%80%E5%8C%96%E5%85%88%E9%AA%8C%E9%A1%B9)
[互信息本质](https://kexue.fm/archives/6024#%E4%BA%92%E4%BF%A1%E6%81%AF%E6%9C%AC%E8%B4%A8)
[攻克互信息](https://kexue.fm/archives/6024#%E6%94%BB%E5%85%8B%E4%BA%92%E4%BF%A1%E6%81%AF)
[从全局到局部](https://kexue.fm/archives/6024#%E4%BB%8E%E5%85%A8%E5%B1%80%E5%88%B0%E5%B1%80%E9%83%A8)
[batch内打乱](https://kexue.fm/archives/6024#batch%E5%86%85%E6%89%93%E4%B9%B1)
[局部互信息](https://kexue.fm/archives/6024#%E5%B1%80%E9%83%A8%E4%BA%92%E4%BF%A1%E6%81%AF)
[其他信息](https://kexue.fm/archives/6024#%E5%85%B6%E4%BB%96%E4%BF%A1%E6%81%AF)
[类似的word2vec](https://kexue.fm/archives/6024#%E7%B1%BB%E4%BC%BC%E7%9A%84word2vec)
[开源和效果图](https://kexue.fm/archives/6024#%E5%BC%80%E6%BA%90%E5%92%8C%E6%95%88%E6%9E%9C%E5%9B%BE)
[参考代码](https://kexue.fm/archives/6024#%E5%8F%82%E8%80%83%E4%BB%A3%E7%A0%81)
[来，上图片](https://kexue.fm/archives/6024#%E6%9D%A5%EF%BC%8C%E4%B8%8A%E5%9B%BE%E7%89%87)
[cifar10](https://kexue.fm/archives/6024#cifar10)
[Tiny Imagenet](https://kexue.fm/archives/6024#Tiny%20Imagenet)
[全局 vs 局部](https://kexue.fm/archives/6024#%E5%85%A8%E5%B1%80%20vs%20%E5%B1%80%E9%83%A8)
[又到终点站](https://kexue.fm/archives/6024#%E5%8F%88%E5%88%B0%E7%BB%88%E7%82%B9%E7%AB%99)

### 智能搜索

支持整句搜索！网站自动使用 [结巴分词](https://github.com/fxsjy/jieba) 进行分词，并结合ngrams排序算法给出合理的搜索结果。

### 热门标签

[生成模型](https://kexue.fm/tag/%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B/) [attention](https://kexue.fm/tag/attention/) [优化](https://kexue.fm/tag/%E4%BC%98%E5%8C%96/) [语言模型](https://kexue.fm/tag/%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/) [模型](https://kexue.fm/tag/%E6%A8%A1%E5%9E%8B/) [网站](https://kexue.fm/tag/%E7%BD%91%E7%AB%99/) [概率](https://kexue.fm/tag/%E6%A6%82%E7%8E%87/) [梯度](https://kexue.fm/tag/%E6%A2%AF%E5%BA%A6/) [转载](https://kexue.fm/tag/%E8%BD%AC%E8%BD%BD/) [微分方程](https://kexue.fm/tag/%E5%BE%AE%E5%88%86%E6%96%B9%E7%A8%8B/) [矩阵](https://kexue.fm/tag/%E7%9F%A9%E9%98%B5/) [天象](https://kexue.fm/tag/%E5%A4%A9%E8%B1%A1/) [分析](https://kexue.fm/tag/%E5%88%86%E6%9E%90/) [深度学习](https://kexue.fm/tag/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/) [积分](https://kexue.fm/tag/%E7%A7%AF%E5%88%86/) [python](https://kexue.fm/tag/python/) [优化器](https://kexue.fm/tag/%E4%BC%98%E5%8C%96%E5%99%A8/) [力学](https://kexue.fm/tag/%E5%8A%9B%E5%AD%A6/) [无监督](https://kexue.fm/tag/%E6%97%A0%E7%9B%91%E7%9D%A3/) [扩散](https://kexue.fm/tag/%E6%89%A9%E6%95%A3/) [几何](https://kexue.fm/tag/%E5%87%A0%E4%BD%95/) [节日](https://kexue.fm/tag/%E8%8A%82%E6%97%A5/) [生活](https://kexue.fm/tag/%E7%94%9F%E6%B4%BB/) [文本生成](https://kexue.fm/tag/%E6%96%87%E6%9C%AC%E7%94%9F%E6%88%90/) [数论](https://kexue.fm/tag/%E6%95%B0%E8%AE%BA/)

### 随机文章

- [对称多项式不等式的“物理证明”](https://kexue.fm/archives/1460)
- [NBCE：使用朴素贝叶斯扩展LLM的Context处理长度](https://kexue.fm/archives/9617)
- [从费马大定理谈起（九）：n=3](https://kexue.fm/archives/2910)
- [集训结束了——入选了IOAA](https://kexue.fm/archives/696)
- [第100篇文章了](https://kexue.fm/archives/103)
- [祝大家马年快乐！](https://kexue.fm/archives/2339)
- [矩阵描述三维空间旋转](https://kexue.fm/archives/2224)
- [两名美国经济学家同获2009年诺贝尔经济学奖](https://kexue.fm/archives/190)
- [今天把Blog升级了](https://kexue.fm/archives/21)
- [期待上学，期待九月](https://kexue.fm/archives/919)

### 最近评论

- [PengchengMa](https://kexue.fm/archives/10996/comment-page-1#comment-27811): 牛啊
- [xczh](https://kexue.fm/archives/10958/comment-page-1#comment-27810): 已使用mean flow policy，一步推理效果确实惊人，性能跟多步推理的diffusio...
- [Cosine](https://kexue.fm/archives/10945/comment-page-1#comment-27809): 是不是因为shared experts每次都激活，而routed experts是依概率被选中...
- [rpsun](https://kexue.fm/archives/10699/comment-page-1#comment-27808): 这样似乎与传统的经验正交函数之类的有相似之处。把样本的平均值减掉之后做正交分解。那么如果单纯地...
- [贵阳机场接机](https://kexue.fm/archives/1490/comment-page-1#comment-27807): 怎么不更新啦
- [czvzb](https://kexue.fm/archives/10958/comment-page-1#comment-27806): 具身智能模型目前主流也是在使用扩散和流匹配这类方法来预测动作。
苏神推荐你看这几篇文章：
1....
- [Shawn\_yang](https://kexue.fm/archives/10945/comment-page-1#comment-27802): 苏神，关于您所说的：“推理阶段可以事先预估Routed Expert的实际分布，只要细致地进行...
- [OceanYU](https://kexue.fm/archives/9164/comment-page-4#comment-27801): 您好，关于由式（7）推导出高斯分布，我这里有一点问题，式（7）只能保证关于x\_t-1是二次函数...
- [jorjiang](https://kexue.fm/archives/10907/comment-page-2#comment-27800): 训练和prefill这个compute-bound阶段不做矩阵吸收，这个用我这个解释更好理解了...
- [amy](https://kexue.fm/archives/10907/comment-page-2#comment-27799): 苏老师，您有关注傅里叶旋转位置编码这篇工作吗，想知道您对这篇工作的看法是什么，这篇工作可以wo...

### 友情链接

- [Cool Papers](https://papers.cool)
- [数学研发](https://bbs.emath.ac.cn)
- [Seatop](http://www.seatop.com.cn/)
- [Xiaoxia](https://xiaoxia.org/)
- [积分表-网络版](https://kexue.fm/sci/integral/index.html)
- [丝路博傲](http://blog.dvxj.com/)
- [ph4ntasy 饭特稀](http://www.ph4ntasy.com/)
- [数学之家](http://www.2math.cn/)
- [有趣天文奇观](http://interesting-sky.china-vo.org/)
- [TwistedW](http://www.twistedwg.com/)
- [godweiyang](https://godweiyang.com/)
- [AI柠檬](https://blog.ailemon.net/)
- [王登科-DK博客](https://greatdk.com)
- [ESON](https://blog.eson.org/)
- [枫之羽](https://fzhiy.net/)
- [Mathor's blog](https://wmathor.com/)
- [coding-zuo](https://coding-zuo.github.io/)
- [博科园](https://www.bokeyuan.net/)
- [孔皮皮的博客](https://www.kppkkp.top/)
- [运鹏的博客](https://yunpengtai.top/)
- [jiming.site](https://jiming.site/)
- [OmegaXYZ](https://www.omegaxyz.com/)
- [Blog by Eacls](https://www.eacls.top/)
- [EAI猩球](https://www.robotech.ink/)
- [文举的博客](https://liwenju0.com/)
- [用代码打点酱油](https://bruceyuan.com/)
- [申请链接](https://kexue.fm/links.html)

本站采用创作共用版权协议，要求署名、非商业用途和保持一致。转载本站内容必须也遵循“ [署名-非商业用途-保持一致](http://creativecommons.org/licenses/by-nc-nd/2.5/cn/)”的创作共用协议。
© 2009-2025 Scientific Spaces. All rights reserved. Theme by [laogui](http://www.laogui.com). Powered by [Typecho](http://typecho.org). 备案号: [粤ICP备09093259号-1/2](https://beian.miit.gov.cn/)。