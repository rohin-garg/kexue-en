## SEARCH

## MENU

- [打赏](https://kexue.fm/reward.html)
- [公式](https://kexue.fm/latex.html)
- [天象](https://kexue.fm/ac.html)
- [链接](https://kexue.fm/links.html)
- [时光](https://kexue.fm/me.html)
- [博览](https://kexue.fm/science.html)
- [归档](https://kexue.fm/content.html)

## CATEGORIES

- [千奇百怪](https://kexue.fm/category/Everything)
- [天文探索](https://kexue.fm/category/Astronomy)
- [数学研究](https://kexue.fm/category/Mathematics)
- [物理化学](https://kexue.fm/category/Phy-chem)
- [信息时代](https://kexue.fm/category/Big-Data)
- [生物自然](https://kexue.fm/category/Biology)
- [图片摄影](https://kexue.fm/category/Photograph)
- [问题百科](https://kexue.fm/category/Questions)
- [生活/情感](https://kexue.fm/category/Life-Feeling)
- [资源共享](https://kexue.fm/category/Resources)

## NEWPOSTS

- [msign算子的Newton-Sc...](https://kexue.fm/archives/10996)
- [从无穷范数求导到等值振荡定理](https://kexue.fm/archives/10972)
- [生成扩散模型漫谈（三十）：从瞬时速...](https://kexue.fm/archives/10958)
- [MoE环游记：5、均匀分布的反思](https://kexue.fm/archives/10945)
- [msign算子的Newton-Sc...](https://kexue.fm/archives/10922)
- [Transformer升级之路：2...](https://kexue.fm/archives/10907)
- [一道概率不等式：盯着它到显然成立为止！](https://kexue.fm/archives/10902)
- [SVD的导数](https://kexue.fm/archives/10878)
- [智能家居之手搓一套能接入米家的零冷水装置](https://kexue.fm/archives/10869)
- [Transformer升级之路：1...](https://kexue.fm/archives/10862)

## COMMENTS

- [PengchengMa: 牛啊](https://kexue.fm/archives/10996/comment-page-1#comment-27811)
- [xczh: 已使用mean flow policy，一步推理效果确实惊人，...](https://kexue.fm/archives/10958/comment-page-1#comment-27810)
- [Cosine: 是不是因为shared experts每次都激活，而route...](https://kexue.fm/archives/10945/comment-page-1#comment-27809)
- [rpsun: 这样似乎与传统的经验正交函数之类的有相似之处。把样本的平均值减...](https://kexue.fm/archives/10699/comment-page-1#comment-27808)
- [贵阳机场接机: 怎么不更新啦](https://kexue.fm/archives/1490/comment-page-1#comment-27807)
- [czvzb: 具身智能模型目前主流也是在使用扩散和流匹配这类方法来预测动作。...](https://kexue.fm/archives/10958/comment-page-1#comment-27806)
- [Shawn\_yang: 苏神，关于您所说的：“推理阶段可以事先预估Routed Exp...](https://kexue.fm/archives/10945/comment-page-1#comment-27802)
- [OceanYU: 您好，关于由式（7）推导出高斯分布，我这里有一点问题，式（7）...](https://kexue.fm/archives/9164/comment-page-4#comment-27801)
- [jorjiang: 训练和prefill这个compute-bound阶段不做矩阵...](https://kexue.fm/archives/10907/comment-page-2#comment-27800)
- [amy: 苏老师，您有关注傅里叶旋转位置编码这篇工作吗，想知道您对这篇工...](https://kexue.fm/archives/10907/comment-page-2#comment-27799)

## USERLOGIN

- [登录](https://kexue.fm/admin/login.php)

[科学空间\|Scientific Spaces](https://kexue.fm)

- [登录](https://kexue.fm/admin/login.php)
- [打赏](https://kexue.fm/reward.html)
- [公式](https://kexue.fm/latex.html)
- [天象](https://kexue.fm/ac.html)
- [链接](https://kexue.fm/links.html)
- [时光](https://kexue.fm/me.html)
- [博览](https://kexue.fm/science.html)
- [归档](https://kexue.fm/content.html)

渴望成为一个小飞侠

- [欢迎订阅](https://kexue.fm/feed)
- [个性邮箱](https://kexue.fm/archives/119)
- [天象信息](https://kexue.fm/ac.html)
- [观测ISS](https://kexue.fm/archives/41)
- [LaTeX](https://kexue.fm/latex.html)
- [关于博主](https://kexue.fm/me.html)

欢迎访问“科学空间”，这里将与您共同探讨自然科学，回味人生百态；也期待大家的分享～

- [**千奇百怪** Everything](https://kexue.fm/category/Everything)
- [**天文探索** Astronomy](https://kexue.fm/category/Astronomy)
- [**数学研究** Mathematics](https://kexue.fm/category/Mathematics)
- [**物理化学** Phy-chem](https://kexue.fm/category/Phy-chem)
- [**信息时代** Big-Data](https://kexue.fm/category/Big-Data)
- [**生物自然** Biology](https://kexue.fm/category/Biology)
- [**图片摄影** Photograph](https://kexue.fm/category/Photograph)
- [**问题百科** Questions](https://kexue.fm/category/Questions)
- [**生活/情感** Life-Feeling](https://kexue.fm/category/Life-Feeling)
- [**资源共享** Resources](https://kexue.fm/category/Resources)

- [**千奇百怪**](https://kexue.fm/category/Everything)
- [**天文探索**](https://kexue.fm/category/Astronomy)
- [**数学研究**](https://kexue.fm/category/Mathematics)
- [**物理化学**](https://kexue.fm/category/Phy-chem)
- [**信息时代**](https://kexue.fm/category/Big-Data)
- [**生物自然**](https://kexue.fm/category/Biology)
- [**图片摄影**](https://kexue.fm/category/Photograph)
- [**问题百科**](https://kexue.fm/category/Questions)
- [**生活/情感**](https://kexue.fm/category/Life-Feeling)
- [**资源共享**](https://kexue.fm/category/Resources)

[首页](https://kexue.fm) [信息时代](https://kexue.fm/category/Big-Data) 记录一次半监督的情感分析

4May

# [记录一次半监督的情感分析](https://kexue.fm/archives/4374)

By 苏剑林 \|
2017-05-04 \|
59009位读者\|

本文是一次不怎么成功的半监督学习的尝试：在IMDB的数据集上，用随机抽取的1000个标注样本训练一个文本情感分类模型，并且在余下的49000个测试样本中，测试准确率为73.48%。

## 思路 [\#](https://kexue.fm/archives/4374\#%E6%80%9D%E8%B7%AF)

本文的思路来源于OpenAI的这篇文章：
[《OpenAI新研究发现无监督情感神经元：可直接调控生成文本的情感》](http://jiqizhixin.com/article/2612?from=singlemessage)

文章里边介绍了一种无监督（实际上是半监督）做情感分类的模型的方法，并且实验效果很好。然而文章里边的实验很庞大，对于个人来说几乎不可能重现（在4块Pascal GPU花了1个月时间训练）。不过，文章里边的思想是很简单的，根据里边的思想，我们可以做个“山寨版”的。思路如下：

我们一般用深度学习做情感分类，比较常规的思路就是Embedding层+LSTM层+Dense层(Sigmoid激活)，我们常说的词向量，相当于预训练了Embedding层（这一层的参数量最大，最容易过拟合），而OpenAI的思想就是，为啥不连LSTM层一并预训练了呢？预训练的方法也是用语言模型来训练。当然，为了使得预训练的结果不至于丢失情感信息，LSTM的隐藏层节点要大一些。

如果连LSTM层预训练了，那么剩下的Dense层参数就不多了，因此可以用少量标注样本就能够训练完备了。这就是整个半监督学习的思路了。至少OpenAI文章说的什么情感神经元，那不过是形象的描述罢了。

当然，从情感分析这个任务上来看，本文的73.48%准确率实在难登大雅之台，随便一个“词典+规则”的方案，都有80%以上的准确率。我只是验证了这种实验方案的可行性。我相信，如果规模能做到OpenAI那么大，效果应该会更好的。 **而且，本文想描述的是一种建模策略，并非局限于情感分析，同样的思想可以用于任意的二分类甚至多分类问题**。

## 过程 [\#](https://kexue.fm/archives/4374\#%E8%BF%87%E7%A8%8B)

首先，加载数据集并且重新划分训练集和测试集：

```
from keras.preprocessing import sequence
from keras.models import Model
from keras.layers import Input, Embedding, LSTM, Dense, Dropout
from keras.datasets import imdb
from keras import backend as K
import numpy as np

max_features = 10000 #保留前max_features个词
maxlen = 100 #填充/阶段到100词
batch_size = 1000
nb_grams = 10 #训练一个10-gram的语言模型
nb_train = 1000 #训练样本数

#加载内置的IMDB数据集
(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)
x_lm_ = np.append(x_train, x_test)

#构造用来训练语言模型的数据
#这里只用了已有数据，实际环境中，可以补充其他数据使得训练更加充分
x_lm = []
y_lm = []
for x in x_lm_:
 for i in range(len(x)):
 x_lm.append([0]*(nb_grams - i + max(0,i-nb_grams))+x[max(0,i-nb_grams):i])
 y_lm.append([x[i]])

x_lm = np.array(x_lm)
y_lm = np.array(y_lm)
x_train = sequence.pad_sequences(x_train, maxlen=maxlen)
x_test = sequence.pad_sequences(x_test, maxlen=maxlen)
x = np.vstack([x_train, x_test])
y = np.hstack([y_train, y_test])

#重新划分训练集和测试集
#合并原来的训练集和测试集，随机挑选1000个样本，作为新的训练集，剩下为测试集
idx = range(len(x))
np.random.shuffle(idx)
x_train = x[idx[:nb_train]]
y_train = y[idx[:nb_train]]
x_test = x[idx[nb_train:]]
y_test = y[idx[nb_train:]]
```

然后搭建模型

```
embedded_size = 100 #词向量维度
hidden_size = 1000 #LSTM的维度，可以理解为编码后的句向量维度。

#encoder部分
inputs = Input(shape=(None,), dtype='int32')
embedded = Embedding(max_features, embedded_size)(inputs)
lstm = LSTM(hidden_size)(embedded)
encoder = Model(inputs=inputs, outputs=lstm)

#完全用ngram模型训练encode部分
input_grams = Input(shape=(nb_grams,), dtype='int32')
encoded_grams = encoder(input_grams)
softmax = Dense(max_features, activation='softmax')(encoded_grams)
lm = Model(inputs=input_grams, outputs=softmax)
lm.compile(loss='sparse_categorical_crossentropy', optimizer='adam')
#用sparse交叉熵，可以不用事先将类别转换为one hot形式。

#情感分析部分
#固定encoder，后面接一个简单的Dense层（相当于逻辑回归）
#这时候训练的只有hidden_size+1=1001个参数
#因此理论上来说，少量标注样本就可以训练充分
for layer in encoder.layers:
 layer.trainable=False

sentence = Input(shape=(maxlen,), dtype='int32')
encoded_sentence = encoder(sentence)
sigmoid = Dense(10, activation='relu')(encoded_sentence)
sigmoid = Dropout(0.5)(sigmoid)
sigmoid = Dense(1, activation='sigmoid')(sigmoid)
model = Model(inputs=sentence, outputs=sigmoid)
model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])
```

好了，训练语言模型，这部分工作比较耗时：
#训练语言模型，比较耗时，一般迭代两三次就好

```
lm.fit(x_lm, y_lm,
 batch_size=batch_size,
 epochs=3)
```

语言模型的训练结果为

> Epoch 1/3
> 11737946/11737946 \[==============================\] - 2400s - loss: 5.0376
> Epoch 2/3
> 11737946/11737946 \[==============================\] - 2404s - loss: 4.5587
> Epoch 3/3
> 11737946/11737946 \[==============================\] - 2404s - loss: 4.3968

接着，开始用1000个样本训练情感分析模型。由于前面已经预训练好，因此这里需要训练的参数不多，配合Dropout的话，因此1000个样本也不会导致严重过拟合。
#训练情感分析模型

```
model.fit(x_train, y_train,
 batch_size=batch_size,
 epochs=200)
```

训练结果为

> Epoch 198/200
> 1000/1000 \[==============================\] - 0s - loss: 0.2481 - acc: 0.9250
> Epoch 199/200
> 1000/1000 \[==============================\] - 0s - loss: 0.2376 - acc: 0.9330
> Epoch 200/200
> 1000/1000 \[==============================\] - 0s - loss: 0.2386 - acc: 0.9350

接着来评估一下模型：

```
#评估一下模型的效果
model.evaluate(x_test, y_test, verbose=True, batch_size=batch_size)
```

准确率73.04％，一般般～～试试迁移学习，把训练集连同测试集的预测结果一起进行训练：

```
#把训练集连同测试集的预测结果（即可能包含有误的数据），重新训练模型
y_pred = model.predict(x_test, verbose=True, batch_size=batch_size)
y_pred = (y_pred.reshape(-1) > 0.5).astype(int)
xt = np.vstack([x_train, x_test])
yt = np.hstack([y_train, y_pred])

model.fit(xt, yt,
 batch_size=batch_size,
 epochs=10)

#评估一下模型的效果
model.evaluate(x_test, y_test, verbose=True, batch_size=batch_size)
```

训练结果为

> Epoch 8/10
> 50000/50000 \[==============================\] - 27s - loss: 0.1455 - acc: 0.9561
> Epoch 9/10
> 50000/50000 \[==============================\] - 27s - loss: 0.1390 - acc: 0.9590
> Epoch 10/10
> 50000/50000 \[==============================\] - 27s - loss: 0.1349 - acc: 0.9600

这次我们得到了73.33%的准确率。不难发现，事实上这个过程可以重复迭代，再重复一次，得到73.33%的准确率，重复第二次，得到73.47%...可以预料，这会趋于一个稳定值，我再重复了5次，稳定在73.48%。

从刚开始的73.04％，到迁移学习后的73.48%，约有0.44%的提升，看上去不大，但是如果对于做比赛或者写论文的同学来说，0.44%的提升，可以小书一笔了～

## 点评 [\#](https://kexue.fm/archives/4374\#%E7%82%B9%E8%AF%84)

文章开头已经说了，这次是个不大成功的尝试，毕竟是“山寨版”的，因此大家不要太纠结准确率不高的问题。而从本文的实验结果来看，这种方案是靠谱的。通过大量的、混合情感的语料训练语言模型，确实能够很好地提取文本的特征，这类似于图像的自编码过程。

本文做得很简单，没有细微调过超参。改进的思路大概有那么几个：增大语言模型的规模、增加情感语料（只需要是情感评论的，不需要标签）、训练细节上的优化。这个就暂时不做了～

_**转载到请包括本文地址：** [https://kexue.fm/archives/4374](https://kexue.fm/archives/4374)_

_**更详细的转载事宜请参考：**_ [《科学空间FAQ》](https://kexue.fm/archives/6508#%E6%96%87%E7%AB%A0%E5%A6%82%E4%BD%95%E8%BD%AC%E8%BD%BD/%E5%BC%95%E7%94%A8)

**如果您还有什么疑惑或建议，欢迎在下方评论区继续讨论。**

**如果您觉得本文还不错，欢迎 [分享](https://kexue.fm/archives/4374#share)/ [打赏](https://kexue.fm/archives/4374#pay) 本文。打赏并非要从中获得收益，而是希望知道科学空间获得了多少读者的真心关注。当然，如果你无视它，也不会影响你的阅读。再次表示欢迎和感谢！**

打赏

微信打赏

支付宝打赏

因为网站后台对打赏并无记录，因此欢迎在打赏时候备注留言。你还可以 [**点击这里**](http://mail.qq.com/cgi-bin/qm_share?t=qm_mailme&email=tN7d1drY3drrx8H0xcWa19vZ) 或在下方评论区留言来告知你的建议或需求。

**如果您需要引用本文，请参考：**

苏剑林. (May. 04, 2017). 《记录一次半监督的情感分析 》\[Blog post\]. Retrieved from [https://kexue.fm/archives/4374](https://kexue.fm/archives/4374)

@online{kexuefm-4374,
        title={记录一次半监督的情感分析},
        author={苏剑林},
        year={2017},
        month={May},
        url={\\url{https://kexue.fm/archives/4374}},
}

分类： [信息时代](https://kexue.fm/category/Big-Data)    标签： [深度学习](https://kexue.fm/tag/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/), [文本挖掘](https://kexue.fm/tag/%E6%96%87%E6%9C%AC%E6%8C%96%E6%8E%98/)[6 评论](https://kexue.fm/archives/4374#comments)

< [【不可思议的Word2Vec】 4.不一样的“相似”](https://kexue.fm/archives/4368) \| [如何“扒”站？手把手教你爬百度百科～](https://kexue.fm/archives/4385) >

### 你也许还对下面的内容感兴趣

- [为什么需要残差？一个来自DeepNet的视角](https://kexue.fm/archives/8994)
- [多任务学习漫谈（三）：分主次之序](https://kexue.fm/archives/8907)
- [多任务学习漫谈（二）：行梯度之事](https://kexue.fm/archives/8896)
- [多任务学习漫谈（一）：以损失之名](https://kexue.fm/archives/8870)
- [也来谈谈RNN的梯度消失/爆炸问题](https://kexue.fm/archives/7888)
- [L2正则没有想象那么好？可能是“权重尺度偏移”惹的祸](https://kexue.fm/archives/7681)
- [我们真的需要把训练集的损失降低到零吗？](https://kexue.fm/archives/7643)
- [节省显存的重计算技巧也有了Keras版了](https://kexue.fm/archives/7367)
- [突破瓶颈，打造更强大的Transformer](https://kexue.fm/archives/7325)
- [ON-LSTM：用有序神经元表达层次结构](https://kexue.fm/archives/6621)

[发表你的看法](https://kexue.fm/archives/4374#comment_form)

jljde123

January 3rd, 2018

那个准确率是怎么计算的

[回复评论](https://kexue.fm/archives/4374/comment-page-1?replyTo=8513#respond-post-4374)

[苏剑林](http://kexue.fm) 发表于
January 4th, 2018

就是普通的准确率，预测正确的样本数除以总样本数。预测大于0.5就视为正样本，否则负样本

[回复评论](https://kexue.fm/archives/4374/comment-page-1?replyTo=8519#respond-post-4374)

jljde123

January 8th, 2018

是loss小于0.5的就是正样本吗？最近在学习深度学习的文本情感分析，对这些不是很清楚。

[回复评论](https://kexue.fm/archives/4374/comment-page-1?replyTo=8534#respond-post-4374)

kk 发表于
January 30th, 2018

y\_pred = model.predict(x\_test, verbose=True, batch\_size=batch\_size)
y\_pred = (y\_pred.reshape(-1) > 0.5).astype(int)

model.predict输出的就是概率值（小数），(y\_pred.reshape(-1) > 0.5).astype(int)就是对它进行整数化处理，大于0.5就视为正样本，否则负样本

[回复评论](https://kexue.fm/archives/4374/comment-page-1?replyTo=8628#respond-post-4374)

kk 发表于
January 30th, 2018

其实你把代码调试成功跑通了，自然就知道输出是怎样的啦~(◎\_◎;)

[回复评论](https://kexue.fm/archives/4374/comment-page-1?replyTo=8629#respond-post-4374)

ada

June 15th, 2018

训练语言模型的意义何在呢？
并且训练是用前10个单词预测第11个单词，LSTM的输入就代表了描述前10个单词的一个向量。
为什么不是输入不是所有的单词呢？即w1，w2,......wn预测w2,w3,.....wn? 更符合情感分类的输入呢？
LSTM输出的这个向量能适用于各种场景？如情感分类，文本分类等等，LSTM输入的向量是不是比其它论文提取的各种网络结构提取的句子向量效果弱一些？只是在这种场景下（标注数据少）而采取的一种折中办法？ 先用语言模型这个普适办法提取句子向量，然后就可以对接各种场景了？

[回复评论](https://kexue.fm/archives/4374/comment-page-1?replyTo=9344#respond-post-4374)

[取消回复](https://kexue.fm/archives/4374#respond-post-4374)

你的大名

电子邮箱

个人网站（选填）

1\. 可以使用LaTeX代码，点击“预览效果”可查看效果；2. 可以通过点击评论楼层编号来引用该楼层；3. 网站可能会有点卡，如非确认评论失败，请不要重复点击提交。

### 内容速览

[思路](https://kexue.fm/archives/4374#%E6%80%9D%E8%B7%AF)
[过程](https://kexue.fm/archives/4374#%E8%BF%87%E7%A8%8B)
[点评](https://kexue.fm/archives/4374#%E7%82%B9%E8%AF%84)

### 智能搜索

支持整句搜索！网站自动使用 [结巴分词](https://github.com/fxsjy/jieba) 进行分词，并结合ngrams排序算法给出合理的搜索结果。

### 热门标签

[生成模型](https://kexue.fm/tag/%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B/) [attention](https://kexue.fm/tag/attention/) [优化](https://kexue.fm/tag/%E4%BC%98%E5%8C%96/) [语言模型](https://kexue.fm/tag/%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/) [模型](https://kexue.fm/tag/%E6%A8%A1%E5%9E%8B/) [网站](https://kexue.fm/tag/%E7%BD%91%E7%AB%99/) [概率](https://kexue.fm/tag/%E6%A6%82%E7%8E%87/) [梯度](https://kexue.fm/tag/%E6%A2%AF%E5%BA%A6/) [转载](https://kexue.fm/tag/%E8%BD%AC%E8%BD%BD/) [微分方程](https://kexue.fm/tag/%E5%BE%AE%E5%88%86%E6%96%B9%E7%A8%8B/) [天象](https://kexue.fm/tag/%E5%A4%A9%E8%B1%A1/) [矩阵](https://kexue.fm/tag/%E7%9F%A9%E9%98%B5/) [分析](https://kexue.fm/tag/%E5%88%86%E6%9E%90/) [深度学习](https://kexue.fm/tag/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/) [积分](https://kexue.fm/tag/%E7%A7%AF%E5%88%86/) [python](https://kexue.fm/tag/python/) [优化器](https://kexue.fm/tag/%E4%BC%98%E5%8C%96%E5%99%A8/) [力学](https://kexue.fm/tag/%E5%8A%9B%E5%AD%A6/) [无监督](https://kexue.fm/tag/%E6%97%A0%E7%9B%91%E7%9D%A3/) [扩散](https://kexue.fm/tag/%E6%89%A9%E6%95%A3/) [几何](https://kexue.fm/tag/%E5%87%A0%E4%BD%95/) [节日](https://kexue.fm/tag/%E8%8A%82%E6%97%A5/) [生活](https://kexue.fm/tag/%E7%94%9F%E6%B4%BB/) [文本生成](https://kexue.fm/tag/%E6%96%87%E6%9C%AC%E7%94%9F%E6%88%90/) [数论](https://kexue.fm/tag/%E6%95%B0%E8%AE%BA/)

### 随机文章

- [OCR技术浅探：2. 背景与假设](https://kexue.fm/archives/3781)
- [层次分解位置编码，让BERT可以处理超长文本](https://kexue.fm/archives/7947)
- [数学基本技艺之23、24（上）](https://kexue.fm/archives/2083)
- [2013年全年天象](https://kexue.fm/archives/2249)
- [这样的世界之最你见过没有？](https://kexue.fm/archives/36)
- [轻微的扰动——摄动法简介(1)](https://kexue.fm/archives/1878)
- [OCR技术浅探：7. 语言模型](https://kexue.fm/archives/3842)
- [注意力和Softmax的两点有趣发现：鲁棒性和信息量](https://kexue.fm/archives/9593)
- [从马尔科夫过程到主方程（推导过程）](https://kexue.fm/archives/4598)
- [【NASA每日一图】黎明天空中的奋进号太空梭](https://kexue.fm/archives/486)

### 最近评论

- [PengchengMa](https://kexue.fm/archives/10996/comment-page-1#comment-27811): 牛啊
- [xczh](https://kexue.fm/archives/10958/comment-page-1#comment-27810): 已使用mean flow policy，一步推理效果确实惊人，性能跟多步推理的diffusio...
- [Cosine](https://kexue.fm/archives/10945/comment-page-1#comment-27809): 是不是因为shared experts每次都激活，而routed experts是依概率被选中...
- [rpsun](https://kexue.fm/archives/10699/comment-page-1#comment-27808): 这样似乎与传统的经验正交函数之类的有相似之处。把样本的平均值减掉之后做正交分解。那么如果单纯地...
- [贵阳机场接机](https://kexue.fm/archives/1490/comment-page-1#comment-27807): 怎么不更新啦
- [czvzb](https://kexue.fm/archives/10958/comment-page-1#comment-27806): 具身智能模型目前主流也是在使用扩散和流匹配这类方法来预测动作。
苏神推荐你看这几篇文章：
1....
- [Shawn\_yang](https://kexue.fm/archives/10945/comment-page-1#comment-27802): 苏神，关于您所说的：“推理阶段可以事先预估Routed Expert的实际分布，只要细致地进行...
- [OceanYU](https://kexue.fm/archives/9164/comment-page-4#comment-27801): 您好，关于由式（7）推导出高斯分布，我这里有一点问题，式（7）只能保证关于x\_t-1是二次函数...
- [jorjiang](https://kexue.fm/archives/10907/comment-page-2#comment-27800): 训练和prefill这个compute-bound阶段不做矩阵吸收，这个用我这个解释更好理解了...
- [amy](https://kexue.fm/archives/10907/comment-page-2#comment-27799): 苏老师，您有关注傅里叶旋转位置编码这篇工作吗，想知道您对这篇工作的看法是什么，这篇工作可以wo...

### 友情链接

- [Cool Papers](https://papers.cool)
- [数学研发](https://bbs.emath.ac.cn)
- [Seatop](http://www.seatop.com.cn/)
- [Xiaoxia](https://xiaoxia.org/)
- [积分表-网络版](https://kexue.fm/sci/integral/index.html)
- [丝路博傲](http://blog.dvxj.com/)
- [ph4ntasy 饭特稀](http://www.ph4ntasy.com/)
- [数学之家](http://www.2math.cn/)
- [有趣天文奇观](http://interesting-sky.china-vo.org/)
- [TwistedW](http://www.twistedwg.com/)
- [godweiyang](https://godweiyang.com/)
- [AI柠檬](https://blog.ailemon.net/)
- [王登科-DK博客](https://greatdk.com)
- [ESON](https://blog.eson.org/)
- [枫之羽](https://fzhiy.net/)
- [Mathor's blog](https://wmathor.com/)
- [coding-zuo](https://coding-zuo.github.io/)
- [博科园](https://www.bokeyuan.net/)
- [孔皮皮的博客](https://www.kppkkp.top/)
- [运鹏的博客](https://yunpengtai.top/)
- [jiming.site](https://jiming.site/)
- [OmegaXYZ](https://www.omegaxyz.com/)
- [Blog by Eacls](https://www.eacls.top/)
- [EAI猩球](https://www.robotech.ink/)
- [文举的博客](https://liwenju0.com/)
- [用代码打点酱油](https://bruceyuan.com/)
- [申请链接](https://kexue.fm/links.html)

本站采用创作共用版权协议，要求署名、非商业用途和保持一致。转载本站内容必须也遵循“ [署名-非商业用途-保持一致](http://creativecommons.org/licenses/by-nc-nd/2.5/cn/)”的创作共用协议。
© 2009-2025 Scientific Spaces. All rights reserved. Theme by [laogui](http://www.laogui.com). Powered by [Typecho](http://typecho.org). 备案号: [粤ICP备09093259号-1/2](https://beian.miit.gov.cn/)。