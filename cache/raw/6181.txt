## SEARCH

## MENU

- [打赏](https://kexue.fm/reward.html)
- [公式](https://kexue.fm/latex.html)
- [天象](https://kexue.fm/ac.html)
- [链接](https://kexue.fm/links.html)
- [时光](https://kexue.fm/me.html)
- [博览](https://kexue.fm/science.html)
- [归档](https://kexue.fm/content.html)

## CATEGORIES

- [千奇百怪](https://kexue.fm/category/Everything)
- [天文探索](https://kexue.fm/category/Astronomy)
- [数学研究](https://kexue.fm/category/Mathematics)
- [物理化学](https://kexue.fm/category/Phy-chem)
- [信息时代](https://kexue.fm/category/Big-Data)
- [生物自然](https://kexue.fm/category/Biology)
- [图片摄影](https://kexue.fm/category/Photograph)
- [问题百科](https://kexue.fm/category/Questions)
- [生活/情感](https://kexue.fm/category/Life-Feeling)
- [资源共享](https://kexue.fm/category/Resources)

## NEWPOSTS

- [线性注意力简史：从模仿、创新到反哺](https://kexue.fm/archives/11033)
- [msign的导数](https://kexue.fm/archives/11025)
- [通过msign来计算mclip（奇...](https://kexue.fm/archives/11006)
- [msign算子的Newton-Sc...](https://kexue.fm/archives/10996)
- [等值振荡定理：最优多项式逼近的充要条件](https://kexue.fm/archives/10972)
- [生成扩散模型漫谈（三十）：从瞬时速...](https://kexue.fm/archives/10958)
- [MoE环游记：5、均匀分布的反思](https://kexue.fm/archives/10945)
- [msign算子的Newton-Sc...](https://kexue.fm/archives/10922)
- [Transformer升级之路：2...](https://kexue.fm/archives/10907)
- [一道概率不等式：盯着它到显然成立为止！](https://kexue.fm/archives/10902)

## COMMENTS

- [Skyler Lin: respect苏神！](https://kexue.fm/archives/11033/comment-page-1#comment-27949)
- [宋佳铭: 对，个人感觉mean flow就是continuous tim...](https://kexue.fm/archives/10958/comment-page-1#comment-27947)
- [宋佳铭: 的确，对sg这个事情我感觉如果是用‘归纳’法做是不太能避免的，...](https://kexue.fm/archives/10958/comment-page-1#comment-27946)
- [MoFHeka: 苏老师您好，请问一下这套结论在稀疏参数上应该如何应用？比如大规...](https://kexue.fm/archives/10542/comment-page-1#comment-27945)
- [苏剑林: Temp LoRA倒是有印象，其实思想是一样的，如果我单独开一...](https://kexue.fm/archives/11033/comment-page-1#comment-27944)
- [苏剑林: 你搜搜mamba、rwkv甚至rnn做vision的工作，其实...](https://kexue.fm/archives/11033/comment-page-1#comment-27943)
- [苏剑林: 问题1可以看看 https://kexue.fm/archiv...](https://kexue.fm/archives/9379/comment-page-1#comment-27942)
- [苏剑林: 你的“信息量”怎么定义？直观来说，reflow训练的是切线模型...](https://kexue.fm/archives/10958/comment-page-2#comment-27941)
- [苏剑林: 加大codebook也不行吗？主要是我也不大了解SDXL有什么...](https://kexue.fm/archives/10711/comment-page-2#comment-27940)
- [苏剑林: 如果考虑额外的scale，那情况肯定不一样了，甚至手工scal...](https://kexue.fm/archives/9009/comment-page-2#comment-27939)

## USERLOGIN

- [登录](https://kexue.fm/admin/login.php)

[科学空间\|Scientific Spaces](https://kexue.fm)

- [登录](https://kexue.fm/admin/login.php)
- [打赏](https://kexue.fm/reward.html)
- [公式](https://kexue.fm/latex.html)
- [天象](https://kexue.fm/ac.html)
- [链接](https://kexue.fm/links.html)
- [时光](https://kexue.fm/me.html)
- [博览](https://kexue.fm/science.html)
- [归档](https://kexue.fm/content.html)

渴望成为一个小飞侠

- [欢迎订阅](https://kexue.fm/feed)
- [个性邮箱](https://kexue.fm/archives/119)
- [天象信息](https://kexue.fm/ac.html)
- [观测ISS](https://kexue.fm/archives/41)
- [LaTeX](https://kexue.fm/latex.html)
- [关于博主](https://kexue.fm/me.html)

欢迎访问“科学空间”，这里将与您共同探讨自然科学，回味人生百态；也期待大家的分享～

- [**千奇百怪** Everything](https://kexue.fm/category/Everything)
- [**天文探索** Astronomy](https://kexue.fm/category/Astronomy)
- [**数学研究** Mathematics](https://kexue.fm/category/Mathematics)
- [**物理化学** Phy-chem](https://kexue.fm/category/Phy-chem)
- [**信息时代** Big-Data](https://kexue.fm/category/Big-Data)
- [**生物自然** Biology](https://kexue.fm/category/Biology)
- [**图片摄影** Photograph](https://kexue.fm/category/Photograph)
- [**问题百科** Questions](https://kexue.fm/category/Questions)
- [**生活/情感** Life-Feeling](https://kexue.fm/category/Life-Feeling)
- [**资源共享** Resources](https://kexue.fm/category/Resources)

- [**千奇百怪**](https://kexue.fm/category/Everything)
- [**天文探索**](https://kexue.fm/category/Astronomy)
- [**数学研究**](https://kexue.fm/category/Mathematics)
- [**物理化学**](https://kexue.fm/category/Phy-chem)
- [**信息时代**](https://kexue.fm/category/Big-Data)
- [**生物自然**](https://kexue.fm/category/Biology)
- [**图片摄影**](https://kexue.fm/category/Photograph)
- [**问题百科**](https://kexue.fm/category/Questions)
- [**生活/情感**](https://kexue.fm/category/Life-Feeling)
- [**资源共享**](https://kexue.fm/category/Resources)

[首页](https://kexue.fm) [信息时代](https://kexue.fm/category/Big-Data) 从变分编码、信息瓶颈到正态分布：论遗忘的重要性

27Nov

# [从变分编码、信息瓶颈到正态分布：论遗忘的重要性](https://kexue.fm/archives/6181)

By 苏剑林 \|
2018-11-27 \|
193353位读者\|

这是一篇“散文”，我们来谈一下有着千丝万缕联系的三个东西：变分自编码器、信息瓶颈、正态分布。

众所周知，变分自编码器是一个很经典的生成模型，但实际上它有着超越生成模型的含义；而对于信息瓶颈，大家也许相对陌生一些，然而事实上信息瓶颈在去年也热闹了一阵子；至于正态分布，那就不用说了，它几乎跟所有机器学习领域都有或多或少的联系。

那么，当它们三个碰撞在一块时，又有什么样的故事可说呢？它们跟“遗忘”又有什么关系呢？

## 变分自编码器 [\#](https://kexue.fm/archives/6181\#%E5%8F%98%E5%88%86%E8%87%AA%E7%BC%96%E7%A0%81%E5%99%A8)

在本博客你可以搜索到 [若干几篇介绍VAE的文章](https://kexue.fm/search/%E5%8F%98%E5%88%86%E8%87%AA%E7%BC%96%E7%A0%81%E5%99%A8/)。下面简单回顾一下。

### 理论形式回顾 [\#](https://kexue.fm/archives/6181\#%E7%90%86%E8%AE%BA%E5%BD%A2%E5%BC%8F%E5%9B%9E%E9%A1%BE)

简单来说，VAE的优化目标是：
\\begin{equation}KL(\\tilde{p}(x)p(z\|x)\\Vert q(z)q(x\|z))=\\iint \\tilde{p}(x)p(z\|x)\\log \\frac{\\tilde{p}(x)p(z\|x)}{q(x\|z)q(z)} dzdx\\end{equation}
其中$q(z)$是标准正态分布，$p(z\|x),q(x\|z)$是条件正态分布，分别对应编码器、解码器。具体细节可以参考 [《变分自编码器（二）：从贝叶斯观点出发》](https://kexue.fm/archives/5343)。

这个目标最终可以简化为
\\begin{equation}\\mathbb{E}\_{x\\sim \\tilde{p}(x)} \\Big\[\\mathbb{E}\_{z\\sim p(z\|x)}\\big\[-\\log q(x\|z)\\big\]+KL\\big(p(z\|x)\\big\\Vert q(z)\\big)\\Big\]\\label{eq:vae}\\end{equation}
显然，它可以分开来看：$\\mathbb{E}\_{z\\sim p(z\|x)}\\big\[-\\log q(x\|z)\\big\]$这一项相当于普通的自编码器损失（加上了 [重参数](https://kexue.fm/archives/5253#%E9%87%8D%E5%8F%82%E6%95%B0%E6%8A%80%E5%B7%A7)），$KL\\big(p(z\|x)\\big\\Vert q(z)\\big)$是后验分布与先验分布的KL散度。第一项是希望重构损失越小越好，也就是希望中间的隐变量$z$能尽可能保留更多的信息，第二项是要隐变量空间跟正态分布对齐，意思是希望隐变量的分布更加规整一些。

### 与自编码器的比较 [\#](https://kexue.fm/archives/6181\#%E4%B8%8E%E8%87%AA%E7%BC%96%E7%A0%81%E5%99%A8%E7%9A%84%E6%AF%94%E8%BE%83)

所以，相比普通的自编码器，VAE的改动就是：

> 1、引入了均值和方差的概念，加入了重参数操作；
>
> 2、加入了KL散度为额外的损失函数。

## 信息瓶颈 [\#](https://kexue.fm/archives/6181\#%E4%BF%A1%E6%81%AF%E7%93%B6%E9%A2%88)

自认为本空间介绍VAE的信息已经够多了，因此不再赘述，马上转到信息瓶颈（Information Bottleneck，IB）的介绍。

### 揭开DL的黑箱？ [\#](https://kexue.fm/archives/6181\#%E6%8F%AD%E5%BC%80DL%E7%9A%84%E9%BB%91%E7%AE%B1%EF%BC%9F)

去年九月份有一场关于深度学习与信息瓶颈的演讲，声称能解开深度学习（DL）的黑箱，然后大牛Hinton听后评价“这太有趣了，我需要再看上10000遍...”（参考 [《揭开深度学习黑箱：希伯来大学计算机科学教授提出「信息瓶颈」》](https://mp.weixin.qq.com/s/cesmzbpzX8vVqsAmYFKrMg)），然后信息瓶颈就热闹起来了。不久之后，有一篇文章来怼这个结果，表明信息瓶颈的结论不是普适的（参考 [《戳穿泡沫：对「信息瓶颈」理论的批判性分析》](https://mp.weixin.qq.com/s/vH9vl15eQz5aarhgSLMh-A)），所以就更热闹了。

不管信息瓶颈能否揭开深度学习的秘密，作为实用派，我们主要看信息瓶颈能够真的能提取出一些有价值的东西出来用。所谓信息瓶颈，是一个比较朴素的思想，它说我们面对一个任务，应用试图用最少的信息来完成。这其实跟我们之前讨论的“ [最小熵系列](https://kexue.fm/archives/5448)”是类似的，因为信息对应着学习成本，我们用最少的信息来完成一个任务，就意味着用最低的成本来完成这个任务，这意味着得到泛化性能更好的模型。

### 信息瓶颈的原理 [\#](https://kexue.fm/archives/6181\#%E4%BF%A1%E6%81%AF%E7%93%B6%E9%A2%88%E7%9A%84%E5%8E%9F%E7%90%86)

为什么更低的成本／更少的信息就能得到更好的泛化能力？这不难理解，比如在公司中，我们如果要为每个客户都定制一个解决方案，派专人去跟进，那么成本是很大的；如果我们能找出一套普适的方案来，以后只需要在这个方案基础上进行微调，那么成本就会低很多。“普适的方案”是因为我们找到了客户需求的共性和规律，所以很显然，一个成本最低的方案意味着我们能找到一些普适的规律和特性，这就意味着泛化性能。

信息瓶颈示意图

在深度学习中，我们要如何体现这一点呢？答案就是“**变分信息瓶颈**”（VIB），源于文章 [《Deep Variational Information Bottleneck》](https://papers.cool/arxiv/1612.00410)。

假设我们面对分类任务，标注数据对是$(x\_1,y\_1),\\dots,(x\_N,y\_N)$。我们把这个任务分为两步来理解，第一步是编码，第二步就是分类。第一步是把$x$编码为一个隐变量$z$，然后分类器把$z$识别为类别$y$。
$$x \\quad \\to \\quad z \\quad \\to \\quad y$$

然后我们试想在$z$处加一个“瓶颈”$\\beta$，它像一个沙漏，进入的信息量可能有很多，但是出口就只有$\\beta$那么大，所以这个瓶颈的作用是：不允许流过$z$的信息量多于$\\beta$。跟沙漏不同的是，沙漏的沙过了瓶颈就完事了，而信息过了信息瓶颈后，还需要完成它要完成的任务（分类、回归等），所以模型迫不得已，只好想办法让最重要的信息通过瓶颈。这就是信息瓶颈的原理。

### 变分信息瓶颈 [\#](https://kexue.fm/archives/6181\#%E5%8F%98%E5%88%86%E4%BF%A1%E6%81%AF%E7%93%B6%E9%A2%88)

定量上是怎么操作呢？我们用“互信息”作为指标，来度量通过的信息量：
\\begin{equation}\\iint p(z\|x)\\tilde{p}(x)\\log \\frac{p(z\|x)}{p(z)}dxdz\\end{equation}
这里的$p(z)$不是任意指定的分布，而是真实的隐变量的分布，理论上，知道$p(z\|x)$后，我们就可以将$p(z)$算出来，因为它形式上等于
\\begin{equation}p(z) = \\int p(z\|x)\\tilde{p}(x)dx\\end{equation}
当然，这个积分往往不好算，后面我们再另想办法。

然后，我们还有个任务的loss，比如分类任务通常是交叉熵：
\\begin{equation}-\\iint p(z\|x)\\tilde{p}(x)\\log p(y\|z)dxdz\\end{equation}
写成这样的形式，表明我们有个编码器先将$x$编码为$z$，然后再对$z$分类。

怎么“不允许流过$z$的信息量多于$\\beta$”呢？我们可以直接把它当作惩罚项加入，使得最终的loss为
\\begin{equation}-\\iint p(z\|x)\\tilde{p}(x)\\log p(y\|z)dxdz + \\lambda \\iint p(z\|x)\\tilde{p}(x)\\max\\left(\\log \\frac{p(z\|x)}{p(z)} - \\beta, 0\\right)dxdz\\end{equation}
也就是说，互信息大于$\\beta$之后，就会出现一个正的惩罚项。当然，很多时候我们不知道$\\beta$设为多少才好，所以一个更干脆的做法是去掉$\\beta$，得到
\\begin{equation}-\\iint p(z\|x)\\tilde{p}(x)\\log p(y\|z)dxdz + \\lambda \\iint p(z\|x)\\tilde{p}(x)\\log \\frac{p(z\|x)}{p(z)}dxdz\\end{equation}
这就单纯地希望信息量越小越好，而不设置一个特定的阈值。

现在，公式已经有了，可是我们说过$p(z)$是算不出的，我们只好估计它的一个上界：假设$q(z)$是形式已知的分布，我们有
\\begin{equation}\\begin{aligned}&\\iint p(z\|x)\\tilde{p}(x)\\log \\frac{p(z\|x)}{p(z)}dxdz\\\
=&\\iint p(z\|x)\\tilde{p}(x)\\log \\frac{p(z\|x)}{q(z)}\\frac{q(z)}{p(z)}dxdz\\\
=&\\iint p(z\|x)\\tilde{p}(x)\\log \\frac{p(z\|x)}{q(z)} + \\iint p(z\|x)\\tilde{p}(x)\\log \\frac{q(z)}{p(z)}dxdz\\\
=&\\iint p(z\|x)\\tilde{p}(x)\\log \\frac{p(z\|x)}{q(z)} + \\int p(z)\\log \\frac{q(z)}{p(z)}dz\\\
=&\\iint p(z\|x)\\tilde{p}(x)\\log \\frac{p(z\|x)}{q(z)} - \\int p(z)\\log \\frac{p(z)}{q(z)}dz\\\
=&\\int \\tilde{p}(x) KL\\big(p(z\|x)\\big\\Vert q(z)\\big) dx - KL\\big(p(z)\\big\\Vert q(z)\\big)\\\
< &\\int \\tilde{p}(x) KL\\big(p(z\|x)\\big\\Vert q(z)\\big) dx\\end{aligned}\\end{equation}
这就表明，$\\int\\tilde{p}(x) KL\\big(p(z\|x)\\big\\Vert q(z)\\big) dx$是$\\iint p(z\|x)\\tilde{p}(x)\\log \\frac{p(z\|x)}{p(z)}dxdz$的上界，如果我们优化前者，那么后者也不会超过前者。既然后者没法直接算出来，那么只好来优化前者了。所以，最终可用的loss是
\\begin{equation}-\\iint p(z\|x)\\tilde{p}(x)\\log p(y\|z)dxdz + \\lambda \\int\\tilde{p}(x) KL\\big(p(z\|x)\\big\\Vert q(z)\\big) dx\\end{equation}
或者等价地写成
\\begin{equation}\\mathbb{E}\_{x\\sim \\tilde{p}(x)} \\Big\[\\mathbb{E}\_{z\\sim p(z\|x)}\\big\[-\\log p(y\|z)\\big\]+\\lambda\\cdot KL\\big(p(z\|x)\\big\\Vert q(z)\\big)\\Big\]\\label{eq:vib}\\end{equation}
这就是“ **变分信息瓶颈**”。

### 结果观察与实现 [\#](https://kexue.fm/archives/6181\#%E7%BB%93%E6%9E%9C%E8%A7%82%E5%AF%9F%E4%B8%8E%E5%AE%9E%E7%8E%B0)

可以看到，如果$q(z)$取标准正态分布（事实上我们一直都是这样取，所以这个“如果”是满足的），$\\eqref{eq:vib}$跟VAE的损失函$\\eqref{eq:vae}$几乎是一样的。只不过$\\eqref{eq:vib}$讨论的是有监督的任务，而$\\eqref{eq:vae}$是无监督学习，但我们可以将VAE看成是标签为自身$x$的有监督学习任务，那么它就是$\\eqref{eq:vib}$的一个特例了。

所以，相比原始的监督学习任务，变分信息瓶颈的改动就是：

> 1、引入了均值和方差的概念，加入了重参数操作；
>
> 2、加入了KL散度为额外的损失函数。

跟VAE如出一辙！

在Keras中实现变分信息瓶颈的方式非常简单，我定义了一个层，方便大家调用：

```
from keras.layers import Layer
import keras.backend as K

class VIB(Layer):
 """变分信息瓶颈层
 """
 def __init__(self, lamb, **kwargs):
 self.lamb = lamb
 super(VIB, self).__init__(**kwargs)
 def call(self, inputs):
 z_mean, z_log_var = inputs
 u = K.random_normal(shape=K.shape(z_mean))
 kl_loss = - 0.5 * K.sum(K.mean(1 + z_log_var - K.square(z_mean) - K.exp(z_log_var), 0))
 self.add_loss(self.lamb * kl_loss)
 u = K.in_train_phase(u, 0.)
 return z_mean + K.exp(z_log_var / 2) * u
 def compute_output_shape(self, input_shape):
 return input_shape[0]

```

用法很简单，在原来的任务上稍做改动即可，请参考：
[https://github.com/bojone/vib/blob/master/cnn\_imdb\_vib.py](https://github.com/bojone/vib/blob/master/cnn_imdb_vib.py)

**效果：相比没有加VIB的模型，加了VIB的模型收敛更快，而且容易跑到89%+的验证正确率，而不加VIB的模型，通常只能跑到88%+的正确率，并且收敛速度更慢些。**

### 变分判别瓶颈 [\#](https://kexue.fm/archives/6181\#%E5%8F%98%E5%88%86%E5%88%A4%E5%88%AB%E7%93%B6%E9%A2%88)

原论文 [《Deep Variational Information Bottleneck》](https://papers.cool/arxiv/1612.00410) 表明，VIB是一种颇为有效的正则手段，在多个任务上都提高了原模型性能。

然而信息瓶颈的故事还没有完，就在前不久，一篇名为 [《Variational Discriminator Bottleneck》](https://papers.cool/arxiv/1810.00821) 的论文被评为ICLR 2019的高分论文（同期出现的还有那著名的的BigGAN），论文作者已经不满足仅仅将变分信息瓶颈用于普通的监督任务了，论文发展出“变分判别瓶颈”，并且一举将它用到了GAN、强化学习等多种任务上，并都取得了一定的提升！信息瓶颈的威力可见一斑。

不同于$\\eqref{eq:vib}$式，在《Variational Discriminator Bottleneck》一文信息瓶颈的更新机制做了修改，使它绝有一定的自适应能力，但根本思想没有变化，都是通过限制互信息来对模型起到正则作用。不过这已经不是本文的重点了，有兴趣的读者请阅读原论文。

## 正态分布 [\#](https://kexue.fm/archives/6181\#%E6%AD%A3%E6%80%81%E5%88%86%E5%B8%83)

通过对比，我们已经发现，VAE和VIB都只是在原来的任务上引入了重参数，并且加入了KL散度项。直观来看，正则项的作用都是希望隐变量的分布更接近标准正态分布。那么，正态分布究竟有什么好处呢？

### 规整和解耦 [\#](https://kexue.fm/archives/6181\#%E8%A7%84%E6%95%B4%E5%92%8C%E8%A7%A3%E8%80%A6)

其实要说正态分布的来源、故事、用途等等，可以写上一整本书了，很多关于正态分布的性质，在其他地方已经出现过，这里仅仅介绍一下跟本文比较相关的部分。

其实，KL散度的作用，要让隐变量的分布对齐（多元的）标准正态分布，而不是任意正态分布。标准正态分布相对规整，均有零均值、标准方差等好处，但更重要的是标准正态分布拥有一个非常有价值的特点：它的每个分量是解耦的，用概率的话说，就是相互独立的，满足$p(x,y)=p(x)p(y)$。

我们知道如果特征相互独立的话，建模就会容易得多（朴素贝叶斯分类器就是完全精确的模型），而且相互独立的特征可解释行也好很多，因此我们总希望特征相互独立。早在1992年LSTM之父Schmidhuber就提出了PM模型（Predictability Minimization），致力于构建一个特征解耦的自编码器，相关故事可以参考 [《从PM到GAN - LSTM之父Schmidhuber横跨22年的怨念》](https://mp.weixin.qq.com/s/ANVrDYqy52eo_hBwIoKPVg)。没错，就是在我还没有来到地球的那些年，大牛们就已经着手研究特征解耦了，可见特征解耦的价值有多大。

在VAE中（包括后来的对抗自编码器），直接通过KL散度让隐变量的分布对齐一个解耦的先验分布，这样带来的好处便是隐变量本身也接近解耦的，从而拥有前面说的解耦的各种好处。因此，现在我们可以回答一个很可能会被问到的问题：

> 问：从特征编码的角度看，变分自编码器相比普通的自编码器有什么好处？
>
> 答：变分自编码器通过KL散度让隐变量分布靠近标准正态分布，从而能解耦隐变量特征，简化后面的建立在该特征之上的模型。（当然，你也可以联系前面说的变分信息瓶颈来回答一波，比如增强泛化性能等^\_^）

### 线性插值与卷积 [\#](https://kexue.fm/archives/6181\#%E7%BA%BF%E6%80%A7%E6%8F%92%E5%80%BC%E4%B8%8E%E5%8D%B7%E7%A7%AF)

此外，正态分布还有一个重要的性质，这个性质通常被用来演示生成模型的效果，它就是如下的线性插值：

引用自Glow模型的插值效果图

这种线性插值的过程是：首先采样两个随机向量$z\_1,z\_2\\sim \\mathcal{N}(0, 1)$，显然好的生成器能将$z\_1,z\_2$都生成一个真实的图片$g(z\_1),g(z\_2)$，然后我们考虑$g(z\_{\\alpha})$，其中
\\begin{equation}z\_{\\alpha} = (1 - \\alpha) z\_1 + \\alpha z\_2,\\quad 0 \\leq \\alpha \\leq 1\\end{equation}
考虑$\\alpha$从0到1的变化过程，我们期望看到的$g(z\_{\\alpha})$是图片$g(z\_1)$逐渐过渡到图片$g(z\_2)$，事实上也正是如此。

为什么插值一定要在隐变量空间插值呢？为什么直接对原始图片插值得不到有价值的结果。这其实和也和正态分布有关，因为我们有如下的卷积定理（此卷积非彼卷积，它是数学的卷积算子，不是神经网络的卷积层）：

> 如果$z\_1\\sim \\mathcal{N}(\\mu\_1, \\sigma\_1^2),z\_2\\sim \\mathcal{N}(\\mu\_2, \\sigma\_2^2)$，并且它们是相互独立的随机变量，那么
> $$z\_1 + z\_2 \\sim\\mathcal{N}(\\mu\_1+\\mu\_2, \\sigma\_1^2+\\sigma\_2^2)$$
>
> 特别地，如果$z\_1\\sim \\mathcal{N}(0, 1),z\_2\\sim \\mathcal{N}(0, 1)$，那么
> $$\\alpha z\_1 + \\beta z\_2 \\sim \\mathcal{N}(0, \\alpha^2+\\beta^2)$$

这也就是说正态分布的随机变量的和还是正态分布。这意味着什么呢？意味着在正态分布的世界里，两个变量的线性插值还仍然处在这个世界。这不是一个普通的性质，因为显然对两个真实样本插值就不一定是真实的图片了。

对于有监督任务，线性插值这个性质有什么价值呢？有，而且很重要。我们知道标注数据集很难得，如果我们能将有限的训练集的隐变量空间都合理地映射到标准正态分布，那么，我们可以期望训练集没有覆盖到的部分也可能被我们考虑到了，因为它的隐变量可能只是原来训练集的隐变量的线性插值。

也就是说，当我们完成有监督训练，并且把隐变量分布也规整为标准正态分布之后，我们实际上已经把训练集之间的过渡样本也考虑进去了，从而相当于覆盖了更多的样本了。

> **注：** 我们通常考虑的是空间域均匀线性插值，即$\\beta = 1-\\alpha$的形式，但从$\\alpha z\_1 + \\beta z\_2 \\sim \\mathcal{N}(0, \\alpha^2+\\beta^2)$的角度看，最好的选择是$\\alpha^2+\\beta^2=1$的插值，即
> $$z\_{\\theta}=z\_1\\cdot\\cos\\theta + z\_2\\cdot\\sin\\theta$$
>
> 其次，可能读者会相当，当GAN的先验分布用均匀分布时，不也可以线性插值吗？这好像不是正态分布独有的呀？其实均匀分布的卷积不再是均匀分布了，但是它的概率密度函数刚好集中在原来均匀分布的中间（只不过不均匀了，相当于取了原来的一个子集），所以插值效果还能让人满意，只不过从理论上来看它不够优美。另外从实践来看，目前GAN的训练中也多用正态分布了，训练起来比用均匀分布效果更好些。

## 学习与遗忘 [\#](https://kexue.fm/archives/6181\#%E5%AD%A6%E4%B9%A0%E4%B8%8E%E9%81%97%E5%BF%98)

最后，说了那么多，其实所有内容有一个非常直观的对应： **遗忘**。

遗忘也是深度学习中一个很重要的课题，时不时有相关的研究成果出来。比如我们用新领域的数据集的微调训练好的模型，模型往往就只能适用于新领域，而不是两个领域都能用，这就是深度学习的“灾难性遗忘”问题。又比如前段时间出来个研究，说LSTM的三个门之中，只保留“遗忘门”其实就足够了。

至于前面说了很长的信息瓶颈，跟遗忘也是对应的。因为大脑的容量就固定在那里，你只好用有限的信息完成你的任务，这就提取出了有价值的信息。还是那个经典的例子，银行的工作人员也许看一看、摸一摸就能辨别出假币出来，但是他们对人民币非常了解吗？他们能画出人民币的轮廓吗？我觉得应该做不到。因为他们为了完成这个任务所分配的大脑容量也有限的，他们只需要辨别假币的最重要的信息。这就是大脑的信息瓶颈。

前面说的深度学习的信息瓶颈，也可以同样地类比。一般认为神经网络有效的基础是信息损失，逐层把无用的信息损失掉（忘记），最后保留有效的、泛化的信息，但神经网络参数实在太多，有时候不一定能达到这个目的，所以信息瓶颈就往神经网络加了个约束，相当于“强迫”神经网络去忘记无用的信息。但也正因为如此，VIB并非总是能提升你原来模型的效果，因为如果你的模型本来就已经“逐层把无用的信息损失掉（忘记），最后保留有效的、泛化的信息”了，那么VIB就是多余的了。VIB只是一个正则项，跟其他所有正则项一样，效果都不是绝对的。

我突然想到了《倚天屠龙记》里边张无忌学太极剑的一段描述：

> 要知张三丰传给他的乃是“剑意”，而非“剑招”，要他将所见到的剑招忘得半点不剩，才能得其神髓，临敌时以意驭剑，千变万化，无穷无尽。倘若尚有一两招剑法忘不乾净，心有拘囿，剑法便不能纯。

原来遗忘才是最高境界！所以，本文虽然看上去不扣题，但却是一篇实实在在的散文——《论遗忘的重要性》。

《倚天屠龙记之魔教教主》截屏

_**转载到请包括本文地址：** [https://kexue.fm/archives/6181](https://kexue.fm/archives/6181)_

_**更详细的转载事宜请参考：**_ [《科学空间FAQ》](https://kexue.fm/archives/6508#%E6%96%87%E7%AB%A0%E5%A6%82%E4%BD%95%E8%BD%AC%E8%BD%BD/%E5%BC%95%E7%94%A8)

**如果您还有什么疑惑或建议，欢迎在下方评论区继续讨论。**

**如果您觉得本文还不错，欢迎 [分享](https://kexue.fm/archives/6181#share)/ [打赏](https://kexue.fm/archives/6181#pay) 本文。打赏并非要从中获得收益，而是希望知道科学空间获得了多少读者的真心关注。当然，如果你无视它，也不会影响你的阅读。再次表示欢迎和感谢！**

打赏

微信打赏

支付宝打赏

因为网站后台对打赏并无记录，因此欢迎在打赏时候备注留言。你还可以 [**点击这里**](http://mail.qq.com/cgi-bin/qm_share?t=qm_mailme&email=tN7d1drY3drrx8H0xcWa19vZ) 或在下方评论区留言来告知你的建议或需求。

**如果您需要引用本文，请参考：**

苏剑林. (Nov. 27, 2018). 《从变分编码、信息瓶颈到正态分布：论遗忘的重要性 》\[Blog post\]. Retrieved from [https://kexue.fm/archives/6181](https://kexue.fm/archives/6181)

@online{kexuefm-6181,
        title={从变分编码、信息瓶颈到正态分布：论遗忘的重要性},
        author={苏剑林},
        year={2018},
        month={Nov},
        url={\\url{https://kexue.fm/archives/6181}},
}

分类： [信息时代](https://kexue.fm/category/Big-Data)    标签： [概率](https://kexue.fm/tag/%E6%A6%82%E7%8E%87/), [变分](https://kexue.fm/tag/%E5%8F%98%E5%88%86/), [互信息](https://kexue.fm/tag/%E4%BA%92%E4%BF%A1%E6%81%AF/), [vae](https://kexue.fm/tag/vae/)[56 评论](https://kexue.fm/archives/6181#comments)

< [不用L约束又不会梯度消失的GAN，了解一下？](https://kexue.fm/archives/6163) \| [最小熵原理（四）：“物以类聚”之从图书馆到词向量](https://kexue.fm/archives/6191) >

### 你也许还对下面的内容感兴趣

- [一道概率不等式：盯着它到显然成立为止！](https://kexue.fm/archives/10902)
- [Softmax后传：寻找Top-K的光滑近似](https://kexue.fm/archives/10373)
- [通向最优分布之路：概率空间的最小化](https://kexue.fm/archives/10289)
- [通向概率分布之路：盘点Softmax及其替代品](https://kexue.fm/archives/10145)
- [用傅里叶级数拟合一维概率密度函数](https://kexue.fm/archives/10007)
- [随机分词再探：从Viterbi Sampling到完美采样算法](https://kexue.fm/archives/9811)
- [EMO：基于最优传输思想设计的分类损失函数](https://kexue.fm/archives/9797)
- [随机分词浅探：从Viterbi Decoding到Viterbi Sampling](https://kexue.fm/archives/9768)
- [大词表语言模型在续写任务上的一个问题及对策](https://kexue.fm/archives/9762)
- [如何度量数据的稀疏程度？](https://kexue.fm/archives/9595)

[发表你的看法](https://kexue.fm/archives/6181#comment_form)

1. [«](https://kexue.fm/archives/6181/comment-page-2#comments)
2. [1](https://kexue.fm/archives/6181/comment-page-1#comments)
3. [2](https://kexue.fm/archives/6181/comment-page-2#comments)
4. [3](https://kexue.fm/archives/6181/comment-page-3#comments)

du\_summer

August 7th, 2023

苏老师您好，非常感谢您的文章，学习受益匪浅。有一个问题想向您请教，在VAE和变分信息瓶颈VIB中的编码器P(Z\|X)生成均值和方差，VIB的论文中，作者提到使用一个MLP的编码器fe输出的结果是K维的均值和K\*K的协方差矩阵，我的理解是，编码器生成的潜在变量Z是将每个数据样本Xi生成了K个维度的多元高斯分布，多元高斯分布的均值是向量，协方差矩阵是K\*K是没问题的，但是为什么在VIB和VAE的代码实现中，通过神经网络fe得到的均值和方差shape都是\[N,K\]形式的呢？所有，我很疑惑的就是编码器P(Z\|X)生成的高斯分布到底是什么意思？非常感谢~

[回复评论](https://kexue.fm/archives/6181/comment-page-3?replyTo=22445#respond-post-6181)

[苏剑林](https://kexue.fm) 发表于
August 9th, 2023

\[N, K\]就是每个样本对应一个K维的均值和方差，K维的方差指的是假设每个维度相互独立，也就是假设是K个相互独立的1维分布，每个维度独立算自己的1维方差，算出来的K个1维方差。

[回复评论](https://kexue.fm/archives/6181/comment-page-3?replyTo=22467#respond-post-6181)

night

May 7th, 2025

似乎有个小问题，不知道是不是我的理解有误，分享如下。
公式(10)
\\begin{equation\*}
\\mathbb{E}\_{x\\sim \\tilde{p}(x)} \\Big\[\\mathbb{E}\_{z\\sim p(z\|x)}\\big\[-\\log p(y\|z)\\big\]+\\lambda\\cdot KL\\big(p(z\|x)\\big\\Vert q(z)\\big)\\Big\]
\\end{equation\*}
似乎没有体现出有监督学习的场景,i.e., 输入$x$和输出$y$都是已知的，感觉得把$\\mathbb{E}\_{x\\sim \\tilde{p}(x)}$写成$\\mathbb{E}\_{x,y\\sim {p}(x,y)}$更合理一些，然后细看了一下推导，发现是在引入公式(5)
\\begin{equation\*}-\\iint p(z\|x)\\tilde{p}(x)\\log p(y\|z)dxdz\\end{equation\*}
的时候出现的问题。按VIB的原文，这一项应该是$I(Z,Y;\\theta)$，按照祖先采样(Ancestral Sampling)以及马尔科夫链$Y\\leftrightarrow X\\leftrightarrow Z$的性质的话，写成
\\begin{equation\*}
-\\iint p(z\|x)\\tilde{p}(x) p (y\|x)\\log p(y\|z)dxdydz = -\\iint p(z\|x)p (y,x)\\log p(y\|z)dxdydz
\\end{equation\*}
好像会更好。这样子最终在公式(10)的时候就可以体现出这一点$\\mathbb{E}\_{x,y\\sim {p}(x,y)}$这一点。

[回复评论](https://kexue.fm/archives/6181/comment-page-3?replyTo=27541#respond-post-6181)

1. [«](https://kexue.fm/archives/6181/comment-page-2#comments)
2. [1](https://kexue.fm/archives/6181/comment-page-1#comments)
3. [2](https://kexue.fm/archives/6181/comment-page-2#comments)
4. [3](https://kexue.fm/archives/6181/comment-page-3#comments)

[取消回复](https://kexue.fm/archives/6181#respond-post-6181)

你的大名

电子邮箱

个人网站（选填）

1\. 可以使用LaTeX代码，点击“预览效果”可查看效果；2. 可以通过点击评论楼层编号来引用该楼层；3. 网站可能会有点卡，如非确认评论失败，请 **不要重复点击提交**。

### 内容速览

[变分自编码器](https://kexue.fm/archives/6181#%E5%8F%98%E5%88%86%E8%87%AA%E7%BC%96%E7%A0%81%E5%99%A8)
[理论形式回顾](https://kexue.fm/archives/6181#%E7%90%86%E8%AE%BA%E5%BD%A2%E5%BC%8F%E5%9B%9E%E9%A1%BE)
[与自编码器的比较](https://kexue.fm/archives/6181#%E4%B8%8E%E8%87%AA%E7%BC%96%E7%A0%81%E5%99%A8%E7%9A%84%E6%AF%94%E8%BE%83)
[信息瓶颈](https://kexue.fm/archives/6181#%E4%BF%A1%E6%81%AF%E7%93%B6%E9%A2%88)
[揭开DL的黑箱？](https://kexue.fm/archives/6181#%E6%8F%AD%E5%BC%80DL%E7%9A%84%E9%BB%91%E7%AE%B1%EF%BC%9F)
[信息瓶颈的原理](https://kexue.fm/archives/6181#%E4%BF%A1%E6%81%AF%E7%93%B6%E9%A2%88%E7%9A%84%E5%8E%9F%E7%90%86)
[变分信息瓶颈](https://kexue.fm/archives/6181#%E5%8F%98%E5%88%86%E4%BF%A1%E6%81%AF%E7%93%B6%E9%A2%88)
[结果观察与实现](https://kexue.fm/archives/6181#%E7%BB%93%E6%9E%9C%E8%A7%82%E5%AF%9F%E4%B8%8E%E5%AE%9E%E7%8E%B0)
[变分判别瓶颈](https://kexue.fm/archives/6181#%E5%8F%98%E5%88%86%E5%88%A4%E5%88%AB%E7%93%B6%E9%A2%88)
[正态分布](https://kexue.fm/archives/6181#%E6%AD%A3%E6%80%81%E5%88%86%E5%B8%83)
[规整和解耦](https://kexue.fm/archives/6181#%E8%A7%84%E6%95%B4%E5%92%8C%E8%A7%A3%E8%80%A6)
[线性插值与卷积](https://kexue.fm/archives/6181#%E7%BA%BF%E6%80%A7%E6%8F%92%E5%80%BC%E4%B8%8E%E5%8D%B7%E7%A7%AF)
[学习与遗忘](https://kexue.fm/archives/6181#%E5%AD%A6%E4%B9%A0%E4%B8%8E%E9%81%97%E5%BF%98)

### 智能搜索

支持整句搜索！网站自动使用 [结巴分词](https://github.com/fxsjy/jieba) 进行分词，并结合ngrams排序算法给出合理的搜索结果。

### 热门标签

[生成模型](https://kexue.fm/tag/%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B/) [attention](https://kexue.fm/tag/attention/) [优化](https://kexue.fm/tag/%E4%BC%98%E5%8C%96/) [语言模型](https://kexue.fm/tag/%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/) [模型](https://kexue.fm/tag/%E6%A8%A1%E5%9E%8B/) [网站](https://kexue.fm/tag/%E7%BD%91%E7%AB%99/) [概率](https://kexue.fm/tag/%E6%A6%82%E7%8E%87/) [梯度](https://kexue.fm/tag/%E6%A2%AF%E5%BA%A6/) [转载](https://kexue.fm/tag/%E8%BD%AC%E8%BD%BD/) [微分方程](https://kexue.fm/tag/%E5%BE%AE%E5%88%86%E6%96%B9%E7%A8%8B/) [矩阵](https://kexue.fm/tag/%E7%9F%A9%E9%98%B5/) [天象](https://kexue.fm/tag/%E5%A4%A9%E8%B1%A1/) [分析](https://kexue.fm/tag/%E5%88%86%E6%9E%90/) [深度学习](https://kexue.fm/tag/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/) [积分](https://kexue.fm/tag/%E7%A7%AF%E5%88%86/) [python](https://kexue.fm/tag/python/) [优化器](https://kexue.fm/tag/%E4%BC%98%E5%8C%96%E5%99%A8/) [力学](https://kexue.fm/tag/%E5%8A%9B%E5%AD%A6/) [无监督](https://kexue.fm/tag/%E6%97%A0%E7%9B%91%E7%9D%A3/) [扩散](https://kexue.fm/tag/%E6%89%A9%E6%95%A3/) [几何](https://kexue.fm/tag/%E5%87%A0%E4%BD%95/) [节日](https://kexue.fm/tag/%E8%8A%82%E6%97%A5/) [生活](https://kexue.fm/tag/%E7%94%9F%E6%B4%BB/) [文本生成](https://kexue.fm/tag/%E6%96%87%E6%9C%AC%E7%94%9F%E6%88%90/) [数论](https://kexue.fm/tag/%E6%95%B0%E8%AE%BA/)

### 随机文章

- [文本情感分类（四）：更好的损失函数](https://kexue.fm/archives/4293)
- [背景资料：从数字看诺贝尔物理学奖](https://kexue.fm/archives/166)
- [RealFormer：把残差转移到Attention矩阵上面去](https://kexue.fm/archives/8027)
- [中国队2010年再获IMO团体总分第一](https://kexue.fm/archives/729)
- [“末日”的快乐！](https://kexue.fm/archives/1840)
- [Coming Back...](https://kexue.fm/archives/3735)
- [集合的划分与贝尔数](https://kexue.fm/archives/2985)
- [Bias项的神奇作用：RoPE + Bias = 更好的长度外推性](https://kexue.fm/archives/9577)
- [【理解黎曼几何】2\. 从勾股定理到黎曼度量](https://kexue.fm/archives/3969)
- [多标签“Softmax+交叉熵”的软标签版本](https://kexue.fm/archives/9064)

### 最近评论

- [Skyler Lin](https://kexue.fm/archives/11033/comment-page-1#comment-27949): respect苏神！
- [宋佳铭](https://kexue.fm/archives/10958/comment-page-1#comment-27947): 对，个人感觉mean flow就是continuous time CTM
- [宋佳铭](https://kexue.fm/archives/10958/comment-page-1#comment-27946): 的确，对sg这个事情我感觉如果是用‘归纳’法做是不太能避免的，因为毕竟是用步长短的模型去约束步...
- [MoFHeka](https://kexue.fm/archives/10542/comment-page-1#comment-27945): 苏老师您好，请问一下这套结论在稀疏参数上应该如何应用？比如大规模稀疏Embedding，每个B...
- [苏剑林](https://kexue.fm/archives/11033/comment-page-1#comment-27944): Temp LoRA倒是有印象，其实思想是一样的，如果我单独开一篇文章介绍TTT的话，应该会提到...
- [苏剑林](https://kexue.fm/archives/11033/comment-page-1#comment-27943): 你搜搜mamba、rwkv甚至rnn做vision的工作，其实不少。不过多数确实像你说的，正反...
- [苏剑林](https://kexue.fm/archives/9379/comment-page-1#comment-27942): 问题1可以看看 https://kexue.fm/archives/4718 ，简单来说就是点...
- [苏剑林](https://kexue.fm/archives/10958/comment-page-2#comment-27941): 你的“信息量”怎么定义？直观来说，reflow训练的是切线模型，而一步生成需要的是割线模型，m...
- [苏剑林](https://kexue.fm/archives/10711/comment-page-2#comment-27940): 加大codebook也不行吗？主要是我也不大了解SDXL有什么特别之处～
- [苏剑林](https://kexue.fm/archives/9009/comment-page-2#comment-27939): 如果考虑额外的scale，那情况肯定不一样了，甚至手工scale得好的话，layernorm都...

### 友情链接

- [Cool Papers](https://papers.cool)
- [数学研发](https://bbs.emath.ac.cn)
- [Seatop](http://www.seatop.com.cn/)
- [Xiaoxia](https://xiaoxia.org/)
- [积分表-网络版](https://kexue.fm/sci/integral/index.html)
- [丝路博傲](http://blog.dvxj.com/)
- [ph4ntasy 饭特稀](http://www.ph4ntasy.com/)
- [数学之家](http://www.2math.cn/)
- [有趣天文奇观](http://interesting-sky.china-vo.org/)
- [TwistedW](http://www.twistedwg.com/)
- [godweiyang](https://godweiyang.com/)
- [AI柠檬](https://blog.ailemon.net/)
- [王登科-DK博客](https://greatdk.com)
- [ESON](https://blog.eson.org/)
- [枫之羽](https://fzhiy.net/)
- [Mathor's blog](https://wmathor.com/)
- [coding-zuo](https://coding-zuo.github.io/)
- [博科园](https://www.bokeyuan.net/)
- [孔皮皮的博客](https://www.kppkkp.top/)
- [运鹏的博客](https://yunpengtai.top/)
- [jiming.site](https://jiming.site/)
- [OmegaXYZ](https://www.omegaxyz.com/)
- [Blog by Eacls](https://www.eacls.top/)
- [EAI猩球](https://www.robotech.ink/)
- [文举的博客](https://liwenju0.com/)
- [用代码打点酱油](https://bruceyuan.com/)
- [申请链接](https://kexue.fm/links.html)

本站采用创作共用版权协议，要求署名、非商业用途和保持一致。转载本站内容必须也遵循“ [署名-非商业用途-保持一致](http://creativecommons.org/licenses/by-nc-nd/2.5/cn/)”的创作共用协议。
© 2009-2025 Scientific Spaces. All rights reserved. Theme by [laogui](http://www.laogui.com). Powered by [Typecho](http://typecho.org). 备案号: [粤ICP备09093259号-1/2](https://beian.miit.gov.cn/)。