## SEARCH

## MENU

- [打赏](https://kexue.fm/reward.html)
- [公式](https://kexue.fm/latex.html)
- [天象](https://kexue.fm/ac.html)
- [链接](https://kexue.fm/links.html)
- [时光](https://kexue.fm/me.html)
- [博览](https://kexue.fm/science.html)
- [归档](https://kexue.fm/content.html)

## CATEGORIES

- [千奇百怪](https://kexue.fm/category/Everything)
- [天文探索](https://kexue.fm/category/Astronomy)
- [数学研究](https://kexue.fm/category/Mathematics)
- [物理化学](https://kexue.fm/category/Phy-chem)
- [信息时代](https://kexue.fm/category/Big-Data)
- [生物自然](https://kexue.fm/category/Biology)
- [图片摄影](https://kexue.fm/category/Photograph)
- [问题百科](https://kexue.fm/category/Questions)
- [生活/情感](https://kexue.fm/category/Life-Feeling)
- [资源共享](https://kexue.fm/category/Resources)

## NEWPOSTS

- [n个正态随机数的最大值的渐近估计](https://kexue.fm/archives/11390)
- [流形上的最速下降：5\. 对偶梯度下降](https://kexue.fm/archives/11388)
- [低精度Attention可能存在有...](https://kexue.fm/archives/11371)
- [MuP之上：1. 好模型的三个特征](https://kexue.fm/archives/11340)
- [随机矩阵的谱范数的快速估计](https://kexue.fm/archives/11335)
- [DiVeQ：一种非常简洁的VQ训练方案](https://kexue.fm/archives/11328)
- [为什么线性注意力要加Short C...](https://kexue.fm/archives/11320)
- [AdamW的Weight RMS的...](https://kexue.fm/archives/11307)
- [重新思考学习率与Batch Siz...](https://kexue.fm/archives/11301)
- [重新思考学习率与Batch Siz...](https://kexue.fm/archives/11285)

## COMMENTS

- [出售公众号吗: 您好，请问您出售停更的公众号吗，有出售意向可以加v：Follo...](https://kexue.fm/archives/11390/comment-page-1#comment-28816)
- [pki: 苏老师您好，weightdecay=0 也不需要重新调整吗](https://kexue.fm/archives/10739/comment-page-2#comment-28815)
- [pang: 还有一个蛮有意思的想法想跟您讨论下，请问您怎么看目前一些端侧模...](https://kexue.fm/archives/10862/comment-page-1#comment-28814)
- [zgz: \[comment=25121\]苏剑林\[/comment\]我理解...](https://kexue.fm/archives/8791/comment-page-1#comment-28813)
- [Cuddle: 苏神，在 linear-attention的应用场景中，如果去...](https://kexue.fm/archives/8265/comment-page-8#comment-28812)
- [pang: 好的，感谢您](https://kexue.fm/archives/10862/comment-page-1#comment-28811)
- [danyao12: "理想的根治办法是Stochastic Rounding，也就...](https://kexue.fm/archives/11371/comment-page-1#comment-28805)
- [wade: 那公式18，是缺少了，$-\\frac{1}{2} \\left ...](https://kexue.fm/archives/5977/comment-page-3#comment-28804)
- [李双良: 你好，公式23中分母的H对角线元素求和的因子为什么只有（1−β...](https://kexue.fm/archives/11280/comment-page-1#comment-28802)
- [ljj: 博主您好，我在您的另一篇文章中（https://spaces....](https://kexue.fm/archives/10542/comment-page-1#comment-28801)

## USERLOGIN

- [登录](https://kexue.fm/admin/login.php)

[科学空间\|Scientific Spaces](https://kexue.fm)

- [登录](https://kexue.fm/admin/login.php)
- [打赏](https://kexue.fm/reward.html)
- [公式](https://kexue.fm/latex.html)
- [天象](https://kexue.fm/ac.html)
- [链接](https://kexue.fm/links.html)
- [时光](https://kexue.fm/me.html)
- [博览](https://kexue.fm/science.html)
- [归档](https://kexue.fm/content.html)

渴望成为一个小飞侠

- [欢迎订阅](https://kexue.fm/feed)
- [个性邮箱](https://kexue.fm/archives/119)
- [天象信息](https://kexue.fm/ac.html)
- [观测ISS](https://kexue.fm/archives/41)
- [LaTeX](https://kexue.fm/latex.html)
- [关于博主](https://kexue.fm/me.html)

欢迎访问“科学空间”，这里将与您共同探讨自然科学，回味人生百态；也期待大家的分享～

- [**千奇百怪** Everything](https://kexue.fm/category/Everything)
- [**天文探索** Astronomy](https://kexue.fm/category/Astronomy)
- [**数学研究** Mathematics](https://kexue.fm/category/Mathematics)
- [**物理化学** Phy-chem](https://kexue.fm/category/Phy-chem)
- [**信息时代** Big-Data](https://kexue.fm/category/Big-Data)
- [**生物自然** Biology](https://kexue.fm/category/Biology)
- [**图片摄影** Photograph](https://kexue.fm/category/Photograph)
- [**问题百科** Questions](https://kexue.fm/category/Questions)
- [**生活/情感** Life-Feeling](https://kexue.fm/category/Life-Feeling)
- [**资源共享** Resources](https://kexue.fm/category/Resources)

- [**千奇百怪**](https://kexue.fm/category/Everything)
- [**天文探索**](https://kexue.fm/category/Astronomy)
- [**数学研究**](https://kexue.fm/category/Mathematics)
- [**物理化学**](https://kexue.fm/category/Phy-chem)
- [**信息时代**](https://kexue.fm/category/Big-Data)
- [**生物自然**](https://kexue.fm/category/Biology)
- [**图片摄影**](https://kexue.fm/category/Photograph)
- [**问题百科**](https://kexue.fm/category/Questions)
- [**生活/情感**](https://kexue.fm/category/Life-Feeling)
- [**资源共享**](https://kexue.fm/category/Resources)

[首页](https://kexue.fm) [数学研究](https://kexue.fm/category/Mathematics) 重新思考学习率与Batch Size（四）：EMA

22Sep

# [重新思考学习率与Batch Size（四）：EMA](https://kexue.fm/archives/11301)

By 苏剑林 \|
2025-09-22 \|
24457位读者\|

我们在 [《重新思考学习率与Batch Size（二）：平均场》](https://kexue.fm/archives/11280) 中提到，关注SignSGD的原因之一是我们通常将它作为Adam的理论近似，这是Adam做理论分析时常用的简化策略。除了分析学习率的场景外，在 [《配置不同的学习率，LoRA还能再涨一点？》](https://kexue.fm/archives/10001)、 [《初探MuP：超参数的跨模型尺度迁移规律》](https://kexue.fm/archives/10770) 等地方我们也用了这个简化。

然而，SignSGD真是Adam的良好近似吗？一个明显差异是SignSGD的Update RMS总是1，而Adam并非如此。笔者发现，导致这一差异的核心原因是动量，它普遍存在于Adam、Lion、Muon等优化器中。所以，本文我们来考察动量——更广义地说是EMA——的影响。

## 问题分析 [\#](https://kexue.fm/kexue.fm\#%E9%97%AE%E9%A2%98%E5%88%86%E6%9E%90)

从Adam的视角看，SignSGD对应$\\beta\_1=\\beta\_2=0$这个特例，或者对应于Adam的第一步更新量（不管$\\beta\_1,\\beta\_2$如何）。因此，我们认为它跟Adam肯定有一些共性，能够捕捉到一些通用的规律。

但是，它们之间也有一些明显的差异。比较典型的就是Update RMS的差异，SignSGD总是1，但Adam往往明显小于1；还有，Adam看上去更贴近SGD，它更像是SignSGD和SGD的一个中间版本。一开始，笔者以为这是Adam分母中的$\\epsilon$导致的差异，所以在 [《Adam的epsilon如何影响学习率的Scaling Law？》](https://kexue.fm/archives/10563) 还特意计算了带$\\epsilon$的SoftSignSGD。

后来，我们在 [《为什么Adam的Update RMS是0.2？》](https://kexue.fm/archives/11267) 从模拟和理论两方面估计了Adam的Update RMS，其实平均场近似的估计结果为$\\sqrt{\\frac{1-\\beta\_1}{1+\\beta\_1}}$，并且验证了它跟模拟结果和实际实验都很吻合。这个结果显式地依赖于$\\beta\_1$，所以很明显，它将我们的思考方向引向动量。

这就有了下面的分析过程。综下所述，我们可以确认，$\\epsilon$的角色确实是次要的，真正的主角其实是动量——它是梯度的“滑动平均”——这也正是本文的主角“EMA（Exponential Moving Average）”。

## 梯度下降 [\#](https://kexue.fm/kexue.fm\#%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D)

为了分析EMA带来的变数，我们从SGDM入手，也就是带动量的SGD，实际上我们在用SGD的时候极少情况是不加动量的：
\\begin{equation}\\begin{aligned}
&\\boldsymbol{m}\_t = \\beta\_1 \\boldsymbol{m}\_{t-1} + \\left(1 - \\beta\_1\\right) \\boldsymbol{g}\_t \\\\[4pt\]
&\\boldsymbol{w}\_t = \\boldsymbol{w}\_{t-1} - \\eta\_t \\boldsymbol{m}\_t
\\end{aligned}\\end{equation}
实际使用中，$\\boldsymbol{g}\_t$替换为$\\tilde{\\boldsymbol{g}}\_{B,t}$，它是一个随机变量，均值为$\\boldsymbol{g}\_t$，协方差矩阵为$\\boldsymbol{\\Sigma}\_t/B$，这些基本设置跟 [《重新思考学习率与Batch Size（一）：现状》](https://kexue.fm/archives/11260) 是一样的。这里的噪声，是由随机采样不同的Batch引起的，所以我们可以合理地假设，不同$t$之间的$\\tilde{\\boldsymbol{g}}\_{B,t}$是相互独立的。

我们的任务，是计算
\\begin{equation}\\newcommand{tr}{\\mathop{\\text{tr}}}\\eta^\* \\approx \\frac{\\mathbb{E}\[\\tilde{\\boldsymbol{\\varphi}}\_B\]^{\\top}\\boldsymbol{g}}{\\tr(\\mathbb{E}\[\\tilde{\\boldsymbol{\\varphi}}\_B\\tilde{\\boldsymbol{\\varphi}}\_B^{\\top}\]\\boldsymbol{H})}\\label{eq:eta-opt}\\end{equation}
相关推导在前面几篇文章已经给出，这里就不再重复。对于SGDM来说$\\tilde{\\boldsymbol{\\varphi}}\_B = \\boldsymbol{m}\_t$，它可以展开成
\\begin{equation}\\boldsymbol{m}\_t = (1 - \\beta\_1)\\sum\\limits\_{s=1}^t \\beta\_1^{t-s}\\tilde{\\boldsymbol{g}}\_{B,s}\\end{equation}

## 放大批量 [\#](https://kexue.fm/kexue.fm\#%E6%94%BE%E5%A4%A7%E6%89%B9%E9%87%8F)

现在可以计算
\\begin{equation}\\mathbb{E}\[\\boldsymbol{m}\_t\] = (1 - \\beta\_1)\\sum\_{s=1}^t \\beta\_1^{t-s}\\mathbb{E}\[\\tilde{\\boldsymbol{g}}\_{B,s}\] = (1 - \\beta\_1)\\sum\_{s=1}^t \\beta\_1^{t-s}\\boldsymbol{g}\_s\\end{equation}
我们进一步假设当模型训练进入“正轨”后，梯度是缓变的，那么我们可以用当前梯度$\\boldsymbol{g}\_t$近似$\\boldsymbol{g}\_s$，得到
\\begin{equation}\\mathbb{E}\[\\boldsymbol{m}\_t\] = (1 - \\beta\_1)\\sum\_{s=1}^t \\beta\_1^{t-s}\\boldsymbol{g}\_t = (1 - \\beta\_1^t) \\boldsymbol{g}\_t \\approx \\boldsymbol{g}\_t \\qquad (t\\to\\infty)\\end{equation}
至于$\\mathbb{E}\[\\boldsymbol{m}\_t \\boldsymbol{m}\_t^{\\top}\]$，我们利用恒等式$\\mathbb{E}\[\\boldsymbol{m}\_t \\boldsymbol{m}\_t^{\\top}\] = \\mathbb{E}\[\\boldsymbol{m}\_t\] \\mathbb{E}\[\\boldsymbol{m}\_t\]^{\\top} + \\mathbb{C}\\text{ov}\[\\boldsymbol{m}\_t,\\boldsymbol{m}\_t\]$，然后利用方差的可加性得到：
\\begin{equation}\\mathbb{C}\\text{ov}\[\\boldsymbol{m}\_t,\\boldsymbol{m}\_t\] = (1 - \\beta\_1)^2\\sum\_{s=1}^t \\beta\_1^{2(t-s)}\\boldsymbol{\\Sigma}\_s/B\\end{equation}
类似地，我们假设协方差矩阵的缓变性，那么
\\begin{equation}\\mathbb{C}\\text{ov}\[\\boldsymbol{m}\_t\] \\approx (1 - \\beta\_1)^2\\sum\_{s=1}^t \\beta\_1^{2(t-s)}\\boldsymbol{\\Sigma}\_t/B = (1 - \\beta\_1)^2\\frac{1-\\beta\_1^{2t}}{1-\\beta\_1^2}\\boldsymbol{\\Sigma}\_t/B = \\frac{1 - \\beta\_1}{1 + \\beta\_1}\\boldsymbol{\\Sigma}\_t/B \\qquad (t\\to\\infty)\\end{equation}
代入式$\\eqref{eq:eta-opt}$得
\\begin{equation}\\eta^\* \\approx \\frac{\\eta\_{\\max}}{1 + \\frac{1 - \\beta\_1}{1 + \\beta\_1}\\mathcal{B}\_{\\text{noise}}/B},\\qquad \\eta\_{\\max} = \\frac{\\boldsymbol{g}^{\\top}\\boldsymbol{g}}{\\boldsymbol{g}^{\\top}\\boldsymbol{H}\\boldsymbol{g}},\\quad\\mathcal{B}\_{\\text{noise}} = \\frac{\\tr(\\boldsymbol{\\Sigma}\\boldsymbol{H})}{\\boldsymbol{g}^{\\top}\\boldsymbol{H}\\boldsymbol{g}}\\end{equation}
从这个结果可以看出，动量机制的引入，相当于把SGD的Batch Size放大到了$\\frac{1 + \\beta\_1}{1 - \\beta\_1}$倍。按照笔者的理解，动量就是通过对优化轨迹上的梯度做EMA来低成本地消除梯度噪声，所以这个结果这跟笔者所理解的动量意义是相符的。

## 符号动量 [\#](https://kexue.fm/kexue.fm\#%E7%AC%A6%E5%8F%B7%E5%8A%A8%E9%87%8F)

进一步地，我们考虑SignSGDM，它可以视作 [Lion](https://kexue.fm/archives/9473) 的一个特例，也就是SGDM多加了个$\\newcommand{sign}{\\mathop{\\text{sign}}}\\sign$：
\\begin{equation}\\begin{aligned}
&\\boldsymbol{m}\_t = \\beta\_1 \\boldsymbol{m}\_{t-1} + \\left(1 - \\beta\_1\\right) \\boldsymbol{g}\_t \\\\[4pt\]
&\\boldsymbol{w}\_t = \\boldsymbol{w}\_{t-1} - \\eta\_t \\sign(\\boldsymbol{m}\_t)
\\end{aligned}\\end{equation}
实际训练中$\\boldsymbol{g}\_t$同样替换为$\\tilde{\\boldsymbol{g}}\_{B,t}$。对SignSGDM来说$\\tilde{\\boldsymbol{\\varphi}}\_B = \\sign(\\boldsymbol{m}\_t)$，那么根据平均场近似得
\\begin{equation}\\mathbb{E}\[\\tilde{\\boldsymbol{\\varphi}}\_B\] = \\mathbb{E}\\bigg\[\\frac{\\boldsymbol{m}\_t}{\\sqrt{\\boldsymbol{m}\_t^2}}\\bigg\]\\approx \\frac{\\mathbb{E}\[\\boldsymbol{m}\_t\]}{\\sqrt{\\mathbb{E}\[\\boldsymbol{m}\_t^2\]}}\\end{equation}
其中向量乘法默认是Hadamard积。分子$\\mathbb{E}\[\\boldsymbol{m}\_t\]$我们在上一节已经算了，分母$\\mathbb{E}\[\\boldsymbol{m}\_t^2\]$其实等于$\\newcommand{diag}{\\mathop{\\text{diag}}}\\diag(\\mathbb{E}\[\\boldsymbol{m}\_t \\boldsymbol{m}\_t^{\\top}\])$，所以也可以代入上一节的结果，得到
\\begin{equation}\\mathbb{E}\[\\tilde{\\boldsymbol{\\varphi}}\_B\] \\approx \\frac{\\boldsymbol{g}\_t}{\\sqrt{\\boldsymbol{g}\_t^2 + \\frac{1 - \\beta\_1}{1 + \\beta\_1}\\boldsymbol{\\sigma}\_t^2/B}} = \\frac{\\sign(\\boldsymbol{g}\_t)}{\\sqrt{1 + \\frac{1 - \\beta\_1}{1 + \\beta\_1}(\\boldsymbol{\\sigma}\_t^2/\\boldsymbol{g}\_t^2)/B}} \\approx \\frac{\\sign(\\boldsymbol{g}\_t)}{\\sqrt{1 + \\frac{1 - \\beta\_1}{1 + \\beta\_1} \\mathcal{B}\_{\\text{simple}}/B}}\\end{equation}
其中$\\boldsymbol{\\sigma}\_t^2 = \\diag(\\boldsymbol{\\Sigma}\_t), \\mathcal{B}\_{\\text{simple}} = \\tr(\\boldsymbol{\\Sigma}\_t)/\\boldsymbol{g}\_t^{\\top}\\boldsymbol{g}\_t$。上式相当于SignSGD的$B$换成了$\\frac{1 + \\beta\_1}{1 - \\beta\_1}B$，如果我们进一步计算$\\mathbb{E}\[\\tilde{\\boldsymbol{\\varphi}}\_B\\tilde{\\boldsymbol{\\varphi}}\_B^{\\top}\]$就会发现结论也是如此。所以，跟SGDM一样，动量相当于把SignSGD的Batch Size放大到了$\\frac{1 + \\beta\_1}{1 - \\beta\_1}$倍。

在 [《重新思考学习率与Batch Size（三）：Muon》](https://kexue.fm/archives/11285) 中我们计算过Muon的学习率规律，发现它跟SignSGD一致，所以我们可以断言，动量在Muon中的作用跟SignSGDM一样，都约等于将Batch Size放大成$\\frac{1 + \\beta\_1}{1 - \\beta\_1}$倍。

## 双重滑动 [\#](https://kexue.fm/kexue.fm\#%E5%8F%8C%E9%87%8D%E6%BB%91%E5%8A%A8)

最后我们来看Adam：
\\begin{equation}\\begin{aligned}
&\\boldsymbol{m}\_t = \\beta\_1 \\boldsymbol{m}\_{t-1} + \\left(1 - \\beta\_1\\right) \\boldsymbol{g}\_t\\\
&\\boldsymbol{v}\_t = \\beta\_2 \\boldsymbol{v}\_{t-1} + \\left(1 - \\beta\_2\\right) \\boldsymbol{g}\_t^2\\\
&\\hat{\\boldsymbol{m}}\_t = \\boldsymbol{m}\_t\\left/\\left(1 - \\beta\_1^t\\right)\\right.\\\
&\\hat{\\boldsymbol{v}}\_t = \\boldsymbol{v}\_t\\left/\\left(1 - \\beta\_2^t\\right)\\right.\\\
&\\boldsymbol{\\theta}\_t = \\boldsymbol{\\theta}\_{t-1} - \\eta\_t \\hat{\\boldsymbol{m}}\_t\\left/\\left(\\sqrt{\\hat{\\boldsymbol{v}}\_t} + \\epsilon\\right)\\right.
\\end{aligned}\\end{equation}
实际训练中$\\boldsymbol{g}\_t$替换为$\\tilde{\\boldsymbol{g}}\_{B,t}$。我们考虑的都是训练已经进入“正轨”的状态，即$t\\to\\infty$，所以不区分$\\boldsymbol{m}\_t$和$\\hat{\\boldsymbol{m}}\_t$、$\\boldsymbol{v}\_t$和$\\hat{\\boldsymbol{v}}\_t$，同时我们聚焦于EMA的作用，所以设$\\epsilon = 0$。那么对于Adam来说有$\\tilde{\\boldsymbol{\\varphi}}\_B=\\boldsymbol{m}\_t/\\sqrt{\\boldsymbol{v}\_t}$，它跟SignSGDM的区别，就是分母的$\\boldsymbol{m}\_t^2$换成了另一个EMA的统计量$\\boldsymbol{v}\_t$。

由平均场近似得
\\begin{equation}\\mathbb{E}\[\\tilde{\\boldsymbol{\\varphi}}\_B\] = \\mathbb{E}\\bigg\[\\frac{\\boldsymbol{m}\_t}{\\sqrt{\\boldsymbol{v}\_t}}\\bigg\]\\approx \\frac{\\mathbb{E}\[\\boldsymbol{m}\_t\]}{\\sqrt{\\mathbb{E}\[\\boldsymbol{v}\_t\]}}\\end{equation}
$\\mathbb{E}\[\\boldsymbol{m}\_t\]$我们已经算过，只需算$\\mathbb{E}\[\\boldsymbol{v}\_t\]$：
\\begin{equation}\\mathbb{E}\[\\boldsymbol{v}\_t\] = (1 - \\beta\_2)\\sum\_{s=1}^t \\beta\_2^{t-s}\\mathbb{E}\[\\tilde{\\boldsymbol{g}}\_{B,s}^2\] = (1 - \\beta\_2)\\sum\_{s=1}^t \\beta\_2^{t-s}(\\boldsymbol{g}\_s^2 + \\boldsymbol{\\sigma}\_s^2/B)\\approx \\boldsymbol{g}\_t^2 + \\boldsymbol{\\sigma}\_t^2/B\\end{equation}
跟前面一样，最后一个约等号假设了梯度和方差的缓变性，以及$t\\to\\infty$。于是我们有
\\begin{equation}\\mathbb{E}\[\\tilde{\\boldsymbol{\\varphi}}\_B\] \\approx \\frac{\\boldsymbol{g}\_t}{\\sqrt{\\boldsymbol{g}\_t^2 + \\boldsymbol{\\sigma}\_t^2/B}} \\approx \\frac{\\sign(\\boldsymbol{g}\_t)}{\\sqrt{1 + \\mathcal{B}\_{\\text{simple}}/B}}\\end{equation}
这个结果倒是跟SignSGD相同，所以单从一阶矩看，SignSGD作为Adam的近似是合理的。但我们还有二阶矩$\\mathbb{E}\[\\tilde{\\boldsymbol{\\varphi}}\_B \\tilde{\\boldsymbol{\\varphi}}\_B^{\\top}\]$，在分量独立的假设下，我们只需要算$\\mathbb{E}\[\\tilde{\\boldsymbol{\\varphi}}\_B^2\]$：
\\begin{equation}\\mathbb{E}\[\\tilde{\\boldsymbol{\\varphi}}\_B^2\] = \\mathbb{E}\\bigg\[\\frac{\\boldsymbol{m}\_t^2}{\\boldsymbol{v}\_t}\\bigg\]\\approx \\frac{\\mathbb{E}\[\\boldsymbol{m}\_t^2\]}{\\mathbb{E}\[\\boldsymbol{v}\_t\]} \\approx \\frac{\\boldsymbol{g}\_t^2 + \\frac{1 - \\beta\_1}{1 + \\beta\_1}\\boldsymbol{\\sigma}\_t^2/B}{\\boldsymbol{g}\_t^2 + \\boldsymbol{\\sigma}\_t^2/B}\\label{eq:u2-adam}\\end{equation}

## 两个特例 [\#](https://kexue.fm/kexue.fm\#%E4%B8%A4%E4%B8%AA%E7%89%B9%E4%BE%8B)

我们观察两个特例。首先是$\\beta\_1=0$，这时候分子分母相同，$\\mathbb{E}\[\\tilde{\\boldsymbol{\\varphi}}\_B^2\]$是全1向量，跟SignSGD一致。所以说，SignSGD是$\\beta\_1=0$的Adam——也就是 [RMSProp](https://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf)——的良好近似，当$\\beta\_1$增大时，近似程度开始变差。

当$\\beta\_1=1$时，我们有
\\begin{equation}\\mathbb{E}\[\\tilde{\\boldsymbol{\\varphi}}\_B^2\] \\approx \\frac{\\boldsymbol{g}\_t^2}{\\boldsymbol{g}\_t^2 + \\boldsymbol{\\sigma}\_t^2/B}\\approx \\mathbb{E}\[\\tilde{\\boldsymbol{\\varphi}}\_B\]^2\\end{equation}
由此得到$\\mathbb{E}\[\\tilde{\\boldsymbol{\\varphi}}\_B \\tilde{\\boldsymbol{\\varphi}}\_B^{\\top}\] \\approx \\mathbb{E}\[\\tilde{\\boldsymbol{\\varphi}}\_B\] \\mathbb{E}\[\\tilde{\\boldsymbol{\\varphi}}\_B\]^{\\top}$，代入到式$\\eqref{eq:eta-opt}$得
\\begin{equation}\\eta^\* \\approx \\frac{\\Vert \\boldsymbol{g}\\Vert\_1 \\sqrt{1 + \\mathcal{B}\_{\\text{simple}}/B}}{\\sign(\\boldsymbol{g})^{\\top} \\boldsymbol{H} \\sign(\\boldsymbol{g})}\\end{equation}
注意，它是关于$B$的单调递减函数，即当Batch Size增大时学习率应该减小。由此我们可以推测，Adam的$\\beta\_1$的增大，将会加速“ [Surge现象](https://kexue.fm/archives/11280#%E5%8F%8D%E5%B8%B8%E7%8E%B0%E8%B1%A1)”的出现。

这个结论看似有点费解，但其实换个角度就容易理解了。“Surge现象”指当Batch Size超过某个阈值后，最优学习率随着Batch Size的增大而减少，而前面SGDM、SignSGDM的结果都表明，动量的引入约等于将Batch Size扩大到$\\frac{1 + \\beta\_1}{1 - \\beta\_1} > 1$倍，这自然增加了超过阈值的可能性。

换句话说，“随着$\\beta\_1$的增大，‘Surge现象’将更容易出现”的结论，即便对于SignSGDM也是成立的。而Adam相比SignSGDM有一些新的特性，但“动量机制约等于放大Batch Size”这一点始终是成立的，所以出现同样的结论就不难理解了。

## 一般分析 [\#](https://kexue.fm/kexue.fm\#%E4%B8%80%E8%88%AC%E5%88%86%E6%9E%90)

我们改写一下式$\\eqref{eq:u2-adam}$：
\\begin{equation}\\mathbb{E}\[\\tilde{\\boldsymbol{\\varphi}}\_B^2\] \\approx \\frac{\\boldsymbol{g}\_t^2 + \\frac{1 - \\beta\_1}{1 + \\beta\_1}\\boldsymbol{\\sigma}\_t^2/B}{\\boldsymbol{g}\_t^2 + \\boldsymbol{\\sigma}\_t^2/B} = \\frac{2\\beta\_1}{1+\\beta\_1}\\frac{\\boldsymbol{g}\_t^2}{\\boldsymbol{g}\_t^2 + \\boldsymbol{\\sigma}\_t^2/B} + \\frac{1 - \\beta\_1}{1 + \\beta\_1} \\approx \\frac{2\\beta\_1}{1+\\beta\_1}\\mathbb{E}\[\\tilde{\\boldsymbol{\\varphi}}\_B\]^2 + \\frac{1 - \\beta\_1}{1 + \\beta\_1}\\end{equation}
由此我们可以写出
\\begin{equation}\\mathbb{E}\[\\tilde{\\boldsymbol{\\varphi}}\_B \\tilde{\\boldsymbol{\\varphi}}\_B^{\\top}\] \\approx \\mathbb{E}\[\\tilde{\\boldsymbol{\\varphi}}\_B\] \\mathbb{E}\[\\tilde{\\boldsymbol{\\varphi}}\_B\]^{\\top} + \\frac{1 - \\beta\_1}{1 + \\beta\_1}\\diag\\left(1 - \\mathbb{E}\[\\tilde{\\boldsymbol{\\varphi}}\_B\]^2\\right)\\end{equation}
那么
\\begin{equation}\\eta^\* \\approx \\frac{\\sum\_i \|g\_i\|}{\\frac{1}{\\beta}\\frac{1 - \\beta\_1}{1 + \\beta\_1}\\sum\_i H\_{i,i} + \\beta\\left(\\sum\_{i,j} H\_{i,j}\\sign(g\_i g\_j) - \\frac{1 - \\beta\_1}{1 + \\beta\_1}\\sum\_i H\_{i,i}\\right)}\\end{equation}
这里没有下标的$\\beta$等于$(1 + \\mathcal{B}\_{\\text{simple}}/B)^{-1/2}$，不仔细看的话可能会跟$\\beta\_1,\\beta\_2$混淆，笔者表示很抱歉，因为这是前两篇文章的记号，这里只好沿用了。跟SignSGD不同的是，SignSGD如果假设Hessian矩阵是对角阵，那么就不会出现Surge现象，但上式即便是在对角Hessian假设下依然出现Surge现象，此时：
\\begin{equation}\\eta^\* \\approx \\frac{\\sum\_i \|g\_i\|}{\\left(\\frac{1}{\\beta}\\frac{1 - \\beta\_1}{1 + \\beta\_1} + \\beta\\frac{2\\beta\_1}{1 + \\beta\_1}\\right)\\sum\_i H\_{i,i}}\\end{equation}
由均值不等式知上式在$\\beta^\*=\\sqrt{\\frac{1-\\beta\_1}{2\\beta\_1}}$处取到最大值，但要注意根据$\\beta$定义，它是$\\in(0,1)$的，所以还要判断$\\beta^\*\\in(0,1)$，即$\\beta\_1 > 1/3$，不满足这个条件时最大值依然在$\\beta=1$取到，此时没有Surge现象。反之，当$\\beta\_1 > 1/3$且$\\beta > \\beta^\*$（即$B > \\frac{1-\\beta\_1}{3\\beta\_1-1}\\mathcal{B}\_{\\text{simple}}$）时，学习率应该随着Batch Size的增加而减小。

这个结论可以初步解释为啥Muon能支持更大Batch Size。由 [《重新思考学习率与Batch Size（三）：Muon》](https://kexue.fm/archives/11285) 可知，Muon的表现跟SignSGDM类似，在特定Hessian结构假设下它不会出现Surge现象，这意味着增大Batch Size总可以提高学习效率，尽管相对收益会越来越小。

相反，Adam在常用设置（如$\\beta\_1=0.9$）下，哪怕假设Hessian是对角阵也会出现Surge现象，这意味着Batch Size超过一定值后，学习效率就下降了。

## 文章小结 [\#](https://kexue.fm/kexue.fm\#%E6%96%87%E7%AB%A0%E5%B0%8F%E7%BB%93)

本文初步分析了优化器的EMA机制对学习率与Batch Size的尺度定律的影响，确认了EMA特别是动量机制的引入会稍微改变尺度定律，而Adam这种带有双重EMA运算的优化器，则会呈现出一些跟SignSGD不同的新特性。

_**转载到请包括本文地址：** [https://kexue.fm/archives/11301](https://kexue.fm/archives/11301)_

_**更详细的转载事宜请参考：**_ [《科学空间FAQ》](https://kexue.fm/archives/6508#%E6%96%87%E7%AB%A0%E5%A6%82%E4%BD%95%E8%BD%AC%E8%BD%BD/%E5%BC%95%E7%94%A8)

**如果您还有什么疑惑或建议，欢迎在下方评论区继续讨论。**

**如果您觉得本文还不错，欢迎 [分享](https://kexue.fm/kexue.fm#share)/ [打赏](https://kexue.fm/kexue.fm#pay) 本文。打赏并非要从中获得收益，而是希望知道科学空间获得了多少读者的真心关注。当然，如果你无视它，也不会影响你的阅读。再次表示欢迎和感谢！**

打赏

微信打赏

支付宝打赏

因为网站后台对打赏并无记录，因此欢迎在打赏时候备注留言。你还可以 [**点击这里**](http://mail.qq.com/cgi-bin/qm_share?t=qm_mailme&email=tN7d1drY3drrx8H0xcWa19vZ) 或在下方评论区留言来告知你的建议或需求。

**如果您需要引用本文，请参考：**

苏剑林. (Sep. 22, 2025). 《重新思考学习率与Batch Size（四）：EMA 》\[Blog post\]. Retrieved from [https://kexue.fm/archives/11301](https://kexue.fm/archives/11301)

@online{kexuefm-11301,
        title={重新思考学习率与Batch Size（四）：EMA},
        author={苏剑林},
        year={2025},
        month={Sep},
        url={\\url{https://kexue.fm/archives/11301}},
}

分类： [数学研究](https://kexue.fm/category/Mathematics)    标签： [学习率](https://kexue.fm/tag/%E5%AD%A6%E4%B9%A0%E7%8E%87/), [优化器](https://kexue.fm/tag/%E4%BC%98%E5%8C%96%E5%99%A8/), [尺度定律](https://kexue.fm/tag/%E5%B0%BA%E5%BA%A6%E5%AE%9A%E5%BE%8B/), [平均场](https://kexue.fm/tag/%E5%B9%B3%E5%9D%87%E5%9C%BA/)[抢沙发](https://kexue.fm/archives/11301#comments)

< [重新思考学习率与Batch Size（三）：Muon](https://kexue.fm/archives/11285) \| [AdamW的Weight RMS的渐近估计](https://kexue.fm/archives/11307) >

### 你也许还对下面的内容感兴趣

- [流形上的最速下降：5\. 对偶梯度下降](https://kexue.fm/archives/11388)
- [MuP之上：1. 好模型的三个特征](https://kexue.fm/archives/11340)
- [AdamW的Weight RMS的渐近估计](https://kexue.fm/archives/11307)
- [重新思考学习率与Batch Size（三）：Muon](https://kexue.fm/archives/11285)
- [重新思考学习率与Batch Size（二）：平均场](https://kexue.fm/archives/11280)
- [为什么Adam的Update RMS是0.2？](https://kexue.fm/archives/11267)
- [重新思考学习率与Batch Size（一）：现状](https://kexue.fm/archives/11260)
- [流形上的最速下降：4\. Muon + 谱球面](https://kexue.fm/archives/11241)
- [流形上的最速下降：3\. Muon + Stiefel](https://kexue.fm/archives/11221)
- [流形上的最速下降：2\. Muon + 正交](https://kexue.fm/archives/11215)

[发表你的看法](https://kexue.fm/kexue.fm#comment_form)

[取消回复](https://kexue.fm/archives/11301#respond-post-11301)

你的大名

电子邮箱

个人网站（选填）

1\. 可以使用LaTeX代码，点击“预览效果”可查看效果；2. 可以通过点击评论楼层编号来引用该楼层；3. 网站可能会有点卡，如非确认评论失败，请 **不要重复点击提交**。

### 内容速览

[问题分析](https://kexue.fm/kexue.fm#%E9%97%AE%E9%A2%98%E5%88%86%E6%9E%90)
[梯度下降](https://kexue.fm/kexue.fm#%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D)
[放大批量](https://kexue.fm/kexue.fm#%E6%94%BE%E5%A4%A7%E6%89%B9%E9%87%8F)
[符号动量](https://kexue.fm/kexue.fm#%E7%AC%A6%E5%8F%B7%E5%8A%A8%E9%87%8F)
[双重滑动](https://kexue.fm/kexue.fm#%E5%8F%8C%E9%87%8D%E6%BB%91%E5%8A%A8)
[两个特例](https://kexue.fm/kexue.fm#%E4%B8%A4%E4%B8%AA%E7%89%B9%E4%BE%8B)
[一般分析](https://kexue.fm/kexue.fm#%E4%B8%80%E8%88%AC%E5%88%86%E6%9E%90)
[文章小结](https://kexue.fm/kexue.fm#%E6%96%87%E7%AB%A0%E5%B0%8F%E7%BB%93)

### 智能搜索

支持整句搜索！网站自动使用 [结巴分词](https://github.com/fxsjy/jieba) 进行分词，并结合ngrams排序算法给出合理的搜索结果。

### 热门标签

[生成模型](https://kexue.fm/tag/%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B/) [attention](https://kexue.fm/tag/attention/) [优化](https://kexue.fm/tag/%E4%BC%98%E5%8C%96/) [语言模型](https://kexue.fm/tag/%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/) [模型](https://kexue.fm/tag/%E6%A8%A1%E5%9E%8B/) [网站](https://kexue.fm/tag/%E7%BD%91%E7%AB%99/) [概率](https://kexue.fm/tag/%E6%A6%82%E7%8E%87/) [梯度](https://kexue.fm/tag/%E6%A2%AF%E5%BA%A6/) [矩阵](https://kexue.fm/tag/%E7%9F%A9%E9%98%B5/) [转载](https://kexue.fm/tag/%E8%BD%AC%E8%BD%BD/) [优化器](https://kexue.fm/tag/%E4%BC%98%E5%8C%96%E5%99%A8/) [微分方程](https://kexue.fm/tag/%E5%BE%AE%E5%88%86%E6%96%B9%E7%A8%8B/) [分析](https://kexue.fm/tag/%E5%88%86%E6%9E%90/) [天象](https://kexue.fm/tag/%E5%A4%A9%E8%B1%A1/) [深度学习](https://kexue.fm/tag/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/) [积分](https://kexue.fm/tag/%E7%A7%AF%E5%88%86/) [python](https://kexue.fm/tag/python/) [力学](https://kexue.fm/tag/%E5%8A%9B%E5%AD%A6/) [无监督](https://kexue.fm/tag/%E6%97%A0%E7%9B%91%E7%9D%A3/) [扩散](https://kexue.fm/tag/%E6%89%A9%E6%95%A3/) [几何](https://kexue.fm/tag/%E5%87%A0%E4%BD%95/) [节日](https://kexue.fm/tag/%E8%8A%82%E6%97%A5/) [生活](https://kexue.fm/tag/%E7%94%9F%E6%B4%BB/) [文本生成](https://kexue.fm/tag/%E6%96%87%E6%9C%AC%E7%94%9F%E6%88%90/) [数论](https://kexue.fm/tag/%E6%95%B0%E8%AE%BA/)

### 随机文章

- [Lamost被冠名为“郭守敬望远镜”](https://kexue.fm/archives/604)
- [从一个单位向量变换到另一个单位向量的正交矩阵](https://kexue.fm/archives/8453)
- [澳洲恐龙洞穴揭示气候变化](https://kexue.fm/archives/19)
- [【理解黎曼几何】8\. 处处皆几何 (力学几何化)](https://kexue.fm/archives/4046)
- [OCR技术浅探：2. 背景与假设](https://kexue.fm/archives/3781)
- [冥王星呀，你究竟是什么？](https://kexue.fm/archives/43)
- [Transformer升级之路：15、Key归一化助力长度外推](https://kexue.fm/archives/9859)
- [SimBERTv2来了！融合检索和生成的RoFormer-Sim模型](https://kexue.fm/archives/8454)
- [一维弹簧的运动（上）](https://kexue.fm/archives/2430)
- [2009诺贝尔奖揭晓时间表](https://kexue.fm/archives/155)

### 最近评论

- [出售公众号吗](https://kexue.fm/archives/11390/comment-page-1#comment-28816): 您好，请问您出售停更的公众号吗，有出售意向可以加v：Followmeinsh
- [pki](https://kexue.fm/archives/10739/comment-page-2#comment-28815): 苏老师您好，weightdecay=0 也不需要重新调整吗
- [pang](https://kexue.fm/archives/10862/comment-page-1#comment-28814): 还有一个蛮有意思的想法想跟您讨论下，请问您怎么看目前一些端侧模型中的embedding和lmh...
- [zgz](https://kexue.fm/archives/8791/comment-page-1#comment-28813): \[comment=25121\]苏剑林\[/comment\]我理解在模型训练完后要计算某一个样本的...
- [Cuddle](https://kexue.fm/archives/8265/comment-page-8#comment-28812): 苏神，在 linear-attention的应用场景中，如果去掉分母中的 rope好像需要多一...
- [pang](https://kexue.fm/archives/10862/comment-page-1#comment-28811): 好的，感谢您
- [danyao12](https://kexue.fm/archives/11371/comment-page-1#comment-28805): "理想的根治办法是Stochastic Rounding，也就是依概率向上/向下舍入，这样最大...
- [wade](https://kexue.fm/archives/5977/comment-page-3#comment-28804): 那公式18，是缺少了，$-\\frac{1}{2} \\left \\\| u \\right \\\| ...
- [李双良](https://kexue.fm/archives/11280/comment-page-1#comment-28802): 你好，公式23中分母的H对角线元素求和的因子为什么只有（1−β^2）了，νi^2β^2为什么删...
- [ljj](https://kexue.fm/archives/10542/comment-page-1#comment-28801): 博主您好，我在您的另一篇文章中（https://spaces.ac.cn/archives/5...

### 友情链接

- [Cool Papers](https://papers.cool)
- [数学研发](https://bbs.emath.ac.cn)
- [Seatop](http://www.seatop.com.cn/)
- [Xiaoxia](https://xiaoxia.org/)
- [积分表-网络版](https://kexue.fm/sci/integral/index.html)
- [丝路博傲](http://blog.dvxj.com/)
- [数学之家](http://www.2math.cn/)
- [有趣天文奇观](http://interesting-sky.china-vo.org/)
- [TwistedW](http://www.twistedwg.com/)
- [godweiyang](https://godweiyang.com/)
- [AI柠檬](https://blog.ailemon.net/)
- [王登科-DK博客](https://greatdk.com)
- [ESON](https://blog.eson.org/)
- [枫之羽](https://fzhiy.net/)
- [Mathor's blog](https://wmathor.com/)
- [coding-zuo](https://coding-zuo.github.io/)
- [博科园](https://www.bokeyuan.net/)
- [孔皮皮的博客](https://www.kppkkp.top/)
- [运鹏的博客](https://yunpengtai.top/)
- [jiming.site](https://jiming.site/)
- [OmegaXYZ](https://www.omegaxyz.com/)
- [EAI猩球](https://www.robotech.ink/)
- [文举的博客](https://liwenju0.com/)
- [申请链接](https://kexue.fm/links.html)

本站采用创作共用版权协议，要求署名、非商业用途和保持一致。转载本站内容必须也遵循“ [署名-非商业用途-保持一致](http://creativecommons.org/licenses/by-nc-nd/2.5/cn/)”的创作共用协议。
© 2009-2025 Scientific Spaces. All rights reserved. Theme by [laogui](http://www.laogui.com). Powered by [Typecho](http://typecho.org). 备案号: [粤ICP备09093259号-1/2](https://beian.miit.gov.cn/)。