![MobileSideBar](https://kexue.fm/usr/themes/geekg/images/slide-button.png)

## SEARCH

## MENU

- [打赏](https://kexue.fm/reward.html)
- [公式](https://kexue.fm/latex.html)
- [天象](https://kexue.fm/ac.html)
- [链接](https://kexue.fm/links.html)
- [时光](https://kexue.fm/me.html)
- [博览](https://kexue.fm/science.html)
- [归档](https://kexue.fm/content.html)

## CATEGORIES

- [千奇百怪](https://kexue.fm/category/Everything)
- [天文探索](https://kexue.fm/category/Astronomy)
- [数学研究](https://kexue.fm/category/Mathematics)
- [物理化学](https://kexue.fm/category/Phy-chem)
- [信息时代](https://kexue.fm/category/Big-Data)
- [生物自然](https://kexue.fm/category/Biology)
- [图片摄影](https://kexue.fm/category/Photograph)
- [问题百科](https://kexue.fm/category/Questions)
- [生活/情感](https://kexue.fm/category/Life-Feeling)
- [资源共享](https://kexue.fm/category/Resources)

## NEWPOSTS

- [通向概率分布之路：盘点Softma...](https://kexue.fm/archives/10145)
- [重温SSM（二）：HiPPO的一些...](https://kexue.fm/archives/10137)
- [Transformer升级之路：1...](https://kexue.fm/archives/10122)
- [重温SSM（一）：线性系统和HiP...](https://kexue.fm/archives/10114)
- [缓存与效果的极限拉扯：从MHA、M...](https://kexue.fm/archives/10091)
- [Cool Papers更新：简单搭...](https://kexue.fm/archives/10088)
- [以蒸馏的名义：“从去噪自编码器到生...](https://kexue.fm/archives/10085)
- [生成扩散模型漫谈（二十四）：少走捷...](https://kexue.fm/archives/10077)
- [生成扩散模型漫谈（二十三）：信噪比...](https://kexue.fm/archives/10055)
- [生成扩散模型漫谈（二十二）：信噪比...](https://kexue.fm/archives/10047)

## COMMENTS

- [苏剑林: 刚刷到这篇paper，它是每个像素都视为一个token，这种做...](https://kexue.fm/archives/9984/comment-page-2#comment-24552)
- [苏剑林: 谢谢，已更正。](https://kexue.fm/archives/10114/comment-page-1#comment-24551)
- [苏剑林: 感谢提醒。Softmax Bottleneck有所耳闻，但我个...](https://kexue.fm/archives/10145/comment-page-1#comment-24550)
- [苏剑林: Chrome和Safari测试正常，暂时无法测试所有浏览器，抱歉。](https://kexue.fm/archives/9164/comment-page-3#comment-24549)
- [苏剑林: 哦，$n$是$s\_i$的总个数。前面有个笔误，现在更正了（$i...](https://kexue.fm/archives/9812/comment-page-1#comment-24548)
- [苏剑林: 可以，但一来比较费token，二来其实我不大希望通过作者、机构...](https://kexue.fm/archives/9978/comment-page-1#comment-24547)
- [苏剑林: $p(z)$是高斯分布，$p(x\|z)$是条件高斯分布，不意味...](https://kexue.fm/archives/9164/comment-page-3#comment-24546)
- [苏剑林: 你的后半段我没看懂。固定的傅立叶基也只适用于固定区间，因为傅立...](https://kexue.fm/archives/10114/comment-page-1#comment-24545)
- [Liuertong: 苏神好，今天 meta发布了一篇直接用 vanilla tr...](https://kexue.fm/archives/9984/comment-page-2#comment-24544)
- [巡星: 写得真好！一个小typo：“代入第二个公式(17)得到”应为“...](https://kexue.fm/archives/10114/comment-page-1#comment-24543)

## USERLOGIN

- [登录](https://kexue.fm/admin/login.php)

[科学空间\|Scientific Spaces](https://kexue.fm)

- [登录](https://kexue.fm/admin/login.php)
- [打赏](https://kexue.fm/reward.html)
- [公式](https://kexue.fm/latex.html)
- [天象](https://kexue.fm/ac.html)
- [链接](https://kexue.fm/links.html)
- [时光](https://kexue.fm/me.html)
- [博览](https://kexue.fm/science.html)
- [归档](https://kexue.fm/content.html)

渴望成为一个小飞侠

- [![](https://kexue.fm/usr/themes/geekg/images/rss.png)\
\
欢迎订阅](https://kexue.fm/feed)
- [![](https://kexue.fm/usr/themes/geekg/images/mail.png)\
\
个性邮箱](https://kexue.fm/archives/119)
- [![](https://kexue.fm/usr/themes/geekg/images/Saturn.png)\
\
天象信息](https://kexue.fm/ac.html)
- [![](https://kexue.fm/usr/themes/geekg/images/iss.png)\
\
观测ISS](https://kexue.fm/archives/41)
- [![](https://kexue.fm/usr/themes/geekg/images/pi.png)\
\
LaTeX](https://kexue.fm/latex.html)
- [![](https://kexue.fm/usr/themes/geekg/images/mlogo.png)\
\
关于博主](https://kexue.fm/me.html)

欢迎访问“科学空间”，这里将与您共同探讨自然科学，回味人生百态；也期待大家的分享～

- [**千奇百怪** Everything](https://kexue.fm/category/Everything)
- [**天文探索** Astronomy](https://kexue.fm/category/Astronomy)
- [**数学研究** Mathematics](https://kexue.fm/category/Mathematics)
- [**物理化学** Phy-chem](https://kexue.fm/category/Phy-chem)
- [**信息时代** Big-Data](https://kexue.fm/category/Big-Data)
- [**生物自然** Biology](https://kexue.fm/category/Biology)
- [**图片摄影** Photograph](https://kexue.fm/category/Photograph)
- [**问题百科** Questions](https://kexue.fm/category/Questions)
- [**生活/情感** Life-Feeling](https://kexue.fm/category/Life-Feeling)
- [**资源共享** Resources](https://kexue.fm/category/Resources)

- [**千奇百怪**](https://kexue.fm/category/Everything)
- [**天文探索**](https://kexue.fm/category/Astronomy)
- [**数学研究**](https://kexue.fm/category/Mathematics)
- [**物理化学**](https://kexue.fm/category/Phy-chem)
- [**信息时代**](https://kexue.fm/category/Big-Data)
- [**生物自然**](https://kexue.fm/category/Biology)
- [**图片摄影**](https://kexue.fm/category/Photograph)
- [**问题百科**](https://kexue.fm/category/Questions)
- [**生活/情感**](https://kexue.fm/category/Life-Feeling)
- [**资源共享**](https://kexue.fm/category/Resources)

[首页](https://kexue.fm) [信息时代](https://kexue.fm/category/Big-Data) ON-LSTM：用有序神经元表达层次结构

28May

# [ON-LSTM：用有序神经元表达层次结构](https://kexue.fm/archives/6621)

By 苏剑林 \|
2019-05-28 \|
169270位读者\|

今天介绍一个有意思的LSTM变种：ON-LSTM，其中“ON”的全称是“Ordered Neurons”，即有序神经元，换句话说这种LSTM内部的神经元是经过特定排序的，从而能够表达更丰富的信息。ON-LSTM来自文章 [《Ordered Neurons: Integrating Tree Structures into Recurrent Neural Networks》](https://papers.cool/arxiv/1810.09536)，顾名思义，将神经元经过特定排序是为了将层级结构（树结构）整合到LSTM中去，从而允许LSTM能自动学习到层级结构信息。这篇论文还有另一个身份：ICLR 2019的两篇最佳论文之一，这表明在神经网络中融合层级结构（而不是纯粹简单地全向链接）是很多学者共同感兴趣的课题。

笔者留意到ON-LSTM是因为 [机器之心的介绍](https://mp.weixin.qq.com/s/vIL-bKHZK-6eXZYWxrc9vw)，里边提到它除了提高了语言模型的效果之外，甚至还可以无监督地学习到句子的句法结构！正是这一点特性深深吸引了我，而它最近获得ICLR 2019最佳论文的认可，更是坚定了我要弄懂它的决心。认真研读、推导了差不多一星期之后，终于有点眉目了，遂写下此文。

在正式介绍ON-LSTM之后，我忍不住要先吐槽一下这篇文章实在是写得太差了，将一个明明很生动形象的设计，讲得异常晦涩难懂，其中的核心是$\\tilde{f}\_t$和$\\tilde{i}\_t$的定义，文中几乎没有任何铺垫就贴了出来，也没有多少诠释，开始的读了好几次仍然像天书一样...总之，文章写法实在不敢恭维～

## 背景内容 [\#](https://kexue.fm/archives/6621\#%E8%83%8C%E6%99%AF%E5%86%85%E5%AE%B9)

通常来说，在文章的前半部分，都是要先扯一些背景知识的～

### 回顾LSTM [\#](https://kexue.fm/archives/6621\#%E5%9B%9E%E9%A1%BELSTM)

首先来回顾一下普通的LSTM。用常见的记号，普通的LSTM写为：

\\begin{equation}\\begin{aligned} f\_{t} & = \\sigma \\left( W\_{f} x\_{t} + U\_{f} h\_{t - 1} + b\_{f} \\right) \\\

i\_{t} & = \\sigma \\left( W\_{i} x\_{t} + U\_{i} h\_{t - 1} + b\_{i} \\right) \\\

o\_{t} & = \\sigma \\left( W\_{o} x\_{t} + U\_{o} h\_{t - 1} + b\_{o} \\right) \\\

\\hat{c}\_t & = \\tanh \\left( W\_{c} x\_{t} + U\_{c} h\_{t - 1} + b\_{c} \\right)\\\

c\_{t} & = f\_{t} \\circ c\_{t - 1} + i\_{t} \\circ \\hat{c}\_t \\\

h\_{t} & = o\_{t} \\circ \\tanh \\left( c\_{t} \\right)\\end{aligned}\\label{eq:lstm}\\end{equation}

如果熟悉了神经网络本身，其实这样的结构没有什么神秘的，$f\_t,i\_t,o\_t$就是三个单层全连接模型，输入是历史信息$h\_{t-1}$和当前信息$x\_t$，用sigmoid激活，因为sigmoid的结果在0～1之间，所以它们的含义可以诠释为“门（gate）”，分别称为遗忘门、输入门、输出门。不过我个人觉着gate这个名字是不够贴切的，“valve（阀门）”也许更贴切些。

有了门之后，$x\_t$被整合为$\\hat{c}\_t$，然后通过$\\circ$运算（对应逐位相乘，有时候也记为$\\otimes$）与前面的“门”结合起来，来对$c\_{t-1}$和$\\hat{c}\_t$进行加权求和。

下面是自己画的一个LSTM的示意图：

[![LSTM运算流程示意图](https://kexue.fm/usr/uploads/2019/05/203965127.png)](https://kexue.fm/usr/uploads/2019/05/203965127.png)

LSTM运算流程示意图

### 语言和序信息 [\#](https://kexue.fm/archives/6621\#%E8%AF%AD%E8%A8%80%E5%92%8C%E5%BA%8F%E4%BF%A1%E6%81%AF)

在常见的神经网络中，神经元通常都是无序的，比如遗忘门$f\_t$是一个向量，向量的各个元素的位置没有什么规律。如果把LSTM运算过程中涉及到的所有向量的位置按照同一方式重新打乱，权重的顺序也相应地打乱，然后输出结果可以只是原来向量的重新排序（考虑多层的情况下，甚至可以完全不变），信息量不变，不影响后续网络对它的使用。

换言之，LSTM以及普通的神经网络都没有用到神经元的序信息，ON-LSTM则试图把这些神经元排个序，并且用这个序来表示一些特定的结构，从而把神经元的序信息利用起来。

ON-LSTM的思考对象是自然语言。一个自然句子通常能表示为一些层级结构，这些结构如果人为地抽象出来，就是我们所说的句法信息，而ON-LSTM希望能够模型在训练的过程中自然地学习到这种层级结构，并且训练完成后还能把它解析出来（可视化），这就利用到了前面说的神经元的序信息。（曾经做过的相关研究 [《最小熵原理（三）：“飞象过河”之句模版和语言结构》](https://kexue.fm/archives/5577)）

为了达到这个目标，我们需要有一个层级的概念，层级越低代表语言中颗粒度越小的结构，而层级越高则代表颗粒度越粗的结构，比如在中文句子中，“字”可以认为是最低层级的结构，词次之，再上面是词组、短语等。层级越高，颗粒度越粗，那么它在句子中的跨度就越大。

用原文的图示就是：

[![层级结构导致了不同模型的不同跨度，也就是不同层级信息的传输距离不一样，这最终将引导我们将层级结构进行矩阵化表示，从而融入到神经网络中。](https://kexue.fm/usr/uploads/2019/05/373971065.png)](https://kexue.fm/usr/uploads/2019/05/373971065.png)

层级结构导致了不同模型的不同跨度，也就是不同层级信息的传输距离不一样，这最终将引导我们将层级结构进行矩阵化表示，从而融入到神经网络中。

## ON-LSTM [\#](https://kexue.fm/archives/6621\#ON-LSTM)

最后一句“层级越高，颗粒度越粗，那么它在句子中的跨度就越大”看起来是废话，但它对于ON-LSTM的设计有着指导作用。首先，这要求我们在设计ON-LSTM的编码时能区分高低层级的信息；其次，这也告诉我们，高层级的信息意味着它要在高层级对应的编码区间保留更久（不那么容易被遗忘门过滤掉），而低层级的信息则意味着它在对应的区间更容易被遗忘。

### 设计：分区间更新 [\#](https://kexue.fm/archives/6621\#%E8%AE%BE%E8%AE%A1%EF%BC%9A%E5%88%86%E5%8C%BA%E9%97%B4%E6%9B%B4%E6%96%B0)

有了这个指导之后，我们可以着手建立。假设ON-LSTM中的神经元都排好序后，向量$c\_t$的index越小的元素，表示越低层级的信息，而index越大的元素，则表示越高层级的信息。然后，ON-LSTM的门结构和输出结构依然和普通的LSTM一样：

\\begin{equation}\\begin{aligned} f\_{t} & = \\sigma \\left( W\_{f} x\_{t} + U\_{f} h\_{t - 1} + b\_{f} \\right) \\\

i\_{t} & = \\sigma \\left( W\_{i} x\_{t} + U\_{i} h\_{t - 1} + b\_{i} \\right) \\\

o\_{t} & = \\sigma \\left( W\_{o} x\_{t} + U\_{o} h\_{t - 1} + b\_{o} \\right) \\\

\\hat{c}\_t & = \\tanh \\left( W\_{c} x\_{t} + U\_{c} h\_{t - 1} + b\_{c} \\right)\\\

h\_{t} & = o\_{t} \\circ \\tanh \\left( c\_{t} \\right) \\end{aligned}\\label{eq:update-o0}\\end{equation}

不同的是从$\\hat{c}\_t$到$c\_t$的更新机制不一样。

接下来，初始化一个全零的$c\_t$，即没有任何记忆，或者想象为一个空的U盘。然后，我们将历史信息和当前输入按一定规律存入到$c\_t$中（即更新$c\_t$）。每次在更新$c\_t$之前，首先预测两个整数$d\_f$和$d\_i$，分别表示历史信息$h\_{t-1}$和当前输入$x\_t$的层级：

\\begin{equation}\\begin{aligned} d\_f = F\_1\\left(x\_t, h\_{t-1}\\right) \\\ d\_i = F\_2\\left(x\_t, h\_{t-1}\\right) \\end{aligned}\\end{equation}

至于$F\_1, F\_2$的具体结构，我们后面再补充，先把核心思路讲清楚。这便是我不满原论文写作的原因，一上来就定义$\\text{cumax}$，事前事后都没能把思想讲清楚。

有了$d\_f,d\_i$之后，那么有两种可能：

**1、$d\_f \\leq d\_i$，这意味着当前输入$x\_t$的层级要高于历史记录$h\_{t-1}$的层级，那就是说，两者之间的信息流有交汇，当前输入信息要整合到高于等于$d\_f$的层级中**，方法是：

\\begin{equation}\\begin{aligned} c\_t = \\begin{pmatrix}\\hat{c}\_{t,< d\_f} \\\

f\_{t,\[d\_f:d\_i\]}\\circ c\_{t-1,\[d\_f:d\_i\]} + i\_{t,\[d\_f:d\_i\]}\\circ \\hat{c}\_{t,\[d\_f:d\_i\]} \\\

c\_{t-1,> d\_i}

\\end{pmatrix}\\end{aligned}\\label{eq:update-o1}\\end{equation}

这个公式是说，由于当前输入层级更高，它影响到了交集$\[d\_f, d\_i\]$的部分，这部分由普通的LSTM更新公式来更新，小于$d\_f$的部分，直接覆盖为当前输入$\\hat{c}\_t$对应的部分，大于$d\_i$的部分，保持历史记录$c\_{t-1}$对应的部分不变。

这个更新公式是符合直觉的，因为我们已经将神经元排好序了，位置越前的神经元储存越低层结构信息。而对于当前输入来说，显然更容易影响低层信息，所以当前输入的“波及”范围是$\[0, d\_i\]$（自下而上），也可以理解为当前输入所需要的储蓄空间就是$\[0, d\_i\]$；而对于历史记录来说，它保留的是高层信息，所以“波及”范围是$\[d\_f, d\_{\\max}\]$（自上而下，$d\_{\\max}$是最高层级），或许说历史信息所需要的储蓄空间是$\[d\_f, d\_{\\max}\]$。在不相交部分，它们“各自为政”，各自保留自己的信息；在相交部分，信息要进行融合，退化为普通的LSTM。

[![ON-LSTM设计图。主要想法是将LSTM的神经元排序，然后分段更新。](https://kexue.fm/usr/uploads/2019/05/524726490.png)](https://kexue.fm/usr/uploads/2019/05/524726490.png)

ON-LSTM设计图。主要想法是将LSTM的神经元排序，然后分段更新。

**2、$d\_f > d\_i$，这意味着历史记录$h\_{t-1}$和当前输入$x\_t$互不相交，那么对于$(d\_i, d\_f)$的区间“无人问津”，所以只好保持初始状态（即全零，可以理解为没有东西写入）；而剩下部分，当前输入直接把自己的信息写入到$\[0, d\_i\]$区间，而历史信息直接写入到$\[d\_f, d\_{\\max}\]$区间。这种情况下，当前输入和历史信息合并起来都不能占满整个储蓄空间，从而空下了一些剩余容易（中间那部分全零）。**：

\\begin{equation}\\begin{aligned} c\_t = \\begin{pmatrix}\\hat{c}\_{t,\\leq d\_i} \\\

0\_{(d\_i : d\_f)} \\\

c\_{t-1,\\geq d\_f}

\\end{pmatrix}\\end{aligned}\\label{eq:update-o2}\\end{equation}

其中$(d\_i : d\_f)$表示大于$d\_i$、小于$d\_f$的区间；而前面的$\[d\_f : d\_i\]$表示大于等于$d\_f$、小于等于$d\_i$的区间。

至此，我们能够理解ON-LSTM的基本原理了，它将神经元排序之后，通过位置的前后来表示信息层级的高低，然后在更新神经元时，先分别预测历史的层级$d\_f$和输入的层级$d\_i$，通过这两个层级来对神经元实行分区间更新。

[![ON-LSTM分区间更新图示。图上数字都是随机生成的，最上为历史信息，最下为当前输入，中间为当前整合的输出。最上方黄色部分为历史信息层级（主遗忘门），最下方绿色部分为输入信息层级（主输入门），中间部分黄色则是直接复制的历史信息，绿色则是直接复制的输入信息，紫色是按照LSTM方式融合的交集信息，白色是互不相关的“空白地带”。从历史信息（最上层黄色部分）的复制传递中，我们就可以析出对应的如右图的层次结构（注意右图的层次结构与作图的过程不是精确对应的，仅作粗略示意，读者应侧重对图像与模型的直观感知而不是细抠对应关系）。](https://kexue.fm/usr/uploads/2019/06/956027511.png)](https://kexue.fm/usr/uploads/2019/06/956027511.png)

ON-LSTM分区间更新图示。图上数字都是随机生成的，最上为历史信息，最下为当前输入，中间为当前整合的输出。最上方黄色部分为历史信息层级（主遗忘门），最下方绿色部分为输入信息层级（主输入门），中间部分黄色则是直接复制的历史信息，绿色则是直接复制的输入信息，紫色是按照LSTM方式融合的交集信息，白色是互不相关的“空白地带”。从历史信息（最上层黄色部分）的复制传递中，我们就可以析出对应的如右图的层次结构（注意右图的层次结构与作图的过程不是精确对应的，仅作粗略示意，读者应侧重对图像与模型的直观感知而不是细抠对应关系）。

**这样一来，高层信息就可能保留相当长的距离（因为高层直接复制历史信息，导致历史信息可能不断被复制而不改变），而低层信息在每一步输入时都可能被更新（因为低层直接复制输入，而输入是不断改变的），所以就通过信息分级来嵌入了层级结构。更通俗地说就是分组更新，更高的组信息传得更远（跨度更大），更低的组跨度更小，这些不同的跨度就形成了输入序列的层级结构。**

**（请反复阅读这段话，必要时对照上图，直接完全理解为止，这段话称得上是ON-LSTM的设计总纲。）**

### 成型：分段软化 [\#](https://kexue.fm/archives/6621\#%E6%88%90%E5%9E%8B%EF%BC%9A%E5%88%86%E6%AE%B5%E8%BD%AF%E5%8C%96)

现在要解决的问题就是，这两个层级怎么预测，即$F\_1, F\_2$怎么构建。用一个模型来输出一个整数不难，但是这样的模型通常都是不可导的，无法很好地整合到整个模型进行反向传播，所以，更好的方案是进行“软化”，即寻求一些光滑近似。

为了进行软化，我们先对$\\eqref{eq:update-o1},\\eqref{eq:update-o2}$进行改写。引入记号$1\_k$，它表示第$k$位为1、其他都为0的$d\_{\\max}$维向量（即one hot向量），那么$\\eqref{eq:update-o1},\\eqref{eq:update-o2}$可以统一地写为

\\begin{equation}\\begin{aligned}\\tilde{f}\_t & = \\stackrel{\\rightarrow}{\\text{cs}}\\left(1\_{d\_f}\\right), \\quad \\tilde{i}\_t = \\stackrel{\\leftarrow}{\\text{cs}}\\left(1\_{d\_i}\\right) \\\

\\omega\_t & = \\tilde{f}\_t \\circ \\tilde{i}\_t \\quad (\\text{用来表示交集})\\\

c\_t & = \\underbrace{\\omega\_t \\circ \\left(f\_{t} \\circ c\_{t - 1} + i\_{t} \\circ \\hat{c}\_t \\right)}\_{\\text{交集部分}} + \\underbrace{\\left(\\tilde{f}\_t - \\omega\_t\\right)\\circ c\_{t - 1}}\_{\\text{大于}\\max\\left(d\_f, d\_i\\right)\\text{的部分}} + \\underbrace{\\left(\\tilde{i}\_t - \\omega\_t\\right)\\circ \\hat{c}\_{t}}\_{\\text{小于}\\min\\left(d\_f,d\_i\\right)\\text{的部分}}

\\end{aligned}\\label{eq:update-o3}\\end{equation}

其中$\\stackrel{\\rightarrow}{\\text{cs}}$/$\\stackrel{\\leftarrow}{\\text{cs}}$分别是右向/左向的cumsum操作：

\\begin{equation}\\begin{aligned}\\stackrel{\\rightarrow}{\\text{cs}}(\[x\_1,x\_2,\\dots,x\_n\]) & = \[x\_1, x\_1+x\_2, \\dots,x\_1+x\_2+\\dots+x\_n\]\\\

\\stackrel{\\leftarrow}{\\text{cs}}(\[x\_1,x\_2,\\dots,x\_n\]) & = \[x\_1+x\_2+\\dots+x\_n,\\dots,x\_n+x\_{n-1},x\_n\]\\end{aligned}\\end{equation}

注意，这里指的是$\\eqref{eq:update-o3}$所给出的结果，跟$\\eqref{eq:update-o1},\\eqref{eq:update-o2}$分情况给出的结果，是 **完全等价** 的。这只需要留意到$\\tilde{f}\_t$给出了一个从$d\_f$位开始后面全是1、其他位全是0的$d\_{\\max}$维向量，而$\\tilde{i}\_t$给出了一个从0到$d\_i$位全是1、其他位全是0的$d\_{\\max}$维向量，那么$\\omega\_t = \\tilde{f}\_t \\circ \\tilde{i}\_t$正好给出了交集部分为1、其余全是0的向量（如果没有交集，那就是全0向量），所以$\\omega\_t \\circ \\left(f\_{t} \\circ c\_{t - 1} + i\_{t} \\circ \\hat{c}\_t \\right)$这部分就是在处理交集部分；而$\\left(\\tilde{f}\_t - \\omega\_t\\right)$得到一个从$\\max\\left(d\_f,d\_i\\right)$位开始后面全是1、其他位全是0的$d\_{\\max}$维向量，正好标记了历史信息的范围$\[d\_f, d\_{\\max}\]$去掉了交集之后的部分；而$\\left(\\tilde{i}\_t - \\omega\_t\\right)$得到一个从$0\\sim \\min\\left(d\_f,d\_i\\right)$位全是1、其他位全是0的$d\_{\\max}$维向量，正好标记了当前输入的范围$\[0, d\_i\]$去掉了交集之后的部分。

现在，$c\_t$的更新公式由式$\\eqref{eq:update-o3}$来描述，两个one hot向量$1\_{d\_f},1\_{d\_i}$由两个整数$d\_f,d\_i$决定，而这两个整数本身是由模型$F\_1, F\_2$预测出来的，所以我们可以干脆直接用模型预测$1\_{d\_f},1\_{d\_i}$就是了。当然，就算预测出来两个one hot向量，也没有改变整个更新过程不可导的事实。但是，我们可以考虑将$1\_{d\_f},1\_{d\_i}$用一般的浮点数向量来代替，比如：

\\begin{equation}\\begin{aligned}1\_{d\_f}\\approx& softmax\\left( W\_{\\tilde{f}} x\_{t} + U\_{\\tilde{f}} h\_{t - 1} + b\_{\\tilde{f}} \\right)\\\

1\_{d\_i}\\approx& softmax\\left( W\_{\\tilde{i}} x\_{t} + U\_{\\tilde{i}} h\_{t - 1} + b\_{\\tilde{i}} \\right)

\\end{aligned}\\end{equation}

这样一来，我们用一个$h\_{t-1}$和$x\_t$的全连接层，来预测两个向量并且做$softmax$，就可以作为$1\_{d\_f},1\_{d\_i}$的近似，并且它是完全可导的，从而我们将它们取代$1\_{d\_f},1\_{d\_i}$代入到$\\eqref{eq:update-o3}$中，就得到ON-LSTM的$c\_t$的更新公式：

\\begin{equation}\\begin{aligned}\\tilde{f}\_t & = \\stackrel{\\rightarrow}{\\text{cs}}\\left(softmax\\left( W\_{\\tilde{f}} x\_{t} + U\_{\\tilde{f}} h\_{t - 1} + b\_{\\tilde{f}} \\right)\\right)\\\

\\tilde{i}\_t & = \\stackrel{\\leftarrow}{\\text{cs}}\\left(softmax\\left( W\_{\\tilde{i}} x\_{t} + U\_{\\tilde{i}} h\_{t - 1} + b\_{\\tilde{i}} \\right)\\right) \\\

\\omega\_t & = \\tilde{f}\_t \\circ \\tilde{i}\_t \\quad (\\text{用来表示交集})\\\

c\_t & = \\underbrace{\\omega\_t \\circ \\left(f\_{t} \\circ c\_{t - 1} + i\_{t} \\circ \\hat{c}\_t \\right)}\_{\\text{交集部分}} + \\underbrace{\\left(\\tilde{f}\_t - \\omega\_t\\right)\\circ c\_{t - 1}}\_{\\text{大于}\\max\\left(d\_f, d\_i\\right)\\text{的部分}} + \\underbrace{\\left(\\tilde{i}\_t - \\omega\_t\\right)\\circ \\hat{c}\_{t}}\_{\\text{小于}\\min\\left(d\_f,d\_i\\right)\\text{的部分}}

\\end{aligned}\\end{equation}

把剩余部分（即$\\eqref{eq:update-o0}$）也写在一起，整个ON-LSTM的更新公式就是：

\\begin{equation}\\begin{aligned} f\_{t} & = \\sigma \\left( W\_{f} x\_{t} + U\_{f} h\_{t - 1} + b\_{f} \\right) \\\

i\_{t} & = \\sigma \\left( W\_{i} x\_{t} + U\_{i} h\_{t - 1} + b\_{i} \\right) \\\

o\_{t} & = \\sigma \\left( W\_{o} x\_{t} + U\_{o} h\_{t - 1} + b\_{o} \\right) \\\

\\hat{c}\_t & = \\tanh \\left( W\_{c} x\_{t} + U\_{c} h\_{t - 1} + b\_{c} \\right)\\\

\\tilde{f}\_t & = \\stackrel{\\rightarrow}{\\text{cs}}\\left(softmax\\left( W\_{\\tilde{f}} x\_{t} + U\_{\\tilde{f}} h\_{t - 1} + b\_{\\tilde{f}} \\right)\\right)\\\

\\tilde{i}\_t & = \\stackrel{\\leftarrow}{\\text{cs}}\\left(softmax\\left( W\_{\\tilde{i}} x\_{t} + U\_{\\tilde{i}} h\_{t - 1} + b\_{\\tilde{i}} \\right)\\right) \\\

\\omega\_t & = \\tilde{f}\_t \\circ \\tilde{i}\_t\\\

c\_t & = \\omega\_t \\circ \\left(f\_{t} \\circ c\_{t - 1} + i\_{t} \\circ \\hat{c}\_t \\right) + \\left(\\tilde{f}\_t - \\omega\_t\\right)\\circ c\_{t - 1} + \\left(\\tilde{i}\_t - \\omega\_t\\right)\\circ \\hat{c}\_{t}\\\

h\_{t} & = o\_{t} \\circ \\tanh \\left( c\_{t} \\right)\\end{aligned}\\end{equation}

示意图如下。对比LSTM的$\\eqref{eq:lstm}$，就可以发现主要改动在哪了。其中新引入的$\\tilde{f}\_t$和$\\tilde{i}\_t$被作者称为“主遗忘门（master forget gate）”和“主输入门（master input gate）”。

[![ON-LSTM运算流程示意图。主要是将分段函数用cumax光滑化变成可导。](https://kexue.fm/usr/uploads/2019/05/1083836927.png)](https://kexue.fm/usr/uploads/2019/05/1083836927.png)

ON-LSTM运算流程示意图。主要是将分段函数用cumax光滑化变成可导。

> **注：**
>
> 1、论文中将$\\stackrel{\\rightarrow}{\\text{cs}}(softmax(x))$简记为$\\text{cumax}(x)$，这只是记号上的转换而已；
>
> 2、作为数列来看，$\\hat{f}\_t$是一个单调递增的数列，而$\\hat{i}\_t$是一个单调递减的数列；
>
> 3、对于$\\tilde{i}\_t$，论文定义为
>
> \\begin{equation}1-\\text{cumax}\\left( W\_{\\tilde{i}} x\_{t} + U\_{\\tilde{i}} h\_{t - 1} + b\_{\\tilde{i}} \\right)\\end{equation}
>
> 这个选择会产生类似的单调递减的向量，一般情况下没有什么差别，但从对称性的角度来看，我认为我的选择更合理一些。

## 实验与思考 [\#](https://kexue.fm/archives/6621\#%E5%AE%9E%E9%AA%8C%E4%B8%8E%E6%80%9D%E8%80%83)

下面简单汇总一下ON-LSTM的实验，其中包括原作者的实现（PyTroch）以及笔者自己的复现（Keras），最后谈及笔者对此ON-LSTM的一些思考。

**作者实现：** [https://github.com/yikangshen/Ordered-Neurons](https://github.com/yikangshen/Ordered-Neurons)

**个人实现：** [https://github.com/bojone/on-lstm](https://github.com/bojone/on-lstm)

（限于笔者水平，个人的理解、复现可能存在问题，如果读者发现，请不吝指出，谢谢。个人复现目前只保证Python 2.7 + Tensorflow 1.8 + Keras 2.24能跑通，其他环境不保证。）

### 分组的层级 [\#](https://kexue.fm/archives/6621\#%E5%88%86%E7%BB%84%E7%9A%84%E5%B1%82%E7%BA%A7)

代表层级的向量$\\tilde{f}\_t,\\tilde{i}\_t$要与$f\_t$等做$\\circ$运算，这意味着它们的维度大小（即神经元数目）要相等。而我们知道，根据不同的需求，LSTM的隐层神经元数可以达到几百甚至几千，这意味着$\\tilde{f}\_t,\\tilde{i}\_t$所描述的层级数也有几百甚至几千。而事实上，序列的层级结构（如果存在的话）的总层级数一般不会太大，也就是说两者之间存在一点矛盾之处。

ON-LSTM的作者想了个比较合理的解决方法，假设隐层神经元数目为$n$，它可以分解为$n=pq$，那么我们可以只构造一个$p$个神经元的$\\tilde{f}\_t,\\tilde{i}\_t$，然后将$\\tilde{f}\_t,\\tilde{i}\_t$的每个神经元依次重复$q$次，这样就得到一个$n$维的$\\tilde{f}\_t,\\tilde{i}\_t$，然后再与$f\_t$等做$\\circ$运算。例如$n=6=2\\times 3$，那么先构造一个2维向量如$\[0.1, 0.9\]$，然后依次重复3次得到$\[0.1, 0.1, 0.1, 0.9, 0.9, 0.9\]$。

这样一来，我们既减少了层级的总数，同时还减少了模型的参数量，因为$p$通常可以取得比较小（比$n$小1～2个数量级），因此相比普通的LSTM，ON-LSTM并没有增加太多参数量。

### 语言模型 [\#](https://kexue.fm/archives/6621\#%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B)

作者做了若干个实验，包括语言模型、句法评价、逻辑推理等，不少实验中均达到当前最优效果，普遍超越了普通的LSTM，这证明了ON-LSTM所引入的层级结构信息是有价值的。其中我比较熟悉的也就只有语言模型了，就放一个语言模型实验的截图好了：

[![ON-LSTM原论文中的语言模型实验效果](https://kexue.fm/usr/uploads/2019/05/3484159577.png)](https://kexue.fm/usr/uploads/2019/05/3484159577.png)

ON-LSTM原论文中的语言模型实验效果

### 无监督句法 [\#](https://kexue.fm/archives/6621\#%E6%97%A0%E7%9B%91%E7%9D%A3%E5%8F%A5%E6%B3%95)

如果仅仅是在常规的一些语言任务中超过普通LSTM，那么ON-LSTM也算不上什么突破，但ON-LSTM的一个令人兴奋的特性是它能够无监督地从训练好的模型（比如语言模型）中提取输入序列的层级树结构。提取的思路如下：

首先我们考虑：

\\begin{equation}p\_f = softmax\\left( W\_{\\tilde{f}} x\_{t} + U\_{\\tilde{f}} h\_{t - 1} + b\_{\\tilde{f}} \\right)\\end{equation}

它是$\\tilde{f}\_t$在$\\stackrel{\\rightarrow}{\\text{cs}}$之前的结果，根据我们前面的推导，它就是历史信息的层级$d\_f$的一个软化版本，那么我们可以写出：

\\begin{equation}d\_f\\approx\\mathop{\\arg\\max}\_{k} p\_f(k)\\end{equation}

这里的$p\_f(k)$就是指向量$p\_f$的第$k$个元素。但是，$p\_f$中所包含的$softmax$本身就是一个“软化”后的算子，这种情况下我们可能考虑“软化”的$\\arg\\max$比较好（参考 [《函数光滑化杂谈：不可导函数的可导逼近》](https://kexue.fm/archives/6620/comment-page-1#argmax)），即

\\begin{equation}d\_f\\approx \\sum\_{k=1}^n k\\times p\_f(k)=n\\left(1 - \\frac{1}{n}\\sum\_{k=1}^n \\tilde{f}\_t(k)\\right)+1\\end{equation}

第二个等号是恒等变换，大家可以自行证明一下（原论文$(15)$式是有误的）。这样我们就得到了一个层级的计算公式了，当$n$固定时它直接取决于它$\\left(1 - \\frac{1}{n}\\sum\\limits\_{k=1}^n \\tilde{f}\_t(k)\\right)$

这样以来，我们就可以用序列

\\begin{equation}\\left\\{d\_{f,t}\\right\\}\_{t=1}^{\\text{seq\_len}}=\\left\\{\\left(1 - \\frac{1}{n}\\sum\\limits\_{k=1}^n \\tilde{f}\_t(k)\\right)\\right\\}\_{t=1}^{\\text{seq\_len}}\\end{equation}

来表示输入序列的层级变化。有了这个层级序列后，按照下述贪心算法来析出层次结构：

> 给定输入序列$\\left\\{x\_{t}\\right\\}$到预训练好的ON-LSTM，输出对应的层级序列$\\left\\{d\_{f,t}\\right\\}$，然后找出层级序列中最大值所在的下标，比如$k$，那么就将输入序列分区为$\[x\_{t < k}, \[x\_k, x\_{t > k}\]\]$。然后对子序列$x\_{t < k}$和$x\_{t > k}$重复上述步骤，直到每个子序列长度为1。

算法的大概意思是从最高层级处断开（这意味着当此处包含的历史信息最少，与前面所有内容的联系最为薄弱，最有可能是一个新的子结构的开始），然后递归处理，从而逐渐得到输入序列隐含的嵌套结构。作者是用三层的ON-LSTM训练了一个语言模型，然后用中间那层ON-LSTM的$\\tilde{f}\_t$来计算层级，然后跟标注的句法结构对比，发现准确率颇高。我自己也在中文语料下尝试了一下： [https://github.com/bojone/on-lstm/blob/master/lm\_model.py](https://github.com/bojone/on-lstm/blob/master/lm_model.py)

至于效果，因为我没做过也不了解句法分析，我也不知道怎么评价，反正好像看着是那么一回事，但是又好像不大对一样，所以各位读者自己评价好了～近一两年，无监督句法分析其实还有不少研究工作，可能要都读一读才能更深刻地理解ON-LSTM。

> **输入：苹果的颜色是什么**
>
> **输出：**
>
> \[
>
> \[
>
>     \[
>
>       '苹果',
>
>       '的'
>
>     \],
>
>     \[
>
>       '颜色',
>
>       '是'
>
>     \]
>
> \],
>
> '什么'
>
> \]
>
> **输入：爱真的需要勇气**
>
> **输出：**
>
> \[
>
> '爱',
>
> \[
>
>     '真的',
>
>     \[
>
>       '需要',
>
>       '勇气'
>
>     \]
>
> \]
>
> \]

### 思考与发散 [\#](https://kexue.fm/archives/6621\#%E6%80%9D%E8%80%83%E4%B8%8E%E5%8F%91%E6%95%A3)

文章最后，我们来一起思考几个问题。

> RNN还有研究价值？

首先，有读者可能会困惑，都9102年了，居然还有人研究RNN类模型，还有研究价值吗？近年来，BERT、GPT等基于 [Attention](https://kexue.fm/archives/4765) 和语言模型的预训练模型，在NLP的诸多任务上都提升了效果，甚至有文章直接说“RNN已死”之类的。事实上真的如此吗？我认为，RNN活得好好的，并且在将来的相当长时间内都不会死，原因至少包含下面几个：

第一，BERT之类的模型，以增加好几个数量级的算力为代价，在一些任务上提升了也就一两个百分点的效果，这样的性价比只有在学术研究和比赛刷榜才有价值，在工程上几乎没什么用（至少没法直接用）；第二，RNN类的模型本身具有一些无可比拟的优势，比如它能轻松模拟一个计数函数，在很多序列分析的场景，RNN效果好得很；第三，几乎所有seq2seq模型（哪怕是BERT中）decoder都是一种RNN，因为它们基本都是递归解码的，RNN哪会消失？

> 单向ON-LSTM就够了？

然后，读者可能会有疑惑：你要析出层级结构，但是只用了单向的ON-LSTM，这意味着当前的层级分析还不依赖于将来的输入，这显然是不大符合事实的。这个笔者也有同样的困惑，但是作者的实验表明这样做效果已经够好了，可能自然语言的整体结构都倾向于是局部的、单向的（从左往右），所以对于自然语言来说单向也就够了。

如果一般情况下是否用双向比较好呢？双向的话是不是要像BERT那样用masked language model的方式来训练呢？双向的话又怎么计算层级序列呢？这一切都还没有完整的答案。至于无监督析出的结构是不是一定就符合人类自身理解的层级结构呢？这个也说不准，因为比较没有什么监督指引，神经网络就“按照自己的方式去理解”了，而幸运的是，神经网络的“自己的方式”，似乎跟人类自身的方式有不少重叠之处。

> 为什么析出层级考虑的是$d\_f$而不是$d\_i$？

读者可能会困惑，明明有两个master gate，为什么析出层级用$d\_f$而不是$d\_i$？要回答这个问题，我们要理解$d\_f$的含义。我们说$d\_f$是历史信息的层级，换言之，它告诉我们做出当前决策还要用多少历史信息。如果$d\_f$很大，意味着当前决策几乎用不着历史信息了，这意味着从当前开始就是一个新层级的开始，与历史输入几乎割断了联系。也就是从这种割断和联系中析出了层级结构，所以只能用$d\_f$。

> 能否用到CNN或者Attention？

最后，可能想到的一个困惑是，这种设计能不能用到CNN、Attention之中呢？换句话说能不能将CNN、Attention的神经元也排个序，融入层级结构信息呢？个人感觉是有可能的，但需要重新设计，因为层级结构被假设为连续嵌套的，RNN的递归性正好可以描述了这种连续性，而CNN、Attention的非递归性导致我们很难直接去表达这种连续嵌套结构。

不管怎样，我觉得这是个值得思考的主题，有进一步的思考结果我会和大家分享，当然也欢迎读者们和我分享你的思考。

## 文章总结 [\#](https://kexue.fm/archives/6621\#%E6%96%87%E7%AB%A0%E6%80%BB%E7%BB%93)

**本文梳理了LSTM的一个新变种ON-LSTM的来龙去脉，主要突出了它在表达层级结构上的设计原理。个人感觉整体的设计还是比较巧妙和有趣的，值得细细思考一番。**

最后，学习和研究都关键是有自己的判断能力，不要人云亦云，更不能轻信媒体的“标题党”。BERT的Transformer固然有它的优势，但是LSTM等RNN模型的魅力依然不可小觑。我甚至觉得，诸如LSTM之类的RNN模型，会在将来的某天，焕发出更强烈的光彩，transformer与之相比将会相当逊色。

让我们拭目以待好了。

_**转载到请包括本文地址：** [https://kexue.fm/archives/6621](https://kexue.fm/archives/6621)_

_**更详细的转载事宜请参考：**_ [《科学空间FAQ》](https://kexue.fm/archives/6508#%E6%96%87%E7%AB%A0%E5%A6%82%E4%BD%95%E8%BD%AC%E8%BD%BD/%E5%BC%95%E7%94%A8)

**如果您还有什么疑惑或建议，欢迎在下方评论区继续讨论。**

**如果您觉得本文还不错，欢迎 [分享](https://kexue.fm/archives/6621#share)/ [打赏](https://kexue.fm/archives/6621#pay) 本文。打赏并非要从中获得收益，而是希望知道科学空间获得了多少读者的真心关注。当然，如果你无视它，也不会影响你的阅读。再次表示欢迎和感谢！**

打赏

![科学空间](https://kexue.fm/usr/themes/geekg/payment/wx.png)

微信打赏

![科学空间](https://kexue.fm/usr/themes/geekg/payment/zfb.png)

支付宝打赏

因为网站后台对打赏并无记录，因此欢迎在打赏时候备注留言。

你还可以 [**点击这里**](http://mail.qq.com/cgi-bin/qm_share?t=qm_mailme&email=tN7d1drY3drrx8H0xcWa19vZ) 或在下方评论区留言来告知你的建议或需求。

**如果您需要引用本文，请参考：**

苏剑林. (May. 28, 2019). 《ON-LSTM：用有序神经元表达层次结构 》\[Blog post\]. Retrieved from [https://kexue.fm/archives/6621](https://kexue.fm/archives/6621)

@online{kexuefm-6621,

        title={ON-LSTM：用有序神经元表达层次结构},

        author={苏剑林},

        year={2019},

        month={May},

        url={\\url{https://kexue.fm/archives/6621}},

}

分类： [信息时代](https://kexue.fm/category/Big-Data)    标签： [模型](https://kexue.fm/tag/%E6%A8%A1%E5%9E%8B/), [深度学习](https://kexue.fm/tag/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/), [无监督](https://kexue.fm/tag/%E6%97%A0%E7%9B%91%E7%9D%A3/), [NLP](https://kexue.fm/tag/NLP/)[78 评论](https://kexue.fm/archives/6621#comments)

< [函数光滑化杂谈：不可导函数的可导逼近](https://kexue.fm/archives/6620) \| [基于DGCNN和概率图的轻量级信息抽取模型](https://kexue.fm/archives/6671) >

### 你也许还对下面的内容感兴趣

- [生成扩散模型漫谈（二十三）：信噪比与大图生成（下）](https://kexue.fm/archives/10055)
- [简单得令人尴尬的FSQ：“四舍五入”超越了VQ-VAE](https://kexue.fm/archives/9826)
- [BytePiece：更纯粹、更高压缩率的Tokenizer](https://kexue.fm/archives/9752)
- [基于量子化假设推导模型的尺度定律（Scaling Law）](https://kexue.fm/archives/9607)
- [Tiger：一个“抠”到极致的优化器](https://kexue.fm/archives/9512)
- [用热传导方程来指导自监督学习](https://kexue.fm/archives/9359)
- [在bert4keras中使用混合精度和XLA加速训练](https://kexue.fm/archives/9059)
- [为什么需要残差？一个来自DeepNet的视角](https://kexue.fm/archives/8994)
- [门控注意力单元（GAU）还需要Warmup吗？](https://kexue.fm/archives/8990)
- [GPLinker：基于GlobalPointer的事件联合抽取](https://kexue.fm/archives/8926)

[发表你的看法](https://kexue.fm/archives/6621#comment_form)

1. [«](https://kexue.fm/archives/6621/comment-page-2#comments)
2. [1](https://kexue.fm/archives/6621/comment-page-1#comments)
3. [2](https://kexue.fm/archives/6621/comment-page-2#comments)
4. [3](https://kexue.fm/archives/6621/comment-page-3#comments)

[jlting](http://%E6%97%A0)

September 29th, 2019

苏老师 请问一下下面这段代码为什么要循环三次

for i in range(3):

onlstm = ONLSTM(word\_size, num\_levels, return\_sequences=True, dropconnect=0.25)

onlstms.append(onlstm)

x = onlstm(x)

[回复评论](https://kexue.fm/archives/6621/comment-page-3?replyTo=12079#respond-post-6621)

[苏剑林](https://kexue.fm) 发表于
September 29th, 2019

就是搭建三层ONLSTM模型咯，没什么原因。

[回复评论](https://kexue.fm/archives/6621/comment-page-3?replyTo=12085#respond-post-6621)

zwd13122889

December 12th, 2019

苏老师，您好，想请教个问题。问题如下：

Tensor("dropout/cond/Merge:0", shape=(32, 90, 768), dtype=float32)

Tensor("onlstm\_1/strided\_slice\_17:0", shape=(32, ?, 128), dtype=float32)

第一个Tensor是我的输入，shape是三维的。第二个是经过一层onlstm的输出，为啥shape的第二位是？， 是不是onlstm的输入必须是二维的？

报的错如下：

AttributeError: 'ONLSTM' object has no attribute 'outbound\_nodes'

[回复评论](https://kexue.fm/archives/6621/comment-page-3?replyTo=12612#respond-post-6621)

[苏剑林](https://kexue.fm) 发表于
December 13th, 2019

是不是用tf.keras调用了？

[回复评论](https://kexue.fm/archives/6621/comment-page-3?replyTo=12618#respond-post-6621)

yufeng

January 4th, 2020

RaFM: Rank-Aware Factorization Machines, ICML2019.

有空关注一下谢谢，讨论可以联系

[回复评论](https://kexue.fm/archives/6621/comment-page-3?replyTo=12715#respond-post-6621)

zz

December 2nd, 2020

老师好，这个网络可以代替传统的lstm去做中文命名体识别吗？

[回复评论](https://kexue.fm/archives/6621/comment-page-3?replyTo=14944#respond-post-6621)

[苏剑林](https://kexue.fm) 发表于
December 2nd, 2020

理论上没有什么毛病，你可以尝试。

[回复评论](https://kexue.fm/archives/6621/comment-page-3?replyTo=14946#respond-post-6621)

ruirui 发表于
September 20th, 2021

你好，我最近也想用这个网路做中文命名实体识别，可以交流一下吗？

[回复评论](https://kexue.fm/archives/6621/comment-page-3?replyTo=17402#respond-post-6621)

[苏剑林](https://kexue.fm) 发表于
September 22nd, 2021

欢迎交流。但就“ON-LSTM + NER”这个组合本身我没有任何经验。建议找原作者交流，可能会更有收获。

[回复评论](https://kexue.fm/archives/6621/comment-page-3?replyTo=17408#respond-post-6621)

wusong

July 18th, 2021

苏老师，您好，onlstm的层级结构应该是通过生成单词之间的水平距离来检索层次结构的吧，我看您文中说的是直接生成层次结构。

[回复评论](https://kexue.fm/archives/6621/comment-page-3?replyTo=16925#respond-post-6621)

[苏剑林](https://kexue.fm) 发表于
July 19th, 2021

怎么理解“检索”？

[回复评论](https://kexue.fm/archives/6621/comment-page-3?replyTo=16930#respond-post-6621)

1. [«](https://kexue.fm/archives/6621/comment-page-2#comments)
2. [1](https://kexue.fm/archives/6621/comment-page-1#comments)
3. [2](https://kexue.fm/archives/6621/comment-page-2#comments)
4. [3](https://kexue.fm/archives/6621/comment-page-3#comments)

[取消回复](https://kexue.fm/archives/6621#respond-post-6621)

你的大名

电子邮箱

个人网站（选填）

1\. 可以在评论中使用LaTeX代码，点击“预览效果”可即时查看效果，点击 [这里](https://kexue.fm/content.html) 可以查看更多内容；

2\. 可以通过点击评论楼层编号来引用该楼层；

3\. **提交评论之前，建议复制一下评论内容，避免提交失败导致辛苦打的字没了。**

### 内容速览

[背景内容](https://kexue.fm/archives/6621#%E8%83%8C%E6%99%AF%E5%86%85%E5%AE%B9)
[回顾LSTM](https://kexue.fm/archives/6621#%E5%9B%9E%E9%A1%BELSTM)
[语言和序信息](https://kexue.fm/archives/6621#%E8%AF%AD%E8%A8%80%E5%92%8C%E5%BA%8F%E4%BF%A1%E6%81%AF)
[ON-LSTM](https://kexue.fm/archives/6621#ON-LSTM)
[设计：分区间更新](https://kexue.fm/archives/6621#%E8%AE%BE%E8%AE%A1%EF%BC%9A%E5%88%86%E5%8C%BA%E9%97%B4%E6%9B%B4%E6%96%B0)
[成型：分段软化](https://kexue.fm/archives/6621#%E6%88%90%E5%9E%8B%EF%BC%9A%E5%88%86%E6%AE%B5%E8%BD%AF%E5%8C%96)
[实验与思考](https://kexue.fm/archives/6621#%E5%AE%9E%E9%AA%8C%E4%B8%8E%E6%80%9D%E8%80%83)
[分组的层级](https://kexue.fm/archives/6621#%E5%88%86%E7%BB%84%E7%9A%84%E5%B1%82%E7%BA%A7)
[语言模型](https://kexue.fm/archives/6621#%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B)
[无监督句法](https://kexue.fm/archives/6621#%E6%97%A0%E7%9B%91%E7%9D%A3%E5%8F%A5%E6%B3%95)
[思考与发散](https://kexue.fm/archives/6621#%E6%80%9D%E8%80%83%E4%B8%8E%E5%8F%91%E6%95%A3)
[文章总结](https://kexue.fm/archives/6621#%E6%96%87%E7%AB%A0%E6%80%BB%E7%BB%93)

### 智能搜索

支持整句搜索！网站自动使用 [结巴分词](https://github.com/fxsjy/jieba) 进行分词，并结合ngrams排序算法给出合理的搜索结果。

### 热门标签

[生成模型](https://kexue.fm/tag/%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B/) [attention](https://kexue.fm/tag/attention/) [模型](https://kexue.fm/tag/%E6%A8%A1%E5%9E%8B/) [语言模型](https://kexue.fm/tag/%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/) [优化](https://kexue.fm/tag/%E4%BC%98%E5%8C%96/) [概率](https://kexue.fm/tag/%E6%A6%82%E7%8E%87/) [网站](https://kexue.fm/tag/%E7%BD%91%E7%AB%99/) [转载](https://kexue.fm/tag/%E8%BD%AC%E8%BD%BD/) [微分方程](https://kexue.fm/tag/%E5%BE%AE%E5%88%86%E6%96%B9%E7%A8%8B/) [天象](https://kexue.fm/tag/%E5%A4%A9%E8%B1%A1/) [深度学习](https://kexue.fm/tag/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/) [积分](https://kexue.fm/tag/%E7%A7%AF%E5%88%86/) [分析](https://kexue.fm/tag/%E5%88%86%E6%9E%90/) [python](https://kexue.fm/tag/python/) [梯度](https://kexue.fm/tag/%E6%A2%AF%E5%BA%A6/) [无监督](https://kexue.fm/tag/%E6%97%A0%E7%9B%91%E7%9D%A3/) [力学](https://kexue.fm/tag/%E5%8A%9B%E5%AD%A6/) [节日](https://kexue.fm/tag/%E8%8A%82%E6%97%A5/) [几何](https://kexue.fm/tag/%E5%87%A0%E4%BD%95/) [文本生成](https://kexue.fm/tag/%E6%96%87%E6%9C%AC%E7%94%9F%E6%88%90/) [数论](https://kexue.fm/tag/%E6%95%B0%E8%AE%BA/) [矩阵](https://kexue.fm/tag/%E7%9F%A9%E9%98%B5/) [GAN](https://kexue.fm/tag/GAN/) [生活](https://kexue.fm/tag/%E7%94%9F%E6%B4%BB/) [扩散](https://kexue.fm/tag/%E6%89%A9%E6%95%A3/)

### 随机文章

- [力的无穷分解与格林函数法](https://kexue.fm/archives/3092)
- [基于Xception的腾讯验证码识别（样本+代码）](https://kexue.fm/archives/4503)
- [新浪sina.cn邮箱体验(免费邀请您来体验)](https://kexue.fm/archives/126)
- [从动力学角度看优化算法（五）：为什么学习率不宜过小？](https://kexue.fm/archives/7787)
- [变分与理论力学略览](https://kexue.fm/archives/1304)
- [ARXIV数学论文分布：偏微分方程最热门！](https://kexue.fm/archives/3511)
- [也来扯几句“全国青少年科技创新大赛”](https://kexue.fm/archives/7611)
- [WGAN新方案：通过梯度归一化来实现L约束](https://kexue.fm/archives/8757)
- [2012北约自主招生数学](https://kexue.fm/archives/1551)
- [2010年诺贝尔文学奖落户秘鲁](https://kexue.fm/archives/981)

### 最近评论

- [苏剑林](https://kexue.fm/archives/9984/comment-page-2#comment-24552): 刚刷到这篇paper，它是每个像素都视为一个token，这种做法远比我说的激进，而且它自己越承...
- [苏剑林](https://kexue.fm/archives/10114/comment-page-1#comment-24551): 谢谢，已更正。
- [苏剑林](https://kexue.fm/archives/10145/comment-page-1#comment-24550): 感谢提醒。Softmax Bottleneck有所耳闻，但我个人觉得它本质上是Logits的低...
- [苏剑林](https://kexue.fm/archives/9164/comment-page-3#comment-24549): Chrome和Safari测试正常，暂时无法测试所有浏览器，抱歉。
- [苏剑林](https://kexue.fm/archives/9812/comment-page-1#comment-24548): 哦，$n$是$s\_i$的总个数。前面有个笔误，现在更正了（$i\\in\\{1,2,\\cdots,...
- [苏剑林](https://kexue.fm/archives/9978/comment-page-1#comment-24547): 可以，但一来比较费token，二来其实我不大希望通过作者、机构等带有刻板印象的信息来筛选论文，...
- [苏剑林](https://kexue.fm/archives/9164/comment-page-3#comment-24546): $p(z)$是高斯分布，$p(x\|z)$是条件高斯分布，不意味着$p(x)=\\int p(x\|...
- [苏剑林](https://kexue.fm/archives/10114/comment-page-1#comment-24545): 你的后半段我没看懂。固定的傅立叶基也只适用于固定区间，因为傅立叶基是周期函数，它只适用于所有基...
- [Liuertong](https://kexue.fm/archives/9984/comment-page-2#comment-24544): 苏神好，今天 meta发布了一篇直接用 vanilla transformer学习原始像素，...
- [巡星](https://kexue.fm/archives/10114/comment-page-1#comment-24543): 写得真好！一个小typo：“代入第二个公式(17)得到”应为“代入第二个公式(18)得到”。

### 友情链接

- [Cool Papers](https://papers.cool)
- [数学研发](https://bbs.emath.ac.cn)
- [Seatop](http://www.seatop.com.cn/)
- [Xiaoxia](https://xiaoxia.org/)
- [积分表-网络版](https://kexue.fm/sci/integral/index.html)
- [丝路博傲](http://blog.dvxj.com/)
- [ph4ntasy 饭特稀](http://www.ph4ntasy.com/)
- [数学之家](http://www.2math.cn/)
- [有趣天文奇观](http://interesting-sky.china-vo.org/)
- [bsky](https://bsky.spaces.ac.cn/)
- [TwistedW](http://www.twistedwg.com/)
- [godweiyang](https://godweiyang.com/)
- [AI柠檬](https://blog.ailemon.net/)
- [王登科-DK博客](https://greatdk.com)
- [ESON](https://blog.eson.org/)
- [枫之羽](https://fzhiy.net/)
- [Mathor's blog](https://wmathor.com/)
- [孙云增的博客](https://sunyunzeng.com/)
- [coding-zuo](https://coding-zuo.github.io/)
- [博科园](https://www.bokeyuan.net/)
- [孔皮皮的博客](https://www.kppkkp.top/)
- [运鹏的博客](https://yunpengtai.top/)
- [jiming.site](https://jiming.site/)
- [OmegaXYZ](https://www.omegaxyz.com/)
- [Blog by Eacls](https://www.eacls.top/)
- [申请链接](https://kexue.fm/links.html)

[![署名-非商业用途-保持一致](https://kexue.fm/usr/themes/geekg/images/cc.gif)](http://creativecommons.org/licenses/by-nc-nd/2.5/cn/) 本站采用创作共用版权协议，要求署名、非商业用途和保持一致。转载本站内容必须也遵循“ [署名-非商业用途-保持一致](http://creativecommons.org/licenses/by-nc-nd/2.5/cn/)”的创作共用协议。

© 2009-2024 Scientific Spaces. All rights reserved. Theme by [laogui](http://www.laogui.com). Powered by [Typecho](http://typecho.org). 备案号: [粤ICP备09093259号-1/2](https://beian.miit.gov.cn/)。