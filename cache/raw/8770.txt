## SEARCH

## MENU

- [打赏](https://kexue.fm/reward.html)
- [公式](https://kexue.fm/latex.html)
- [天象](https://kexue.fm/ac.html)
- [链接](https://kexue.fm/links.html)
- [时光](https://kexue.fm/me.html)
- [博览](https://kexue.fm/science.html)
- [归档](https://kexue.fm/content.html)

## CATEGORIES

- [千奇百怪](https://kexue.fm/category/Everything)
- [天文探索](https://kexue.fm/category/Astronomy)
- [数学研究](https://kexue.fm/category/Mathematics)
- [物理化学](https://kexue.fm/category/Phy-chem)
- [信息时代](https://kexue.fm/category/Big-Data)
- [生物自然](https://kexue.fm/category/Biology)
- [图片摄影](https://kexue.fm/category/Photograph)
- [问题百科](https://kexue.fm/category/Questions)
- [生活/情感](https://kexue.fm/category/Life-Feeling)
- [资源共享](https://kexue.fm/category/Resources)

## NEWPOSTS

- [通过msign来计算奇异值裁剪mc...](https://kexue.fm/archives/11059)
- [矩阵符号函数mcsgn能计算什么？](https://kexue.fm/archives/11056)
- [线性注意力简史：从模仿、创新到反哺](https://kexue.fm/archives/11033)
- [msign的导数](https://kexue.fm/archives/11025)
- [通过msign来计算奇异值裁剪mc...](https://kexue.fm/archives/11006)
- [msign算子的Newton-Sc...](https://kexue.fm/archives/10996)
- [等值振荡定理：最优多项式逼近的充要条件](https://kexue.fm/archives/10972)
- [生成扩散模型漫谈（三十）：从瞬时速...](https://kexue.fm/archives/10958)
- [MoE环游记：5、均匀分布的反思](https://kexue.fm/archives/10945)
- [msign算子的Newton-Sc...](https://kexue.fm/archives/10922)

## COMMENTS

- [abcm: 公式(14)为什么是直接带入，我的理解是0到T积分再换元\
\\b...](https://kexue.fm/archives/10114/comment-page-3#comment-28017)
- [Truenobility303: 苏神好，关于RMS对齐这里有些疑问。1. 如果从 A Spec...](https://kexue.fm/archives/10739/comment-page-2#comment-28016)
- [wolfzdf: 您好，请问cool paper中收集的会议论文，有保存整理文章...](https://kexue.fm/archives/9907/comment-page-4#comment-27987)
- [无敌大铁锤: 嗯嗯笔误了，就是去掉Causal的mask，变成一个纯Self...](https://kexue.fm/archives/11033/comment-page-1#comment-27986)
- [HikaruNight: 苏老师，想请教一下如果把四元数放在三维RoPE里面是不是可行的](https://kexue.fm/archives/8397/comment-page-3#comment-27985)
- [Leco: 请问LoRA的A,B矩阵初始化时，一个高斯随机一个全零还是只能...](https://kexue.fm/archives/9590/comment-page-2#comment-27984)
- [苏剑林: 如果你把你这里提到的数学都学通透了，数学基础基本上可以胜任95...](https://kexue.fm/archives/9119/comment-page-13#comment-27983)
- [苏剑林: 我跑过这个项目，效果是能复现的。“在 CIFAR-10 上效果...](https://kexue.fm/archives/10958/comment-page-2#comment-27982)
- [Henry Zha: 苏神你好，我是一名管理科学与工程专业的博士生，研究方向是结合人...](https://kexue.fm/archives/9119/comment-page-13#comment-27981)
- [SunlightZero: 我根据 https://github.com/haidog-y...](https://kexue.fm/archives/10958/comment-page-2#comment-27980)

## USERLOGIN

- [登录](https://kexue.fm/admin/login.php)

[科学空间\|Scientific Spaces](https://kexue.fm)

- [登录](https://kexue.fm/admin/login.php)
- [打赏](https://kexue.fm/reward.html)
- [公式](https://kexue.fm/latex.html)
- [天象](https://kexue.fm/ac.html)
- [链接](https://kexue.fm/links.html)
- [时光](https://kexue.fm/me.html)
- [博览](https://kexue.fm/science.html)
- [归档](https://kexue.fm/content.html)

渴望成为一个小飞侠

- [欢迎订阅](https://kexue.fm/feed)
- [个性邮箱](https://kexue.fm/archives/119)
- [天象信息](https://kexue.fm/ac.html)
- [观测ISS](https://kexue.fm/archives/41)
- [LaTeX](https://kexue.fm/latex.html)
- [关于博主](https://kexue.fm/me.html)

欢迎访问“科学空间”，这里将与您共同探讨自然科学，回味人生百态；也期待大家的分享～

- [**千奇百怪** Everything](https://kexue.fm/category/Everything)
- [**天文探索** Astronomy](https://kexue.fm/category/Astronomy)
- [**数学研究** Mathematics](https://kexue.fm/category/Mathematics)
- [**物理化学** Phy-chem](https://kexue.fm/category/Phy-chem)
- [**信息时代** Big-Data](https://kexue.fm/category/Big-Data)
- [**生物自然** Biology](https://kexue.fm/category/Biology)
- [**图片摄影** Photograph](https://kexue.fm/category/Photograph)
- [**问题百科** Questions](https://kexue.fm/category/Questions)
- [**生活/情感** Life-Feeling](https://kexue.fm/category/Life-Feeling)
- [**资源共享** Resources](https://kexue.fm/category/Resources)

- [**千奇百怪**](https://kexue.fm/category/Everything)
- [**天文探索**](https://kexue.fm/category/Astronomy)
- [**数学研究**](https://kexue.fm/category/Mathematics)
- [**物理化学**](https://kexue.fm/category/Phy-chem)
- [**信息时代**](https://kexue.fm/category/Big-Data)
- [**生物自然**](https://kexue.fm/category/Biology)
- [**图片摄影**](https://kexue.fm/category/Photograph)
- [**问题百科**](https://kexue.fm/category/Questions)
- [**生活/情感**](https://kexue.fm/category/Life-Feeling)
- [**资源共享**](https://kexue.fm/category/Resources)

[首页](https://kexue.fm) [信息时代](https://kexue.fm/category/Big-Data) Dropout视角下的MLM和MAE：一些新的启发

29Nov

# [Dropout视角下的MLM和MAE：一些新的启发](https://kexue.fm/archives/8770)

By 苏剑林 \|
2021-11-29 \|
94632位读者\|

大家都知道，BERT的MLM（Masked Language Model）任务在预训练和微调时的不一致，也就是预训练出现了\[MASK\]而下游任务微调时没有\[MASK\]，是经常被吐槽的问题，很多工作都认为这是影响BERT微调性能的重要原因，并针对性地提出了很多改进，如 [XL-NET](https://papers.cool/arxiv/1906.08237)、 [ELECTRA](https://papers.cool/arxiv/2003.10555)、 [MacBERT](https://papers.cool/arxiv/2004.13922) 等。本文我们将从Dropout的角度来分析MLM的这种不一致性，并且提出一种简单的操作来修正这种不一致性。

同样的分析还可以用于何恺明最近提出的比较热门的MAE（Masked Autoencoder）模型，结果是MAE相比MLM确实具有更好的一致性，由此我们可以引出一种可以能加快训练速度的正则化手段。

## Dropout [\#](https://kexue.fm/archives/8770\#Dropout)

首先，我们重温一下Dropout。从数学上来看，Dropout是通过伯努利分布来为模型引入随机噪声的操作，所以我们也简单复习一下伯努利分布。

### 伯努利分布 [\#](https://kexue.fm/archives/8770\#%E4%BC%AF%E5%8A%AA%E5%88%A9%E5%88%86%E5%B8%83)

伯努利分布（Bernoulli Distribution）算得上是最简单的概率分布了，它是一个二元分布，取值空间是$\\{0,1\\}$，其中$\\varepsilon$取1的概率为$p$，取0的概率为$1-p$，记为
\\begin{equation}\\varepsilon\\sim \\text{Bernoulli}(p)\\end{equation}
伯努利分布的一个有趣的性质是它的任意阶矩都为$p$，即
\\begin{equation}\\mathbb{E}\_{\\varepsilon}\[\\varepsilon^n\] = p\\times 1^n + (1-p)\\times 0^n = p\\end{equation}
所以我们知道它的均值为$p$，以及方差为
\\begin{equation}\\mathbb{V}ar\_{\\varepsilon}\[\\varepsilon\] = \\mathbb{E}\_{\\varepsilon}\[\\varepsilon^2\] - \\mathbb{E}\_{\\varepsilon}\[\\varepsilon\]^2 = p(1-p)\\end{equation}

### 训练和预测 [\#](https://kexue.fm/archives/8770\#%E8%AE%AD%E7%BB%83%E5%92%8C%E9%A2%84%E6%B5%8B)

Dropout在训练阶段，将会以$1-p$将某些值置零，而其余值则除以$p$，所以Dropout事实上是引入了随机变量$\\varepsilon\\sim \\text{Bernoulli}(p)$，使得模型从$f(x)$变成$f(x\\varepsilon/p)$。其中$\\varepsilon$可以有多个分量，对应多个独立的伯努利分布，但大多数情况下其结果跟$\\varepsilon$是标量是没有本质区别，所以我们只需要针对$\\varepsilon$是标量时进行推导。

在 [《又是Dropout两次！这次它做到了有监督任务的SOTA》](https://kexue.fm/archives/8496) 中我们证明过，如果损失函数是MSE，那么训练完成后的最佳预测模型应该是
\\begin{equation}\\mathbb{E}\_{\\varepsilon}\[f(x\\varepsilon/p)\]\\end{equation}
这意味着我们应该要不关闭Dropout地预测多次，然后将预测结果进行平均来作为最终的预测结果，即进行“模型平均”。但很显然这样做计算量很大，所以实际中我们很少会用这种做法，更多的是直接关闭Dropout，即将$\\varepsilon/p$改为1。而我们知道
\\begin{equation}f(x)=f(x\\,\\mathbb{E}\_{\\varepsilon}\[\\varepsilon\]/p)\\end{equation}
所以关闭Dropout事实上是一种“权重平均”（将$\\varepsilon$视为模型的随机权重）。也就是说，理论的最优解是“模型平均”，但由于计算量的原因，我们通常用“权重平均”来近似，它可以视为“模型平均”的一阶近似。

## MLM模型 [\#](https://kexue.fm/archives/8770\#MLM%E6%A8%A1%E5%9E%8B)

在这一节中，我们将MLM模型视为一种特殊的Dropout，由此可以清楚描述地预训练和微调的不一致之处，并且可以导出一个简单的修正策略，可以更好地缓解这种不一致性。

### Dropout视角 [\#](https://kexue.fm/archives/8770\#Dropout%E8%A7%86%E8%A7%92)

简单起见，我们先来分析一个简化版本的MLM：假设在预训练阶段，每个token以$p$的概率保持不变，以$1-p$的概率被替换为\[MASK\]，并且第$i$个token的Embedding记为$x\_i$，\[MASK\]的Embedding记为$m$，那么我们可以同样引入随机变量$\\varepsilon\\sim \\text{Bernoulli}(p)$，将MLM的模型记为
\\begin{equation}f(\\cdots,x\_i,\\cdots)\\quad\\rightarrow\\quad f(\\cdots,x\_i \\varepsilon + m(1-\\varepsilon),\\cdots)\\end{equation}
这样，MLM跟Dropout本质是相同的，它们都是通过伯努利分布给模型引入了随机扰动。现在，按照Dropout的常规用法，它的预测模型应该是“权重平均”，即
\\begin{equation}f(\\cdots,\\mathbb{E}\_{\\varepsilon}\[x\_i \\varepsilon + m(1-\\varepsilon)\],\\cdots) = f(\\cdots,x\_i p + m (1-p),\\cdots)\\end{equation}
此时，MLM在微调阶段的不一致性就体现出来了：我们将预训练的MLM视为一种特殊的Dropout，那么微调阶段对应的是“取消Dropout”，按照常规做法，此时我们应该将每个token的Embedding改为$x\_i p + m (1-p)$，但事实上我们没有，而是保留了原始的$x\_i$。

### 修正Embedding [\#](https://kexue.fm/archives/8770\#%E4%BF%AE%E6%AD%A3Embedding)

按照BERT的默认设置，在训练MLM的时候，会有15%的token被选中来做MLM预测，而在这15%的token中，有80%的概率被替换为\[MASK\]，有10%的概率保持不变，剩下10%的概率则随机替换为一个随机token，这样根据上述分析，我们在MLM预训练完成之后，应该对Embedding进行如下调整：
\\begin{equation}\\text{Embedding\[i\]} \\leftarrow 0.85\\times \\text{Embedding\[i\]} + 0.15\\times\\left(\\begin{array}{l}0.8\\times \\text{Embedding\[m\]} \\,+\\\
0.1 \\times \\text{Embedding\[i\]} \\,+ \\\
0.1\\times \\text{Avg\[Embedding\]}\\end{array}\\right)
\\end{equation}
其中$\\text{Embedding\[m\]}$是\[MASK\]的Embedding，而$\\text{Avg\[Embedding\]}$的全体token的平均Embedding。在bert4keras中，参考代码如下：

```
embeddings = model.get_weights()[0] # 一般第一个权重就是Token Embedding
v1 = embeddings[tokenizer._token_mask_id][None] # [MASK]的Embedding
v2 = embeddings.mean(0)[None] # 平均Embedding
embeddings = 0.85 * embeddings + 0.15 * (0.8 * v1 + 0.1 * embeddings + 0.1 * v2) # 加权平均
K.set_value(model.weights[0], embeddings) # 重新赋值
```

那么，该修改是否跟我们期望的那样有所提升呢？笔者在CLUE上对比了BERT和RoBERTa修改前后的实验结果（baseline代码参考 [《bert4keras在手，baseline我有：CLUE基准代码》](https://kexue.fm/archives/8739)），结论是“没有显著变化”。

看到这里，读者也许会感到失望：敢情你前面说那么多都是白说了？笔者认为，上述操作确实是可以缓解预训练和微调的不一致性的（否则我们不是否定了Dropout？）；至于修改后的效果没有提升，意味着这种不一致性的问题并没有我们想象中那么严重，至少在CLUE的任务上是这样。一个类似的结果出现的MacBERT中，它在预训练阶段用近义词来代替\[MASK\]来修正这种不一致性，但笔者也在用同样的baseline代码测试过MacBERT，结果显示它跟RoBERTa也没显著差别。因此，也许只有在特定的任务或者更大的mask比例下，才能显示出修正这种不一致性的必要性。

## MAE模型 [\#](https://kexue.fm/archives/8770\#MAE%E6%A8%A1%E5%9E%8B)

不少读者可能已经听说过何恺明最近提出的 [MAE（Masked Autoencoder）模型](https://papers.cool/arxiv/2111.06377)，它以一种简单高效的方式将MLM任务引入到图像的预训练之中，并获得了有效的提升。在这一节中，我们将会看到，MAE同样可以作为一种特殊的Dropout来理解，从中我们可以得到一种防止过拟合的新方法。

### Dropout视角 [\#](https://kexue.fm/archives/8770\#Dropout%E8%A7%86%E8%A7%92)

如下图所示，MAE将模型分为encoder和decoder两部分，并且具有“encoder深、decoder浅”的特点，然后它将\[MASK\]只放到decoder中，而encoder不处理\[MASK\]。这样一来，encoder要处理的序列就变短了，最关键的一步是，MAE使用了75%的mask比例，这意味着encoder的序列长度只有通常的1/4，加上“encoder深、decoder浅”的特点，总的来说模型的预训练速度快了3倍多！

MAE模型示意图

我们也可以从另一个角度来实现MAE模型：MAE把\[MASK\]从encoder中移除，这等价于剩下的token不与被mask掉的token交互，而对于Transformer模型来说，token之间的交互来源于Self Attention，所以我们依然可以保持原始输入，但在Attention矩阵中mask掉对应的列。如图所示，假设第$i$个token被mask掉，事实上就相当于Attention矩阵的第$i$列的所有元素被强制置0：

MAE的等价Attention Dropout示意图

当然，从实用的角度看，这种做法纯粹是浪费算力，但它有助于我们得到一个有意思的理论结果。我们设有$n$的输入token，原始的Attention矩阵为$A$（softmax后的），定义$M\_i$为一个$n\\times n$矩阵，它的第$i$列为0、其余都为1，然后定义随机矩阵$\\tilde{M}\_i$，它以$p$的概率为全1矩阵，以$1-p$的概率为$M\_i$，那么MAE模型可以写成
\\begin{equation}f(\\cdots,A,\\cdots)\\quad\\rightarrow\\quad f(\\cdots,\\text{Norm}(A\\otimes \\tilde{M}\_1\\otimes \\tilde{M}\_2\\otimes \\cdots\\otimes \\tilde{M}\_n),\\cdots)\\end{equation}
这里$\\text{Norm}$是指将矩阵重新按行归一化；$\\otimes$时逐个元素对应相乘；当有多个Attention层时，各个Attention层共用同一批$\\tilde{M}\_1,\\tilde{M}\_2,\\cdots,\\tilde{M}\_n$。

这样，我们将MAE转换为了一种特殊的Attention Dropout。那么同样按照微调阶段“取消Dropout”的做法，我们知道它对应的模型应该是
\\begin{equation}\\begin{aligned}
&\\,f(\\cdots,\\text{Norm}(A\\otimes \\mathbb{E}\[\\tilde{M}\_1\\otimes \\tilde{M}\_2\\otimes \\cdots\\otimes \\tilde{M}\_n\]),\\cdots)\\\
=&\\,f(\\cdots,\\text{Norm}(A\\otimes \\mathbb{E}\[\\tilde{M}\_1\]\\otimes \\mathbb{E}\[\\tilde{M}\_2\]\\otimes \\cdots\\otimes \\mathbb{E}\[\\tilde{M}\_n\]),\\cdots)\\\
=&\\,f(\\cdots,\\text{Norm}(Ap),\\cdots)\\\
=&\\,f(\\cdots,A,\\cdots)
\\end{aligned}\\end{equation}
其中第二个等号是因为$\\mathbb{E}\[\\tilde{M}\_i\]$是一个第$i$列为$p$、其余为1的矩阵，那么$\\mathbb{E}\[\\tilde{M}\_1\]\\otimes \\mathbb{E}\[\\tilde{M}\_2\]\\otimes \\cdots\\otimes \\mathbb{E}\[\\tilde{M}\_n\]$事实上就是一个全为$p$的矩阵，所以与$A$相乘的结果等价于$A$直接乘以常数$p$；第三个等号则是因为全体元素乘以同一个常数，不影响归一化结果。

从这个结果中看到，对于MAE来说，“取消Dropout”之后跟原模型一致，这说明了MAE相比原始的MLM模型，不仅仅是速度上的提升，还具有更好的预训练与微调的一致性。

### 防止过拟合 [\#](https://kexue.fm/archives/8770\#%E9%98%B2%E6%AD%A2%E8%BF%87%E6%8B%9F%E5%90%88)

反过来想，既然MAE也可以视为一种Dropout，而Dropout有防止过拟合的作用，那么我们能不能将MAE的做法当作一种防止过拟合的正则化手段来使用呢？如下图所示，在训练阶段，我们可以随机扔掉一些token，但要保持剩余token的原始位置，我们暂且称之为“DropToken”：

DropToken示意图

之所以会这样想，是因为常规的Dropout虽然通常被直接地理解为采样一个子网络训练，但那纯粹是直观的想象，实际上Dropout的加入还会降低训练速度，而DropToken由于显式了缩短了序列长度，是可以提高训练速度的，如果有效那必然是一种非常实用的技巧。此外，有些读者可能已经试过删除某些字词的方式来进行数据扩增，它跟DropToken的区别在于DropToken虽然删除了一些Token，但依然保留了剩余token的原始位置，这个实现依赖于Transformer结构本身。

在CLUE上做的几个实验对比，基准模型为BERT base，下标的数字是drop比例，最终的效果参差不齐，除了IFLYTEK明确有效外，其他看缘分（其实很多防止过拟合手段都这样），最优drop比例在0.1～0.15之间：
$$\\begin{array}{c}
\\text{CLUE分类任务对比实验（验证集）} \\\
{\\begin{array}{c\|ccccccc}
\\hline
& \\text{IFLYTEK} & \\text{TNEWS} & \\text{AFQMC} & \\text{OCNLI} & \\text{WSC} & \\text{CSL} \\\
\\hline
\\text{BERT}\_{\\text{0.00}} & 60.06 & 56.80 & 72.41 & 73.93 & 78.62 & 83.93 \\\
\\text{BERT}\_{\\text{0.10}} & 60.56 & 57.00 & 72.61 & 73.76 & 77.30 & 83.33\\\
\\text{BERT}\_{\\text{0.15}} & 60.10 & 56.68 & 72.50 & 74.54 & 77.30 & 83.30\\\
\\text{BERT}\_{\\text{0.25}} & 61.29 & 56.88 & 72.34 & 73.09 & 73.68 & 83.37\\\
\\text{BERT}\_{\\text{0.50}} & 61.45 & 57.02 & 69.76 & 70.68 & 69.41 & 82.56\\\
\\hline
\\end{array}}
\\end{array}$$

## 本文小结 [\#](https://kexue.fm/archives/8770\#%E6%9C%AC%E6%96%87%E5%B0%8F%E7%BB%93)

本文从Dropout的视角考察了MLM和MAE两个模型，它们均可视为特殊的Dropout，从这个视角中，我们可以得到了一种修正MLM的不一致性的技巧，以及得到一种类似MAE的防止过拟合技巧。

_**转载到请包括本文地址：** [https://kexue.fm/archives/8770](https://kexue.fm/archives/8770)_

_**更详细的转载事宜请参考：**_ [《科学空间FAQ》](https://kexue.fm/archives/6508#%E6%96%87%E7%AB%A0%E5%A6%82%E4%BD%95%E8%BD%AC%E8%BD%BD/%E5%BC%95%E7%94%A8)

**如果您还有什么疑惑或建议，欢迎在下方评论区继续讨论。**

**如果您觉得本文还不错，欢迎 [分享](https://kexue.fm/archives/8770#share)/ [打赏](https://kexue.fm/archives/8770#pay) 本文。打赏并非要从中获得收益，而是希望知道科学空间获得了多少读者的真心关注。当然，如果你无视它，也不会影响你的阅读。再次表示欢迎和感谢！**

打赏

微信打赏

支付宝打赏

因为网站后台对打赏并无记录，因此欢迎在打赏时候备注留言。你还可以 [**点击这里**](http://mail.qq.com/cgi-bin/qm_share?t=qm_mailme&email=tN7d1drY3drrx8H0xcWa19vZ) 或在下方评论区留言来告知你的建议或需求。

**如果您需要引用本文，请参考：**

苏剑林. (Nov. 29, 2021). 《Dropout视角下的MLM和MAE：一些新的启发 》\[Blog post\]. Retrieved from [https://kexue.fm/archives/8770](https://kexue.fm/archives/8770)

@online{kexuefm-8770,
        title={Dropout视角下的MLM和MAE：一些新的启发},
        author={苏剑林},
        year={2021},
        month={Nov},
        url={\\url{https://kexue.fm/archives/8770}},
}

分类： [信息时代](https://kexue.fm/category/Big-Data)    标签： [模型](https://kexue.fm/tag/%E6%A8%A1%E5%9E%8B/), [概率](https://kexue.fm/tag/%E6%A6%82%E7%8E%87/), [分析](https://kexue.fm/tag/%E5%88%86%E6%9E%90/), [优化](https://kexue.fm/tag/%E4%BC%98%E5%8C%96/)[19 评论](https://kexue.fm/archives/8770#comments)

< [ChildTuning：试试把Dropout加到梯度上去？](https://kexue.fm/archives/8764) \| [开局一段扯，数据全靠编？真被一篇“神论文”气到了](https://kexue.fm/archives/8783) >

### 你也许还对下面的内容感兴趣

- [等值振荡定理：最优多项式逼近的充要条件](https://kexue.fm/archives/10972)
- [MoE环游记：5、均匀分布的反思](https://kexue.fm/archives/10945)
- [Transformer升级之路：20、MLA好在哪里?（上）](https://kexue.fm/archives/10907)
- [一道概率不等式：盯着它到显然成立为止！](https://kexue.fm/archives/10902)
- [SVD的导数](https://kexue.fm/archives/10878)
- [通过梯度近似寻找Normalization的替代品](https://kexue.fm/archives/10831)
- [MoE环游记：4、难处应当多投入](https://kexue.fm/archives/10815)
- [MoE环游记：1、从几何意义出发](https://kexue.fm/archives/10699)
- [为什么梯度裁剪的默认模长是1？](https://kexue.fm/archives/10657)
- [从谱范数梯度到新式权重衰减的思考](https://kexue.fm/archives/10648)

[发表你的看法](https://kexue.fm/archives/8770#comment_form)

[JiaxiangBU](https://github.com/JiaxiangBU)

November 29th, 2021

\- 从 Dropout 来理解 BERT 的 MLM，这个思路蛮巧妙的。
\- \`而encodr不处理\[MASK\]\`有一个笔误。

[回复评论](https://kexue.fm/archives/8770/comment-page-1?replyTo=17928#respond-post-8770)

[苏剑林](https://kexue.fm) 发表于
November 29th, 2021

谢谢，已修正

[回复评论](https://kexue.fm/archives/8770/comment-page-1?replyTo=17929#respond-post-8770)

slwang9353

November 30th, 2021

"常规的Dropout虽然通常被直接地理解为采样一个子网络训练，但那纯粹是直观的想象"
完了，被当反例了哈哈哈

[回复评论](https://kexue.fm/archives/8770/comment-page-1?replyTo=17930#respond-post-8770)

[日常半躺](https://space.bilibili.com/272226120)

December 1st, 2021

这么来看的话DropToken有点像GNN里的DropNode

[回复评论](https://kexue.fm/archives/8770/comment-page-1?replyTo=17939#respond-post-8770)

天地

December 1st, 2021

我是这样理解的，MASK是不一致，Dropput也会导致训练和预测不一致，正是由于不一致才提升了模型的鲁棒性，但是不一致到底设置多少能达到最好效果，能不能自主学习不一致参数，这个探索比较少。

[回复评论](https://kexue.fm/archives/8770/comment-page-1?replyTo=17943#respond-post-8770)

[苏剑林](https://kexue.fm) 发表于
December 2nd, 2021

“由于不一致才提升了模型的鲁棒性”这个结论怕是要证明一下比较好。

[回复评论](https://kexue.fm/archives/8770/comment-page-1?replyTo=17954#respond-post-8770)

泉昕

December 9th, 2021

“此时，MLM在微调阶段的不一致性就体现出来了....但事实上我们没有，而是保留了原始的xi”

感觉这里好像不太对？真正对等到dropout概念的做法是不是这样：
在每个batch里，随机以概率p，将token i的embedding换成 mask的embedding，做了forward之后，bp的梯度同时作用到embedding xi 和mask上。

最后预估的时候，还是用xi的embedding。

[回复评论](https://kexue.fm/archives/8770/comment-page-1?replyTo=17986#respond-post-8770)

[苏剑林](https://kexue.fm) 发表于
December 10th, 2021

建议从定量的角度来理解一下Dropout，不要纯粹自己想象，因为没有数学支撑的想象多了，很可能就变成了脱离实际的幻想。

常规的Dropout，是以$p$的概率选择一些元素除以$p$，其余元素置0，写成数学运算就是$f(x)$变成了$f(x\\otimes \\varepsilon / p)$，其中$\\varepsilon\\sim \\text{Bernoulli}(p)$，而预测的时候是关闭Dropout，即$\\varepsilon / p$改为1，而刚好$\\mathbb{E}\_{\\varepsilon}\[\\varepsilon\] / p = 1$。

所以广义上来说，Dropout就是训练时为模型引入噪声$f(x,\\varepsilon/p)$，而预测的时候变为$f(x,1)$；或者也可以说训练时为模型引入噪声$f(x,\\varepsilon)$，而预测的时候变为$f(x,p)$。事实上早期有些Dropout确实是这样实现的，它们在训练的时候直接将某些元素置零，但剩余元素不除以$p$，而是在预测的时候整个向量乘上$p$。

也就是说，模型在训练时可以表达为$f(x,\\varepsilon),\\varepsilon\\sim \\text{Bernoulli}(p)$的，都可以视为广义的Dropout，而预测阶段则应该变为$f(x,p)$，这是广义的“关闭Dropout”。这些都是可以定量地描述的，不是靠想象的。

[回复评论](https://kexue.fm/archives/8770/comment-page-1?replyTo=17993#respond-post-8770)

safawu

December 13th, 2021

在VL相关任务中，很多工作会对language侧做MLM，我们做了一些实验，发现MLM中起效果的一大部分其实是token dropout的贡献，但是image侧我们做drop效果却比较小

[回复评论](https://kexue.fm/archives/8770/comment-page-1?replyTo=18012#respond-post-8770)

[苏剑林](https://kexue.fm) 发表于
December 14th, 2021

patches are all you need～

[回复评论](https://kexue.fm/archives/8770/comment-page-1?replyTo=18023#respond-post-8770)

anduyin shen 发表于
December 24th, 2021

感觉和信息密度有比较大的关系，我们做asr的时候预训练模型的mask大小也是设计为和音素(最小的语音信息单元)时间长度接近。相比之下，文本本身已经是高度抽象的信息表征了，而图像的信息密度低好多。kaiming的文章里出门就说了这一点。

[回复评论](https://kexue.fm/archives/8770/comment-page-1?replyTo=18085#respond-post-8770)

[苏剑林](https://kexue.fm) 发表于
December 27th, 2021

嗯，信息密度，或者说像素之间的相关性问题。

[回复评论](https://kexue.fm/archives/8770/comment-page-1?replyTo=18097#respond-post-8770)

欧阳滨滨

January 5th, 2022

哈哈,苏神跟dropout有不解的情缘

[回复评论](https://kexue.fm/archives/8770/comment-page-1?replyTo=18153#respond-post-8770)

Burning

July 8th, 2022

在修正Embedding一节中，实验无显著效果的原因可不可能是由于Avg\[Embedding\]和实际的随机替换词之间的差距比较大

[回复评论](https://kexue.fm/archives/8770/comment-page-1?replyTo=19440#respond-post-8770)

[苏剑林](https://kexue.fm) 发表于
July 12th, 2022

这个“差距比较大”，理论上在所有Dropout中都可能出现，但Dropout的预测就是这样做的。

事实上，“无显著效果”既是“无显著好的效果”，也是“无显著坏的效果”，所以也说明Avg\[Embedding\]的修改没啥问题呀。

[回复评论](https://kexue.fm/archives/8770/comment-page-1?replyTo=19451#respond-post-8770)

梦幻济公

December 9th, 2022

“DropToken虽然删除了一些Token，但依然保留了剩余token的原始位置”
请问从实现上就是token对应的pos emb和seg emb不变么？

[回复评论](https://kexue.fm/archives/8770/comment-page-1?replyTo=20544#respond-post-8770)

[苏剑林](https://kexue.fm) 发表于
December 16th, 2022

对

[回复评论](https://kexue.fm/archives/8770/comment-page-1?replyTo=20565#respond-post-8770)

jongjyh

June 19th, 2023

"它可以视为“模型平均”的一阶近似。"，苏神这一段可以再细化一些吗？原谅我的数学不太好，但是没看懂这就往下看又非常的难受。

[回复评论](https://kexue.fm/archives/8770/comment-page-1?replyTo=22021#respond-post-8770)

[苏剑林](https://kexue.fm) 发表于
June 25th, 2023

其实说的就是$\\mathbb{E}\_{\\varepsilon}\[f(\\varepsilon)\]\\approx f(\\mathbb{E}\_{\\varepsilon}\[\\varepsilon\])$，推导过程很简单，记$\\mu=\\mathbb{E}\_{\\varepsilon}\[\\varepsilon\])$，将$f(x)$在$\\mu$处泰勒展开：
$$f(\\varepsilon)=f(\\mu) + f'(\\mu) (\\varepsilon - \\mu) + \\frac{1}{2} f''(\\mu)(\\varepsilon-\\mu)^2 + \\cdots$$
两边求期望：
$$\\mathbb{E}\_{\\varepsilon}\[f(\\varepsilon)\]=f(\\mu) + f'(\\mu) \\mathbb{E}\_{\\varepsilon}\[(\\varepsilon - \\mu)\] + \\frac{1}{2} f''(\\mu)\\mathbb{E}\_{\\varepsilon}\[(\\varepsilon-\\mu)^2\] + \\cdots$$
右端第一项期望正好是0，第二项期望正好是方差，即
$$\\mathbb{E}\_{\\varepsilon}\[f(\\varepsilon)\]=f(\\mu) + \\frac{1}{2} f''(\\mu)\\mathbb{V}ar\[\\varepsilon\] + \\cdots$$
所以$f(\\mu)$精确到一阶，方差越小或者$f''(\\mu)$越小时，精度越高。

[回复评论](https://kexue.fm/archives/8770/comment-page-1?replyTo=22042#respond-post-8770)

[取消回复](https://kexue.fm/archives/8770#respond-post-8770)

你的大名

电子邮箱

个人网站（选填）

1\. 可以使用LaTeX代码，点击“预览效果”可查看效果；2. 可以通过点击评论楼层编号来引用该楼层；3. 网站可能会有点卡，如非确认评论失败，请 **不要重复点击提交**。

### 内容速览

[Dropout](https://kexue.fm/archives/8770#Dropout)
[伯努利分布](https://kexue.fm/archives/8770#%E4%BC%AF%E5%8A%AA%E5%88%A9%E5%88%86%E5%B8%83)
[训练和预测](https://kexue.fm/archives/8770#%E8%AE%AD%E7%BB%83%E5%92%8C%E9%A2%84%E6%B5%8B)
[MLM模型](https://kexue.fm/archives/8770#MLM%E6%A8%A1%E5%9E%8B)
[Dropout视角](https://kexue.fm/archives/8770#Dropout%E8%A7%86%E8%A7%92)
[修正Embedding](https://kexue.fm/archives/8770#%E4%BF%AE%E6%AD%A3Embedding)
[MAE模型](https://kexue.fm/archives/8770#MAE%E6%A8%A1%E5%9E%8B)
[Dropout视角](https://kexue.fm/archives/8770#Dropout%E8%A7%86%E8%A7%92)
[防止过拟合](https://kexue.fm/archives/8770#%E9%98%B2%E6%AD%A2%E8%BF%87%E6%8B%9F%E5%90%88)
[本文小结](https://kexue.fm/archives/8770#%E6%9C%AC%E6%96%87%E5%B0%8F%E7%BB%93)

### 智能搜索

支持整句搜索！网站自动使用 [结巴分词](https://github.com/fxsjy/jieba) 进行分词，并结合ngrams排序算法给出合理的搜索结果。

### 热门标签

[生成模型](https://kexue.fm/tag/%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B/) [attention](https://kexue.fm/tag/attention/) [优化](https://kexue.fm/tag/%E4%BC%98%E5%8C%96/) [语言模型](https://kexue.fm/tag/%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/) [模型](https://kexue.fm/tag/%E6%A8%A1%E5%9E%8B/) [网站](https://kexue.fm/tag/%E7%BD%91%E7%AB%99/) [概率](https://kexue.fm/tag/%E6%A6%82%E7%8E%87/) [梯度](https://kexue.fm/tag/%E6%A2%AF%E5%BA%A6/) [转载](https://kexue.fm/tag/%E8%BD%AC%E8%BD%BD/) [微分方程](https://kexue.fm/tag/%E5%BE%AE%E5%88%86%E6%96%B9%E7%A8%8B/) [矩阵](https://kexue.fm/tag/%E7%9F%A9%E9%98%B5/) [天象](https://kexue.fm/tag/%E5%A4%A9%E8%B1%A1/) [分析](https://kexue.fm/tag/%E5%88%86%E6%9E%90/) [深度学习](https://kexue.fm/tag/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/) [积分](https://kexue.fm/tag/%E7%A7%AF%E5%88%86/) [python](https://kexue.fm/tag/python/) [优化器](https://kexue.fm/tag/%E4%BC%98%E5%8C%96%E5%99%A8/) [力学](https://kexue.fm/tag/%E5%8A%9B%E5%AD%A6/) [无监督](https://kexue.fm/tag/%E6%97%A0%E7%9B%91%E7%9D%A3/) [扩散](https://kexue.fm/tag/%E6%89%A9%E6%95%A3/) [几何](https://kexue.fm/tag/%E5%87%A0%E4%BD%95/) [节日](https://kexue.fm/tag/%E8%8A%82%E6%97%A5/) [生活](https://kexue.fm/tag/%E7%94%9F%E6%B4%BB/) [文本生成](https://kexue.fm/tag/%E6%96%87%E6%9C%AC%E7%94%9F%E6%88%90/) [数论](https://kexue.fm/tag/%E6%95%B0%E8%AE%BA/)

### 随机文章

- [MoE环游记：3、换个思路来分配](https://kexue.fm/archives/10757)
- [2012年天象](https://kexue.fm/archives/1788)
- [期待上学，期待九月](https://kexue.fm/archives/919)
- [Mitchell近似：乘法变为加法，误差不超过1/9](https://kexue.fm/archives/7991)
- [生日\|野炊\|嫦娥](https://kexue.fm/archives/957)
- [我害怕](https://kexue.fm/archives/1928)
- [python简单实现gillespie模拟](https://kexue.fm/archives/5607)
- [两道无穷级数：自然数及其平方的倒数和](https://kexue.fm/archives/56)
- [揭秘美国宇航局将如何把人类送上火星(图)](https://kexue.fm/archives/106)
- [高斯型积分的微扰展开（一）](https://kexue.fm/archives/3217)

### 最近评论

- [abcm](https://kexue.fm/archives/10114/comment-page-3#comment-28017): 公式(14)为什么是直接带入，我的理解是0到T积分再换元
\\begin{align}
\\int...
- [Truenobility303](https://kexue.fm/archives/10739/comment-page-2#comment-28016): 苏神好，关于RMS对齐这里有些疑问。1. 如果从 A Spectral Condition f...
- [wolfzdf](https://kexue.fm/archives/9907/comment-page-4#comment-27987): 您好，请问cool paper中收集的会议论文，有保存整理文章（通讯）作者的邮箱吗？
现在计算...
- [无敌大铁锤](https://kexue.fm/archives/11033/comment-page-1#comment-27986): 嗯嗯笔误了，就是去掉Causal的mask，变成一个纯Self-Attention的形式,然后...
- [HikaruNight](https://kexue.fm/archives/8397/comment-page-3#comment-27985): 苏老师，想请教一下如果把四元数放在三维RoPE里面是不是可行的
- [Leco](https://kexue.fm/archives/9590/comment-page-2#comment-27984): 请问LoRA的A,B矩阵初始化时，一个高斯随机一个全零还是只能A高斯，B全零呢？
- [苏剑林](https://kexue.fm/archives/9119/comment-page-13#comment-27983): 如果你把你这里提到的数学都学通透了，数学基础基本上可以胜任95%以上的场景了吧？至于“直觉”这...
- [苏剑林](https://kexue.fm/archives/10958/comment-page-2#comment-27982): 我跑过这个项目，效果是能复现的。“在 CIFAR-10 上效果非常差，生成的图片都是模糊的”是...
- [Henry Zha](https://kexue.fm/archives/9119/comment-page-13#comment-27981): 苏神你好，我是一名管理科学与工程专业的博士生，研究方向是结合人工智能模型建模用户行为之类的管理...
- [SunlightZero](https://kexue.fm/archives/10958/comment-page-2#comment-27980): 我根据 https://github.com/haidog-yaqub/MeanFlow 尝试...

### 友情链接

- [Cool Papers](https://papers.cool)
- [数学研发](https://bbs.emath.ac.cn)
- [Seatop](http://www.seatop.com.cn/)
- [Xiaoxia](https://xiaoxia.org/)
- [积分表-网络版](https://kexue.fm/sci/integral/index.html)
- [丝路博傲](http://blog.dvxj.com/)
- [ph4ntasy 饭特稀](http://www.ph4ntasy.com/)
- [数学之家](http://www.2math.cn/)
- [有趣天文奇观](http://interesting-sky.china-vo.org/)
- [TwistedW](http://www.twistedwg.com/)
- [godweiyang](https://godweiyang.com/)
- [AI柠檬](https://blog.ailemon.net/)
- [王登科-DK博客](https://greatdk.com)
- [ESON](https://blog.eson.org/)
- [枫之羽](https://fzhiy.net/)
- [Mathor's blog](https://wmathor.com/)
- [coding-zuo](https://coding-zuo.github.io/)
- [博科园](https://www.bokeyuan.net/)
- [孔皮皮的博客](https://www.kppkkp.top/)
- [运鹏的博客](https://yunpengtai.top/)
- [jiming.site](https://jiming.site/)
- [OmegaXYZ](https://www.omegaxyz.com/)
- [Blog by Eacls](https://www.eacls.top/)
- [EAI猩球](https://www.robotech.ink/)
- [文举的博客](https://liwenju0.com/)
- [用代码打点酱油](https://bruceyuan.com/)
- [申请链接](https://kexue.fm/links.html)

本站采用创作共用版权协议，要求署名、非商业用途和保持一致。转载本站内容必须也遵循“ [署名-非商业用途-保持一致](http://creativecommons.org/licenses/by-nc-nd/2.5/cn/)”的创作共用协议。
© 2009-2025 Scientific Spaces. All rights reserved. Theme by [laogui](http://www.laogui.com). Powered by [Typecho](http://typecho.org). 备案号: [粤ICP备09093259号-1/2](https://beian.miit.gov.cn/)。