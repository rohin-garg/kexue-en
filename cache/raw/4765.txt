![MobileSideBar](https://kexue.fm/usr/themes/geekg/images/slide-button.png)

## SEARCH

## MENU

- [打赏](https://kexue.fm/reward.html)
- [公式](https://kexue.fm/latex.html)
- [天象](https://kexue.fm/ac.html)
- [链接](https://kexue.fm/links.html)
- [时光](https://kexue.fm/me.html)
- [博览](https://kexue.fm/science.html)
- [归档](https://kexue.fm/content.html)

## CATEGORIES

- [千奇百怪](https://kexue.fm/category/Everything)
- [天文探索](https://kexue.fm/category/Astronomy)
- [数学研究](https://kexue.fm/category/Mathematics)
- [物理化学](https://kexue.fm/category/Phy-chem)
- [信息时代](https://kexue.fm/category/Big-Data)
- [生物自然](https://kexue.fm/category/Biology)
- [图片摄影](https://kexue.fm/category/Photograph)
- [问题百科](https://kexue.fm/category/Questions)
- [生活/情感](https://kexue.fm/category/Life-Feeling)
- [资源共享](https://kexue.fm/category/Resources)

## NEWPOSTS

- [通向概率分布之路：盘点Softma...](https://kexue.fm/archives/10145)
- [重温SSM（二）：HiPPO的一些...](https://kexue.fm/archives/10137)
- [Transformer升级之路：1...](https://kexue.fm/archives/10122)
- [重温SSM（一）：线性系统和HiP...](https://kexue.fm/archives/10114)
- [缓存与效果的极限拉扯：从MHA、M...](https://kexue.fm/archives/10091)
- [Cool Papers更新：简单搭...](https://kexue.fm/archives/10088)
- [以蒸馏的名义：“从去噪自编码器到生...](https://kexue.fm/archives/10085)
- [生成扩散模型漫谈（二十四）：少走捷...](https://kexue.fm/archives/10077)
- [生成扩散模型漫谈（二十三）：信噪比...](https://kexue.fm/archives/10055)
- [生成扩散模型漫谈（二十二）：信噪比...](https://kexue.fm/archives/10047)

## COMMENTS

- [lcz: 苏神，为什么“找一个在整个实数域上都单调递增的函数，而且增长速...](https://kexue.fm/archives/3290/comment-page-2#comment-24554)
- [周名远: Kiro: 很高兴你通过实践验证了SiD的稳定性。SDXL的d...](https://kexue.fm/archives/10085/comment-page-1#comment-24553)
- [苏剑林: 刚刷到这篇paper，它是每个像素都视为一个token，这种做...](https://kexue.fm/archives/9984/comment-page-2#comment-24552)
- [苏剑林: 谢谢，已更正。](https://kexue.fm/archives/10114/comment-page-1#comment-24551)
- [苏剑林: 感谢提醒。Softmax Bottleneck有所耳闻，但我个...](https://kexue.fm/archives/10145/comment-page-1#comment-24550)
- [苏剑林: Chrome和Safari测试正常，暂时无法测试所有浏览器，抱歉。](https://kexue.fm/archives/9164/comment-page-3#comment-24549)
- [苏剑林: 哦，$n$是$s\_i$的总个数。前面有个笔误，现在更正了（$i...](https://kexue.fm/archives/9812/comment-page-1#comment-24548)
- [苏剑林: 可以，但一来比较费token，二来其实我不大希望通过作者、机构...](https://kexue.fm/archives/9978/comment-page-1#comment-24547)
- [苏剑林: $p(z)$是高斯分布，$p(x\|z)$是条件高斯分布，不意味...](https://kexue.fm/archives/9164/comment-page-3#comment-24546)
- [苏剑林: 你的后半段我没看懂。固定的傅立叶基也只适用于固定区间，因为傅立...](https://kexue.fm/archives/10114/comment-page-1#comment-24545)

## USERLOGIN

- [登录](https://kexue.fm/admin/login.php)

[科学空间\|Scientific Spaces](https://kexue.fm)

- [登录](https://kexue.fm/admin/login.php)
- [打赏](https://kexue.fm/reward.html)
- [公式](https://kexue.fm/latex.html)
- [天象](https://kexue.fm/ac.html)
- [链接](https://kexue.fm/links.html)
- [时光](https://kexue.fm/me.html)
- [博览](https://kexue.fm/science.html)
- [归档](https://kexue.fm/content.html)

渴望成为一个小飞侠

- [![](https://kexue.fm/usr/themes/geekg/images/rss.png)\
\
欢迎订阅](https://kexue.fm/feed)
- [![](https://kexue.fm/usr/themes/geekg/images/mail.png)\
\
个性邮箱](https://kexue.fm/archives/119)
- [![](https://kexue.fm/usr/themes/geekg/images/Saturn.png)\
\
天象信息](https://kexue.fm/ac.html)
- [![](https://kexue.fm/usr/themes/geekg/images/iss.png)\
\
观测ISS](https://kexue.fm/archives/41)
- [![](https://kexue.fm/usr/themes/geekg/images/pi.png)\
\
LaTeX](https://kexue.fm/latex.html)
- [![](https://kexue.fm/usr/themes/geekg/images/mlogo.png)\
\
关于博主](https://kexue.fm/me.html)

欢迎访问“科学空间”，这里将与您共同探讨自然科学，回味人生百态；也期待大家的分享～

- [**千奇百怪** Everything](https://kexue.fm/category/Everything)
- [**天文探索** Astronomy](https://kexue.fm/category/Astronomy)
- [**数学研究** Mathematics](https://kexue.fm/category/Mathematics)
- [**物理化学** Phy-chem](https://kexue.fm/category/Phy-chem)
- [**信息时代** Big-Data](https://kexue.fm/category/Big-Data)
- [**生物自然** Biology](https://kexue.fm/category/Biology)
- [**图片摄影** Photograph](https://kexue.fm/category/Photograph)
- [**问题百科** Questions](https://kexue.fm/category/Questions)
- [**生活/情感** Life-Feeling](https://kexue.fm/category/Life-Feeling)
- [**资源共享** Resources](https://kexue.fm/category/Resources)

- [**千奇百怪**](https://kexue.fm/category/Everything)
- [**天文探索**](https://kexue.fm/category/Astronomy)
- [**数学研究**](https://kexue.fm/category/Mathematics)
- [**物理化学**](https://kexue.fm/category/Phy-chem)
- [**信息时代**](https://kexue.fm/category/Big-Data)
- [**生物自然**](https://kexue.fm/category/Biology)
- [**图片摄影**](https://kexue.fm/category/Photograph)
- [**问题百科**](https://kexue.fm/category/Questions)
- [**生活/情感**](https://kexue.fm/category/Life-Feeling)
- [**资源共享**](https://kexue.fm/category/Resources)

[首页](https://kexue.fm) [信息时代](https://kexue.fm/category/Big-Data) 《Attention is All You Need》浅读（简介+代码）

6Jan

# [《Attention is All You Need》浅读（简介+代码）](https://kexue.fm/archives/4765)

By 苏剑林 \|
2018-01-06 \|
770278位读者\|

2017年中，有两篇类似同时也是笔者非常欣赏的论文，分别是FaceBook的《Convolutional Sequence to Sequence Learning》和Google的《Attention is All You Need》，它们都算是Seq2Seq上的创新，本质上来说，都是抛弃了RNN结构来做Seq2Seq任务。

这篇博文中，笔者对 [《Attention is All You Need》](https://papers.cool/arxiv/1706.03762) 做一点简单的分析。当然，这两篇论文本身就比较火，因此网上已经有很多解读了（不过很多解读都是直接翻译论文的，鲜有自己的理解），因此这里尽可能多自己的文字，尽量不重复网上各位大佬已经说过的内容。

## 序列编码 [\#](https://kexue.fm/archives/4765\#%E5%BA%8F%E5%88%97%E7%BC%96%E7%A0%81)

深度学习做NLP的方法，基本上都是先将句子分词，然后每个词转化为对应的词向量序列。这样一来，每个句子都对应的是一个矩阵$\\boldsymbol{X}=(\\boldsymbol{x}\_1,\\boldsymbol{x}\_2,\\dots,\\boldsymbol{x}\_t)$，其中$\\boldsymbol{x}\_i$都代表着第$i$个词的词向量（行向量），维度为$d$维，故$\\boldsymbol{X}\\in \\mathbb{R}^{n\\times d}$。这样的话，问题就变成了编码这些序列了。

第一个基本的思路是RNN层，RNN的方案很简单，递归式进行：

\\begin{equation}\\boldsymbol{y}\_t = f(\\boldsymbol{y}\_{t-1},\\boldsymbol{x}\_t)\\end{equation}

不管是已经被广泛使用的LSTM、GRU还是最近的SRU，都并未脱离这个递归框架。RNN结构本身比较简单，也很适合序列建模，但RNN的明显缺点之一就是无法并行，因此速度较慢，这是递归的天然缺陷。另外我个人觉得RNN无法很好地学习到全局的结构信息，因为它本质是一个马尔科夫决策过程。

第二个思路是CNN层，其实CNN的方案也是很自然的，窗口式遍历，比如尺寸为3的卷积，就是

\\begin{equation}\\boldsymbol{y}\_t = f(\\boldsymbol{x}\_{t-1},\\boldsymbol{x}\_t,\\boldsymbol{x}\_{t+1})\\end{equation}

在FaceBook的论文中，纯粹使用卷积也完成了Seq2Seq的学习，是卷积的一个精致且极致的使用案例，热衷卷积的读者必须得好好读读这篇文论。CNN方便并行，而且容易捕捉到一些全局的结构信息，笔者本身是比较偏爱CNN的，在目前的工作或竞赛模型中，我都已经尽量用CNN来代替已有的RNN模型了，并形成了自己的一套使用经验，这部分我们以后再谈。

Google的大作提供了第三个思路：**纯Attention！单靠注意力就可以！**RNN要逐步递归才能获得全局信息，因此一般要双向RNN才比较好；CNN事实上只能获取局部信息，是通过层叠来增大感受野；Attention的思路最为粗暴，它一步到位获取了全局信息！它的解决方案是：

\\begin{equation}\\boldsymbol{y}\_t = f(\\boldsymbol{x}\_{t},\\boldsymbol{A},\\boldsymbol{B})\\end{equation}

其中$\\boldsymbol{A},\\boldsymbol{B}$是另外一个序列（矩阵）。如果都取$\\boldsymbol{A}=\\boldsymbol{B}=\\boldsymbol{X}$，那么就称为Self Attention，它的意思是直接将$\\boldsymbol{x}\_t$与原来的每个词进行比较，最后算出$\\boldsymbol{y}\_t$！

## Attention层 [\#](https://kexue.fm/archives/4765\#Attention%E5%B1%82)

### Attention定义 [\#](https://kexue.fm/archives/4765\#Attention%E5%AE%9A%E4%B9%89)

[![Attention](https://kexue.fm/usr/uploads/2018/01/458889390.png)](https://kexue.fm/usr/uploads/2018/01/458889390.png)

Attention

Google的一般化Attention思路也是一个编码序列的方案，因此我们也可以认为它跟RNN、CNN一样，都是一个序列编码的层。

前面给出的是一般化的框架形式的描述，事实上Google给出的方案是很具体的。首先，它先把Attention的定义给了出来：

\\begin{equation}Attention(\\boldsymbol{Q},\\boldsymbol{K},\\boldsymbol{V}) = softmax\\left(\\frac{\\boldsymbol{Q}\\boldsymbol{K}^{\\top}}{\\sqrt{d\_k}}\\right)\\boldsymbol{V}\\end{equation}

这里用的是跟Google的论文一致的符号，其中$\\boldsymbol{Q}\\in\\mathbb{R}^{n\\times d\_k}, \\boldsymbol{K}\\in\\mathbb{R}^{m\\times d\_k}, \\boldsymbol{V}\\in\\mathbb{R}^{m\\times d\_v}$。如果忽略激活函数$softmax$的话，那么事实上它就是三个$n\\times d\_k,d\_k\\times m, m\\times d\_v$的矩阵相乘，最后的结果就是一个$n\\times d\_v$的矩阵。于是我们可以认为：这是一个Attention层，将$n\\times d\_k$的序列$\\boldsymbol{Q}$编码成了一个新的$n\\times d\_v$的序列。

那怎么理解这种结构呢？我们不妨逐个向量来看。

\\begin{equation}Attention(\\boldsymbol{q}\_t,\\boldsymbol{K},\\boldsymbol{V}) = \\sum\_{s=1}^m \\frac{1}{Z}\\exp\\left(\\frac{\\langle\\boldsymbol{q}\_t, \\boldsymbol{k}\_s\\rangle}{\\sqrt{d\_k}}\\right)\\boldsymbol{v}\_s\\end{equation}

其中$Z$是归一化因子。事实上$q,k,v$分别是$query,key,value$的简写，$\\boldsymbol{K},\\boldsymbol{V}$是一一对应的，它们就像是key-value的关系，那么上式的意思就是通过$\\boldsymbol{q}\_t$这个query，通过与各个$\\boldsymbol{k}\_s$内积的并softmax的方式，来得到$\\boldsymbol{q}\_t$与各个$\\boldsymbol{v}\_s$的相似度，然后加权求和，得到一个$d\_v$维的向量。其中因子$\\sqrt{d\_k}$起到调节作用，使得内积不至于太大（太大的话softmax后就非0即1了，不够“soft”了）。

**事实上这种Attention的定义并不新鲜，但由于Google的影响力，我们可以认为现在是更加正式地提出了这个定义，并将其视为一个层地看待；此外这个定义只是注意力的一种形式，还有一些其他选择，比如$query$跟$key$的运算方式不一定是点乘（还可以是拼接后再内积一个参数向量），甚至权重都不一定要归一化，等等。**

### Multi-Head Attention [\#](https://kexue.fm/archives/4765\#Multi-Head%20Attention)

[![Multi-Head Attention](https://kexue.fm/usr/uploads/2018/01/2809060486.png)](https://kexue.fm/usr/uploads/2018/01/2809060486.png)

Multi-Head Attention

这个是Google提出的新概念，是Attention机制的完善。不过从形式上看，它其实就再简单不过了，就是把$\\boldsymbol{Q},\\boldsymbol{K},\\boldsymbol{V}$通过参数矩阵映射一下，然后再做Attention，把这个过程重复做$h$次，结果拼接起来就行了，可谓“大道至简”了。具体来说

\\begin{equation}head\_i = Attention(\\boldsymbol{Q}\\boldsymbol{W}\_i^Q,\\boldsymbol{K}\\boldsymbol{W}\_i^K,\\boldsymbol{V}\\boldsymbol{W}\_i^V)\\end{equation}

这里$\\boldsymbol{W}\_i^Q\\in\\mathbb{R}^{d\_k\\times \\tilde{d}\_k}, \\boldsymbol{W}\_i^K\\in\\mathbb{R}^{d\_k\\times \\tilde{d}\_k}, \\boldsymbol{W}\_i^V\\in\\mathbb{R}^{d\_v\\times \\tilde{d}\_v}$，然后

\\begin{equation}MultiHead(\\boldsymbol{Q},\\boldsymbol{K},\\boldsymbol{V}) = Concat(head\_1,...,head\_h)\\end{equation}

最后得到一个$n\\times (h\\tilde{d}\_v)$的序列。 **所谓“多头”（Multi-Head），就是只多做几次同样的事情（参数不共享），然后把结果拼接**。

### Self Attention [\#](https://kexue.fm/archives/4765\#Self%20Attention)

到目前为止，对Attention层的描述都是一般化的，我们可以落实一些应用。比如，如果做阅读理解的话，$\\boldsymbol{Q}$可以是篇章的向量序列，取$\\boldsymbol{K}=\\boldsymbol{V}$为问题的向量序列，那么输出就是所谓的Aligned Question Embedding。

而在Google的论文中，大部分的Attention都是Self Attention，即“自注意力”，或者叫内部注意力。

所谓Self Attention，其实就是$Attention(\\boldsymbol{X},\\boldsymbol{X},\\boldsymbol{X})$，$\\boldsymbol{X}$就是前面说的输入序列。也就是说，在序列内部做Attention，寻找序列内部的联系。Google论文的主要贡献之一是它表明了内部注意力在机器翻译（甚至是一般的Seq2Seq任务）的序列编码上是相当重要的，而之前关于Seq2Seq的研究基本都只是把注意力机制用在解码端。类似的事情是，目前SQUAD阅读理解的榜首模型R-Net也加入了自注意力机制，这也使得它的模型有所提升。

当然，更准确来说，Google所用的是Self Multi-Head Attention：

\\begin{equation}\\boldsymbol{Y}=MultiHead(\\boldsymbol{X},\\boldsymbol{X},\\boldsymbol{X})\\end{equation}

## Position Embedding [\#](https://kexue.fm/archives/4765\#Position%20Embedding)

然而，只要稍微思考一下就会发现，这样的模型 **并不能捕捉序列的顺序**！换句话说，如果将$\\boldsymbol{K},\\boldsymbol{V}$按行打乱顺序（相当于句子中的词序打乱），那么Attention的结果还是一样的。这就表明了，到目前为止，Attention模型顶多是一个非常精妙的“词袋模型”而已。

这问题就比较严重了，大家知道，对于时间序列来说，尤其是对于NLP中的任务来说，顺序是很重要的信息，它代表着局部甚至是全局的结构，学习不到顺序信息，那么效果将会大打折扣（比如机器翻译中，有可能只把每个词都翻译出来了，但是不能组织成合理的句子）。

于是Google再祭出了一招—— **Position Embedding**，也就是“位置向量”，将每个位置编号，然后每个编号对应一个向量，通过结合位置向量和词向量，就给每个词都引入了一定的位置信息，这样Attention就可以分辨出不同位置的词了。

Position Embedding并不算新鲜的玩意，在FaceBook的《Convolutional Sequence to Sequence Learning》也用到了这个东西。但在Google的这个作品中，它的Position Embedding有几点区别：

> 1、以前在RNN、CNN模型中其实都出现过Position Embedding，但在那些模型中，Position Embedding是锦上添花的辅助手段，也就是“有它会更好、没它也就差一点点”的情况，因为RNN、CNN本身就能捕捉到位置信息。但是在这个纯Attention模型中，Position Embedding是位置信息的唯一来源，因此它是模型的核心成分之一，并非仅仅是简单的辅助手段。
>
> 2、在以往的Position Embedding中，基本都是根据任务训练出来的向量。而Google直接给出了一个构造Position Embedding的公式：
>
> \\begin{equation}\\left\\{\\begin{aligned}&PE\_{2i}(p)=\\sin\\Big(p/10000^{2i/{d\_{pos}}}\\Big)\\\
>
> &PE\_{2i+1}(p)=\\cos\\Big(p/10000^{2i/{d\_{pos}}}\\Big)
>
> \\end{aligned}\\right.\\end{equation}
>
> 这里的意思是将id为$p$的位置映射为一个$d\_{pos}$维的位置向量，这个向量的第$i$个元素的数值就是$PE\_i(p)$。Google在论文中说到他们比较过直接训练出来的位置向量和上述公式计算出来的位置向量，效果是接近的。因此显然我们更乐意使用公式构造的Position Embedding了，我们称之为Sinusoidal形式的Position Embedding。
>
> 3、Position Embedding本身是一个绝对位置的信息，但在语言中，相对位置也很重要，Google选择前述的位置向量公式的一个重要原因是：由于我们有$\\sin(\\alpha+\\beta)=\\sin\\alpha\\cos\\beta+\\cos\\alpha\\sin\\beta$以及$\\cos(\\alpha+\\beta)=\\cos\\alpha\\cos\\beta-\\sin\\alpha\\sin\\beta$，这表明位置$p+k$的向量可以表示成位置$p$的向量的线性变换，这提供了表达相对位置信息的可能性。

结合位置向量和词向量有几个可选方案，可以把它们拼接起来作为一个新向量，也可以把位置向量定义为跟词向量一样大小，然后两者加起来。FaceBook的论文和Google论文中用的都是后者。直觉上相加会导致信息损失，似乎不可取，但Google的成果说明相加也是很好的方案。看来我理解还不够深刻。

还有，尽管论文给出的Position Embedding是$\\sin,\\cos$交错的形式，但其实这个交错形式没有特别的意义，你可以按照任意的方式重排它（比如前$\\sin$后$\\cos$地拼接），原因如下：

> 1、假如你的Position\_Embedding是拼接到原来的词向量中，那么将$\\cos$和$\\sin$前后连接还是交叉连接，都是没区别的，因为你下一步都是接一个变换矩阵而已；
>
> 2、如果你的Position\_Embedding是加到原来的词向量中，那么两种方式貌似是有点区别的，但是要注意的是，词向量本身没有局部结构，也就是说，50维的词向量，将每一维打乱重新排个序（当然整体要按同样的顺序来重新排序），它还是等价于原来的词向量。既然相加的对象（词向量）都没有局部结构，我们也没必要强调被加的对象（Position\_Embedding）的局部结构（也就是交叉连接）了。

## 一些不足之处 [\#](https://kexue.fm/archives/4765\#%E4%B8%80%E4%BA%9B%E4%B8%8D%E8%B6%B3%E4%B9%8B%E5%A4%84)

到这里，Attention机制已经基本介绍完了。 **Attention层的好处是能够一步到位捕捉到全局的联系，因为它直接把序列两两比较（代价是计算量变为$\\mathscr{O}(n^2)$，当然由于是纯矩阵运算，这个计算量相当也不是很严重）；相比之下，RNN需要一步步递推才能捕捉到，而CNN则需要通过层叠来扩大感受野，这是Attention层的明显优势。**

Google论文剩下的工作，就是介绍它怎么用到机器翻译中，这是个应用和调参的问题，我们这里不特别关心它。当然，Google的结果表明将纯注意力机制用在机器翻译中，能取得目前最好的效果，这结果的确是辉煌的。

然而，我还是想谈谈这篇论文本身和Attention层自身的一些不足的地方。

> 1、论文标题为《Attention is All You Need》，因此论文中刻意避免出现了RNN、CNN的字眼，但我觉得这种做法过于刻意了。事实上，论文还专门命名了一种Position-wise Feed-Forward Networks，事实上它就是窗口大小为1的一维卷积，因此有种为了不提卷积还专门换了个名称的感觉，有点不厚道。（也有可能是我过于臆测了）
>
> 2、Attention虽然跟CNN没有直接联系，但事实上充分借鉴了CNN的思想，比如Multi-Head Attention就是Attention做多次然后拼接，这跟CNN中的多个卷积核的思想是一致的；还有论文用到了残差结构，这也源于CNN网络。
>
> 3、无法对位置信息进行很好地建模，这是硬伤。尽管可以引入Position Embedding，但我认为这只是一个缓解方案，并没有根本解决问题。举个例子，用这种纯Attention机制训练一个文本分类模型或者是机器翻译模型，效果应该都还不错，但是用来训练一个序列标注模型（分词、实体识别等），效果就不怎么好了。那为什么在机器翻译任务上好？我觉得原因是机器翻译这个任务并不特别强调语序，因此Position Embedding所带来的位置信息已经足够了，此外翻译任务的评测指标BLEU也并不特别强调语序。
>
> 4、并非所有问题都需要长程的、全局的依赖的，也有很多问题只依赖于局部结构，这时候用纯Attention也不大好。事实上，Google似乎也意识到了这个问题，因此论文中也提到了一个restricted版的Self-Attention（不过论文正文应该没有用到它），它假设当前词只与前后$r$个词发生联系，因此注意力也只发生在这$2r+1$个词之间，这样计算量就是$\\mathscr{O}(nr)$，这样也能捕捉到序列的局部结构了。但是很明显，这就是卷积核中的卷积窗口的概念！

通过以上讨论，我们可以体会到，把Attention作为一个单独的层来看，跟CNN、RNN等结构混合使用，应该能更充分融合它们各自的优势，而不必像Google论文号称Attention is All You Need，那样实在有点“矫枉过正”了（“口气”太大），事实上也做不到。就论文的工作而言，也许降低一下身段，称为Attention is All Seq2Seq Need（事实上也这标题的“口气”也很大），会获得更多的肯定。

## 代码实现 [\#](https://kexue.fm/archives/4765\#%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0)

最后，为了使得本文有点实用价值，笔者试着给出了论文的Multi-Head Attention的实现代码。有需要的读者可以直接使用，或者参考着修改。

注意的是，Multi-Head的意思虽然很简单——重复做几次然后拼接，但事实上不能按照这个思路来写程序，这样会非常慢。因为tensorflow是不会自动并行的，比如

```
a = tf.zeros((10, 10))
b = a + 1
c = a + 2

```

其中b,c的计算是串联的，尽管b、c没有相互依赖。因此我们必须把Multi-Head的操作合并到一个张量来运算，因为单个张量的乘法内部则会自动并行。

此外，我们要对序列做Mask以忽略填充部分的影响。一般的Mask是将填充部分置零，但Attention中的Mask是要在softmax之前，把填充部分减去一个大整数（这样softmax之后就非常接近0了）。这些内容都在代码中有对应的实现。

### tensorflow版 [\#](https://kexue.fm/archives/4765\#tensorflow%E7%89%88)

下面是tf的实现：

[https://github.com/bojone/attention/blob/master/attention\_tf.py](https://github.com/bojone/attention/blob/master/attention_tf.py)

### Keras版 [\#](https://kexue.fm/archives/4765\#Keras%E7%89%88)

Keras仍然是我最喜爱的深度学习框架之一，因此必须也得给Keras写一个出来：

[https://github.com/bojone/attention/blob/master/attention\_keras.py](https://github.com/bojone/attention/blob/master/attention_keras.py)

### 代码测试 [\#](https://kexue.fm/archives/4765\#%E4%BB%A3%E7%A0%81%E6%B5%8B%E8%AF%95)

在Keras上对IMDB进行简单的测试（不做Mask）：

```
from __future__ import print_function
from keras.preprocessing import sequence
from keras.datasets import imdb
from attention_keras import *

max_features = 20000
maxlen = 80
batch_size = 32

print('Loading data...')
(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)
print(len(x_train), 'train sequences')
print(len(x_test), 'test sequences')

print('Pad sequences (samples x time)')
x_train = sequence.pad_sequences(x_train, maxlen=maxlen)
x_test = sequence.pad_sequences(x_test, maxlen=maxlen)
print('x_train shape:', x_train.shape)
print('x_test shape:', x_test.shape)

from keras.models import Model
from keras.layers import *

S_inputs = Input(shape=(None,), dtype='int32')
embeddings = Embedding(max_features, 128)(S_inputs)
# embeddings = SinCosPositionEmbedding(128)(embeddings) # 增加Position_Embedding能轻微提高准确率
O_seq = Attention(8,16)([embeddings,embeddings,embeddings])
O_seq = GlobalAveragePooling1D()(O_seq)
O_seq = Dropout(0.5)(O_seq)
outputs = Dense(1, activation='sigmoid')(O_seq)

model = Model(inputs=S_inputs, outputs=outputs)
# try using different optimizers and different optimizer configs
model.compile(loss='binary_crossentropy',
 optimizer='adam',
 metrics=['accuracy'])

print('Train...')
model.fit(x_train, y_train,
 batch_size=batch_size,
 epochs=5,
 validation_data=(x_test, y_test))

```

无Position Embedding的结果：

> Train on 25000 samples, validate on 25000 samples
>
> 25000/25000 \[==============================\] - 9s - loss: 0.4090 - acc: 0.8126 - val\_loss: 0.3541 - val\_acc: 0.8430
>
> Epoch 2/5
>
> 25000/25000 \[==============================\] - 9s - loss: 0.2528 - acc: 0.8976 - val\_loss: 0.3962 - val\_acc: 0.8284
>
> Epoch 3/5
>
> 25000/25000 \[==============================\] - 9s - loss: 0.1731 - acc: 0.9335 - val\_loss: 0.5172 - val\_acc: 0.8137
>
> Epoch 4/5
>
> 25000/25000 \[==============================\] - 9s - loss: 0.1172 - acc: 0.9568 - val\_loss: 0.6185 - val\_acc: 0.8009
>
> Epoch 5/5
>
> 25000/25000 \[==============================\] - 9s - loss: 0.0750 - acc: 0.9730 - val\_loss: 0.9310 - val\_acc: 0.7925

有Position Embedding的结构：

> Train on 25000 samples, validate on 25000 samples
>
> Epoch 1/5
>
> 25000/25000 \[==============================\] - 9s - loss: 0.5179 - acc: 0.7183 - val\_loss: 0.3540 - val\_acc: 0.8413
>
> Epoch 2/5
>
> 25000/25000 \[==============================\] - 9s - loss: 0.2880 - acc: 0.8786 - val\_loss: 0.3464 - val\_acc: 0.8447
>
> Epoch 3/5
>
> 25000/25000 \[==============================\] - 9s - loss: 0.1584 - acc: 0.9404 - val\_loss: 0.4398 - val\_acc: 0.8313
>
> Epoch 4/5
>
> 25000/25000 \[==============================\] - 9s - loss: 0.0588 - acc: 0.9803 - val\_loss: 0.5836 - val\_acc: 0.8243
>
> Epoch 5/5
>
> 25000/25000 \[==============================\] - 9s - loss: 0.0182 - acc: 0.9947 - val\_loss: 0.8095 - val\_acc: 0.8178

貌似最高准确率比单层的LSTM准确率还高一点，另外还可以看到Position Embedding能提高准确率、减弱过拟合。

### 计算量分析 [\#](https://kexue.fm/archives/4765\#%E8%AE%A1%E7%AE%97%E9%87%8F%E5%88%86%E6%9E%90)

可以看到，事实上 **Attention的计算量并不低**。比如Self Attention中，首先要对$\\boldsymbol{X}$做三次线性映射，这计算量已经相当于卷积核大小为3的一维卷积了，不过这部分计算量还只是$\\mathscr{O}(n)$的；然后还包含了两次序列自身的矩阵乘法，这两次矩阵乘法的计算量都是$\\mathscr{O}(n^2)$的，要是序列足够长，这个计算量其实是很难接受的。

这也表明，restricted版的Attention是接下来的研究重点，并且将Attention与CNN、RNN混合使用，才是比较适中的道路。

## 结语 [\#](https://kexue.fm/archives/4765\#%E7%BB%93%E8%AF%AD)

感谢Google提供的精彩的使用案例，让我等在大开眼界之余，还对Attention的认识更深一层。Google的这个成果在某种程度上体现了“大道至简”的理念，的确是NLP中不可多得的精品。本文围绕着Google的大作，班门弄斧一番，但愿能够帮助有需要的读者更好的理解Attention。最后恳请大家建议和批评。

_**转载到请包括本文地址：** [https://kexue.fm/archives/4765](https://kexue.fm/archives/4765)_

_**更详细的转载事宜请参考：**_ [《科学空间FAQ》](https://kexue.fm/archives/6508#%E6%96%87%E7%AB%A0%E5%A6%82%E4%BD%95%E8%BD%AC%E8%BD%BD/%E5%BC%95%E7%94%A8)

**如果您还有什么疑惑或建议，欢迎在下方评论区继续讨论。**

**如果您觉得本文还不错，欢迎 [分享](https://kexue.fm/archives/4765#share)/ [打赏](https://kexue.fm/archives/4765#pay) 本文。打赏并非要从中获得收益，而是希望知道科学空间获得了多少读者的真心关注。当然，如果你无视它，也不会影响你的阅读。再次表示欢迎和感谢！**

打赏

![科学空间](https://kexue.fm/usr/themes/geekg/payment/wx.png)

微信打赏

![科学空间](https://kexue.fm/usr/themes/geekg/payment/zfb.png)

支付宝打赏

因为网站后台对打赏并无记录，因此欢迎在打赏时候备注留言。

你还可以 [**点击这里**](http://mail.qq.com/cgi-bin/qm_share?t=qm_mailme&email=tN7d1drY3drrx8H0xcWa19vZ) 或在下方评论区留言来告知你的建议或需求。

**如果您需要引用本文，请参考：**

苏剑林. (Jan. 06, 2018). 《《Attention is All You Need》浅读（简介+代码） 》\[Blog post\]. Retrieved from [https://kexue.fm/archives/4765](https://kexue.fm/archives/4765)

@online{kexuefm-4765,

        title={《Attention is All You Need》浅读（简介+代码）},

        author={苏剑林},

        year={2018},

        month={Jan},

        url={\\url{https://kexue.fm/archives/4765}},

}

分类： [信息时代](https://kexue.fm/category/Big-Data)    标签： [模型](https://kexue.fm/tag/%E6%A8%A1%E5%9E%8B/), [深度学习](https://kexue.fm/tag/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/), [attention](https://kexue.fm/tag/attention/)[280 评论](https://kexue.fm/archives/4765#comments)

< [2018年全年天象](https://kexue.fm/archives/4760) \| [增强typecho的搜索功能](https://kexue.fm/archives/4797) >

### 你也许还对下面的内容感兴趣

- [Transformer升级之路：18、RoPE的底数选择原则](https://kexue.fm/archives/10122)
- [缓存与效果的极限拉扯：从MHA、MQA、GQA到MLA](https://kexue.fm/archives/10091)
- [Transformer升级之路：17、多模态位置编码的简单思考](https://kexue.fm/archives/10040)
- [时空之章：将Attention视为平方复杂度的RNN](https://kexue.fm/archives/10017)
- [“闭门造车”之多模态模型方案浅谈](https://kexue.fm/archives/9984)
- [Transformer升级之路：16、“复盘”长度外推技术](https://kexue.fm/archives/9948)
- [注意力机制真的可以“集中注意力”吗？](https://kexue.fm/archives/9889)
- [我在Performer中发现了Transformer-VQ的踪迹](https://kexue.fm/archives/9862)
- [Transformer升级之路：15、Key归一化助力长度外推](https://kexue.fm/archives/9859)
- [VQ一下Key，Transformer的复杂度就变成线性了](https://kexue.fm/archives/9844)

[发表你的看法](https://kexue.fm/archives/4765#comment_form)

1. [«](https://kexue.fm/archives/4765/comment-page-11#comments)
2. [1](https://kexue.fm/archives/4765/comment-page-1#comments)
3. ...
4. [9](https://kexue.fm/archives/4765/comment-page-9#comments)
5. [10](https://kexue.fm/archives/4765/comment-page-10#comments)
6. [11](https://kexue.fm/archives/4765/comment-page-11#comments)
7. [12](https://kexue.fm/archives/4765/comment-page-12#comments)

[seq2seq总结【转载以学习、回忆】\_Johngo学长](https://www.johngo689.com/568108/)

June 4th, 2023

\[...\]至于Attention的具体做法，笔者之前已经撰文介绍过了，请参考《Attention is All You Need》浅读（简介+代码）。Attention一般分为乘性和加性两种，笔者介绍的是Google系统介绍的乘性的Attention，加性的Attention读者可以自行查阅，只要抓住query、key、value三个要素，Attention就都不难理解了。\[...\]

[回复评论](https://kexue.fm/archives/4765/comment-page-12?replyTo=21868#respond-post-4765)

[Naive Bayes is all you need ? R11; AI 資訊](https://news.aitime.space/2023/06/3197/)

June 8th, 2023

\[...\]很抱歉，起了这么个具有标题党特征的题目。在写完《NBCE：使用朴素贝叶斯扩展LLM的Context处理长度》之后，笔者就觉得朴素贝叶斯（Naive Bayes）跟Attention机制有很多相同的特征，后来再推导了一下发现，Attention机制其实可以看成是一种广义的、参数化的朴素贝叶斯。既然如此，那么“Attention is All You Need”不也就意味着“Naive Bayes i\[...\]

[回复评论](https://kexue.fm/archives/4765/comment-page-12?replyTo=21910#respond-post-4765)

[BERT通俗笔记：从Word2Vec/Transformer逐步理解到BERT\_Johngo学长](https://www.johngo689.com/626112/)

June 16th, 2023

\[...\]《Attention is All You Need》浅读（简介+代码）\[...\]

[回复评论](https://kexue.fm/archives/4765/comment-page-12?replyTo=22003#respond-post-4765)

[从梯度最大化看Attention的Scale操作 R11; AI 資訊](https://news.aitime.space/2023/10/62827/)

October 22nd, 2023

\[...\]我们知道，Scaled-Dot Product Attention的Scale因子是$frac{1}{sqrt{d}}$，其中$d$是$boldsymbol{q},boldsymbol{k}$的维度。这个Scale因子的一般解释是：如果不除以$sqrt{d}$，那么初始的Attention就会很接近one hot分布，这会造成梯度消失，导致模型训练不起来。然而，可以证明的是，当Scale等于0时同\[...\]

[回复评论](https://kexue.fm/archives/4765/comment-page-12?replyTo=22934#respond-post-4765)

1. [«](https://kexue.fm/archives/4765/comment-page-11#comments)
2. [1](https://kexue.fm/archives/4765/comment-page-1#comments)
3. ...
4. [9](https://kexue.fm/archives/4765/comment-page-9#comments)
5. [10](https://kexue.fm/archives/4765/comment-page-10#comments)
6. [11](https://kexue.fm/archives/4765/comment-page-11#comments)
7. [12](https://kexue.fm/archives/4765/comment-page-12#comments)

[取消回复](https://kexue.fm/archives/4765#respond-post-4765)

你的大名

电子邮箱

个人网站（选填）

1\. 可以在评论中使用LaTeX代码，点击“预览效果”可即时查看效果，点击 [这里](https://kexue.fm/content.html) 可以查看更多内容；

2\. 可以通过点击评论楼层编号来引用该楼层；

3\. **提交评论之前，建议复制一下评论内容，避免提交失败导致辛苦打的字没了。**

### 内容速览

[序列编码](https://kexue.fm/archives/4765#%E5%BA%8F%E5%88%97%E7%BC%96%E7%A0%81)
[Attention层](https://kexue.fm/archives/4765#Attention%E5%B1%82)
[Attention定义](https://kexue.fm/archives/4765#Attention%E5%AE%9A%E4%B9%89)
[Multi-Head Attention](https://kexue.fm/archives/4765#Multi-Head%20Attention)
[Self Attention](https://kexue.fm/archives/4765#Self%20Attention)
[Position Embedding](https://kexue.fm/archives/4765#Position%20Embedding)
[一些不足之处](https://kexue.fm/archives/4765#%E4%B8%80%E4%BA%9B%E4%B8%8D%E8%B6%B3%E4%B9%8B%E5%A4%84)
[代码实现](https://kexue.fm/archives/4765#%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0)
[tensorflow版](https://kexue.fm/archives/4765#tensorflow%E7%89%88)
[Keras版](https://kexue.fm/archives/4765#Keras%E7%89%88)
[代码测试](https://kexue.fm/archives/4765#%E4%BB%A3%E7%A0%81%E6%B5%8B%E8%AF%95)
[计算量分析](https://kexue.fm/archives/4765#%E8%AE%A1%E7%AE%97%E9%87%8F%E5%88%86%E6%9E%90)
[结语](https://kexue.fm/archives/4765#%E7%BB%93%E8%AF%AD)

### 智能搜索

支持整句搜索！网站自动使用 [结巴分词](https://github.com/fxsjy/jieba) 进行分词，并结合ngrams排序算法给出合理的搜索结果。

### 热门标签

[生成模型](https://kexue.fm/tag/%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B/) [attention](https://kexue.fm/tag/attention/) [模型](https://kexue.fm/tag/%E6%A8%A1%E5%9E%8B/) [语言模型](https://kexue.fm/tag/%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/) [优化](https://kexue.fm/tag/%E4%BC%98%E5%8C%96/) [概率](https://kexue.fm/tag/%E6%A6%82%E7%8E%87/) [网站](https://kexue.fm/tag/%E7%BD%91%E7%AB%99/) [转载](https://kexue.fm/tag/%E8%BD%AC%E8%BD%BD/) [微分方程](https://kexue.fm/tag/%E5%BE%AE%E5%88%86%E6%96%B9%E7%A8%8B/) [天象](https://kexue.fm/tag/%E5%A4%A9%E8%B1%A1/) [深度学习](https://kexue.fm/tag/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/) [积分](https://kexue.fm/tag/%E7%A7%AF%E5%88%86/) [分析](https://kexue.fm/tag/%E5%88%86%E6%9E%90/) [python](https://kexue.fm/tag/python/) [梯度](https://kexue.fm/tag/%E6%A2%AF%E5%BA%A6/) [无监督](https://kexue.fm/tag/%E6%97%A0%E7%9B%91%E7%9D%A3/) [力学](https://kexue.fm/tag/%E5%8A%9B%E5%AD%A6/) [节日](https://kexue.fm/tag/%E8%8A%82%E6%97%A5/) [几何](https://kexue.fm/tag/%E5%87%A0%E4%BD%95/) [文本生成](https://kexue.fm/tag/%E6%96%87%E6%9C%AC%E7%94%9F%E6%88%90/) [数论](https://kexue.fm/tag/%E6%95%B0%E8%AE%BA/) [矩阵](https://kexue.fm/tag/%E7%9F%A9%E9%98%B5/) [GAN](https://kexue.fm/tag/GAN/) [生活](https://kexue.fm/tag/%E7%94%9F%E6%B4%BB/) [扩散](https://kexue.fm/tag/%E6%89%A9%E6%95%A3/)

### 随机文章

- [RNN模型中输入的重要性的评估](https://kexue.fm/archives/4582)
- [曾被嫌弃的预训练任务NSP，做出了优秀的Zero Shot效果](https://kexue.fm/archives/8671)
- [基于遗忘假设的平滑公式](https://kexue.fm/archives/4182)
- [双固定引力中心问题](https://kexue.fm/archives/1350)
- [《转山》，动人之旅](https://kexue.fm/archives/2063)
- [“让Keras更酷一些！”：精巧的层与花式的回调](https://kexue.fm/archives/5765)
- [【NASA每日一图】飞马座的星系](https://kexue.fm/archives/60)
- [重温SSM（一）：线性系统和HiPPO矩阵](https://kexue.fm/archives/10114)
- [生成扩散模型漫谈（二十）：从ReFlow到WGAN-GP](https://kexue.fm/archives/9668)
- [班门弄斧：Python的代码能有多简洁？](https://kexue.fm/archives/2971)

### 最近评论

- [lcz](https://kexue.fm/archives/3290/comment-page-2#comment-24554): 苏神，为什么“找一个在整个实数域上都单调递增的函数，而且增长速度要快于线性增长，然后求和，最后...
- [周名远](https://kexue.fm/archives/10085/comment-page-1#comment-24553): Kiro: 很高兴你通过实践验证了SiD的稳定性。SDXL的distill目前实验还没具体开展...
- [苏剑林](https://kexue.fm/archives/9984/comment-page-2#comment-24552): 刚刷到这篇paper，它是每个像素都视为一个token，这种做法远比我说的激进，而且它自己越承...
- [苏剑林](https://kexue.fm/archives/10114/comment-page-1#comment-24551): 谢谢，已更正。
- [苏剑林](https://kexue.fm/archives/10145/comment-page-1#comment-24550): 感谢提醒。Softmax Bottleneck有所耳闻，但我个人觉得它本质上是Logits的低...
- [苏剑林](https://kexue.fm/archives/9164/comment-page-3#comment-24549): Chrome和Safari测试正常，暂时无法测试所有浏览器，抱歉。
- [苏剑林](https://kexue.fm/archives/9812/comment-page-1#comment-24548): 哦，$n$是$s\_i$的总个数。前面有个笔误，现在更正了（$i\\in\\{1,2,\\cdots,...
- [苏剑林](https://kexue.fm/archives/9978/comment-page-1#comment-24547): 可以，但一来比较费token，二来其实我不大希望通过作者、机构等带有刻板印象的信息来筛选论文，...
- [苏剑林](https://kexue.fm/archives/9164/comment-page-3#comment-24546): $p(z)$是高斯分布，$p(x\|z)$是条件高斯分布，不意味着$p(x)=\\int p(x\|...
- [苏剑林](https://kexue.fm/archives/10114/comment-page-1#comment-24545): 你的后半段我没看懂。固定的傅立叶基也只适用于固定区间，因为傅立叶基是周期函数，它只适用于所有基...

### 友情链接

- [Cool Papers](https://papers.cool)
- [数学研发](https://bbs.emath.ac.cn)
- [Seatop](http://www.seatop.com.cn/)
- [Xiaoxia](https://xiaoxia.org/)
- [积分表-网络版](https://kexue.fm/sci/integral/index.html)
- [丝路博傲](http://blog.dvxj.com/)
- [ph4ntasy 饭特稀](http://www.ph4ntasy.com/)
- [数学之家](http://www.2math.cn/)
- [有趣天文奇观](http://interesting-sky.china-vo.org/)
- [bsky](https://bsky.spaces.ac.cn/)
- [TwistedW](http://www.twistedwg.com/)
- [godweiyang](https://godweiyang.com/)
- [AI柠檬](https://blog.ailemon.net/)
- [王登科-DK博客](https://greatdk.com)
- [ESON](https://blog.eson.org/)
- [枫之羽](https://fzhiy.net/)
- [Mathor's blog](https://wmathor.com/)
- [孙云增的博客](https://sunyunzeng.com/)
- [coding-zuo](https://coding-zuo.github.io/)
- [博科园](https://www.bokeyuan.net/)
- [孔皮皮的博客](https://www.kppkkp.top/)
- [运鹏的博客](https://yunpengtai.top/)
- [jiming.site](https://jiming.site/)
- [OmegaXYZ](https://www.omegaxyz.com/)
- [Blog by Eacls](https://www.eacls.top/)
- [申请链接](https://kexue.fm/links.html)

[![署名-非商业用途-保持一致](https://kexue.fm/usr/themes/geekg/images/cc.gif)](http://creativecommons.org/licenses/by-nc-nd/2.5/cn/) 本站采用创作共用版权协议，要求署名、非商业用途和保持一致。转载本站内容必须也遵循“ [署名-非商业用途-保持一致](http://creativecommons.org/licenses/by-nc-nd/2.5/cn/)”的创作共用协议。

© 2009-2024 Scientific Spaces. All rights reserved. Theme by [laogui](http://www.laogui.com). Powered by [Typecho](http://typecho.org). 备案号: [粤ICP备09093259号-1/2](https://beian.miit.gov.cn/)。