## SEARCH

## MENU

- [打赏](https://kexue.fm/reward.html)
- [公式](https://kexue.fm/latex.html)
- [天象](https://kexue.fm/ac.html)
- [链接](https://kexue.fm/links.html)
- [时光](https://kexue.fm/me.html)
- [博览](https://kexue.fm/science.html)
- [归档](https://kexue.fm/content.html)

## CATEGORIES

- [千奇百怪](https://kexue.fm/category/Everything)
- [天文探索](https://kexue.fm/category/Astronomy)
- [数学研究](https://kexue.fm/category/Mathematics)
- [物理化学](https://kexue.fm/category/Phy-chem)
- [信息时代](https://kexue.fm/category/Big-Data)
- [生物自然](https://kexue.fm/category/Biology)
- [图片摄影](https://kexue.fm/category/Photograph)
- [问题百科](https://kexue.fm/category/Questions)
- [生活/情感](https://kexue.fm/category/Life-Feeling)
- [资源共享](https://kexue.fm/category/Resources)

## NEWPOSTS

- [通过msign来计算mclip（奇...](https://kexue.fm/archives/11006)
- [msign算子的Newton-Sc...](https://kexue.fm/archives/10996)
- [等值振荡定理：最优多项式逼近的充要条件](https://kexue.fm/archives/10972)
- [生成扩散模型漫谈（三十）：从瞬时速...](https://kexue.fm/archives/10958)
- [MoE环游记：5、均匀分布的反思](https://kexue.fm/archives/10945)
- [msign算子的Newton-Sc...](https://kexue.fm/archives/10922)
- [Transformer升级之路：2...](https://kexue.fm/archives/10907)
- [一道概率不等式：盯着它到显然成立为止！](https://kexue.fm/archives/10902)
- [SVD的导数](https://kexue.fm/archives/10878)
- [智能家居之手搓一套能接入米家的零冷水装置](https://kexue.fm/archives/10869)

## COMMENTS

- [苏剑林: 刚入门那会的文章，不用深究了。](https://kexue.fm/archives/481/comment-page-1#comment-27835)
- [苏剑林: 目前各方面的实测效果看来不会，我觉得本质上就是因为partia...](https://kexue.fm/archives/10122/comment-page-1#comment-27834)
- [苏剑林: 首先，瞬时速度为什么跟$t$无关？其次，现在reflow和me...](https://kexue.fm/archives/10958/comment-page-1#comment-27833)
- [苏剑林: 对于每一步数据都严格对齐来说，0.01的loss差距不小了，因...](https://kexue.fm/archives/10907/comment-page-2#comment-27832)
- [苏剑林: \[comment=27808\]rpsun\[/comment\]\
...](https://kexue.fm/archives/10699/comment-page-1#comment-27831)
- [苏剑林: 自己都没怎么关注天象了，惭愧](https://kexue.fm/archives/1490/comment-page-1#comment-27830)
- [苏剑林: 原来如此。其实只要预测空间是连续空间，并且任务本质是一对多的输...](https://kexue.fm/archives/10958/comment-page-1#comment-27829)
- [苏剑林: 你可以拿一批语料去eval，看各个expert分别激活了多少次呀。](https://kexue.fm/archives/10945/comment-page-1#comment-27828)
- [苏剑林: 首先这个分布肯定是存在的（贝叶斯公式无关分布），然后它的概率密...](https://kexue.fm/archives/9164/comment-page-4#comment-27827)
- [苏剑林: 这两天看了下FoPE，感觉它的分析有点道理，但它实现的代码跟论...](https://kexue.fm/archives/10907/comment-page-2#comment-27826)

## USERLOGIN

- [登录](https://kexue.fm/admin/login.php)

[科学空间\|Scientific Spaces](https://kexue.fm)

- [登录](https://kexue.fm/admin/login.php)
- [打赏](https://kexue.fm/reward.html)
- [公式](https://kexue.fm/latex.html)
- [天象](https://kexue.fm/ac.html)
- [链接](https://kexue.fm/links.html)
- [时光](https://kexue.fm/me.html)
- [博览](https://kexue.fm/science.html)
- [归档](https://kexue.fm/content.html)

渴望成为一个小飞侠

- [欢迎订阅](https://kexue.fm/feed)
- [个性邮箱](https://kexue.fm/archives/119)
- [天象信息](https://kexue.fm/ac.html)
- [观测ISS](https://kexue.fm/archives/41)
- [LaTeX](https://kexue.fm/latex.html)
- [关于博主](https://kexue.fm/me.html)

欢迎访问“科学空间”，这里将与您共同探讨自然科学，回味人生百态；也期待大家的分享～

- [**千奇百怪** Everything](https://kexue.fm/category/Everything)
- [**天文探索** Astronomy](https://kexue.fm/category/Astronomy)
- [**数学研究** Mathematics](https://kexue.fm/category/Mathematics)
- [**物理化学** Phy-chem](https://kexue.fm/category/Phy-chem)
- [**信息时代** Big-Data](https://kexue.fm/category/Big-Data)
- [**生物自然** Biology](https://kexue.fm/category/Biology)
- [**图片摄影** Photograph](https://kexue.fm/category/Photograph)
- [**问题百科** Questions](https://kexue.fm/category/Questions)
- [**生活/情感** Life-Feeling](https://kexue.fm/category/Life-Feeling)
- [**资源共享** Resources](https://kexue.fm/category/Resources)

- [**千奇百怪**](https://kexue.fm/category/Everything)
- [**天文探索**](https://kexue.fm/category/Astronomy)
- [**数学研究**](https://kexue.fm/category/Mathematics)
- [**物理化学**](https://kexue.fm/category/Phy-chem)
- [**信息时代**](https://kexue.fm/category/Big-Data)
- [**生物自然**](https://kexue.fm/category/Biology)
- [**图片摄影**](https://kexue.fm/category/Photograph)
- [**问题百科**](https://kexue.fm/category/Questions)
- [**生活/情感**](https://kexue.fm/category/Life-Feeling)
- [**资源共享**](https://kexue.fm/category/Resources)

[首页](https://kexue.fm) [信息时代](https://kexue.fm/category/Big-Data) Seq2Seq中Exposure Bias现象的浅析与对策

9Mar

# [Seq2Seq中Exposure Bias现象的浅析与对策](https://kexue.fm/archives/7259)

By 苏剑林 \|
2020-03-09 \|
104202位读者\|

前些天笔者写了 [《CRF用过了，不妨再了解下更快的MEMM？》](https://kexue.fm/archives/7213)，里边提到了MEMM的局部归一化和CRF的全局归一化的优劣。同时，笔者联想到了Seq2Seq模型，因为Seq2Seq模型的典型训练方案Teacher Forcing就是一个局部归一化模型，所以它也存在着局部归一化所带来的毛病——也就是我们经常说的“Exposure Bias”。带着这个想法，笔者继续思考了一翻，将最后的思考结果记录在此文。

经典的Seq2Seq模型图示

本文算是一篇进阶文章，适合对Seq2Seq模型已经有一定的了解、希望进一步提升模型的理解或表现的读者。关于Seq2Seq的入门文章，可以阅读旧作 [《玩转Keras之seq2seq自动生成标题》](https://kexue.fm/archives/5861) 和 [《从语言模型到Seq2Seq：Transformer如戏，全靠Mask》](https://kexue.fm/archives/6933)。

本文的内容大致为：

> 1、Exposure Bias的成因分析及例子；
>
> 2、简单可行的缓解Exposure Bias问题的策略。

## Softmax [\#](https://kexue.fm/archives/7259\#Softmax)

首先，我们来回顾Softmax相关内容。大家都知道，对于向量$(x\_1,x\_2,\\dots,x\_n)$，它的Softmax为
\\begin{equation}(p\_1,p\_2,\\dots,p\_n)=\\frac{1}{\\sum\\limits\_{i=1}^n e^{x\_i}}\\left(e^{x\_1},e^{x\_2},\\dots,e^{x\_n}\\right)\\end{equation}
由于$e^t$是关于$t$的严格单调递增函数，所以如果$x\_k$是$x\_1,x\_2,\\dots,x\_n$中的最大者，那么$p\_k$也是$p\_1,p\_2,\\dots,p\_n$中的最大者。

对于分类问题，我们所用的loss一般是交叉熵，也就是
\\begin{equation}-\\log p\_t = \\log\\left(\\sum\\limits\_{i=1}^n e^{x\_i}\\right) - x\_t\\end{equation}
其中$t$是目标类。如文章 [《寻求一个光滑的最大值函数》](https://kexue.fm/archives/3290) 所述，上式第一项实际上是$\\max\\left(x\_1,x\_2,\\dots,x\_n\\right)$的光滑近似，所以为了形象理解交叉熵，我们可以写出
\\begin{equation}-\\log p\_t \\approx \\max\\left(x\_1,x\_2,\\dots,x\_n\\right) - x\_t\\end{equation}
也就是说，交叉熵实际上在缩小目标类得分$x\_t$与全局最大值的差距，显然这个差距最小只能为0，并且此时目标类得分就是最大值者。所以，Softmax加交叉熵的效果就是“希望目标类的得分成为最大值”。

## Teacher Forcing [\#](https://kexue.fm/archives/7259\#Teacher%20Forcing)

现在，我们来看Seq2Seq，它通过条件分解来建模联合概率分布：
\\begin{equation}\\begin{aligned}p(\\boldsymbol{y}\|\\boldsymbol{x})=&\\,p(y\_1,y\_2,\\dots,y\_n\|\\boldsymbol{x})\\\
=&\\,p(y\_1\|\\boldsymbol{x})p(y\_2\|\\boldsymbol{x},y\_1)\\dots p(y\_n\|\\boldsymbol{x},y\_1,\\dots,y\_{n-1})
\\end{aligned}\\end{equation}
每一项自然也就用Softmax来建模的，即
\\begin{equation}\\begin{aligned}&p(y\_1\|\\boldsymbol{x})=\\frac{e^{f(y\_1;\\boldsymbol{x})}}{\\sum\\limits\_{y\_1}e^{f(y\_1;\\boldsymbol{x})}},\\\
&p(y\_2\|\\boldsymbol{x},y\_1)=\\frac{e^{f(y\_1,y\_2;\\boldsymbol{x})}}{\\sum\\limits\_{y\_2}e^{f(y\_1,y\_2;\\boldsymbol{x})}},\\\
&\\dots,\\\
&p(y\_n\|\\boldsymbol{x},y\_1,\\dots,y\_{n-1})=\\frac{e^{f(y\_1,y\_2,\\dots,y\_n;\\boldsymbol{x})}}{\\sum\\limits\_{y\_n}e^{f(y\_1,y\_2,\\dots,y\_n;\\boldsymbol{x})}}
\\end{aligned}\\end{equation}
乘起来就是
\\begin{equation}p(\\boldsymbol{y}\|\\boldsymbol{x})=\\frac{e^{f(y\_1;\\boldsymbol{x})+f(y\_1,y\_2;\\boldsymbol{x})+\\dots+f(y\_1,y\_2,\\dots,y\_n;\\boldsymbol{x})}}{\\left(\\sum\\limits\_{y\_1}e^{f(y\_1;\\boldsymbol{x})}\\right)\\left(\\sum\\limits\_{y\_2}e^{f(y\_1,y\_2;\\boldsymbol{x})}\\right)\\dots\\left(\\sum\\limits\_{y\_n}e^{f(y\_1,y\_2,\\dots,y\_n;\\boldsymbol{x})}\\right)}\\label{eq:join-target}\\end{equation}
而训练目标就是
\\begin{equation}-\\log p(\\boldsymbol{y}\|\\boldsymbol{x})=-\\log p(y\_1\|\\boldsymbol{x})-\\log p(y\_2\|\\boldsymbol{x},y\_1)-\\dots -\\log p(y\_n\|\\boldsymbol{x},y\_1,\\dots,y\_{n-1})\\end{equation}
这个直接的训练目标就叫做Teacher Forcing，因为在算$-\\log p(y\_2\|\\boldsymbol{x},y\_1)$的时候我们要知道真实的$y\_1$，在算$-\\log p(y\_3\|\\boldsymbol{x},y\_1,y\_2)$我们需要知道真实的$y\_1,y\_2$，依此类推，这就好像有一个经验丰富的老师预先给我们铺好了大部分的路，让我们只需要求下一步即可。这种方法训练起来简单，而且结合CNN或Transformer那样的模型就可以实现并行的训练，但它可能会带来Exposure Bias问题。

## Exposure Bias [\#](https://kexue.fm/archives/7259\#Exposure%20Bias)

其实Teacher Forcing这个名称本身就意味着它本身会存在Exposure Bias问题。回想一下老师教学生解题的过程，一般的步骤为：

> 1、第一步应该怎么思考；
>
> 2、第一步想出来后，第二步我们有哪些选择；
>
> 3、确定了第二步后，第三步我们可以怎么做；
>
> ...
>
> n、有了这n-1步后，最后一步就不难想到了。

这个过程其实跟Seq2Seq的Teacher Forcing方案的假设是一样的。有过教学经验的读者就知道，通常来说学生们都能听得频频点头，感觉全都懂了，然后让学生课后自己做题，多数还是一脸懵比。为什么会这样呢？其中一个原因就是Exposure Bias。说白了，问题就在于，老师总是假设学生能想到前面若干步后，然后教学生下一步，但如果前面有一步想错了或者想不出来呢？这时候这个过程就无法进行下去了，也就是没法得到正确答案了，这就是Exposure Bias问题。

## Beam Search [\#](https://kexue.fm/archives/7259\#Beam%20Search)

事实上，我们真正做题的时候并不总是这样子，假如我们卡在某步无法确定时，我们就遍历几种选择，然后继续推下去，看后面的结果反过来辅助我们确定前面无法确定的那步。对应到Seq2Seq来说，这其实就相当于基于Beam Search的解码过程。

对于Beam Search，我们应该能发现，beam size并不是越大越好，有些情况甚至是beam size等于1时最好，这看起来有点不合理，因为beam size越大，理论上找到的序列就越接近最优序列，所以应该越有可能正确才对。事实上这也算是Exposure Bias的现象之一。

从式$\\eqref{eq:join-target}$我们可以看出，Seq2Seq对目标序列$y\_1,y\_2,\\dots,y\_n$的打分函数为：
\\begin{equation}f(y\_1;\\boldsymbol{x})+f(y\_1,y\_2;\\boldsymbol{x})+\\dots+f(y\_1,y\_2,\\dots,y\_n;\\boldsymbol{x})\\end{equation}
正常来说，我们希望目标序列是所有候选序列之中分数最高的，根据本文开头介绍的Softmax方法，我们建立的概率分布应该是
\\begin{equation}p(\\boldsymbol{y}\|\\boldsymbol{x})=\\frac{e^{f(y\_1;\\boldsymbol{x})+f(y\_1,y\_2;\\boldsymbol{x})+\\dots+f(y\_1,y\_2,\\dots,y\_n;\\boldsymbol{x})}}{\\sum\\limits\_{y\_1,y\_2,\\dots,y\_n}e^{f(y\_1;\\boldsymbol{x})+f(y\_1,y\_2;\\boldsymbol{x})+\\dots+f(y\_1,y\_2,\\dots,y\_n;\\boldsymbol{x})}}\\label{eq:ideal-target}\\end{equation}
但上式的分母需要遍历所有路径求和，难以实现，而式$\\eqref{eq:join-target}$就作为一种折衷的选择得到了广泛应用。但式$\\eqref{eq:join-target}$跟式$\\eqref{eq:ideal-target}$并不等价，因此哪怕模型已经成功优化，也可能出现“最优序列并不是目标序列”的现象。

## 简单例子 [\#](https://kexue.fm/archives/7259\#%E7%AE%80%E5%8D%95%E4%BE%8B%E5%AD%90)

我们来举一个简单例子。设序列长度只有2，候选序列是$(a,b)$和$(c,d)$，而目标序列是$(a,b)$，训练完成后，模型的概率分布情况为
$$\\begin{array}{c\|c}
\\hline
p(a) & p(c)\\\
\\hline
0.6 & 0.4 \\\
\\hline
\\end{array}\\qquad \\begin{array}{c\|c\|c\|c}
\\hline
p(b\|a) & p(d\|a) & p(b\|c) & p(d\|c)\\\
\\hline
0.55 & 0.45 & 0.1 & 0.9\\\
\\hline
\\end{array}$$

如果beam size为1，那么因为$p(a) > p(c)$，所以第一步只能输出$a$，接着因为$p(b\|a) > p(d\|a)$，所以第二步只能输出$b$，成功输出了正确序列$(a,b)$。但如果beam size为2，那么第一步输出$(a,0.6),(c,0.4)$，而第二步遍历所有组合，我们得到
\\begin{array}{c\|c\|c\|c}
\\hline
(a, b) & (a, d) & (c, b) & (c, d)\\\
\\hline
0.33 & 0.27 & 0.04 & 0.36\\\
\\hline
\\end{array}
所以输出了错误的序列$(c,d)$。

那是因为模型没训练好吗？并不是，前面说过Softmax加交叉熵的目的就是让目标的得分最大，对于第一步我们有$p(a) > p(c)$，所以第一步的训练目标已经达到了，而第二步在$a$已经预先知道的前提下我们有$p(b\|a) > p(d\|a)$，这说明第二步的训练目标也达到了。因此，模型已经算是训练好了，只不过可能因为模型表达能力限制等原因，得分并没有特别高，但“让目标的得分最大”这个目标已经完成了。

## 思考对策 [\#](https://kexue.fm/archives/7259\#%E6%80%9D%E8%80%83%E5%AF%B9%E7%AD%96)

从上述例子中读者或许可以看出问题所在了：主要是$p(d\|c)$太高了，而$p(d\|c)$是没有经过训练的，没有任何显式的机制去抑制$p(d\|c)$变大，因此就出现了“最优序列并不是目标序列”的现象。

看到这里，读者可能就能想到一个朴素的对策了：添加额外的优化目标，降低那些Beam Search出来的非目标序列不就行了？事实上，这的确是一个有效的解决方法，相关结果发表在2016年的论文 [《Sequence-to-Sequence Learning as Beam-Search Optimization》](https://papers.cool/arxiv/1606.02960)。但这样一来几乎要求每步训练前的每个样本都要进行一次Beam Search，计算成本太大。还有一些更新的结果，比如ACL 2019的最佳长论文 [《Bridging the Gap between Training and Inference for Neural Machine Translation》](https://papers.cool/arxiv/1906.02448) 就是聚焦于解决Exposure Bias问题。此外，通过强化学习直接优化BLEU等方法，也能一定程度上缓解Exposure Bias。

然而，据笔者所了解，这些致力于解决Exposure Bias的方法，大部分都是大刀阔斧地改动了训练过程，甚至会牺牲原来模型的训练并行性（需要递归地采样负样本，如果模型本身是RNN那倒无妨，但如果本身是CNN或Transformer，那伤害就很大了），成本的提升幅度比效果的提升幅度大得多。

## 构建负样本 [\#](https://kexue.fm/archives/7259\#%E6%9E%84%E5%BB%BA%E8%B4%9F%E6%A0%B7%E6%9C%AC)

纵观大部分解决Exposure Bias的论文，以及结合我们前面的例子和体会，不难想到，其主要思想就是构造有代表性的负样本，然后在训练过程中降低这些负样本的概率，所以问题就是如何构造“有代表性”的负样本了。这里给出笔者构思的一种简单策略，实验证明它能一定程度上缓解Exposure Bias，提升文本生成的表现，重要的是，这种策略比较简单，基本能做到即插即用，几乎不损失训练性能。

方法很简单，就是随机替换一下Decoder的输入词（Decoder的输入词有个专门的名字，叫做oracle words），如下图所示：

一种缓解Exposure Bias的简单策略：直接将Decoder的部分输入词随机替换为别的词。

其中紫色的\[R\]代表被随机替换的词。其实不少Exposure Bias的论文也是这个思路，只不过随机选词的方案不一样。笔者提出的方案很简单：

> 1、50%的概率不做改变；
>
> 2、50%的概率把输入序列中30%的词替换掉，替换对象为原目标序列的任意一个词。

也就是说，随机替换发生概率是50%，随机替换的比例是30%，随机抽取空间就是目标序列的词集。这个策略的灵感在于：尽管Seq2Seq不一定能完全生成目标序列，但它通常能生成大部分目标序列的词（但顺序可能不对，或者重复出现同一些词），因此这样替换后的输入序列通常可以作为有代表性的负样本。对了，说明一下，50%和30%这两个比例纯粹是拍脑袋的，没仔细调参，因为生成模型调一次实在是太累了。

效果如何呢？笔者做了两个标题（摘要）生成的实验（就是 [CLGE](https://github.com/CLUEbenchmark/CLGE) 的前两个），其中baseline是 [task\_seq2seq\_autotitle\_csl.py](https://github.com/bojone/bert4keras/blob/master/examples/task_seq2seq_autotitle_csl.py)，代码开源于：

> **Github地址：** [https://github.com/bojone/exposure\_bias](https://github.com/bojone/exposure_bias)

结果如下表：
\\begin{array}{c}
\\text{CSL标题生成实验结果}\\\
{\\begin{array}{c\|c\|cccc}
\\hline
& \\text{beam size} & \\text{Rouge-L} & \\text{Rouge-1} & \\text{Rouge-2} & \\text{BLEU} \\\
\\hline
\\text{baseline} & 1 & 63.81 & 65.45 & 54.91 & 45.52 \\\
\\text{随机替换} & 1 & \\textbf{64.44} & \\textbf{66.09} & \\textbf{55.56} & \\textbf{46.1} \\\
\\hline
\\text{baseline} & 2 & 64.44 & 66.09 & 55.75 & 46.39 \\\
\\text{随机替换} & 2 & \\textbf{65.04} & \\textbf{66.75} & \\textbf{56.51} & \\textbf{47.19} \\\
\\hline
\\text{baseline} & 3 & 64.75 & 66.34 & 56.06 & 46.7 \\\
\\text{随机替换} & 3 & \\textbf{65.15} & \\textbf{66.96} & \\textbf{56.74} & \\textbf{47.42} \\\
\\hline
\\end{array}}\\\
\\\
\\text{LCSTS摘要生成实验结果}\\\
{\\begin{array}{c\|c\|cccc}
\\hline
& \\text{beam size} & \\text{Rouge-L} & \\text{Rouge-1} & \\text{Rouge-2} & \\text{BLEU} \\\
\\hline
\\text{baseline} & 1 & 27.99 & 29.57 & \\textbf{18.04} & \\textbf{11.72} \\\
\\text{随机替换} & 1 & \\textbf{28.61} & \\textbf{29.92} & 17.72 & 11.23 \\\
\\hline
\\text{baseline} & 2 & \\textbf{29.2} & 30.7 & \\textbf{19.17} & \\textbf{12.64} \\\
\\text{随机替换} & 2 & 29.15 & \\textbf{30.79} & 18.56 & 11.75 \\\
\\hline
\\text{baseline} & 3 & \\textbf{29.45} & \\textbf{30.95} & \\textbf{19.5} & \\textbf{12.93} \\\
\\text{随机替换} & 3 & 29.14 & 30.88 & 18.76 & 11.91 \\\
\\hline
\\end{array}}
\\end{array}

可以发现，在CSL任务中，基于随机替换的策略稳定提升了文本生成的所有指标，而LCSTS任务的各个指标则各有优劣，考虑到LCSTS本身比较难，各项指标本来就低，所以应该说CSL的结果更有说服力一些。这表明，笔者提出的上述策略确实是一种值得尝试的方案。（注：所有实验都重复了两次然后取平均，所以实验结果应该是比较可靠的了。）

## 对抗训练 [\#](https://kexue.fm/archives/7259\#%E5%AF%B9%E6%8A%97%E8%AE%AD%E7%BB%83)

思考到这里，我们不妨再“天马行空”一下：既然解决Exposure Bias的思路之一就是要构造有代表性的负样本输入，说白了就是让模型在扰动下依然能预测正确，而前些天我们不是才讨论了一种生成扰动样本的方法吗？不错，那就是 [对抗训练](https://kexue.fm/archives/7234)。如果直接往baseline模型里边加入对抗训练，能不能提升模型的性能呢？简单起见，笔者做了往baseline模型里边梯度惩罚（也算是对抗训练的一种）的实验，结果对比如下：
\\begin{array}{c}
\\text{CSL标题生成实验结果}\\\
{\\begin{array}{c\|c\|cccc}
\\hline
& \\text{beam size} & \\text{Rouge-L} & \\text{Rouge-1} & \\text{Rouge-2} & \\text{BLEU} \\\
\\hline
\\text{baseline} & 1 & 63.81 & 65.45 & 54.91 & 45.52 \\\
\\text{随机替换} & 1 & 64.44 & 66.09 & 55.56 & 46.1 \\\
\\text{梯度惩罚} & 1 & \\textbf{65.41} & \\textbf{67.29} & \\textbf{56.64} & \\textbf{47.37} \\\
\\hline
\\text{baseline} & 2 & 64.44 & 66.09 & 55.75 & 46.39 \\\
\\text{随机替换} & 2 & 65.04 & 66.75 & 56.51 & 47.19 \\\
\\text{梯度惩罚} & 2 & \\textbf{65.94} & \\textbf{67.84} & \\textbf{57.38} & \\textbf{48.16} \\\
\\hline
\\text{baseline} & 3 & 64.75 & 66.34 & 56.06 & 46.7 \\\
\\text{随机替换} & 3 & 65.15 & 66.96 & 56.74 & 47.42 \\\
\\text{梯度惩罚} & 3 & \\textbf{66.1} & \\textbf{68.08} & \\textbf{57.7} & \\textbf{48.56} \\\
\\hline
\\end{array}}\\\
\\\
\\text{LCSTS摘要生成实验结果}\\\
{\\begin{array}{c\|c\|cccc}
\\hline
& \\text{beam size} & \\text{Rouge-L} & \\text{Rouge-1} & \\text{Rouge-2} & \\text{BLEU} \\\
\\hline
\\text{baseline} & 1 & 27.99 & 29.57 & 18.04 & 11.72 \\\
\\text{随机替换} & 1 & 28.61 & 29.92 & 17.72 & 11.23 \\\
\\text{梯度惩罚} & 1 & \\textbf{30.75} & \\textbf{31.83} & \\textbf{19.38} & \\textbf{11.78} \\\
\\hline
\\text{baseline} & 2 & 29.2 & 30.7 & 19.17 & \\textbf{12.64} \\\
\\text{随机替换} & 2 & 29.15 & 30.79 & 18.56 & 11.75 \\\
\\text{梯度惩罚} & 2 & \\textbf{30.88} & \\textbf{32.19} & \\textbf{19.96} & 12.32 \\\
\\hline
\\text{baseline} & 3 & 29.45 & 30.95 & 19.5 & \\textbf{12.93} \\\
\\text{随机替换} & 3 & 29.14 & 30.88 & 18.76 & 11.91 \\\
\\text{梯度惩罚} & 3 & \\textbf{30.39} & \\textbf{31.76} & \\textbf{19.74} & 12.14 \\\
\\hline
\\end{array}}
\\end{array}

可以看到，对抗训练（梯度惩罚）进一步提升了CSL生成的所有指标，而LCSTS上主要提升的是Roune指标，BLEU则有所下降。因此，对抗训练也可以列入“提升文本生成模型的潜力技巧”名单之中。

## 本文小结 [\#](https://kexue.fm/archives/7259\#%E6%9C%AC%E6%96%87%E5%B0%8F%E7%BB%93)

本文讨论了Seq2Seq中的Exposure Bias现象，尝试从直观上和理论上分析Exposure Bias的原因，并给出了简单可行的缓解Exposure Bias问题的对策，其中包括笔者构思的一种随机替换策略，以及基于对抗训练的策略，这两种策略的好处是它们几乎是即插即用的，并且实验表明它们能一定程度上提升文本生成的各个指标。

_**转载到请包括本文地址：** [https://kexue.fm/archives/7259](https://kexue.fm/archives/7259)_

_**更详细的转载事宜请参考：**_ [《科学空间FAQ》](https://kexue.fm/archives/6508#%E6%96%87%E7%AB%A0%E5%A6%82%E4%BD%95%E8%BD%AC%E8%BD%BD/%E5%BC%95%E7%94%A8)

**如果您还有什么疑惑或建议，欢迎在下方评论区继续讨论。**

**如果您觉得本文还不错，欢迎 [分享](https://kexue.fm/archives/7259#share)/ [打赏](https://kexue.fm/archives/7259#pay) 本文。打赏并非要从中获得收益，而是希望知道科学空间获得了多少读者的真心关注。当然，如果你无视它，也不会影响你的阅读。再次表示欢迎和感谢！**

打赏

微信打赏

支付宝打赏

因为网站后台对打赏并无记录，因此欢迎在打赏时候备注留言。你还可以 [**点击这里**](http://mail.qq.com/cgi-bin/qm_share?t=qm_mailme&email=tN7d1drY3drrx8H0xcWa19vZ) 或在下方评论区留言来告知你的建议或需求。

**如果您需要引用本文，请参考：**

苏剑林. (Mar. 09, 2020). 《Seq2Seq中Exposure Bias现象的浅析与对策 》\[Blog post\]. Retrieved from [https://kexue.fm/archives/7259](https://kexue.fm/archives/7259)

@online{kexuefm-7259,
        title={Seq2Seq中Exposure Bias现象的浅析与对策},
        author={苏剑林},
        year={2020},
        month={Mar},
        url={\\url{https://kexue.fm/archives/7259}},
}

分类： [信息时代](https://kexue.fm/category/Big-Data)    标签： [语言模型](https://kexue.fm/tag/%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/), [文本生成](https://kexue.fm/tag/%E6%96%87%E6%9C%AC%E7%94%9F%E6%88%90/), [对抗训练](https://kexue.fm/tag/%E5%AF%B9%E6%8A%97%E8%AE%AD%E7%BB%83/)[33 评论](https://kexue.fm/archives/7259#comments)

< [对抗训练浅谈：意义、方法和思考（附Keras实现）](https://kexue.fm/archives/7234) \| [现在可以用Keras玩中文GPT2了（GPT2\_ML）](https://kexue.fm/archives/7292) >

### 你也许还对下面的内容感兴趣

- [Transformer升级之路：20、MLA究竟好在哪里？](https://kexue.fm/archives/10907)
- [Transformer升级之路：19、第二类旋转位置编码](https://kexue.fm/archives/10862)
- [Decoder-only的LLM为什么需要位置编码？](https://kexue.fm/archives/10347)
- [Monarch矩阵：计算高效的稀疏型矩阵分解](https://kexue.fm/archives/10249)
- [缓存与效果的极限拉扯：从MHA、MQA、GQA到MLA](https://kexue.fm/archives/10091)
- [时空之章：将Attention视为平方复杂度的RNN](https://kexue.fm/archives/10017)
- [我在Performer中发现了Transformer-VQ的踪迹](https://kexue.fm/archives/9862)
- [预训练一下，Transformer的长序列成绩还能涨不少！](https://kexue.fm/archives/9787)
- [脑洞大开：非线性RNN居然也可以并行计算？](https://kexue.fm/archives/9783)
- [大词表语言模型在续写任务上的一个问题及对策](https://kexue.fm/archives/9762)

[发表你的看法](https://kexue.fm/archives/7259#comment_form)

1. [«](https://kexue.fm/archives/7259/comment-page-1#comments)
2. [1](https://kexue.fm/archives/7259/comment-page-1#comments)
3. [2](https://kexue.fm/archives/7259/comment-page-2#comments)

blue\_sky

November 20th, 2020

csl只有训练集和验证集，没有测试集，能够提供下测试集网盘下载地址吗

[回复评论](https://kexue.fm/archives/7259/comment-page-2?replyTo=14842#respond-post-7259)

Virgil

July 20th, 2021

你好 拜读了https://github.com/bojone/exposure\_bias/blob/master/random\_replacement.py中的源码，有个小疑问想请教一下：

代码第96行：
train\_model = Model(model.inputs + \[o\_in\], model.outputs + \[o\_in\])
Model中的outputs为什么要加\[o\_in\]，因为在第101行也只是用到train\_model.output\[0\]的值，个人认为第96行可以改成：
train\_model = Model(model.inputs + \[o\_in\], model.outputs)
不知这样理解是否正确？

[回复评论](https://kexue.fm/archives/7259/comment-page-2?replyTo=16953#respond-post-7259)

[苏剑林](https://kexue.fm) 发表于
July 20th, 2021

这是我的习惯而已（有输入就有输出），如果你删掉没有报错，那就继续删掉好了。

[回复评论](https://kexue.fm/archives/7259/comment-page-2?replyTo=16958#respond-post-7259)

tanglifu

August 3rd, 2021

这个exposure bias 这个bingo 在一篇文章中有提到，schedule 的方法，采用输入端一定概率使用前一步结果，一定概率使用正确结果，并且随着训练加深，把概率给减小到0

[回复评论](https://kexue.fm/archives/7259/comment-page-2?replyTo=17049#respond-post-7259)

[苏剑林](https://kexue.fm) 发表于
August 3rd, 2021

问题是“前一步的结果”需要串行地预测，牺牲了并行性，得不偿失。

[回复评论](https://kexue.fm/archives/7259/comment-page-2?replyTo=17050#respond-post-7259)

1. [«](https://kexue.fm/archives/7259/comment-page-1#comments)
2. [1](https://kexue.fm/archives/7259/comment-page-1#comments)
3. [2](https://kexue.fm/archives/7259/comment-page-2#comments)

[取消回复](https://kexue.fm/archives/7259#respond-post-7259)

你的大名

电子邮箱

个人网站（选填）

1\. 可以使用LaTeX代码，点击“预览效果”可查看效果；2. 可以通过点击评论楼层编号来引用该楼层；3. 网站可能会有点卡，如非确认评论失败，请 **不要重复点击提交**。

### 内容速览

[Softmax](https://kexue.fm/archives/7259#Softmax)
[Teacher Forcing](https://kexue.fm/archives/7259#Teacher%20Forcing)
[Exposure Bias](https://kexue.fm/archives/7259#Exposure%20Bias)
[Beam Search](https://kexue.fm/archives/7259#Beam%20Search)
[简单例子](https://kexue.fm/archives/7259#%E7%AE%80%E5%8D%95%E4%BE%8B%E5%AD%90)
[思考对策](https://kexue.fm/archives/7259#%E6%80%9D%E8%80%83%E5%AF%B9%E7%AD%96)
[构建负样本](https://kexue.fm/archives/7259#%E6%9E%84%E5%BB%BA%E8%B4%9F%E6%A0%B7%E6%9C%AC)
[对抗训练](https://kexue.fm/archives/7259#%E5%AF%B9%E6%8A%97%E8%AE%AD%E7%BB%83)
[本文小结](https://kexue.fm/archives/7259#%E6%9C%AC%E6%96%87%E5%B0%8F%E7%BB%93)

### 智能搜索

支持整句搜索！网站自动使用 [结巴分词](https://github.com/fxsjy/jieba) 进行分词，并结合ngrams排序算法给出合理的搜索结果。

### 热门标签

[生成模型](https://kexue.fm/tag/%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B/) [attention](https://kexue.fm/tag/attention/) [优化](https://kexue.fm/tag/%E4%BC%98%E5%8C%96/) [语言模型](https://kexue.fm/tag/%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/) [模型](https://kexue.fm/tag/%E6%A8%A1%E5%9E%8B/) [网站](https://kexue.fm/tag/%E7%BD%91%E7%AB%99/) [概率](https://kexue.fm/tag/%E6%A6%82%E7%8E%87/) [梯度](https://kexue.fm/tag/%E6%A2%AF%E5%BA%A6/) [转载](https://kexue.fm/tag/%E8%BD%AC%E8%BD%BD/) [微分方程](https://kexue.fm/tag/%E5%BE%AE%E5%88%86%E6%96%B9%E7%A8%8B/) [矩阵](https://kexue.fm/tag/%E7%9F%A9%E9%98%B5/) [天象](https://kexue.fm/tag/%E5%A4%A9%E8%B1%A1/) [分析](https://kexue.fm/tag/%E5%88%86%E6%9E%90/) [深度学习](https://kexue.fm/tag/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/) [积分](https://kexue.fm/tag/%E7%A7%AF%E5%88%86/) [python](https://kexue.fm/tag/python/) [优化器](https://kexue.fm/tag/%E4%BC%98%E5%8C%96%E5%99%A8/) [力学](https://kexue.fm/tag/%E5%8A%9B%E5%AD%A6/) [无监督](https://kexue.fm/tag/%E6%97%A0%E7%9B%91%E7%9D%A3/) [扩散](https://kexue.fm/tag/%E6%89%A9%E6%95%A3/) [几何](https://kexue.fm/tag/%E5%87%A0%E4%BD%95/) [节日](https://kexue.fm/tag/%E8%8A%82%E6%97%A5/) [生活](https://kexue.fm/tag/%E7%94%9F%E6%B4%BB/) [文本生成](https://kexue.fm/tag/%E6%96%87%E6%9C%AC%E7%94%9F%E6%88%90/) [数论](https://kexue.fm/tag/%E6%95%B0%E8%AE%BA/)

### 随机文章

- [随机分词再探：从Viterbi Sampling到完美采样算法](https://kexue.fm/archives/9811)
- [通过ssh动态端口转发共享校园资源（附带干货）](https://kexue.fm/archives/3651)
- [蘑菇的最优形状模型](https://kexue.fm/archives/1339)
- [为什么是抛物线？——聚光面研究](https://kexue.fm/archives/1055)
- [《向量》系列——3.当天体力学遇到向量(1)](https://kexue.fm/archives/740)
- [沉痛,默哀！中国科学巨星钱学森逝世](https://kexue.fm/archives/232)
- [用热传导方程来指导自监督学习](https://kexue.fm/archives/9359)
- [生成扩散模型漫谈（二十六）：基于恒等式的蒸馏（下）](https://kexue.fm/archives/10567)
- [科学空间：2010年10月重要天象](https://kexue.fm/archives/951)
- [2009年目视流星雨星历表](https://kexue.fm/archives/87)

### 最近评论

- [苏剑林](https://kexue.fm/archives/481/comment-page-1#comment-27835): 刚入门那会的文章，不用深究了。
- [苏剑林](https://kexue.fm/archives/10122/comment-page-1#comment-27834): 目前各方面的实测效果看来不会，我觉得本质上就是因为partial rope的实测效果优于rop...
- [苏剑林](https://kexue.fm/archives/10958/comment-page-1#comment-27833): 首先，瞬时速度为什么跟$t$无关？其次，现在reflow和meanflow的第一、第二目标，不...
- [苏剑林](https://kexue.fm/archives/10907/comment-page-2#comment-27832): 对于每一步数据都严格对齐来说，0.01的loss差距不小了，因为它代表了每一个step的los...
- [苏剑林](https://kexue.fm/archives/10699/comment-page-1#comment-27831): \[comment=27808\]rpsun\[/comment\]
有人这样做了：https://a...
- [苏剑林](https://kexue.fm/archives/1490/comment-page-1#comment-27830): 自己都没怎么关注天象了，惭愧
- [苏剑林](https://kexue.fm/archives/10958/comment-page-1#comment-27829): 原来如此。其实只要预测空间是连续空间，并且任务本质是一对多的输出，那么都有可能关联到Diffu...
- [苏剑林](https://kexue.fm/archives/10945/comment-page-1#comment-27828): 你可以拿一批语料去eval，看各个expert分别激活了多少次呀。
- [苏剑林](https://kexue.fm/archives/9164/comment-page-4#comment-27827): 首先这个分布肯定是存在的（贝叶斯公式无关分布），然后它的概率密度对数是二次函数形式，概率密度的...
- [苏剑林](https://kexue.fm/archives/10907/comment-page-2#comment-27826): 这两天看了下FoPE，感觉它的分析有点道理，但它实现的代码跟论文其实是不一样的。看论文的描述，...

### 友情链接

- [Cool Papers](https://papers.cool)
- [数学研发](https://bbs.emath.ac.cn)
- [Seatop](http://www.seatop.com.cn/)
- [Xiaoxia](https://xiaoxia.org/)
- [积分表-网络版](https://kexue.fm/sci/integral/index.html)
- [丝路博傲](http://blog.dvxj.com/)
- [ph4ntasy 饭特稀](http://www.ph4ntasy.com/)
- [数学之家](http://www.2math.cn/)
- [有趣天文奇观](http://interesting-sky.china-vo.org/)
- [TwistedW](http://www.twistedwg.com/)
- [godweiyang](https://godweiyang.com/)
- [AI柠檬](https://blog.ailemon.net/)
- [王登科-DK博客](https://greatdk.com)
- [ESON](https://blog.eson.org/)
- [枫之羽](https://fzhiy.net/)
- [Mathor's blog](https://wmathor.com/)
- [coding-zuo](https://coding-zuo.github.io/)
- [博科园](https://www.bokeyuan.net/)
- [孔皮皮的博客](https://www.kppkkp.top/)
- [运鹏的博客](https://yunpengtai.top/)
- [jiming.site](https://jiming.site/)
- [OmegaXYZ](https://www.omegaxyz.com/)
- [Blog by Eacls](https://www.eacls.top/)
- [EAI猩球](https://www.robotech.ink/)
- [文举的博客](https://liwenju0.com/)
- [用代码打点酱油](https://bruceyuan.com/)
- [申请链接](https://kexue.fm/links.html)

本站采用创作共用版权协议，要求署名、非商业用途和保持一致。转载本站内容必须也遵循“ [署名-非商业用途-保持一致](http://creativecommons.org/licenses/by-nc-nd/2.5/cn/)”的创作共用协议。
© 2009-2025 Scientific Spaces. All rights reserved. Theme by [laogui](http://www.laogui.com). Powered by [Typecho](http://typecho.org). 备案号: [粤ICP备09093259号-1/2](https://beian.miit.gov.cn/)。