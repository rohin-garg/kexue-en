## SEARCH

## MENU

- [打赏](https://kexue.fm/reward.html)
- [公式](https://kexue.fm/latex.html)
- [天象](https://kexue.fm/ac.html)
- [链接](https://kexue.fm/links.html)
- [时光](https://kexue.fm/me.html)
- [博览](https://kexue.fm/science.html)
- [归档](https://kexue.fm/content.html)

## CATEGORIES

- [千奇百怪](https://kexue.fm/category/Everything)
- [天文探索](https://kexue.fm/category/Astronomy)
- [数学研究](https://kexue.fm/category/Mathematics)
- [物理化学](https://kexue.fm/category/Phy-chem)
- [信息时代](https://kexue.fm/category/Big-Data)
- [生物自然](https://kexue.fm/category/Biology)
- [图片摄影](https://kexue.fm/category/Photograph)
- [问题百科](https://kexue.fm/category/Questions)
- [生活/情感](https://kexue.fm/category/Life-Feeling)
- [资源共享](https://kexue.fm/category/Resources)

## NEWPOSTS

- [通过msign来计算奇异值裁剪mc...](https://kexue.fm/archives/11059)
- [矩阵符号函数mcsgn能计算什么？](https://kexue.fm/archives/11056)
- [线性注意力简史：从模仿、创新到反哺](https://kexue.fm/archives/11033)
- [msign的导数](https://kexue.fm/archives/11025)
- [通过msign来计算奇异值裁剪mc...](https://kexue.fm/archives/11006)
- [msign算子的Newton-Sc...](https://kexue.fm/archives/10996)
- [等值振荡定理：最优多项式逼近的充要条件](https://kexue.fm/archives/10972)
- [生成扩散模型漫谈（三十）：从瞬时速...](https://kexue.fm/archives/10958)
- [MoE环游记：5、均匀分布的反思](https://kexue.fm/archives/10945)
- [msign算子的Newton-Sc...](https://kexue.fm/archives/10922)

## COMMENTS

- [苏剑林: 如果你把你这里提到的数学都学通透了，数学基础基本上可以胜任95...](https://kexue.fm/archives/9119/comment-page-13#comment-27983)
- [苏剑林: 我跑过这个项目，效果是能复现的。“在 CIFAR-10 上效果...](https://kexue.fm/archives/10958/comment-page-2#comment-27982)
- [Henry Zha: 苏神你好，我是一名管理科学与工程专业的博士生，研究方向是结合人...](https://kexue.fm/archives/9119/comment-page-13#comment-27981)
- [SunlightZero: 我根据 https://github.com/haidog-y...](https://kexue.fm/archives/10958/comment-page-2#comment-27980)
- [苏剑林: 噢，是笔误，更正了，感谢指出。](https://kexue.fm/archives/11025/comment-page-1#comment-27979)
- [苏剑林: 这里有很多因素。如果推理数据跟训练数据同分布，那么理论上就是均...](https://kexue.fm/archives/10945/comment-page-1#comment-27978)
- [苏剑林: 目前看来给O加rmsnorm挺稳的，效果甚至还好点。我其实是直...](https://kexue.fm/archives/11033/comment-page-1#comment-27977)
- [苏剑林: 你应该说的是$\\exp(\\boldsymbol{Q}\\bold...](https://kexue.fm/archives/11033/comment-page-1#comment-27976)
- [苏剑林: 可以这么说吧，通过某种分母归一化的操作，导数格式都类似，毕竟公...](https://kexue.fm/archives/10831/comment-page-1#comment-27975)
- [苏剑林: 这不是得看实测结果嘛～可能对于某些垂直领域会有帮助。](https://kexue.fm/archives/10945/comment-page-1#comment-27974)

## USERLOGIN

- [登录](https://kexue.fm/admin/login.php)

[科学空间\|Scientific Spaces](https://kexue.fm)

- [登录](https://kexue.fm/admin/login.php)
- [打赏](https://kexue.fm/reward.html)
- [公式](https://kexue.fm/latex.html)
- [天象](https://kexue.fm/ac.html)
- [链接](https://kexue.fm/links.html)
- [时光](https://kexue.fm/me.html)
- [博览](https://kexue.fm/science.html)
- [归档](https://kexue.fm/content.html)

渴望成为一个小飞侠

- [欢迎订阅](https://kexue.fm/feed)
- [个性邮箱](https://kexue.fm/archives/119)
- [天象信息](https://kexue.fm/ac.html)
- [观测ISS](https://kexue.fm/archives/41)
- [LaTeX](https://kexue.fm/latex.html)
- [关于博主](https://kexue.fm/me.html)

欢迎访问“科学空间”，这里将与您共同探讨自然科学，回味人生百态；也期待大家的分享～

- [**千奇百怪** Everything](https://kexue.fm/category/Everything)
- [**天文探索** Astronomy](https://kexue.fm/category/Astronomy)
- [**数学研究** Mathematics](https://kexue.fm/category/Mathematics)
- [**物理化学** Phy-chem](https://kexue.fm/category/Phy-chem)
- [**信息时代** Big-Data](https://kexue.fm/category/Big-Data)
- [**生物自然** Biology](https://kexue.fm/category/Biology)
- [**图片摄影** Photograph](https://kexue.fm/category/Photograph)
- [**问题百科** Questions](https://kexue.fm/category/Questions)
- [**生活/情感** Life-Feeling](https://kexue.fm/category/Life-Feeling)
- [**资源共享** Resources](https://kexue.fm/category/Resources)

- [**千奇百怪**](https://kexue.fm/category/Everything)
- [**天文探索**](https://kexue.fm/category/Astronomy)
- [**数学研究**](https://kexue.fm/category/Mathematics)
- [**物理化学**](https://kexue.fm/category/Phy-chem)
- [**信息时代**](https://kexue.fm/category/Big-Data)
- [**生物自然**](https://kexue.fm/category/Biology)
- [**图片摄影**](https://kexue.fm/category/Photograph)
- [**问题百科**](https://kexue.fm/category/Questions)
- [**生活/情感**](https://kexue.fm/category/Life-Feeling)
- [**资源共享**](https://kexue.fm/category/Resources)

[首页](https://kexue.fm) [信息时代](https://kexue.fm/category/Big-Data) 6个派生优化器的简单介绍及其实现

25Nov

# [6个派生优化器的简单介绍及其实现](https://kexue.fm/archives/7094)

By 苏剑林 \|
2019-11-25 \|
61931位读者\|

优化器可能是深度学习最“玄学”的一个模块之一了：有时候换一个优化器就能带来明显的提升，有时候别人说提升很多的优化器用到自己的任务上却一丁点用都没有，理论性质好的优化器不一定工作得很好，纯粹拍脑袋而来的优化器也未必就差了。但不管怎样，优化器终究也为热爱“深度炼丹”的同学提供了多一个选择。

近几年来，关于优化器的工作似乎也在慢慢增多，很多论文都提出了对常用优化器（尤其是 [Adam](https://papers.cool/arxiv/1412.6980)）的大大小小的改进。本文就汇总一些优化器工作或技巧，并统一给出了代码实现，供读者有需调用。

## 基本形式 [\#](https://kexue.fm/archives/7094\#%E5%9F%BA%E6%9C%AC%E5%BD%A2%E5%BC%8F)

所谓“派生”，就是指相关的技巧都是建立在已有的优化器上的，任意一个已有的优化器都可以用上这些技巧，从而变成一个新的优化器。

已有的优化器的基本形式为：
\\begin{equation}\\begin{aligned}\\boldsymbol{g}\_t =&\\, \\nabla\_{\\boldsymbol{\\theta}} L\\\
\\boldsymbol{h}\_t =&\\, f(\\boldsymbol{g}\_{\\leq t})\\\
\\boldsymbol{\\theta}\_{t+1} =&\\, \\boldsymbol{\\theta}\_t - \\gamma \\boldsymbol{h}\_t
\\end{aligned}\\end{equation}
其中$\\boldsymbol{g}\_t$即梯度，而$\\boldsymbol{g}\_{\\leq t}$指的是截止到当前步的所有梯度信息，它们经过某种运算$f$（比如累积动量、累积二阶矩校正学习率等）后得到$\\boldsymbol{h}\_t$，然后由$\\boldsymbol{h}\_t$来更新参数，这里的$\\gamma$就是指学习率。

## 变式杂烩 [\#](https://kexue.fm/archives/7094\#%E5%8F%98%E5%BC%8F%E6%9D%82%E7%83%A9)

下面介绍优化器的6个变式，也可以理解为用优化器时的一些技巧。这些技巧有时候会很有效，有时候也可能无效甚至反作用，不能一概而论，只能理解为多一种选择就多一种可能。

### 权重衰减 [\#](https://kexue.fm/archives/7094\#%E6%9D%83%E9%87%8D%E8%A1%B0%E5%87%8F)

权重衰减指的是直接在优化器每一步更新后面加上一个衰减项：
\\begin{equation}\\begin{aligned}\\boldsymbol{g}\_t =&\\, \\nabla\_{\\boldsymbol{\\theta}} L\\\
\\boldsymbol{h}\_t =&\\, f(\\boldsymbol{g}\_{\\leq t})\\\
\\boldsymbol{\\theta}\_{t+1} =&\\, \\boldsymbol{\\theta}\_t - \\gamma \\boldsymbol{h}\_t - \\gamma \\lambda \\boldsymbol{\\theta}\_t
\\end{aligned}\\end{equation}
其中$\\lambda$称为“衰减率”。在SGD中，权重衰减等价于往loss里边加入$l\_2$正则项$\\frac{1}{2}\\lambda \\Vert \\boldsymbol{\\theta}\\Vert\_2^2$，但在Adagrad、Adam等带有自适应学习率的优化器中，$f$变成了非线性，所以两者不在等价。 [《Decoupled Weight Decay Regularization》](https://papers.cool/arxiv/1711.05101) 一文特别指出权重衰减的防过拟合能力优于对应的$l\_2$正则，推荐大家使用权重衰减而不是$l\_2$正则。

### 层自适应 [\#](https://kexue.fm/archives/7094\#%E5%B1%82%E8%87%AA%E9%80%82%E5%BA%94)

在优化器中，最后的更新量由$\\boldsymbol{h}\_t$和学习率$\\gamma$决定，有时候$\\boldsymbol{h}\_t$的模长会大于参数$\\boldsymbol{\\theta}\_t$的模长，这可能会导致更新的不稳定。所以，一个直接的想法是：每一层的参数的更新幅度应该由$\\boldsymbol{\\theta}\_t$的模长的模长来调控。这个直接的想法就导致了如下的优化器变体：
\\begin{equation}\\begin{aligned}\\boldsymbol{g}\_t =&\\, \\nabla\_{\\boldsymbol{\\theta}} L\\\
\\boldsymbol{h}\_t =&\\, f(\\boldsymbol{g}\_{\\leq t})\\\
\\boldsymbol{\\theta}\_{t+1} =&\\, \\boldsymbol{\\theta}\_t - \\gamma \\boldsymbol{h}\_t\\times \\frac{\\Vert\\boldsymbol{\\theta}\_t\\Vert\_2}{\\Vert\\boldsymbol{h}\_t\\Vert\_2}
\\end{aligned}\\end{equation}
如果基础优化器是Adam，那么上述优化器就是LAMB。论文 [《Large Batch Optimization for Deep Learning: Training BERT in 76 minutes》](https://papers.cool/arxiv/1904.00962) 指出LAMB在batch size较大（成千上万）的时候比Adam效果要好。

### 分段线性学习率 [\#](https://kexue.fm/archives/7094\#%E5%88%86%E6%AE%B5%E7%BA%BF%E6%80%A7%E5%AD%A6%E4%B9%A0%E7%8E%87)

学习率也是优化器中的一个迷之存在，通常来说精调学习率策略能够获得一定提升，而不恰当的学习率甚至可能导致模型不收敛。常见的学习率策略有warmup、指数衰减、断层式下降（比如某个epoch后直接降到原来的1/10）等，比较迷的还有cos型学习率策略、多项式型学习率策略等。

考虑到常见的函数都可以用分段线性函数逼近，所以笔者干脆引入了一个分段线性学习率的策略，供大家随便玩。形式如下：
\\begin{equation}\\begin{aligned}\\boldsymbol{g}\_t =&\\, \\nabla\_{\\boldsymbol{\\theta}} L\\\
\\boldsymbol{h}\_t =&\\, f(\\boldsymbol{g}\_{\\leq t})\\\
\\boldsymbol{\\theta}\_{t+1} =&\\, \\boldsymbol{\\theta}\_t - \\gamma \\rho\_t\\boldsymbol{h}\_t
\\end{aligned}\\end{equation}
其中$\\rho\_t$是某个以步数$t$为自变量的分段线性函数。

### 梯度累积 [\#](https://kexue.fm/archives/7094\#%E6%A2%AF%E5%BA%A6%E7%B4%AF%E7%A7%AF)

梯度累积之前在 [《用时间换取效果：Keras梯度累积优化器》](https://kexue.fm/archives/6794) 一文中也介绍过了，其实不算优化器的变式，但可以写到优化器中，通过小batch size达到大batch size的效果，实现时间换空间。更大的batch size有时候能提升效果，尤其是基准batch size过小的情况下（8以下？）。

重述 [《用时间换取效果：Keras梯度累积优化器》](https://kexue.fm/archives/6794) 一文关于梯度下降的描述：

> 所谓梯度累积，其实很简单，我们梯度下降所用的梯度，实际上是多个样本算出来的梯度的平均值，以batch\_size=128为例，你可以一次性算出128个样本的梯度然后平均，我也可以每次算16个样本的平均梯度，然后缓存累加起来，算够了8次之后，然后把总梯度除以8，然后才执行参数更新。当然，必须累积到了8次之后，用8次的平均梯度才去更新参数，不能每算16个就去更新一次，不然就是batch\_size=16了。

### Lookahead [\#](https://kexue.fm/archives/7094\#Lookahead)

Lookahead优化器来自论文 [《Lookahead Optimizer: k steps forward, 1 step back》](https://papers.cool/arxiv/1907.08610)，在之前的文章 [《Keras实现两个优化器：Lookahead和LazyOptimizer》](https://kexue.fm/archives/6869) 也介绍过。Lookahead的含义是用常用优化器预先往前摸索几步，然后根据摸索结果来更新，流程如下：
\\begin{equation}\\begin{aligned}&\\boldsymbol{g}\_t =\\, \\nabla\_{\\boldsymbol{\\theta}} L\\\
&\\boldsymbol{h}\_t =\\, f(\\boldsymbol{g}\_{\\leq t})\\\
&\\boldsymbol{\\theta}\_{t+1} =\\, \\boldsymbol{\\theta}\_t - \\gamma\\boldsymbol{h}\_t\\\
&\\text{如果}t\\,\\text{mod}\\,k = 0\\text{:}\\\
&\\qquad\\boldsymbol{\\Theta}\_{t+1} = \\boldsymbol{\\Theta}\_t + \\alpha (\\boldsymbol{\\theta}\_{t+1}- \\boldsymbol{\\Theta}\_t)\\\
&\\qquad\\boldsymbol{\\theta}\_{t+1} = \\boldsymbol{\\Theta}\_{t+1} \\,(\\text{即覆盖原来的}\\boldsymbol{\\theta}\_{t+1})
\\end{aligned}\\end{equation}

其实这个优化器叫Lookaback也无妨，也就是每走几步就往后看看，跟几步前的权重做个插值。

### Lazy优化器 [\#](https://kexue.fm/archives/7094\#Lazy%E4%BC%98%E5%8C%96%E5%99%A8)

Lazy优化器在刚才的文章 [《Keras实现两个优化器：Lookahead和LazyOptimizer》](https://kexue.fm/archives/6869) 中也介绍过，其本意就是Embedding层的更新应当稀疏化一些，这有助于防止过拟合。（参考 [知乎讨论](https://www.zhihu.com/question/265357659/answer/580469438)）

## 参考实现 [\#](https://kexue.fm/archives/7094\#%E5%8F%82%E8%80%83%E5%AE%9E%E7%8E%B0)

前面的介绍比较简单，事实上这些变式本身确实不难理解，关键还是代码实现。从前面的介绍中大家可以发现，这6个变式并无矛盾之处，因此良好的实现应当能让我们能搭积木般组合其中一个或多个变式使用；另外，目前keras也有两个分支：纯keras和tf.keras，良好的实现应当能同时兼容它们（或者同时给出两种实现）。

### 终极移花接木 [\#](https://kexue.fm/archives/7094\#%E7%BB%88%E6%9E%81%E7%A7%BB%E8%8A%B1%E6%8E%A5%E6%9C%A8)

虽然部分变式在之前也实现过了，但这里还是用新的方式重新实现了它们。这里的实现方式源于笔者意外发现的一种移花接木技巧。

假设我们有这样一个类：

```
import numpy as np

class A(object):
 def __init__(self):
 self.a = np.ones(1)
 self.b = np.ones(2)
 self.c = np.ones(3)
```

然后假设我们要继承A类得到一个B类，B类是要将 `__init__` 方法的所有 `np.ones` 替换为 `np.zeros`，其余都不变。由于 `__init__` 可能是一个很复杂的流程，如果将它完整复制过来然后改写显然太冗余了。

有没有直接写几行代码就能替换所有的呢？还真有！

```
class B(A):
 def __init__(self):
 _ = np.ones
 np.ones = np.zeros
 super(B, self).__init__()
 np.ones = _
```

有了这个demo，我们就可以“魔改”已有优化器了。在keras中，参数的更新都是通过 `K.update` 来实现的（参考 [keras的optimizers.py](https://github.com/keras-team/keras/blob/master/keras/optimizers.py))，我们只需要用上述方式重新定义一下 `K.update` 就好。

如果是tf.keras呢？很遗憾，这种方式不可行，因为tf.keras中常用优化器的迭代流程都被写到C里边去了（参考 [tf.keras的adam.py](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/keras/optimizer_v2/adam.py)），我们看不到代码，也就不能用这种方法改了。一个解决办法是我们自己重新实现一个优化器如Adam，将迭代流程暴露出来，这样我们就能用上述方式魔改了。

### 使用示例 [\#](https://kexue.fm/archives/7094\#%E4%BD%BF%E7%94%A8%E7%A4%BA%E4%BE%8B)

根据上述思路统一实现的6个优化器变体，都被放在笔者的bert4keras项目中： [**bert4keras.optimizers**](https://github.com/bojone/bert4keras/blob/master/bert4keras/optimizers.py)。

所有函数会根据keras还是tf.keras来进行正确的导入，从而实现keras/tf.keras都能用同样的方式使用。里边自带了一个Adam实现，这个Adam是专门写给tf.keras的。 **对于tf.keras，如果想要实现上述变式，那只能用bert4keras自带的优化器（目前只有Adam），不能用tf.keras内置的优化器。**

参考代码：

```
from bert4keras.optimizers import *

# 变成带权重衰减的Adam
AdamW = extend_with_weight_decay(Adam, 'AdamW')
optimizer = AdamW(learning_rate=0.001, weight_decay_rate=0.01)

# 变成带分段线性学习率的Adam
AdamLR = extend_with_piecewise_linear_lr(Adam, 'AdamLR')
# 实现warmup，前1000步学习率从0增加到0.001
optimizer = AdamLR(learning_rate=0.001, lr_schedule={1000: 1.})

# 变成带梯度累积的Adam
AdamGA = extend_with_gradient_accumulation(Adam, 'AdamGA')
optimizer = AdamGA(learning_rate=0.001, grad_accum_steps=10)

# 组合使用
AdamWLR = extend_with_piecewise_linear_lr(AdamW, 'AdamWLR')
# 带权重衰减和warmup的优化器
optimizer = AdamWLR(learning_rate=0.001,
 weight_decay_rate=0.01,
 lr_schedule={1000: 1.})

```

**（注：一下实现这么多个优化器，而且同时要兼容keras和tf.keras，难免可能会有错漏之处，如有发现，万望不吝指正。）**

## 写在最后 [\#](https://kexue.fm/archives/7094\#%E5%86%99%E5%9C%A8%E6%9C%80%E5%90%8E)

炼丹不易，且炼且珍惜。

_**转载到请包括本文地址：** [https://kexue.fm/archives/7094](https://kexue.fm/archives/7094)_

_**更详细的转载事宜请参考：**_ [《科学空间FAQ》](https://kexue.fm/archives/6508#%E6%96%87%E7%AB%A0%E5%A6%82%E4%BD%95%E8%BD%AC%E8%BD%BD/%E5%BC%95%E7%94%A8)

**如果您还有什么疑惑或建议，欢迎在下方评论区继续讨论。**

**如果您觉得本文还不错，欢迎 [分享](https://kexue.fm/archives/7094#share)/ [打赏](https://kexue.fm/archives/7094#pay) 本文。打赏并非要从中获得收益，而是希望知道科学空间获得了多少读者的真心关注。当然，如果你无视它，也不会影响你的阅读。再次表示欢迎和感谢！**

打赏

微信打赏

支付宝打赏

因为网站后台对打赏并无记录，因此欢迎在打赏时候备注留言。你还可以 [**点击这里**](http://mail.qq.com/cgi-bin/qm_share?t=qm_mailme&email=tN7d1drY3drrx8H0xcWa19vZ) 或在下方评论区留言来告知你的建议或需求。

**如果您需要引用本文，请参考：**

苏剑林. (Nov. 25, 2019). 《6个派生优化器的简单介绍及其实现 》\[Blog post\]. Retrieved from [https://kexue.fm/archives/7094](https://kexue.fm/archives/7094)

@online{kexuefm-7094,
        title={6个派生优化器的简单介绍及其实现},
        author={苏剑林},
        year={2019},
        month={Nov},
        url={\\url{https://kexue.fm/archives/7094}},
}

分类： [信息时代](https://kexue.fm/category/Big-Data)    标签： [python](https://kexue.fm/tag/python/), [keras](https://kexue.fm/tag/keras/), [优化器](https://kexue.fm/tag/%E4%BC%98%E5%8C%96%E5%99%A8/)[10 评论](https://kexue.fm/archives/7094#comments)

< [n维空间下两个随机向量的夹角分布](https://kexue.fm/archives/7076) \| [级联抑制：提升GAN表现的一种简单有效的方法](https://kexue.fm/archives/7105) >

### 你也许还对下面的内容感兴趣

- [msign算子的Newton-Schulz迭代（下）](https://kexue.fm/archives/10996)
- [msign算子的Newton-Schulz迭代（上）](https://kexue.fm/archives/10922)
- [高阶muP：更简明但更高明的谱条件缩放](https://kexue.fm/archives/10795)
- [初探muP：超参数的跨模型尺度迁移规律](https://kexue.fm/archives/10770)
- [Muon续集：为什么我们选择尝试Muon？](https://kexue.fm/archives/10739)
- [为什么梯度裁剪的默认模长是1？](https://kexue.fm/archives/10657)
- [从谱范数梯度到新式权重衰减的思考](https://kexue.fm/archives/10648)
- [Muon优化器赏析：从向量到矩阵的本质跨越](https://kexue.fm/archives/10592)
- [从Hessian近似看自适应学习率优化器](https://kexue.fm/archives/10588)
- [Adam的epsilon如何影响学习率的Scaling Law？](https://kexue.fm/archives/10563)

[发表你的看法](https://kexue.fm/archives/7094#comment_form)

云天河

March 15th, 2020

执行优化器说 无learning\_rate 参数

[回复评论](https://kexue.fm/archives/7094/comment-page-1?replyTo=13000#respond-post-7094)

[苏剑林](https://kexue.fm) 发表于
March 15th, 2020

keras < 2.3.0 需要改learning\_rate为lr，这些只需要去看看keras的源码就知道答案了。

[回复评论](https://kexue.fm/archives/7094/comment-page-1?replyTo=13003#respond-post-7094)

木子多多

July 4th, 2020

苏神，bert4keras里的这个extend\_with\_gradient\_accumulation是不是有bug? 现在训一个unilm 优化器用AdamGA loss不会下降，但换成Adam或AdamW的时候都很正常

[回复评论](https://kexue.fm/archives/7094/comment-page-1?replyTo=13726#respond-post-7094)

挠挠

September 24th, 2020

感谢大佬分享！！

[回复评论](https://kexue.fm/archives/7094/comment-page-1?replyTo=14416#respond-post-7094)

Virgil

August 21st, 2021

苏神 请教一下 打比赛用Keras的时候，要求结果复现。
于是，我在主程序最前面加上了
random.seed(SEED)
os.environ\['PYTHONHASHSEED'\] = str(SEED)
np.random.seed(SEED)
tf.set\_random\_seed(SEED)
但是，结果仍然无法复现...需要怎么改吗？还是说keras的确存在无法复现的问题...而且经过测试优化器使用SGD可以完全复现，但是使用Adam或者RMSprop结果不能完全复现

具体环境如下：
TensorFlow backend (yes / no): yes
TensorFlow version: 1.14.0
Keras version:2.3.1
Python version: 3.6.13
CUDA/cuDNN version: Cuda compilation tools, release 10.0, V10.0.130
GPU model and memory: Tesla V100-DGXS-32GB

[回复评论](https://kexue.fm/archives/7094/comment-page-1?replyTo=17168#respond-post-7094)

[苏剑林](https://kexue.fm) 发表于
August 21st, 2021

tf + gpu基本上都没什么完全的可复现性，不要做这个尝试了～

[回复评论](https://kexue.fm/archives/7094/comment-page-1?replyTo=17174#respond-post-7094)

barpor 发表于
December 12th, 2023

我也遇到这个问题，在pytorch多次实验发现，只有把Adam系的beta2改得很小（变回SGD），才能保证复现

[回复评论](https://kexue.fm/archives/7094/comment-page-1?replyTo=23287#respond-post-7094)

Bro

May 4th, 2022

哈喽,请教一个问题, Adam优化器有权重衰减变种AdamW, 即将weight decay decouple开, AdamW优化器在很多任务中被采用.那么为啥LAMB优化器的权重衰减变种(假设叫做LAMBW,hhh)却没有看到大家使用呢? 这里头存在什么更深的原因吗?

[回复评论](https://kexue.fm/archives/7094/comment-page-1?replyTo=19063#respond-post-7094)

[苏剑林](https://kexue.fm) 发表于
May 5th, 2022

你说的LAMBW，是指将weight decay放到LAMB外边？LAMB本身是想着通过参数的模长来控制更新量的模长，将weight decay放到LAMB外边不符合LAMB的设计初衷。

[回复评论](https://kexue.fm/archives/7094/comment-page-1?replyTo=19069#respond-post-7094)

[参数更新-深度学习优化算法\_Johngo学长](https://www.johngo689.com/6523/)

August 8th, 2022

\[...\]6个派生优化器的简单介绍及其实现 – 科学空间\[...\]

[回复评论](https://kexue.fm/archives/7094/comment-page-1?replyTo=19586#respond-post-7094)

[取消回复](https://kexue.fm/archives/7094#respond-post-7094)

你的大名

电子邮箱

个人网站（选填）

1\. 可以使用LaTeX代码，点击“预览效果”可查看效果；2. 可以通过点击评论楼层编号来引用该楼层；3. 网站可能会有点卡，如非确认评论失败，请 **不要重复点击提交**。

### 内容速览

[基本形式](https://kexue.fm/archives/7094#%E5%9F%BA%E6%9C%AC%E5%BD%A2%E5%BC%8F)
[变式杂烩](https://kexue.fm/archives/7094#%E5%8F%98%E5%BC%8F%E6%9D%82%E7%83%A9)
[权重衰减](https://kexue.fm/archives/7094#%E6%9D%83%E9%87%8D%E8%A1%B0%E5%87%8F)
[层自适应](https://kexue.fm/archives/7094#%E5%B1%82%E8%87%AA%E9%80%82%E5%BA%94)
[分段线性学习率](https://kexue.fm/archives/7094#%E5%88%86%E6%AE%B5%E7%BA%BF%E6%80%A7%E5%AD%A6%E4%B9%A0%E7%8E%87)
[梯度累积](https://kexue.fm/archives/7094#%E6%A2%AF%E5%BA%A6%E7%B4%AF%E7%A7%AF)
[Lookahead](https://kexue.fm/archives/7094#Lookahead)
[Lazy优化器](https://kexue.fm/archives/7094#Lazy%E4%BC%98%E5%8C%96%E5%99%A8)
[参考实现](https://kexue.fm/archives/7094#%E5%8F%82%E8%80%83%E5%AE%9E%E7%8E%B0)
[终极移花接木](https://kexue.fm/archives/7094#%E7%BB%88%E6%9E%81%E7%A7%BB%E8%8A%B1%E6%8E%A5%E6%9C%A8)
[使用示例](https://kexue.fm/archives/7094#%E4%BD%BF%E7%94%A8%E7%A4%BA%E4%BE%8B)
[写在最后](https://kexue.fm/archives/7094#%E5%86%99%E5%9C%A8%E6%9C%80%E5%90%8E)

### 智能搜索

支持整句搜索！网站自动使用 [结巴分词](https://github.com/fxsjy/jieba) 进行分词，并结合ngrams排序算法给出合理的搜索结果。

### 热门标签

[生成模型](https://kexue.fm/tag/%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B/) [attention](https://kexue.fm/tag/attention/) [优化](https://kexue.fm/tag/%E4%BC%98%E5%8C%96/) [语言模型](https://kexue.fm/tag/%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/) [模型](https://kexue.fm/tag/%E6%A8%A1%E5%9E%8B/) [网站](https://kexue.fm/tag/%E7%BD%91%E7%AB%99/) [概率](https://kexue.fm/tag/%E6%A6%82%E7%8E%87/) [梯度](https://kexue.fm/tag/%E6%A2%AF%E5%BA%A6/) [转载](https://kexue.fm/tag/%E8%BD%AC%E8%BD%BD/) [微分方程](https://kexue.fm/tag/%E5%BE%AE%E5%88%86%E6%96%B9%E7%A8%8B/) [矩阵](https://kexue.fm/tag/%E7%9F%A9%E9%98%B5/) [天象](https://kexue.fm/tag/%E5%A4%A9%E8%B1%A1/) [分析](https://kexue.fm/tag/%E5%88%86%E6%9E%90/) [深度学习](https://kexue.fm/tag/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/) [积分](https://kexue.fm/tag/%E7%A7%AF%E5%88%86/) [python](https://kexue.fm/tag/python/) [优化器](https://kexue.fm/tag/%E4%BC%98%E5%8C%96%E5%99%A8/) [力学](https://kexue.fm/tag/%E5%8A%9B%E5%AD%A6/) [无监督](https://kexue.fm/tag/%E6%97%A0%E7%9B%91%E7%9D%A3/) [扩散](https://kexue.fm/tag/%E6%89%A9%E6%95%A3/) [几何](https://kexue.fm/tag/%E5%87%A0%E4%BD%95/) [节日](https://kexue.fm/tag/%E8%8A%82%E6%97%A5/) [生活](https://kexue.fm/tag/%E7%94%9F%E6%B4%BB/) [文本生成](https://kexue.fm/tag/%E6%96%87%E6%9C%AC%E7%94%9F%E6%88%90/) [数论](https://kexue.fm/tag/%E6%95%B0%E8%AE%BA/)

### 随机文章

- [BLOG评论故障修复，部分数据丢失](https://kexue.fm/archives/31)
- [我是一个费曼迷](https://kexue.fm/archives/3204)
- [轻微的扰动——摄动法简介(1)](https://kexue.fm/archives/1878)
- [从梯度最大化看Attention的Scale操作](https://kexue.fm/archives/9812)
- [外出集训，网站暂停更新...](https://kexue.fm/archives/694)
- [解答不等式的误区...](https://kexue.fm/archives/644)
- [关于交错级数的审敛法则](https://kexue.fm/archives/159)
- [从费马大定理谈起（十一）：有理点与切割线法](https://kexue.fm/archives/2996)
- [【翻译】星空之夜：夏季恒星的色彩](https://kexue.fm/archives/2038)
- [月球上的多角度反射镜](https://kexue.fm/archives/904)

### 最近评论

- [苏剑林](https://kexue.fm/archives/9119/comment-page-13#comment-27983): 如果你把你这里提到的数学都学通透了，数学基础基本上可以胜任95%以上的场景了吧？至于“直觉”这...
- [苏剑林](https://kexue.fm/archives/10958/comment-page-2#comment-27982): 我跑过这个项目，效果是能复现的。“在 CIFAR-10 上效果非常差，生成的图片都是模糊的”是...
- [Henry Zha](https://kexue.fm/archives/9119/comment-page-13#comment-27981): 苏神你好，我是一名管理科学与工程专业的博士生，研究方向是结合人工智能模型建模用户行为之类的管理...
- [SunlightZero](https://kexue.fm/archives/10958/comment-page-2#comment-27980): 我根据 https://github.com/haidog-yaqub/MeanFlow 尝试...
- [苏剑林](https://kexue.fm/archives/11025/comment-page-1#comment-27979): 噢，是笔误，更正了，感谢指出。
- [苏剑林](https://kexue.fm/archives/10945/comment-page-1#comment-27978): 这里有很多因素。如果推理数据跟训练数据同分布，那么理论上就是均匀分布，但实际上同分布假设不一定...
- [苏剑林](https://kexue.fm/archives/11033/comment-page-1#comment-27977): 目前看来给O加rmsnorm挺稳的，效果甚至还好点。我其实是直接在flash attentio...
- [苏剑林](https://kexue.fm/archives/11033/comment-page-1#comment-27976): 你应该说的是$\\exp(\\boldsymbol{Q}\\boldsymbol{K}^{\\top}...
- [苏剑林](https://kexue.fm/archives/10831/comment-page-1#comment-27975): 可以这么说吧，通过某种分母归一化的操作，导数格式都类似，毕竟公式$(f/g)'=f'/g-fg...
- [苏剑林](https://kexue.fm/archives/10945/comment-page-1#comment-27974): 这不是得看实测结果嘛～可能对于某些垂直领域会有帮助。

### 友情链接

- [Cool Papers](https://papers.cool)
- [数学研发](https://bbs.emath.ac.cn)
- [Seatop](http://www.seatop.com.cn/)
- [Xiaoxia](https://xiaoxia.org/)
- [积分表-网络版](https://kexue.fm/sci/integral/index.html)
- [丝路博傲](http://blog.dvxj.com/)
- [ph4ntasy 饭特稀](http://www.ph4ntasy.com/)
- [数学之家](http://www.2math.cn/)
- [有趣天文奇观](http://interesting-sky.china-vo.org/)
- [TwistedW](http://www.twistedwg.com/)
- [godweiyang](https://godweiyang.com/)
- [AI柠檬](https://blog.ailemon.net/)
- [王登科-DK博客](https://greatdk.com)
- [ESON](https://blog.eson.org/)
- [枫之羽](https://fzhiy.net/)
- [Mathor's blog](https://wmathor.com/)
- [coding-zuo](https://coding-zuo.github.io/)
- [博科园](https://www.bokeyuan.net/)
- [孔皮皮的博客](https://www.kppkkp.top/)
- [运鹏的博客](https://yunpengtai.top/)
- [jiming.site](https://jiming.site/)
- [OmegaXYZ](https://www.omegaxyz.com/)
- [Blog by Eacls](https://www.eacls.top/)
- [EAI猩球](https://www.robotech.ink/)
- [文举的博客](https://liwenju0.com/)
- [用代码打点酱油](https://bruceyuan.com/)
- [申请链接](https://kexue.fm/links.html)

本站采用创作共用版权协议，要求署名、非商业用途和保持一致。转载本站内容必须也遵循“ [署名-非商业用途-保持一致](http://creativecommons.org/licenses/by-nc-nd/2.5/cn/)”的创作共用协议。
© 2009-2025 Scientific Spaces. All rights reserved. Theme by [laogui](http://www.laogui.com). Powered by [Typecho](http://typecho.org). 备案号: [粤ICP备09093259号-1/2](https://beian.miit.gov.cn/)。