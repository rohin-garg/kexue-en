Loading \[MathJax\]/jax/output/HTML-CSS/fonts/TeX/fontdata.js

![MobileSideBar](https://kexue.fm/usr/themes/geekg/images/slide-button.png)

## SEARCH

## MENU

- [打赏](https://kexue.fm/reward.html)
- [公式](https://kexue.fm/latex.html)
- [天象](https://kexue.fm/ac.html)
- [链接](https://kexue.fm/links.html)
- [时光](https://kexue.fm/me.html)
- [博览](https://kexue.fm/science.html)
- [归档](https://kexue.fm/content.html)

## CATEGORIES

- [千奇百怪](https://kexue.fm/category/Everything)
- [天文探索](https://kexue.fm/category/Astronomy)
- [数学研究](https://kexue.fm/category/Mathematics)
- [物理化学](https://kexue.fm/category/Phy-chem)
- [信息时代](https://kexue.fm/category/Big-Data)
- [生物自然](https://kexue.fm/category/Biology)
- [图片摄影](https://kexue.fm/category/Photograph)
- [问题百科](https://kexue.fm/category/Questions)
- [生活/情感](https://kexue.fm/category/Life-Feeling)
- [资源共享](https://kexue.fm/category/Resources)

## NEWPOSTS

- [让炼丹更科学一些（五）：基于梯度精...](https://kexue.fm/archives/11530)
- [让炼丹更科学一些（四）：新恒等式，...](https://kexue.fm/archives/11494)
- [为什么DeltaNet要加L2 N...](https://kexue.fm/archives/11486)
- [让炼丹更科学一些（三）：SGD的终...](https://kexue.fm/archives/11480)
- [让炼丹更科学一些（二）：将结论推广...](https://kexue.fm/archives/11469)
- [滑动平均视角下的权重衰减和学习率](https://kexue.fm/archives/11459)
- [生成扩散模型漫谈（三十一）：预测数...](https://kexue.fm/archives/11428)
- [Muon优化器指南：快速上手与关键细节](https://kexue.fm/archives/11416)
- [AdamW的Weight RMS的...](https://kexue.fm/archives/11404)
- [n个正态随机数的最大值的渐近估计](https://kexue.fm/archives/11390)

## COMMENTS

- [Bin: 今天偶然从某个论坛看到有人推荐您的博客，定睛一看竟然是华师同院...](https://kexue.fm/archives/1990/comment-page-2#comment-29105)
- [Rapture D: 我有一个问题，为什么不考虑亥姆霍兹定理和斯托克斯公式。](https://kexue.fm/archives/11530/comment-page-1#comment-29104)
- [mofheka: 苏神是还在用jax是么？最近在做基于Google Pathwa...](https://kexue.fm/archives/11390/comment-page-1#comment-29103)
- [长琴: 看懂这篇博客也不是一件容易的事情。](https://kexue.fm/archives/11530/comment-page-1#comment-29102)
- [AlexLi: 苏老师，请教一下(7)式中将 \\mu(x\_t) 传给 $p...](https://kexue.fm/archives/9257/comment-page-4#comment-29101)
- [tyler\_zxc: "Performer的思想是将标准的Attention线性化，...](https://kexue.fm/archives/7921/comment-page-2#comment-29100)
- [我: 似乎并非mHC提出矩阵的思想？之前hyper connecti...](https://kexue.fm/archives/11494/comment-page-1#comment-29099)
- [winter: 苏神您好，假如对于比较均匀的attention weightP...](https://kexue.fm/archives/10847/comment-page-1#comment-29098)
- [苏剑林: KL散度、JS散度、W距离啥的，都行啊，看你喜欢哪个](https://kexue.fm/archives/8512/comment-page-2#comment-29097)
- [苏剑林: 没有绝对公平的对比方法，主要看你关心什么。比如，如果只关心推理...](https://kexue.fm/archives/9119/comment-page-14#comment-29096)

## USERLOGIN

- [登录](https://kexue.fm/admin/login.php)

[科学空间\|Scientific Spaces](https://kexue.fm/)

- [登录](https://kexue.fm/admin/login.php)
- [打赏](https://kexue.fm/reward.html)
- [公式](https://kexue.fm/latex.html)
- [天象](https://kexue.fm/ac.html)
- [链接](https://kexue.fm/links.html)
- [时光](https://kexue.fm/me.html)
- [博览](https://kexue.fm/science.html)
- [归档](https://kexue.fm/content.html)

渴望成为一个小飞侠

- [![](https://kexue.fm/usr/themes/geekg/images/rss.png)\\
\\
欢迎订阅](https://kexue.fm/feed)
- [![](https://kexue.fm/usr/themes/geekg/images/mail.png)\\
\\
个性邮箱](https://kexue.fm/archives/119)
- [![](https://kexue.fm/usr/themes/geekg/images/Saturn.png)\\
\\
天象信息](https://kexue.fm/ac.html)
- [![](https://kexue.fm/usr/themes/geekg/images/iss.png)\\
\\
观测ISS](https://kexue.fm/archives/41)
- [![](https://kexue.fm/usr/themes/geekg/images/pi.png)\\
\\
LaTeX](https://kexue.fm/latex.html)
- [![](https://kexue.fm/usr/themes/geekg/images/mlogo.png)\\
\\
关于博主](https://kexue.fm/me.html)

欢迎访问“科学空间”，这里将与您共同探讨自然科学，回味人生百态；也期待大家的分享～

- [**千奇百怪** Everything](https://kexue.fm/category/Everything)
- [**天文探索** Astronomy](https://kexue.fm/category/Astronomy)
- [**数学研究** Mathematics](https://kexue.fm/category/Mathematics)
- [**物理化学** Phy-chem](https://kexue.fm/category/Phy-chem)
- [**信息时代** Big-Data](https://kexue.fm/category/Big-Data)
- [**生物自然** Biology](https://kexue.fm/category/Biology)
- [**图片摄影** Photograph](https://kexue.fm/category/Photograph)
- [**问题百科** Questions](https://kexue.fm/category/Questions)
- [**生活/情感** Life-Feeling](https://kexue.fm/category/Life-Feeling)
- [**资源共享** Resources](https://kexue.fm/category/Resources)

- [**千奇百怪**](https://kexue.fm/category/Everything)
- [**天文探索**](https://kexue.fm/category/Astronomy)
- [**数学研究**](https://kexue.fm/category/Mathematics)
- [**物理化学**](https://kexue.fm/category/Phy-chem)
- [**信息时代**](https://kexue.fm/category/Big-Data)
- [**生物自然**](https://kexue.fm/category/Biology)
- [**图片摄影**](https://kexue.fm/category/Photograph)
- [**问题百科**](https://kexue.fm/category/Questions)
- [**生活/情感**](https://kexue.fm/category/Life-Feeling)
- [**资源共享**](https://kexue.fm/category/Resources)

[首页](https://kexue.fm/) [信息时代](https://kexue.fm/category/Big-Data) SVD分解(三)：连Word2Vec都只不过是个SVD？

23Feb

# [SVD分解(三)：连Word2Vec都只不过是个SVD？](https://kexue.fm/archives/4233)

By 苏剑林 \|
2017-02-23 \|
123875位读者 \|

这篇文章要带来一个“重磅”消息，如标题所示，居然连大名鼎鼎的深度学习词向量工具Word2Vec都只不过是个SVD！

当然，Word2Vec的超级忠实粉丝们，你们也不用太激动，这里只是说模型结构上是等价的，并非完全等价，Word2Vec还是有它的独特之处。只不过，经过我这样解释之后，估计很多问题就可以类似想通了。

## 词向量=one hot [\#](https://kexue.fm/archives/4233\#%E8%AF%8D%E5%90%91%E9%87%8F=one%20hot)

让我们先来回顾一下去年的一篇文章 [《词向量与Embedding究竟是怎么回事？》](https://kexue.fm/archives/4122/)，这篇文章主要说的是：所谓Embedding层，就是一个one hot的全连接层罢了（再次强调，这里说的完全等价，而不是“相当于”），而词向量，就是这个全连接层的参数；至于Word2Vec，就通过大大简化的语言模型来训练Embedding层，从而得到词向量（它的优化技巧有很多，但模型结构就只是这么简单）；词向量能够减少过拟合风险，是因为用Word2Vec之类的工具、通过大规模语料来无监督地预训练了这个Embedding层，而跟one hot还是Embedding还是词向量本身没啥关系。

有了这个观点后，马上可以解释我们以前的一个做法为什么可行了。在做情感分类问题时，如果有了词向量，想要得到句向量，最简单的一个方案就是直接对句子中的词语的词向量求和或者求平均，这约能达到85%的准确率。事实上这也是facebook出品的文本分类工具FastText的做法了（FastText还多引入了ngram特征，来缓解词序问题，但总的来说，依旧是把特征向量求平均来得到句向量）。为什么这么一个看上去毫不直观的、简单粗暴的方案也能达到这么不错的准确率？

回到我们用one hot的时代，我们是这样表示一个句子的。假如我们将“我”、“爱”、“科学”、“空间”、“不”、“错”六个词用下面的one hot编码：

\\begin{array}{c\|c}\\hline\\text{我} & \[1, 0, 0, 0, 0, 0\]\\\
\\text{爱} & \[0, 1, 0, 0, 0, 0\]\\\
\\text{科学} & \[0, 0, 1, 0, 0, 0\]\\\
\\text{空间} & \[0, 0, 0, 1, 0, 0\]\\\
\\text{不} & \[0, 0, 0, 0, 1, 0\]\\\
\\text{错} & \[0, 0, 0, 0, 0, 1\]\\\
\\hline
\\end{array}

那么不考虑词序的话，“我爱空间科学”这个短语就可以用下面的向量（词袋）表示：

\\begin{pmatrix}1 & 1 & 1 & 1 & 0 & 0\\end{pmatrix}

有了这个向量之后，要用神经网络做分类，后面可以接一个全连接层，隐藏节点为3：

\\begin{aligned}&\\begin{pmatrix}1 & 1 & 1 & 1 & 0 & 0\\end{pmatrix}\\begin{pmatrix}w\_{11} & w\_{12} & w\_{13}\\\
w\_{21} & w\_{22} & w\_{23}\\\
w\_{31} & w\_{32} & w\_{33}\\\
w\_{41} & w\_{42} & w\_{43}\\\
w\_{51} & w\_{52} & w\_{53}\\\
w\_{61} & w\_{62} & w\_{63}\\end{pmatrix}\\\
=&\\begin{pmatrix}w\_{11}+w\_{21}+w\_{31}+w\_{41} & w\_{12}+w\_{22}+w\_{32}+w\_{42} & w\_{13}+w\_{23}+w\_{33}+w\_{43}\\end{pmatrix}\\end{aligned}

咦？这不就是将前4个向量拿出来相加吗？

这样情况就清晰了。 **如果我们用传统的词袋模型，不考虑词序，然后后面接一个全连接层。为了防止过拟合，这个全连接层的参数用预训练的词向量代替，那么结果就是等价于直接将对应的词向量取出，然后求和！也就是说，词向量求和得到句向量，实际上就是传统的词袋模型的等价物！**

## Word2Vec=SVD？ [\#](https://kexue.fm/archives/4233\#Word2Vec=SVD%EF%BC%9F)

自始至终，笔者谈及Word2Vec与SVD的等价性时，都会带一个问号。这是因为，它们两者的模型结构是等价的，但实现方式又不一样，这算不算等价，只能看读者心中对于“等价”的定义了。

事实上，词向量这个概念很早就有了，当时还不叫Word Embedding，是叫distributed representation，即分布式表示，其本意就是词的上下文能够帮助我们理解我们这个词。现在假设总词表有N个词，用one hot表示就是将每个词表示成一个N维的向量；而分布式表示，就是设想开一个窗口（前后若干个词加上当前词，作为一个窗口），然后统计当前词的前后若干个词的分布情况，就用这个分布情况来表示当前词，而这个分布也可以用相应的N维的向量来表示。由于是通过上下文分布来表示一个词，而不再是孤立地“独热”了，因此能够表示语义的相关性，但问题是它还是N维的，维度还是太大了，整个词向量表（共现矩阵）太稀疏。

怎么办呢？其实数学家早就有解决办法了，对于稀疏矩阵，一个既能够降维，又可以提高泛化能力的方案就是对矩阵SVD分解。而本系列的第一篇就说了，SVD分解等价于一个三层的自编码器，于是放到今天来看，这种方案就是说：原始的分布式表示的词向量是N维的，太大，我们可以用自编码器来降低维度嘛，自编码器的中间节点数记为n，把n设置为一个适当的值，训练完自编码器后，直接把中间层的n维结果就作为新的词向量就行了。所以说，这就是一个N维输入，中间节点为n个，N维输出的自编码器方案，也等价于一个SVD分解。

那么，还没有说到Word2Vec呢？Word2Vec的模型又要怎么看呢？Word2Vec的一个CBOW方案是，将前后若干个词的词向量求和，然后接一个N维的全连接层，并做一个softmax来预测当前词的概率。本文的前半部分说了，这种词向量求和，等价于原来的词袋模型接一个全连接层（这个全连接层的参数就是词向量表），这样来看，Word2Vec也只是一个N维输入，中间节点为n个，N维输出的三层神经网络罢了，所以从网络结构上来看，它跟自编码器等价，也就是跟SVD分解等价。

从实现上来看，区别也很明显：

> 1、Word2Vec的这种方案，可以看作是通过前后词来预测当前词，而自编码器或者SVD则是通过前后词来预测前后词；
>
> 2、Word2Vec最后接的是softmax来预测概率，也就是说实现了一个非线性变换，而自编码器或者SVD并没有。

这两个区别在多大程度上影响了词向量的质量，我没有严格的数学证明，但是从实际测试来看，相同的语料情况下，Word2Vec的词向量质量似乎更胜一筹。

## 不是总结的总结 [\#](https://kexue.fm/archives/4233\#%E4%B8%8D%E6%98%AF%E6%80%BB%E7%BB%93%E7%9A%84%E6%80%BB%E7%BB%93)

本文通过一系列头脑风暴，来思考传统的分布式表示和Word2Vec的模型的练习，最后得到这样的结果。这样的思考的最主要的一个目的是把各个方面的东西联系起来，使我们的认识更加贴近本质，而这或许会对我们应用模型甚至构建模型都起到重要的指导作用。总的来说，这系列的几篇文章告诉我们，将很多运算或者模型写成矩阵形式后，能够看出很多东西的本质，并且有助于引导我们思考改进的方向，比如很多模型实际上可以写成矩阵乘法，然后矩阵乘法就相当于一层神经网络，既然神经网络，那是不是可以加多层？是不是可以加激活函数？是不是可以用SVD分解一下？因为根据本系列第二篇，SVD具有明确的聚类意义。诸如此类的思考，就可以帮助我们搭建模型甚至泛化模型了。

_**转载到请包括本文地址：** [https://kexue.fm/archives/4233](https://kexue.fm/archives/4233 "SVD分解(三)：连Word2Vec都只不过是个SVD？")_

_**更详细的转载事宜请参考：**_ [《科学空间FAQ》](https://kexue.fm/archives/6508#%E6%96%87%E7%AB%A0%E5%A6%82%E4%BD%95%E8%BD%AC%E8%BD%BD/%E5%BC%95%E7%94%A8 "《科学空间FAQ》")

**如果您还有什么疑惑或建议，欢迎在下方评论区继续讨论。**

**如果您觉得本文还不错，欢迎 [分享](https://kexue.fm/archives/4233#share)/ [打赏](https://kexue.fm/archives/4233#pay) 本文。打赏并非要从中获得收益，而是希望知道科学空间获得了多少读者的真心关注。当然，如果你无视它，也不会影响你的阅读。再次表示欢迎和感谢！**

打赏

![科学空间](https://kexue.fm/usr/themes/geekg/payment/wx.png)

微信打赏

![科学空间](https://kexue.fm/usr/themes/geekg/payment/zfb.png)

支付宝打赏

因为网站后台对打赏并无记录，因此欢迎在打赏时候备注留言。

你还可以 [**点击这里**](http://mail.qq.com/cgi-bin/qm_share?t=qm_mailme&email=tN7d1drY3drrx8H0xcWa19vZ) 或在下方评论区留言来告知你的建议或需求。

**如果您需要引用本文，请参考：**

苏剑林. (Feb. 23, 2017). 《SVD分解(三)：连Word2Vec都只不过是个SVD？ 》\[Blog post\]. Retrieved from [https://kexue.fm/archives/4233](https://kexue.fm/archives/4233)

@online{kexuefm-4233,

         title={SVD分解(三)：连Word2Vec都只不过是个SVD？},

         author={苏剑林},

         year={2017},

         month={Feb},

         url={\\url{https://kexue.fm/archives/4233}},

}


分类： [信息时代](https://kexue.fm/category/Big-Data)    标签： [深度学习](https://kexue.fm/tag/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/), [SVD](https://kexue.fm/tag/SVD/), [词向量](https://kexue.fm/tag/%E8%AF%8D%E5%90%91%E9%87%8F/)[27 评论](https://kexue.fm/archives/4233#comments)

< [Python的多进程编程技巧](https://kexue.fm/archives/4231 "Python的多进程编程技巧") \| [【中文分词系列】 7\. 深度学习分词？只需一个词典！](https://kexue.fm/archives/4245 "【中文分词系列】 7. 深度学习分词？只需一个词典！") >

### 你也许还对下面的内容感兴趣

- [通过msign来计算奇异值裁剪mclip（下）](https://kexue.fm/archives/11059 "通过msign来计算奇异值裁剪mclip（下）")
- [通过msign来计算奇异值裁剪mclip（上）](https://kexue.fm/archives/11006 "通过msign来计算奇异值裁剪mclip（上）")
- [SVD的导数](https://kexue.fm/archives/10878 "SVD的导数")
- [低秩近似之路（二）：SVD](https://kexue.fm/archives/10407 "低秩近似之路（二）：SVD")
- [为什么需要残差？一个来自DeepNet的视角](https://kexue.fm/archives/8994 "为什么需要残差？一个来自DeepNet的视角")
- [多任务学习漫谈（三）：分主次之序](https://kexue.fm/archives/8907 "多任务学习漫谈（三）：分主次之序")
- [多任务学习漫谈（二）：行梯度之事](https://kexue.fm/archives/8896 "多任务学习漫谈（二）：行梯度之事")
- [多任务学习漫谈（一）：以损失之名](https://kexue.fm/archives/8870 "多任务学习漫谈（一）：以损失之名")
- [关于维度公式“n > 8.33 log N”的可用性分析](https://kexue.fm/archives/8711 "关于维度公式“n > 8.33 log N”的可用性分析")
- [一个二值化词向量模型，是怎么跟果蝇搭上关系的？](https://kexue.fm/archives/8159 "一个二值化词向量模型，是怎么跟果蝇搭上关系的？")

[发表你的看法](https://kexue.fm/archives/4233#comment_form)

1. [«](https://kexue.fm/archives/4233/comment-page-1#comments)
2. [1](https://kexue.fm/archives/4233/comment-page-1#comments)
3. [2](https://kexue.fm/archives/4233/comment-page-2#comments)

[深度学习推荐系统-笔记05：embedding技术 R11; 稳住·能赢](https://notlate.cn/p/%e6%b7%b1%e5%ba%a6%e5%ad%a6%e4%b9%a0%e6%8e%a8%e8%8d%90%e7%b3%bb%e7%bb%9f-%e7%ac%94%e8%ae%b005%ef%bc%9aembedding%e6%8a%80%e6%9c%af-2/)

April 5th, 2023

\[...\]没找到特别好的材料，欢迎留言，参考 SVD分解(三)：连Word2Vec都只不过是个SVD？中的说法：\[...\]

[回复评论](https://kexue.fm/archives/4233/comment-page-2?replyTo=21349#respond-post-4233)

[深度学习推荐系统-笔记05：embedding技术 R11; 稳住·能赢](https://notlate.cn/%e6%b7%b1%e5%ba%a6%e5%ad%a6%e4%b9%a0%e6%8e%a8%e8%8d%90%e7%b3%bb%e7%bb%9f-%e7%ac%94%e8%ae%b005%ef%bc%9aembedding%e6%8a%80%e6%9c%af/)

April 6th, 2023

\[...\]没找到特别好的材料，欢迎留言，参考 SVD分解(三)：连Word2Vec都只不过是个SVD？中的说法：\[...\]

[回复评论](https://kexue.fm/archives/4233/comment-page-2?replyTo=21350#respond-post-4233)

[推荐系统-笔记05：embedding技术 R11; 稳住·能赢](https://notlate.cn/2023/04/06/%e6%8e%a8%e8%8d%90%e7%b3%bb%e7%bb%9f-%e7%ac%94%e8%ae%b005%ef%bc%9aembedding%e6%8a%80%e6%9c%af/)

April 6th, 2023

\[...\]没找到特别好的材料，欢迎留言，参考 SVD分解(三)：连Word2Vec都只不过是个SVD？中的说法：\[...\]

[回复评论](https://kexue.fm/archives/4233/comment-page-2?replyTo=21351#respond-post-4233)

[推荐系统-笔记05：embedding技术 R11; 稳住·能赢](https://notlate.cn/p/59/)

April 6th, 2023

\[...\]没找到特别好的材料，欢迎留言，参考 SVD分解(三)：连Word2Vec都只不过是个SVD？中的说法：\[...\]

[回复评论](https://kexue.fm/archives/4233/comment-page-2?replyTo=21352#respond-post-4233)

[推荐系统-笔记05：embedding技术 R11; 稳住·能赢](https://notlate.cn/p/%e6%8e%a8%e8%8d%90%e7%b3%bb%e7%bb%9f-%e7%ac%94%e8%ae%b005%ef%bc%9aembedding%e6%8a%80%e6%9c%af-2/)

April 6th, 2023

\[...\]没找到特别好的材料，欢迎留言，参考 SVD分解(三)：连Word2Vec都只不过是个SVD？中的说法：\[...\]

[回复评论](https://kexue.fm/archives/4233/comment-page-2?replyTo=21360#respond-post-4233)

[推荐系统-笔记05：embedding技术 R11; 稳住·能赢](https://notlate.cn/p/%e6%8e%a8%e8%8d%90%e7%b3%bb%e7%bb%9f-%e7%ac%94%e8%ae%b005%ef%bc%9aembedding%e6%8a%80%e6%9c%af/)

April 6th, 2023

\[...\]没找到特别好的材料，欢迎留言，参考 SVD分解(三)：连Word2Vec都只不过是个SVD？中的说法：\[...\]

[回复评论](https://kexue.fm/archives/4233/comment-page-2?replyTo=21361#respond-post-4233)

[推荐系统-笔记05：embedding技术 R11; 稳住·能赢](https://notlate.cn/p/6f4c2d7f91a35b725197cd461e6b6d4e7f2a102a/)

April 6th, 2023

\[...\]没找到特别好的材料，欢迎留言，参考 SVD分解(三)：连Word2Vec都只不过是个SVD？中的说法：\[...\]

[回复评论](https://kexue.fm/archives/4233/comment-page-2?replyTo=21362#respond-post-4233)

[推荐系统-笔记05：embedding技术 R11; 稳住·能赢](https://notlate.cn/p/6f4c2d7f91a35b72/)

April 6th, 2023

\[...\]没找到特别好的材料，欢迎留言，参考 SVD分解(三)：连Word2Vec都只不过是个SVD？中的说法：\[...\]

[回复评论](https://kexue.fm/archives/4233/comment-page-2?replyTo=21363#respond-post-4233)

1. [«](https://kexue.fm/archives/4233/comment-page-1#comments)
2. [1](https://kexue.fm/archives/4233/comment-page-1#comments)
3. [2](https://kexue.fm/archives/4233/comment-page-2#comments)

[取消回复](https://kexue.fm/archives/4233#respond-post-4233)

你的大名

电子邮箱

个人网站（选填）

1\. 可以使用LaTeX代码，点击“预览效果”可查看效果；

2\. 可以通过点击评论楼层编号来引用该楼层；

3\. 网站可能会有点卡，如非确认评论失败，请 **不要重复点击提交**。

### 内容速览

[词向量=one hot](https://kexue.fm/archives/4233#%E8%AF%8D%E5%90%91%E9%87%8F=one%20hot)
[Word2Vec=SVD？](https://kexue.fm/archives/4233#Word2Vec=SVD%EF%BC%9F)
[不是总结的总结](https://kexue.fm/archives/4233#%E4%B8%8D%E6%98%AF%E6%80%BB%E7%BB%93%E7%9A%84%E6%80%BB%E7%BB%93)

### 智能搜索

支持整句搜索！网站自动使用 [结巴分词](https://github.com/fxsjy/jieba) 进行分词，并结合ngrams排序算法给出合理的搜索结果。

### 热门标签

[生成模型](https://kexue.fm/tag/%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B/) [attention](https://kexue.fm/tag/attention/) [优化](https://kexue.fm/tag/%E4%BC%98%E5%8C%96/) [语言模型](https://kexue.fm/tag/%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/) [模型](https://kexue.fm/tag/%E6%A8%A1%E5%9E%8B/) [梯度](https://kexue.fm/tag/%E6%A2%AF%E5%BA%A6/) [网站](https://kexue.fm/tag/%E7%BD%91%E7%AB%99/) [概率](https://kexue.fm/tag/%E6%A6%82%E7%8E%87/) [矩阵](https://kexue.fm/tag/%E7%9F%A9%E9%98%B5/) [优化器](https://kexue.fm/tag/%E4%BC%98%E5%8C%96%E5%99%A8/) [转载](https://kexue.fm/tag/%E8%BD%AC%E8%BD%BD/) [微分方程](https://kexue.fm/tag/%E5%BE%AE%E5%88%86%E6%96%B9%E7%A8%8B/) [分析](https://kexue.fm/tag/%E5%88%86%E6%9E%90/) [天象](https://kexue.fm/tag/%E5%A4%A9%E8%B1%A1/) [深度学习](https://kexue.fm/tag/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/) [积分](https://kexue.fm/tag/%E7%A7%AF%E5%88%86/) [python](https://kexue.fm/tag/python/) [扩散](https://kexue.fm/tag/%E6%89%A9%E6%95%A3/) [力学](https://kexue.fm/tag/%E5%8A%9B%E5%AD%A6/) [无监督](https://kexue.fm/tag/%E6%97%A0%E7%9B%91%E7%9D%A3/) [几何](https://kexue.fm/tag/%E5%87%A0%E4%BD%95/) [节日](https://kexue.fm/tag/%E8%8A%82%E6%97%A5/) [生活](https://kexue.fm/tag/%E7%94%9F%E6%B4%BB/) [文本生成](https://kexue.fm/tag/%E6%96%87%E6%9C%AC%E7%94%9F%E6%88%90/) [数论](https://kexue.fm/tag/%E6%95%B0%E8%AE%BA/)

### 随机文章

- [基于CNN和序列标注的对联机器人](https://kexue.fm/archives/6270)
- [相对论和量子力学的初探](https://kexue.fm/archives/1744)
- [殊途同归的策略梯度与零阶优化](https://kexue.fm/archives/7737)
- [从语言模型到Seq2Seq：Transformer如戏，全靠Mask](https://kexue.fm/archives/6933)
- [关于“平衡态公理”的更正与思考](https://kexue.fm/archives/1902)
- [无固定点的单摆运动](https://kexue.fm/archives/1344)
- [ReLU/GeLU/Swish的一个恒等式](https://kexue.fm/archives/11233)
- [“让Keras更酷一些！”：分层的学习率和自由的梯度](https://kexue.fm/archives/6418)
- [“闭门造车”之多模态思路浅谈（三）：位置编码](https://kexue.fm/archives/10352)
- [矩阵符号函数mcsgn能计算什么？](https://kexue.fm/archives/11056)

### 最近评论

- [Bin](https://kexue.fm/archives/1990/comment-page-2#comment-29105): 今天偶然从某个论坛看到有人推荐您的博客，定睛一看竟然是华师同院的往届师兄！看到这篇2013年的...
- [Rapture D](https://kexue.fm/archives/11530/comment-page-1#comment-29104): 我有一个问题，为什么不考虑亥姆霍兹定理和斯托克斯公式。
- [mofheka](https://kexue.fm/archives/11390/comment-page-1#comment-29103): 苏神是还在用jax是么？最近在做基于Google Pathway的理念做一个动态版的MPMD框...
- [长琴](https://kexue.fm/archives/11530/comment-page-1#comment-29102): 看懂这篇博客也不是一件容易的事情。
- [AlexLi](https://kexue.fm/archives/9257/comment-page-4#comment-29101): 苏老师，请教一下(7)式中将 \\mu(x\_t) 传给 p\_o 进行推理的操作。 $x\_...
- [tyler\_zxc](https://kexue.fm/archives/7921/comment-page-2#comment-29100): "Performer的思想是将标准的Attention线性化，所以为什么不干脆直接训练一个线性...
- [我](https://kexue.fm/archives/11494/comment-page-1#comment-29099): 似乎并非mHC提出矩阵的思想？之前hyper connection就是了
- [winter](https://kexue.fm/archives/10847/comment-page-1#comment-29098): 苏神您好，假如对于比较均匀的attention weightP，往往呈现long tail分布...
- [苏剑林](https://kexue.fm/archives/8512/comment-page-2#comment-29097): KL散度、JS散度、W距离啥的，都行啊，看你喜欢哪个
- [苏剑林](https://kexue.fm/archives/9119/comment-page-14#comment-29096): 没有绝对公平的对比方法，主要看你关心什么。比如，如果只关心推理成本和推理效果，那么有的方法可以...

### 友情链接

- [Cool Papers](https://papers.cool/)
- [数学研发](https://bbs.emath.ac.cn/)
- [Seatop](http://www.seatop.com.cn/)
- [Xiaoxia](https://xiaoxia.org/)
- [积分表-网络版](https://kexue.fm/sci/integral/index.html)
- [丝路博傲](http://blog.dvxj.com/)
- [数学之家](http://www.2math.cn/)
- [有趣天文奇观](http://interesting-sky.china-vo.org/)
- [TwistedW](http://www.twistedwg.com/)
- [godweiyang](https://godweiyang.com/)
- [AI柠檬](https://blog.ailemon.net/)
- [王登科-DK博客](https://greatdk.com/)
- [ESON](https://blog.eson.org/)
- [枫之羽](https://fzhiy.net/)
- [coding-zuo](https://coding-zuo.github.io/)
- [博科园](https://www.bokeyuan.net/)
- [孔皮皮的博客](https://www.kppkkp.top/)
- [运鹏的博客](https://yunpengtai.top/)
- [jiming.site](https://jiming.site/)
- [OmegaXYZ](https://www.omegaxyz.com/)
- [EAI猩球](https://www.robotech.ink/)
- [文举的博客](https://liwenju0.com/)
- [申请链接](https://kexue.fm/links.html)

[![署名-非商业用途-保持一致](https://kexue.fm/usr/themes/geekg/images/cc.gif)](http://creativecommons.org/licenses/by-nc-nd/2.5/cn/) 本站采用创作共用版权协议，要求署名、非商业用途和保持一致。转载本站内容必须也遵循“ [署名-非商业用途-保持一致](http://creativecommons.org/licenses/by-nc-nd/2.5/cn/)”的创作共用协议。



© 2009-2026 Scientific Spaces. All rights reserved. Theme by [laogui](http://www.laogui.com/). Powered by [Typecho](http://typecho.org/). 备案号: [粤ICP备09093259号-1/2](https://beian.miit.gov.cn/ "粤ICP备09093259号")。