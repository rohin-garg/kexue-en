## SEARCH

## MENU

- [打赏](https://kexue.fm/reward.html)
- [公式](https://kexue.fm/latex.html)
- [天象](https://kexue.fm/ac.html)
- [链接](https://kexue.fm/links.html)
- [时光](https://kexue.fm/me.html)
- [博览](https://kexue.fm/science.html)
- [归档](https://kexue.fm/content.html)

## CATEGORIES

- [千奇百怪](https://kexue.fm/category/Everything)
- [天文探索](https://kexue.fm/category/Astronomy)
- [数学研究](https://kexue.fm/category/Mathematics)
- [物理化学](https://kexue.fm/category/Phy-chem)
- [信息时代](https://kexue.fm/category/Big-Data)
- [生物自然](https://kexue.fm/category/Biology)
- [图片摄影](https://kexue.fm/category/Photograph)
- [问题百科](https://kexue.fm/category/Questions)
- [生活/情感](https://kexue.fm/category/Life-Feeling)
- [资源共享](https://kexue.fm/category/Resources)

## NEWPOSTS

- [Muon优化器指南：快速上手与关键细节](https://kexue.fm/archives/11416)
- [AdamW的Weight RMS的...](https://kexue.fm/archives/11404)
- [n个正态随机数的最大值的渐近估计](https://kexue.fm/archives/11390)
- [流形上的最速下降：5\. 对偶梯度下降](https://kexue.fm/archives/11388)
- [低精度Attention可能存在有...](https://kexue.fm/archives/11371)
- [MuP之上：1. 好模型的三个特征](https://kexue.fm/archives/11340)
- [随机矩阵的谱范数的快速估计](https://kexue.fm/archives/11335)
- [DiVeQ：一种非常简洁的VQ训练方案](https://kexue.fm/archives/11328)
- [为什么线性注意力要加Short C...](https://kexue.fm/archives/11320)
- [AdamW的Weight RMS的...](https://kexue.fm/archives/11307)

## COMMENTS

- [苏剑林: 总结一下几点经验，供参考。1、用Muon的理由可以很多，比如追...](https://kexue.fm/archives/11416/comment-page-1#comment-28855)
- [苏剑林: 我自己没特别关注这种物理景观特别明显的研究，或者说这类研究真正...](https://kexue.fm/archives/9305/comment-page-1#comment-28854)
- [苏剑林: 你可以理解为给这个loss多乘以了一项标量权重，它不改变这个l...](https://kexue.fm/archives/9209/comment-page-7#comment-28853)
- [苏剑林: 如果是$t\\to\\infty$，那么理论上是这样子；但如果$t...](https://kexue.fm/archives/11307/comment-page-1#comment-28852)
- [BaoLi: 返回去又看了博主相关论文。如果我理解没错的话，考虑两种情形：1...](https://kexue.fm/archives/11416/comment-page-1#comment-28851)
- [z: 可以省显存，加速收敛，提高训练效率啊](https://kexue.fm/archives/11416/comment-page-1#comment-28850)
- [BaoLi: 感谢大佬分享，我有几个问题：1，看了一遍以后，感觉整个流程比起...](https://kexue.fm/archives/11416/comment-page-1#comment-28849)
- [Dhuzi: 苏神，能多发点这种物理和机器学习结合的研究内容吗？或者您觉得机...](https://kexue.fm/archives/9305/comment-page-1#comment-28848)
- [小白一枚: 苏老师，（14）到（15）你给的解释是直接删去分母，确实对（1...](https://kexue.fm/archives/9209/comment-page-7#comment-28847)
- [danyao12: 好的，感谢！](https://kexue.fm/archives/11371/comment-page-1#comment-28845)

## USERLOGIN

- [登录](https://kexue.fm/admin/login.php)

[科学空间\|Scientific Spaces](https://kexue.fm)

- [登录](https://kexue.fm/admin/login.php)
- [打赏](https://kexue.fm/reward.html)
- [公式](https://kexue.fm/latex.html)
- [天象](https://kexue.fm/ac.html)
- [链接](https://kexue.fm/links.html)
- [时光](https://kexue.fm/me.html)
- [博览](https://kexue.fm/science.html)
- [归档](https://kexue.fm/content.html)

渴望成为一个小飞侠

- [欢迎订阅](https://kexue.fm/feed)
- [个性邮箱](https://kexue.fm/archives/119)
- [天象信息](https://kexue.fm/ac.html)
- [观测ISS](https://kexue.fm/archives/41)
- [LaTeX](https://kexue.fm/latex.html)
- [关于博主](https://kexue.fm/me.html)

欢迎访问“科学空间”，这里将与您共同探讨自然科学，回味人生百态；也期待大家的分享～

- [**千奇百怪** Everything](https://kexue.fm/category/Everything)
- [**天文探索** Astronomy](https://kexue.fm/category/Astronomy)
- [**数学研究** Mathematics](https://kexue.fm/category/Mathematics)
- [**物理化学** Phy-chem](https://kexue.fm/category/Phy-chem)
- [**信息时代** Big-Data](https://kexue.fm/category/Big-Data)
- [**生物自然** Biology](https://kexue.fm/category/Biology)
- [**图片摄影** Photograph](https://kexue.fm/category/Photograph)
- [**问题百科** Questions](https://kexue.fm/category/Questions)
- [**生活/情感** Life-Feeling](https://kexue.fm/category/Life-Feeling)
- [**资源共享** Resources](https://kexue.fm/category/Resources)

- [**千奇百怪**](https://kexue.fm/category/Everything)
- [**天文探索**](https://kexue.fm/category/Astronomy)
- [**数学研究**](https://kexue.fm/category/Mathematics)
- [**物理化学**](https://kexue.fm/category/Phy-chem)
- [**信息时代**](https://kexue.fm/category/Big-Data)
- [**生物自然**](https://kexue.fm/category/Biology)
- [**图片摄影**](https://kexue.fm/category/Photograph)
- [**问题百科**](https://kexue.fm/category/Questions)
- [**生活/情感**](https://kexue.fm/category/Life-Feeling)
- [**资源共享**](https://kexue.fm/category/Resources)

[首页](https://kexue.fm) [信息时代](https://kexue.fm/category/Big-Data) Muon优化器指南：快速上手与关键细节

19Nov

# [Muon优化器指南：快速上手与关键细节](https://kexue.fm/archives/11416)

By 苏剑林 \|
2025-11-19 \|
2782位读者\|

这段时间，相信很多读者已经刷到过Muon优化器的相关消息。实际上，Muon的提出时间大致是去年的10月份，由 [Keller Jordan](https://x.com/kellerjordan0/status/1842300916864844014) 在推特上提出，距今也不过一年多一点。然而，就在这一年里，Muon已经经历了百亿、千亿乃至万亿参数模型的训练考验，足以表明它是一个相当有竞争力的优化器。

如今，Muon已经内置在 [Torch](https://docs.pytorch.org/docs/stable/generated/torch.optim.Muon.html)、 [Keras](https://keras.io/api/optimizers/muon/) 等训练框架中，就连 [Megatron](https://github.com/NVIDIA/Megatron-LM/blob/dev/megatron/core/optimizer/muon.py) 这样的大型框架也逐渐开始支持，这意味它已经获得了业界的普遍认可。不过，对于仅熟悉Adam的读者来说，如何快速有效地切换到Muon，可能依然是一件让人困惑的事情。所以，本文试图给出一个快速上手教程。

## 简要介绍 [\#](https://kexue.fm/kexue.fm\#%E7%AE%80%E8%A6%81%E4%BB%8B%E7%BB%8D)

Muon的正式提出者是 [Keller Jordan](https://x.com/kellerjordan0/status/1842300916864844014) ，目前任职于OpenAI。开头说了，Muon最早发表在推特上，而直到现在，作者也只是多写了篇博客 [《Muon: An optimizer for hidden layers in neural networks》](https://kellerjordan.github.io/posts/muon/) 而不是一篇Paper，作者的观点是“是否写成Paper，跟优化器是否有效，没有任何关系\[ [原文](https://x.com/kellerjordan0/status/1890178773586489716)\]”。

Muon是一个专门为矩阵参数定制的优化器，也有一些相关工作具有类似的特点，比如 [Shampoo](https://papers.cool/arxiv/1802.09568)，还有更早一些的 [Stochastic Spectral Descent](https://kexue.fm/archives/10592)，等等。很多工作或多或少都能关联上Muon，但没有一个是能够完全覆盖Muon的，所以在笔者看来Muon算是一个全新的工作。

在国内，最早向大家科普Muon的文章，应该是笔者的博客 [《Muon优化器赏析：从向量到矩阵的本质跨越》](https://kexue.fm/archives/10592)，而首次在较大规模的模型上验证Muon，应该是我们二月份发布的 [Moonlight](https://papers.cool/arxiv/2502.16982)，其所提的Moonlight版Muon，用到了后来的万亿参数的 [K2](https://papers.cool/arxiv/2507.20534) 中。K2之后， [GLM-4.5](https://papers.cool/arxiv/2508.06471) 同样用到了这个Muon变体。

跟Muon的作者之一Jeremy Bernstein在他的博客 [《Deriving Muon》](https://jeremybernste.in/writing/deriving-muon) 所说的一样，对笔者而言，Muon的独特之处在于它可以基于更本质的优化原理推导而来，并在实践中有效，相比之下，虽然Adam也很有效，但它更像是一种启发式方案。

## 四个版本 [\#](https://kexue.fm/kexue.fm\#%E5%9B%9B%E4%B8%AA%E7%89%88%E6%9C%AC)

本文不打算介绍Muon的数学细节，也不打算介绍Muon的实现，而是主要介绍从Adam切换到Muon的一些技术细节和注意事项。刚才说了，Muon是专用于矩阵参数优化的，并且是非Element-wise的更新规则，这可能导致新用户上手起来会比较困惑。

还有，据笔者所知，Muon目前至少有四个略微不同的版本，这种多版本现象也促进了这种困惑。如果用户不了解其中的细节，可能会因为调错超参数（特别是学习率）而导致不好的效果，下面会着重理清楚这些内容。首先，对于一个矩阵$\\boldsymbol{W}\\in\\mathbb{R}^{d\_{in}\\times d\_{out}}$，$\\boldsymbol{G}$是它的梯度，四个Muon变体分别是：

\\begin{align}\\newcommand{msign}{\\mathop{\\text{msign}}}
\\boldsymbol{M}\_t =&\\, \\beta \\boldsymbol{M}\_{t-1} + \\boldsymbol{G}\_t \\\\[7pt\]
\\boldsymbol{W}\_t =&\\, \\boldsymbol{W}\_{t-1} - \\eta\_t \\left(\\msign(\\boldsymbol{M}\_t) + \\lambda \\boldsymbol{W}\_{t-1}\\right) \\quad &\\color{skyblue}{(\\text{朴素版})} \\\\[5pt\]
\\boldsymbol{W}\_t =&\\, \\boldsymbol{W}\_{t-1} - \\eta\_t \\left(\\sqrt{\\max(1, d\_{out}/d\_{in})}\\msign(\\boldsymbol{M}\_t) + \\lambda \\boldsymbol{W}\_{t-1}\\right) \\quad &\\color{skyblue}{(\\text{KellerJordan版})} \\\\[5pt\]
\\boldsymbol{W}\_t =&\\, \\boldsymbol{W}\_{t-1} - \\eta\_t \\left(\\sqrt{ d\_{out}/d\_{in}}\\msign(\\boldsymbol{M}\_t) + \\lambda \\boldsymbol{W}\_{t-1}\\right) \\quad &\\color{skyblue}{(\\text{MuP版})} \\\\[5pt\]
\\boldsymbol{W}\_t =&\\, \\boldsymbol{W}\_{t-1} - \\eta\_t \\left(0.2\\times\\sqrt{\\max(d\_{out},d\_{in})}\\msign(\\boldsymbol{M}\_t) + \\lambda \\boldsymbol{W}\_{t-1}\\right) \\quad &\\color{skyblue}{(\\text{Moonlight版})}
\\end{align}

如果要开Nesterov动量则将$\\msign(\\boldsymbol{M}\_t)$换成$\\msign(\\beta\\boldsymbol{M}\_t + \\boldsymbol{G}\_t)$，其中$\\msign$在实现中通常以 `zeropower_via_newtonschulz` 命名，具体实现细节普通用户可以不管。

四个版本的唯一区别是$\\msign$前的缩放因子，其中“KellerJordan版”和“MuP版”大同小异，“Moonlight版”则稍微特别一点。Keras只实现了“KellerJordan版”，而Torch则实现了“KellerJordan版”和“Moonlight版”，朴素版目前似乎比较少见，对笔者来说常用的是自己写的“MuP版”。

## 两个维度 [\#](https://kexue.fm/kexue.fm\#%E4%B8%A4%E4%B8%AA%E7%BB%B4%E5%BA%A6)

这里我们要注意一个重要细节，“KellerJordan版”和“MuP版”对$d\_{in},d\_{out}$的顺序是敏感的，所以第一件事情就是要搞清楚$d\_{in},d\_{out}$的含义，并不是说矩阵的第一个维度就一定是$d\_{in}$、第二个维度就一定是$d\_{out}$。

$d\_{in}$、$d\_{out}$的含义分别是线性层的输入、输出维度，所以哪个是$d\_{in}$哪个是$d\_{out}$，要看线性层的具体实现。比如Keras的Dense层实现是$\\boldsymbol{x}\\boldsymbol{W}$，那么矩阵$\\boldsymbol{W}$的第一个维度是$d\_{in}$、第二个维度是$d\_{out}$；然而，Torch的Linear层实现的事$\\boldsymbol{x}\\boldsymbol{W}^{\\top}$，所以矩阵$\\boldsymbol{W}$的第二个维度是$d\_{in}$，第一个维度才是$d\_{out}$。

所以，如果要实现“KellerJordan版”的Muon，对于Torch的Linear层，缩放因子应该是 `max(1, W.shape[0]/W.shape[1])**0.5`，而对于Keras则应该是 `max(1, W.shape[1]/W.shape[0])**0.5`。所以，当前Keras的Muon实现其实是不正确的，因为它照搬了Torch的缩放因子实现（ [源码](https://github.com/keras-team/keras/blob/v3.12.0/keras/src/optimizers/muon.py#L198)）。

如果是自己写的模型，则需要根据自己的写法谨慎判断了，比如不排除在Torch中混用了自带的Linear层和手写的 `x @ W`，这样就不能一概而论是 `W.shape[0]/W.shape[1]` 还是 `W.shape[1]/W.shape[0]` 了。当然，如果你嫌搞清楚这些比较麻烦，那就可以考虑用“Moonlight版”，它的缩放因子关于$d\_{in},d\_{out}$是对称的。

## 超参设置 [\#](https://kexue.fm/kexue.fm\#%E8%B6%85%E5%8F%82%E8%AE%BE%E7%BD%AE)

搞清楚$d\_{in},d\_{out}$后，剩下就是学习率$\\eta\_t$和权重衰减系数$\\lambda$怎么设置了。这里的假设是用户已经有Adam的调参经验，在Adam下已经得到了不错的效果，想要快速迁移到Muon体验一波。

我们先来看“Moonlight版”，它的缩放因子是通过对齐Adam的Update RMS得到的，如果想要了解细节，可以参考 [《Muon续集：为什么我们选择尝试Muon？》](https://kexue.fm/archives/10739)，至于$0.2$这个“Magic Number”，可以参考 [《为什么Adam的Update RMS是0.2？》](https://kexue.fm/archives/11267)。简单来说，“Moonlight版”Muon对齐了Adam的更新幅度，所以从Adam迁移过来的最简单的做法是： **啥也不用改**，用回Adam的$\\eta\_t$和$\\lambda$就行。

然后看剩余三个版本。我们知道，主流模型通常有个hidden\_size（记为$d$），模型的矩阵形状多数不会明显偏离$d\\times d$，因此我们以$d\_{in}=d\_{out}=d$来近似处理，此时这三个版本都是一样的，相比“Moonlight版”则少了个$0.2\\sqrt{d}$。既然“Moonlight版”对齐了Adam更新幅度可以不改变超参，那么这三个版本的学习率应该要放大$0.2\\sqrt{d}$倍，才能对齐Adam的更新幅度，相应地，$\\lambda$则要除以$0.2\\sqrt{d}$。

代入$d=1024,2048,4096$，结果分别是$6.4, 9, 12.8$，如果记不住$0.2\\sqrt{d}$，那么可以简单记住，如果我们使用另外三个版本的Muon，那么 **直接将Adam的学习率放大10倍** 来作为Muon的学习率。如果直接代入Adam的学习率到Muon中，就会因为欠拟合得到Muon远不如Adam的结论，据笔者所知，Muon的一些差评便来源于此。

这样看还是“Moonlight版”更好用？“Moonlight版”确实有不错的实践效果，但如果就此说它更好用，其实是站在Adam的角度下评价了。“MuP版”或“KellerJordan版”的好处是学习率可迁移，即在小模型调好学习率后，直接用到大模型往往也有不错的效果，这部分可以参考Jeremy Bernstein的博客 [《Deriving Muon》](https://jeremybernste.in/writing/deriving-muon) 或笔者的博客 [《高阶MuP：更简明但更高明的谱条件缩放》](https://kexue.fm/archives/10795)

## 其他参数 [\#](https://kexue.fm/kexue.fm\#%E5%85%B6%E4%BB%96%E5%8F%82%E6%95%B0)

如果说Muon只管矩阵参数，那么其余参数怎么办呢？比如线性层的Bias项、RMSNorm的gamma项，这些是1维的参数；又比如卷积层可能出现3维、4维数组的参数。

这里先要更正一下，Muon并不是只管矩阵参数，Muon是只管“ **稠密输入的线性层的矩阵参数**”，如果读者觉得这个比较费解，那么只需要记住Embedding层和最后的分类层（包括GPT的LM Head）的矩阵参数都不能用Muon，否则效果会明显差。这些不能用Muon的矩阵参数，还有1维、3维及更高维的参数，如果读者不想费太多心思，那么直接用Adam就行，基本上Muon实现都是混合了Adam的，用户可选某些层用Adam。

如果读者愿意捣鼓，那么像卷积层的3、4维参数，也可以用上Muon。以Conv2D为例，卷积核形状通常是$(w, h, d\_{in}, d\_{out})$，它的等效实现其实是把$(w, h, d\_{in})$的Patch输入展平为$w \\times h \\times d\_{in}$的向量，然后卷积核也Reshape为$(w\\times h \\times d\_{in}, d\_{out})$，最后做矩阵乘法，所以它想要用Muon，那就要先将动量Reshape为$(w\\times h \\times d\_{in}, d\_{out})$，计算$\\msign$后再Reshape回去更新。

类似地，还有RMSNorm的gamma参数，可以视为跟对角矩阵做乘法，于是将它的动量视为对角矩阵，也可以算$\\msign$，结果等效于SignSGDM；Embedding层可以视为多个$(1,d)$矩阵去计算$\\msign$，结果是Normalized SGDM（参考 [《Muon优化器赏析：从向量到矩阵的本质跨越》](https://kexue.fm/archives/10592)）。如果还想折腾，比如Multi-Head的Attention，每个Head的投影矩阵是不是可以考虑单独拿出来分别做$\\msign$...

生命不息，折腾不止～

## 期望结果 [\#](https://kexue.fm/kexue.fm\#%E6%9C%9F%E6%9C%9B%E7%BB%93%E6%9E%9C)

最后，如果用户按照上述说明正确设置并跑起来了，那么就可以开始祈祷幸运之神的降临了。

我们应该期待一个什么样的结果呢？如果没有出现梯度爆炸等异常情况，那么多数情况下Muon会比Adam略好一些，当然也不排除某些情况Muon会略差，但不论如何，它们的差距不会非常大。如果出现一方比另一方好非常多的现象，那么可能得反思一下哪边的设置出现问题了。

不过，这都不是绝对的，比如某些极端的设置下，确实也会出现Muon比Adam好很多，Adam怎么调也不行的现象。总之，祝你幸运。如果出现有意思的现象，欢迎一起交流分析。

_**转载到请包括本文地址：** [https://kexue.fm/archives/11416](https://kexue.fm/archives/11416)_

_**更详细的转载事宜请参考：**_ [《科学空间FAQ》](https://kexue.fm/archives/6508#%E6%96%87%E7%AB%A0%E5%A6%82%E4%BD%95%E8%BD%AC%E8%BD%BD/%E5%BC%95%E7%94%A8)

**如果您还有什么疑惑或建议，欢迎在下方评论区继续讨论。**

**如果您觉得本文还不错，欢迎 [分享](https://kexue.fm/kexue.fm#share)/ [打赏](https://kexue.fm/kexue.fm#pay) 本文。打赏并非要从中获得收益，而是希望知道科学空间获得了多少读者的真心关注。当然，如果你无视它，也不会影响你的阅读。再次表示欢迎和感谢！**

打赏

微信打赏

支付宝打赏

因为网站后台对打赏并无记录，因此欢迎在打赏时候备注留言。你还可以 [**点击这里**](http://mail.qq.com/cgi-bin/qm_share?t=qm_mailme&email=tN7d1drY3drrx8H0xcWa19vZ) 或在下方评论区留言来告知你的建议或需求。

**如果您需要引用本文，请参考：**

苏剑林. (Nov. 19, 2025). 《Muon优化器指南：快速上手与关键细节 》\[Blog post\]. Retrieved from [https://kexue.fm/archives/11416](https://kexue.fm/archives/11416)

@online{kexuefm-11416,
        title={Muon优化器指南：快速上手与关键细节},
        author={苏剑林},
        year={2025},
        month={Nov},
        url={\\url{https://kexue.fm/archives/11416}},
}

分类： [信息时代](https://kexue.fm/category/Big-Data)    标签： [矩阵](https://kexue.fm/tag/%E7%9F%A9%E9%98%B5/), [优化](https://kexue.fm/tag/%E4%BC%98%E5%8C%96/), [优化器](https://kexue.fm/tag/%E4%BC%98%E5%8C%96%E5%99%A8/), [muon](https://kexue.fm/tag/muon/)[4 评论](https://kexue.fm/archives/11416#comments)

< [AdamW的Weight RMS的渐近估计（下）](https://kexue.fm/archives/11404) \| >

### 你也许还对下面的内容感兴趣

- [AdamW的Weight RMS的渐近估计（下）](https://kexue.fm/archives/11404)
- [流形上的最速下降：5\. 对偶梯度下降](https://kexue.fm/archives/11388)
- [低精度Attention可能存在有偏的舍入误差](https://kexue.fm/archives/11371)
- [MuP之上：1. 好模型的三个特征](https://kexue.fm/archives/11340)
- [随机矩阵的谱范数的快速估计](https://kexue.fm/archives/11335)
- [AdamW的Weight RMS的渐近估计（上）](https://kexue.fm/archives/11307)
- [重新思考学习率与Batch Size（四）：EMA](https://kexue.fm/archives/11301)
- [重新思考学习率与Batch Size（三）：Muon](https://kexue.fm/archives/11285)
- [重新思考学习率与Batch Size（二）：平均场](https://kexue.fm/archives/11280)
- [为什么Adam的Update RMS是0.2？](https://kexue.fm/archives/11267)

[发表你的看法](https://kexue.fm/kexue.fm#comment_form)

BaoLi

November 20th, 2025

感谢大佬分享，我有几个问题：

1，看了一遍以后，感觉整个流程比起Adam还是有些麻烦，但是拉到最后看到你说Muon大多数情况也只是比Adam略好一点。那这样切换优化器的意义在哪里呢？从SGD到Adam的进步是云泥之别，是属于"不换活不下去"级别的需求。但是看了很多report，感觉muon比其Adam则最多像是略微锦上添花，似乎没有必换的理由？

2，关于“Muon之于Adam是从向量到矩阵的本质飞跃”这一观点，我觉得还有很大的讨论空间。如果考虑Adam下的离散动力系统 \\theta\_t = F(\\theta\_{t-1})，那么unroll可以得到theta\_t = F(F(F....(theta\_0)))。 而Adam规则下的A\_ij = \\frac{\\partial \\theta\_{t,j}}{\\partial \\theta\_{0,i}}组成的矩阵是dense的，非对角一般是不等于0的。我们最多可以说，Muon是显式地enforce了这种dependency，但是Adam也依然隐式地利用了这种矩阵关联信息，至于Adam->muon的推广，可能已经有某种边际效应在里面了。直观上来说，如果muon真的利用了某种重要的、被Adam完全遗漏掉了的信息，那么二者的优化效果应该有更显著的差异。

3，继续上一点，其实有很多文献有过类似分析。我能马上想到的是https://arxiv.org/pdf/2402.13810 至少在stationary point附近+positive definite Hessian, 任何的矩阵preconditioning的预期loss都可以被对角版本的preconditioning所达到。这也（某种程度上）justify了Adam这种没有显式enforce矩阵preconditioning的方法可能完全够用，尤其现代LLM训练的大背景就是过度训练,finite time对结论的影响会相对较小。（除了这篇以外，印象中还有不同文章在不同的setup下得出了类似结论）

[回复评论](https://kexue.fm/archives/11416/comment-page-1?replyTo=28849#respond-post-11416)

z 发表于
November 20th, 2025

可以省显存，加速收敛，提高训练效率啊

[回复评论](https://kexue.fm/archives/11416/comment-page-1?replyTo=28850#respond-post-11416)

BaoLi 发表于
November 20th, 2025

返回去又看了博主相关论文。如果我理解没错的话，考虑两种情形：1，使用者有较为充裕多卡资源进行优化器并行；2，使用者没有较为充裕的多卡资源进行优化器并行。

在情形1，主流的并行方案下对优化器状态都会进行切片，Adam多出来的二阶状态本身就对内存占用没有什么影响

在情形2，你都没有资源搞并行了，还跑Newton Schulz迭代，每步的挂钟时间是要比Adam显著高的，综合下来Muon并不能加速收敛

这不两头都不占吗

[回复评论](https://kexue.fm/archives/11416/comment-page-1?replyTo=28851#respond-post-11416)

[苏剑林](https://kexue.fm) 发表于
November 20th, 2025

总结一下几点经验，供参考。

1、用Muon的理由可以很多，比如追求效果的，或者单纯尝鲜的，或者单纯从审美角度出发的，对笔者来说，哪怕Muon的效果仅仅和Adam一样，笔者也会选择Muon，因为对于笔者来说Muon是一个更优雅的优化器，更不用说它实践中它往往还能跑出更优秀的效果；

2、本文的目的，是给想要尝试Muon优化器的用户一个快速上手指南，倒不是想给大家一个用Muon的理由，这其实并不在本文的考虑范围内；

3、相比Adam，Muon使用上确实会显得麻烦一些，不过很多时候这是站在Adam立场下的结果，比如MuP版Muon，调好学习率后理论上就可以在不同宽度的模型之间直接迁移，这便是Muon独有的很不错的独特性质；

4、与其说Muon比Adam更麻烦，倒不如说是Adam这类Element-wise的优化器把麻烦掩盖了，比如本文有一节是区分$d\_{in},d\_{out}$的，有人觉得不优雅，但问题是一个矩阵的输入输出维度的角色和作用本就都是不对等的，为什么非要是对称的更新规则才合理呢？也许不对称的规则才更本质呢？

5、即便是Adam优化器下，也有fan\_in/fan\_out/fan\_avg初始化的区别，也有MuP这类研究方向，它们的细节区分并不比Muon简单多少，所以优化的尽头都是“麻烦”；

6、关于你说的Adam理论性质如何合理，这个我理解，但正如我文末说的，我们确实遇到了Muon训效果很好，但Adam怎么训都不行的case（比如将Adam学习率调得很低loss依然爆炸），但反过来的例子没遇到过，所以我们相信Muon在稳定性和效果方面还是有优势的；

7、关于overtrain的问题，我同意如果大家都极度overtrain，Adam和Muon不会有区别，但事实是现在越大的模型，越没机会overtrain，比如K2有万亿参数，才训了15T tokens，未来可能还会几万亿的参数，但数据就那么多，没机会翻几倍了（即便有，也基本上都是合成数据），所以Muon在非overtrain时的效率优势显得更为重要；

8、按照我们的实践，在数千卡这个规模上，Adam换Muon所增加的总时间大概是2%左右，并不多，这是因为Muon虽然比Adam慢，但优化器的更新时间在整个模型训练时间中的占比本身并不多（大头是fwd和bwd的时间），好好实现还是能做到基本无感切换的。

[回复评论](https://kexue.fm/archives/11416/comment-page-1?replyTo=28855#respond-post-11416)

[取消回复](https://kexue.fm/archives/11416#respond-post-11416)

你的大名

电子邮箱

个人网站（选填）

1\. 可以使用LaTeX代码，点击“预览效果”可查看效果；2. 可以通过点击评论楼层编号来引用该楼层；3. 网站可能会有点卡，如非确认评论失败，请 **不要重复点击提交**。

### 内容速览

[简要介绍](https://kexue.fm/kexue.fm#%E7%AE%80%E8%A6%81%E4%BB%8B%E7%BB%8D)
[四个版本](https://kexue.fm/kexue.fm#%E5%9B%9B%E4%B8%AA%E7%89%88%E6%9C%AC)
[两个维度](https://kexue.fm/kexue.fm#%E4%B8%A4%E4%B8%AA%E7%BB%B4%E5%BA%A6)
[超参设置](https://kexue.fm/kexue.fm#%E8%B6%85%E5%8F%82%E8%AE%BE%E7%BD%AE)
[其他参数](https://kexue.fm/kexue.fm#%E5%85%B6%E4%BB%96%E5%8F%82%E6%95%B0)
[期望结果](https://kexue.fm/kexue.fm#%E6%9C%9F%E6%9C%9B%E7%BB%93%E6%9E%9C)

### 智能搜索

支持整句搜索！网站自动使用 [结巴分词](https://github.com/fxsjy/jieba) 进行分词，并结合ngrams排序算法给出合理的搜索结果。

### 热门标签

[生成模型](https://kexue.fm/tag/%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B/) [attention](https://kexue.fm/tag/attention/) [优化](https://kexue.fm/tag/%E4%BC%98%E5%8C%96/) [语言模型](https://kexue.fm/tag/%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/) [模型](https://kexue.fm/tag/%E6%A8%A1%E5%9E%8B/) [网站](https://kexue.fm/tag/%E7%BD%91%E7%AB%99/) [概率](https://kexue.fm/tag/%E6%A6%82%E7%8E%87/) [梯度](https://kexue.fm/tag/%E6%A2%AF%E5%BA%A6/) [矩阵](https://kexue.fm/tag/%E7%9F%A9%E9%98%B5/) [转载](https://kexue.fm/tag/%E8%BD%AC%E8%BD%BD/) [优化器](https://kexue.fm/tag/%E4%BC%98%E5%8C%96%E5%99%A8/) [微分方程](https://kexue.fm/tag/%E5%BE%AE%E5%88%86%E6%96%B9%E7%A8%8B/) [分析](https://kexue.fm/tag/%E5%88%86%E6%9E%90/) [天象](https://kexue.fm/tag/%E5%A4%A9%E8%B1%A1/) [深度学习](https://kexue.fm/tag/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/) [积分](https://kexue.fm/tag/%E7%A7%AF%E5%88%86/) [python](https://kexue.fm/tag/python/) [力学](https://kexue.fm/tag/%E5%8A%9B%E5%AD%A6/) [无监督](https://kexue.fm/tag/%E6%97%A0%E7%9B%91%E7%9D%A3/) [扩散](https://kexue.fm/tag/%E6%89%A9%E6%95%A3/) [几何](https://kexue.fm/tag/%E5%87%A0%E4%BD%95/) [节日](https://kexue.fm/tag/%E8%8A%82%E6%97%A5/) [生活](https://kexue.fm/tag/%E7%94%9F%E6%B4%BB/) [文本生成](https://kexue.fm/tag/%E6%96%87%E6%9C%AC%E7%94%9F%E6%88%90/) [数论](https://kexue.fm/tag/%E6%95%B0%E8%AE%BA/)

### 随机文章

- [通用爬虫探索（三）：效果展示与代码](https://kexue.fm/archives/4430)
- [能量视角下的GAN模型（一）：GAN＝“挖坑”＋“跳坑”](https://kexue.fm/archives/6316)
- [《环球科学》:超越费曼图](https://kexue.fm/archives/1790)
- [又折腾网络了......](https://kexue.fm/archives/1715)
- [有质动力：倒立单摆的稳定性](https://kexue.fm/archives/2232)
- [流形上的最速下降：3\. Muon + Stiefel](https://kexue.fm/archives/11221)
- [从动力学角度看优化算法（六）：为什么SimSiam不退化？](https://kexue.fm/archives/7980)
- [从动力学角度看优化算法（二）：自适应学习率算法](https://kexue.fm/archives/6234)
- [数学歌曲：《歌德巴赫猜》](https://kexue.fm/archives/42)
- [科学空间：2009年12月重要天象](https://kexue.fm/archives/297)

### 最近评论

- [苏剑林](https://kexue.fm/archives/11416/comment-page-1#comment-28855): 总结一下几点经验，供参考。1、用Muon的理由可以很多，比如追求效果的，或者单纯尝鲜的，或者单...
- [苏剑林](https://kexue.fm/archives/9305/comment-page-1#comment-28854): 我自己没特别关注这种物理景观特别明显的研究，或者说这类研究真正明显work的似乎不多。
- [苏剑林](https://kexue.fm/archives/9209/comment-page-7#comment-28853): 你可以理解为给这个loss多乘以了一项标量权重，它不改变这个loss的理论最优解（万能拟合能力...
- [苏剑林](https://kexue.fm/archives/11307/comment-page-1#comment-28852): 如果是$t\\to\\infty$，那么理论上是这样子；但如果$t$并非那么大，或者说$\\lamb...
- [BaoLi](https://kexue.fm/archives/11416/comment-page-1#comment-28851): 返回去又看了博主相关论文。如果我理解没错的话，考虑两种情形：1，使用者有较为充裕多卡资源进行优...
- [z](https://kexue.fm/archives/11416/comment-page-1#comment-28850): 可以省显存，加速收敛，提高训练效率啊
- [BaoLi](https://kexue.fm/archives/11416/comment-page-1#comment-28849): 感谢大佬分享，我有几个问题：1，看了一遍以后，感觉整个流程比起Adam还是有些麻烦，但是拉到最...
- [Dhuzi](https://kexue.fm/archives/9305/comment-page-1#comment-28848): 苏神，能多发点这种物理和机器学习结合的研究内容吗？或者您觉得机器学习在物理研究当中作用大吗？现...
- [小白一枚](https://kexue.fm/archives/9209/comment-page-7#comment-28847): 苏老师，（14）到（15）你给的解释是直接删去分母，确实对（14）式子不影响，但是损失函数（1...
- [danyao12](https://kexue.fm/archives/11371/comment-page-1#comment-28845): 好的，感谢！

### 友情链接

- [Cool Papers](https://papers.cool)
- [数学研发](https://bbs.emath.ac.cn)
- [Seatop](http://www.seatop.com.cn/)
- [Xiaoxia](https://xiaoxia.org/)
- [积分表-网络版](https://kexue.fm/sci/integral/index.html)
- [丝路博傲](http://blog.dvxj.com/)
- [数学之家](http://www.2math.cn/)
- [有趣天文奇观](http://interesting-sky.china-vo.org/)
- [TwistedW](http://www.twistedwg.com/)
- [godweiyang](https://godweiyang.com/)
- [AI柠檬](https://blog.ailemon.net/)
- [王登科-DK博客](https://greatdk.com)
- [ESON](https://blog.eson.org/)
- [枫之羽](https://fzhiy.net/)
- [Mathor's blog](https://wmathor.com/)
- [coding-zuo](https://coding-zuo.github.io/)
- [博科园](https://www.bokeyuan.net/)
- [孔皮皮的博客](https://www.kppkkp.top/)
- [运鹏的博客](https://yunpengtai.top/)
- [jiming.site](https://jiming.site/)
- [OmegaXYZ](https://www.omegaxyz.com/)
- [EAI猩球](https://www.robotech.ink/)
- [文举的博客](https://liwenju0.com/)
- [申请链接](https://kexue.fm/links.html)

本站采用创作共用版权协议，要求署名、非商业用途和保持一致。转载本站内容必须也遵循“ [署名-非商业用途-保持一致](http://creativecommons.org/licenses/by-nc-nd/2.5/cn/)”的创作共用协议。
© 2009-2025 Scientific Spaces. All rights reserved. Theme by [laogui](http://www.laogui.com). Powered by [Typecho](http://typecho.org). 备案号: [粤ICP备09093259号-1/2](https://beian.miit.gov.cn/)。