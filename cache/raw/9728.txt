## SEARCH

## MENU

- [打赏](https://kexue.fm/reward.html)
- [公式](https://kexue.fm/latex.html)
- [天象](https://kexue.fm/ac.html)
- [链接](https://kexue.fm/links.html)
- [时光](https://kexue.fm/me.html)
- [博览](https://kexue.fm/science.html)
- [归档](https://kexue.fm/content.html)

## CATEGORIES

- [千奇百怪](https://kexue.fm/category/Everything)
- [天文探索](https://kexue.fm/category/Astronomy)
- [数学研究](https://kexue.fm/category/Mathematics)
- [物理化学](https://kexue.fm/category/Phy-chem)
- [信息时代](https://kexue.fm/category/Big-Data)
- [生物自然](https://kexue.fm/category/Biology)
- [图片摄影](https://kexue.fm/category/Photograph)
- [问题百科](https://kexue.fm/category/Questions)
- [生活/情感](https://kexue.fm/category/Life-Feeling)
- [资源共享](https://kexue.fm/category/Resources)

## NEWPOSTS

- [流形上的最速下降：5\. 对偶梯度下降](https://kexue.fm/archives/11388)
- [低精度Attention可能存在有...](https://kexue.fm/archives/11371)
- [MuP之上：1. 好模型的三个特征](https://kexue.fm/archives/11340)
- [随机矩阵的谱范数的快速估计](https://kexue.fm/archives/11335)
- [DiVeQ：一种非常简洁的VQ训练方案](https://kexue.fm/archives/11328)
- [为什么线性注意力要加Short C...](https://kexue.fm/archives/11320)
- [AdamW的Weight RMS的...](https://kexue.fm/archives/11307)
- [重新思考学习率与Batch Siz...](https://kexue.fm/archives/11301)
- [重新思考学习率与Batch Siz...](https://kexue.fm/archives/11285)
- [重新思考学习率与Batch Siz...](https://kexue.fm/archives/11280)

## COMMENTS

- [ZHANG Ruiqi: 谢谢苏神的教程和回复，我同意你的观点。数学上 VQ-VAE 不...](https://kexue.fm/archives/6760/comment-page-8#comment-28777)
- [苏剑林: 很详细，我认为这个理解没有问题，但我依然不认同VQ-VAE的命...](https://kexue.fm/archives/6760/comment-page-8#comment-28776)
- [苏剑林: 激活函数是$f(x)$，$\\phi(x)$是构造$f(x)$的...](https://kexue.fm/archives/11233/comment-page-1#comment-28775)
- [苏剑林: 这里的等价性只是一个具体例子，如果你认为一个image tok...](https://kexue.fm/archives/10352/comment-page-2#comment-28774)
- [苏剑林: 好像是。你可以去X上@一下Jeremy Bernstein～](https://kexue.fm/archives/11388/comment-page-1#comment-28773)
- [苏剑林: 1\. 我们暂时认为MLA相比GQA有优势；\
2\. Kimi L...](https://kexue.fm/archives/11111/comment-page-2#comment-28772)
- [苏剑林: 我自己测过，效果尚可。](https://kexue.fm/archives/10862/comment-page-1#comment-28771)
- [苏剑林: 你是不是指kv cache只保存加RoPE前的，推理阶段实时给...](https://kexue.fm/archives/10862/comment-page-1#comment-28770)
- [苏剑林: 我有点没懂你的问题，你是不是指推理时噪声分布改为其他分布，而不...](https://kexue.fm/archives/10958/comment-page-3#comment-28769)
- [苏剑林: 可能的，但online update缓解了这个问题，加之现在主...](https://kexue.fm/archives/11033/comment-page-2#comment-28768)

## USERLOGIN

- [登录](https://kexue.fm/admin/login.php)

[科学空间\|Scientific Spaces](https://kexue.fm)

- [登录](https://kexue.fm/admin/login.php)
- [打赏](https://kexue.fm/reward.html)
- [公式](https://kexue.fm/latex.html)
- [天象](https://kexue.fm/ac.html)
- [链接](https://kexue.fm/links.html)
- [时光](https://kexue.fm/me.html)
- [博览](https://kexue.fm/science.html)
- [归档](https://kexue.fm/content.html)

渴望成为一个小飞侠

- [欢迎订阅](https://kexue.fm/feed)
- [个性邮箱](https://kexue.fm/archives/119)
- [天象信息](https://kexue.fm/ac.html)
- [观测ISS](https://kexue.fm/archives/41)
- [LaTeX](https://kexue.fm/latex.html)
- [关于博主](https://kexue.fm/me.html)

欢迎访问“科学空间”，这里将与您共同探讨自然科学，回味人生百态；也期待大家的分享～

- [**千奇百怪** Everything](https://kexue.fm/category/Everything)
- [**天文探索** Astronomy](https://kexue.fm/category/Astronomy)
- [**数学研究** Mathematics](https://kexue.fm/category/Mathematics)
- [**物理化学** Phy-chem](https://kexue.fm/category/Phy-chem)
- [**信息时代** Big-Data](https://kexue.fm/category/Big-Data)
- [**生物自然** Biology](https://kexue.fm/category/Biology)
- [**图片摄影** Photograph](https://kexue.fm/category/Photograph)
- [**问题百科** Questions](https://kexue.fm/category/Questions)
- [**生活/情感** Life-Feeling](https://kexue.fm/category/Life-Feeling)
- [**资源共享** Resources](https://kexue.fm/category/Resources)

- [**千奇百怪**](https://kexue.fm/category/Everything)
- [**天文探索**](https://kexue.fm/category/Astronomy)
- [**数学研究**](https://kexue.fm/category/Mathematics)
- [**物理化学**](https://kexue.fm/category/Phy-chem)
- [**信息时代**](https://kexue.fm/category/Big-Data)
- [**生物自然**](https://kexue.fm/category/Biology)
- [**图片摄影**](https://kexue.fm/category/Photograph)
- [**问题百科**](https://kexue.fm/category/Questions)
- [**生活/情感**](https://kexue.fm/category/Life-Feeling)
- [**资源共享**](https://kexue.fm/category/Resources)

[首页](https://kexue.fm) [信息时代](https://kexue.fm/category/Big-Data) Transformer升级之路：13、逆用Leaky ReRoPE

14Aug

# [Transformer升级之路：13、逆用Leaky ReRoPE](https://kexue.fm/archives/9728)

By 苏剑林 \|
2023-08-14 \|
32429位读者\|

上周在 [《Transformer升级之路：12、无限外推的ReRoPE？》](https://kexue.fm/archives/9708) 中，笔者提出了ReRoPE和Leaky ReRoPE，诸多实验结果表明，它们能够在几乎不损失训练效果的情况下免微调地扩展LLM的Context长度，并且实现了“longer context, lower loss”的理想特性，此外跟NTK-aware Scaled RoPE不同的是，其中ReRoPE似乎还有表现出了无限的Context处理能力。

总之，ReRoPE看起来相当让人满意，但美中不足的是会增加推理成本，具体表现为第一步推理需要算两次Attention，以及后续每步推理需要重新计算位置编码。本文试图通过在训练中逆用Leaky ReRoPE的方法来解决这个问题。

## 回顾 [\#](https://kexue.fm/kexue.fm\#%E5%9B%9E%E9%A1%BE)

让我们不厌其烦地重温一下：RoPE形式上是一种绝对位置编码，但实际达到的效果是相对位置编码，对应的相对位置矩阵是：
\\begin{equation}\\begin{pmatrix}0 & \\\
1 & 0 & \\\
2 & 1 & 0 &\\\
3 & 2 & 1 & 0 & \\\
\\ddots & 3 & 2 & 1 & 0 & \\\
\\ddots & \\ddots & 3 & 2 & 1 & 0 & \\\
\\ddots & \\ddots & \\ddots & \\ddots & \\ddots & \\ddots & \\ddots \\\
\\small{L - 2} & \\ddots & \\ddots & \\ddots & \\ddots & \\ddots & \\ddots & \\ddots \\\
\\small{L - 1} & \\small{L - 2} & \\ddots & \\ddots & \\ddots & 3 & 2 & 1 & 0 & \\\
\\end{pmatrix}\\label{eq:rope}\\end{equation}
为了在保留局域性的同时避免Long Context导致位置越界问题，Leaky ReRoPE将推理阶段的相对位置矩阵改为：
\\begin{equation}\\begin{pmatrix}
\\color{red}{0} & \\\
\\color{red}{1} & \\color{red}{0} & \\\
\\color{red}{2} & \\color{red}{1} & \\color{red}{0} & \\\
\\color{red}{\\ddots} & \\color{red}{2} & \\color{red}{1} & \\color{red}{0} & \\\
\\color{red}{\\small{w - 1}} & \\color{red}{\\ddots} & \\color{red}{2} & \\color{red}{1} & \\color{red}{0} & \\\
\\color{green}{w} & \\color{red}{\\small{w - 1}} & \\color{red}{\\ddots} & \\color{red}{2} & \\color{red}{1} & \\color{red}{0} & \\\
\\color{green}{\\small{w + \\frac{1}{k}}} & \\color{green}{w} & \\color{red}{\\ddots} & \\color{red}{\\ddots} & \\color{red}{2} & \\color{red}{1} & \\color{red}{0} & \\\
\\color{green}{\\small{w + \\frac{2}{k}}} & \\color{green}{\\small{w + \\frac{1}{k}}} & \\color{green}{\\ddots} & \\color{red}{\\ddots} & \\color{red}{\\ddots} & \\color{red}{2} & \\color{red}{1} & \\color{red}{0} & \\\
\\color{green}{\\ddots} & \\color{green}{\\small{w + \\frac{2}{k}}} & \\color{green}{\\ddots} & \\color{green}{\\ddots} & \\color{red}{\\ddots} & \\color{red}{\\ddots} & \\color{red}{2} & \\color{red}{1} & \\color{red}{0} & \\\
\\color{green}{\\ddots} & \\color{green}{\\ddots} & \\color{green}{\\ddots} & \\color{green}{\\ddots} & \\color{green}{\\ddots} & \\color{red}{\\ddots} & \\color{red}{\\ddots} & \\color{red}{\\ddots} & \\color{red}{\\ddots} & \\color{red}{\\ddots} & \\\
\\color{green}{\\ddots} & \\color{green}{\\ddots} & \\color{green}{\\ddots} & \\color{green}{\\small{w + \\frac{2}{k}}} & \\color{green}{\\small{w + \\frac{1}{k}}} & \\color{green}{w} & \\color{red}{\\small{w - 1}} & \\color{red}{\\ddots} & \\color{red}{2} & \\color{red}{1} & \\color{red}{0} & \\\
\\color{green}{\\small{w + \\frac{L-1-w}{k}}} & \\color{green}{\\ddots} & \\color{green}{\\ddots} & \\color{green}{\\ddots} & \\color{green}{\\small{w + \\frac{2}{k}}} & \\color{green}{\\small{w + \\frac{1}{k}}} & \\color{green}{w} & \\color{red}{\\small{w - 1}} & \\color{red}{\\ddots} & \\color{red}{2} & \\color{red}{1} & \\color{red}{0} & \\\
\\end{pmatrix}\\label{eq:leaky-rerope}\\end{equation}
其中$w$是窗口宽度，大概取训练长度的$\\frac{1}{4}$到$\\frac{1}{2}$，$k$用来调节可处理的最大长度，一般使得$w + \\frac{L-1-w}{k}$不超过训练长度的一半为佳。至于ReRoPE，则是直接取了$k\\to\\infty$的极限：
\\begin{equation}\\begin{pmatrix}
\\color{red}{0} & \\\
\\color{red}{1} & \\color{red}{0} & \\\
\\color{red}{2} & \\color{red}{1} & \\color{red}{0} & \\\
\\color{red}{\\ddots} & \\color{red}{2} & \\color{red}{1} & \\color{red}{0} & \\\
\\color{red}{\\small{w - 1}} & \\color{red}{\\ddots} & \\color{red}{2} & \\color{red}{1} & \\color{red}{0} & \\\
\\color{green}{w} & \\color{red}{\\small{w - 1}} & \\color{red}{\\ddots} & \\color{red}{2} & \\color{red}{1} & \\color{red}{0} & \\\
\\color{green}{w} & \\color{green}{w} & \\color{red}{\\ddots} & \\color{red}{\\ddots} & \\color{red}{2} & \\color{red}{1} & \\color{red}{0} & \\\
\\color{green}{w} & \\color{green}{w} & \\color{green}{\\ddots} & \\color{red}{\\ddots} & \\color{red}{\\ddots} & \\color{red}{2} & \\color{red}{1} & \\color{red}{0} & \\\
\\color{green}{\\ddots} & \\color{green}{w} & \\color{green}{\\ddots} & \\color{green}{\\ddots} & \\color{red}{\\ddots} & \\color{red}{\\ddots} & \\color{red}{2} & \\color{red}{1} & \\color{red}{0} & \\\
\\color{green}{\\ddots} & \\color{green}{\\ddots} & \\color{green}{\\ddots} & \\color{green}{\\ddots} & \\color{green}{\\ddots} & \\color{red}{\\ddots} & \\color{red}{\\ddots} & \\color{red}{\\ddots} & \\color{red}{\\ddots} & \\color{red}{\\ddots} & \\\
\\color{green}{\\ddots} & \\color{green}{\\ddots} & \\color{green}{\\ddots} & \\color{green}{w} & \\color{green}{w} & \\color{green}{w} & \\color{red}{\\small{w - 1}} & \\color{red}{\\ddots} & \\color{red}{2} & \\color{red}{1} & \\color{red}{0} & \\\
\\color{green}{w} & \\color{green}{\\ddots} & \\color{green}{\\ddots} & \\color{green}{\\ddots} & \\color{green}{w} & \\color{green}{w} & \\color{green}{w} & \\color{red}{\\small{w - 1}} & \\color{red}{\\ddots} & \\color{red}{2} & \\color{red}{1} & \\color{red}{0} & \\\
\\end{pmatrix}\\label{eq:rerope}\\end{equation}

## 反转 [\#](https://kexue.fm/kexue.fm\#%E5%8F%8D%E8%BD%AC)

从上一篇的评测结果来看，作为一种免训练的外推方案，ReRoPE和Leaky ReRoPE的效果都是相当让人满意的，既没有损失训练长度内的效果，又实现了“Longer Context, Lower Loss”。唯一美中不足的是，它们的推理速度相比原本的Attention来说是变慢的，并且目前尚不兼容Flash Attention等加速技术。

那么，能否反过来呢？ReRoPE/Leaky ReRoPE在训练阶段是正常速度的RoPE，推理阶段则是变慢了，反过来也就是说：能否让训练阶段变慢，让推理阶段变为常规的RoPE？可能有读者疑惑：为什么会想要让训练阶段变慢？训练成本不是更高吗？这是因为ReRoPE/Leaky ReRoPE是一种长度外推方法，场景是“Train Short, Test Long”，训练速度的变慢是短期的、可控的，推理速度的变慢才是长期的、难顶的，所以相较之下，如果是同等程度的变慢的话，我们更愿意将变慢的部分放到训练阶段。

让我们再看一下Leaky ReRoPE，它在训练阶段的相对位置矩阵是步长为1的式$\\eqref{eq:rope}$，推理阶段则在$w$的窗口内使用$1$的步长，在窗口外使用$\\frac{1}{k} < 1$的步长，即式$\\eqref{eq:leaky-rerope}$，换句话说，差别是推理阶段窗口外使用更小的步长。如果我们反过来，在训练阶段使用Leaky ReRoPE，并让它窗口外的步长大于$1$，那么按照“推理阶段窗口外使用更小的步长”的原则，推理阶段窗口外是否就可以使用等于$1$的步长，从而退化为RoPE了？

笔者将以上想法称之为“InvLeaky ReRoPE（Inverse Leaky ReRoPE）”。事不宜迟，我们马上做实验测试。

## 实验 [\#](https://kexue.fm/kexue.fm\#%E5%AE%9E%E9%AA%8C)

继续之前的“ [GAU](https://kexue.fm/archives/8934) \+ [Deep Norm](https://kexue.fm/archives/8978) \+ [Tiger](https://kexue.fm/archives/9512) \+ 语言模型”实验组合，在训练阶段使用$k=1/16, w=128$的Leaky ReRoPE，在推理阶段使用正常的RoPE，测试结果如下：

\\begin{array}{c\|cc}
\\hline
\\text{测试长度} & 512(\\text{训练}) & 4096(\\text{重复}) & 4096(\\text{不重复})\\\
\\hline
\\text{Baseline} & 49.41\\% & 24.17\\% & 23.16\\% \\\
\\text{Baseline-}\\log n & 49.40\\% & 24.60\\% & 24.02\\% \\\
\\hline
\\text{NTK-RoPE-fixed} & 49.41\\% & 51.86\\% & 39.61\\% \\\
\\text{NTK-RoPE-}\\log n^{\\color{red}{\\dagger}}\\text{-fixed} & 49.41\\% & 55.94\\% & 41.11\\% \\\
\\text{NTK-RoPE-}\\log n\\text{-fixed} & 49.40\\% & 62.85\\% & 44.14\\% \\\
\\text{NTK-RoPE-mixed} & 49.41\\% & 53.09\\% & 40.12\\% \\\
\\text{NTK-RoPE-}\\log n^{\\color{red}{\\dagger}}\\text{-mixed} & 49.41\\% & 59.11\\% & 42.38\\% \\\
\\text{NTK-RoPE-}\\log n\\text{-mixed} & 49.40\\% & 68.91\\% & 45.41\\% \\\
\\hline
\\text{ReRoPE-w256} & 49.41\\% & 77.90\\% & 48.48\\% \\\
\\text{ReRoPE-w256-}\\log n^{\\color{red}{\\dagger}} & 49.41\\% & 82.40\\% & 48.85\\% \\\
\\text{ReRoPE-w256-}\\log n & 49.40\\% & \\boldsymbol{85.12\\%} & \\boldsymbol{49.07\\%} \\\
\\hline
\\text{InvLeaky ReRoPE-w128-}\\log n & 49.38\\% & 82.25\\% & 48.32\\% \\\
\\text{InvLeaky ReRoPE-w128-b8-}\\log n & 49.62\\% & 81.15\\% & 48.85\\% \\\
\\hline
\\text{HFWA} & 48.70\\% & 80.84\\% & 48.15\\% \\\
\\hline
\\end{array}

其中$\\text{b8}$是指RoPE的频率底数从10000换成了80000。可以看到，“Leaky ReRoPE → RoPE”的InvLeaky ReRoPE虽然效果上不如“RoPE → ReRoPE/Leaky ReRoPE”，但依然胜过了HFWA，并且由于推理阶段是常规的RoPE，可以套用现成的加速技术，因此依然是有相当竞争力的。此外，笔者对$k,w,b$等参数做了一些简单的调参，发现最优解基本上就是以上两个组合了，即“$k$设置为‘扩展倍数的2倍的倒数’、$w$设置为训练长度的$\\frac{1}{4}$、$b$可选乘以扩展倍数”。

那么，InvLeaky ReRoPE对训练速度有多大影响呢？在上述实验中，模型是1亿参数量，训练长度是512，每1000步的训练时间从330秒增加到了350秒，增加不到10%，当然这里边有GAU的原因，因为GAU是单头的注意力，本就比多头注意力快。如果多头注意力或者训练长度更长的话，增加幅度应该会大一些，但目测应该不超过50%都是可以接受的。

## 小结 [\#](https://kexue.fm/kexue.fm\#%E5%B0%8F%E7%BB%93)

本文提出了Leaky ReRoPE的“逆用”做法，通过在训练阶段使用更大步长的Leaky ReRoPE，使得推理阶段可以退回常规的RoPE，从而可以保持推理速度不变，实验结果显示这种做法还是有一定的竞争力的。

_**转载到请包括本文地址：** [https://kexue.fm/archives/9728](https://kexue.fm/archives/9728)_

_**更详细的转载事宜请参考：**_ [《科学空间FAQ》](https://kexue.fm/archives/6508#%E6%96%87%E7%AB%A0%E5%A6%82%E4%BD%95%E8%BD%AC%E8%BD%BD/%E5%BC%95%E7%94%A8)

**如果您还有什么疑惑或建议，欢迎在下方评论区继续讨论。**

**如果您觉得本文还不错，欢迎 [分享](https://kexue.fm/kexue.fm#share)/ [打赏](https://kexue.fm/kexue.fm#pay) 本文。打赏并非要从中获得收益，而是希望知道科学空间获得了多少读者的真心关注。当然，如果你无视它，也不会影响你的阅读。再次表示欢迎和感谢！**

打赏

微信打赏

支付宝打赏

因为网站后台对打赏并无记录，因此欢迎在打赏时候备注留言。你还可以 [**点击这里**](http://mail.qq.com/cgi-bin/qm_share?t=qm_mailme&email=tN7d1drY3drrx8H0xcWa19vZ) 或在下方评论区留言来告知你的建议或需求。

**如果您需要引用本文，请参考：**

苏剑林. (Aug. 14, 2023). 《Transformer升级之路：13、逆用Leaky ReRoPE 》\[Blog post\]. Retrieved from [https://kexue.fm/archives/9728](https://kexue.fm/archives/9728)

@online{kexuefm-9728,
        title={Transformer升级之路：13、逆用Leaky ReRoPE},
        author={苏剑林},
        year={2023},
        month={Aug},
        url={\\url{https://kexue.fm/archives/9728}},
}

分类： [信息时代](https://kexue.fm/category/Big-Data)    标签： [attention](https://kexue.fm/tag/attention/), [位置编码](https://kexue.fm/tag/%E4%BD%8D%E7%BD%AE%E7%BC%96%E7%A0%81/), [泛化](https://kexue.fm/tag/%E6%B3%9B%E5%8C%96/), [外推](https://kexue.fm/tag/%E5%A4%96%E6%8E%A8/), [rope](https://kexue.fm/tag/rope/)[6 评论](https://kexue.fm/archives/9728#comments)

< [Transformer升级之路：12、无限外推的ReRoPE？](https://kexue.fm/archives/9708) \| [Transformer升级之路：14、当HWFA遇见ReRoPE](https://kexue.fm/archives/9731) >

### 你也许还对下面的内容感兴趣

- [低精度Attention可能存在有偏的舍入误差](https://kexue.fm/archives/11371)
- [为什么线性注意力要加Short Conv？](https://kexue.fm/archives/11320)
- [QK-Clip：让Muon在Scaleup之路上更进一步](https://kexue.fm/archives/11126)
- [Transformer升级之路：21、MLA好在哪里?（下）](https://kexue.fm/archives/11111)
- [“对角+低秩”三角阵的高效求逆方法](https://kexue.fm/archives/11072)
- [线性注意力简史：从模仿、创新到反哺](https://kexue.fm/archives/11033)
- [Transformer升级之路：20、MLA好在哪里?（上）](https://kexue.fm/archives/10907)
- [Transformer升级之路：19、第二类旋转位置编码](https://kexue.fm/archives/10862)
- [细水长flow之TARFLOW：流模型满血归来？](https://kexue.fm/archives/10667)
- [“闭门造车”之多模态思路浅谈（三）：位置编码](https://kexue.fm/archives/10352)

[发表你的看法](https://kexue.fm/kexue.fm#comment_form)

[Transformer升级之路：13、逆用Leaky ReRoPE R11; AI 資訊](https://news.aitime.space/2023/08/23209/)

August 14th, 2023

\[...\]​Read More \[...\]

[回复评论](https://kexue.fm/archives/9728/comment-page-1?replyTo=22497#respond-post-9728)

denghao li

August 14th, 2023

苏神这个“难顶的”一词成功阻止了大学生们直接复制粘贴进自己论文里^。^

[回复评论](https://kexue.fm/archives/9728/comment-page-1?replyTo=22501#respond-post-9728)

[pan](http://zyinfo.pro) 发表于
August 29th, 2023

我们做了个基于AI大模型的自动校对工具：
http://zyinfo.pro/ai/review/
以后这类小错误估计都能查出来。

其实也可以通过AI 重写一部分？

[回复评论](https://kexue.fm/archives/9728/comment-page-1?replyTo=22604#respond-post-9728)

[苏剑林](https://kexue.fm) 发表于
August 31st, 2023

谢谢推荐，有机会试用一下。

不过“难顶的”不是错误，而是故意口语化罢了（或者说白话化）。

[回复评论](https://kexue.fm/archives/9728/comment-page-1?replyTo=22619#respond-post-9728)

[Mingyang Deng](http://lambertae.github.io)

January 2nd, 2024

苏老师您好，今天读到您的博客，很受启发。关于速度问题，ReRope有没有可能这样实现呢：
(1) 将输入划分成大小是w/2的batch
(2) 每个batch和自己及上一个batch采用RoPe计算attention；和再之前的batch使用w计算attention
这样推理的开销应该是一样的（每两个batch之间只计算了一次attention），同时效果几乎是ReRope（除了有一些w/2到w之间的值被近似成了w）

[回复评论](https://kexue.fm/archives/9728/comment-page-1?replyTo=23431#respond-post-9728)

[苏剑林](https://kexue.fm) 发表于
January 5th, 2024

这正是flash attention的分块思路！已经有读者提供过一个参考实现了 [https://kexue.fm/archives/9708/comment-page-2#comment-22614](https://kexue.fm/archives/9708/comment-page-2#comment-22614)

[回复评论](https://kexue.fm/archives/9728/comment-page-1?replyTo=23453#respond-post-9728)

[取消回复](https://kexue.fm/archives/9728#respond-post-9728)

你的大名

电子邮箱

个人网站（选填）

1\. 可以使用LaTeX代码，点击“预览效果”可查看效果；2. 可以通过点击评论楼层编号来引用该楼层；3. 网站可能会有点卡，如非确认评论失败，请 **不要重复点击提交**。

### 内容速览

[回顾](https://kexue.fm/kexue.fm#%E5%9B%9E%E9%A1%BE)
[反转](https://kexue.fm/kexue.fm#%E5%8F%8D%E8%BD%AC)
[实验](https://kexue.fm/kexue.fm#%E5%AE%9E%E9%AA%8C)
[小结](https://kexue.fm/kexue.fm#%E5%B0%8F%E7%BB%93)

### 智能搜索

支持整句搜索！网站自动使用 [结巴分词](https://github.com/fxsjy/jieba) 进行分词，并结合ngrams排序算法给出合理的搜索结果。

### 热门标签

[生成模型](https://kexue.fm/tag/%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B/) [attention](https://kexue.fm/tag/attention/) [优化](https://kexue.fm/tag/%E4%BC%98%E5%8C%96/) [语言模型](https://kexue.fm/tag/%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/) [模型](https://kexue.fm/tag/%E6%A8%A1%E5%9E%8B/) [网站](https://kexue.fm/tag/%E7%BD%91%E7%AB%99/) [概率](https://kexue.fm/tag/%E6%A6%82%E7%8E%87/) [梯度](https://kexue.fm/tag/%E6%A2%AF%E5%BA%A6/) [矩阵](https://kexue.fm/tag/%E7%9F%A9%E9%98%B5/) [转载](https://kexue.fm/tag/%E8%BD%AC%E8%BD%BD/) [优化器](https://kexue.fm/tag/%E4%BC%98%E5%8C%96%E5%99%A8/) [微分方程](https://kexue.fm/tag/%E5%BE%AE%E5%88%86%E6%96%B9%E7%A8%8B/) [分析](https://kexue.fm/tag/%E5%88%86%E6%9E%90/) [天象](https://kexue.fm/tag/%E5%A4%A9%E8%B1%A1/) [深度学习](https://kexue.fm/tag/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/) [积分](https://kexue.fm/tag/%E7%A7%AF%E5%88%86/) [python](https://kexue.fm/tag/python/) [力学](https://kexue.fm/tag/%E5%8A%9B%E5%AD%A6/) [无监督](https://kexue.fm/tag/%E6%97%A0%E7%9B%91%E7%9D%A3/) [扩散](https://kexue.fm/tag/%E6%89%A9%E6%95%A3/) [几何](https://kexue.fm/tag/%E5%87%A0%E4%BD%95/) [节日](https://kexue.fm/tag/%E8%8A%82%E6%97%A5/) [生活](https://kexue.fm/tag/%E7%94%9F%E6%B4%BB/) [文本生成](https://kexue.fm/tag/%E6%96%87%E6%9C%AC%E7%94%9F%E6%88%90/) [数论](https://kexue.fm/tag/%E6%95%B0%E8%AE%BA/)

### 随机文章

- [必须要GPT3吗？不，BERT的MLM模型也能小样本学习](https://kexue.fm/archives/7764)
- [与首都机场的“难分难舍”](https://kexue.fm/archives/2013)
- [通向概率分布之路：盘点Softmax及其替代品](https://kexue.fm/archives/10145)
- [第114号化学元素再次被实验确认](https://kexue.fm/archives/692)
- [函数光滑化杂谈：不可导函数的可导逼近](https://kexue.fm/archives/6620)
- [广州亚运歌曲《重逢》歌词(中英文版)](https://kexue.fm/archives/136)
- [历史上的谜案——刘徽有没有使用外推法？](https://kexue.fm/archives/1292)
- [\[问题解答\]双曲线上的最短距离](https://kexue.fm/archives/1904)
- [又一道川菜！媲美“开水白菜”的瓜燕穗肚](https://kexue.fm/archives/6158)
- [一道比较函数大小的题目](https://kexue.fm/archives/1395)

### 最近评论

- [ZHANG Ruiqi](https://kexue.fm/archives/6760/comment-page-8#comment-28777): 谢谢苏神的教程和回复，我同意你的观点。数学上 VQ-VAE 不像 VAE 这么优美，感觉像计算...
- [苏剑林](https://kexue.fm/archives/6760/comment-page-8#comment-28776): 很详细，我认为这个理解没有问题，但我依然不认同VQ-VAE的命名。常规的VAE，是在训练Aut...
- [苏剑林](https://kexue.fm/archives/11233/comment-page-1#comment-28775): 激活函数是$f(x)$，$\\phi(x)$是构造$f(x)$的一部份。
- [苏剑林](https://kexue.fm/archives/10352/comment-page-2#comment-28774): 这里的等价性只是一个具体例子，如果你认为一个image token等价于0.5个text to...
- [苏剑林](https://kexue.fm/archives/11388/comment-page-1#comment-28773): 好像是。你可以去X上@一下Jeremy Bernstein～
- [苏剑林](https://kexue.fm/archives/11111/comment-page-2#comment-28772): 1\. 我们暂时认为MLA相比GQA有优势；
2\. Kimi Linear其实还是个toy，用来...
- [苏剑林](https://kexue.fm/archives/10862/comment-page-1#comment-28771): 我自己测过，效果尚可。
- [苏剑林](https://kexue.fm/archives/10862/comment-page-1#comment-28770): 你是不是指kv cache只保存加RoPE前的，推理阶段实时给k/v加RoPE，从而减少kv ...
- [苏剑林](https://kexue.fm/archives/10958/comment-page-3#comment-28769): 我有点没懂你的问题，你是不是指推理时噪声分布改为其他分布，而不是默认的高斯分布，有没有可能提高...
- [苏剑林](https://kexue.fm/archives/11033/comment-page-2#comment-28768): 可能的，但online update缓解了这个问题，加之现在主流的用法都是主动加上decay，...

### 友情链接

- [Cool Papers](https://papers.cool)
- [数学研发](https://bbs.emath.ac.cn)
- [Seatop](http://www.seatop.com.cn/)
- [Xiaoxia](https://xiaoxia.org/)
- [积分表-网络版](https://kexue.fm/sci/integral/index.html)
- [丝路博傲](http://blog.dvxj.com/)
- [数学之家](http://www.2math.cn/)
- [有趣天文奇观](http://interesting-sky.china-vo.org/)
- [TwistedW](http://www.twistedwg.com/)
- [godweiyang](https://godweiyang.com/)
- [AI柠檬](https://blog.ailemon.net/)
- [王登科-DK博客](https://greatdk.com)
- [ESON](https://blog.eson.org/)
- [枫之羽](https://fzhiy.net/)
- [Mathor's blog](https://wmathor.com/)
- [coding-zuo](https://coding-zuo.github.io/)
- [博科园](https://www.bokeyuan.net/)
- [孔皮皮的博客](https://www.kppkkp.top/)
- [运鹏的博客](https://yunpengtai.top/)
- [jiming.site](https://jiming.site/)
- [OmegaXYZ](https://www.omegaxyz.com/)
- [EAI猩球](https://www.robotech.ink/)
- [文举的博客](https://liwenju0.com/)
- [申请链接](https://kexue.fm/links.html)

本站采用创作共用版权协议，要求署名、非商业用途和保持一致。转载本站内容必须也遵循“ [署名-非商业用途-保持一致](http://creativecommons.org/licenses/by-nc-nd/2.5/cn/)”的创作共用协议。
© 2009-2025 Scientific Spaces. All rights reserved. Theme by [laogui](http://www.laogui.com). Powered by [Typecho](http://typecho.org). 备案号: [粤ICP备09093259号-1/2](https://beian.miit.gov.cn/)。