## SEARCH

## MENU

- [打赏](https://kexue.fm/reward.html)
- [公式](https://kexue.fm/latex.html)
- [天象](https://kexue.fm/ac.html)
- [链接](https://kexue.fm/links.html)
- [时光](https://kexue.fm/me.html)
- [博览](https://kexue.fm/science.html)
- [归档](https://kexue.fm/content.html)

## CATEGORIES

- [千奇百怪](https://kexue.fm/category/Everything)
- [天文探索](https://kexue.fm/category/Astronomy)
- [数学研究](https://kexue.fm/category/Mathematics)
- [物理化学](https://kexue.fm/category/Phy-chem)
- [信息时代](https://kexue.fm/category/Big-Data)
- [生物自然](https://kexue.fm/category/Biology)
- [图片摄影](https://kexue.fm/category/Photograph)
- [问题百科](https://kexue.fm/category/Questions)
- [生活/情感](https://kexue.fm/category/Life-Feeling)
- [资源共享](https://kexue.fm/category/Resources)

## NEWPOSTS

- [msign的导数](https://kexue.fm/archives/11025)
- [通过msign来计算mclip（奇...](https://kexue.fm/archives/11006)
- [msign算子的Newton-Sc...](https://kexue.fm/archives/10996)
- [等值振荡定理：最优多项式逼近的充要条件](https://kexue.fm/archives/10972)
- [生成扩散模型漫谈（三十）：从瞬时速...](https://kexue.fm/archives/10958)
- [MoE环游记：5、均匀分布的反思](https://kexue.fm/archives/10945)
- [msign算子的Newton-Sc...](https://kexue.fm/archives/10922)
- [Transformer升级之路：2...](https://kexue.fm/archives/10907)
- [一道概率不等式：盯着它到显然成立为止！](https://kexue.fm/archives/10902)
- [SVD的导数](https://kexue.fm/archives/10878)

## COMMENTS

- [rpsun: 老师您好，最近在自己的任务上尝试了muon，甚至只修改了学习率...](https://kexue.fm/archives/10592/comment-page-2#comment-27912)
- [盏一: 我之前做的笔记:Q: 公式 (14) 的理解.A: 首先基于 ...](https://kexue.fm/archives/8265/comment-page-8#comment-27911)
- [盏一: 哦哦哦 你是说 $\\exp nB$ 是正交矩阵! 并不是说 B.](https://kexue.fm/archives/8397/comment-page-3#comment-27910)
- [盏一: 呃, 是我脑子乱了... 忘了 $\\exp(0) = I$. ...](https://kexue.fm/archives/8397/comment-page-3#comment-27908)
- [盏一: 苏神, 请教一下\> 并且还可以证明它一定是正交矩阵是怎么证明的...](https://kexue.fm/archives/8397/comment-page-3#comment-27907)
- [sk: 请问公式14是怎么得出来的？](https://kexue.fm/archives/8265/comment-page-8#comment-27906)
- [tll1945tll1937: 真心实意的向大家请教问题：看了文章“对齐全量微调！这是我看过最...](https://kexue.fm/archives/10266/comment-page-1#comment-27901)
- [oYo\_logan: \[comment=27017\]苏剑林\[/comment\]苏神，...](https://kexue.fm/archives/10757/comment-page-1#comment-27897)
- [z123: 在参数矩阵较多的CNN小模型上，Muon会明显慢于Adam，这...](https://kexue.fm/archives/10592/comment-page-1#comment-27896)
- [dry: 苏神好，一直有个疑问，ReFlow构建的ODE是$dx\_t/d...](https://kexue.fm/archives/10958/comment-page-2#comment-27895)

## USERLOGIN

- [登录](https://kexue.fm/admin/login.php)

[科学空间\|Scientific Spaces](https://kexue.fm)

- [登录](https://kexue.fm/admin/login.php)
- [打赏](https://kexue.fm/reward.html)
- [公式](https://kexue.fm/latex.html)
- [天象](https://kexue.fm/ac.html)
- [链接](https://kexue.fm/links.html)
- [时光](https://kexue.fm/me.html)
- [博览](https://kexue.fm/science.html)
- [归档](https://kexue.fm/content.html)

渴望成为一个小飞侠

- [欢迎订阅](https://kexue.fm/feed)
- [个性邮箱](https://kexue.fm/archives/119)
- [天象信息](https://kexue.fm/ac.html)
- [观测ISS](https://kexue.fm/archives/41)
- [LaTeX](https://kexue.fm/latex.html)
- [关于博主](https://kexue.fm/me.html)

欢迎访问“科学空间”，这里将与您共同探讨自然科学，回味人生百态；也期待大家的分享～

- [**千奇百怪** Everything](https://kexue.fm/category/Everything)
- [**天文探索** Astronomy](https://kexue.fm/category/Astronomy)
- [**数学研究** Mathematics](https://kexue.fm/category/Mathematics)
- [**物理化学** Phy-chem](https://kexue.fm/category/Phy-chem)
- [**信息时代** Big-Data](https://kexue.fm/category/Big-Data)
- [**生物自然** Biology](https://kexue.fm/category/Biology)
- [**图片摄影** Photograph](https://kexue.fm/category/Photograph)
- [**问题百科** Questions](https://kexue.fm/category/Questions)
- [**生活/情感** Life-Feeling](https://kexue.fm/category/Life-Feeling)
- [**资源共享** Resources](https://kexue.fm/category/Resources)

- [**千奇百怪**](https://kexue.fm/category/Everything)
- [**天文探索**](https://kexue.fm/category/Astronomy)
- [**数学研究**](https://kexue.fm/category/Mathematics)
- [**物理化学**](https://kexue.fm/category/Phy-chem)
- [**信息时代**](https://kexue.fm/category/Big-Data)
- [**生物自然**](https://kexue.fm/category/Biology)
- [**图片摄影**](https://kexue.fm/category/Photograph)
- [**问题百科**](https://kexue.fm/category/Questions)
- [**生活/情感**](https://kexue.fm/category/Life-Feeling)
- [**资源共享**](https://kexue.fm/category/Resources)

[首页](https://kexue.fm) [数学研究](https://kexue.fm/category/Mathematics) [信息时代](https://kexue.fm/category/Big-Data) f-GAN简介：GAN模型的生产车间

29Sep

# [f-GAN简介：GAN模型的生产车间](https://kexue.fm/archives/6016)

By 苏剑林 \|
2018-09-29 \|
192638位读者\|

今天介绍一篇比较经典的工作，作者命名为 [f-GAN](https://papers.cool/arxiv/1606.00709)，他在文章中给出了通过一般的$f$散度来构造一般的GAN的方案。可以毫不夸张地说，这论文就是一个GAN模型的“生产车间”，它一般化的囊括了很多GAN变种，并且可以启发我们快速地构建新的GAN变种（当然有没有价值是另一回事，但理论上是这样）。

## 局部变分 [\#](https://kexue.fm/archives/6016\#%E5%B1%80%E9%83%A8%E5%8F%98%E5%88%86)

整篇文章对$f$散度的处理事实上在机器学习中被称为“局部变分方法”，它是一种非常经典且有用的估算技巧。事实上本文将会花大部分篇幅介绍这种估算技巧在$f$散度中的应用结果。至于GAN，只不过是这个结果的基本应用而已。

### f散度 [\#](https://kexue.fm/archives/6016\#f%E6%95%A3%E5%BA%A6)

首先我们还是对$f$散度进行基本的介绍。所谓$f$散度，是KL散度的一般化：
$$\\begin{equation}\\mathcal{D}\_f(P\\Vert Q) = \\int q(x) f\\left(\\frac{p(x)}{q(x)}\\right)dx\\label{eq:f-div}\\end{equation}$$
注意，按照通用的约定写法，括号内是$p/q$而不是$q/p$，大家不要自然而言地根据KL散度的形式以为是$q/p$。

可以发现，这种形式能覆盖我们见过的很多概率分布之间的度量了，这里直接把论文中的表格搬进来（部分）
$$\\begin{array}{c\|c\|c}\\hline
\\textbf{距离名称} & \\textbf{计算公式} & \\textbf{对应的}f\\\
\\hline
\\text{总变差} & \\frac{1}{2}\\int \| p(x) - q(x)\| dx & \\frac{1}{2}\|u - 1\|\\\
\\hline
\\text{KL散度} & \\int p(x)\\log \\frac{p(x)}{q(x)} dx & u \\log u\\\
\\hline
\\text{逆KL散度} & \\int q(x)\\log \\frac{q(x)}{p(x)} dx & - \\log u\\\
\\hline
\\text{Pearson }\\chi^2 & \\int \\frac{(q(x) - p(x))^{2}}{p(x)} dx & \\frac{(1 - u)^{2}}{u}\\\
\\hline
\\text{Neyman }\\chi^2 & \\int \\frac{(p(x) - q(x))^{2}}{q(x)} dx & (u - 1)^{2}\\\
\\hline
\\text{Hellinger距离} & \\int \\left(\\sqrt{p(x)} - \\sqrt{q(x)}\\right)^{2} dx & (\\sqrt{u} - 1)^{2}\\\
\\hline
\\text{Jeffrey距离} & \\int (p(x) - q(x))\\log \\left(\\frac{p(x)}{q(x)}\\right) dx & (u - 1)\\log u\\\
\\hline
\\text{JS散度} & \\frac{1}{2}\\int p(x)\\log \\frac{2 p(x)}{p(x) + q(x)} + q(x)\\log \\frac{2 q(x)}{p(x) + q(x)} dx & -\\frac{u + 1}{2}\\log \\frac{1 + u}{2} + \\frac{u}{2} \\log u\\\
\\hline
\\end{array}$$

### 凸函数 [\#](https://kexue.fm/archives/6016\#%E5%87%B8%E5%87%BD%E6%95%B0)

上面列举了一堆的分布度量以及对应的$f$，那么一个很自然的问题是这些$f$的共同特点是什么呢？

答案是：

> 1、它们都是非负实数到实数的映射（$\\mathbb{R}^\* \\to \\mathbb{R}$）；
>
> 2、$f(1)=0$；
>
> 3、它们都是凸函数。

第一点是常规的，第二点$f(1)=0$保证了$\\mathcal{D}\_f(P\\Vert P)=0$，那第三点凸函数是怎么理解呢？其实它是凸函数性质的一个最基本的应用，因为凸函数有一个非常重要的性质（詹森不等式）：
$$\\begin{equation}\\mathbb{E}\\big\[f(x)\\big\]\\geq f\\big(\\mathbb{E}\[x\]\\big)\\label{eq:tuhanshu-xingzhi}\\end{equation}$$
也就是“函数的平均大于平均的函数”，有些教程会直接将这个性质作为凸函数的定义。而如果$f(u)$是光滑的函数，我们一般会通过二阶导数$f''(u)$是否恒大于等于0来判断是否凸函数。

利用$\\eqref{eq:tuhanshu-xingzhi}$，我们有
$$\\begin{equation}\\begin{aligned}\\int q(x) f\\left(\\frac{p(x)}{q(x)}\\right)dx =& \\mathbb{E}\_{x\\sim q(x)} \\left\[f\\left(\\frac{p(x)}{q(x)}\\right)\\right\]\\\
\\geq& f\\left(\\mathbb{E}\_{x\\sim q(x)} \\left\[\\frac{p(x)}{q(x)}\\right\]\\right)\\\
=& f\\left(\\int q(x) \\frac{p(x)}{q(x)}dx\\right)\\\
=& f\\left(\\int p(x)dx\\right)\\\
=& f(1) = 0
\\end{aligned}\\end{equation}$$
也就是说，这三个条件保证了$f$散度是非负，而且当两个分布一模一样时$f$散度就为0，这使得$\\mathcal{D}\_f$可以用来简单地度量分布之间的差异性。当然，$f$散度原则上并没有保证$P\\neq Q$时$\\mathcal{D}\_f(P\\Vert Q) \\gt 0$。但通常我们会选择严格凸的$f$（即$f''(u)$恒大于0），那么这时候可以保证$P\\neq Q$时$\\mathcal{D}\_f(P\\Vert Q)\\gt 0$，也就是说这时候有$\\mathcal{D}\_f(P\\Vert Q)=0\\,\\Leftrightarrow\\,P=Q$。（注：即便如此，一般情况下$\\mathcal{D}\_f(P\\Vert Q)$仍然不是满足公理化定义的“距离”，不过这个跟本文主题关系不大，这里只是顺便一提。）

### 凸共轭 [\#](https://kexue.fm/archives/6016\#%E5%87%B8%E5%85%B1%E8%BD%AD)

现在从比较数学的角度讨论一下凸函数，一般地，记凸函数的定义域为$\\mathbb{D}$（对于本文来说，$\\mathbb{D}=\\mathbb{R}\_+$）。选择任意一个点$\\xi$，我们求$y=f(u)$在$u=\\xi$处的切线，结果是
$$\\begin{equation}y = f(\\xi) + f'(\\xi)(u - \\xi)\\end{equation}$$
考虑两者的差函数
$$\\begin{equation}h(u) = f(u) - f(\\xi) - f'(\\xi)(u - \\xi)\\end{equation}$$
所谓凸函数，直观理解，就是它的图像总在它的（任意一条）切线上方，因此对于凸函数来说下式恒成立
$$\\begin{equation}f(u) - f(\\xi) - f'(\\xi)(u - \\xi)\\geq 0\\end{equation}$$
整理成
$$\\begin{equation}f(u) \\geq f(\\xi) - f'(\\xi) \\xi + f'(\\xi)u\\end{equation}$$
因为不等式是恒成立的，并且等号是有可能取到的，因此可以导出
$$\\begin{equation}f(u) = \\max\_{\\xi\\in\\mathbb{D}}\\big\\{f(\\xi) - f'(\\xi) \\xi + f'(\\xi)u\\big\\}\\end{equation}$$
换新的记号，记$t=f'(\\xi)$，并从中反解出$\\xi$（对于凸函数，这总能做到，读者可以自己尝试证明），然后记
$$\\begin{equation}g(t) = - f(\\xi) + f'(\\xi) \\xi\\end{equation}$$
那么就有
$$\\begin{equation}f(u) = \\max\_{t\\in f'(\\mathbb{D})}\\big\\{t u - g(t)\\big\\}\\end{equation}$$
这里的$g(t)$就称为$f(u)$的共轭函数。留意花括号里边的式子，给定$f$后，$g$也确定了，并且整个式子关于$u$是线性的。所以总的来说，我们做了这样的一件事情：

> 对一个凸函数给出了线性近似，并且通过最大化里边的参数就可以达到原来的值。

注意给定$u$，我们都要最大化一次$t$才能得到尽可能接近$f(u)$的结果，否则随便代入一个$t$，只能保证得到下界，而不能确保误差大小。所以它称为“局部变分方法”，因为要在每一个点（局部）处都要进行最大化（变分）。这样一来，我们可以理解为$t$实际上是$u$的函数，即
$$\\begin{equation}f(u) = \\max\_{T\\text{是值域为}f'(\\mathbb{D})\\text{的函数}}\\big\\{T(u) u - g(T(u))\\big\\}\\label{eq:max-conj}\\end{equation}$$

上述讨论过程实际上已经给出了计算凸共轭的方法，在这里我们直接给出上表对应的凸函数的共轭函数。
$$\\begin{array}{c\|c}\\hline
f(u) & \\textbf{对应的共轭}g(t) & f'(\\mathbb{D}) & 激活函数\\\
\\hline
\\frac{1}{2}\|u - 1\| & t & \\left\[-\\frac{1}{2},\\frac{1}{2}\\right\] & \\frac{1}{2}\\tanh(x)\\\
\\hline
u \\log u & e^{t-1} & \\mathbb{R} & x\\\
\\hline
\- \\log u & -1 - \\log(-t) & \\mathbb{R}\_- & -e^{x}\\\
\\hline
\\frac{(1 - u)^{2}}{u} & 2 - 2\\sqrt{1-t} & (-\\infty, 1) & 1-e^x\\\
\\hline
(u - 1)^{2} & \\frac{1}{4}t^2+t & (-2,+\\infty) & e^x-2\\\
\\hline
(\\sqrt{u} - 1)^{2} & \\frac{t}{1-t} & (-\\infty, 1) & 1-e^x\\\
\\hline
(u - 1)\\log u & W(e^{1-t})+\\frac{1}{W(e^{1-t})}+t-2 & \\mathbb{R} & x\\\
\\hline
-\\frac{u + 1}{2}\\log \\frac{1 + u}{2} + \\frac{u}{2} \\log u & -\\frac{1}{2}\\log(2-e^{2t}) & \\left(-\\infty,\\frac{\\log 2}{2}\\right) & \\frac{\\log 2}{2}-\\frac{1}{2}\\log(1+e^{-x})\\\
\\hline
\\end{array}$$
（注：这里的$W$为 [朗伯W函数](https://en.wikipedia.org/wiki/Lambert_W_function)。）

## f-GAN [\#](https://kexue.fm/archives/6016\#f-GAN)

由上述推导，我们就可以给出f散度的估算公式，并且进一步给出f-GAN的一般框架。

### f散度估计 [\#](https://kexue.fm/archives/6016\#f%E6%95%A3%E5%BA%A6%E4%BC%B0%E8%AE%A1)

计算$f$散度有什么困难呢？根据定义$\\eqref{eq:f-div}$，我们同时需要知道两个概率分布$P,Q$才可以计算两者的$f$散度，但事实上在机器学习中很难做到这一点，有时我们最多只知道其中一个概率分布的解析形式，另外一个分布只有采样出来的样本，甚至很多情况下我们两个分布都不知道，只有对应的样本（也就是说要比较两批样本之间的相似性），所以就不能直接根据$\\eqref{eq:f-div}$来计算$f$散度了。

结合$\\eqref{eq:f-div}$和$\\eqref{eq:max-conj}$，我们得到
$$\\begin{equation}\\begin{aligned}\\mathcal{D}\_f(P\\Vert Q) =& \\max\_{T}\\int q(x) \\left\[\\frac{p(x)}{q(x)}T\\left(\\frac{p(x)}{q(x)}\\right)-g\\left(T\\left(\\frac{p(x)}{q(x)}\\right)\\right)\\right\]dx\\\
=& \\max\_{T}\\int\\left\[p(x)\\cdot T\\left(\\frac{p(x)}{q(x)}\\right)-q(x)\\cdot g\\left(T\\left(\\frac{p(x)}{q(x)}\\right)\\right)\\right\]dx\\end{aligned}\\end{equation}$$
将$T\\left(\\frac{p(x)}{q(x)}\\right)$记为整体$T(x)$，那么就有
$$\\begin{equation}\\mathcal{D}\_f(P\\Vert Q) = \\max\_{T}\\Big(\\mathbb{E}\_{x\\sim p(x)}\[T(x)\]-\\mathbb{E}\_{x\\sim q(x)}\[g(T(x))\]\\Big)\\label{eq:f-div-e}\\end{equation}$$
式$\\eqref{eq:f-div-e}$就是估计$f$散度的基础公式了。意思就是说：分别从两个分布中采样，然后分别计算$T(x)$和$g(T(x))$的平均值，优化$T$，让它们的差尽可能地大，最终的结果就是$f$散度的近似值了。显然$T(x)$可以用足够复杂的神经网络拟合，我们只需要优化神经网络的参数。

注意在对凸函数的讨论中，我们在最大化目标的时候，对$T$的值域是有限制的。因此，在$T$的最后一层，我们必须设计适当的激活函数，使得$T$满足要求的值域。当然激活函数的选择不是唯一的，参考的激活函数已经列举在前表。注意，尽管理论上激活函数的选取是任意的，但是为了优化上的容易，应该遵循几个原则：

> 1、对应的定义域为$\\mathbb{R}$，对应的值域为要求值域（边界点可以忽略）；
>
> 2、最好选择全局光滑的函数，不要简单地截断，例如要求值域为$\\mathbb{R}\_+$的话，不要直接用$relu(x)$，可以考虑的是$e^x$或者$\\log(1+e^x)$；
>
> 3、注意式$\\eqref{eq:f-div-e}$的第二项包含了$g(T(x))$，也就是$g$和$T$的复合计算，因此选择激活函数时，最好使得它与$g$的复合运算比较简单。

### GAN批发 [\#](https://kexue.fm/archives/6016\#GAN%E6%89%B9%E5%8F%91)

好了，说了那么久，几乎都已经到文章结尾了，似乎还没有正式说到GAN。事实上，GAN可以算是整篇文章的副产物而已。

GAN希望训练一个生成器，将高斯分布映射到我们所需要的数据集分布，那就需要比较两个分布之间的差异了，经过前面的过程，其实就很简单了，随便找一种$f$散度都可以了。然后用式$\\eqref{eq:f-div-e}$对$f$散度进行估计，估计完之后，我们就有$f$散度的模型了，这时候生成器不是希望缩小分布的差异吗？最小化$f$散度就行了。所以写成一个表达式就是
$$\\begin{equation}\\min\_G\\max\_{T}\\Big(\\mathbb{E}\_{x\\sim p(x)}\[T(x)\]-\\mathbb{E}\_{x=G(z),z\\sim q(z)}\[g(T(x))\]\\Big)\\label{eq:f-div-gan}\\end{equation}$$
或者反过来：
$$\\begin{equation}\\min\_G\\max\_{T}\\Big(\\mathbb{E}\_{x=G(z),z\\sim q(z)}\[T(x)\]-\\mathbb{E}\_{x\\sim p(x)}\[g(T(x))\]\\Big)\\label{eq:f-div-gan-2}\\end{equation}$$
就这样完了～

需要举几个例子？好吧，先用JS散度看看。把所有东西式子一步步代进去，你会发现最终结果是（略去了$\\log 2$的常数项）
$$\\begin{equation}\\min\_G\\max\_{D}\\Big(\\mathbb{E}\_{x\\sim p(x)}\[\\log D(x)\] + \\mathbb{E}\_{x=G(z),z\\sim q(z)}\[\\log(1-D(x))\]\\Big)\\end{equation}$$
其中$D$用$\\sigma(x)=1/(1+e^{-x})$激活。这就是最原始版本的GAN了。

用Hellinger距离试试？结果是
$$\\begin{equation}\\min\_G\\max\_{D}\\Big(-\\mathbb{E}\_{x\\sim p(x)}\[e^{D(x)}\] - \\mathbb{E}\_{x=G(z),z\\sim q(z)}\[e^{-D(x)}\]\\Big)\\end{equation}$$
这里的$D(x)$是线性激活。这个貌似还没有命名？不过论文中已经对它做过实验了。

那用KL散度呢？因为KL散度是不对称的，所以有两个结果，分别为
$$\\begin{equation}\\min\_G\\max\_{D}\\Big(\\mathbb{E}\_{x\\sim p(x)}\[D(x)\] - \\mathbb{E}\_{x=G(z),z\\sim q(z)}\[e^{D(x)-1}\]\\Big)\\end{equation}$$
或
$$\\begin{equation}\\min\_G\\max\_{D}\\Big(\\mathbb{E}\_{x=G(z),z\\sim q(z)}\[D(x)\] - \\mathbb{E}\_{x\\sim p(x)}\[e^{D(x)-1}\]\\Big)\\end{equation}$$
这里的$D(x)$也是线性激活。

好吧，不再举例了。其实这些$f$散度本质上都差不多，看不到效果差别有多大。不过可以注意到，JS散度和Hellinger距离都是对称的、有界的，这是一个非常好的性质，以后我们会用到。

## 总结 [\#](https://kexue.fm/archives/6016\#%E6%80%BB%E7%BB%93)

说白了，本文主要目的还是介绍$f$散度及其局部变分估算而已～所以大部分还是理论文字，GAN只占一小部分。

当然，经过一番折腾，确实可以达到“GAN生产车间”的结果（取决于你有多少种$f$散度），这些新折腾出来的GAN可能并不像我们想象中的GAN，但它们确实在优化$f$散度。不过，以往标准GAN（对应JS散度）有的问题，其实$f$散度照样会有，因此f-GAN这个工作更大的价值在于“统一”，从生成模型的角度，并没有什么突破。

_**转载到请包括本文地址：** [https://kexue.fm/archives/6016](https://kexue.fm/archives/6016)_

_**更详细的转载事宜请参考：**_ [《科学空间FAQ》](https://kexue.fm/archives/6508#%E6%96%87%E7%AB%A0%E5%A6%82%E4%BD%95%E8%BD%AC%E8%BD%BD/%E5%BC%95%E7%94%A8)

**如果您还有什么疑惑或建议，欢迎在下方评论区继续讨论。**

**如果您觉得本文还不错，欢迎 [分享](https://kexue.fm/archives/6016#share)/ [打赏](https://kexue.fm/archives/6016#pay) 本文。打赏并非要从中获得收益，而是希望知道科学空间获得了多少读者的真心关注。当然，如果你无视它，也不会影响你的阅读。再次表示欢迎和感谢！**

打赏

微信打赏

支付宝打赏

因为网站后台对打赏并无记录，因此欢迎在打赏时候备注留言。你还可以 [**点击这里**](http://mail.qq.com/cgi-bin/qm_share?t=qm_mailme&email=tN7d1drY3drrx8H0xcWa19vZ) 或在下方评论区留言来告知你的建议或需求。

**如果您需要引用本文，请参考：**

苏剑林. (Sep. 29, 2018). 《f-GAN简介：GAN模型的生产车间 》\[Blog post\]. Retrieved from [https://kexue.fm/archives/6016](https://kexue.fm/archives/6016)

@online{kexuefm-6016,
        title={f-GAN简介：GAN模型的生产车间},
        author={苏剑林},
        year={2018},
        month={Sep},
        url={\\url{https://kexue.fm/archives/6016}},
}

分类： [数学研究](https://kexue.fm/category/Mathematics), [信息时代](https://kexue.fm/category/Big-Data)    标签： [变分](https://kexue.fm/tag/%E5%8F%98%E5%88%86/), [GAN](https://kexue.fm/tag/GAN/), [推断](https://kexue.fm/tag/%E6%8E%A8%E6%96%AD/), [生成模型](https://kexue.fm/tag/%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B/)[59 评论](https://kexue.fm/archives/6016#comments)

< [细水长flow之f-VAEs：Glow与VAEs的联姻](https://kexue.fm/archives/5977) \| [深度学习的互信息：无监督提取特征](https://kexue.fm/archives/6024) >

### 你也许还对下面的内容感兴趣

- [生成扩散模型漫谈（三十）：从瞬时速度到平均速度](https://kexue.fm/archives/10958)
- [Transformer升级之路：20、MLA究竟好在哪里？](https://kexue.fm/archives/10907)
- [生成扩散模型漫谈（二十九）：用DDPM来离散编码](https://kexue.fm/archives/10711)
- [细水长flow之TARFLOW：流模型满血归来？](https://kexue.fm/archives/10667)
- [生成扩散模型漫谈（二十八）：分步理解一致性模型](https://kexue.fm/archives/10633)
- [生成扩散模型漫谈（二十七）：将步长作为条件输入](https://kexue.fm/archives/10617)
- [生成扩散模型漫谈（二十六）：基于恒等式的蒸馏（下）](https://kexue.fm/archives/10567)
- [VQ的又一技巧：给编码表加一个线性变换](https://kexue.fm/archives/10519)
- [VQ的旋转技巧：梯度直通估计的一般推广](https://kexue.fm/archives/10489)
- [“闭门造车”之多模态思路浅谈（二）：自回归](https://kexue.fm/archives/10197)

[发表你的看法](https://kexue.fm/archives/6016#comment_form)

1. [«](https://kexue.fm/archives/6016/comment-page-1#comments)
2. [1](https://kexue.fm/archives/6016/comment-page-1#comments)
3. [2](https://kexue.fm/archives/6016/comment-page-2#comments)

xht033

November 26th, 2019

您好，我怎么无法推出JS散度对应的f是凸函数？

[回复评论](https://kexue.fm/archives/6016/comment-page-2?replyTo=12465#respond-post-6016)

xht033 发表于
November 26th, 2019

抱歉，请忽视我这条回复。

[回复评论](https://kexue.fm/archives/6016/comment-page-2?replyTo=12466#respond-post-6016)

[LinkerLin](http://bbs.zhipu.us/)

September 3rd, 2020

对散度的追求，让我想到了 压缩感知。
似乎都是在求解 基追踪？

[回复评论](https://kexue.fm/archives/6016/comment-page-2?replyTo=14243#respond-post-6016)

[苏剑林](https://kexue.fm) 发表于
September 4th, 2020

不了解你说的。

[回复评论](https://kexue.fm/archives/6016/comment-page-2?replyTo=14250#respond-post-6016)

tobby

November 16th, 2020

将 $T(\\frac{p(x)}{q(x)})$ 记为整体 $T(x)$ 这里，似乎是放宽了限制。因为$k(x)=\\frac{p(x)}{q(x)}$ 不是一个一一映射的函数。

[回复评论](https://kexue.fm/archives/6016/comment-page-2?replyTo=14807#respond-post-6016)

[苏剑林](https://kexue.fm) 发表于
March 16th, 2021

是这么说，但可以反过来从$(13)$式出发退得原来的f散度，所以就可以证明互为充要条件了。

[回复评论](https://kexue.fm/archives/6016/comment-page-2?replyTo=15790#respond-post-6016)

[zhb](https://github.com/zhb2000) 发表于
February 28th, 2024

这个没有影响吧，既然 $t$ 可以表示成关于 $\\frac{p(x)}{q(x)}$ 的函数，而 $\\frac{p(x)}{q(x)}$ 是关于 $x$ 的函数，那么 $t$ 自然可以写成关于 $x$ 的函数 $T(x)$。

[回复评论](https://kexue.fm/archives/6016/comment-page-2?replyTo=23795#respond-post-6016)

[苏剑林](https://kexue.fm) 发表于
February 29th, 2024

楼上的这个担忧是有道理的，比如$T(x^2)$跟$T(x)$就不等价，因为$x^2$不是$x$的单调函数，但是$T(x^3)$跟$T(x)$是等价的。

[回复评论](https://kexue.fm/archives/6016/comment-page-2?replyTo=23814#respond-post-6016)

Renat 发表于
June 1st, 2024

这里似乎没有问题？可能是记法上应该改成 $T\\left(\\frac{p(x)}{q(x)}\\right) = T'(x)$ 的形式，相当于把 $\\frac{p(x)}{q(x)}$ 也放到神经网络 $T'$ 去拟合，此时应该也有 $T'(\\mathbb D\_{T'})\\subset T(\\mathbb D\_{T})$，所以也不影响激活函数的分析

[回复评论](https://kexue.fm/archives/6016/comment-page-2?replyTo=24467#respond-post-6016)

[苏剑林](https://kexue.fm) 发表于
June 2nd, 2024

如果没有反向推导证明充分性的话，是有问题的。因为记$T\\left(\\frac{p(x)}{q(x)}\\right) = T'(x)$，$T'(x)$往往是带有额外的约束的，我们必须要反过来证明这个约束是可以去掉的，否则如果它应该加上约束，而我们在实现的时候没有加上约束，那么结果必然就不合理。

举个例子，对于WGAN的判别器我们有$\\Vert D\\Vert\_L\\leq 1$的约束，所以必须在训练之前就给$D$加上这个约束（或者相应的惩罚项），什么都不加直接训的话，那就没法训练出有意义的结果了。

[回复评论](https://kexue.fm/archives/6016/comment-page-2?replyTo=24480#respond-post-6016)

Renat 发表于
June 3rd, 2024

反应过来了，就比如 $T'(x)=T(x^2)$，其实暗含 $T'(x)=T'(-x)$ 的约束，但如果不考虑直接对 $T'$ 这个神经网络开训的话，就没有任何保证说 $T'$ 能自然满足这样的约束，那训好的网络就很可能有问题。感谢苏老师~

[回复评论](https://kexue.fm/archives/6016/comment-page-2?replyTo=24484#respond-post-6016)

[苏剑林](https://kexue.fm) 发表于
June 5th, 2024

是的，你这个例子更直观，我也学习了。

CHT

March 16th, 2021

请教苏神：minG maxD 和maxD minG的区别是？

[回复评论](https://kexue.fm/archives/6016/comment-page-2?replyTo=15789#respond-post-6016)

[苏剑林](https://kexue.fm) 发表于
March 16th, 2021

$\\min$和$\\max$的顺序不一样。

[回复评论](https://kexue.fm/archives/6016/comment-page-2?replyTo=15791#respond-post-6016)

CHT 发表于
March 16th, 2021

就是说先最大化保证f的准确性，才能继续最小化。不然就不知道会是什么结果了？不是很懂，请教一下哈！

[回复评论](https://kexue.fm/archives/6016/comment-page-2?replyTo=15797#respond-post-6016)

[苏剑林](https://kexue.fm) 发表于
March 16th, 2021

是，理论上要先完成$\\max$，两者的顺序不能交换。至于为什么交替训练可以，那其实算是很奇妙的巧合（变成一个动力学问题）/

[回复评论](https://kexue.fm/archives/6016/comment-page-2?replyTo=15804#respond-post-6016)

Alisa

February 17th, 2022

苏老师您好，想请教一下，第二个表格中，每个共轭函数对应的激活函数是怎么来的？另外激活函数是不是就是$T(x)$？感谢老师！

[回复评论](https://kexue.fm/archives/6016/comment-page-2?replyTo=18459#respond-post-6016)

[苏剑林](https://kexue.fm) 发表于
February 18th, 2022

1、激活函数取决于$f'(\\mathbb{D})$的范围，即$f'(\\mathbb{D})=Activation(\\mathbb{R})$。

2、$T(x)$是判别器网络，用上述激活函数激活。。

[回复评论](https://kexue.fm/archives/6016/comment-page-2?replyTo=18470#respond-post-6016)

邝宏政

August 29th, 2023

苏老师您好，请问我们在具体实现f-GAN时，例如最原始的GAN即使用JS散度对应代码里的实际的损失函数是交叉熵损失函数（理论上是先确定的交叉熵损失函数再推导出其等价于JS散度）。而现在我们如何根据f-散度族反向推导出代码里具体的损失函数呢？假如我现在要使用f散度族中的KL散度，那代码具体实现时的损失函数是什么呢？

[回复评论](https://kexue.fm/archives/6016/comment-page-2?replyTo=22601#respond-post-6016)

[苏剑林](https://kexue.fm) 发表于
August 31st, 2023

整篇文章都在做你说的事情，式$(14)$和式$(15)$就是答案。

[回复评论](https://kexue.fm/archives/6016/comment-page-2?replyTo=22616#respond-post-6016)

龙行

November 22nd, 2023

苏神，这里不知道能不能这么理解：
所以GAN的鉴别器并没有想着最大化假样本与真样本之间的f散度，而只是平淡的计算了当前生成的假样本与真样本之间的f散度，然后据此散度来优化生成器。之前，我一直认为鉴别器的作用是最大化假样本和真样本之间的距离。直接看原始GAN的损失函数给我这个感觉，但其实原始GAN最大化D只是在逼近JS散度，而不是最大化JS散度。

[回复评论](https://kexue.fm/archives/6016/comment-page-2?replyTo=23146#respond-post-6016)

[苏剑林](https://kexue.fm) 发表于
November 24th, 2023

是的，就是这个意思。之所以造成“最大化假样本和真样本之间的距离”，是因为单独看了单个样本的loss，而没有考虑到对样本求期望的效果。

[回复评论](https://kexue.fm/archives/6016/comment-page-2?replyTo=23162#respond-post-6016)

yangxin

April 1st, 2025

苏神，为何可以假设T(x)和D(x)之间得关系满足激活函数，因为我在你得另外一篇文章看到D(x)关于能量函数得推论，所以自己不太能理解能量函数和T(x)如何建立起这样得一个关系

[回复评论](https://kexue.fm/archives/6016/comment-page-2?replyTo=27299#respond-post-6016)

[苏剑林](https://kexue.fm) 发表于
April 3rd, 2025

是我们用激活函数来实现满足特定值域的$T(x)$，你有其他实现方式也可以。

[回复评论](https://kexue.fm/archives/6016/comment-page-2?replyTo=27315#respond-post-6016)

1. [«](https://kexue.fm/archives/6016/comment-page-1#comments)
2. [1](https://kexue.fm/archives/6016/comment-page-1#comments)
3. [2](https://kexue.fm/archives/6016/comment-page-2#comments)

[取消回复](https://kexue.fm/archives/6016#respond-post-6016)

你的大名

电子邮箱

个人网站（选填）

1\. 可以使用LaTeX代码，点击“预览效果”可查看效果；2. 可以通过点击评论楼层编号来引用该楼层；3. 网站可能会有点卡，如非确认评论失败，请 **不要重复点击提交**。

### 内容速览

[局部变分](https://kexue.fm/archives/6016#%E5%B1%80%E9%83%A8%E5%8F%98%E5%88%86)
[f散度](https://kexue.fm/archives/6016#f%E6%95%A3%E5%BA%A6)
[凸函数](https://kexue.fm/archives/6016#%E5%87%B8%E5%87%BD%E6%95%B0)
[凸共轭](https://kexue.fm/archives/6016#%E5%87%B8%E5%85%B1%E8%BD%AD)
[f-GAN](https://kexue.fm/archives/6016#f-GAN)
[f散度估计](https://kexue.fm/archives/6016#f%E6%95%A3%E5%BA%A6%E4%BC%B0%E8%AE%A1)
[GAN批发](https://kexue.fm/archives/6016#GAN%E6%89%B9%E5%8F%91)
[总结](https://kexue.fm/archives/6016#%E6%80%BB%E7%BB%93)

### 智能搜索

支持整句搜索！网站自动使用 [结巴分词](https://github.com/fxsjy/jieba) 进行分词，并结合ngrams排序算法给出合理的搜索结果。

### 热门标签

[生成模型](https://kexue.fm/tag/%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B/) [attention](https://kexue.fm/tag/attention/) [优化](https://kexue.fm/tag/%E4%BC%98%E5%8C%96/) [语言模型](https://kexue.fm/tag/%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/) [模型](https://kexue.fm/tag/%E6%A8%A1%E5%9E%8B/) [网站](https://kexue.fm/tag/%E7%BD%91%E7%AB%99/) [概率](https://kexue.fm/tag/%E6%A6%82%E7%8E%87/) [梯度](https://kexue.fm/tag/%E6%A2%AF%E5%BA%A6/) [转载](https://kexue.fm/tag/%E8%BD%AC%E8%BD%BD/) [微分方程](https://kexue.fm/tag/%E5%BE%AE%E5%88%86%E6%96%B9%E7%A8%8B/) [矩阵](https://kexue.fm/tag/%E7%9F%A9%E9%98%B5/) [天象](https://kexue.fm/tag/%E5%A4%A9%E8%B1%A1/) [分析](https://kexue.fm/tag/%E5%88%86%E6%9E%90/) [深度学习](https://kexue.fm/tag/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/) [积分](https://kexue.fm/tag/%E7%A7%AF%E5%88%86/) [python](https://kexue.fm/tag/python/) [优化器](https://kexue.fm/tag/%E4%BC%98%E5%8C%96%E5%99%A8/) [力学](https://kexue.fm/tag/%E5%8A%9B%E5%AD%A6/) [无监督](https://kexue.fm/tag/%E6%97%A0%E7%9B%91%E7%9D%A3/) [扩散](https://kexue.fm/tag/%E6%89%A9%E6%95%A3/) [几何](https://kexue.fm/tag/%E5%87%A0%E4%BD%95/) [节日](https://kexue.fm/tag/%E8%8A%82%E6%97%A5/) [生活](https://kexue.fm/tag/%E7%94%9F%E6%B4%BB/) [文本生成](https://kexue.fm/tag/%E6%96%87%E6%9C%AC%E7%94%9F%E6%88%90/) [数论](https://kexue.fm/tag/%E6%95%B0%E8%AE%BA/)

### 随机文章

- [Muon续集：为什么我们选择尝试Muon？](https://kexue.fm/archives/10739)
- [文本情感分类（二）：深度学习模型](https://kexue.fm/archives/3414)
- [胡闹的胜利：将算子引入级数求和](https://kexue.fm/archives/3320)
- [为什么现在的LLM都是Decoder-only的架构？](https://kexue.fm/archives/9529)
- [基于CNN和VAE的作诗机器人：随机成诗](https://kexue.fm/archives/5332)
- [时空之章：将Attention视为平方复杂度的RNN](https://kexue.fm/archives/10017)
- [多任务学习漫谈（三）：分主次之序](https://kexue.fm/archives/8907)
- [2009年目视流星雨星历表](https://kexue.fm/archives/87)
- [基于双向LSTM和迁移学习的seq2seq核心实体识别](https://kexue.fm/archives/3942)
- [浅谈神经网络中激活函数的设计](https://kexue.fm/archives/4647)

### 最近评论

- [rpsun](https://kexue.fm/archives/10592/comment-page-2#comment-27912): 老师您好，最近在自己的任务上尝试了muon，甚至只修改了学习率，同时不加区分地对所有二维以上的...
- [盏一](https://kexue.fm/archives/8265/comment-page-8#comment-27911): 我之前做的笔记:Q: 公式 (14) 的理解.A: 首先基于 \[定理 5\](https://b...
- [盏一](https://kexue.fm/archives/8397/comment-page-3#comment-27910): 哦哦哦 你是说 $\\exp nB$ 是正交矩阵! 并不是说 B.
- [盏一](https://kexue.fm/archives/8397/comment-page-3#comment-27908): 呃, 是我脑子乱了... 忘了 $\\exp(0) = I$. 所以只要 $\\Vert B^T+...
- [盏一](https://kexue.fm/archives/8397/comment-page-3#comment-27907): 苏神, 请教一下\> 并且还可以证明它一定是正交矩阵是怎么证明的. 我本来以为隐式利用了 $\\V...
- [sk](https://kexue.fm/archives/8265/comment-page-8#comment-27906): 请问公式14是怎么得出来的？
- [tll1945tll1937](https://kexue.fm/archives/10266/comment-page-1#comment-27901): 真心实意的向大家请教问题：看了文章“对齐全量微调！这是我看过最精彩的LoRA改进（二）”，我实...
- [oYo\_logan](https://kexue.fm/archives/10757/comment-page-1#comment-27897): \[comment=27017\]苏剑林\[/comment\]苏神，想请教一下，我理解在一个batc...
- [z123](https://kexue.fm/archives/10592/comment-page-1#comment-27896): 在参数矩阵较多的CNN小模型上，Muon会明显慢于Adam，这方面有什么优化提速的方案吗？
- [dry](https://kexue.fm/archives/10958/comment-page-2#comment-27895): 苏神好，一直有个疑问，ReFlow构建的ODE是$dx\_t/dt=x\_1-x\_0$，为什么这并...

### 友情链接

- [Cool Papers](https://papers.cool)
- [数学研发](https://bbs.emath.ac.cn)
- [Seatop](http://www.seatop.com.cn/)
- [Xiaoxia](https://xiaoxia.org/)
- [积分表-网络版](https://kexue.fm/sci/integral/index.html)
- [丝路博傲](http://blog.dvxj.com/)
- [ph4ntasy 饭特稀](http://www.ph4ntasy.com/)
- [数学之家](http://www.2math.cn/)
- [有趣天文奇观](http://interesting-sky.china-vo.org/)
- [TwistedW](http://www.twistedwg.com/)
- [godweiyang](https://godweiyang.com/)
- [AI柠檬](https://blog.ailemon.net/)
- [王登科-DK博客](https://greatdk.com)
- [ESON](https://blog.eson.org/)
- [枫之羽](https://fzhiy.net/)
- [Mathor's blog](https://wmathor.com/)
- [coding-zuo](https://coding-zuo.github.io/)
- [博科园](https://www.bokeyuan.net/)
- [孔皮皮的博客](https://www.kppkkp.top/)
- [运鹏的博客](https://yunpengtai.top/)
- [jiming.site](https://jiming.site/)
- [OmegaXYZ](https://www.omegaxyz.com/)
- [Blog by Eacls](https://www.eacls.top/)
- [EAI猩球](https://www.robotech.ink/)
- [文举的博客](https://liwenju0.com/)
- [用代码打点酱油](https://bruceyuan.com/)
- [申请链接](https://kexue.fm/links.html)

本站采用创作共用版权协议，要求署名、非商业用途和保持一致。转载本站内容必须也遵循“ [署名-非商业用途-保持一致](http://creativecommons.org/licenses/by-nc-nd/2.5/cn/)”的创作共用协议。
© 2009-2025 Scientific Spaces. All rights reserved. Theme by [laogui](http://www.laogui.com). Powered by [Typecho](http://typecho.org). 备案号: [粤ICP备09093259号-1/2](https://beian.miit.gov.cn/)。