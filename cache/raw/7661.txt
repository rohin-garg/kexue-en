## SEARCH

## MENU

- [打赏](https://kexue.fm/reward.html)
- [公式](https://kexue.fm/latex.html)
- [天象](https://kexue.fm/ac.html)
- [链接](https://kexue.fm/links.html)
- [时光](https://kexue.fm/me.html)
- [博览](https://kexue.fm/science.html)
- [归档](https://kexue.fm/content.html)

## CATEGORIES

- [千奇百怪](https://kexue.fm/category/Everything)
- [天文探索](https://kexue.fm/category/Astronomy)
- [数学研究](https://kexue.fm/category/Mathematics)
- [物理化学](https://kexue.fm/category/Phy-chem)
- [信息时代](https://kexue.fm/category/Big-Data)
- [生物自然](https://kexue.fm/category/Biology)
- [图片摄影](https://kexue.fm/category/Photograph)
- [问题百科](https://kexue.fm/category/Questions)
- [生活/情感](https://kexue.fm/category/Life-Feeling)
- [资源共享](https://kexue.fm/category/Resources)

## NEWPOSTS

- [MuP之上：1. 好模型的三个特征](https://kexue.fm/archives/11340)
- [随机矩阵的谱范数的快速估计](https://kexue.fm/archives/11335)
- [DiVeQ：一种非常简洁的VQ训练方案](https://kexue.fm/archives/11328)
- [为什么线性注意力要加Short C...](https://kexue.fm/archives/11320)
- [AdamW的Weight RMS的...](https://kexue.fm/archives/11307)
- [重新思考学习率与Batch Siz...](https://kexue.fm/archives/11301)
- [重新思考学习率与Batch Siz...](https://kexue.fm/archives/11285)
- [重新思考学习率与Batch Siz...](https://kexue.fm/archives/11280)
- [为什么Adam的Update RM...](https://kexue.fm/archives/11267)
- [重新思考学习率与Batch Siz...](https://kexue.fm/archives/11260)

## COMMENTS

- [Zhan-Wang Mao: 苏老师，请教一下(4)式的泰勒展开式为什么严格来说和$t$有关...](https://kexue.fm/archives/9257/comment-page-4#comment-28676)
- [yzlnew: 可以相呼应的是，这样的好模型能被浮点数以误差比较低的方式表示和...](https://kexue.fm/archives/11340/comment-page-1#comment-28675)
- [Henry: 想请问苏老师，方程7是如何推导到方程10的，是否有化简的一些小技巧？](https://kexue.fm/archives/9181/comment-page-5#comment-28673)
- [szsheep: 牛啊，还可以从这方面推出loss的函数最终式。原本是从KL散度...](https://kexue.fm/archives/9119/comment-page-13#comment-28672)
- [pang: 对于目前的MLA算法softmax(X×WQ×WukT×CjT...](https://kexue.fm/archives/10862/comment-page-1#comment-28671)
- [苏剑林: 你是说 chatglm2-6b 里边的？那个没用，预设的常数是...](https://kexue.fm/archives/11126/comment-page-3#comment-28670)
- [苏剑林: 已发](https://kexue.fm/archives/443/comment-page-1#comment-28669)
- [苏剑林: 只想要“方向”，步长只是个标量，另外手工控制。](https://kexue.fm/archives/10592/comment-page-2#comment-28668)
- [苏剑林: 参考 https://kexue.fm/archives/10795](https://kexue.fm/archives/11241/comment-page-1#comment-28667)
- [苏剑林: $KV$本身通常就不是low-rank的，所以就无所谓升不升秩...](https://kexue.fm/archives/7546/comment-page-4#comment-28666)

## USERLOGIN

- [登录](https://kexue.fm/admin/login.php)

[科学空间\|Scientific Spaces](https://kexue.fm)

- [登录](https://kexue.fm/admin/login.php)
- [打赏](https://kexue.fm/reward.html)
- [公式](https://kexue.fm/latex.html)
- [天象](https://kexue.fm/ac.html)
- [链接](https://kexue.fm/links.html)
- [时光](https://kexue.fm/me.html)
- [博览](https://kexue.fm/science.html)
- [归档](https://kexue.fm/content.html)

渴望成为一个小飞侠

- [欢迎订阅](https://kexue.fm/feed)
- [个性邮箱](https://kexue.fm/archives/119)
- [天象信息](https://kexue.fm/ac.html)
- [观测ISS](https://kexue.fm/archives/41)
- [LaTeX](https://kexue.fm/latex.html)
- [关于博主](https://kexue.fm/me.html)

欢迎访问“科学空间”，这里将与您共同探讨自然科学，回味人生百态；也期待大家的分享～

- [**千奇百怪** Everything](https://kexue.fm/category/Everything)
- [**天文探索** Astronomy](https://kexue.fm/category/Astronomy)
- [**数学研究** Mathematics](https://kexue.fm/category/Mathematics)
- [**物理化学** Phy-chem](https://kexue.fm/category/Phy-chem)
- [**信息时代** Big-Data](https://kexue.fm/category/Big-Data)
- [**生物自然** Biology](https://kexue.fm/category/Biology)
- [**图片摄影** Photograph](https://kexue.fm/category/Photograph)
- [**问题百科** Questions](https://kexue.fm/category/Questions)
- [**生活/情感** Life-Feeling](https://kexue.fm/category/Life-Feeling)
- [**资源共享** Resources](https://kexue.fm/category/Resources)

- [**千奇百怪**](https://kexue.fm/category/Everything)
- [**天文探索**](https://kexue.fm/category/Astronomy)
- [**数学研究**](https://kexue.fm/category/Mathematics)
- [**物理化学**](https://kexue.fm/category/Phy-chem)
- [**信息时代**](https://kexue.fm/category/Big-Data)
- [**生物自然**](https://kexue.fm/category/Biology)
- [**图片摄影**](https://kexue.fm/category/Photograph)
- [**问题百科**](https://kexue.fm/category/Questions)
- [**生活/情感**](https://kexue.fm/category/Life-Feeling)
- [**资源共享**](https://kexue.fm/category/Resources)

[首页](https://kexue.fm) [信息时代](https://kexue.fm/category/Big-Data) 修改Transformer结构，设计一个更快更好的MLM模型

7Aug

# [修改Transformer结构，设计一个更快更好的MLM模型](https://kexue.fm/archives/7661)

By 苏剑林 \|
2020-08-07 \|
75102位读者\|

大家都知道，MLM（Masked Language Model）是BERT、RoBERTa的预训练方式，顾名思义，就是mask掉原始序列的一些token，然后让模型去预测这些被mask掉的token。随着研究的深入，大家发现MLM不单单可以作为预训练方式，还能有很丰富的应用价值，比如笔者之前就发现直接加载BERT的MLM权重就可以当作UniLM来做Seq2Seq任务（参考 [这里](https://kexue.fm/archives/6933)），又比如发表在ACL 2020的 [《Spelling Error Correction with Soft-Masked BERT》](https://papers.cool/arxiv/2005.07421) 将MLM模型用于文本纠错。

然而，仔细读过BERT的论文或者亲自尝试过的读者应该都知道，原始的MLM的训练效率是比较低的，因为每次只能mask掉一小部分的token来训练。ACL 2020的论文 [《Fast and Accurate Deep Bidirectional Language Representations for Unsupervised Learning》](https://papers.cool/arxiv/2004.08097) 也思考了这个问题，并且提出了一种新的MLM模型设计，能够有更高的训练效率和更好的效果。

## MLM模型 [\#](https://kexue.fm/kexue.fm\#MLM%E6%A8%A1%E5%9E%8B)

假设原始序列为$\\boldsymbol{x}=\[x\_1,x\_2,\\dots,x\_T\]$，$\\boldsymbol{x}\\backslash \\{x\_i\\}$表示将第i个token替换为$\\text{\[MASK\]}$后的序列，那么MLM模型就是建模
\\begin{equation}p\\big(x\_i, x\_j, x\_k, \\cdots\\big\|\\,\\boldsymbol{x}\\backslash \\{x\_i,x\_j,x\_k,\\cdots\\}\\big)\\end{equation}
我们说它效率低，是因为每次只能选择一小部分token来mask，比如15%，那么也就是说每个样本只有15%的token被训练到了，所以同一个样本需要反复训练多次。在BERT里边，每个样本都被mask了多遍然后存为tfrecord，训练效率低的同时还增加了硬盘空间占用。

MLM任务示意图

如果训练的时候每个样本的所有token都可以作为预测目标，那么训练效率自然就能提升了。像GPT这样的单向语言模型是可以做到的，但是MLM是双向的模型，并不能直接做到这一点。为了达到这个目标，我们需要简化一下上式，假设每次只mask掉一个token，也就是要构建的分布为
\\begin{equation}p\\big(x\_i\\big\|\\,\\boldsymbol{x}\\backslash \\{x\_i\\}\\big),\\,i=1,2,\\dots,T\\end{equation}
然后我们希望通过单个模型一次预测就同时得到$p(x\_1\|\\,\\boldsymbol{x}\\backslash \\{x\_1\\}),p(x\_2\|\\,\\boldsymbol{x}\\backslash \\{x\_2\\}),\\dots,p(x\_T\|\\,\\boldsymbol{x}\\backslash \\{x\_T\\})$。怎么做到这一点呢？这就来到本文要介绍的论文结果了，它提出了一种称之为T-TA（Transformer-based Text Autoencoder）的设计，能让我们一并预测所有token的分布。

## T-TA介绍 [\#](https://kexue.fm/kexue.fm\#T-TA%E4%BB%8B%E7%BB%8D)

T-TA的Attention Mask模式

首先，我们知道Transformer的核心运算是$Attention(\\boldsymbol{Q},\\boldsymbol{K},\\boldsymbol{V})$，在BERT里边$\\boldsymbol{Q},\\boldsymbol{K},\\boldsymbol{V}$都是同一个，也就是Self Attention。而在MLM中，我们既然要建模$p(x\_i\|\\,\\boldsymbol{x}\\backslash \\{x\_i\\})$，那么第$i$个输出肯定是不能包含第$i$个token的信息的，为此，第一步要做出的改动是：去掉$\\boldsymbol{Q}$里边的token输入，也就是说第一层的Attention的$\\boldsymbol{Q}$不能包含token信息，只能包含位置向量。这是因为我们是通过$\\boldsymbol{Q}$把$\\boldsymbol{K},\\boldsymbol{V}$的信息聚合起来的，如果$\\boldsymbol{Q}$本身就有token信息，那么就会造成信息泄漏了。然后，我们要防止$\\boldsymbol{K},\\boldsymbol{V}$的信息泄漏，这需要修改Attention Mask，把对角线部分的Attention（也就是自身的）给Mask掉，如图所示。

如果还不理解这一点，我们可以从Attention的一般形式来理解：Attention的一般定义为
\\begin{equation}Attention(\\boldsymbol{Q},\\boldsymbol{K},\\boldsymbol{V})\_i = \\frac{\\sum\\limits\_{j=1}^n \\text{sim}(\\boldsymbol{q}\_i, \\boldsymbol{k}\_j)\\boldsymbol{v}\_j}{\\sum\\limits\_{j=1}^n \\text{sim}(\\boldsymbol{q}\_i, \\boldsymbol{k}\_j)}\\label{eq:gen-att}\\end{equation}
所以很明显，$Attention(\\boldsymbol{Q},\\boldsymbol{K},\\boldsymbol{V})\_i$一定跟$\\boldsymbol{q}\_i$有联系，所以$\\boldsymbol{q}\_i$绝对不能包含第$i$个token的信息；但它不一定跟$\\boldsymbol{k}\_i,\\boldsymbol{v}\_i$有联系，因为只需要当$\\text{sim}(\\boldsymbol{q}\_i, \\boldsymbol{k}\_i)=0$时$\\boldsymbol{k}\_i,\\boldsymbol{v}\_i$就相当于不存在了，因此需要Mask掉对角线部分的Attention。

但是，这种防泄漏的Attention Mask只能维持一层！也就是说即便这样做之后，$Attention(\\boldsymbol{Q},\\boldsymbol{K},\\boldsymbol{V})\_j$已经融入了第$i$个token的信息了，所以从第二层开始，如果你还是以第一层的输出为$\\boldsymbol{K},\\boldsymbol{V}$，即便配合了上述Attention Mask，也会出现信息泄漏了。

原论文的解决很粗暴，但貌似也只能这样解决了：每一层Attention都共用原始输入为$\\boldsymbol{K},\\boldsymbol{V}$！所以，设$\\boldsymbol{E}$为token的embedding序列，$\\boldsymbol{P}$为对应的位置向量，那么T-TA与BERT的计算过程可以简写为：
\\begin{equation}
\\begin{array}{c}\\bbox\[border: 1px dashed red; padding: 5px\]{\\begin{aligned}&\\boldsymbol{Q}\_0 = \\boldsymbol{E}+\\boldsymbol{P}\\\
&\\boldsymbol{Q}\_1 = Attention(\\boldsymbol{Q}\_0,\\boldsymbol{Q}\_0,\\boldsymbol{Q}\_0)
\\\
&\\boldsymbol{Q}\_2 = Attention(\\boldsymbol{Q}\_1,\\boldsymbol{Q}\_1,\\boldsymbol{Q}\_1)
\\\
&\\qquad\\vdots\\\
&\\boldsymbol{Q}\_n = Attention(\\boldsymbol{Q}\_{n-1},\\boldsymbol{Q}\_{n-1},\\boldsymbol{Q}\_{n-1})
\\end{aligned}} \\\ \\text{BERT运算示意图}\\quad\\end{array}\\qquad
\\begin{array}{c}\\bbox\[border: 1px dashed red; padding: 5px\]{\\begin{aligned}&\\boldsymbol{Q}\_0 = \\boldsymbol{P}\\\
&\\boldsymbol{Q}\_1 = Attention(\\boldsymbol{Q}\_0,\\boldsymbol{E}+\\boldsymbol{P},\\boldsymbol{E}+\\boldsymbol{P})
\\\
&\\boldsymbol{Q}\_2 = Attention(\\boldsymbol{Q}\_1,\\boldsymbol{E}+\\boldsymbol{P},\\boldsymbol{E}+\\boldsymbol{P})
\\\
&\\qquad\\vdots\\\
&\\boldsymbol{Q}\_n = Attention(\\boldsymbol{Q}\_{n-1},\\boldsymbol{E}+\\boldsymbol{P},\\boldsymbol{E}+\\boldsymbol{P})
\\end{aligned}} \\\ \\text{T-TA运算示意图}\\quad\\end{array}\\end{equation}
当然残差、FFN等细节已经省略掉了，只保留了核心运算部分，预训练阶段T-TA的Attention是进行了对角线形式的Attention Mask的，如果是下游任务的微调，则可以把它去掉。

## 实验结果 [\#](https://kexue.fm/kexue.fm\#%E5%AE%9E%E9%AA%8C%E7%BB%93%E6%9E%9C)

原论文的实验表格之一。可以看到T-TA在语义表达方面有它的独特优势。

基于上述设计，T-TA它能一次性预测所有的token，所以训练效率高，并且不需要额外的$\\text{\[MASK\]}$符号，所以实现了预训练和微调之间的一致性。但是不难理解，T-TA实则是对标准Transformer的一种简化，所以理论上它的拟合能力是变弱了。这样一收一放之下，具体表现还有没有提升呢？当然，论文的实验结果是有的。原论文做了多个实验，结果显示T-TA这种设计在同样的参数情况下基本都能媲美甚至超过标准的MLM训练出来的模型。作者还很慷慨地开源了代码，以便大家复现结果（ [链接](https://github.com/joongbo/tta)）。

说到修改Transformer结构，大家可能联想到大量的GPU、TPU在并行运算。但事实上，虽然作者没有具体列出自己的实验设备，但从论文可以看到设备阵容应该不算“豪华”。为此，作者只训练了3层的T-TA，并且按照同样的模式复现了3层的MLM和GPT（也就是单向语言模型），然后对比了效果。没错，论文中所有T-TA的结果都只是3层的模型，而其中有些都超过了Base版本的BERT。所以作者生动地给我们上了一课：没有土豪的设备，也可以做修改Transformer的工作，也可以发ACL，关键是你有真正有效的idea。

## 个人分析 [\#](https://kexue.fm/kexue.fm\#%E4%B8%AA%E4%BA%BA%E5%88%86%E6%9E%90)

最后，再来简单谈谈T-TA为什么有效。读者可能会质疑，既然作者只做了3层的实验，那么如何保证在更多层的时候也能有效呢？那好，我们来从另外一个角度看这个模型。

从设计上看，对于T-TA来说，当输入给定后，$\\boldsymbol{K},\\boldsymbol{V}$在所有Attention层中的保持不变，变化的只有$\\boldsymbol{Q}$，所以读者质疑它效果也不意外。但是别忘了，前段时候Google才提出了个Synthesizer（参考 [《Google新作Synthesizer：我们还不够了解自注意力》](https://kexue.fm/archives/7430)），里边探索了几种Attention变种，其中一种简称为“R”的，相当于$\\boldsymbol{Q},\\boldsymbol{K}$固定为常数，结果居然也能work得不错！要注意，“R”里边的$\\boldsymbol{Q},\\boldsymbol{K}$是彻彻底底的常数，跟输入都没关系。

所以，既然$\\boldsymbol{Q},\\boldsymbol{K}$为常数效果都还可以，那么$\\boldsymbol{K},\\boldsymbol{V}$为什么不能为常数呢？更何况T-TA的$\\boldsymbol{K},\\boldsymbol{V}$动态依赖于输入的，只是输入确定后它才算是常数，因此理论上来讲T-TA的拟合能力比Synthesizer的“R”模型要强，既然“R”都能好了，T-TA能好应该也是不奇怪。

当然，还是期望后续会有更深的实验结果出现。

_**转载到请包括本文地址：** [https://kexue.fm/archives/7661](https://kexue.fm/archives/7661)_

_**更详细的转载事宜请参考：**_ [《科学空间FAQ》](https://kexue.fm/archives/6508#%E6%96%87%E7%AB%A0%E5%A6%82%E4%BD%95%E8%BD%AC%E8%BD%BD/%E5%BC%95%E7%94%A8)

**如果您还有什么疑惑或建议，欢迎在下方评论区继续讨论。**

**如果您觉得本文还不错，欢迎 [分享](https://kexue.fm/kexue.fm#share)/ [打赏](https://kexue.fm/kexue.fm#pay) 本文。打赏并非要从中获得收益，而是希望知道科学空间获得了多少读者的真心关注。当然，如果你无视它，也不会影响你的阅读。再次表示欢迎和感谢！**

打赏

微信打赏

支付宝打赏

因为网站后台对打赏并无记录，因此欢迎在打赏时候备注留言。你还可以 [**点击这里**](http://mail.qq.com/cgi-bin/qm_share?t=qm_mailme&email=tN7d1drY3drrx8H0xcWa19vZ) 或在下方评论区留言来告知你的建议或需求。

**如果您需要引用本文，请参考：**

苏剑林. (Aug. 07, 2020). 《修改Transformer结构，设计一个更快更好的MLM模型 》\[Blog post\]. Retrieved from [https://kexue.fm/archives/7661](https://kexue.fm/archives/7661)

@online{kexuefm-7661,
        title={修改Transformer结构，设计一个更快更好的MLM模型},
        author={苏剑林},
        year={2020},
        month={Aug},
        url={\\url{https://kexue.fm/archives/7661}},
}

分类： [信息时代](https://kexue.fm/category/Big-Data)    标签： [语言模型](https://kexue.fm/tag/%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/), [attention](https://kexue.fm/tag/attention/)[18 评论](https://kexue.fm/archives/7661#comments)

< [我们真的需要把训练集的损失降低到零吗？](https://kexue.fm/archives/7643) \| [L2正则没有想象那么好？可能是“权重尺度偏移”惹的祸](https://kexue.fm/archives/7681) >

### 你也许还对下面的内容感兴趣

- [为什么线性注意力要加Short Conv？](https://kexue.fm/archives/11320)
- [QK-Clip：让Muon在Scaleup之路上更进一步](https://kexue.fm/archives/11126)
- [Transformer升级之路：21、MLA好在哪里?（下）](https://kexue.fm/archives/11111)
- [“对角+低秩”三角阵的高效求逆方法](https://kexue.fm/archives/11072)
- [线性注意力简史：从模仿、创新到反哺](https://kexue.fm/archives/11033)
- [Transformer升级之路：20、MLA好在哪里?（上）](https://kexue.fm/archives/10907)
- [Transformer升级之路：19、第二类旋转位置编码](https://kexue.fm/archives/10862)
- [细水长flow之TARFLOW：流模型满血归来？](https://kexue.fm/archives/10667)
- [“闭门造车”之多模态思路浅谈（三）：位置编码](https://kexue.fm/archives/10352)
- [Decoder-only的LLM为什么需要位置编码？](https://kexue.fm/archives/10347)

[发表你的看法](https://kexue.fm/kexue.fm#comment_form)

M31

August 13th, 2020

使用XLNet那种two stream attention的形式，一个attention只有位置信息，另一个包含了上下文，使用只包含位置信息的attention来预测各个位置的token。这样行得通吗？（由于位置attention和上下文的attention因为存在交互，在多层网络的情况下同样会造成KV信息泄漏的问题？）

[回复评论](https://kexue.fm/archives/7661/comment-page-1?replyTo=14055#respond-post-7661)

ykwang 发表于
August 13th, 2020

xlnet是不会泄露的，它每个位置能看多少信息是提前算好的，所以不会看到不应该看的

[回复评论](https://kexue.fm/archives/7661/comment-page-1?replyTo=14057#respond-post-7661)

[苏剑林](https://kexue.fm) 发表于
August 14th, 2020

作为语言模型来说没问题，但作为mlm来说还是不够的，还是会泄漏，原因文本已经说了。

[回复评论](https://kexue.fm/archives/7661/comment-page-1?replyTo=14066#respond-post-7661)

ykwang

August 13th, 2020

xlnet的做法不就是正确解决了mask导致训练效率低的问题么。。。xlnet是有mask的，双流就是一个带自己，一个不带自己信。那这篇论文的意义在哪呢？

[回复评论](https://kexue.fm/archives/7661/comment-page-1?replyTo=14056#respond-post-7661)

[苏剑林](https://kexue.fm) 发表于
August 14th, 2020

xlnet只是一个单向语言模型，虽然它的顺序可以随机，但选定顺序后它就是单向的。而这篇文章要的就是mlm。其次，transformer-xl太丑，所以xlnet也没什么意思，不妨碍新的研究。

“那这篇论文的意义在哪呢？”，我很疑惑怎么会提出这么奇怪的问题，哪怕T-TA的效果跟xlnet一样，甚至退一步说就算T-TA效果还不如xlnet，那么它就没有存在意义了么？新的思想本就应该好好学习，何况这两者又不是互为替代品，没有取代之说。

[回复评论](https://kexue.fm/archives/7661/comment-page-1?replyTo=14065#respond-post-7661)

xxxw

September 10th, 2020

可以"R"里边的Q,K虽然是常数 , 但是却是可以训练的 , 跟这里的只变化Q , 而K,V一直不变 , 还是有区别的吧

[回复评论](https://kexue.fm/archives/7661/comment-page-1?replyTo=14294#respond-post-7661)

[苏剑林](https://kexue.fm) 发表于
September 10th, 2020

K,V是输入的Embedding序列，Embedding层是可训练的。

此外，“R”里边的Q,K虽然是可训练的，但是训练完成之后，所有的输入公用一个Q,K；T-TA里边的K,V则是动态依赖于输入（只是交互程度没那么深）。从这个角度来看，其实T-TA的拟合能力应当要比R强。

[回复评论](https://kexue.fm/archives/7661/comment-page-1?replyTo=14295#respond-post-7661)

liukoulong

September 25th, 2020

可以看看icml的这篇paper，同样的想法，用来做autoregressive的翻译都不掉点的https://arxiv.org/abs/2001.05136

[回复评论](https://kexue.fm/archives/7661/comment-page-1?replyTo=14421#respond-post-7661)

[苏剑林](https://kexue.fm) 发表于
September 26th, 2020

谢谢推荐。当时略微看过这篇文章，也看过香侬科技的解读，不过最终还是没提起兴趣来。

[回复评论](https://kexue.fm/archives/7661/comment-page-1?replyTo=14434#respond-post-7661)

[运用 BERT 的 MLM 模型进行小样本学习 R11; chenpaopao](http://139.9.1.231/index.php/2022/10/30/bert-mlm/)

October 30th, 2022

\[...\]然而，随着研究的深入，研究人员发现不止 BERT 的 Encoder 很有用，预训练用的 MLM 本身也很有用。比如论文《BERT has a Mouth, and It Must Speak: BERT as a Markov Random Field Language Model》指出 MLM 可以作为一般的生成模型用，论文《Spelling Error Correction with Sof\[...\]

[回复评论](https://kexue.fm/archives/7661/comment-page-1?replyTo=20209#respond-post-7661)

Allen7575

January 21st, 2024

想請教一下，你在復現實驗的時候，是同一個樣本，每次隨機 Mask ，反覆訓練嗎？還是一個樣本從一開始 MASK 給定之後就不再動了？

我最近嘗試復現，發現這個問題：如果每次都隨機MASK同一句話，反覆訓練多次，那幾個epoch後，模型是不是就相當於看過那整句話，比方說只要看到某個符號出現在某個位置上，就能推測MASK的字是甚麼，而無須真的理解字義？這樣就變成像是overfitting了。

似乎比較正確的做法是，在 preprocessing 的時候隨機 MASK ，後面就不再改變了。這樣一來，不論反覆訓練幾次，都不會因此拼湊出整個句子，不會在其他epoch裡面看到同一個句子MASK其他字。還是其實沒有差？

[回复评论](https://kexue.fm/archives/7661/comment-page-1?replyTo=23535#respond-post-7661)

Allen7575 发表于
January 22nd, 2024

我目前的評估方法是拿沒有被 Mask 的 token 來當作 testing set，而有 Mask 的token 就是training set，看testing accuracy 有沒有成長。但如果同一句話套用不同MASK重複使用多次的話，就相當於我的 testing set 混入了training set的資料。如此一來，沒有被mask的token 預測準確率就失去意義了。在inference的時候，我們是拿一句沒看過的句子輸入，希望沒有MASK的地方應該也要能夠正確還原，才能代表該句子。所以訓練時，用沒有 mask 的部分當作 testing 應該是符合 inference 的情況的。

或者應該是這樣：一開始選出的15% 就是training set，剩下的85%是testing set。然後在training set中，80% 被mask、10%隨機、10%不變，但是在這training set裡面，mask、隨機、不變的token 是可以每次都不同的，這樣一來，剩下的85% testing set 就不會混入training set的答案，而可以當作評估的標準。

不曉得你們實際上是不是這樣操作？

[回复评论](https://kexue.fm/archives/7661/comment-page-1?replyTo=23540#respond-post-7661)

Allen7575 发表于
January 28th, 2024

另外，我發現80% mask是有道理的。主要是因為Transformer 中 multihead attention 輸出有加上 input token embedding 的殘差連結。當 MASK 比例太小時，模型可以讓 attention 輸出同一個值，直接使用input token embedding 來做預測，等於沒有利用到上下文 attention 的資訊。所以只能夠透過加大MASK比例，來強迫模型透過 attention head 學習上下文資訊。

但是我在小資料上，MASK太大就幾乎沒東西可學了。若拿掉 input token embedding 的殘差連結，就能讓模型完全透過attention 學習，但是拿掉殘差連結後，又沒辦法訓練深層。此時，我發現RealFormer [https://kexue.fm/archives/8027](https://kexue.fm/archives/8027) 這篇提供了另一條路徑，將殘差加在attention score上。

但是就像我在留言中 [https://kexue.fm/archives/8027/comment-page-2#comment-23539](https://kexue.fm/archives/8027/comment-page-2#comment-23539) 說的，這種殘差有可能會退化成只使用第一層。除非在score中加上一些random noise，才可讓每一層學到不太一樣的score。

然而，實驗結果顯示，就算加了 random noise 或 dropout，將殘差加在attention score上的這種方法，每一層的attention 還是會趨同，這就違背了我想看每一層不同attention 變化的初衷。

最近想到，也許可以藉由變動 MASK ratio (0.4~0.8) 來達到避免模型直接使用 input token embedding 的殘差連結。當MASK ratio=0.4的時候，模型幾乎可以使用殘差連結直接預測，而不透過attention；但當MASK ratio=0.8時，模型沒辦法只透過殘差連結來預測答案，此時就需要來自 attention 的資訊。隨機的變動MASK ratio，也許可以讓模型學會更多使用attention 的資訊。

目前在測試這種方法能否學出比較有意義的attention。

[回复评论](https://kexue.fm/archives/7661/comment-page-1?replyTo=23618#respond-post-7661)

Allen7575 发表于
January 29th, 2024

實驗結果，變動 MASK ratio 還是不能避免 attention 對每個字輸出一樣的值。

後來我又想到，既然我希望對每個字不同，那就是要增加 attention weights 的diversity! 定義 diversity 為 attention weights 的 std，計算 \[query1,query2,query3,...\] 對 value1 的std，即 std(\[attn11,attn21,attn31,...\])=div1, 對 value2 的std，即std(\[attn12,attn22,attn32,...\])=div2,...以此類推。然後在loss中加入平均div即可。這是在softmax的情況。

但是我是用sigmoid，每個attention weight 是獨立的0~1分布，沒有沿著序列方向的歸一化。因此，有可能出現 \[1111100000\] 這種std值很高，但對每個 value 都是 \[1111100000\] 這種分布。因此，我希望增加 value 這個方向的std 的 diversity，也就是std的std。第一個std是沿query方向，算出來後再延value方向算一次std，然後取平均，加到loss裡。

實驗結果顯示，確實能夠避免每個字都輸出相同的attention weights。這個方法應該也可以用來鼓勵 RealFormer 中的每次層輸出不同的 attention weights，只是手動調整loss 的比例確實有些麻煩...。

[回复评论](https://kexue.fm/archives/7661/comment-page-1?replyTo=23622#respond-post-7661)

Allen7575 发表于
January 29th, 2024

我發現在loss中加入attention weights分別沿著兩個方向的 std of stds 非常關鍵！他可以鼓勵 model 產生每個 token 不同的 attention weight，從而讓model必須利用到 attention 而非只用到殘差連線的資訊。這給小資料預訓練減少使用MASK開啟了一條路！

不同於你在 [https://kexue.fm/archives/9889](https://kexue.fm/archives/9889) 這篇說的稀疏性。稀疏性只能告訴我們關注到少數幾個token的能力，但並沒有告訴我們每個token 需要關注的資訊不同。所以有可能很稀疏，但每個token 關注的部分幾乎都相同，例如都集中在\[cls\], 逗號、句號等。

而 token 間的 diversity 則可以告訴我們 token 表示方法的多樣性。若說self-attention 是透過 attention weight 來對整個 sequence 加權得到 token 表示，那麼token對token 的attention weight應該要儘量多元，才能得到每個token不同的表示。

傳統的 Transformer，attention weight 是透過大量資料學出來的，其實並沒有辦法保證token表示的多樣性，只能夠過增加層數並加大MASK 比率，期待它自動學出多樣性。

但是如果在loss中顯式加入attention weights 的diversity，以 std of stds 來衡量，模型就會非常有效地為每個token產生不同的表示。這樣就避免模型跳過attention 直接使用殘差連結，從而可以減少MASK的比率。(我甚至認為也許可以不用MASK，因為 diversity 會鼓勵模型混到其他 token。)

我甚至認為 Linear Attention 或 Flash 等非歸一化的 attention，也可以透過加入這個 attention diversity loss 來提升在小資料上的訓練效果。

[回复评论](https://kexue.fm/archives/7661/comment-page-1?replyTo=23624#respond-post-7661)

[苏剑林](https://kexue.fm) 发表于
January 31st, 2024

[@Allen7575\|comment-23622](https://kexue.fm/archives/7661/comment-page-1#comment-23622)

我还是不大理解你说的“attention对每個字输出一样的值”具体含义是什么？还是指一对多的建模没有输出随机性的结果？或者是说realformer的attention矩阵在最后趋同？

顺便说，realformer的attention矩阵在最后只是趋同，且趋于one hot，但不一定是趋于第一层的attention，所以你说的“退化为只使用第一层”是不成立的。

[回复评论](https://kexue.fm/archives/7661/comment-page-1?replyTo=23634#respond-post-7661)

[苏剑林](https://kexue.fm) 发表于
January 22nd, 2024

我是随机mask的，可能出现你说的问题。但只要数据量够多，这个问题就可以忽略。

从压缩的角度想，我当时训练MLM的数据少则几十GB，多则上百GB，但模型权重大小也就是300来MB（1亿参数量），所以数据远远是充足的，不用担心这种过拟合问题，所以后来dropout都直接去掉了。

[回复评论](https://kexue.fm/archives/7661/comment-page-1?replyTo=23556#respond-post-7661)

Allen7575 发表于
January 22nd, 2024

確實，不過我是想要用在小資料上，所以確實可能需要這層考慮

[回复评论](https://kexue.fm/archives/7661/comment-page-1?replyTo=23562#respond-post-7661)

[取消回复](https://kexue.fm/archives/7661#respond-post-7661)

你的大名

电子邮箱

个人网站（选填）

1\. 可以使用LaTeX代码，点击“预览效果”可查看效果；2. 可以通过点击评论楼层编号来引用该楼层；3. 网站可能会有点卡，如非确认评论失败，请 **不要重复点击提交**。

### 内容速览

[MLM模型](https://kexue.fm/kexue.fm#MLM%E6%A8%A1%E5%9E%8B)
[T-TA介绍](https://kexue.fm/kexue.fm#T-TA%E4%BB%8B%E7%BB%8D)
[实验结果](https://kexue.fm/kexue.fm#%E5%AE%9E%E9%AA%8C%E7%BB%93%E6%9E%9C)
[个人分析](https://kexue.fm/kexue.fm#%E4%B8%AA%E4%BA%BA%E5%88%86%E6%9E%90)

### 智能搜索

支持整句搜索！网站自动使用 [结巴分词](https://github.com/fxsjy/jieba) 进行分词，并结合ngrams排序算法给出合理的搜索结果。

### 热门标签

[生成模型](https://kexue.fm/tag/%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B/) [attention](https://kexue.fm/tag/attention/) [优化](https://kexue.fm/tag/%E4%BC%98%E5%8C%96/) [语言模型](https://kexue.fm/tag/%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/) [模型](https://kexue.fm/tag/%E6%A8%A1%E5%9E%8B/) [网站](https://kexue.fm/tag/%E7%BD%91%E7%AB%99/) [概率](https://kexue.fm/tag/%E6%A6%82%E7%8E%87/) [梯度](https://kexue.fm/tag/%E6%A2%AF%E5%BA%A6/) [矩阵](https://kexue.fm/tag/%E7%9F%A9%E9%98%B5/) [转载](https://kexue.fm/tag/%E8%BD%AC%E8%BD%BD/) [优化器](https://kexue.fm/tag/%E4%BC%98%E5%8C%96%E5%99%A8/) [微分方程](https://kexue.fm/tag/%E5%BE%AE%E5%88%86%E6%96%B9%E7%A8%8B/) [分析](https://kexue.fm/tag/%E5%88%86%E6%9E%90/) [天象](https://kexue.fm/tag/%E5%A4%A9%E8%B1%A1/) [深度学习](https://kexue.fm/tag/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/) [积分](https://kexue.fm/tag/%E7%A7%AF%E5%88%86/) [python](https://kexue.fm/tag/python/) [力学](https://kexue.fm/tag/%E5%8A%9B%E5%AD%A6/) [无监督](https://kexue.fm/tag/%E6%97%A0%E7%9B%91%E7%9D%A3/) [扩散](https://kexue.fm/tag/%E6%89%A9%E6%95%A3/) [几何](https://kexue.fm/tag/%E5%87%A0%E4%BD%95/) [节日](https://kexue.fm/tag/%E8%8A%82%E6%97%A5/) [生活](https://kexue.fm/tag/%E7%94%9F%E6%B4%BB/) [文本生成](https://kexue.fm/tag/%E6%96%87%E6%9C%AC%E7%94%9F%E6%88%90/) [数论](https://kexue.fm/tag/%E6%95%B0%E8%AE%BA/)

### 随机文章

- [线性微分方程组：已知特解求通解](https://kexue.fm/archives/2644)
- [已知中心五边形，作五边形](https://kexue.fm/archives/753)
- [数学竞赛广东预赛\|组成三角形的概率](https://kexue.fm/archives/1477)
- [写在2013年即将逝去之际](https://kexue.fm/archives/2236)
- [\[问题解答\]有多少个5？](https://kexue.fm/archives/1921)
- [你好，2011！](https://kexue.fm/archives/1139)
- [有限内存下全局打乱几百G文件（Python）](https://kexue.fm/archives/8662)
- [必须要GPT3吗？不，BERT的MLM模型也能小样本学习](https://kexue.fm/archives/7764)
- [2009年英仙座流星雨观测](https://kexue.fm/archives/62)
- [【NASA每日一图】壮观的银河系](https://kexue.fm/archives/137)

### 最近评论

- [Zhan-Wang Mao](https://kexue.fm/archives/9257/comment-page-4#comment-28676): 苏老师，请教一下(4)式的泰勒展开式为什么严格来说和$t$有关？不是在$x\_t$处关于$x$的...
- [yzlnew](https://kexue.fm/archives/11340/comment-page-1#comment-28675): 可以相呼应的是，这样的好模型能被浮点数以误差比较低的方式表示和训练，并且也易于量化。
- [Henry](https://kexue.fm/archives/9181/comment-page-5#comment-28673): 想请问苏老师，方程7是如何推导到方程10的，是否有化简的一些小技巧？
- [szsheep](https://kexue.fm/archives/9119/comment-page-13#comment-28672): 牛啊，还可以从这方面推出loss的函数最终式。原本是从KL散度入手，没想到作者完全用另外一种方...
- [pang](https://kexue.fm/archives/10862/comment-page-1#comment-28671): 对于目前的MLA算法softmax(X×WQ×WukT×CjT)×Cj×Wuv来说其实X,WQ...
- [苏剑林](https://kexue.fm/archives/11126/comment-page-3#comment-28670): 你是说 chatglm2-6b 里边的？那个没用，预设的常数是压不住的...
- [苏剑林](https://kexue.fm/archives/443/comment-page-1#comment-28669): 已发
- [苏剑林](https://kexue.fm/archives/10592/comment-page-2#comment-28668): 只想要“方向”，步长只是个标量，另外手工控制。
- [苏剑林](https://kexue.fm/archives/11241/comment-page-1#comment-28667): 参考 https://kexue.fm/archives/10795
- [苏剑林](https://kexue.fm/archives/7546/comment-page-4#comment-28666): $KV$本身通常就不是low-rank的，所以就无所谓升不升秩了。主要问题是，这种最基础的线性...

### 友情链接

- [Cool Papers](https://papers.cool)
- [数学研发](https://bbs.emath.ac.cn)
- [Seatop](http://www.seatop.com.cn/)
- [Xiaoxia](https://xiaoxia.org/)
- [积分表-网络版](https://kexue.fm/sci/integral/index.html)
- [丝路博傲](http://blog.dvxj.com/)
- [数学之家](http://www.2math.cn/)
- [有趣天文奇观](http://interesting-sky.china-vo.org/)
- [TwistedW](http://www.twistedwg.com/)
- [godweiyang](https://godweiyang.com/)
- [AI柠檬](https://blog.ailemon.net/)
- [王登科-DK博客](https://greatdk.com)
- [ESON](https://blog.eson.org/)
- [枫之羽](https://fzhiy.net/)
- [Mathor's blog](https://wmathor.com/)
- [coding-zuo](https://coding-zuo.github.io/)
- [博科园](https://www.bokeyuan.net/)
- [孔皮皮的博客](https://www.kppkkp.top/)
- [运鹏的博客](https://yunpengtai.top/)
- [jiming.site](https://jiming.site/)
- [OmegaXYZ](https://www.omegaxyz.com/)
- [EAI猩球](https://www.robotech.ink/)
- [文举的博客](https://liwenju0.com/)
- [用代码打点酱油](https://bruceyuan.com/)
- [Zhang's blog](https://armcvai.cn/)
- [申请链接](https://kexue.fm/links.html)

本站采用创作共用版权协议，要求署名、非商业用途和保持一致。转载本站内容必须也遵循“ [署名-非商业用途-保持一致](http://creativecommons.org/licenses/by-nc-nd/2.5/cn/)”的创作共用协议。
© 2009-2025 Scientific Spaces. All rights reserved. Theme by [laogui](http://www.laogui.com). Powered by [Typecho](http://typecho.org). 备案号: [粤ICP备09093259号-1/2](https://beian.miit.gov.cn/)。