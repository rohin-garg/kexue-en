## SEARCH

## MENU

- [打赏](https://kexue.fm/reward.html)
- [公式](https://kexue.fm/latex.html)
- [天象](https://kexue.fm/ac.html)
- [链接](https://kexue.fm/links.html)
- [时光](https://kexue.fm/me.html)
- [博览](https://kexue.fm/science.html)
- [归档](https://kexue.fm/content.html)

## CATEGORIES

- [千奇百怪](https://kexue.fm/category/Everything)
- [天文探索](https://kexue.fm/category/Astronomy)
- [数学研究](https://kexue.fm/category/Mathematics)
- [物理化学](https://kexue.fm/category/Phy-chem)
- [信息时代](https://kexue.fm/category/Big-Data)
- [生物自然](https://kexue.fm/category/Biology)
- [图片摄影](https://kexue.fm/category/Photograph)
- [问题百科](https://kexue.fm/category/Questions)
- [生活/情感](https://kexue.fm/category/Life-Feeling)
- [资源共享](https://kexue.fm/category/Resources)

## NEWPOSTS

- [msign的导数](https://kexue.fm/archives/11025)
- [通过msign来计算mclip（奇...](https://kexue.fm/archives/11006)
- [msign算子的Newton-Sc...](https://kexue.fm/archives/10996)
- [等值振荡定理：最优多项式逼近的充要条件](https://kexue.fm/archives/10972)
- [生成扩散模型漫谈（三十）：从瞬时速...](https://kexue.fm/archives/10958)
- [MoE环游记：5、均匀分布的反思](https://kexue.fm/archives/10945)
- [msign算子的Newton-Sc...](https://kexue.fm/archives/10922)
- [Transformer升级之路：2...](https://kexue.fm/archives/10907)
- [一道概率不等式：盯着它到显然成立为止！](https://kexue.fm/archives/10902)
- [SVD的导数](https://kexue.fm/archives/10878)

## COMMENTS

- [盏一: 哦哦哦 你是说 $\\exp nB$ 是正交矩阵! 并不是说 B.](https://kexue.fm/archives/8397/comment-page-3#comment-27910)
- [盏一: 呃, 是我脑子乱了... 忘了 $\\exp(0) = I$. ...](https://kexue.fm/archives/8397/comment-page-3#comment-27909)
- [盏一: 呃, 是我脑子乱了... 忘了 $\\exp(0) = I$. ...](https://kexue.fm/archives/8397/comment-page-3#comment-27908)
- [盏一: 苏神, 请教一下\> 并且还可以证明它一定是正交矩阵是怎么证明的...](https://kexue.fm/archives/8397/comment-page-3#comment-27907)
- [sk: 请问公式14是怎么得出来的？](https://kexue.fm/archives/8265/comment-page-8#comment-27906)
- [tll1945tll1937: 真心实意的向大家请教问题：看了文章“对齐全量微调！这是我看过最...](https://kexue.fm/archives/10266/comment-page-1#comment-27901)
- [oYo\_logan: \[comment=27017\]苏剑林\[/comment\]苏神，...](https://kexue.fm/archives/10757/comment-page-1#comment-27897)
- [z123: 在参数矩阵较多的CNN小模型上，Muon会明显慢于Adam，这...](https://kexue.fm/archives/10592/comment-page-1#comment-27896)
- [dry: 苏神好，一直有个疑问，ReFlow构建的ODE是$dx\_t/d...](https://kexue.fm/archives/10958/comment-page-2#comment-27895)
- [tyj: 感觉和之前的一篇文章很像，应该算是concurrent wor...](https://kexue.fm/archives/10958/comment-page-2#comment-27894)

## USERLOGIN

- [登录](https://kexue.fm/admin/login.php)

[科学空间\|Scientific Spaces](https://kexue.fm)

- [登录](https://kexue.fm/admin/login.php)
- [打赏](https://kexue.fm/reward.html)
- [公式](https://kexue.fm/latex.html)
- [天象](https://kexue.fm/ac.html)
- [链接](https://kexue.fm/links.html)
- [时光](https://kexue.fm/me.html)
- [博览](https://kexue.fm/science.html)
- [归档](https://kexue.fm/content.html)

渴望成为一个小飞侠

- [欢迎订阅](https://kexue.fm/feed)
- [个性邮箱](https://kexue.fm/archives/119)
- [天象信息](https://kexue.fm/ac.html)
- [观测ISS](https://kexue.fm/archives/41)
- [LaTeX](https://kexue.fm/latex.html)
- [关于博主](https://kexue.fm/me.html)

欢迎访问“科学空间”，这里将与您共同探讨自然科学，回味人生百态；也期待大家的分享～

- [**千奇百怪** Everything](https://kexue.fm/category/Everything)
- [**天文探索** Astronomy](https://kexue.fm/category/Astronomy)
- [**数学研究** Mathematics](https://kexue.fm/category/Mathematics)
- [**物理化学** Phy-chem](https://kexue.fm/category/Phy-chem)
- [**信息时代** Big-Data](https://kexue.fm/category/Big-Data)
- [**生物自然** Biology](https://kexue.fm/category/Biology)
- [**图片摄影** Photograph](https://kexue.fm/category/Photograph)
- [**问题百科** Questions](https://kexue.fm/category/Questions)
- [**生活/情感** Life-Feeling](https://kexue.fm/category/Life-Feeling)
- [**资源共享** Resources](https://kexue.fm/category/Resources)

- [**千奇百怪**](https://kexue.fm/category/Everything)
- [**天文探索**](https://kexue.fm/category/Astronomy)
- [**数学研究**](https://kexue.fm/category/Mathematics)
- [**物理化学**](https://kexue.fm/category/Phy-chem)
- [**信息时代**](https://kexue.fm/category/Big-Data)
- [**生物自然**](https://kexue.fm/category/Biology)
- [**图片摄影**](https://kexue.fm/category/Photograph)
- [**问题百科**](https://kexue.fm/category/Questions)
- [**生活/情感**](https://kexue.fm/category/Life-Feeling)
- [**资源共享**](https://kexue.fm/category/Resources)

[首页](https://kexue.fm) [信息时代](https://kexue.fm/category/Big-Data) FlatNCE：小批次对比学习效果差的原因竟是浮点误差？

26Jul

# [FlatNCE：小批次对比学习效果差的原因竟是浮点误差？](https://kexue.fm/archives/8586)

By 苏剑林 \|
2021-07-26 \|
54787位读者\|

自 [SimCLR](https://papers.cool/arxiv/2002.05709) 在视觉无监督学习大放异彩以来，对比学习逐渐在CV乃至NLP中流行了起来，相关研究和工作越来越多。标准的对比学习的一个广为人知的缺点是需要比较大的batch\_size（SimCLR在batch\_size=4096时效果最佳），小batch\_size的时候效果会明显降低，为此，后续工作的改进方向之一就是降低对大batch\_size的依赖。那么，一个很自然的问题是：标准的对比学习在小batch\_size时效果差的原因究竟是什么呢？

近日，一篇名为 [《Simpler, Faster, Stronger: Breaking The log-K Curse On Contrastive Learners With FlatNCE》](https://papers.cool/arxiv/2107.01152) 对此问题作出了回答：因为浮点误差。看起来真的很让人难以置信，但论文的分析确实颇有道理，并且所提出的改进FlatNCE确实也工作得更好，让人不得不信服。

## 细微之处 [\#](https://kexue.fm/archives/8586\#%E7%BB%86%E5%BE%AE%E4%B9%8B%E5%A4%84)

接下来，笔者将按照自己的理解和记号来介绍原论文的主要内容。对比学习（Contrastive Learning）就不帮大家详细复习了，大体上来说，对于某个样本$x$，我们需要构建$K$个配对样本$y\_1,y\_2,\\cdots,y\_K$，其中$y\_t$是正样本而其余都是负样本，然后分别给每个样本对$(x, y\_i)$打分，分别记为$s\_1,s\_2,\\cdots,s\_K$，对比学习希望拉大正负样本对的得分差，通常直接用交叉熵作为损失：
\\begin{equation}-\\log \\frac{e^{s\_t}}{\\sum\\limits\_i e^{s\_i}} = \\log \\left(\\sum\_i e^{s\_i}\\right) - s\_t = \\log \\left(1 + \\sum\_{i\\neq t} e^{s\_i - s\_t}\\right)\\end{equation}
简单起见，后面都记$\\xi=\\sum\\limits\_{i\\neq t} e^{s\_i - s\_t}$。在实践时，正样本通常是数据扩增而来的高相似样本，而负样本则是把batch内所有其他样本都算上，因此大致上可以认为负样本是随机选择的$K-1$个样本。这就说明，正负样本对的差距还是很明显的，因此模型很容易做到$s\_t \\gg s\_i(i\\neq t)$，也即$e^{s\_i - s\_t}\\approx 0$。于是，当batch\_size比较小的时候（等价于$K$比较小），$\\xi$也会相当接近于0，这意味着上述损失函数也会相当接近于0。

损失函数接近于0，通常也意味着梯度接近于0了，然而，这不意味着模型的更新量就很小了。因为当前对比学习用的都是自适应优化器如Adam，它们的更新量大致形式为$\\frac{\\text{梯度}}{\\sqrt{\\text{梯度}\\otimes\\text{梯度}}}\\times\\text{学习率}$，这就意味着，不管梯度多小，只要它稳定，那么更新量就会保持着$\\text{学习率}$的数量级。对比学习正是这样的场景，要想$e^{s\_i - s\_t}\\to 0$，那么就要$s\_i - s\_t\\to -\\infty$，但对比学习的打分通常是余弦值除以温度参数，所以它是有界的，$s\_i - s\_t\\to -\\infty$是无法实现的，因此经过一定的训练步数后，损失函数将会长期保持接近于0但又大于0的状态。

然而，$\\xi$的计算本身就存在浮点误差，当$\\xi$很接近于0时，浮点误差可能比精确值还要大，然后$\\log(1+\\xi)$的计算也会存在浮点误差，再然后梯度的计算也会存在浮点误差，这一系列误差累积下来，很可能导致最后算出来的梯度都接近于随机噪声了，而不能提供有效的更新指引。这就是原论文认为的对比学习在小batch\_size时效果明显变差的原因。

## 变微为著 [\#](https://kexue.fm/archives/8586\#%E5%8F%98%E5%BE%AE%E4%B8%BA%E8%91%97)

理解了这个原因后，其实也就不难针对性地提出解决方案了。对损失函数做一阶展开我们有：
\\begin{equation}\\log \\left(1 + \\sum\_{i\\neq t} e^{s\_i - s\_t}\\right)\\approx \\sum\_{i\\neq t} e^{s\_i - s\_t}\\end{equation}
也就是说，一定训练步数之后，模型相当于以$\\xi$为损失函数了。当然，由于$\\log(1+\\xi)\\leq \\xi$，即$\\xi$是$\\log(1+\\xi)$的上界，所以就算一开始就以$\\xi$为损失函数，结果也没什么差别，现在主要还是解决的问题是$\\xi$接近于0而导致了浮点误差问题。刚才说了，自适应优化器的更新量大致上都是$\\frac{\\text{梯度}}{\\sqrt{\\text{梯度}\\otimes\\text{梯度}}}\\times\\text{学习率}$的形式，这意味着如果我们直接将损失函数乘以一个常数，那么理论上更新量是不会改变的，所以既然$\\xi$过小，那么我们就将它乘以一个常数放大就好了。

乘以什么好呢？比较直接的想法是损失函数不能过小，也不能过大，控制在$\\mathcal{O}(1)$级别最好，所以我们干脆乘以$\\xi$的倒数，也就是以
\\begin{equation}\\frac{\\xi}{\\text{sg}(\\xi)} = \\frac{\\sum\\limits\_{i\\neq t} e^{s\_i - s\_t}}{\\text{sg}\\left(\\sum\\limits\_{i\\neq t} e^{s\_i - s\_t}\\right)}\\label{eq:flatnce-1}\\end{equation}
为损失函数。这里$\\text{sg}$是stop\_gradient的意思（原论文称为detach），也就是把分母纯粹当成一个常数，求梯度的时候只需要对分子求。这就是原论文提出的替代方案，称为FlatNCE。

不过，上述带$\\text{sg}$算子形式的损失函数毕竟不是我们习惯的形式，我们可以转换一下。观察到：
\\begin{equation}\\nabla\_{\\theta}\\left(\\frac{\\xi}{\\text{sg}(\\xi)}\\right) = \\frac{\\nabla\_{\\theta}\\xi}{\\xi} = \\nabla\_{\\theta}\\log \\xi\\end{equation}
也就是说，$\\frac{\\xi}{\\text{sg}(\\xi)}$作为损失函数提供的梯度跟$\\log \\xi$作为损失函数的梯度是一模一样的，因此我们可以把损失函数换为不带$\\text{sg}$算子的$\\log \\xi$：
\\begin{equation}\\log\\left(\\sum\\limits\_{i\\neq t} e^{s\_i - s\_t}\\right) = \\log\\left(\\sum\\limits\_{i\\neq t} e^{s\_i}\\right) - s\_t\\label{eq:flatnce-2}\\end{equation}
相比于交叉熵，上述损失就是在$\\text{logsumexp}$运算中去掉了正样本对的得分$s\_t$。注意到$\\text{logsumexp}$通常可以有效地计算，浮点误差不会占主导，因此我们用上述损失函数取代交叉熵，理论上跟交叉熵是等效的，而实践上在小batch\_size时效果比交叉熵要好。此外，需要指出的是，上式结果不一定是非负的，因此换用上述损失函数后在训练过程中出现负的损失值也不需要意外，这是正常现象。

## 实践真知 [\#](https://kexue.fm/archives/8586\#%E5%AE%9E%E8%B7%B5%E7%9C%9F%E7%9F%A5)

分析似乎有那么点道理，那么事实是否有效呢？这自然是要靠实验来说话了。不出意料，FlatNCE确实工作得非常好。

原论文的实验都是CV的，主要是把SimCLR的损失换为FlatNCE进行实验，对应的结果称为FlatCLR。其中，我们最关心的大概是FlatNCE是否真的解决了对大batch\_size的依赖问题，下面的图像则作出了肯定回答：

不同batch\_size下SimCLR与FlatCLR对比图

下面则是SimCLR和FlatCLR在各个任务上的结果对比，显示出FlatCLR更好的性能：

SimCLR和FlatCLR在各个任务上的对比

## 吹毛求疵 [\#](https://kexue.fm/archives/8586\#%E5%90%B9%E6%AF%9B%E6%B1%82%E7%96%B5)

总的来说，原论文的结果非常有创造性，“浮点误差”这一视角非常“刁钻”但也相当精准，让人不得不点赞。

直观来看，原来交叉熵的目标是“正样本得分与负样本得分的差尽量大”，这对于常规的分类问题是没问题的，但对于对比学习来说还不够，因为对比学习目的是学习特征，除了正样本要比负样本得分高这种“粗”特征外，负样本之间也要继续对比以学习更精细的特征；FlatNCE的目标则是“正样本的得分要尽量大，负样本的得分要尽量小”，也即从相对值的学习变成了绝对值的学习，从而使得正负样本拉开一定距离后，依然能够继续优化，而不至于过早停止（对于非自适应优化器），或者让浮点误差带来的噪声占了主导（对于自适应优化器）。

然而，原论文的某些内容设置也不得不让人吐槽。比如，论文花了较大的篇幅讨论互信息的估计，但这跟论文主体并无实质关联，加大了读者的理解难度。当然，paper跟科普不一样，为了使文章更充实而增加额外的理论推导也无可厚非，只是如果能更突出浮点误差部分的分析更好。然后，论文最让我不能理解的地方是直接以$\\eqref{eq:flatnce-1}$为最终结果，这种带“stop\_gradient”的表述方式虽然算不上难，但也不友好，通常来说这种方式是难以寻求原函数的时候才“不得不”使用的，但FlatNCE显然不是这样。

## 总结全文 [\#](https://kexue.fm/archives/8586\#%E6%80%BB%E7%BB%93%E5%85%A8%E6%96%87)

本文介绍了对比学习的一个新工作，该工作分析了小批次对比学习时交叉熵的浮点误差问题，指出这可能是小批次对比学习效果差的主要原因，并且针对性地提出了改进的损失函数FlatNCE，实验表明基于FlatNCE的对比学习确实能缓解对大batch\_size的依赖，并且能获得更好的效果。

_**转载到请包括本文地址：** [https://kexue.fm/archives/8586](https://kexue.fm/archives/8586)_

_**更详细的转载事宜请参考：**_ [《科学空间FAQ》](https://kexue.fm/archives/6508#%E6%96%87%E7%AB%A0%E5%A6%82%E4%BD%95%E8%BD%AC%E8%BD%BD/%E5%BC%95%E7%94%A8)

**如果您还有什么疑惑或建议，欢迎在下方评论区继续讨论。**

**如果您觉得本文还不错，欢迎 [分享](https://kexue.fm/archives/8586#share)/ [打赏](https://kexue.fm/archives/8586#pay) 本文。打赏并非要从中获得收益，而是希望知道科学空间获得了多少读者的真心关注。当然，如果你无视它，也不会影响你的阅读。再次表示欢迎和感谢！**

打赏

微信打赏

支付宝打赏

因为网站后台对打赏并无记录，因此欢迎在打赏时候备注留言。你还可以 [**点击这里**](http://mail.qq.com/cgi-bin/qm_share?t=qm_mailme&email=tN7d1drY3drrx8H0xcWa19vZ) 或在下方评论区留言来告知你的建议或需求。

**如果您需要引用本文，请参考：**

苏剑林. (Jul. 26, 2021). 《FlatNCE：小批次对比学习效果差的原因竟是浮点误差？ 》\[Blog post\]. Retrieved from [https://kexue.fm/archives/8586](https://kexue.fm/archives/8586)

@online{kexuefm-8586,
        title={FlatNCE：小批次对比学习效果差的原因竟是浮点误差？},
        author={苏剑林},
        year={2021},
        month={Jul},
        url={\\url{https://kexue.fm/archives/8586}},
}

分类： [信息时代](https://kexue.fm/category/Big-Data)    标签： [优化](https://kexue.fm/tag/%E4%BC%98%E5%8C%96/), [损失函数](https://kexue.fm/tag/%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0/), [对比学习](https://kexue.fm/tag/%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/)[8 评论](https://kexue.fm/archives/8586#comments)

< [概率视角下的线性模型：逻辑回归有解析解吗？](https://kexue.fm/archives/8578) \| [Transformer升级之路：5、作为无限维的线性Attention](https://kexue.fm/archives/8601) >

### 你也许还对下面的内容感兴趣

- [MoE环游记：5、均匀分布的反思](https://kexue.fm/archives/10945)
- [Transformer升级之路：20、MLA究竟好在哪里？](https://kexue.fm/archives/10907)
- [MoE环游记：4、难处应当多投入](https://kexue.fm/archives/10815)
- [MoE环游记：3、换个思路来分配](https://kexue.fm/archives/10757)
- [MoE环游记：2、不患寡而患不均](https://kexue.fm/archives/10735)
- [为什么梯度裁剪的默认模长是1？](https://kexue.fm/archives/10657)
- [从谱范数梯度到新式权重衰减的思考](https://kexue.fm/archives/10648)
- [从Hessian近似看自适应学习率优化器](https://kexue.fm/archives/10588)
- [通向最优分布之路：概率空间的最小化](https://kexue.fm/archives/10289)
- [通向概率分布之路：盘点Softmax及其替代品](https://kexue.fm/archives/10145)

[发表你的看法](https://kexue.fm/archives/8586#comment_form)

郑炎钊

July 26th, 2021

针对NLP用余弦相似度作为指标的对比学习任务，是否可以直接移用呀，苏神做过测试吗～

[回复评论](https://kexue.fm/archives/8586/comment-page-1?replyTo=17003#respond-post-8586)

[苏剑林](https://kexue.fm) 发表于
July 27th, 2021

没详细做过实验，主要我现在也没遇到过特别依赖于大batch\_size的对比学习场景。

不过我简单试了试，simcse中直接换成flatnce的loss，效果至少是可以不变的，说明flatnce至少不至于变差。然后roformer-sim直接有监督学习cos的实验中，如果损失换成flatnce，那么收敛更快，更接近sentence-bert的结果。

[回复评论](https://kexue.fm/archives/8586/comment-page-1?replyTo=17009#respond-post-8586)

Joe

July 29th, 2021

这么看的话，对于类别比较多的多分类，FlatNCE是否也会有提升呢，苏神怎么看

[回复评论](https://kexue.fm/archives/8586/comment-page-1?replyTo=17037#respond-post-8586)

[苏剑林](https://kexue.fm) 发表于
July 29th, 2021

可能有，但理论上没有。因为浮点误差占主导的时候，$\\xi$就很小了，而$\\xi$很小意味着已经成功分类了。

[回复评论](https://kexue.fm/archives/8586/comment-page-1?replyTo=17043#respond-post-8586)

chen dian

August 16th, 2021

请教苏神，我测试发现，用这个损失函数测试simcse时，在计算cos相似度的时候，如果不除以temperature 不将 cos值放大，效果会很差，但是 self.cos(x, y) / self.temp， temp 取较小的时候，比如说0.05， 效果会有所提升，虽然temperature 常被用作softmax平滑，但是这里不加这个，发现效果明显下降，这是为什么呢？

[回复评论](https://kexue.fm/archives/8586/comment-page-1?replyTo=17145#respond-post-8586)

[苏剑林](https://kexue.fm) 发表于
August 18th, 2021

因为学习目标是one hot的，模型需要预测出接近one hot的结果才算学完。$\\cos$值的范围是$\[-1,1\]$，将若干个$\[-1,1\]$范围内的数直接softmax，得到的结果离one hot还很远，因此不除以$\\tau$来把范围放大的话基本不收敛。

[回复评论](https://kexue.fm/archives/8586/comment-page-1?replyTo=17152#respond-post-8586)

hzk123

November 9th, 2021

我发现这个方法的梯度特别大，不能随便做梯度裁剪，不知道是不是我写错了

[回复评论](https://kexue.fm/archives/8586/comment-page-1?replyTo=17763#respond-post-8586)

[苏剑林](https://kexue.fm) 发表于
November 10th, 2021

相比原来的交叉熵loss，肯定是放大了梯度的。看$(4)$式就知道了。

[回复评论](https://kexue.fm/archives/8586/comment-page-1?replyTo=17777#respond-post-8586)

[取消回复](https://kexue.fm/archives/8586#respond-post-8586)

你的大名

电子邮箱

个人网站（选填）

1\. 可以使用LaTeX代码，点击“预览效果”可查看效果；2. 可以通过点击评论楼层编号来引用该楼层；3. 网站可能会有点卡，如非确认评论失败，请 **不要重复点击提交**。

### 内容速览

[细微之处](https://kexue.fm/archives/8586#%E7%BB%86%E5%BE%AE%E4%B9%8B%E5%A4%84)
[变微为著](https://kexue.fm/archives/8586#%E5%8F%98%E5%BE%AE%E4%B8%BA%E8%91%97)
[实践真知](https://kexue.fm/archives/8586#%E5%AE%9E%E8%B7%B5%E7%9C%9F%E7%9F%A5)
[吹毛求疵](https://kexue.fm/archives/8586#%E5%90%B9%E6%AF%9B%E6%B1%82%E7%96%B5)
[总结全文](https://kexue.fm/archives/8586#%E6%80%BB%E7%BB%93%E5%85%A8%E6%96%87)

### 智能搜索

支持整句搜索！网站自动使用 [结巴分词](https://github.com/fxsjy/jieba) 进行分词，并结合ngrams排序算法给出合理的搜索结果。

### 热门标签

[生成模型](https://kexue.fm/tag/%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B/) [attention](https://kexue.fm/tag/attention/) [优化](https://kexue.fm/tag/%E4%BC%98%E5%8C%96/) [语言模型](https://kexue.fm/tag/%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/) [模型](https://kexue.fm/tag/%E6%A8%A1%E5%9E%8B/) [网站](https://kexue.fm/tag/%E7%BD%91%E7%AB%99/) [概率](https://kexue.fm/tag/%E6%A6%82%E7%8E%87/) [梯度](https://kexue.fm/tag/%E6%A2%AF%E5%BA%A6/) [转载](https://kexue.fm/tag/%E8%BD%AC%E8%BD%BD/) [微分方程](https://kexue.fm/tag/%E5%BE%AE%E5%88%86%E6%96%B9%E7%A8%8B/) [矩阵](https://kexue.fm/tag/%E7%9F%A9%E9%98%B5/) [天象](https://kexue.fm/tag/%E5%A4%A9%E8%B1%A1/) [分析](https://kexue.fm/tag/%E5%88%86%E6%9E%90/) [深度学习](https://kexue.fm/tag/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/) [积分](https://kexue.fm/tag/%E7%A7%AF%E5%88%86/) [python](https://kexue.fm/tag/python/) [优化器](https://kexue.fm/tag/%E4%BC%98%E5%8C%96%E5%99%A8/) [力学](https://kexue.fm/tag/%E5%8A%9B%E5%AD%A6/) [无监督](https://kexue.fm/tag/%E6%97%A0%E7%9B%91%E7%9D%A3/) [扩散](https://kexue.fm/tag/%E6%89%A9%E6%95%A3/) [几何](https://kexue.fm/tag/%E5%87%A0%E4%BD%95/) [节日](https://kexue.fm/tag/%E8%8A%82%E6%97%A5/) [生活](https://kexue.fm/tag/%E7%94%9F%E6%B4%BB/) [文本生成](https://kexue.fm/tag/%E6%96%87%E6%9C%AC%E7%94%9F%E6%88%90/) [数论](https://kexue.fm/tag/%E6%95%B0%E8%AE%BA/)

### 随机文章

- [我害怕](https://kexue.fm/archives/1928)
- [当Bert遇上Keras：这可能是Bert最简单的打开姿势](https://kexue.fm/archives/6736)
- [《重逢》——最终亚运会会歌](https://kexue.fm/archives/960)
- [让风筝飞](https://kexue.fm/archives/2891)
- [复分析学习1：揭示微分与积分的联系](https://kexue.fm/archives/1683)
- [记录一次爬取淘宝/天猫评论数据的过程](https://kexue.fm/archives/3298)
- [彗星(非小行星)重创月球](https://kexue.fm/archives/65)
- [我们打算飞到小行星上——但是，哪一颗好呢？](https://kexue.fm/archives/639)
- [数学基本技艺之23、24（上）](https://kexue.fm/archives/2083)
- [三百年之谜——费马大定理(历史+证明)](https://kexue.fm/archives/38)

### 最近评论

- [盏一](https://kexue.fm/archives/8397/comment-page-3#comment-27910): 哦哦哦 你是说 $\\exp nB$ 是正交矩阵! 并不是说 B.
- [盏一](https://kexue.fm/archives/8397/comment-page-3#comment-27909): 呃, 是我脑子乱了... 忘了 $\\exp(0) = I$. 所以只要 $\\Vert B^T+...
- [盏一](https://kexue.fm/archives/8397/comment-page-3#comment-27908): 呃, 是我脑子乱了... 忘了 $\\exp(0) = I$. 所以只要 $\\Vert B^T+...
- [盏一](https://kexue.fm/archives/8397/comment-page-3#comment-27907): 苏神, 请教一下\> 并且还可以证明它一定是正交矩阵是怎么证明的. 我本来以为隐式利用了 $\\V...
- [sk](https://kexue.fm/archives/8265/comment-page-8#comment-27906): 请问公式14是怎么得出来的？
- [tll1945tll1937](https://kexue.fm/archives/10266/comment-page-1#comment-27901): 真心实意的向大家请教问题：看了文章“对齐全量微调！这是我看过最精彩的LoRA改进（二）”，我实...
- [oYo\_logan](https://kexue.fm/archives/10757/comment-page-1#comment-27897): \[comment=27017\]苏剑林\[/comment\]苏神，想请教一下，我理解在一个batc...
- [z123](https://kexue.fm/archives/10592/comment-page-1#comment-27896): 在参数矩阵较多的CNN小模型上，Muon会明显慢于Adam，这方面有什么优化提速的方案吗？
- [dry](https://kexue.fm/archives/10958/comment-page-2#comment-27895): 苏神好，一直有个疑问，ReFlow构建的ODE是$dx\_t/dt=x\_1-x\_0$，为什么这并...
- [tyj](https://kexue.fm/archives/10958/comment-page-2#comment-27894): 感觉和之前的一篇文章很像，应该算是concurrent work： https://arxiv...

### 友情链接

- [Cool Papers](https://papers.cool)
- [数学研发](https://bbs.emath.ac.cn)
- [Seatop](http://www.seatop.com.cn/)
- [Xiaoxia](https://xiaoxia.org/)
- [积分表-网络版](https://kexue.fm/sci/integral/index.html)
- [丝路博傲](http://blog.dvxj.com/)
- [ph4ntasy 饭特稀](http://www.ph4ntasy.com/)
- [数学之家](http://www.2math.cn/)
- [有趣天文奇观](http://interesting-sky.china-vo.org/)
- [TwistedW](http://www.twistedwg.com/)
- [godweiyang](https://godweiyang.com/)
- [AI柠檬](https://blog.ailemon.net/)
- [王登科-DK博客](https://greatdk.com)
- [ESON](https://blog.eson.org/)
- [枫之羽](https://fzhiy.net/)
- [Mathor's blog](https://wmathor.com/)
- [coding-zuo](https://coding-zuo.github.io/)
- [博科园](https://www.bokeyuan.net/)
- [孔皮皮的博客](https://www.kppkkp.top/)
- [运鹏的博客](https://yunpengtai.top/)
- [jiming.site](https://jiming.site/)
- [OmegaXYZ](https://www.omegaxyz.com/)
- [Blog by Eacls](https://www.eacls.top/)
- [EAI猩球](https://www.robotech.ink/)
- [文举的博客](https://liwenju0.com/)
- [用代码打点酱油](https://bruceyuan.com/)
- [申请链接](https://kexue.fm/links.html)

本站采用创作共用版权协议，要求署名、非商业用途和保持一致。转载本站内容必须也遵循“ [署名-非商业用途-保持一致](http://creativecommons.org/licenses/by-nc-nd/2.5/cn/)”的创作共用协议。
© 2009-2025 Scientific Spaces. All rights reserved. Theme by [laogui](http://www.laogui.com). Powered by [Typecho](http://typecho.org). 备案号: [粤ICP备09093259号-1/2](https://beian.miit.gov.cn/)。