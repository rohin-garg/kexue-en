## SEARCH

## MENU

- [打赏](https://kexue.fm/reward.html)
- [公式](https://kexue.fm/latex.html)
- [天象](https://kexue.fm/ac.html)
- [链接](https://kexue.fm/links.html)
- [时光](https://kexue.fm/me.html)
- [博览](https://kexue.fm/science.html)
- [归档](https://kexue.fm/content.html)

## CATEGORIES

- [千奇百怪](https://kexue.fm/category/Everything)
- [天文探索](https://kexue.fm/category/Astronomy)
- [数学研究](https://kexue.fm/category/Mathematics)
- [物理化学](https://kexue.fm/category/Phy-chem)
- [信息时代](https://kexue.fm/category/Big-Data)
- [生物自然](https://kexue.fm/category/Biology)
- [图片摄影](https://kexue.fm/category/Photograph)
- [问题百科](https://kexue.fm/category/Questions)
- [生活/情感](https://kexue.fm/category/Life-Feeling)
- [资源共享](https://kexue.fm/category/Resources)

## NEWPOSTS

- [msign的导数](https://kexue.fm/archives/11025)
- [通过msign来计算mclip（奇...](https://kexue.fm/archives/11006)
- [msign算子的Newton-Sc...](https://kexue.fm/archives/10996)
- [等值振荡定理：最优多项式逼近的充要条件](https://kexue.fm/archives/10972)
- [生成扩散模型漫谈（三十）：从瞬时速...](https://kexue.fm/archives/10958)
- [MoE环游记：5、均匀分布的反思](https://kexue.fm/archives/10945)
- [msign算子的Newton-Sc...](https://kexue.fm/archives/10922)
- [Transformer升级之路：2...](https://kexue.fm/archives/10907)
- [一道概率不等式：盯着它到显然成立为止！](https://kexue.fm/archives/10902)
- [SVD的导数](https://kexue.fm/archives/10878)

## COMMENTS

- [盏一: 哦哦哦 你是说 $\\exp nB$ 是正交矩阵! 并不是说 B.](https://kexue.fm/archives/8397/comment-page-3#comment-27910)
- [盏一: 呃, 是我脑子乱了... 忘了 $\\exp(0) = I$. ...](https://kexue.fm/archives/8397/comment-page-3#comment-27909)
- [盏一: 呃, 是我脑子乱了... 忘了 $\\exp(0) = I$. ...](https://kexue.fm/archives/8397/comment-page-3#comment-27908)
- [盏一: 苏神, 请教一下\> 并且还可以证明它一定是正交矩阵是怎么证明的...](https://kexue.fm/archives/8397/comment-page-3#comment-27907)
- [sk: 请问公式14是怎么得出来的？](https://kexue.fm/archives/8265/comment-page-8#comment-27906)
- [tll1945tll1937: 真心实意的向大家请教问题：看了文章“对齐全量微调！这是我看过最...](https://kexue.fm/archives/10266/comment-page-1#comment-27901)
- [oYo\_logan: \[comment=27017\]苏剑林\[/comment\]苏神，...](https://kexue.fm/archives/10757/comment-page-1#comment-27897)
- [z123: 在参数矩阵较多的CNN小模型上，Muon会明显慢于Adam，这...](https://kexue.fm/archives/10592/comment-page-1#comment-27896)
- [dry: 苏神好，一直有个疑问，ReFlow构建的ODE是$dx\_t/d...](https://kexue.fm/archives/10958/comment-page-2#comment-27895)
- [tyj: 感觉和之前的一篇文章很像，应该算是concurrent wor...](https://kexue.fm/archives/10958/comment-page-2#comment-27894)

## USERLOGIN

- [登录](https://kexue.fm/admin/login.php)

[科学空间\|Scientific Spaces](https://kexue.fm)

- [登录](https://kexue.fm/admin/login.php)
- [打赏](https://kexue.fm/reward.html)
- [公式](https://kexue.fm/latex.html)
- [天象](https://kexue.fm/ac.html)
- [链接](https://kexue.fm/links.html)
- [时光](https://kexue.fm/me.html)
- [博览](https://kexue.fm/science.html)
- [归档](https://kexue.fm/content.html)

渴望成为一个小飞侠

- [欢迎订阅](https://kexue.fm/feed)
- [个性邮箱](https://kexue.fm/archives/119)
- [天象信息](https://kexue.fm/ac.html)
- [观测ISS](https://kexue.fm/archives/41)
- [LaTeX](https://kexue.fm/latex.html)
- [关于博主](https://kexue.fm/me.html)

欢迎访问“科学空间”，这里将与您共同探讨自然科学，回味人生百态；也期待大家的分享～

- [**千奇百怪** Everything](https://kexue.fm/category/Everything)
- [**天文探索** Astronomy](https://kexue.fm/category/Astronomy)
- [**数学研究** Mathematics](https://kexue.fm/category/Mathematics)
- [**物理化学** Phy-chem](https://kexue.fm/category/Phy-chem)
- [**信息时代** Big-Data](https://kexue.fm/category/Big-Data)
- [**生物自然** Biology](https://kexue.fm/category/Biology)
- [**图片摄影** Photograph](https://kexue.fm/category/Photograph)
- [**问题百科** Questions](https://kexue.fm/category/Questions)
- [**生活/情感** Life-Feeling](https://kexue.fm/category/Life-Feeling)
- [**资源共享** Resources](https://kexue.fm/category/Resources)

- [**千奇百怪**](https://kexue.fm/category/Everything)
- [**天文探索**](https://kexue.fm/category/Astronomy)
- [**数学研究**](https://kexue.fm/category/Mathematics)
- [**物理化学**](https://kexue.fm/category/Phy-chem)
- [**信息时代**](https://kexue.fm/category/Big-Data)
- [**生物自然**](https://kexue.fm/category/Biology)
- [**图片摄影**](https://kexue.fm/category/Photograph)
- [**问题百科**](https://kexue.fm/category/Questions)
- [**生活/情感**](https://kexue.fm/category/Life-Feeling)
- [**资源共享**](https://kexue.fm/category/Resources)

[首页](https://kexue.fm) [信息时代](https://kexue.fm/category/Big-Data) 变分自编码器（七）：球面上的VAE（vMF-VAE）

17May

# [变分自编码器（七）：球面上的VAE（vMF-VAE）](https://kexue.fm/archives/8404)

By 苏剑林 \|
2021-05-17 \|
171956位读者\|

在 [《变分自编码器（五）：VAE + BN = 更好的VAE》](https://kexue.fm/archives/7381) 中，我们讲到了NLP中训练VAE时常见的KL散度消失现象，并且提到了通过BN来使得KL散度项有一个正的下界，从而保证KL散度项不会消失。事实上，早在2018年的时候，就有类似思想的工作就被提出了，它们是通过在VAE中改用新的先验分布和后验分布，来使得KL散度项有一个正的下界。

该思路出现在2018年的两篇相近的论文中，分别是 [《Hyperspherical Variational Auto-Encoders》](https://papers.cool/arxiv/1804.00891) 和 [《Spherical Latent Spaces for Stable Variational Autoencoders》](https://papers.cool/arxiv/1808.10805)，它们都是用定义在超球面的von Mises–Fisher（vMF）分布来构建先后验分布。某种程度上来说，该分布比我们常用的高斯分布还更简单和有趣～

## KL散度消失 [\#](https://kexue.fm/archives/8404\#KL%E6%95%A3%E5%BA%A6%E6%B6%88%E5%A4%B1)

我们知道，VAE的训练目标是
\\begin{equation}\\mathcal{L} = \\mathbb{E}\_{x\\sim \\tilde{p}(x)} \\Big\[\\mathbb{E}\_{z\\sim p(z\|x)}\\big\[-\\log q(x\|z)\\big\]+KL\\big(p(z\|x)\\big\\Vert q(z)\\big)\\Big\]
\\end{equation}
其中第一项是重构项，第二项是KL散度项，在 [《变分自编码器（一）：原来是这么一回事》](https://kexue.fm/archives/5253) 中我们就说过，这两项某种意义上是“对抗”的，KL散度项的存在，会加大解码器利用编码信息的难度，如果KL散度项为0，那么说明解码器完全没有利用到编码器的信息。

在NLP中，输入和重构的对象是句子，为了保证效果，解码器一般用自回归模型。然而，自回归模型是非常强大的模型，强大到哪怕没有输入，也能完成训练（退化为无条件语言模型），而刚才我们说了，KL散度项会加大解码器利用编码信息的难度，所以解码器干脆弃之不用，这就出现了KL散度消失现象。

早期比较常见的应对方案是逐渐增加KL项的权重，以引导解码器去利用编码信息。现在比较流行的方案就是通过某些改动，直接让KL散度项有一个正的下界。将先后验分布换为vMF分布，就是这种方案的经典例子之一。

## vMF分布 [\#](https://kexue.fm/archives/8404\#vMF%E5%88%86%E5%B8%83)

vMF分布是定义在$d-1$维超球面的分布，其样本空间为$S^{d-1}=\\{x\|x\\in\\mathbb{R}^d, \\Vert x\\Vert=1\\}$，概率密度函数则为
\\begin{equation}p(x) = \\frac{e^{\\langle\\xi,x\\rangle}}{Z\_{d, \\Vert\\xi\\Vert}},\\quad Z\_{d, \\Vert\\xi\\Vert}=\\int\_{S^{d-1}}e^{\\langle\\xi,x\\rangle} dS^{d-1}\\end{equation}
其中$\\xi\\in\\mathbb{R}^d$是预先给定的参数向量。不难想象，这是$S^{d-1}$上一个以$\\xi$为中心的分布，归一化因子写成$Z\_{d, \\Vert\\xi\\Vert}$的形式，意味着它只依赖于$\\xi$的模长，这是由于各向同性导致的。由于这个特性，vMF分布更常见的记法是设$\\mu=\\xi/\\Vert\\xi\\Vert, \\kappa=\\Vert\\xi\\Vert, C\_{d,\\kappa}=1/Z\_{d, \\Vert\\xi\\Vert}$，从而
\\begin{equation}p(x) = C\_{d,\\kappa} e^{\\kappa\\langle\\mu,x\\rangle}\\end{equation}
这时候$\\langle\\mu,x\\rangle$就是$\\mu,x$的夹角余弦，所以说，vMF分布实际上就是以余弦相似度为度量的一种分布。由于我们经常用余弦值来度量两个向量的相似度，因此基于vMF分布做出来的模型，通常更能满足我们的这个需求。当$\\kappa=0$的时候，vMF分布是球面上的均匀分布。

从归一化因子$Z\_{d, \\Vert\\xi\\Vert}$的积分形式来看，它实际上也是vMF的母函数，从而vMF的各阶矩也可以通过$Z\_{d, \\Vert\\xi\\Vert}$来表达，比如一阶矩为
\\begin{equation}\\mathbb{E}\_{x\\sim p(x)} \[x\] = \\nabla\_{\\xi} \\log Z\_{d, \\Vert\\xi\\Vert}=\\frac{d \\log Z\_{d,\\Vert\\xi\\Vert}}{d\\Vert\\xi\\Vert}\\frac{\\xi}{\\Vert\\xi\\Vert}\\end{equation}
可以看到$\\mathbb{E}\_{x\\sim p(x)} \[x\]$在方向上跟$\\xi$一致。$Z\_{d, \\Vert\\xi\\Vert}$的精确形式可以算出来，但比较复杂，而且很多时候我们也不需要精确知道这个归一化因子，所以这里我们就不算了。

至于参数$\\kappa$的含义，或许设$\\tau=1/\\kappa$我们更好理解，此时$p(x)\\sim e^{\\langle\\mu,x\\rangle/\\tau}$，熟悉能量模型的同学都知道，这里的$\\tau$就是温度参数，如果$\\tau$越小（$\\kappa$越大），那么分布就越集中在$\\mu$附近，反之则越分散（越接近球面上的均匀分布）。因此，$\\kappa$也被形象地称为“凝聚度（concentration）”参数。

## 从vMF采样 [\#](https://kexue.fm/archives/8404\#%E4%BB%8EvMF%E9%87%87%E6%A0%B7)

对于vMF分布来说，需要解决的第一个难题是如何实现从它里边采样出具体的样本来。尤其是如果我们要将它应用到VAE中，那么这一步是至关重要的。

### 均匀分布 [\#](https://kexue.fm/archives/8404\#%E5%9D%87%E5%8C%80%E5%88%86%E5%B8%83)

最简单是$\\kappa=0$的情形，也就是$d-1$维球面上的均匀分布，因为标准正态分布本来就是各向同性的，其概率密度正比于$e^{-\\Vert x\\Vert^2/2}$只依赖于模长，所以我们只需要从$d$为标准正态分布中采样一个$z$，然后让$x=z/\\Vert z\\Vert$就得到了球面上的均匀采样结果。

### 特殊方向 [\#](https://kexue.fm/archives/8404\#%E7%89%B9%E6%AE%8A%E6%96%B9%E5%90%91)

接着，对于$\\kappa > 0$的情形，我们记$x=\[x\_1,x\_2,\\cdots,x\_d\]$，首先考虑一种特殊的情况：$\\mu = \[1, 0, \\cdots, 0\]$。事实上，由于各向同性的原因，很多时候我们都只需要考虑这个特殊情况，然后就可以平行地推广到一般情形。

此时概率密度正比于$e^{\\kappa x\_1}$，然后我们转换到球坐标系：
\\begin{equation}
\\left\\{\\begin{aligned}
x\_1 &= \\cos\\varphi\_1\\\
x\_2 &= \\sin\\varphi\_1 \\cos\\varphi\_2 \\\
x\_3 &= \\sin\\varphi\_1 \\sin\\varphi\_2 \\cos\\varphi\_3 \\\
&\\,\\,\\vdots \\\
x\_{d-1} &= \\sin\\varphi\_1 \\cdots \\sin\\varphi\_{d-2} \\cos\\varphi\_{d-1}\\\
x\_d &= \\sin\\varphi\_1 \\cdots \\sin\\varphi\_{d-2} \\sin\\varphi\_{d-1}
\\end{aligned}\\right.
\\end{equation}
那么（超球坐标的积分变换，请直接参考“ [维基百科](https://en.wikipedia.org/wiki/N-sphere)”）
\\begin{equation}\\begin{aligned}
e^{\\kappa x\_1}dS^{d-1} =& e^{\\kappa\\cos\\varphi\_1}\\sin^{d-2}\\varphi\_1 \\sin^{d-3}\\varphi\_2 \\cdots \\sin\\varphi\_{d-2} d\\varphi\_1 d\\varphi\_2 \\cdots d\\varphi\_{d-1} \\\
=& \\left(e^{\\kappa\\cos\\varphi\_1}\\sin^{d-2}\\varphi\_1 d\\varphi\_1\\right)\\left(\\sin^{d-3}\\varphi\_2 \\cdots \\sin\\varphi\_{d-2} d\\varphi\_2 \\cdots d\\varphi\_{d-1}\\right) \\\
=& \\left(e^{\\kappa\\cos\\varphi\_1}\\sin^{d-2}\\varphi\_1 d\\varphi\_1\\right)dS^{d-2} \\\
\\end{aligned}\\end{equation}
这个分解表明，从该vMF分布中采样，等价于先从概率密度正比于$e^{\\kappa\\cos\\varphi\_1}\\sin^{d-2}\\varphi\_1$的分布采样一个$\\varphi\_1$，然后从$d-2$维超球面上均匀采样一个$d-1$维向量$\\varepsilon = \[\\varepsilon\_2,\\varepsilon\_3,\\cdots,\\varepsilon\_d\]$，通过如下方式组合成最终采样结果
\\begin{equation}x = \[\\cos\\varphi\_1, \\varepsilon\_2\\sin\\varphi\_1, \\varepsilon\_3\\sin\\varphi\_1, \\cdots, \\varepsilon\_d\\sin\\varphi\_1\]\\end{equation}
设$w=\\cos\\phi\_1\\in\[-1,1\]$，那么
\\begin{equation}\\left\|e^{\\kappa\\cos\\varphi\_1}\\sin^{d-2}\\varphi\_1 d\\varphi\_1\\right\| = \\left\|e^{\\kappa w} (1-w^2)^{(d-3)/2}dw\\right\|\\end{equation}
所以我们主要研究从概率密度正比于$e^{\\kappa w} (1-w^2)^{(d-3)/2}$的分布中采样。

然而，笔者所不理解的是，大多数涉及到vMF分布的论文，都采用了1994年的论文 [《Simulation of the von mises fisher distribution》](https://www.tandfonline.com/doi/abs/10.1080/03610919408813161) 提出的基于beta分布的拒绝采样方案，整个采样流程还是颇为复杂的。但现在都2021年了，对于一维分布的采样，居然还需要拒绝采样这么低效的方案？

事实上，对于任意一维分布$p(w)$，设它的累积概率函数为$\\Phi(w)$，那么$w=\\Phi^{-1}(\\varepsilon),\\varepsilon\\sim U\[0,1\]$就是一个最方便通用的采样方案。可能有读者抗议说“累积概率函数不好算呀”、“它的逆函数更不好算呀”，但是在用代码实现采样的时候，我们压根就不需要知道$\\Phi(w)$长啥样，只要直接数值计算就行了，参考实现如下：

```
import numpy as np

def sample_from_pw(size, kappa, dims, epsilon=1e-7):
 x = np.arange(-1 + epsilon, 1, epsilon)
 y = kappa * x + np.log(1 - x**2) * (dims - 3) / 2
 y = np.cumsum(np.exp(y - y.max()))
 y = y / y[-1]
 return np.interp(np.random.random(size), y, x)
```

这里的实现中，计算量最大的是变量 `y` 的计算，而一旦计算好之后，可以缓存下来，之后只需要执行最后一步来完成采样，其速度是非常快的。这样再怎么看，也比从beta分布中拒绝采样要简单方便吧。顺便说，实现上这里还用到了一个技巧，即先计算对数值，然后减去最大值，最后才算指数，这样可以防止溢出，哪怕$\\kappa$成千上万，也可以成功计算。

### 一般情形 [\#](https://kexue.fm/archives/8404\#%E4%B8%80%E8%88%AC%E6%83%85%E5%BD%A2)

现在我们已经实现了从$\\mu=\[1,0,\\cdots,0\]$的vMF分布中采样了，我们可以将采样结果分解为
\\begin{equation}x = w\\times\\underbrace{\[1,0,\\cdots,0\]}\_{\\text{参数向量}\\mu} + \\sqrt{1-w^2}\\times\\underbrace{\[0,\\varepsilon\_2,\\cdots,\\varepsilon\_d\]}\_{\\begin{array}{c}\\text{与}\\mu\\text{正交的}d-2\\text{维}\\\ \\text{超球面均匀采样}\\end{array}}\\end{equation}
同样由于各向同性的原因，对于一般的$\\mu$，采样结果依然具有同样的形式：
\\begin{equation}\\begin{aligned}
&x = w\\mu + \\sqrt{1-w^2}\\nu\\\
&w\\sim e^{\\kappa w} (1-w^2)^{(d-3)/2}\\\
&\\nu\\sim \\text{与}\\mu\\text{正交的}d-2\\text{维超球面均匀分布}
\\end{aligned}\\end{equation}
对于$\\nu$的采样，关键之处是与$\\mu$正交，这也不难实现，先从标准正态分布中采样一个$d$维向量$z$，然后保留与$\\mu$正交的分量并归一化即可：
\\begin{equation}\\nu = \\frac{\\varepsilon - \\langle \\varepsilon,\\mu\\rangle \\mu}{\\Vert \\varepsilon - \\langle \\varepsilon,\\mu\\rangle \\mu\\Vert},\\quad \\varepsilon\\sim\\mathcal{N}(0,1\_d)\\end{equation}

## vMF-VAE [\#](https://kexue.fm/archives/8404\#vMF-VAE)

至此，我们可谓是已经完成了本篇文章最艰难的部分，剩下的构建vMF-VAE可谓是水到渠成了。vMF-VAE选用球面上的均匀分布（$\\kappa=0$）作为先验分布$q(z)$，并将后验分布选取为vMF分布：
\\begin{equation}p(z\|x) = C\_{d,\\kappa} e^{\\kappa\\langle\\mu(x),z\\rangle}\\end{equation}
简单起见，我们将$\\kappa$设为超参数（也可以理解为通过人工而不是梯度下降来更新这个参数），这样一来，$p(z\|x)$的唯一参数来源就是$\\mu(x)$了。此时我们可以计算KL散度项
\\begin{equation}\\begin{aligned}
\\int p(z\|x) \\log\\frac{p(z\|x)}{q(z)} dz =&\\, \\int C\_{d,\\kappa} e^{\\kappa\\langle\\mu(x),z\\rangle}\\left(\\kappa\\langle\\mu(x),z\\rangle + \\log C\_{d,\\kappa} - \\log C\_{d,0}\\right)dz\\\
=&\\,\\kappa\\left\\langle\\mu(x),\\mathbb{E}\_{z\\sim p(z\|x)}\[z\]\\right\\rangle + \\log C\_{d,\\kappa} - \\log C\_{d,0}
\\end{aligned}\\end{equation}
前面我们已经讨论过，vMF分布的均值方向跟$\\mu(x)$一致，模长则只依赖于$d$和$\\kappa$，所以代入上式后我们可以知道KL散度项只依赖于$d$和$\\kappa$，当这两个参数被选定之后，那么它就是一个常数（根据KL散度的性质，当$\\kappa\\neq 0$时，它必然大于0），绝对不会出现KL散度消失现象了。

那么现在就剩下重构项了，我们需要用“重参数（Reparameterization）”来完成采样并保留梯度，在前面我们已经研究了vMF的采样过程，所以也不难实现，综合的流程为：
\\begin{equation}\\begin{aligned}
&\\mathcal{L} = \\Vert x - g(z)\\Vert^2\\\
&z = w\\mu(x) + \\sqrt{1-w^2}\\nu\\\
&w\\sim e^{\\kappa w} (1-w^2)^{(d-3)/2}\\\
&\\nu=\\frac{\\varepsilon - \\langle \\varepsilon,\\mu\\rangle \\mu}{\\Vert \\varepsilon - \\langle \\varepsilon,\\mu\\rangle \\mu\\Vert}\\\
&\\varepsilon\\sim\\mathcal{N}(0,1\_d)
\\end{aligned}\\end{equation}
这里的重构loss以MSE为例，如果是句子重构，那么换用交叉熵就好。其中$\\mu(x)$就是编码器，而$g(z)$就是解码器，由于KL散度项为常数，对优化没影响，所以vMF-VAE相比于普通的自编码器，只是多了一项稍微有点复杂的重参数操作（以及人工调整$\\kappa$）而已，相比基于高斯分布的标准VAE可谓简化了不少了。

此外，从该流程我们也可以看出，除了“简单起见”之外，不将$\\kappa$设为可训练还有一个主要原因，那就是$\\kappa$关系到$w$的采样，而在$w$的采样过程中要保留$\\kappa$的梯度是比较困难的。

## 参考实现 [\#](https://kexue.fm/archives/8404\#%E5%8F%82%E8%80%83%E5%AE%9E%E7%8E%B0)

vMF-VAE的实现难度主要是重参数部分，也就还是从vMF分布中采样，而关键之处就是$w$的采样。前面我们已经给出了$w$的采样的numpy实现，但是在tf中未见类似 `np.interp` 的函数，因此不容易转换为纯tf的实现。当然，如果是torch或者tf2这种动态图框架，直接跟numpy的代码混合使用也无妨，但这里还是想构造一种比较通用的方案。

其实也不难，由于$w$只是一个一维变量，每步训练只需要用到 `batch_size` 个采样结果，所以我们完全可以事先用numpy函数采样好足够多（几十万）个$w$存好，然后训练的时候直接从这批采样好的结果随机抽就行了，参考实现如下：

```
def sampling(mu):
 """vMF分布重参数操作
 """
 dims = K.int_shape(mu)[-1]
 # 预先计算一批w
 epsilon = 1e-7
 x = np.arange(-1 + epsilon, 1, epsilon)
 y = kappa * x + np.log(1 - x**2) * (dims - 3) / 2
 y = np.cumsum(np.exp(y - y.max()))
 y = y / y[-1]
 W = K.constant(np.interp(np.random.random(10**6), y, x))
 # 实时采样w
 idxs = K.random_uniform(K.shape(mu[:, :1]), 0, 10**6, dtype='int32')
 w = K.gather(W, idxs)
 # 实时采样z
 eps = K.random_normal(K.shape(mu))
 nu = eps - K.sum(eps * mu, axis=1, keepdims=True) * mu
 nu = K.l2_normalize(nu, axis=-1)
 return w * mu + (1 - w**2)**0.5 * nu
```

一个基于MNIST的完整例子可见：

> **[https://github.com/bojone/vae/blob/master/vae\_vmf\_keras.py](https://github.com/bojone/vae/blob/master/vae_vmf_keras.py)**

至于vMF-VAE用于NLP的例子，我们日后有机会再分享。本文主要还是以理论介绍和简单演示为主～

## 文章小结 [\#](https://kexue.fm/archives/8404\#%E6%96%87%E7%AB%A0%E5%B0%8F%E7%BB%93)

本文介绍了基于vMF分布的VAE实现，其主要难度在于vMF分布的采样。总的来说，vMF分布建立在余弦相似度度量之上，在某些方面的性质更符合我们的直观认知，将其用于VAE中，能够使得KL散度项为一个常数，从而防止了KL散度消失现象，并且简化了VAE结构。

_**转载到请包括本文地址：** [https://kexue.fm/archives/8404](https://kexue.fm/archives/8404)_

_**更详细的转载事宜请参考：**_ [《科学空间FAQ》](https://kexue.fm/archives/6508#%E6%96%87%E7%AB%A0%E5%A6%82%E4%BD%95%E8%BD%AC%E8%BD%BD/%E5%BC%95%E7%94%A8)

**如果您还有什么疑惑或建议，欢迎在下方评论区继续讨论。**

**如果您觉得本文还不错，欢迎 [分享](https://kexue.fm/archives/8404#share)/ [打赏](https://kexue.fm/archives/8404#pay) 本文。打赏并非要从中获得收益，而是希望知道科学空间获得了多少读者的真心关注。当然，如果你无视它，也不会影响你的阅读。再次表示欢迎和感谢！**

打赏

微信打赏

支付宝打赏

因为网站后台对打赏并无记录，因此欢迎在打赏时候备注留言。你还可以 [**点击这里**](http://mail.qq.com/cgi-bin/qm_share?t=qm_mailme&email=tN7d1drY3drrx8H0xcWa19vZ) 或在下方评论区留言来告知你的建议或需求。

**如果您需要引用本文，请参考：**

苏剑林. (May. 17, 2021). 《变分自编码器（七）：球面上的VAE（vMF-VAE） 》\[Blog post\]. Retrieved from [https://kexue.fm/archives/8404](https://kexue.fm/archives/8404)

@online{kexuefm-8404,
        title={变分自编码器（七）：球面上的VAE（vMF-VAE）},
        author={苏剑林},
        year={2021},
        month={May},
        url={\\url{https://kexue.fm/archives/8404}},
}

分类： [信息时代](https://kexue.fm/category/Big-Data)    标签： [变分](https://kexue.fm/tag/%E5%8F%98%E5%88%86/), [无监督](https://kexue.fm/tag/%E6%97%A0%E7%9B%91%E7%9D%A3/), [vae](https://kexue.fm/tag/vae/), [生成模型](https://kexue.fm/tag/%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B/)[58 评论](https://kexue.fm/archives/8404#comments)

< [Transformer升级之路：4、二维位置的旋转式位置编码](https://kexue.fm/archives/8397) \| [也来盘点一些最近的非Transformer工作](https://kexue.fm/archives/8431) >

### 你也许还对下面的内容感兴趣

- [生成扩散模型漫谈（三十）：从瞬时速度到平均速度](https://kexue.fm/archives/10958)
- [Transformer升级之路：20、MLA究竟好在哪里？](https://kexue.fm/archives/10907)
- [生成扩散模型漫谈（二十九）：用DDPM来离散编码](https://kexue.fm/archives/10711)
- [细水长flow之TARFLOW：流模型满血归来？](https://kexue.fm/archives/10667)
- [生成扩散模型漫谈（二十八）：分步理解一致性模型](https://kexue.fm/archives/10633)
- [生成扩散模型漫谈（二十七）：将步长作为条件输入](https://kexue.fm/archives/10617)
- [生成扩散模型漫谈（二十六）：基于恒等式的蒸馏（下）](https://kexue.fm/archives/10567)
- [VQ的又一技巧：给编码表加一个线性变换](https://kexue.fm/archives/10519)
- [VQ的旋转技巧：梯度直通估计的一般推广](https://kexue.fm/archives/10489)
- [“闭门造车”之多模态思路浅谈（二）：自回归](https://kexue.fm/archives/10197)

[发表你的看法](https://kexue.fm/archives/8404#comment_form)

1. [«](https://kexue.fm/archives/8404/comment-page-1#comments)
2. [1](https://kexue.fm/archives/8404/comment-page-1#comments)
3. [2](https://kexue.fm/archives/8404/comment-page-2#comments)

kz

May 10th, 2022

e^κw \* (1−w2)^(d−3)/2并不一定是一个单调非减函数，为什么能够求得逆函数呢

[回复评论](https://kexue.fm/archives/8404/comment-page-2?replyTo=19083#respond-post-8404)

[苏剑林](https://kexue.fm) 发表于
May 10th, 2022

不是$e^{\\kappa w} (1-w^2)^{(d-3)/2}$的逆函数，是它的累积概率的逆函数，也就是$\\Phi(w)=\\int\_{-1}^w e^{\\kappa t} (1-t^2)^{(d-3)/2} dt$的逆函数。

[回复评论](https://kexue.fm/archives/8404/comment-page-2?replyTo=19087#respond-post-8404)

cyc

May 11th, 2022

$\\kappa > 0$ 并不能直接说明 (13) 式恒大于 0 吧？至少我看来并不显然。因为 $ r(\\kappa) = \\log C\_{d,\\kappa} - \\log C\_{d,0}$ 的符号并不确定。
经过我非严格验证（见r 函数的图像），在三维的情况（即$d=3$)，$\\kappa > 0$ 时，$r(\\kappa) < 0$, 因此，即使 $\\kappa >0$, (13) 式整体还是有可能等于 0 的。

要严格证明，就不得不求出 $C\_{d, \\kappa}$， 并利用函数的单调性，过程还是蛮复杂的。
由于 $\\mathbb{E}\_{z\\sim p(z\|x)}\[z\]$ 与 $\\mu(x)$ 方向一致，(13) 式可以进一步化简为：

$$
\\begin{equation\*}\\begin{aligned}
\\int p(z\|x) \\log\\frac{p(z\|x)}{q(z)} dz =&\\, \\int C\_{d,\\kappa} e^{\\kappa\\langle\\mu(x),z\\rangle}\\left(\\kappa\\langle\\mu(x),z\\rangle + \\log C\_{d,\\kappa} - \\log C\_{d,0}\\right)dz\\\
=&\\,\\kappa\\left\\langle\\mu(x),\\mathbb{E}\_{z\\sim p(z\|x)}\[z\]\\right\\rangle + \\log C\_{d,\\kappa} - \\log C\_{d,0} \\\
=&\\,\\kappa + \\log C\_{d,\\kappa} - \\log C\_{d,0}
\\end{aligned}\\end{equation\*}
$$

需要证明, 对任意 $d\\in \\mathbb{Z}\_+, \\kappa > 0$ 函数
$$
f(\\kappa) = \\kappa + \\log C\_{d,\\kappa} - \\log C\_{d,0} > 0
$$
猜想可以通过证明其在 $\\{0\\} \\cup \\mathbb{R\_+}$ 上单调递增，在 0 点处严格单调递增，以及计算其在 0 点的值大于等于 0，来进行证明。

以三维情况为例，
$$
f(\\kappa) = \\kappa + \\log C\_{3,\\kappa} - \\log C\_{3,0} = \\kappa + \\log\\dfrac{\\kappa }{2\\pi (e^\\kappa - e^{-\\kappa})} - \\log\\dfrac{1}{4\\pi}
$$
其图像如 https://www.wolframalpha.com/input?i=plot+x%2Bln%28x%2F%282\*pi+\*+%28exp%28x%29+-+exp%28-x%29%29%29%29+-+ln%281%2F%284\*pi%29%29+from+-1+to+5 所示。从图像来看，以上三点都得到满足的。我比较懒，就不严格证明了。

[回复评论](https://kexue.fm/archives/8404/comment-page-2?replyTo=19093#respond-post-8404)

[苏剑林](https://kexue.fm) 发表于
May 11th, 2022

如果要简单理解，那就是KL散度$\\int p(z\|x) \\log\\frac{p(z\|x)}{q(z)} dz\\geq 0$，当且仅当$p(z\|x)=q(z)$时取等号，而对于$p(z\|x)$有$\\kappa > 0$，对于$q(z)$有$\\kappa = 0$，两者不相等，所以只能是大于0。

不过，这只是经典分析视角。从泛函角度来看，“相等”是需要被定义的，比如$p(z\|x)$和$q(z)$不严格相等、但几乎处处相等，那么KL散度依然等于0。然而对于vMF分布来说，它的连续性很好，所以不可能存在不严格相等但又几乎处处相等的情形，因此两个$\\kappa$不等的vMF分布的KL散度是必然大于0的。

当然，严格说明这件事情还需要花费一些功夫，即要证明它们差异部分的测度大于0，不过我认为借助“KL散度等于0当且仅当两个分布等于0”这个经典认知，那么这个结论算是比较直观易懂的了。

最后补充一点就是$\\mathbb{E}\_{z\\sim p(z\|x)}\[z\]$ 只是与 $\\mu(x)$ 方向一致，但不等，所以不能得出$\\left\\langle\\mu(x),\\mathbb{E}\_{z\\sim p(z\|x)}\[z\]\\right\\rangle=1$。

[回复评论](https://kexue.fm/archives/8404/comment-page-2?replyTo=19094#respond-post-8404)

cyc 发表于
May 11th, 2022

1\. (2)到(3)式，对 $\\xi$ 做了归一化得到 $\\mu$, 因此 (3) 式中的 $\\mu$ 应该是归一化的。如文章中所述，尽管使用内积的记号，但实际上(3) 式里面的 $\\langle \\mu, x\\rangle$ 实际含义是 $\\mu$ 与 $x$ 夹角的 $\\cos$ 值。 因此 (3) 式中，$\\mu$ 位置的输入值应该是归一化的值。这样 (12) 式中 $\\mu(x)$ 也应该是归一化的才符合定义域。
2\. 由于 $z \\sim p(z\|x)$，因此每个 $z$ 都是归一化的，其期望当然也应该是归一化的, 即 $\\mathbb{E}\_{z\\sim p(z\|x)}\[z\] = 1$

综上两点，是可以证明 $\\left\\langle\\mu(x),\\mathbb{E}\_{z\\sim p(z\|x)}\[z\]\\right\\rangle=1$ 的。

最后，如果 $\\left\\langle\\mu(x),\\mathbb{E}\_{z\\sim p(z\|x)}\[z\]\\right\\rangle \\not=1$. 在小于 1 的时候，会导致$f(\\kappa) = \\kappa + \\log C\_{d,\\kappa} - \\log C\_{d,0}$ 出现负的值。 这在 3 维的时候很好验证。这显然是矛盾的。当然，大于 0 推不出矛盾。

[回复评论](https://kexue.fm/archives/8404/comment-page-2?replyTo=19095#respond-post-8404)

[苏剑林](https://kexue.fm) 发表于
May 11th, 2022

就是你说的第2点不成立，如果$\\Vert x\\Vert=\\Vert y\\Vert = 1$但$x\\neq y$，那么显然$\\Vert (x+y)/2\\Vert < 1$。

[回复评论](https://kexue.fm/archives/8404/comment-page-2?replyTo=19096#respond-post-8404)

cyc 发表于
May 11th, 2022

是的，你是对的。我想成了正态分布期望为 0 的情况了。从 Wikipedia 来看，这个期望是小于 1 的一个$\\kappa$ 和 $d$ 的函数。如果直接从 (13) 式最后的表达式证明其大于 0，的确不是一个简单的问题。谢谢！

[回复评论](https://kexue.fm/archives/8404/comment-page-2?replyTo=19099#respond-post-8404)

kz

May 12th, 2022

式（11）上面一段话，为什么z是从“标准正态分布”中采出的d维向量？不是希望v是d-2维球面上的均匀分布吗

[回复评论](https://kexue.fm/archives/8404/comment-page-2?replyTo=19102#respond-post-8404)

[苏剑林](https://kexue.fm) 发表于
May 13th, 2022

$(11)$式就是$d-2$维球面上的均匀分布。

[回复评论](https://kexue.fm/archives/8404/comment-page-2?replyTo=19105#respond-post-8404)

WangHerr

May 29th, 2022

特殊方向的参考实现里
~~~
import numpy as np

def sample\_from\_pw(size, kappa, dims, epsilon=1e-7):
x = np.arange(-1 + epsilon, 1, epsilon)
y = kappa \* x + np.log(1 - x\*\*2) \* (dims - 3) / 2
y = np.cumsum(np.exp(y - y.max()))
y = y / y\[-1\]
return np.interp(np.random.random(size), y, x)
~~~
x 是（-1，1）
可是在return里，np.random.random是\[0,1)，这样采样会不会遗漏什么？

[回复评论](https://kexue.fm/archives/8404/comment-page-2?replyTo=19191#respond-post-8404)

[苏剑林](https://kexue.fm) 发表于
May 30th, 2022

我不大清楚你想表达什么。你想说没有采样到$1$的可能性，还是说没有采样到$(-1,0)$的可能性？

如果是前者，严格等于某个数的概率是0，去掉也不影响什么；如果是后者，那就是你完全没理解原理，乱提的疑问，文章已经说得很清楚，$w$的采样方式是$w=\\Phi^{-1}(\\varepsilon),\\varepsilon\\sim U\[0,1\]$。

[回复评论](https://kexue.fm/archives/8404/comment-page-2?replyTo=19198#respond-post-8404)

语风

May 31st, 2022

苏神你好，看了您很多VAE的文章觉得受益匪浅，其中感觉最为突出的就是对于VAE“做了一件什么事”的理解。但是我发现在一些非自回归机器翻译的论文中VAE的使用动机就很让人迷惑，比如：https://arxiv.org/pdf/1908.07181v5.pdf。这些Non-Autoregression MT论文都把VAE描述为一种“最直接”的方式从而融入到网络中，但对VAE的作用机理却都未提及。
我目前的个人理解是：相对自回归机翻，非自回归机翻缺失了翻译词之间的依赖关系的捕捉，为了弥补这种缺失，NAT论文向网络中引入各种信息（比如借助外部对齐工具、NER等）和手段来弥补，而VAE就作为一种“直接获取隐变量”的手段被应用，但为什么“隐变量”能奏效却总是想不通。
另外一个关键点也想请教一下苏神看是否正确：应用于机翻的这些VAE并非原始意义上的自编码器了，它将重建loss替换成了翻译loss，或者可以理解成“在另一种语言形式上的重建”。如果确实是这样的话，那是不是VAE可以进一步扩展，不再拘泥于“重建”，用于更多的“目标”？

[回复评论](https://kexue.fm/archives/8404/comment-page-2?replyTo=19207#respond-post-8404)

[苏剑林](https://kexue.fm) 发表于
June 1st, 2022

VAE用于非自回归的理念在于“独立模型相加就不是独立了”，即非自回归模型$p(y\|x)=\\prod\\limits\_t p(y\_t\|x)$，然而如果有两个非自回归模型相见，比如$\\prod\\limits\_t p\_1(y\_t\|x)+\\prod\\limits\_t p\_2(y\_t\|x)$，那么这就不是每个token都独立的模型了。

从这个思想出发，我们可以引入随机变量$z$，使得$p(y\_t\|x)$与$z$相关，然后加权平均，即$p(y\|x)=\\int \\prod\\limits\_t p(y\_t\|x, z) q(z)dz$，这就类似VAE所做的模型假设了，后面的推导就跟VAE一样了。

其实VAE用于非自回归模型的目标也应该是重建，注意到输入$x$只是模型的条件，我们可以理解为是输入$(x,y)$来重建$y$。

[回复评论](https://kexue.fm/archives/8404/comment-page-2?replyTo=19219#respond-post-8404)

nothing2say

March 30th, 2023

关于vmf分布的期望有一点我其实比较好奇：

既然样本空间都在超球面空间上，为什么得到的期望会在球内呢（公式4前面相当于是一个scalar系数 乘上 $\\mu$）？

对应wiki：https://en.wikipedia.org/wiki/Von\_Mises–Fisher\_distribution 里面的$A\_p(\\kappa)$.

[回复评论](https://kexue.fm/archives/8404/comment-page-2?replyTo=21276#respond-post-8404)

[苏剑林](https://kexue.fm) 发表于
April 4th, 2023

这是个好问题。因为VMF的期望运算还是普通的欧氏空间的运算，并没有流形上的定义，所以没法保证加减乘除之后仍然还在流形上。

[回复评论](https://kexue.fm/archives/8404/comment-page-2?replyTo=21327#respond-post-8404)

Jeff tang

June 14th, 2023

苏老师好。 关于vae 不同 变体 选型 可以 指导下吗？

主要考虑 两方面
1、训练稳定性 避免 kl vanish
2、特征提取 效果 更好

例如
1、bn+vae
2、（vMF-VAE）
3、Nvae
等等

大道至简。
最好的 避免 kl vanish 的 方式是？
好的的 vae 架构是？如何根据优劣势 选型呢

[回复评论](https://kexue.fm/archives/8404/comment-page-2?replyTo=21974#respond-post-8404)

[苏剑林](https://kexue.fm) 发表于
June 15th, 2023

按照你这个需求，应该就是本文的vmf-vae比较好吧

[回复评论](https://kexue.fm/archives/8404/comment-page-2?replyTo=21991#respond-post-8404)

Jeff tang

June 14th, 2023

苏老师 好

在 特征提取场景（用VAE 监督学习 估计某个特征,而非还原为自己）

VAE引入了随机性（针对每个样本 都生产了专属的 Z分布）

随机性的引入。会不会 降低 特征提取质量？(特征的 精度)

对于 特征 的 偏差、方差 影响分别是？

在这个场景，选择什么类型的vae比较好？kl vanish用 哪个方法 更好呢？

谢谢

[回复评论](https://kexue.fm/archives/8404/comment-page-2?replyTo=21975#respond-post-8404)

[苏剑林](https://kexue.fm) 发表于
June 15th, 2023

vae本质还是在学习分布，vae本身也不知道特征的好坏，因为好坏取决于你下游想要做的任务。

[回复评论](https://kexue.fm/archives/8404/comment-page-2?replyTo=21992#respond-post-8404)

Jeff tang 发表于
June 17th, 2023

苏老师。我在研究 变分推理的 下界。

1、针对kl loss 有些案例 完全依赖于 两重超参微调。belta\*kl\_weight\*(kl-c\_max) 约束最终kl大小（c\_max） 和 调整值域（kl\_weight 和 belta）
2、c\_max 最优下界是？
各种变分 貌似 有 理论推导的 最优 变分下界。这块儿 完全手动调（成本高），还是用什么理论kl下界 更好？
3、1中 调整值域 （kl\_weight 和 belta） 目的是 让和重建误差 在优化时 控制 优先级。
step 1）根据具体任务 看 重建误差的值域范围。
step 2）调整kl\_weight 让 kl 值域 和 重建误差相近
step 3) 用退火 控制 初七 kl loss=0 后期逐渐增大
我的理解是否有误，苏老师建议是？
4、预热 anneal 是否有必要用？单调增长 还是 不断循环往复的 哪种更好？
5、现在vae 有没有 更自动化调整的方案？
1）anneal 2) c\_max 4) kl\_weight 5)belta 等

苏老师如果方便，可以加Q (942152145) 我在研究变分推理 各个变体时，还有很多思考想和老师探讨

[回复评论](https://kexue.fm/archives/8404/comment-page-2?replyTo=22006#respond-post-8404)

[苏剑林](https://kexue.fm) 发表于
June 19th, 2023

我好像不大了解你这个话题，很抱歉～

[回复评论](https://kexue.fm/archives/8404/comment-page-2?replyTo=22016#respond-post-8404)

龙行

August 2nd, 2023

为了看懂这篇专门去看来VMF分布，苏神这里采用了很多近似吧？
1 采用插值采样来模拟一个维度的VMF分布采样，插值肯定是有误差的。
2 采用一个维度的采样扩展到多个维度，我理解数学上是没问题的，但是ANN算出来的mu一定是一个维度的吗？可以通过施密特正交化扩展到多个维度吗？
3 还是（11）式，这里的eps采用正态分布模拟，如果vmf分布是wrraped 正态或许可以（我不确定），但是vmf分布只是多维球正态分布的近似，所以这里eps正态分布采样应该也是近似。这或许是别人依然用老方法采用的原因？
4 代码实现中只有一个交叉熵，是不是缺少了KL散度项，即输出和隐变量的内积项？

[回复评论](https://kexue.fm/archives/8404/comment-page-2?replyTo=22395#respond-post-8404)

龙行 发表于
August 2nd, 2023

对，这里还有一个问题，如果假设为vmf分布只是为了让KL散度项不为0，那好像完全没必要做这种改进。一方面，我理解正则项就是防止KL为0的，另一方面，苏神你自己也在NVAE那篇博文中提到了，为了计算KL散度项，假设其中三项为高斯分布，就导致了KL散度不会为0。

[回复评论](https://kexue.fm/archives/8404/comment-page-2?replyTo=22396#respond-post-8404)

[苏剑林](https://kexue.fm) 发表于
August 4th, 2023

我不清楚有哪个正则项防止KL为0

[回复评论](https://kexue.fm/archives/8404/comment-page-2?replyTo=22417#respond-post-8404)

[苏剑林](https://kexue.fm) 发表于
August 4th, 2023

1、采样算法理论上是精确的，要不然照你这样说，任何数值算法都有误差了；

2、没看懂你说什么；

3、同样看不懂你说什么，老方法改变的只是$w$的采样方式，其他都是一致的；

4、KL散度是常数。

[回复评论](https://kexue.fm/archives/8404/comment-page-2?replyTo=22416#respond-post-8404)

zhongkk

August 21st, 2023

苏老师，我刚接触这个方向，请问，d维的特征，为啥是定义d-1维的超球面来进行研究？

[回复评论](https://kexue.fm/archives/8404/comment-page-2?replyTo=22551#respond-post-8404)

[苏剑林](https://kexue.fm) 发表于
August 25th, 2023

它还是$d$维的向量，只不过全体集合构成的是$d-1$维的超球面。很多情况下数据分布都只是高维空间中的一个低维子流形，所以这种约束通常都不会有什么大问题。

[回复评论](https://kexue.fm/archives/8404/comment-page-2?replyTo=22568#respond-post-8404)

1. [«](https://kexue.fm/archives/8404/comment-page-1#comments)
2. [1](https://kexue.fm/archives/8404/comment-page-1#comments)
3. [2](https://kexue.fm/archives/8404/comment-page-2#comments)

[取消回复](https://kexue.fm/archives/8404#respond-post-8404)

你的大名

电子邮箱

个人网站（选填）

1\. 可以使用LaTeX代码，点击“预览效果”可查看效果；2. 可以通过点击评论楼层编号来引用该楼层；3. 网站可能会有点卡，如非确认评论失败，请 **不要重复点击提交**。

### 内容速览

[KL散度消失](https://kexue.fm/archives/8404#KL%E6%95%A3%E5%BA%A6%E6%B6%88%E5%A4%B1)
[vMF分布](https://kexue.fm/archives/8404#vMF%E5%88%86%E5%B8%83)
[从vMF采样](https://kexue.fm/archives/8404#%E4%BB%8EvMF%E9%87%87%E6%A0%B7)
[均匀分布](https://kexue.fm/archives/8404#%E5%9D%87%E5%8C%80%E5%88%86%E5%B8%83)
[特殊方向](https://kexue.fm/archives/8404#%E7%89%B9%E6%AE%8A%E6%96%B9%E5%90%91)
[一般情形](https://kexue.fm/archives/8404#%E4%B8%80%E8%88%AC%E6%83%85%E5%BD%A2)
[vMF-VAE](https://kexue.fm/archives/8404#vMF-VAE)
[参考实现](https://kexue.fm/archives/8404#%E5%8F%82%E8%80%83%E5%AE%9E%E7%8E%B0)
[文章小结](https://kexue.fm/archives/8404#%E6%96%87%E7%AB%A0%E5%B0%8F%E7%BB%93)

### 智能搜索

支持整句搜索！网站自动使用 [结巴分词](https://github.com/fxsjy/jieba) 进行分词，并结合ngrams排序算法给出合理的搜索结果。

### 热门标签

[生成模型](https://kexue.fm/tag/%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B/) [attention](https://kexue.fm/tag/attention/) [优化](https://kexue.fm/tag/%E4%BC%98%E5%8C%96/) [语言模型](https://kexue.fm/tag/%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/) [模型](https://kexue.fm/tag/%E6%A8%A1%E5%9E%8B/) [网站](https://kexue.fm/tag/%E7%BD%91%E7%AB%99/) [概率](https://kexue.fm/tag/%E6%A6%82%E7%8E%87/) [梯度](https://kexue.fm/tag/%E6%A2%AF%E5%BA%A6/) [转载](https://kexue.fm/tag/%E8%BD%AC%E8%BD%BD/) [微分方程](https://kexue.fm/tag/%E5%BE%AE%E5%88%86%E6%96%B9%E7%A8%8B/) [矩阵](https://kexue.fm/tag/%E7%9F%A9%E9%98%B5/) [天象](https://kexue.fm/tag/%E5%A4%A9%E8%B1%A1/) [分析](https://kexue.fm/tag/%E5%88%86%E6%9E%90/) [深度学习](https://kexue.fm/tag/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/) [积分](https://kexue.fm/tag/%E7%A7%AF%E5%88%86/) [python](https://kexue.fm/tag/python/) [优化器](https://kexue.fm/tag/%E4%BC%98%E5%8C%96%E5%99%A8/) [力学](https://kexue.fm/tag/%E5%8A%9B%E5%AD%A6/) [无监督](https://kexue.fm/tag/%E6%97%A0%E7%9B%91%E7%9D%A3/) [扩散](https://kexue.fm/tag/%E6%89%A9%E6%95%A3/) [几何](https://kexue.fm/tag/%E5%87%A0%E4%BD%95/) [节日](https://kexue.fm/tag/%E8%8A%82%E6%97%A5/) [生活](https://kexue.fm/tag/%E7%94%9F%E6%B4%BB/) [文本生成](https://kexue.fm/tag/%E6%96%87%E6%9C%AC%E7%94%9F%E6%88%90/) [数论](https://kexue.fm/tag/%E6%95%B0%E8%AE%BA/)

### 随机文章

- [美华裔教授破百年物理定律 获国际同行喝彩(图)](https://kexue.fm/archives/52)
- [GlobalPointer：用统一的方式处理嵌套和非嵌套NER](https://kexue.fm/archives/8373)
- [SPACES：“抽取-生成”式长文本摘要（法研杯总结）](https://kexue.fm/archives/8046)
- [积分梯度：一种新颖的神经网络可视化方法](https://kexue.fm/archives/7533)
- [备忘：椭圆坐标与复三角函数](https://kexue.fm/archives/1314)
- [科学空间：2011年1月重要天象](https://kexue.fm/archives/1148)
- [基于CNN和序列标注的对联机器人](https://kexue.fm/archives/6270)
- [《虚拟的实在(2)》——为什么引力如此复杂？](https://kexue.fm/archives/2004)
- [历史上的谜案——刘徽有没有使用外推法？](https://kexue.fm/archives/1292)
- [“用户评价”靠谱吗？](https://kexue.fm/archives/894)

### 最近评论

- [盏一](https://kexue.fm/archives/8397/comment-page-3#comment-27910): 哦哦哦 你是说 $\\exp nB$ 是正交矩阵! 并不是说 B.
- [盏一](https://kexue.fm/archives/8397/comment-page-3#comment-27909): 呃, 是我脑子乱了... 忘了 $\\exp(0) = I$. 所以只要 $\\Vert B^T+...
- [盏一](https://kexue.fm/archives/8397/comment-page-3#comment-27908): 呃, 是我脑子乱了... 忘了 $\\exp(0) = I$. 所以只要 $\\Vert B^T+...
- [盏一](https://kexue.fm/archives/8397/comment-page-3#comment-27907): 苏神, 请教一下\> 并且还可以证明它一定是正交矩阵是怎么证明的. 我本来以为隐式利用了 $\\V...
- [sk](https://kexue.fm/archives/8265/comment-page-8#comment-27906): 请问公式14是怎么得出来的？
- [tll1945tll1937](https://kexue.fm/archives/10266/comment-page-1#comment-27901): 真心实意的向大家请教问题：看了文章“对齐全量微调！这是我看过最精彩的LoRA改进（二）”，我实...
- [oYo\_logan](https://kexue.fm/archives/10757/comment-page-1#comment-27897): \[comment=27017\]苏剑林\[/comment\]苏神，想请教一下，我理解在一个batc...
- [z123](https://kexue.fm/archives/10592/comment-page-1#comment-27896): 在参数矩阵较多的CNN小模型上，Muon会明显慢于Adam，这方面有什么优化提速的方案吗？
- [dry](https://kexue.fm/archives/10958/comment-page-2#comment-27895): 苏神好，一直有个疑问，ReFlow构建的ODE是$dx\_t/dt=x\_1-x\_0$，为什么这并...
- [tyj](https://kexue.fm/archives/10958/comment-page-2#comment-27894): 感觉和之前的一篇文章很像，应该算是concurrent work： https://arxiv...

### 友情链接

- [Cool Papers](https://papers.cool)
- [数学研发](https://bbs.emath.ac.cn)
- [Seatop](http://www.seatop.com.cn/)
- [Xiaoxia](https://xiaoxia.org/)
- [积分表-网络版](https://kexue.fm/sci/integral/index.html)
- [丝路博傲](http://blog.dvxj.com/)
- [ph4ntasy 饭特稀](http://www.ph4ntasy.com/)
- [数学之家](http://www.2math.cn/)
- [有趣天文奇观](http://interesting-sky.china-vo.org/)
- [TwistedW](http://www.twistedwg.com/)
- [godweiyang](https://godweiyang.com/)
- [AI柠檬](https://blog.ailemon.net/)
- [王登科-DK博客](https://greatdk.com)
- [ESON](https://blog.eson.org/)
- [枫之羽](https://fzhiy.net/)
- [Mathor's blog](https://wmathor.com/)
- [coding-zuo](https://coding-zuo.github.io/)
- [博科园](https://www.bokeyuan.net/)
- [孔皮皮的博客](https://www.kppkkp.top/)
- [运鹏的博客](https://yunpengtai.top/)
- [jiming.site](https://jiming.site/)
- [OmegaXYZ](https://www.omegaxyz.com/)
- [Blog by Eacls](https://www.eacls.top/)
- [EAI猩球](https://www.robotech.ink/)
- [文举的博客](https://liwenju0.com/)
- [用代码打点酱油](https://bruceyuan.com/)
- [申请链接](https://kexue.fm/links.html)

本站采用创作共用版权协议，要求署名、非商业用途和保持一致。转载本站内容必须也遵循“ [署名-非商业用途-保持一致](http://creativecommons.org/licenses/by-nc-nd/2.5/cn/)”的创作共用协议。
© 2009-2025 Scientific Spaces. All rights reserved. Theme by [laogui](http://www.laogui.com). Powered by [Typecho](http://typecho.org). 备案号: [粤ICP备09093259号-1/2](https://beian.miit.gov.cn/)。