![MobileSideBar](https://kexue.fm/usr/themes/geekg/images/slide-button.png)

## SEARCH

## MENU

- [打赏](https://kexue.fm/reward.html)
- [公式](https://kexue.fm/latex.html)
- [天象](https://kexue.fm/ac.html)
- [链接](https://kexue.fm/links.html)
- [时光](https://kexue.fm/me.html)
- [博览](https://kexue.fm/science.html)
- [归档](https://kexue.fm/content.html)

## CATEGORIES

- [千奇百怪](https://kexue.fm/category/Everything)
- [天文探索](https://kexue.fm/category/Astronomy)
- [数学研究](https://kexue.fm/category/Mathematics)
- [物理化学](https://kexue.fm/category/Phy-chem)
- [信息时代](https://kexue.fm/category/Big-Data)
- [生物自然](https://kexue.fm/category/Biology)
- [图片摄影](https://kexue.fm/category/Photograph)
- [问题百科](https://kexue.fm/category/Questions)
- [生活/情感](https://kexue.fm/category/Life-Feeling)
- [资源共享](https://kexue.fm/category/Resources)

## NEWPOSTS

- [生成扩散模型漫谈（二十九）：用DD...](https://kexue.fm/archives/10711)
- [MoE环游记：1、从几何意义出发](https://kexue.fm/archives/10699)
- [三个球的交点坐标（三球交会定位）](https://kexue.fm/archives/10684)
- [细水长flow之TARFLOW：流...](https://kexue.fm/archives/10667)
- [低秩近似之路（五）：CUR](https://kexue.fm/archives/10662)
- [为什么梯度裁剪的默认模长是1？](https://kexue.fm/archives/10657)
- [从谱范数梯度到新式权重衰减的思考](https://kexue.fm/archives/10648)
- [生成扩散模型漫谈（二十八）：分步理...](https://kexue.fm/archives/10633)
- [生成扩散模型漫谈（二十七）：将步长...](https://kexue.fm/archives/10617)
- [Muon优化器赏析：从向量到矩阵的...](https://kexue.fm/archives/10592)

## COMMENTS

- [zzdpaaa: 苏老师好，我自己试着推导了一下DDIM的公式，但遇到了一些问题...](https://kexue.fm/archives/9181/comment-page-5#comment-26613)
- [lljbash: 模型并不是一个字一个字思考，而是每次只讲出来一个字。](https://kexue.fm/archives/7430/comment-page-2#comment-26610)
- [Astrologos: 感觉和Toy Models of Superposition讲的有关](https://kexue.fm/archives/8679/comment-page-2#comment-26608)
- [zehua: 是否可以理解为单层 attention 在计算 $y\_n$ 时...](https://kexue.fm/archives/10347/comment-page-1#comment-26607)
- [jumbo: https://spaces.ac.cn/archives/1...](https://kexue.fm/archives/10091/comment-page-5#comment-26606)
- [kasd: 补充：n我理解是hidden\_size，d是head\_dim](https://kexue.fm/archives/9529/comment-page-2#comment-26603)
- [kasd: 既然分成多个head后会使得d远小于n，导致低秩问题。那么使用...](https://kexue.fm/archives/9529/comment-page-2#comment-26602)
- [报告大王: 感谢大佬，这个分析直击核心，consistency model...](https://kexue.fm/archives/10633/comment-page-1#comment-26601)
- [loki\_ccfa: 感觉是有点像additive quantization？htt...](https://kexue.fm/archives/10699/comment-page-1#comment-26594)
- [西柚汁: 苏老师您好，在最后迁移学习的部分，不够句子最大长度的标签部分，...](https://kexue.fm/archives/4118/comment-page-1#comment-26593)

## USERLOGIN

- [登录](https://kexue.fm/admin/login.php)

[科学空间\|Scientific Spaces](https://kexue.fm)

- [登录](https://kexue.fm/admin/login.php)
- [打赏](https://kexue.fm/reward.html)
- [公式](https://kexue.fm/latex.html)
- [天象](https://kexue.fm/ac.html)
- [链接](https://kexue.fm/links.html)
- [时光](https://kexue.fm/me.html)
- [博览](https://kexue.fm/science.html)
- [归档](https://kexue.fm/content.html)

渴望成为一个小飞侠

- [![](https://kexue.fm/usr/themes/geekg/images/rss.png)\
\
欢迎订阅](https://kexue.fm/feed)
- [![](https://kexue.fm/usr/themes/geekg/images/mail.png)\
\
个性邮箱](https://kexue.fm/archives/119)
- [![](https://kexue.fm/usr/themes/geekg/images/Saturn.png)\
\
天象信息](https://kexue.fm/ac.html)
- [![](https://kexue.fm/usr/themes/geekg/images/iss.png)\
\
观测ISS](https://kexue.fm/archives/41)
- [![](https://kexue.fm/usr/themes/geekg/images/pi.png)\
\
LaTeX](https://kexue.fm/latex.html)
- [![](https://kexue.fm/usr/themes/geekg/images/mlogo.png)\
\
关于博主](https://kexue.fm/me.html)

欢迎访问“科学空间”，这里将与您共同探讨自然科学，回味人生百态；也期待大家的分享～

- [**千奇百怪** Everything](https://kexue.fm/category/Everything)
- [**天文探索** Astronomy](https://kexue.fm/category/Astronomy)
- [**数学研究** Mathematics](https://kexue.fm/category/Mathematics)
- [**物理化学** Phy-chem](https://kexue.fm/category/Phy-chem)
- [**信息时代** Big-Data](https://kexue.fm/category/Big-Data)
- [**生物自然** Biology](https://kexue.fm/category/Biology)
- [**图片摄影** Photograph](https://kexue.fm/category/Photograph)
- [**问题百科** Questions](https://kexue.fm/category/Questions)
- [**生活/情感** Life-Feeling](https://kexue.fm/category/Life-Feeling)
- [**资源共享** Resources](https://kexue.fm/category/Resources)

- [**千奇百怪**](https://kexue.fm/category/Everything)
- [**天文探索**](https://kexue.fm/category/Astronomy)
- [**数学研究**](https://kexue.fm/category/Mathematics)
- [**物理化学**](https://kexue.fm/category/Phy-chem)
- [**信息时代**](https://kexue.fm/category/Big-Data)
- [**生物自然**](https://kexue.fm/category/Biology)
- [**图片摄影**](https://kexue.fm/category/Photograph)
- [**问题百科**](https://kexue.fm/category/Questions)
- [**生活/情感**](https://kexue.fm/category/Life-Feeling)
- [**资源共享**](https://kexue.fm/category/Resources)

[首页](https://kexue.fm) [信息时代](https://kexue.fm/category/Big-Data) MoE环游记：1、从几何意义出发

8Feb

# [MoE环游记：1、从几何意义出发](https://kexue.fm/archives/10699)

By 苏剑林 \|
2025-02-08 \|
9697位读者\|

前两年福至心灵之下，开了一个“ [Transformer升级之路](https://kexue.fm/search/Transformer%E5%8D%87%E7%BA%A7%E4%B9%8B%E8%B7%AF/)”系列，陆续分享了主流Transformer架构的一些改进工作和个人思考，得到了部份读者的认可。这篇文章开始，我们沿着同样的风格，介绍当前另一个主流架构MoE（Mixture of Experts）。

MoE的流行自不必多说，近来火出圈的 [DeepSeek-V3](https://papers.cool/arxiv/2412.19437) 便是MoE架构，传言GPT-4也是MoE架构，国内最近出的一些模型也有不少用上了MoE。然而，虽然MoE的研究由来已久，但其应用长时间内都不愠不火，大致上是从去年初的 [《Mixtral of Experts》](https://papers.cool/arxiv/2401.04088) 开始，MoE才逐渐吸引大家的注意力，其显著优点是参数量大，但训练和推理成本都显著低。

但同时MoE也有一些难题，如训练不稳定、负载不均衡、效果不够好等，这也是它早年没有流行起来的主要原因。不过随着这两年关注度的提升，这些问题在很大程度上已经得到解决，我们在接下来的介绍中会逐一谈到这些内容。

## 问题定义 [\#](https://kexue.fm/archives/10699\#%E9%97%AE%E9%A2%98%E5%AE%9A%E4%B9%89)

首先要指出的是，这里会用笔者自己的一种理解思路来介绍MoE，在必要的地方会附上相应的参考文献，但不会对MoE架构进行系统的追根溯源，还请读者见谅。

我们知道，Transformer模型由Attention层和MLP层组成，MoE替换的是模型中MLP层。MLP层又分FFN（FeedForward Network）和GLU（Gated Linear Unit）两种，主流的是GLU，但简单起见我们还是以FFN为例

\\begin{equation}\\boldsymbol{y} = f(\\boldsymbol{x}\\boldsymbol{W}^{(A)})\\boldsymbol{W}^{(B)}\\end{equation}

其中$\\boldsymbol{x}\\in\\mathbb{R}^{d}$是输入向量（行向量），$\\boldsymbol{W}^{(A)}\\in\\mathbb{R}^{d\\times D},\\boldsymbol{W}^{(B)}\\in\\mathbb{R}^{D\\times d}$是两个参数矩阵，$f$是Element-wise的激活函数。设$n$是一个能整除$D$的整数，那么上述可以等价地用分块矩阵写成

\\begin{equation}\\boldsymbol{y} = f\\big(\\boldsymbol{x}\\begin{bmatrix}\\boldsymbol{W}^{(A)}\_1 & \\boldsymbol{W}^{(A)}\_2 & \\cdots & \\boldsymbol{W}^{(A)}\_n\\end{bmatrix}\\big)\\begin{bmatrix}\\boldsymbol{W}^{(B)}\_1 \\\ \\boldsymbol{W}^{(B)}\_2 \\\ \\vdots \\\ \\boldsymbol{W}^{(B)}\_n\\end{bmatrix} = \\sum\_{i=1}^n \\underbrace{f(\\boldsymbol{x}\\boldsymbol{W}^{(A)}\_i)\\boldsymbol{W}^{(B)}\_i}\_{\\boldsymbol{v}\_i}\\end{equation}

其中$\\boldsymbol{W}^{(A)}\_i = \\boldsymbol{W}^{(A)}\_{\[:,(i-1)c:ic\]}, \\boldsymbol{W}^{(B)}\_i = \\boldsymbol{W}^{(B)}\_{\[(i-1)c:ic,:\]},c= D/n$，这里的切片按照Python规则来。由此可见，FFN可以等价表示成$n$个向量$\\boldsymbol{v}\_1,\\boldsymbol{v}\_2,\\cdots,\\boldsymbol{v}\_n$之和，每个向量代表了一个小模型$f(\\boldsymbol{x}\\boldsymbol{W}^{(A)}\_i)\\boldsymbol{W}^{(B)}\_i$的输出，每个小模型计算量相同，这些小模型就是MoE中的“Expert”。

MoE提出的问题是：

> 能否只挑$k$个向量的和来逼近$n$个向量的和呢？这样就可以将计算量降低到$k/n$了。

## 模长排序 [\#](https://kexue.fm/archives/10699\#%E6%A8%A1%E9%95%BF%E6%8E%92%E5%BA%8F)

这个问题其实我们在 [《低秩近似之路（三）：CR》](https://kexue.fm/archives/10427) 已经探究过，写成数学公式是

\\begin{equation}\\mathop{\\text{argmin}}\_{\\lambda\_1,\\lambda\_2,\\cdots,\\lambda\_n\\in\\{0,1\\}}\\left\\Vert\\sum\_{i=1}^n \\lambda\_i \\boldsymbol{v}\_i - \\sum\_{i=1}^n\\boldsymbol{v}\_i\\right\\Vert^2\\quad\\text{s.t.}\\quad \\sum\_{i=1}^n \\lambda\_i = k\\end{equation}

记$\\gamma\_i = 1 - \\lambda\_i$，那么它又可以写成

\\begin{equation}\\mathop{\\text{argmin}}\_{\\gamma\_1,\\gamma\_2,\\cdots,\\gamma\_n\\in\\{0,1\\}}\\left\\Vert\\sum\_{i=1}^n \\gamma\_i \\boldsymbol{v}\_i\\right\\Vert^2\\quad\\text{s.t.}\\quad \\sum\_{i=1}^n \\gamma\_i = n - k\\end{equation}

这个问题的精确求解是比较困难的，但有一个简单的近似解：当$\\boldsymbol{v}\_i$两两正交时，我们有

\\begin{equation}\\left\\Vert\\sum\_{i=1}^n \\gamma\_i \\boldsymbol{v}\_i\\right\\Vert^2 = \\sum\_{i=1}^n \\gamma\_i^2 \\Vert\\boldsymbol{v}\_i\\Vert^2 = \\sum\_{i=1}^n \\gamma\_i \\Vert\\boldsymbol{v}\_i\\Vert^2\\end{equation}

上式最优解显然就是让模长$\\Vert\\boldsymbol{v}\_i\\Vert$最小的$n-k$个$\\gamma\_i$等于1，这又等价于说挑出模长最大的$k$个向量来逼近$n$个向量之和。当$\\boldsymbol{v}\_i$不满足两两正交的条件时，我们依然用它来作为一个近似解。它的几何意义也很直观，模长越大的向量，在求和过程中越不容易被抵消，从而作用越突出。

此外，在 [《低秩近似之路（三）：CR》](https://kexue.fm/archives/10427) 中我们还讨论了一种依概率采样的逼近过程，在方差最小的假设下得到的最优采样概率同样有正比于模长的特点，所以总的来说按向量模长排序是一个简单但不失有效的策略。

## MoE初现 [\#](https://kexue.fm/archives/10699\#MoE%E5%88%9D%E7%8E%B0)

现在策略已经有了——“挑模长最大的$k$个向量”——可是细想之下我们会发现它并不实用：要挑模长最大的$k$个向量，就得把所有向量的模长都算出来，这又意味着要把所有的$\\boldsymbol{v}\_i$先算出来，可我们的原本目的却是减少$\\boldsymbol{v}\_i$的计算量！

为了解决这个矛盾，我们需要重新设计每个Expert模型，使得它的模长可以低成本地计算出来。什么意思呢？首先我们将$\\boldsymbol{v}\_i$归一化得到$\\boldsymbol{e}\_i = \\boldsymbol{v}\_i/\\Vert\\boldsymbol{v}\_i\\Vert$，这样每个$\\boldsymbol{e}\_i$的模长都相同了。接着我们定义

\\begin{equation}\\underbrace{\[p\_1,p\_2,\\cdots,p\_n\]}\_{\\boldsymbol{p}} = h(\\boldsymbol{x}\\cdot\\boldsymbol{W}^{(R)})\\quad\\in\\mathbb{R}\_{\\geq 0}^n\\end{equation}

其中$\\boldsymbol{W}^{(R)}\\in\\mathbb{R}^{d\\times n}$是参数矩阵，$h(\\cdot)$是一个$\\mathbb{R}\\to\\mathbb{R}\_{\\geq 0}$的激活函数，说白了这就是一个$d$维到$n$维的线性变换加激活函数，所以计算量是比较小的，这部分模型在MoE中被称为“Router”。

$\\boldsymbol{p}$的作用是什么呢？预测每个Expert的模长！换言之，我们将$p\_i$作为第$i$个Expert的模长，$p\_i \\boldsymbol{e}\_i$才是完整的Expert，它被分解为两部分：计算量比较小的模长$p\_i$以及计算量比较大的方向$\\boldsymbol{e}\_i$。为了减少计算量，我们先计算出$\\boldsymbol{p}$，挑出最大的$k$个后才去计算相应的$\\boldsymbol{e}\_i$，最后乘上$p\_i$并求和：

\\begin{equation}\\boldsymbol{y} = \\sum\_{i\\in \\mathop{\\text{argtop}}\_k \\boldsymbol{p}} p\_i \\boldsymbol{e}\_i\\end{equation}

这便是MoE模型的基本公式。由于计算中只保留了Top-$k$部分，所以它本质上属于一种Sparse模型，而原本的FFN或者$k=n$时的模型，通常称为对应的Dense模型。

## 思路概括 [\#](https://kexue.fm/archives/10699\#%E6%80%9D%E8%B7%AF%E6%A6%82%E6%8B%AC)

不管是熟悉MoE还是不熟悉MoE的读者，可能都会对上述过程有点陌生，因为这是笔者自己闭门造车的一种MoE理解路线，但因为其几何意义更明确，所以本质上应该是更好理解的。

我们再来整理一下整个思路：

> 1、一个常规的Dense模型FFN，可以等价改写为$n$个Expert向量$\\boldsymbol{v}\_1,\\boldsymbol{v}\_2,\\cdots,\\boldsymbol{v}\_n$之和；
>
> 2、为了节省计算量，我们试图挑出$k$个向量求和来逼近原本的$n$个向量之和；
>
> 3、转化为数学问题求解后，我们发现挑选规则是模长最大的$k$个向量；
>
> 4、直接去算$n$个Expert的模长然后选$k$个实际上是不省计算量的，所以要重新设计Expert；
>
> 5、将$\\boldsymbol{v}\_i$归一化得到$\\boldsymbol{e}\_i$，然后用另外的小模型（Router）预测模长$p\_i$，最终的Expert为$p\_i \\boldsymbol{e}\_i$；
>
> 6、此时，我们就可以先算全体$p\_i$，挑出$k$个后才去计算$\\boldsymbol{e}\_i$，达到节省计算量的目的。

## 为何如此 [\#](https://kexue.fm/archives/10699\#%E4%B8%BA%E4%BD%95%E5%A6%82%E6%AD%A4)

可能有些读者疑问，为什么要做这个看似复杂的过程？原本的MoE不是挺好理解的吗？一般的MoE形式为

\\begin{equation}\\boldsymbol{y} = \\sum\_{i\\in \\mathop{\\text{argtop}}\_k \\boldsymbol{p}} p\_i \\boldsymbol{v}\_i\\end{equation}

也就是求和前少了对$\\boldsymbol{v}\_i$的归一化，此时$p\_i$也没有模长的意义，它纯粹是一个用来对Expert排序的打分模型（即Router）。可为什么将$p\_i$乘到Expert上去就能让Router学会正确排序Expert呢？笔者发现只有 [《Sparse Backpropagation for MoE Training》](https://papers.cool/arxiv/2310.00811) 对此给出了一个解释，但还是稍欠直观。

而在本文的几何视角下，我们会发现很多问题就“豁然开朗”了。我们将Expert重新参数化为$p\_i \\boldsymbol{e}\_i$后，Dense模型对应于全体$p\_i \\boldsymbol{e}\_i$求和，而MoE对应于$p\_i$选Top-$k$后求和，这是Dense模型的一个有理论保证的逼近。我们没有去考虑Router如何选择Expert，只是每一步都尽可能逼近Dense模型，这可以说是 **既要** 大参数、 **又要** 小计算量的最佳选择。

现在$p\_i$的几何意义是模长而不是概率，所以激活函数$h(\\cdot)$就没有归一化的要求了，除了Softmax外，像Sigmoid、ReLU都可以考虑使用，也可以考虑我们在 [《Softmax后传：寻找Top-K的光滑近似》](https://kexue.fm/archives/10373) 介绍的Top-$k$光滑近似。Router使用非归一化的激活函数，有助于避免$k > 1$时Expert之间的恶性竞争，有时候能取得更好的效果。

最后补充一点，我们前面定义$\\boldsymbol{e}\_i = \\boldsymbol{v}\_i/ \\Vert\\boldsymbol{v}\_i\\Vert$，目的是让所有$\\boldsymbol{e}\_i$模长相同，实际操作中不是一定要L2 Normalize，也可以是其他等价操作，比如gamma参数恒等于1的RMS Norm，它更符合我们的输出习惯。

## 文章小结 [\#](https://kexue.fm/archives/10699\#%E6%96%87%E7%AB%A0%E5%B0%8F%E7%BB%93)

本文从Dense模型的最佳逼近出发来推导和理解MoE，得到了一种特定的MoE形式，它比现有MoE多了一个Normalize步骤，但能让MoE的几何意义更加明显。当然，不管Normalize与否，MoE之路都只是刚刚开始，更多的困难还在路上。

_**转载到请包括本文地址：** [https://kexue.fm/archives/10699](https://kexue.fm/archives/10699)_

_**更详细的转载事宜请参考：**_ [《科学空间FAQ》](https://kexue.fm/archives/6508#%E6%96%87%E7%AB%A0%E5%A6%82%E4%BD%95%E8%BD%AC%E8%BD%BD/%E5%BC%95%E7%94%A8)

**如果您还有什么疑惑或建议，欢迎在下方评论区继续讨论。**

**如果您觉得本文还不错，欢迎 [分享](https://kexue.fm/archives/10699#share)/ [打赏](https://kexue.fm/archives/10699#pay) 本文。打赏并非要从中获得收益，而是希望知道科学空间获得了多少读者的真心关注。当然，如果你无视它，也不会影响你的阅读。再次表示欢迎和感谢！**

打赏

![科学空间](https://kexue.fm/usr/themes/geekg/payment/wx.png)

微信打赏

![科学空间](https://kexue.fm/usr/themes/geekg/payment/zfb.png)

支付宝打赏

因为网站后台对打赏并无记录，因此欢迎在打赏时候备注留言。

你还可以 [**点击这里**](http://mail.qq.com/cgi-bin/qm_share?t=qm_mailme&email=tN7d1drY3drrx8H0xcWa19vZ) 或在下方评论区留言来告知你的建议或需求。

**如果您需要引用本文，请参考：**

苏剑林. (Feb. 08, 2025). 《MoE环游记：1、从几何意义出发 》\[Blog post\]. Retrieved from [https://kexue.fm/archives/10699](https://kexue.fm/archives/10699)

@online{kexuefm-10699,

        title={MoE环游记：1、从几何意义出发},

        author={苏剑林},

        year={2025},

        month={Feb},

        url={\\url{https://kexue.fm/archives/10699}},

}

分类： [信息时代](https://kexue.fm/category/Big-Data)    标签： [模型](https://kexue.fm/tag/%E6%A8%A1%E5%9E%8B/), [几何](https://kexue.fm/tag/%E5%87%A0%E4%BD%95/), [稀疏](https://kexue.fm/tag/%E7%A8%80%E7%96%8F/), [moe](https://kexue.fm/tag/moe/)[22 评论](https://kexue.fm/archives/10699#comments)

< [三个球的交点坐标（三球交会定位）](https://kexue.fm/archives/10684) \| [生成扩散模型漫谈（二十九）：用DDPM来离散编码](https://kexue.fm/archives/10711) >

### 你也许还对下面的内容感兴趣

- [三个球的交点坐标（三球交会定位）](https://kexue.fm/archives/10684)
- [Monarch矩阵：计算高效的稀疏型矩阵分解](https://kexue.fm/archives/10249)
- [注意力机制真的可以“集中注意力”吗？](https://kexue.fm/archives/9889)
- [基于量子化假设推导模型的尺度定律（Scaling Law）](https://kexue.fm/archives/9607)
- [如何度量数据的稀疏程度？](https://kexue.fm/archives/9595)
- [Tiger：一个“抠”到极致的优化器](https://kexue.fm/archives/9512)
- [在bert4keras中使用混合精度和XLA加速训练](https://kexue.fm/archives/9059)
- [为什么需要残差？一个来自DeepNet的视角](https://kexue.fm/archives/8994)
- [门控注意力单元（GAU）还需要Warmup吗？](https://kexue.fm/archives/8990)
- [Efficient GlobalPointer：少点参数，多点效果](https://kexue.fm/archives/8877)

[发表你的看法](https://kexue.fm/archives/10699#comment_form)

mingyu xu

February 8th, 2025

看起来还挺make sense的。能否从这个角度去审视固定专家策略呢？

[回复评论](https://kexue.fm/archives/10699/comment-page-1?replyTo=26502#respond-post-10699)

[苏剑林](https://kexue.fm) 发表于
February 9th, 2025

你是指deepseek提出的shared expert策略？这个后面会写的。

[回复评论](https://kexue.fm/archives/10699/comment-page-1?replyTo=26539#respond-post-10699)

mingyu xu 发表于
February 9th, 2025

确实，期待后面的作品一波

[回复评论](https://kexue.fm/archives/10699/comment-page-1?replyTo=26546#respond-post-10699)

JY 发表于
February 12th, 2025

所以如果从这个角度出发就是可以让一些expert的模长固定为1或者一个大值？这样按照公式每次计算的时候都会选择它

[回复评论](https://kexue.fm/archives/10699/comment-page-1?replyTo=26575#respond-post-10699)

huiqingsong

February 8th, 2025

有一点没看明白，为什么$p\_i$的模长计算量更小

[回复评论](https://kexue.fm/archives/10699/comment-page-1?replyTo=26516#respond-post-10699)

namoe 发表于
February 8th, 2025

$p\_i$是标量，$v\_i$是向量

[回复评论](https://kexue.fm/archives/10699/comment-page-1?replyTo=26518#respond-post-10699)

[苏剑林](https://kexue.fm) 发表于
February 9th, 2025

整个$\\boldsymbol{p}$的计算就是一个线性变换加激活函数，$\\boldsymbol{e}\_i$的计算是两个线性变换层，并且维度也更高。

[回复评论](https://kexue.fm/archives/10699/comment-page-1?replyTo=26540#respond-post-10699)

rpsun

February 9th, 2025

我不是专门做神经网络的 可能对这里面的问题理解的不是很对

对于式（8）$y=∑\\limits\_{i∈argtopkp}p\_iv\_i$如果$p\_i$是经过softmax的，可不可以有另一种理解？

$v\_i$代表模态 而$p\_i$代表对应模态对该个例的解释比例

类似于传统PCA或EOF中的解释方差占比这种东西，只是这里的模态不是固定的基底而是某种固定的变换

这样也许不用top-K而使用累积的$p\_i$作为阈值，比如总计能达到50%、90%之类的

这样似乎在MOE之前可以对模型能够具有原模型的多少性能做一个简单的预判

[回复评论](https://kexue.fm/archives/10699/comment-page-1?replyTo=26520#respond-post-10699)

[苏剑林](https://kexue.fm) 发表于
February 9th, 2025

按Top-$p$来选择Expert也有人在做的。传统MoE的主要问题是$p\_i$跟$\\boldsymbol{v}\_i$未必同步，可能出现$p\_i$很大但$\\boldsymbol{v}\_i$很小的极端情形。

[回复评论](https://kexue.fm/archives/10699/comment-page-1?replyTo=26541#respond-post-10699)

rpsun 发表于
February 9th, 2025

原来如此 感谢！

[回复评论](https://kexue.fm/archives/10699/comment-page-1?replyTo=26542#respond-post-10699)

lidhrandom

February 9th, 2025

这个推导很有道理，但是现在主流的用到MoE的模型架构（DS，Mixtral之类的）里面实际的Router输出是否有预测向量长度的物理意义呢？有没有实验验证过被激活的expert输出的向量长度确实长于没有被激活的expert？

[回复评论](https://kexue.fm/archives/10699/comment-page-1?replyTo=26545#respond-post-10699)

trestad 发表于
February 10th, 2025

可以看 https://arxiv.org/pdf/2501.13074 第三节。expert内部有很多计算节点，他们的模都可以用来作为专家激活程度的参考。不同MoE的模型激活大概率体现在不同的节点上（因为预训练MoE没有任何约束来规定哪个节点的模长最应该体现专家的能力激活程度）。如果直接去掉预训练MoE中的router，用各个专家的内部激活值的模来选择expert，模型也可以做到比较好的效果保持（即，基本上能找到一个节点，使得被选择的expert在这个节点的激活比别的expert大。）

[回复评论](https://kexue.fm/archives/10699/comment-page-1?replyTo=26551#respond-post-10699)

[苏剑林](https://kexue.fm) 发表于
February 10th, 2025

主流的MoE，Expert没有归一化操作，不保证它的模长跟Router的预测结果同步。

[回复评论](https://kexue.fm/archives/10699/comment-page-1?replyTo=26563#respond-post-10699)

trestad

February 10th, 2025

非常精彩的推导和理解！我们最近的一些工作有类似的从模长出发对MoE的理解；但是认为由于router与专家的分离，导致router的预测哪个专家的激活值最大是没有label的学习过程，存在很多问题。

在本文“整理思路”的第五步，可以选择直接用expert的激活模长作为选择expert的依据。为了解决文中提到的“要挑模长最大的k个向量，就得把所有向量的模长都算出来”问题，我们将专家结构进行修改，先将输入降维，并以此低维向量来作为整个专家激活的代理指标，实际吞吐量可以达到MoE的97%，但是下游任务表现和loss，load balance都更优。

论文地址在 https://arxiv.org/pdf/2501.13074，欢迎交流讨论与批评指正！

[回复评论](https://kexue.fm/archives/10699/comment-page-1?replyTo=26550#respond-post-10699)

[苏剑林](https://kexue.fm) 发表于
February 10th, 2025

欢迎作者莅临，贵作前些天也刷到了，在“用模长作为Router”这一点上是相通的，贵作的做法，实际上就是从Expert中精简出一个可以快速运行的子模型来预测该Expert的概率，我之前的思路其实也差不多，但总感觉有些迂回。

后来意识到直接将Expert归一化后，就可以让Router的预测结果直接成为模长，只需要改变一下思路，认为$p\_i\\boldsymbol{e}\_i$才是完整的Expert就行。这样感觉上更加简单明快～

[回复评论](https://kexue.fm/archives/10699/comment-page-1?replyTo=26566#respond-post-10699)

linkping

February 10th, 2025

只选择模长最大的k个专家似乎与输入无关，在实际应用中p应该是由输入决定的吧

[回复评论](https://kexue.fm/archives/10699/comment-page-1?replyTo=26555#respond-post-10699)

[苏剑林](https://kexue.fm) 发表于
February 10th, 2025

Expert是输入的函数，Expert的模长肯定是输入的函数啊，怎么会跟输入无关？

[回复评论](https://kexue.fm/archives/10699/comment-page-1?replyTo=26567#respond-post-10699)

sun某某

February 10th, 2025

按照这篇文章的理论，另外的小模型（Router）是不是可以直接由n

个Expert的神经网络进行剪枝/低秩分解/蒸馏得到？

这一点同样可以进一步扩展：我们可以把n个expert的模长预测任务理解成一个 FFN层 + 一个把间隔 D/n = d内的元素用来计算模长的类似池化层的计算操作 用来蒸馏小模型（Router）

[回复评论](https://kexue.fm/archives/10699/comment-page-1?replyTo=26556#respond-post-10699)

[苏剑林](https://kexue.fm) 发表于
February 10th, 2025

如果是事后想要降低模型推理成本的话，那么玩法就很丰富了，你说的也未尝不可。不过MoE更多是希望能成为一种有竞争力的、可以从零训练的架构，所以还是从设计上就把模长分离开比较适合。

[回复评论](https://kexue.fm/archives/10699/comment-page-1?replyTo=26569#respond-post-10699)

JY

February 12th, 2025

所以这里我理解推到极端情况下的就是可以一个$v\_i$一个expert，最后$n$个expert，但是这个时候前头每一个都加一个$p\_i$的话其实并没有什么计算量，所以可能专家数量就是各个$v\_i$耦合程度和计算量的一个权衡？

[回复评论](https://kexue.fm/archives/10699/comment-page-1?replyTo=26578#respond-post-10699)

[Chaofa Yuan](https://bruceyuan.com/about.html)

February 12th, 2025

在 “ [https://kexue.fm/archives/10699#%E4%B8%BA%E4%BD%95%E5%A6%82%E6%AD%A4](https://kexue.fm/archives/10699#%E4%B8%BA%E4%BD%95%E5%A6%82%E6%AD%A4)” 的推到中，如果最终把 $p\_iv\_i$ 重参数化为 $p\_ie\_i$，这样是不是对于 expert 的输出 $v\_i$ 要额外多做一个 norm？

所以计算过程相对于我们常规的理解多了第二步：

1\. epxert1 2 3 分别计算出 v1 v2 v3 （这里的 expert 内部有 prenorm 等结构）

2\. 然后对 v1 v2 v3 归一化从 e1 e2 e3

3\. router 得出 $p\_i$，挑选出 更大的 k 个 p\_i，和 e\_i 做乘法相加

请问我这个理解有问题吗？

[回复评论](https://kexue.fm/archives/10699/comment-page-1?replyTo=26584#respond-post-10699)

loki\_ccfa

February 13th, 2025

感觉是有点像additive quantization？https://openaccess.thecvf.com/content\_cvpr\_2014/papers/Babenko\_Additive\_Quantization\_for\_2014\_CVPR\_paper.pdf

[回复评论](https://kexue.fm/archives/10699/comment-page-1?replyTo=26594#respond-post-10699)

[取消回复](https://kexue.fm/archives/10699#respond-post-10699)

你的大名

电子邮箱

个人网站（选填）

1\. 可以使用LaTeX代码，点击“预览效果”可查看效果；   2. 可以通过点击评论楼层编号来引用该楼层。

### 内容速览

[问题定义](https://kexue.fm/archives/10699#%E9%97%AE%E9%A2%98%E5%AE%9A%E4%B9%89)
[模长排序](https://kexue.fm/archives/10699#%E6%A8%A1%E9%95%BF%E6%8E%92%E5%BA%8F)
[MoE初现](https://kexue.fm/archives/10699#MoE%E5%88%9D%E7%8E%B0)
[思路概括](https://kexue.fm/archives/10699#%E6%80%9D%E8%B7%AF%E6%A6%82%E6%8B%AC)
[为何如此](https://kexue.fm/archives/10699#%E4%B8%BA%E4%BD%95%E5%A6%82%E6%AD%A4)
[文章小结](https://kexue.fm/archives/10699#%E6%96%87%E7%AB%A0%E5%B0%8F%E7%BB%93)

### 智能搜索

支持整句搜索！网站自动使用 [结巴分词](https://github.com/fxsjy/jieba) 进行分词，并结合ngrams排序算法给出合理的搜索结果。

### 热门标签

[生成模型](https://kexue.fm/tag/%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B/) [attention](https://kexue.fm/tag/attention/) [模型](https://kexue.fm/tag/%E6%A8%A1%E5%9E%8B/) [优化](https://kexue.fm/tag/%E4%BC%98%E5%8C%96/) [语言模型](https://kexue.fm/tag/%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/) [网站](https://kexue.fm/tag/%E7%BD%91%E7%AB%99/) [概率](https://kexue.fm/tag/%E6%A6%82%E7%8E%87/) [转载](https://kexue.fm/tag/%E8%BD%AC%E8%BD%BD/) [梯度](https://kexue.fm/tag/%E6%A2%AF%E5%BA%A6/) [微分方程](https://kexue.fm/tag/%E5%BE%AE%E5%88%86%E6%96%B9%E7%A8%8B/) [天象](https://kexue.fm/tag/%E5%A4%A9%E8%B1%A1/) [深度学习](https://kexue.fm/tag/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/) [矩阵](https://kexue.fm/tag/%E7%9F%A9%E9%98%B5/) [积分](https://kexue.fm/tag/%E7%A7%AF%E5%88%86/) [分析](https://kexue.fm/tag/%E5%88%86%E6%9E%90/) [python](https://kexue.fm/tag/python/) [力学](https://kexue.fm/tag/%E5%8A%9B%E5%AD%A6/) [无监督](https://kexue.fm/tag/%E6%97%A0%E7%9B%91%E7%9D%A3/) [几何](https://kexue.fm/tag/%E5%87%A0%E4%BD%95/) [扩散](https://kexue.fm/tag/%E6%89%A9%E6%95%A3/) [节日](https://kexue.fm/tag/%E8%8A%82%E6%97%A5/) [文本生成](https://kexue.fm/tag/%E6%96%87%E6%9C%AC%E7%94%9F%E6%88%90/) [优化器](https://kexue.fm/tag/%E4%BC%98%E5%8C%96%E5%99%A8/) [生活](https://kexue.fm/tag/%E7%94%9F%E6%B4%BB/) [数论](https://kexue.fm/tag/%E6%95%B0%E8%AE%BA/)

### 随机文章

- [搜狐文本匹配：基于条件LayerNorm的多任务baseline](https://kexue.fm/archives/8337)
- [BERT-of-Theseus：基于模块替换的模型压缩方法](https://kexue.fm/archives/7575)
- [科学空间两岁了](https://kexue.fm/archives/1279)
- [如何应对Seq2Seq中的“根本停不下来”问题？](https://kexue.fm/archives/7500)
- [宇宙驿站定于本周五更换服务器](https://kexue.fm/archives/728)
- [广州亚运歌曲《重逢》歌词(中英文版)](https://kexue.fm/archives/136)
- [日全食多路联合直播频道](https://kexue.fm/archives/25)
- [变分自编码器 = 最小化先验分布 \+ 最大化互信息](https://kexue.fm/archives/6088)
- [(原创)切抛物线法解方程](https://kexue.fm/archives/504)
- [停！](https://kexue.fm/archives/72)

### 最近评论

- [zzdpaaa](https://kexue.fm/archives/9181/comment-page-5#comment-26613): 苏老师好，我自己试着推导了一下DDIM的公式，但遇到了一些问题，希望得到大佬的解答和帮助。
根...
- [lljbash](https://kexue.fm/archives/7430/comment-page-2#comment-26610): 模型并不是一个字一个字思考，而是每次只讲出来一个字。
- [Astrologos](https://kexue.fm/archives/8679/comment-page-2#comment-26608): 感觉和Toy Models of Superposition讲的有关
- [zehua](https://kexue.fm/archives/10347/comment-page-1#comment-26607): 是否可以理解为单层 attention 在计算 $y\_n$ 时还无法感知前面若干 token ...
- [jumbo](https://kexue.fm/archives/10091/comment-page-5#comment-26606): https://spaces.ac.cn/archives/10091#MLA
MHA这部分中...
- [kasd](https://kexue.fm/archives/9529/comment-page-2#comment-26603): 补充：n我理解是hidden\_size，d是head\_dim
- [kasd](https://kexue.fm/archives/9529/comment-page-2#comment-26602): 既然分成多个head后会使得d远小于n，导致低秩问题。那么使用single head不就行了（...
- [报告大王](https://kexue.fm/archives/10633/comment-page-1#comment-26601): 感谢大佬，这个分析直击核心，consistency model的论文我看了好几遍，不得要领。
- [loki\_ccfa](https://kexue.fm/archives/10699/comment-page-1#comment-26594): 感觉是有点像additive quantization？https://openaccess....
- [西柚汁](https://kexue.fm/archives/4118/comment-page-1#comment-26593): 苏老师您好，在最后迁移学习的部分，不够句子最大长度的标签部分，您的代码中是用0填充的，应该是用...

### 友情链接

- [Cool Papers](https://papers.cool)
- [数学研发](https://bbs.emath.ac.cn)
- [Seatop](http://www.seatop.com.cn/)
- [Xiaoxia](https://xiaoxia.org/)
- [积分表-网络版](https://kexue.fm/sci/integral/index.html)
- [丝路博傲](http://blog.dvxj.com/)
- [ph4ntasy 饭特稀](http://www.ph4ntasy.com/)
- [数学之家](http://www.2math.cn/)
- [有趣天文奇观](http://interesting-sky.china-vo.org/)
- [TwistedW](http://www.twistedwg.com/)
- [godweiyang](https://godweiyang.com/)
- [AI柠檬](https://blog.ailemon.net/)
- [王登科-DK博客](https://greatdk.com)
- [ESON](https://blog.eson.org/)
- [枫之羽](https://fzhiy.net/)
- [Mathor's blog](https://wmathor.com/)
- [coding-zuo](https://coding-zuo.github.io/)
- [博科园](https://www.bokeyuan.net/)
- [孔皮皮的博客](https://www.kppkkp.top/)
- [运鹏的博客](https://yunpengtai.top/)
- [jiming.site](https://jiming.site/)
- [OmegaXYZ](https://www.omegaxyz.com/)
- [Blog by Eacls](https://www.eacls.top/)
- [EAI猩球](https://www.robotech.ink/)
- [文举的博客](https://liwenju0.com/)
- [申请链接](https://kexue.fm/links.html)

[![署名-非商业用途-保持一致](https://kexue.fm/usr/themes/geekg/images/cc.gif)](http://creativecommons.org/licenses/by-nc-nd/2.5/cn/) 本站采用创作共用版权协议，要求署名、非商业用途和保持一致。转载本站内容必须也遵循“ [署名-非商业用途-保持一致](http://creativecommons.org/licenses/by-nc-nd/2.5/cn/)”的创作共用协议。

© 2009-2025 Scientific Spaces. All rights reserved. Theme by [laogui](http://www.laogui.com). Powered by [Typecho](http://typecho.org). 备案号: [粤ICP备09093259号-1/2](https://beian.miit.gov.cn/)。