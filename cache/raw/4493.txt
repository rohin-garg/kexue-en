![MobileSideBar](https://kexue.fm/usr/themes/geekg/images/slide-button.png)

## SEARCH

## MENU

- [打赏](https://kexue.fm/reward.html)
- [公式](https://kexue.fm/latex.html)
- [天象](https://kexue.fm/ac.html)
- [链接](https://kexue.fm/links.html)
- [时光](https://kexue.fm/me.html)
- [博览](https://kexue.fm/science.html)
- [归档](https://kexue.fm/content.html)

## CATEGORIES

- [千奇百怪](https://kexue.fm/category/Everything)
- [天文探索](https://kexue.fm/category/Astronomy)
- [数学研究](https://kexue.fm/category/Mathematics)
- [物理化学](https://kexue.fm/category/Phy-chem)
- [信息时代](https://kexue.fm/category/Big-Data)
- [生物自然](https://kexue.fm/category/Biology)
- [图片摄影](https://kexue.fm/category/Photograph)
- [问题百科](https://kexue.fm/category/Questions)
- [生活/情感](https://kexue.fm/category/Life-Feeling)
- [资源共享](https://kexue.fm/category/Resources)

## NEWPOSTS

- [让炼丹更科学一些（五）：基于梯度精...](https://kexue.fm/archives/11530)
- [让炼丹更科学一些（四）：新恒等式，...](https://kexue.fm/archives/11494)
- [为什么DeltaNet要加L2 N...](https://kexue.fm/archives/11486)
- [让炼丹更科学一些（三）：SGD的终...](https://kexue.fm/archives/11480)
- [让炼丹更科学一些（二）：将结论推广...](https://kexue.fm/archives/11469)
- [滑动平均视角下的权重衰减和学习率](https://kexue.fm/archives/11459)
- [生成扩散模型漫谈（三十一）：预测数...](https://kexue.fm/archives/11428)
- [Muon优化器指南：快速上手与关键细节](https://kexue.fm/archives/11416)
- [AdamW的Weight RMS的...](https://kexue.fm/archives/11404)
- [n个正态随机数的最大值的渐近估计](https://kexue.fm/archives/11390)

## COMMENTS

- [Bin: 今天偶然从某个论坛看到有人推荐您的博客，定睛一看竟然是华师同院...](https://kexue.fm/archives/1990/comment-page-2#comment-29105)
- [Rapture D: 我有一个问题，为什么不考虑亥姆霍兹定理和斯托克斯公式。](https://kexue.fm/archives/11530/comment-page-1#comment-29104)
- [mofheka: 苏神是还在用jax是么？最近在做基于Google Pathwa...](https://kexue.fm/archives/11390/comment-page-1#comment-29103)
- [长琴: 看懂这篇博客也不是一件容易的事情。](https://kexue.fm/archives/11530/comment-page-1#comment-29102)
- [AlexLi: 苏老师，请教一下(7)式中将 μ(xt)μ(xt) 传给 $p...](https://kexue.fm/archives/9257/comment-page-4#comment-29101)
- [tyler\_zxc: "Performer的思想是将标准的Attention线性化，...](https://kexue.fm/archives/7921/comment-page-2#comment-29100)
- [我: 似乎并非mHC提出矩阵的思想？之前hyper connecti...](https://kexue.fm/archives/11494/comment-page-1#comment-29099)
- [winter: 苏神您好，假如对于比较均匀的attention weightP...](https://kexue.fm/archives/10847/comment-page-1#comment-29098)
- [苏剑林: KL散度、JS散度、W距离啥的，都行啊，看你喜欢哪个](https://kexue.fm/archives/8512/comment-page-2#comment-29097)
- [苏剑林: 没有绝对公平的对比方法，主要看你关心什么。比如，如果只关心推理...](https://kexue.fm/archives/9119/comment-page-14#comment-29096)

## USERLOGIN

- [登录](https://kexue.fm/admin/login.php)

[科学空间\|Scientific Spaces](https://kexue.fm/)

- [登录](https://kexue.fm/admin/login.php)
- [打赏](https://kexue.fm/reward.html)
- [公式](https://kexue.fm/latex.html)
- [天象](https://kexue.fm/ac.html)
- [链接](https://kexue.fm/links.html)
- [时光](https://kexue.fm/me.html)
- [博览](https://kexue.fm/science.html)
- [归档](https://kexue.fm/content.html)

渴望成为一个小飞侠

- [![](https://kexue.fm/usr/themes/geekg/images/rss.png)\\
\\
欢迎订阅](https://kexue.fm/feed)
- [![](https://kexue.fm/usr/themes/geekg/images/mail.png)\\
\\
个性邮箱](https://kexue.fm/archives/119)
- [![](https://kexue.fm/usr/themes/geekg/images/Saturn.png)\\
\\
天象信息](https://kexue.fm/ac.html)
- [![](https://kexue.fm/usr/themes/geekg/images/iss.png)\\
\\
观测ISS](https://kexue.fm/archives/41)
- [![](https://kexue.fm/usr/themes/geekg/images/pi.png)\\
\\
LaTeX](https://kexue.fm/latex.html)
- [![](https://kexue.fm/usr/themes/geekg/images/mlogo.png)\\
\\
关于博主](https://kexue.fm/me.html)

欢迎访问“科学空间”，这里将与您共同探讨自然科学，回味人生百态；也期待大家的分享～

- [**千奇百怪** Everything](https://kexue.fm/category/Everything)
- [**天文探索** Astronomy](https://kexue.fm/category/Astronomy)
- [**数学研究** Mathematics](https://kexue.fm/category/Mathematics)
- [**物理化学** Phy-chem](https://kexue.fm/category/Phy-chem)
- [**信息时代** Big-Data](https://kexue.fm/category/Big-Data)
- [**生物自然** Biology](https://kexue.fm/category/Biology)
- [**图片摄影** Photograph](https://kexue.fm/category/Photograph)
- [**问题百科** Questions](https://kexue.fm/category/Questions)
- [**生活/情感** Life-Feeling](https://kexue.fm/category/Life-Feeling)
- [**资源共享** Resources](https://kexue.fm/category/Resources)

- [**千奇百怪**](https://kexue.fm/category/Everything)
- [**天文探索**](https://kexue.fm/category/Astronomy)
- [**数学研究**](https://kexue.fm/category/Mathematics)
- [**物理化学**](https://kexue.fm/category/Phy-chem)
- [**信息时代**](https://kexue.fm/category/Big-Data)
- [**生物自然**](https://kexue.fm/category/Biology)
- [**图片摄影**](https://kexue.fm/category/Photograph)
- [**问题百科**](https://kexue.fm/category/Questions)
- [**生活/情感**](https://kexue.fm/category/Life-Feeling)
- [**资源共享**](https://kexue.fm/category/Resources)

[首页](https://kexue.fm/) [信息时代](https://kexue.fm/category/Big-Data) Keras中自定义复杂的loss函数

22Jul

# [Keras中自定义复杂的loss函数](https://kexue.fm/archives/4493)

By 苏剑林 \|
2017-07-22 \|
576518位读者 \|

Keras是一个搭积木式的深度学习框架，用它可以很方便且直观地搭建一些常见的深度学习模型。在tensorflow出来之前，Keras就已经几乎是当时最火的深度学习框架，以theano为后端，而如今Keras已经同时支持四种后端：theano、tensorflow、cntk、mxnet（前三种官方支持，mxnet还没整合到官方中），由此可见Keras的魅力。

Keras是很方便，然而这种方便不是没有代价的，最为人诟病之一的缺点就是灵活性较低，难以搭建一些复杂的模型。的确，Keras确实不是很适合搭建复杂的模型，但并非没有可能，而是搭建太复杂的模型所用的代码量，跟直接用tensorflow写也差不了多少。但不管怎么说，Keras其友好、方便的特性（比如那可爱的训练进度条），使得我们总有使用它的场景。这样，如何更灵活地定制Keras模型，就成为一个值得研究的课题了。这篇文章我们来关心自定义loss。

## 输入-输出设计 [\#](https://kexue.fm/archives/4493\#%E8%BE%93%E5%85%A5-%E8%BE%93%E5%87%BA%E8%AE%BE%E8%AE%A1)

Keras的模型是函数式的，即有输入，也有输出，而loss即为预测值与真实值的某种误差函数。Keras本身也自带了很多 [loss函数](https://keras.io/losses/)，如mse、交叉熵等，直接调用即可。而要自定义loss，最自然的方法就是仿照Keras自带的loss进行改写。

比如，我们做分类问题时，经常用的就是softmax输出，然后用交叉熵作为loss。然而这种做法也有不少缺点，其中之一就是分类太自信，哪怕输入噪音，分类的结果也几乎是非1即0，这通常会导致过拟合的风险，还会使得我们在实际应用中没法很好地确定置信区间、设置阈值。因此很多时候我们也会想办法使得分类别太自信，而修改loss也是手段之一。

如果不修改loss，我们就是使用交叉熵去拟合一个one hot的分布。交叉熵的公式是

S(q\|p)=−∑iqilogpiS(q\|p)=−∑iqilog⁡pi

其中pipi是预测的分布，而qiqi是真实的分布，比如输出为\[z1,z2,z3\]\[z1,z2,z3\]，目标为\[1,0,0\]\[1,0,0\]，那么

loss=−log(ez1/Z),Z=ez1+ez2+ez3loss=−log⁡(ez1/Z),Z=ez1+ez2+ez3

只要z1z1已经是\[z1,z2,z3\]\[z1,z2,z3\]的最大值，那么我们总可以“变本加厉”——通过增大训练步数，使得z1,z2,z3z1,z2,z3增加足够大的比例（等价地，即增大向量\[z1,z2,z3\]\[z1,z2,z3\]的模长），从而ez1/Zez1/Z足够接近1（等价地，loss足够接近0）。这就是通常softmax过于自信的来源：只要盲目增大模长，就可以降低loss，训练器肯定是很乐意了，这代价太低了。为了使得分类不至于太自信，一个方案就是不要单纯地去拟合one hot分布，分一点力气去拟合一下均匀分布，即改为新loss：

loss=−(1−ε)log(ez1/Z)−ε∑i=1n13log(ezi/Z),Z=ez1+ez2+ez3loss=−(1−ε)log⁡(ez1/Z)−ε∑i=1n13log⁡(ezi/Z),Z=ez1+ez2+ez3

这样，盲目地增大比例使得ez1/Zez1/Z接近于1，就不再是最优解了，从而可以缓解softmax过于自信的情况，不少情况下，这种策略还可以增加测试准确率（防止过拟合）。

那么，在Keras中应该怎么写呢？其实挺简单的：

```python

```

也就是自定义一个输入为y\_pred,y\_true的loss函数，放进模型compile即可。这里的mycrossentropy，第一项就是普通的交叉熵，第二项中，先通过K.ones\_like(y\_pred)/nb\_classes构造了一个均匀分布，然后算y\_pred与均匀分布的交叉熵。就这么简单～

## 并不仅仅是输入输出那么简单 [\#](https://kexue.fm/archives/4493\#%E5%B9%B6%E4%B8%8D%E4%BB%85%E4%BB%85%E6%98%AF%E8%BE%93%E5%85%A5%E8%BE%93%E5%87%BA%E9%82%A3%E4%B9%88%E7%AE%80%E5%8D%95)

前面已经说了，Keras的模型有固定的输入和输出，并且loss即为预测值与真实值的某种误差函数，然而，很多模型并非这样的，比如问答模型与triplet loss。

这个的问题是指有固定的答案库的FAQ形式的问答。一种常见的做问答模型的方法就是：先分别将答案和问题都encode成为一个同样长度的向量，然后比较它们的cos值，cos越大就越匹配。这种做法很容易理解，是一个比较通用的框架，比如这里的问题和答案都不需要一定是问题，图片也行，反正只不过是encode的方法不一样，最终只要能encode出一个向量来即可。但是怎么训练呢？我们当然希望正确答案的cos值越大越好，错误答案的\\cos值越小越好，但是这不是必要的，合理的要求应该是：正确答案的cos值比所有错误答案的\\cos值都要大，大多少无所谓，一丁点都行。因此，这就导致了triplet loss：

loss=max(0,m+cos(q,Awrong)−cos(q,Aright))loss=max(0,m+cos⁡(q,Awrong)−cos⁡(q,Aright))

其中mm是一个大于零的正数。

怎么理解这个loss呢？要注意我们要最小化loss，所以只看m+cos(q,Awrong)−cos(q,Aright)m+cos⁡(q,Awrong)−cos⁡(q,Aright)这部分，我们知道目的是拉大正确与错误答案的差距，但是，一旦cos(q,Aright)−cos(q,Awrong)>mcos⁡(q,Aright)−cos⁡(q,Awrong)>m，也就是差距大于mm时，由于maxmax的存在，loss就等于0，这时候就自动达到最小值，就不会优化它了。所以，triplet loss的思想就是：只希望正确比错误答案的差距大一点（并不是越大越好），超过mm就别管它了，集中精力关心那些还没有拉开的样本吧！

我们已经有问题和正确答案，错误答案只要随机挑就行，所以这样训练样本是很容易构造的。不过Keras中怎么实现triplet loss呢？看上去是一个单输入、双输出的模型，但并不是那么简单，Keras中的双输出模型，只能给每个输出分别设置一个loss，然后加权求和，但这里不能简单表示成两项的加权求和。那应该要怎么搭建这样的模型呢？下面是一个例子：

```python

```

如果第一次看不懂，那么请反复阅读几次，这个代码包含了Keras中实现最一般模型的思路： **把目标当成一个输入，构成多输入模型，把loss写成一个层，作为最后的输出，搭建模型的时候，就只需要将模型的output定义为loss，而compile的时候，直接将loss设置为y\_pred（因为模型的输出就是loss，所以y\_pred就是loss），无视y\_true，训练的时候，y\_true随便扔一个符合形状的数组进去就行了。** 最后我们得到的是问题和答案的编码器，也就是问题和答案都分别编码出一个向量来，我们只需要比较coscos，就可以选择最优答案了。

## Embedding层的妙用 [\#](https://kexue.fm/archives/4493\#Embedding%E5%B1%82%E7%9A%84%E5%A6%99%E7%94%A8)

在读这一段之前，请读者务必确定自己对Embedding层有清晰的认识，如果还没有，请移步阅读 [《词向量与Embedding究竟是怎么回事？》](https://kexue.fm/archives/4122/)。这里需要反复强调的是，虽然词向量叫Word Embedding，但是，Embedding层不是词向量，跟词向量没有半毛钱关系！！！不要有“怎么就跟词向量扯上关系了”这样的傻问题，Embedding层从来就没有跟词向量有过任何直接联系（只不过在训练词向量时可以用它）。对于Embedding层，你可以有两种理解：1、是one hot输入的全连接层的加速版本，也就是说，它就是一个以one hot为输入的Dense层，数学上完全等价；2、它就是一个矩阵查找操作，输入一个整数，输出对应下标的向量，只不过这个矩阵是可训练的。（你看，哪里跟词向量有联系了？）

这部分我们来关心center loss。前面已经说了，做分类时，一般是softmax+交叉熵做，用矩阵的写法，softmax就是

softmax(Wx+b)softmax(Wx+b)

其中xx可以理解为提取的特征，而W,bW,b是最后的全连接层的权重，整个模型是一起训练的。问题是，这样的方案所训练出来的特征模型xx，具有怎样的形态呢？

有一些情况下，我们更关心特征xx而不是最后的分类结果，比如人脸识别场景，假如我们有10万个不同的人的人脸数据库，每个人有若干张照片，那么我们就可以训练一个10万分类模型，对于给定的照片，我们可以判断它是10万个中的哪一个。但这仅仅是训练场景，那么怎么应用呢？到了具体的应用环境，比如一个公司内部，可能有只有几百人；在公共安全检测场景，可能有数百万人，所以前面做好的10万分类模型基本上是没有意义的，但是在这个模型softmax之前的特征，也就是前一段所说的xx，可能还是很有意义的。如果对于同一个人（也就是同一类），xx基本一样，那么实际应用中，我们就可以把训练好的模型当作特征提取工具，然后把提取出来的特征直接用KNN（最邻近距离）来做就行了。

设想很美好，但事实很残酷，直接训练softmax的话，事实上得到的特征不一定具有 **聚类特性**，相反，它们会尽量布满整个空间（没有给其他人留出位置，参考center loss的相关论文和文章，比如 [这篇](https://zhuanlan.zhihu.com/p/23340343)。）。那么，怎样训练才使得结果有聚类特性呢？center loss使用了一种简单粗暴但是却很有效的方案——加聚类惩罚项。完整地写出来，就是

loss=−logeW⊤yx+by∑ieW⊤ix+bi+λ∥∥x−cy∥∥2loss=−log⁡eWy⊤x+by∑ieWi⊤x+bi+λ‖x−cy‖2

其中yy对应着正确的类别。可以看到，第一项就是普通的softmax交叉熵，第二项就是额外的惩罚项，它给每个类定义了可训练的中心cc，要求每个类要跟各自的中心靠得很近。所以，总的来说， **第一项负责拉开不同类之间的距离，第二项负责缩小同一类之间的距离**。

那么，Keras中要怎么实现这个方案？关键是，怎么存放聚类中心？答案就是Embedding层！这部分的开头已经提示了，Embedding就是一个待训练的矩阵罢了，正好可以存放聚类中心参数。于是，模仿第二部分的写法，就得到

```python

```

读者可能有疑问，为什么不像第二部分的triplet loss模型那样，将整体的loss写成一个单一的输出，然后搭建模型，而是要像目前这样变成双输出呢？

事实上，Keras爱好者钟情于Keras，其中一个很重要的原因就是它的进度条——能够实时显示训练loss、训练准确率。 **如果像第二部分那样写，那么就不能设置metrics参数，那么训练过程中就不能显示准确率了，这不能说是一个小遗憾。而目前这样写，我们就依然能够在训练过程中看到训练准确率，还能分别看到交叉熵loss、l2\_loss、总的loss分别是多少，非常舒服**。

## Keras就是这么好玩 [\#](https://kexue.fm/archives/4493\#Keras%E5%B0%B1%E6%98%AF%E8%BF%99%E4%B9%88%E5%A5%BD%E7%8E%A9)

有了以上三个案例，读者应该对Keras搭建复杂模型的步骤心中有数了，应当说，也是比较简单灵活的。Keras确实有它不够灵活的地方，但也没有网上评论的那么无能。总的来说，Keras是能够满足大多数人快速实验深度学习模型的需求的。如果你还在纠结深度学习框架的选择，那么请选择Keras吧——当你真正觉得Keras不能满足你的需求时，你已经有能力驾驭任何框架了，也就没有这个纠结了。

_**转载到请包括本文地址：** [https://kexue.fm/archives/4493](https://kexue.fm/archives/4493 "Keras中自定义复杂的loss函数")_

_**更详细的转载事宜请参考：**_ [《科学空间FAQ》](https://kexue.fm/archives/6508#%E6%96%87%E7%AB%A0%E5%A6%82%E4%BD%95%E8%BD%AC%E8%BD%BD/%E5%BC%95%E7%94%A8 "《科学空间FAQ》")

**如果您还有什么疑惑或建议，欢迎在下方评论区继续讨论。**

**如果您觉得本文还不错，欢迎 [分享](https://kexue.fm/archives/4493#share)/ [打赏](https://kexue.fm/archives/4493#pay) 本文。打赏并非要从中获得收益，而是希望知道科学空间获得了多少读者的真心关注。当然，如果你无视它，也不会影响你的阅读。再次表示欢迎和感谢！**

打赏

![科学空间](https://kexue.fm/usr/themes/geekg/payment/wx.png)

微信打赏

![科学空间](https://kexue.fm/usr/themes/geekg/payment/zfb.png)

支付宝打赏

因为网站后台对打赏并无记录，因此欢迎在打赏时候备注留言。

你还可以 [**点击这里**](http://mail.qq.com/cgi-bin/qm_share?t=qm_mailme&email=tN7d1drY3drrx8H0xcWa19vZ) 或在下方评论区留言来告知你的建议或需求。

**如果您需要引用本文，请参考：**

苏剑林. (Jul. 22, 2017). 《Keras中自定义复杂的loss函数 》\[Blog post\]. Retrieved from [https://kexue.fm/archives/4493](https://kexue.fm/archives/4493)

@online{kexuefm-4493,

         title={Keras中自定义复杂的loss函数},

         author={苏剑林},

         year={2017},

         month={Jul},

         url={\\url{https://kexue.fm/archives/4493}},

}


分类： [信息时代](https://kexue.fm/category/Big-Data)    标签： [模型](https://kexue.fm/tag/%E6%A8%A1%E5%9E%8B/), [深度学习](https://kexue.fm/tag/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/), [损失函数](https://kexue.fm/tag/%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0/), [keras](https://kexue.fm/tag/keras/)[157 评论](https://kexue.fm/archives/4493#comments)

< [Linux下的误删大坑与简单的恢复技巧](https://kexue.fm/archives/4491 "Linux下的误删大坑与简单的恢复技巧") \| [基于Xception的腾讯验证码识别（样本+代码）](https://kexue.fm/archives/4503 "基于Xception的腾讯验证码识别（样本+代码）") >

### 你也许还对下面的内容感兴趣

- [生成扩散模型漫谈（三十一）：预测数据而非噪声](https://kexue.fm/archives/11428 "生成扩散模型漫谈（三十一）：预测数据而非噪声")
- [MoE环游记：3、换个思路来分配](https://kexue.fm/archives/10757 "MoE环游记：3、换个思路来分配")
- [MoE环游记：2、不患寡而患不均](https://kexue.fm/archives/10735 "MoE环游记：2、不患寡而患不均")
- [MoE环游记：1、从几何意义出发](https://kexue.fm/archives/10699 "MoE环游记：1、从几何意义出发")
- [通向概率分布之路：盘点Softmax及其替代品](https://kexue.fm/archives/10145 "通向概率分布之路：盘点Softmax及其替代品")
- [生成扩散模型漫谈（二十二）：信噪比与大图生成（上）](https://kexue.fm/archives/10047 "生成扩散模型漫谈（二十二）：信噪比与大图生成（上）")
- [EMO：基于最优传输思想设计的分类损失函数](https://kexue.fm/archives/9797 "EMO：基于最优传输思想设计的分类损失函数")
- [基于量子化假设推导模型的尺度定律（Scaling Law）](https://kexue.fm/archives/9607 "基于量子化假设推导模型的尺度定律（Scaling Law）")
- [缓解交叉熵过度自信的一个简明方案](https://kexue.fm/archives/9526 "缓解交叉熵过度自信的一个简明方案")
- [Tiger：一个“抠”到极致的优化器](https://kexue.fm/archives/9512 "Tiger：一个“抠”到极致的优化器")

[发表你的看法](https://kexue.fm/archives/4493#comment_form)

1. [«](https://kexue.fm/archives/4493/comment-page-6#comments)
2. [1](https://kexue.fm/archives/4493/comment-page-1#comments)
3. ...
4. [4](https://kexue.fm/archives/4493/comment-page-4#comments)
5. [5](https://kexue.fm/archives/4493/comment-page-5#comments)
6. [6](https://kexue.fm/archives/4493/comment-page-6#comments)
7. [7](https://kexue.fm/archives/4493/comment-page-7#comments)

[使用Keras训练神经网络备忘录\_Python技术站](https://pythonjishu.com/sxvuhvayoebj/)

April 8th, 2023

\[...\]例子来源Keras中自定义复杂的loss函数\[...\]

[回复评论](https://kexue.fm/archives/4493/comment-page-7?replyTo=21367#respond-post-4493)

龙行

July 26th, 2023

找到了匿名函数层作loss的出处！不过匿名函数层作loss在高版本keras中好像会导致后续验证中log为空，不过也有可能是我还没完全学懂keras。

[回复评论](https://kexue.fm/archives/4493/comment-page-7?replyTo=22346#respond-post-4493)

[苏剑林](https://kexue.fm/) 发表于
July 27th, 2023

可以试试add\_metric

[回复评论](https://kexue.fm/archives/4493/comment-page-7?replyTo=22358#respond-post-4493)

1. [«](https://kexue.fm/archives/4493/comment-page-6#comments)
2. [1](https://kexue.fm/archives/4493/comment-page-1#comments)
3. ...
4. [4](https://kexue.fm/archives/4493/comment-page-4#comments)
5. [5](https://kexue.fm/archives/4493/comment-page-5#comments)
6. [6](https://kexue.fm/archives/4493/comment-page-6#comments)
7. [7](https://kexue.fm/archives/4493/comment-page-7#comments)

[取消回复](https://kexue.fm/archives/4493#respond-post-4493)

你的大名

电子邮箱

个人网站（选填）

1\. 可以使用LaTeX代码，点击“预览效果”可查看效果；

2\. 可以通过点击评论楼层编号来引用该楼层；

3\. 网站可能会有点卡，如非确认评论失败，请 **不要重复点击提交**。

### 内容速览

[输入-输出设计](https://kexue.fm/archives/4493#%E8%BE%93%E5%85%A5-%E8%BE%93%E5%87%BA%E8%AE%BE%E8%AE%A1)
[并不仅仅是输入输出那么简单](https://kexue.fm/archives/4493#%E5%B9%B6%E4%B8%8D%E4%BB%85%E4%BB%85%E6%98%AF%E8%BE%93%E5%85%A5%E8%BE%93%E5%87%BA%E9%82%A3%E4%B9%88%E7%AE%80%E5%8D%95)
[Embedding层的妙用](https://kexue.fm/archives/4493#Embedding%E5%B1%82%E7%9A%84%E5%A6%99%E7%94%A8)
[Keras就是这么好玩](https://kexue.fm/archives/4493#Keras%E5%B0%B1%E6%98%AF%E8%BF%99%E4%B9%88%E5%A5%BD%E7%8E%A9)

### 智能搜索

支持整句搜索！网站自动使用 [结巴分词](https://github.com/fxsjy/jieba) 进行分词，并结合ngrams排序算法给出合理的搜索结果。

### 热门标签

[生成模型](https://kexue.fm/tag/%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B/) [attention](https://kexue.fm/tag/attention/) [优化](https://kexue.fm/tag/%E4%BC%98%E5%8C%96/) [语言模型](https://kexue.fm/tag/%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/) [模型](https://kexue.fm/tag/%E6%A8%A1%E5%9E%8B/) [梯度](https://kexue.fm/tag/%E6%A2%AF%E5%BA%A6/) [网站](https://kexue.fm/tag/%E7%BD%91%E7%AB%99/) [概率](https://kexue.fm/tag/%E6%A6%82%E7%8E%87/) [矩阵](https://kexue.fm/tag/%E7%9F%A9%E9%98%B5/) [优化器](https://kexue.fm/tag/%E4%BC%98%E5%8C%96%E5%99%A8/) [转载](https://kexue.fm/tag/%E8%BD%AC%E8%BD%BD/) [微分方程](https://kexue.fm/tag/%E5%BE%AE%E5%88%86%E6%96%B9%E7%A8%8B/) [分析](https://kexue.fm/tag/%E5%88%86%E6%9E%90/) [天象](https://kexue.fm/tag/%E5%A4%A9%E8%B1%A1/) [深度学习](https://kexue.fm/tag/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/) [积分](https://kexue.fm/tag/%E7%A7%AF%E5%88%86/) [python](https://kexue.fm/tag/python/) [扩散](https://kexue.fm/tag/%E6%89%A9%E6%95%A3/) [力学](https://kexue.fm/tag/%E5%8A%9B%E5%AD%A6/) [无监督](https://kexue.fm/tag/%E6%97%A0%E7%9B%91%E7%9D%A3/) [几何](https://kexue.fm/tag/%E5%87%A0%E4%BD%95/) [节日](https://kexue.fm/tag/%E8%8A%82%E6%97%A5/) [生活](https://kexue.fm/tag/%E7%94%9F%E6%B4%BB/) [文本生成](https://kexue.fm/tag/%E6%96%87%E6%9C%AC%E7%94%9F%E6%88%90/) [数论](https://kexue.fm/tag/%E6%95%B0%E8%AE%BA/)

### 随机文章

- [2015年全年天象](https://kexue.fm/archives/4170)
- [精确自由落体运动定律的讨论](https://kexue.fm/archives/330)
- [电脑修好了，Blog正常更新](https://kexue.fm/archives/74)
- [强大的整数数列网站OEIS](https://kexue.fm/archives/2765)
- [学习场论（电磁场、重力场）](https://kexue.fm/archives/1980)
- [Transformer升级之路：20、MLA好在哪里?（上）](https://kexue.fm/archives/10907)
- [《科学》：我们发现了磁单极子](https://kexue.fm/archives/141)
- [祝大家端午节快乐！](https://kexue.fm/archives/681)
- [\[追溯\]封装界传奇人物](https://kexue.fm/archives/2726)
- [势能最小问题的探讨](https://kexue.fm/archives/2050)

### 最近评论

- [Bin](https://kexue.fm/archives/1990/comment-page-2#comment-29105): 今天偶然从某个论坛看到有人推荐您的博客，定睛一看竟然是华师同院的往届师兄！看到这篇2013年的...
- [Rapture D](https://kexue.fm/archives/11530/comment-page-1#comment-29104): 我有一个问题，为什么不考虑亥姆霍兹定理和斯托克斯公式。
- [mofheka](https://kexue.fm/archives/11390/comment-page-1#comment-29103): 苏神是还在用jax是么？最近在做基于Google Pathway的理念做一个动态版的MPMD框...
- [长琴](https://kexue.fm/archives/11530/comment-page-1#comment-29102): 看懂这篇博客也不是一件容易的事情。
- [AlexLi](https://kexue.fm/archives/9257/comment-page-4#comment-29101): 苏老师，请教一下(7)式中将 μ(xt)μ(xt) 传给 popo 进行推理的操作。 $x\_...
- [tyler\_zxc](https://kexue.fm/archives/7921/comment-page-2#comment-29100): "Performer的思想是将标准的Attention线性化，所以为什么不干脆直接训练一个线性...
- [我](https://kexue.fm/archives/11494/comment-page-1#comment-29099): 似乎并非mHC提出矩阵的思想？之前hyper connection就是了
- [winter](https://kexue.fm/archives/10847/comment-page-1#comment-29098): 苏神您好，假如对于比较均匀的attention weightP，往往呈现long tail分布...
- [苏剑林](https://kexue.fm/archives/8512/comment-page-2#comment-29097): KL散度、JS散度、W距离啥的，都行啊，看你喜欢哪个
- [苏剑林](https://kexue.fm/archives/9119/comment-page-14#comment-29096): 没有绝对公平的对比方法，主要看你关心什么。比如，如果只关心推理成本和推理效果，那么有的方法可以...

### 友情链接

- [Cool Papers](https://papers.cool/)
- [数学研发](https://bbs.emath.ac.cn/)
- [Seatop](http://www.seatop.com.cn/)
- [Xiaoxia](https://xiaoxia.org/)
- [积分表-网络版](https://kexue.fm/sci/integral/index.html)
- [丝路博傲](http://blog.dvxj.com/)
- [数学之家](http://www.2math.cn/)
- [有趣天文奇观](http://interesting-sky.china-vo.org/)
- [TwistedW](http://www.twistedwg.com/)
- [godweiyang](https://godweiyang.com/)
- [AI柠檬](https://blog.ailemon.net/)
- [王登科-DK博客](https://greatdk.com/)
- [ESON](https://blog.eson.org/)
- [枫之羽](https://fzhiy.net/)
- [coding-zuo](https://coding-zuo.github.io/)
- [博科园](https://www.bokeyuan.net/)
- [孔皮皮的博客](https://www.kppkkp.top/)
- [运鹏的博客](https://yunpengtai.top/)
- [jiming.site](https://jiming.site/)
- [OmegaXYZ](https://www.omegaxyz.com/)
- [EAI猩球](https://www.robotech.ink/)
- [文举的博客](https://liwenju0.com/)
- [申请链接](https://kexue.fm/links.html)

[![署名-非商业用途-保持一致](https://kexue.fm/usr/themes/geekg/images/cc.gif)](http://creativecommons.org/licenses/by-nc-nd/2.5/cn/) 本站采用创作共用版权协议，要求署名、非商业用途和保持一致。转载本站内容必须也遵循“ [署名-非商业用途-保持一致](http://creativecommons.org/licenses/by-nc-nd/2.5/cn/)”的创作共用协议。



© 2009-2026 Scientific Spaces. All rights reserved. Theme by [laogui](http://www.laogui.com/). Powered by [Typecho](http://typecho.org/). 备案号: [粤ICP备09093259号-1/2](https://beian.miit.gov.cn/ "粤ICP备09093259号")。