## SEARCH

## MENU

- [打赏](https://kexue.fm/reward.html)
- [公式](https://kexue.fm/latex.html)
- [天象](https://kexue.fm/ac.html)
- [链接](https://kexue.fm/links.html)
- [时光](https://kexue.fm/me.html)
- [博览](https://kexue.fm/science.html)
- [归档](https://kexue.fm/content.html)

## CATEGORIES

- [千奇百怪](https://kexue.fm/category/Everything)
- [天文探索](https://kexue.fm/category/Astronomy)
- [数学研究](https://kexue.fm/category/Mathematics)
- [物理化学](https://kexue.fm/category/Phy-chem)
- [信息时代](https://kexue.fm/category/Big-Data)
- [生物自然](https://kexue.fm/category/Biology)
- [图片摄影](https://kexue.fm/category/Photograph)
- [问题百科](https://kexue.fm/category/Questions)
- [生活/情感](https://kexue.fm/category/Life-Feeling)
- [资源共享](https://kexue.fm/category/Resources)

## NEWPOSTS

- [“对角+低秩”三角阵的高效求逆方法](https://kexue.fm/archives/11072)
- [通过msign来计算奇异值裁剪mc...](https://kexue.fm/archives/11059)
- [矩阵符号函数mcsgn能计算什么？](https://kexue.fm/archives/11056)
- [线性注意力简史：从模仿、创新到反哺](https://kexue.fm/archives/11033)
- [msign的导数](https://kexue.fm/archives/11025)
- [通过msign来计算奇异值裁剪mc...](https://kexue.fm/archives/11006)
- [msign算子的Newton-Sc...](https://kexue.fm/archives/10996)
- [等值振荡定理：最优多项式逼近的充要条件](https://kexue.fm/archives/10972)
- [生成扩散模型漫谈（三十）：从瞬时速...](https://kexue.fm/archives/10958)
- [MoE环游记：5、均匀分布的反思](https://kexue.fm/archives/10945)

## COMMENTS

- [Kuo: 在 $PaTH$ 论文章节 \`UT Transform for...](https://kexue.fm/archives/11033/comment-page-1#comment-28053)
- [Fanhao: 假定Hessian阵正定，那不是意味着$L(\\theta)$是...](https://kexue.fm/archives/10542/comment-page-1#comment-28052)
- [曲笑一: 对于第一个疑问，我看到分布式的版本已经开源。我在想如果将每个梯...](https://kexue.fm/archives/10739/comment-page-2#comment-28051)
- [曲笑一: 苏老师您好，阅读了您关于Muon系列的博客，受益匪浅。在此有两...](https://kexue.fm/archives/10739/comment-page-2#comment-28050)
- [tll1945tll1937: 老师，您好，向您请教一个问题：会不会因为LoRA中用到的梯度的...](https://kexue.fm/archives/10266/comment-page-1#comment-28049)
- [香蕉大王: 还是刚刚flow matching的例子$\\frac{d x\_...](https://kexue.fm/archives/9280/comment-page-2#comment-28048)
- [香蕉大王: 谢谢老师回复。明白老师您说的了。我再想请问一个小小的问题：既然...](https://kexue.fm/archives/9280/comment-page-2#comment-28047)
- [NoAmateur: 2025年第一次了解到苏前辈，进入科学空间后怀着好奇翻阅苏前辈...](https://kexue.fm/archives/12/comment-page-7#comment-28046)
- [liukoulong: $T^{3}$](https://kexue.fm/archives/11033/comment-page-1#comment-28043)
- [nihaowhut: 请问ϵ为什么要限制到2^-7, 2^-6, .. 2^-1, ...](https://kexue.fm/archives/10617/comment-page-1#comment-28042)

## USERLOGIN

- [登录](https://kexue.fm/admin/login.php)

[科学空间\|Scientific Spaces](https://kexue.fm)

- [登录](https://kexue.fm/admin/login.php)
- [打赏](https://kexue.fm/reward.html)
- [公式](https://kexue.fm/latex.html)
- [天象](https://kexue.fm/ac.html)
- [链接](https://kexue.fm/links.html)
- [时光](https://kexue.fm/me.html)
- [博览](https://kexue.fm/science.html)
- [归档](https://kexue.fm/content.html)

渴望成为一个小飞侠

- [欢迎订阅](https://kexue.fm/feed)
- [个性邮箱](https://kexue.fm/archives/119)
- [天象信息](https://kexue.fm/ac.html)
- [观测ISS](https://kexue.fm/archives/41)
- [LaTeX](https://kexue.fm/latex.html)
- [关于博主](https://kexue.fm/me.html)

欢迎访问“科学空间”，这里将与您共同探讨自然科学，回味人生百态；也期待大家的分享～

- [**千奇百怪** Everything](https://kexue.fm/category/Everything)
- [**天文探索** Astronomy](https://kexue.fm/category/Astronomy)
- [**数学研究** Mathematics](https://kexue.fm/category/Mathematics)
- [**物理化学** Phy-chem](https://kexue.fm/category/Phy-chem)
- [**信息时代** Big-Data](https://kexue.fm/category/Big-Data)
- [**生物自然** Biology](https://kexue.fm/category/Biology)
- [**图片摄影** Photograph](https://kexue.fm/category/Photograph)
- [**问题百科** Questions](https://kexue.fm/category/Questions)
- [**生活/情感** Life-Feeling](https://kexue.fm/category/Life-Feeling)
- [**资源共享** Resources](https://kexue.fm/category/Resources)

- [**千奇百怪**](https://kexue.fm/category/Everything)
- [**天文探索**](https://kexue.fm/category/Astronomy)
- [**数学研究**](https://kexue.fm/category/Mathematics)
- [**物理化学**](https://kexue.fm/category/Phy-chem)
- [**信息时代**](https://kexue.fm/category/Big-Data)
- [**生物自然**](https://kexue.fm/category/Biology)
- [**图片摄影**](https://kexue.fm/category/Photograph)
- [**问题百科**](https://kexue.fm/category/Questions)
- [**生活/情感**](https://kexue.fm/category/Life-Feeling)
- [**资源共享**](https://kexue.fm/category/Resources)

[首页](https://kexue.fm) [数学研究](https://kexue.fm/category/Mathematics) 梯度流：探索通向最小值之路

16Jun

# [梯度流：探索通向最小值之路](https://kexue.fm/archives/9660)

By 苏剑林 \|
2023-06-16 \|
43960位读者\|

在这篇文章中，我们将探讨一个被称为“梯度流（Gradient Flow）”的概念。简单来说，梯度流是将我们在用梯度下降法中寻找最小值的过程中的各个点连接起来，形成一条随（虚拟的）时间变化的轨迹，这条轨迹便被称作“梯度流”。在文章的后半部分，我们将重点讨论如何将梯度流的概念扩展到概率空间，从而形成“Wasserstein梯度流”，为我们理解连续性方程、Fokker-Planck方程等内容提供一个新的视角。

## 梯度下降 [\#](https://kexue.fm/archives/9660\#%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D)

假设我们想搜索光滑函数$f(\\boldsymbol{x})$的最小值，常见的方案是梯度下降（Gradient Descent），即按照如下格式进行迭代：
\\begin{equation}\\boldsymbol{x}\_{t+1} = \\boldsymbol{x}\_t -\\alpha \\nabla\_{\\boldsymbol{x}\_t}f(\\boldsymbol{x}\_t)\\label{eq:gd-d}\\end{equation}
如果$f(\\boldsymbol{x})$关于$\\boldsymbol{x}$是凸的，那么梯度下降通常能够找到最小值点；相反，则通常只能收敛到一个“驻点”——即梯度为0的点，比较理想的情况下能收敛到一个极小值（局部最小值）点。这里没有对极小值和最小值做严格区分，因为在深度学习中，即便是收敛到一个极小值点也是很难得的了。

如果将$\\alpha$记为$\\Delta t$，将$\\boldsymbol{x}\_{t+1}$记为$\\boldsymbol{x}\_{t+\\Delta t}$，那么考虑$\\Delta t\\to 0$的极限，那么式$\\eqref{eq:gd-d}$将变为一个ODE：
\\begin{equation}\\frac{d\\boldsymbol{x}\_t}{dt} = -\\nabla\_{\\boldsymbol{x}\_t}f(\\boldsymbol{x}\_t)\\label{eq:gd-c}\\end{equation}
求解这个ODE所得到的轨迹$\\boldsymbol{x}\_t$，我们就称为“梯度流（Gradient Flow）”，也就是说，梯度流是梯度下降在寻找最小值过程中的轨迹。在式$\\eqref{eq:gd-c}$成立前提下，我们还有：
\\begin{equation}\\frac{df(\\boldsymbol{x}\_t)}{dt} = \\left\\langle\\nabla\_{\\boldsymbol{x}\_t}f(\\boldsymbol{x}\_t),\\frac{d\\boldsymbol{x}\_t}{dt}\\right\\rangle = -\\Vert\\nabla\_{\\boldsymbol{x}\_t}f(\\boldsymbol{x}\_t)\\Vert^2 \\leq 0\\end{equation}
这就意味着，只要$\\nabla\_{\\boldsymbol{x}\_t}f(\\boldsymbol{x}\_t)\\neq\\boldsymbol{0}$，那么当学习率足够小时，梯度下降总能往让$f(\\boldsymbol{x})$变小的方向前进。

更多相关讨论，可以参考之前的优化算法系列，如 [《从动力学角度看优化算法（一）：从SGD到动量加速》](https://kexue.fm/archives/5655)、 [《从动力学角度看优化算法（三）：一个更整体的视角》](https://kexue.fm/archives/6261) 等。

## 最速方向 [\#](https://kexue.fm/archives/9660\#%E6%9C%80%E9%80%9F%E6%96%B9%E5%90%91)

为什么要用梯度下降？一个主流的说法是“梯度的负方向是局部下降最快的方向”，直接搜这句话就可以搜到很多内容。这个说法不能说错，但有点不严谨，因为没说明前提条件——“最快”的“最”必然涉及到定量比较，只有先确定比较的指标，才能确定“最”的结果。

如果只关心下降最快的方向的话，梯度下降的目标应该是：
\\begin{equation}\\boldsymbol{x}\_{t+1} = \\mathop{\\text{argmin}}\_{\\boldsymbol{x},\\Vert\\boldsymbol{x} - \\boldsymbol{x}\_t\\Vert = \\epsilon} f(\\boldsymbol{x})\\label{eq:gd-min-co}\\end{equation}
假设一阶近似够用，那么有
\\begin{equation}\\begin{aligned}
f(\\boldsymbol{x})&\\,=f(\\boldsymbol{x}\_t) + \\langle \\nabla\_{\\boldsymbol{x}\_t}f(\\boldsymbol{x}\_t),\\boldsymbol{x} - \\boldsymbol{x}\_t\\rangle\\\
&\\,\\geq f(\\boldsymbol{x}\_t) - \\Vert\\nabla\_{\\boldsymbol{x}\_t}f(\\boldsymbol{x}\_t)\\Vert \\Vert\\boldsymbol{x} - \\boldsymbol{x}\_t\\Vert\\\
&\\,= f(\\boldsymbol{x}\_t) - \\Vert\\nabla\_{\\boldsymbol{x}\_t}f(\\boldsymbol{x}\_t)\\Vert \\epsilon\\\
\\end{aligned}\\end{equation}
等号成立的条件是
\\begin{equation}\\boldsymbol{x} - \\boldsymbol{x}\_t = -\\epsilon\\frac{\\nabla\_{\\boldsymbol{x}\_t}f(\\boldsymbol{x}\_t)}{\\Vert\\nabla\_{\\boldsymbol{x}\_t}f(\\boldsymbol{x}\_t)\\Vert}\\quad\\Rightarrow\\quad\\boldsymbol{x}\_{t+1} = \\boldsymbol{x}\_t - \\epsilon\\frac{\\nabla\_{\\boldsymbol{x}\_t}f(\\boldsymbol{x}\_t)}{\\Vert\\nabla\_{\\boldsymbol{x}\_t}f(\\boldsymbol{x}\_t)\\Vert}\\label{eq:gd-d-norm}
\\end{equation}
可以看到，更新方向正好是梯度的负方向，所以说它是局部下降最快的方向。然而，别忘了这是在约束条件$\\Vert\\boldsymbol{x} - \\boldsymbol{x}\_t\\Vert = \\epsilon$下得到的，其中$\\Vert\\cdot\\Vert$是欧氏空间的模长，如果换一个模长的定义，或者干脆换一个约束条件，那么结果就不一样了。所以，严谨来说应该是“在欧氏空间中，梯度的负方向是局部下降最快的方向”。

## 优化视角 [\#](https://kexue.fm/archives/9660\#%E4%BC%98%E5%8C%96%E8%A7%86%E8%A7%92)

式$\\eqref{eq:gd-min-co}$是一个带约束优化，推广和求解起来都会比较麻烦。此外，式$\\eqref{eq:gd-min-co}$的求解结果是式$\\eqref{eq:gd-d-norm}$，也不是原始的梯度下降$\\eqref{eq:gd-d}$。事实上，可以证明式$\\eqref{eq:gd-d}$对应的优化目标是
\\begin{equation}\\boldsymbol{x}\_{t+1} = \\mathop{\\text{argmin}}\_{\\boldsymbol{x}} \\frac{\\Vert\\boldsymbol{x} - \\boldsymbol{x}\_t\\Vert^2}{2\\alpha} + f(\\boldsymbol{x})\\label{eq:gd-min}\\end{equation}
也就是说，将约束当成惩罚项加入到优化目标，这样就不用考虑求解约束，也容易推广。而且，即便加入了额外的$\\frac{\\Vert\\boldsymbol{x} - \\boldsymbol{x}\_t\\Vert^2}{2\\alpha}$，也能保证上式的优化不会朝着让更糟糕的方向走，因为代入$\\boldsymbol{x} = \\boldsymbol{x}\_t$后很明显上述目标函数正好是$f(\\boldsymbol{x}\_t)$，所以$\\min\_{\\boldsymbol{x}}$的结果至少不会大于$f(\\boldsymbol{x}\_t)$。

当$\\alpha$足够小时，第一项占主导，因此$\\Vert\\boldsymbol{x} - \\boldsymbol{x}\_t\\Vert$需要足够小时第一项才会变得足够小，即最优点应该是很接近$\\boldsymbol{x}\_t$的，于是我们可以在$\\boldsymbol{x}\_t$处将$f(\\boldsymbol{x})$展开，得到
\\begin{equation}\\boldsymbol{x}\_{t+1} = \\mathop{\\text{argmin}}\_{\\boldsymbol{x}} \\frac{\\Vert\\boldsymbol{x} - \\boldsymbol{x}\_t\\Vert^2}{2\\alpha} + f(\\boldsymbol{x}\_t)+\\langle\\nabla\_{\\boldsymbol{x}\_t}f(\\boldsymbol{x}\_t),\\boldsymbol{x}-\\boldsymbol{x}\_t\\rangle\\end{equation}
此时只是一个二次函数最小值问题，求解结果正是式$\\eqref{eq:gd-d}$。

很明显，除了模长平方外，我们还可以考虑别的正则项，从而形成不同的梯度下降方案。比如，自然梯度下降（Natural Gradient Descent）使用的是KL散度作为正则项：
\\begin{equation}\\boldsymbol{x}\_{t+1} = \\mathop{\\text{argmin}}\_{\\boldsymbol{x}} \\frac{KL(p(\\boldsymbol{y}\|\\boldsymbol{x})\\Vert p(\\boldsymbol{y}\|\\boldsymbol{x}\_t))}{\\alpha} + f(\\boldsymbol{x})\\end{equation}
其中$p(\\boldsymbol{y}\|\\boldsymbol{x})$是某个与$f(\\boldsymbol{x})$相关的概率分布。为了求解上式，同样在$f(\\boldsymbol{x})$处进行展开，$f(\\boldsymbol{x})$同样展开到一阶，但是KL散度比较特殊，它展开到一阶还是零（参考 [这里](https://kexue.fm/archives/7466#%E5%8B%87%E6%95%A2%E5%9C%B0%E7%AE%97)），所以至少要展开到二阶，总的结果是
\\begin{equation}\\boldsymbol{x}\_{t+1} = \\mathop{\\text{argmin}}\_{\\boldsymbol{x}} \\frac{(\\boldsymbol{x}-\\boldsymbol{x}\_t)^{\\top}\\boldsymbol{F}(\\boldsymbol{x}-\\boldsymbol{x}\_t)}{2\\alpha} + f(\\boldsymbol{x}\_t)+\\langle\\nabla\_{\\boldsymbol{x}\_t}f(\\boldsymbol{x}\_t),\\boldsymbol{x}-\\boldsymbol{x}\_t\\rangle\\end{equation}
这里的$\\boldsymbol{F}$是 [Fisher信息矩阵](https://en.wikipedia.org/wiki/Fisher_information)，计算细节就不展开了，过程也可以参考 [这里](https://kexue.fm/archives/7466#%E5%8B%87%E6%95%A2%E5%9C%B0%E7%AE%97)。现在上式本质上也是二次函数的最小值问题，结果为
\\begin{equation}\\boldsymbol{x}\_{t+1} = \\boldsymbol{x}\_t -\\alpha \\boldsymbol{F}^{-1}\\nabla\_{\\boldsymbol{x}\_t}f(\\boldsymbol{x}\_t)\\end{equation}
这就是所谓的“自然梯度下降”。

## 泛函入门 [\#](https://kexue.fm/archives/9660\#%E6%B3%9B%E5%87%BD%E5%85%A5%E9%97%A8)

式$\\eqref{eq:gd-min}$不仅可以将正则项一般化，还可以将求解目标一般化，比如推广到泛函。

“泛函”看起来让人“犯寒”，但事实上对于本站的老读者来说应该接触多次了。简单来说，普通多元函数就是输入一个向量，输出一个标量，泛函则是输入一个函数，输出一个标量，比如定积分运算：
\\begin{equation}\\mathcal{I}\[f\] = \\int\_a^b f(x)dx\\end{equation}
对于任意一个函数$f$，$\\mathcal{I}\[f\]$的计算结果就是一个标量，所以$\\mathcal{I}\[f\]$就是一个泛函。又比如前面提到的KL散度，它定义为
\\begin{equation}KL(p\\Vert q) = \\int p(\\boldsymbol{x})\\log \\frac{p(\\boldsymbol{x})}{q(\\boldsymbol{x})}d\\boldsymbol{x}\\end{equation}
这里积分默认为全空间积分，如果固定$p(\\boldsymbol{x})$，那么它就是关于$q(\\boldsymbol{x})$的泛函，因为$q(\\boldsymbol{x})$是一个函数，输入一个满足条件的函数，$KL(p\\Vert q)$将输出一个标量。更一般地， [《f-GAN简介：GAN模型的生产车间》](https://kexue.fm/archives/6016) 所介绍的$f$散度，也是泛函的一种，这些都是比较简单的泛函，更复杂的泛函可能包含输入函数的导数，如理论物理的 [最小作用量](https://kexue.fm/archives/1304)。

下面我们主要关注的泛函的定义域为全体概率密度函数的集合，即研究输入一个概率密度、输出一个标量的泛函。

## 概率之流 [\#](https://kexue.fm/archives/9660\#%E6%A6%82%E7%8E%87%E4%B9%8B%E6%B5%81)

假如我们有一个泛函$\\mathcal{F}\[q\]$，想要计算它的最小值，那么模仿梯度下降的思路，只要我们能求出它的某种梯度，那么就可以沿着它的负方向进行迭代。

为了确定迭代格式，我们沿着前面的思考，考虑推广式$\\eqref{eq:gd-min}$，其中$f(\\boldsymbol{x})$自然是替换为$\\mathcal{F}\[q\]$，那么第一项正则应该替换成什么呢？在式$\\eqref{eq:gd-min}$中它是欧氏距离的平方，那么很自然想到这里也应该替换为某种距离的平方，对于概率分布来说，性态比较好的距离是Wasserstein距离（准确来说是“2-Wasserstein距离”）：
\\begin{equation}\\mathcal{W}\_2\[p,q\]=\\sqrt{\\inf\_{\\gamma\\in \\Pi\[p,q\]} \\iint \\gamma(\\boldsymbol{x},\\boldsymbol{y}) \\Vert\\boldsymbol{x}-\\boldsymbol{y}\\Vert^2 d\\boldsymbol{x}d\\boldsymbol{y}}\\end{equation}
关于它的介绍，这里就不详细展开了，有兴趣的读者请参考 [《从Wasserstein距离、对偶理论到WGAN》](https://kexue.fm/archives/6280)。如果进一步将式$\\eqref{eq:gd-min}$中的欧氏距离替换为Wasserstein距离，那么最终目标就是
\\begin{equation}q\_{t+1} = \\mathop{\\text{argmin}}\_{q} \\frac{\\mathcal{W}\_2^2\[q,q\_t\]}{2\\alpha} + \\mathcal{F}\[q\]\\end{equation}
很抱歉，笔者没法简明给出上述目标的求解过程，甚至笔者自己也没完全理解它的求解过程，只能根据 [《Introduction to Gradient Flows in the 2-Wasserstein Space》](https://abdulfatir.com/blog/2020/Gradient-Flows/)、 [《{ Euclidean, Metric, and Wasserstein } Gradient Flows: an overview》](https://papers.cool/arxiv/1609.03890) 等文献，直接给出它的求解结果为
\\begin{equation}q\_{t+1}(\\boldsymbol{x}) = q\_t(\\boldsymbol{x}) + \\alpha \\nabla\_{\\boldsymbol{x}}\\cdot\\left(q\_t(\\boldsymbol{x})\\nabla\_{\\boldsymbol{x}}\\frac{\\delta \\mathcal{F}\[q\_t(\\boldsymbol{x})\]}{\\delta q\_t(\\boldsymbol{x})}\\right)\\end{equation}
或者取极限后得到
\\begin{equation}\\frac{\\partial q\_t(\\boldsymbol{x})}{\\partial t} = \\nabla\_{\\boldsymbol{x}}\\cdot\\left(q\_t(\\boldsymbol{x})\\nabla\_{\\boldsymbol{x}}\\frac{\\delta \\mathcal{F}\[q\_t(\\boldsymbol{x})\]}{\\delta q\_t(\\boldsymbol{x})}\\right)\\end{equation}
这就是“Wasserstein梯度流（Wasserstein Gradient Flow）”，其中$\\frac{\\delta \\mathcal{F}\[q\]}{\\delta q}$是$\\mathcal{F}\[q\]$的变分导数，对于定积分泛函来说，变分导数就是被积函数的导数：
\\begin{equation}\\mathcal{F}\[q\] = \\int F(q(\\boldsymbol{x}))d\\boldsymbol{x} \\quad\\Rightarrow\\quad \\frac{\\delta \\mathcal{F}\[q(\\boldsymbol{x})\]}{\\delta q(\\boldsymbol{x})} = \\frac{\\partial F(q(\\boldsymbol{x}))}{\\partial q(\\boldsymbol{x})}\\end{equation}

## 一些例子 [\#](https://kexue.fm/archives/9660\#%E4%B8%80%E4%BA%9B%E4%BE%8B%E5%AD%90)

根据 [《f-GAN简介：GAN模型的生产车间》](https://kexue.fm/archives/6016)，$f$散度的定义为
\\begin{equation}\\mathcal{D}\_f(p\\Vert q) = \\int q(\\boldsymbol{x}) f\\left(\\frac{p(\\boldsymbol{x})}{q(\\boldsymbol{x})}\\right)d\\boldsymbol{x}\\end{equation}
将$p$固定，设$\\mathcal{F}\[q\]=\\mathcal{D}\_f(p\\Vert q)$，那么得到
\\begin{equation}\\frac{\\partial q\_t(\\boldsymbol{x})}{\\partial t} = \\nabla\_{\\boldsymbol{x}}\\cdot\\Big(q\_t(\\boldsymbol{x})\\nabla\_{\\boldsymbol{x}}\\big(f(r\_t(\\boldsymbol{x})) - r\_t(\\boldsymbol{x}) f'(r\_t(\\boldsymbol{x}))\\big)\\Big)\\label{eq:wgd}\\end{equation}
其中$r\_t(\\boldsymbol{x}) = \\frac{p(\\boldsymbol{x})}{q\_t(\\boldsymbol{x})}$。根据 [《测试函数法推导连续性方程和Fokker-Planck方程》](https://kexue.fm/archives/9461) 的内容，上式具备连续性方程的形式，所以通过ODE
\\begin{equation}\\frac{d\\boldsymbol{x}}{dt} = -\\nabla\_{\\boldsymbol{x}}\\big(f(r\_t(\\boldsymbol{x})) - r\_t(\\boldsymbol{x}) f'(r\_t(\\boldsymbol{x}))\\big)\\end{equation}
可以实现从分布$q\_t$中采样，而根据前面的讨论，式$\\eqref{eq:wgd}$是最小化$p,q$的$f$散度的Wasserstein梯度流，当$t\\to\\infty$时$f$散度为零，即$q\_t=p$，所以$t\\to\\infty$时，上述ODE实现了从分布$p$采样。不过，这个结果目前来说只有形式上的意义，并没有实际作用，因为这意味着我们要知道分布$p$的表达式，还要从式$\\eqref{eq:wgd}$中解出$q\_t$的表达式，然后才能算出ODE右端式子，从而完成采样，这个计算难度非常大，通常是没法完成的。

一个相对简单的例子是（逆）KL散度，此时$f=-\\log$，代入式$\\eqref{eq:wgd}$得到
\\begin{equation}\\begin{aligned}\\frac{\\partial q\_t(\\boldsymbol{x})}{\\partial t} =&\\, - \\nabla\_{\\boldsymbol{x}}\\cdot\\left(q\_t(\\boldsymbol{x})\\nabla\_{\\boldsymbol{x}}\\log \\frac{p(\\boldsymbol{x})}{q\_t(\\boldsymbol{x})}\\right)\\\
=&\\, - \\nabla\_{\\boldsymbol{x}}\\cdot\\Big(q\_t(\\boldsymbol{x})\\nabla\_{\\boldsymbol{x}}\\big(\\log p(\\boldsymbol{x}) - \\log q\_t(\\boldsymbol{x})\\big)\\Big)\\\
=&\\, - \\nabla\_{\\boldsymbol{x}}\\cdot\\big(q\_t(\\boldsymbol{x})\\nabla\_{\\boldsymbol{x}}\\log p(\\boldsymbol{x})\\big) + \\nabla\_{\\boldsymbol{x}}\\cdot\\nabla\_{\\boldsymbol{x}} q\_t(\\boldsymbol{x})
\\end{aligned}\\end{equation}
再次对比 [《测试函数法推导连续性方程和Fokker-Planck方程》](https://kexue.fm/archives/9461) 的结果，这正好是个Fokker-Planck方程，对应于SDE：
\\begin{equation}d\\boldsymbol{x} = \\nabla\_{\\boldsymbol{x}}\\log p(\\boldsymbol{x}) dt + \\sqrt{2}dw\\end{equation}
也就是说，如果我们知道$\\log p(\\boldsymbol{x})$，那么就可以实现用上式实现从$p(\\boldsymbol{x})$中采样，相比前面的ODE，免除了求解$q\_t(\\boldsymbol{x})$的过程，是一个相对可用的方案。

## 文章小结 [\#](https://kexue.fm/archives/9660\#%E6%96%87%E7%AB%A0%E5%B0%8F%E7%BB%93)

本文介绍了梯度下降求最小值过程中的“梯度流”概念，其中包括向量空间的梯度流到概率空间的Wasserstein梯度流的拓展，以及它们与连续性方程、Fokker-Planck方程和ODE/SDE采样之间的联系。

_**转载到请包括本文地址：** [https://kexue.fm/archives/9660](https://kexue.fm/archives/9660)_

_**更详细的转载事宜请参考：**_ [《科学空间FAQ》](https://kexue.fm/archives/6508#%E6%96%87%E7%AB%A0%E5%A6%82%E4%BD%95%E8%BD%AC%E8%BD%BD/%E5%BC%95%E7%94%A8)

**如果您还有什么疑惑或建议，欢迎在下方评论区继续讨论。**

**如果您觉得本文还不错，欢迎 [分享](https://kexue.fm/archives/9660#share)/ [打赏](https://kexue.fm/archives/9660#pay) 本文。打赏并非要从中获得收益，而是希望知道科学空间获得了多少读者的真心关注。当然，如果你无视它，也不会影响你的阅读。再次表示欢迎和感谢！**

打赏

微信打赏

支付宝打赏

因为网站后台对打赏并无记录，因此欢迎在打赏时候备注留言。你还可以 [**点击这里**](http://mail.qq.com/cgi-bin/qm_share?t=qm_mailme&email=tN7d1drY3drrx8H0xcWa19vZ) 或在下方评论区留言来告知你的建议或需求。

**如果您需要引用本文，请参考：**

苏剑林. (Jun. 16, 2023). 《梯度流：探索通向最小值之路 》\[Blog post\]. Retrieved from [https://kexue.fm/archives/9660](https://kexue.fm/archives/9660)

@online{kexuefm-9660,
        title={梯度流：探索通向最小值之路},
        author={苏剑林},
        year={2023},
        month={Jun},
        url={\\url{https://kexue.fm/archives/9660}},
}

分类： [数学研究](https://kexue.fm/category/Mathematics)    标签： [泛函](https://kexue.fm/tag/%E6%B3%9B%E5%87%BD/), [动力学](https://kexue.fm/tag/%E5%8A%A8%E5%8A%9B%E5%AD%A6/), [优化](https://kexue.fm/tag/%E4%BC%98%E5%8C%96/), [梯度](https://kexue.fm/tag/%E6%A2%AF%E5%BA%A6/)[12 评论](https://kexue.fm/archives/9660#comments)

< [Naive Bayes is all you need ?](https://kexue.fm/archives/9648) \| [生成扩散模型漫谈（十九）：作为扩散ODE的GAN](https://kexue.fm/archives/9662) >

### 你也许还对下面的内容感兴趣

- [msign的导数](https://kexue.fm/archives/11025)
- [MoE环游记：5、均匀分布的反思](https://kexue.fm/archives/10945)
- [Transformer升级之路：20、MLA好在哪里?（上）](https://kexue.fm/archives/10907)
- [SVD的导数](https://kexue.fm/archives/10878)
- [通过梯度近似寻找Normalization的替代品](https://kexue.fm/archives/10831)
- [MoE环游记：4、难处应当多投入](https://kexue.fm/archives/10815)
- [高阶muP：更简明但更高明的谱条件缩放](https://kexue.fm/archives/10795)
- [初探muP：超参数的跨模型尺度迁移规律](https://kexue.fm/archives/10770)
- [MoE环游记：3、换个思路来分配](https://kexue.fm/archives/10757)
- [Muon续集：为什么我们选择尝试Muon？](https://kexue.fm/archives/10739)

[发表你的看法](https://kexue.fm/archives/9660#comment_form)

[梯度流：探索通往最小值之路 R11; AI 資訊](https://news.aitime.space/2023/06/5062/)

June 16th, 2023

\[...\]​Read More \[...\]

[回复评论](https://kexue.fm/archives/9660/comment-page-1?replyTo=21999#respond-post-9660)

[chuan](https://kanchuan.com)

June 19th, 2023

我对这个博客皮肤倍感亲切，很早之前用过一段时间。数学公式已经看不懂了，支持下博主

[回复评论](https://kexue.fm/archives/9660/comment-page-1?replyTo=22025#respond-post-9660)

[苏剑林](https://kexue.fm) 发表于
June 25th, 2023

用过这个皮肤的怕是都是老建站人了哈哈

[回复评论](https://kexue.fm/archives/9660/comment-page-1?replyTo=22046#respond-post-9660)

王珣

July 3rd, 2023

小笔误：负责-->复杂

[回复评论](https://kexue.fm/archives/9660/comment-page-1?replyTo=22143#respond-post-9660)

[苏剑林](https://kexue.fm) 发表于
July 3rd, 2023

谢谢，已经修正。

[回复评论](https://kexue.fm/archives/9660/comment-page-1?replyTo=22152#respond-post-9660)

chernzy1

September 27th, 2023

“如果换一个模长的定义，或者干脆换一个约束条件，那么结果就不一样了”，这句话怎么理解，可以举个反例么。

[回复评论](https://kexue.fm/archives/9660/comment-page-1?replyTo=22814#respond-post-9660)

[苏剑林](https://kexue.fm) 发表于
December 12th, 2023

“优化视角”一节中的自然梯度下降就是反例了呀，它选择了加权的模长，结果就是自然梯度下降而不是梯度下降。

[回复评论](https://kexue.fm/archives/9660/comment-page-1?replyTo=23297#respond-post-9660)

[Jiming Zheng](https://jiming.site)

December 12th, 2023

梯度流的表达式里，优化对象函数是一方面，另一方面是选用了Wassterstein metric才能得到这样的形式。用otto calculus可以把optimal transport问题表述成与continuity equation有关的微分几何形式，其中metric必须取Wasserstein metric，而后Benamou-Breniour公式表明Wasserstein distance符合该形式下的最小作用量原理。将Wasserstein metric给出的梯度流用欧氏空间的微分算子表达出来就是文中的形式了。

[回复评论](https://kexue.fm/archives/9660/comment-page-1?replyTo=23282#respond-post-9660)

[苏剑林](https://kexue.fm) 发表于
December 12th, 2023

感谢指点，看上去就明白暂时还不是我能理解的，后面再来请教。

[回复评论](https://kexue.fm/archives/9660/comment-page-1?replyTo=23294#respond-post-9660)

[Jiming Zheng](https://jiming.site)

December 12th, 2023

它与Fokker-Planck的关系在于，Fokker-Planck刚好是continuity equation，因此它的解一定是Wasserstein space的梯度流

[回复评论](https://kexue.fm/archives/9660/comment-page-1?replyTo=23283#respond-post-9660)

[Peng](https://github.com/tribbloid)

July 17th, 2024

+F\[q\]应该改成+q

这个用线性微分算子也不好推？

如果可以的化，最好用自动微分推，微分和链式雅可比连乘当成线性微分算子的特例

[回复评论](https://kexue.fm/archives/9660/comment-page-1?replyTo=24854#respond-post-9660)

[苏剑林](https://kexue.fm) 发表于
July 18th, 2024

哪里应该改？

[回复评论](https://kexue.fm/archives/9660/comment-page-1?replyTo=24867#respond-post-9660)

[取消回复](https://kexue.fm/archives/9660#respond-post-9660)

你的大名

电子邮箱

个人网站（选填）

1\. 可以使用LaTeX代码，点击“预览效果”可查看效果；2. 可以通过点击评论楼层编号来引用该楼层；3. 网站可能会有点卡，如非确认评论失败，请 **不要重复点击提交**。

### 内容速览

[梯度下降](https://kexue.fm/archives/9660#%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D)
[最速方向](https://kexue.fm/archives/9660#%E6%9C%80%E9%80%9F%E6%96%B9%E5%90%91)
[优化视角](https://kexue.fm/archives/9660#%E4%BC%98%E5%8C%96%E8%A7%86%E8%A7%92)
[泛函入门](https://kexue.fm/archives/9660#%E6%B3%9B%E5%87%BD%E5%85%A5%E9%97%A8)
[概率之流](https://kexue.fm/archives/9660#%E6%A6%82%E7%8E%87%E4%B9%8B%E6%B5%81)
[一些例子](https://kexue.fm/archives/9660#%E4%B8%80%E4%BA%9B%E4%BE%8B%E5%AD%90)
[文章小结](https://kexue.fm/archives/9660#%E6%96%87%E7%AB%A0%E5%B0%8F%E7%BB%93)

### 智能搜索

支持整句搜索！网站自动使用 [结巴分词](https://github.com/fxsjy/jieba) 进行分词，并结合ngrams排序算法给出合理的搜索结果。

### 热门标签

[生成模型](https://kexue.fm/tag/%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B/) [attention](https://kexue.fm/tag/attention/) [优化](https://kexue.fm/tag/%E4%BC%98%E5%8C%96/) [语言模型](https://kexue.fm/tag/%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/) [模型](https://kexue.fm/tag/%E6%A8%A1%E5%9E%8B/) [网站](https://kexue.fm/tag/%E7%BD%91%E7%AB%99/) [概率](https://kexue.fm/tag/%E6%A6%82%E7%8E%87/) [梯度](https://kexue.fm/tag/%E6%A2%AF%E5%BA%A6/) [转载](https://kexue.fm/tag/%E8%BD%AC%E8%BD%BD/) [矩阵](https://kexue.fm/tag/%E7%9F%A9%E9%98%B5/) [微分方程](https://kexue.fm/tag/%E5%BE%AE%E5%88%86%E6%96%B9%E7%A8%8B/) [天象](https://kexue.fm/tag/%E5%A4%A9%E8%B1%A1/) [分析](https://kexue.fm/tag/%E5%88%86%E6%9E%90/) [深度学习](https://kexue.fm/tag/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/) [积分](https://kexue.fm/tag/%E7%A7%AF%E5%88%86/) [python](https://kexue.fm/tag/python/) [优化器](https://kexue.fm/tag/%E4%BC%98%E5%8C%96%E5%99%A8/) [力学](https://kexue.fm/tag/%E5%8A%9B%E5%AD%A6/) [无监督](https://kexue.fm/tag/%E6%97%A0%E7%9B%91%E7%9D%A3/) [扩散](https://kexue.fm/tag/%E6%89%A9%E6%95%A3/) [几何](https://kexue.fm/tag/%E5%87%A0%E4%BD%95/) [节日](https://kexue.fm/tag/%E8%8A%82%E6%97%A5/) [生活](https://kexue.fm/tag/%E7%94%9F%E6%B4%BB/) [文本生成](https://kexue.fm/tag/%E6%96%87%E6%9C%AC%E7%94%9F%E6%88%90/) [数论](https://kexue.fm/tag/%E6%95%B0%E8%AE%BA/)

### 随机文章

- [视频演示：费曼的茶杯](https://kexue.fm/archives/2345)
- [Openwrt自动扫描WiFi并连接中继](https://kexue.fm/archives/3644)
- [网站本次改版感悟...](https://kexue.fm/archives/10)
- [科学空间：2010年8月重要天象](https://kexue.fm/archives/795)
- [《冲出亚马逊》—在世界立起五星红旗！](https://kexue.fm/archives/111)
- [中文任务还是SOTA吗？我们给SimCSE补充了一些实验](https://kexue.fm/archives/8348)
- [强大的NVAE：以后再也不能说VAE生成的图像模糊了](https://kexue.fm/archives/7574)
- [班门弄斧：Python的代码能有多简洁？](https://kexue.fm/archives/2971)
- [2020年全年天象](https://kexue.fm/archives/7144)
- [生成扩散模型漫谈（三十）：从瞬时速度到平均速度](https://kexue.fm/archives/10958)

### 最近评论

- [Kuo](https://kexue.fm/archives/11033/comment-page-1#comment-28053): 在 $PaTH$ 论文章节 \`UT Transform for Products of Hou...
- [Fanhao](https://kexue.fm/archives/10542/comment-page-1#comment-28052): 假定Hessian阵正定，那不是意味着$L(\\theta)$是$\\theta$的凸函数吗？这一...
- [曲笑一](https://kexue.fm/archives/10739/comment-page-2#comment-28051): 对于第一个疑问，我看到分布式的版本已经开源。我在想如果将每个梯度矩阵G拆分为N\*N,再利用mu...
- [曲笑一](https://kexue.fm/archives/10739/comment-page-2#comment-28050): 苏老师您好，阅读了您关于Muon系列的博客，受益匪浅。在此有两个疑问想请教您：第一个问题是，M...
- [tll1945tll1937](https://kexue.fm/archives/10266/comment-page-1#comment-28049): 老师，您好，向您请教一个问题：会不会因为LoRA中用到的梯度的维度仅仅是全参数微调中梯度的维度...
- [香蕉大王](https://kexue.fm/archives/9280/comment-page-2#comment-28048): 还是刚刚flow matching的例子$\\frac{d x\_t}{dt} = v\_\\thet...
- [香蕉大王](https://kexue.fm/archives/9280/comment-page-2#comment-28047): 谢谢老师回复。明白老师您说的了。我再想请问一个小小的问题：既然ODE未必是唯一的，有没有人尝试...
- [NoAmateur](https://kexue.fm/archives/12/comment-page-7#comment-28046): 2025年第一次了解到苏前辈，进入科学空间后怀着好奇翻阅苏前辈的点滴，让我备受震撼，苏前辈对科...
- [liukoulong](https://kexue.fm/archives/11033/comment-page-1#comment-28043): $T^{3}$
- [nihaowhut](https://kexue.fm/archives/10617/comment-page-1#comment-28042): 请问ϵ为什么要限制到2^-7, 2^-6, .. 2^-1, 2^0这8个数，感觉没有必要

### 友情链接

- [Cool Papers](https://papers.cool)
- [数学研发](https://bbs.emath.ac.cn)
- [Seatop](http://www.seatop.com.cn/)
- [Xiaoxia](https://xiaoxia.org/)
- [积分表-网络版](https://kexue.fm/sci/integral/index.html)
- [丝路博傲](http://blog.dvxj.com/)
- [ph4ntasy 饭特稀](http://www.ph4ntasy.com/)
- [数学之家](http://www.2math.cn/)
- [有趣天文奇观](http://interesting-sky.china-vo.org/)
- [TwistedW](http://www.twistedwg.com/)
- [godweiyang](https://godweiyang.com/)
- [AI柠檬](https://blog.ailemon.net/)
- [王登科-DK博客](https://greatdk.com)
- [ESON](https://blog.eson.org/)
- [枫之羽](https://fzhiy.net/)
- [Mathor's blog](https://wmathor.com/)
- [coding-zuo](https://coding-zuo.github.io/)
- [博科园](https://www.bokeyuan.net/)
- [孔皮皮的博客](https://www.kppkkp.top/)
- [运鹏的博客](https://yunpengtai.top/)
- [jiming.site](https://jiming.site/)
- [OmegaXYZ](https://www.omegaxyz.com/)
- [Blog by Eacls](https://www.eacls.top/)
- [EAI猩球](https://www.robotech.ink/)
- [文举的博客](https://liwenju0.com/)
- [用代码打点酱油](https://bruceyuan.com/)
- [申请链接](https://kexue.fm/links.html)

本站采用创作共用版权协议，要求署名、非商业用途和保持一致。转载本站内容必须也遵循“ [署名-非商业用途-保持一致](http://creativecommons.org/licenses/by-nc-nd/2.5/cn/)”的创作共用协议。
© 2009-2025 Scientific Spaces. All rights reserved. Theme by [laogui](http://www.laogui.com). Powered by [Typecho](http://typecho.org). 备案号: [粤ICP备09093259号-1/2](https://beian.miit.gov.cn/)。