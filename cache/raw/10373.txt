![MobileSideBar](https://kexue.fm/usr/themes/geekg/images/slide-button.png)

## SEARCH

## MENU

- [打赏](https://kexue.fm/reward.html)
- [公式](https://kexue.fm/latex.html)
- [天象](https://kexue.fm/ac.html)
- [链接](https://kexue.fm/links.html)
- [时光](https://kexue.fm/me.html)
- [博览](https://kexue.fm/science.html)
- [归档](https://kexue.fm/content.html)

## CATEGORIES

- [千奇百怪](https://kexue.fm/category/Everything)
- [天文探索](https://kexue.fm/category/Astronomy)
- [数学研究](https://kexue.fm/category/Mathematics)
- [物理化学](https://kexue.fm/category/Phy-chem)
- [信息时代](https://kexue.fm/category/Big-Data)
- [生物自然](https://kexue.fm/category/Biology)
- [图片摄影](https://kexue.fm/category/Photograph)
- [问题百科](https://kexue.fm/category/Questions)
- [生活/情感](https://kexue.fm/category/Life-Feeling)
- [资源共享](https://kexue.fm/category/Resources)

## NEWPOSTS

- [让炼丹更科学一些（五）：基于梯度精...](https://kexue.fm/archives/11530)
- [让炼丹更科学一些（四）：新恒等式，...](https://kexue.fm/archives/11494)
- [为什么DeltaNet要加L2 N...](https://kexue.fm/archives/11486)
- [让炼丹更科学一些（三）：SGD的终...](https://kexue.fm/archives/11480)
- [让炼丹更科学一些（二）：将结论推广...](https://kexue.fm/archives/11469)
- [滑动平均视角下的权重衰减和学习率](https://kexue.fm/archives/11459)
- [生成扩散模型漫谈（三十一）：预测数...](https://kexue.fm/archives/11428)
- [Muon优化器指南：快速上手与关键细节](https://kexue.fm/archives/11416)
- [AdamW的Weight RMS的...](https://kexue.fm/archives/11404)
- [n个正态随机数的最大值的渐近估计](https://kexue.fm/archives/11390)

## COMMENTS

- [Bin: 今天偶然从某个论坛看到有人推荐您的博客，定睛一看竟然是华师同院...](https://kexue.fm/archives/1990/comment-page-2#comment-29105)
- [Rapture D: 我有一个问题，为什么不考虑亥姆霍兹定理和斯托克斯公式。](https://kexue.fm/archives/11530/comment-page-1#comment-29104)
- [mofheka: 苏神是还在用jax是么？最近在做基于Google Pathwa...](https://kexue.fm/archives/11390/comment-page-1#comment-29103)
- [长琴: 看懂这篇博客也不是一件容易的事情。](https://kexue.fm/archives/11530/comment-page-1#comment-29102)
- [AlexLi: 苏老师，请教一下(7)式中将 $\\mu(x\_t)$ 传给 $p...](https://kexue.fm/archives/9257/comment-page-4#comment-29101)
- [tyler\_zxc: "Performer的思想是将标准的Attention线性化，...](https://kexue.fm/archives/7921/comment-page-2#comment-29100)
- [我: 似乎并非mHC提出矩阵的思想？之前hyper connecti...](https://kexue.fm/archives/11494/comment-page-1#comment-29099)
- [winter: 苏神您好，假如对于比较均匀的attention weightP...](https://kexue.fm/archives/10847/comment-page-1#comment-29098)
- [苏剑林: KL散度、JS散度、W距离啥的，都行啊，看你喜欢哪个](https://kexue.fm/archives/8512/comment-page-2#comment-29097)
- [苏剑林: 没有绝对公平的对比方法，主要看你关心什么。比如，如果只关心推理...](https://kexue.fm/archives/9119/comment-page-14#comment-29096)

## USERLOGIN

- [登录](https://kexue.fm/admin/login.php)

[科学空间\|Scientific Spaces](https://kexue.fm/)

- [登录](https://kexue.fm/admin/login.php)
- [打赏](https://kexue.fm/reward.html)
- [公式](https://kexue.fm/latex.html)
- [天象](https://kexue.fm/ac.html)
- [链接](https://kexue.fm/links.html)
- [时光](https://kexue.fm/me.html)
- [博览](https://kexue.fm/science.html)
- [归档](https://kexue.fm/content.html)

渴望成为一个小飞侠

- [![](https://kexue.fm/usr/themes/geekg/images/rss.png)\\
\\
欢迎订阅](https://kexue.fm/feed)
- [![](https://kexue.fm/usr/themes/geekg/images/mail.png)\\
\\
个性邮箱](https://kexue.fm/archives/119)
- [![](https://kexue.fm/usr/themes/geekg/images/Saturn.png)\\
\\
天象信息](https://kexue.fm/ac.html)
- [![](https://kexue.fm/usr/themes/geekg/images/iss.png)\\
\\
观测ISS](https://kexue.fm/archives/41)
- [![](https://kexue.fm/usr/themes/geekg/images/pi.png)\\
\\
LaTeX](https://kexue.fm/latex.html)
- [![](https://kexue.fm/usr/themes/geekg/images/mlogo.png)\\
\\
关于博主](https://kexue.fm/me.html)

欢迎访问“科学空间”，这里将与您共同探讨自然科学，回味人生百态；也期待大家的分享～

- [**千奇百怪** Everything](https://kexue.fm/category/Everything)
- [**天文探索** Astronomy](https://kexue.fm/category/Astronomy)
- [**数学研究** Mathematics](https://kexue.fm/category/Mathematics)
- [**物理化学** Phy-chem](https://kexue.fm/category/Phy-chem)
- [**信息时代** Big-Data](https://kexue.fm/category/Big-Data)
- [**生物自然** Biology](https://kexue.fm/category/Biology)
- [**图片摄影** Photograph](https://kexue.fm/category/Photograph)
- [**问题百科** Questions](https://kexue.fm/category/Questions)
- [**生活/情感** Life-Feeling](https://kexue.fm/category/Life-Feeling)
- [**资源共享** Resources](https://kexue.fm/category/Resources)

- [**千奇百怪**](https://kexue.fm/category/Everything)
- [**天文探索**](https://kexue.fm/category/Astronomy)
- [**数学研究**](https://kexue.fm/category/Mathematics)
- [**物理化学**](https://kexue.fm/category/Phy-chem)
- [**信息时代**](https://kexue.fm/category/Big-Data)
- [**生物自然**](https://kexue.fm/category/Biology)
- [**图片摄影**](https://kexue.fm/category/Photograph)
- [**问题百科**](https://kexue.fm/category/Questions)
- [**生活/情感**](https://kexue.fm/category/Life-Feeling)
- [**资源共享**](https://kexue.fm/category/Resources)

[首页](https://kexue.fm/) [数学研究](https://kexue.fm/category/Mathematics) Softmax后传：寻找Top-K的光滑近似

19Sep

# [Softmax后传：寻找Top-K的光滑近似](https://kexue.fm/archives/10373)

By 苏剑林 \|
2024-09-19 \|
73238位读者 \|

Softmax，顾名思义是“soft的max”，是$\\max$算子（准确来说是$\\text{argmax}$）的光滑近似，它通过指数归一化将任意向量$\\boldsymbol{x}\\in\\mathbb{R}^n$转化为分量非负且和为1的新向量，并允许我们通过温度参数来调节它与$\\text{argmax}$（的one hot形式）的近似程度。除了指数归一化外，我们此前在 [《通向概率分布之路：盘点Softmax及其替代品》](https://kexue.fm/archives/10145) 也介绍过其他一些能实现相同效果的方案。

我们知道，最大值通常又称Top-1，它的光滑近似方案看起来已经相当成熟，那读者有没有思考过，一般的Top-$k$的光滑近似又是怎么样的呢？下面让我们一起来探讨一下这个问题。

## 问题描述 [\#](https://kexue.fm/archives/10373\#%E9%97%AE%E9%A2%98%E6%8F%8F%E8%BF%B0)

设向量$\\boldsymbol{x}=(x\_1,x\_2,\\cdots,x\_n)\\in\\mathbb{R}^n$，简单起见我们假设它们两两不相等，即$i\\neq j \\Leftrightarrow x\_i\\neq x\_j$。记$\\Omega\_k(\\boldsymbol{x})$为$\\boldsymbol{x}$最大的$k$个分量的下标集合，即$\|\\Omega\_k(\\boldsymbol{x})\|=k$以及$\\forall i\\in \\Omega\_k(\\boldsymbol{x}), j \\not\\in \\Omega\_k(\\boldsymbol{x})\\Rightarrow x\_i > x\_j$。我们定义Top-$k$算子$\\mathcal{T}\_k$为$\\mathbb{R}^n\\mapsto\\{0,1\\}^n$的映射：

\\begin{equation}

\[\\mathcal{T}\_k(\\boldsymbol{x})\]\_i = \\left\\{\\begin{aligned}1,\\,\\, i\\in \\Omega\_k(\\boldsymbol{x}) \\\ 0,\\,\\, i \\not\\in \\Omega\_k(\\boldsymbol{x})\\end{aligned}\\right.

\\end{equation}

说白了，如果$x\_i$属于最大的$k$个元素之一，那么对应的位置变成1，否则变成0，最终结果是一个Multi-Hot向量，比如$\\mathcal{T}\_2(\[3,2,1,4\]) = \[1,0,0,1\]$。

从$\\boldsymbol{x}$到$\\mathcal{T}\_k(\\boldsymbol{x})$实际上是一种硬指派（Hard Assignment）运算，它本质上是不连续的，没有保留关于$\\boldsymbol{x}$的有效梯度，因此无法集成到模型中进行端到端训练。为了解决这个问题，我们就需要构造一个能够提供有效梯度信息的$\\mathcal{T}\_k(\\boldsymbol{x})$的光滑近似——在有些文献中也称为“可微Top-$k$算子（Differentiable Top-$k$ Operator）”。

具体来说，我们定义集合

\\begin{equation}\\Delta\_k^{n-1} = \\left\\{\\boldsymbol{p}=(p\_1,p\_2,\\cdots,p\_n)\\left\|\\, p\_1,p\_2,\\cdots,p\_n\\in\[0,1\],\\sum\_{i=1}^n p\_i = k\\right.\\right\\}\\end{equation}

那么我们要做的事情就是构建$\\mathbb{R}^n\\mapsto \\Delta\_k^{n-1}$的一个映射$\\mathcal{ST}\_k(\\boldsymbol{x})$，并尽量满足如下性质

\\begin{align}&{\\color{red}{单调性}}:\\quad \[\\mathcal{ST}\_k(\\boldsymbol{x})\]\_i \\geq \[\\mathcal{ST}\_k(\\boldsymbol{x})\]\_j \\,\\,\\Leftrightarrow\\,\\, x\_i \\geq x\_j \\\\[8pt\]

&{\\color{red}{不变性}}:\\quad \\mathcal{ST}\_k(\\boldsymbol{x}) = \\mathcal{ST}\_k(\\boldsymbol{x} + c),\\,\\,\\forall c\\in\\mathbb{R} \\\\[8pt\]

&{\\color{red}{趋近性}}:\\quad \\lim\_{\\tau\\to 0^+}\\mathcal{ST}\_k(\\boldsymbol{x}/\\tau) = \\mathcal{T}\_k(\\boldsymbol{x}) \\\

\\end{align}

可以验证作为$\\mathcal{ST}\_1(\\boldsymbol{x})$的Softmax是满足如上性质的，所以提出上述性质实际就是希望构建出来的$\\mathcal{ST}\_k(\\boldsymbol{x})$能够成为Softmax的自然推广。当然，构建Top-$k$的光滑近似本身就比Top-1的要难，所以如果遇到困难的话，不必要严格遵循以上性质，只要能表明所构建的映射确实具备$\\mathcal{T}\_k(\\boldsymbol{x})$的光滑近似的特性就行。

## 迭代构造 [\#](https://kexue.fm/archives/10373\#%E8%BF%AD%E4%BB%A3%E6%9E%84%E9%80%A0)

事实上，笔者很早之前就关注过该问题，首次讨论于2019年的文章 [《函数光滑化杂谈：不可导函数的可导逼近》](https://kexue.fm/archives/6620) 中，当时称之为$\\text{soft-}k\\text{-max}$，并给出过一个迭代构造方案：

> 输入为$\\boldsymbol{x}$，初始化$\\boldsymbol{p}^{(0)}$为全0向量；
>
> 执行$\\boldsymbol{x} = \\boldsymbol{x} - \\min(\\boldsymbol{x})$（保证所有元素非负）
>
> 对于$i=1,2,\\dots,k$，执行：
>
>      $\\boldsymbol{y} = (1 - \\boldsymbol{p}^{(i-1)})\\otimes\\boldsymbol{x}$;
>
>      $\\boldsymbol{p}^{(i)} = \\boldsymbol{p}^{(i-1)} + \\text{softmax}(\\boldsymbol{y})$
>
> 返回$\\boldsymbol{p}^{(k)}$。

其实这个迭代的构造思路很简单，我们可以先将$\\text{softmax}(\\boldsymbol{y})$替换为$\\mathcal{T}\_1(\\boldsymbol{y})$来理解，此时算法流程就是先保证分量非负，然后识别出Top-1，再将Top-1置零（最大值变成了最小值），接着识别出剩下的Top-1，依此类推，最终的$\\boldsymbol{p}\_k$正好就是$\\mathcal{T}\_k(\\boldsymbol{x})$。而$\\text{softmax}(\\boldsymbol{y})$作为$\\mathcal{T}\_1(\\boldsymbol{y})$的光滑近似，在迭代过程中使用$\\text{softmax}(\\boldsymbol{y})$自然也就得到$\\mathcal{T}\_k(\\boldsymbol{x})$的光滑近似了。

无独有偶，笔者发现在Stack Exchange上的提问 [《Is there something like softmax but for top k values?》](https://stats.stackexchange.com/a/454788) 中有回复者提出过一个相同思路的方案，它先定义了加权Softmax：

\\begin{equation}\[\\text{softmax}(\\boldsymbol{x};\\boldsymbol{w})\]\_i = \\frac{w\_i e^{x\_i}}{\\sum\\limits\_{i=1}^n w\_i e^{x\_i}}\\end{equation}

然后构建的迭代过程为

> 输入为$\\boldsymbol{x}$，初始化$\\boldsymbol{p}^{(0)}$为全0向量；
>
> 对于$i=1,2,\\dots,k$，执行：
>
>      $\\boldsymbol{p}^{(i)} = \\boldsymbol{p}^{(i-1)} + \\text{softmax}(\\boldsymbol{x}; 1 - \\boldsymbol{p}^{(i-1)})$
>
> 返回$\\boldsymbol{p}^{(k)}$。

这跟笔者所提的迭代过程在思路上是完全一样的，只不过笔者把$1 - \\boldsymbol{p}\_{i-1}$乘到了$\\boldsymbol{x}$上而它则乘到了$e^{\\boldsymbol{x}}$上，借助$e^{\\boldsymbol{x}}$本身的非负性简化了流程。然而，这个迭代实际上是错误的，它不符合“趋近性”，比如当$k=2$时，代入$\\boldsymbol{x}/\\tau$取$\\tau\\to 0^+$的极限并不是Multi-Hot向量，而是最大值变为1.5、次大值变为0.5、其余都变为0的向量，这是因为$1-p\_{\\max}$跟$e^{-x\_{\\max}}$大致上是同阶的，$1-p\_{\\max}$乘到$e^{x\_{\\max}}$上并不能完全消除最大值。

## 作为梯度 [\#](https://kexue.fm/archives/10373\#%E4%BD%9C%E4%B8%BA%E6%A2%AF%E5%BA%A6)

迭代构造全凭经验，可能会隐藏一些难以发现的问题，比如看起来更简单的加权Softmax迭代实际上不合理。由于没有更贴近本质的原理指导，这些方案往往也难以理论分析，比如笔者构造的迭代，虽然测起来没有问题，但我们很难证明$\\boldsymbol{p}\_k$分量都在$\[0,1\]$内，也很难判断它是否满足单调性。

所以，我们希望得到一个更高观点的原理去指导和设计这个光滑近似。就在前几天，笔者突然意识到一个关键的事实

\\begin{equation}\\mathcal{T}\_k(\\boldsymbol{x}) = \\nabla\_{\\boldsymbol{x}} \\sum\_{i\\in\\Omega\_k(\\boldsymbol{x})} x\_i\\end{equation}

也就是说，最大的$k$个分量的和，它的梯度正好是$\\mathcal{T}\_k(\\boldsymbol{x})$。所以我们似乎可以改为找$\\sum\\limits\_{i\\in\\Omega\_k(\\boldsymbol{x})} x\_i$的光滑近似，然后求梯度就得到$\\mathcal{T}\_k(\\boldsymbol{x})$的光滑近似了，前者是一个标量，它的光滑近似找起来更容易一些，比如利用恒等式

\\begin{equation}\\sum\_{i\\in\\Omega\_k(\\boldsymbol{x})} x\_i = \\max\_{i\_1 < \\cdots < i\_k} (x\_{i\_1} + \\cdots + x\_{i\_k})\\end{equation}

即遍历所有$k$个分量之和取最大值，这样一来，问题变成了找$\\max$的光滑近似，这个我们早已解决（参考 [《寻求一个光滑的最大值函数》](https://kexue.fm/archives/3290)），答案是$\\text{logsumexp}$：

\\begin{equation}\\max\_{i\_1 < \\cdots < i\_k} (x\_{i\_1} + \\cdots + x\_{i\_k})\\approx \\log\\sum\_{i\_1 < \\cdots < i\_k} e^{x\_{i\_1} + \\cdots + x\_{i\_k}}\\triangleq \\log Z\_k\\end{equation}

对它求梯度，我们就得到$\\mathcal{ST}\_k(\\boldsymbol{x})$的一个形式：

\\begin{equation}\[\\mathcal{ST}\_k(\\boldsymbol{x})\]\_i = \\frac{\\sum\\limits\_{i\_2 < \\cdots < i\_k} e^{x\_i+x\_{i\_2} + \\cdots + x\_{i\_k}}}{\\sum\\limits\_{i\_1 < \\cdots < i\_k} e^{x\_{i\_1} +x\_{i\_2}+ \\cdots + x\_{i\_k}}}\\triangleq \\frac{Z\_{k,i}}{Z\_k}\\label{eq:k-max-grad}\\end{equation}

分母是所有$k$分量和的指数和，分子则是所有包含$x\_i$的$k$分量和的指数和。根据这个形式，我们可以轻松证明

\\begin{equation}0 < \[\\mathcal{ST}\_k(\\boldsymbol{x})\]\_i < 1,\\quad \\sum\_{i=1}^n \[\\mathcal{ST}\_k(\\boldsymbol{x})\]\_i = k\\end{equation}

所以这样定义的$\\mathcal{ST}\_k(\\boldsymbol{x})$确实是属于$\\Delta\_k^{n-1}$的。事实上，我们还可以证明它同时满足单调性、不变性和趋近性，并且$\\mathcal{ST}\_1(\\boldsymbol{x})$它就是Softmax，这些特性显示它确实是Softmax对于Top-$k$算子的自然推广，我们暂且称之为“ **GradTopK**（Gradient-guided Soft Top-k operator）”。

不过还没到庆祝时刻，因为式$\\eqref{eq:k-max-grad}$的数值计算问题还没解决。如果直接按照式$\\eqref{eq:k-max-grad}$来计算的话，分母就涉及到$C\_n^k$项指数求和，计算量非常可观，所以必须找到一个高效的计算方法。我们已经分别记了分子、分母为$Z\_{k,i},Z\_k$，可以观察到分子$Z\_{k,i}$满足递归式

\\begin{equation}Z\_{k,i} = e^{x\_i}(Z\_{k-1} - Z\_{k-1,i})\\end{equation}

结合$Z\_{k,i}$对$i$求和等于$kZ\_k$的事实，我们可以构建一个递归计算过程：

\\begin{equation}\\begin{aligned}

\\log Z\_{k,i} =&\\, x\_i + \\log(e^{\\log Z\_{k-1}} - e^{\\log Z\_{k-1,i}}) \\\

\\log Z\_k =&\\, \\left(\\log\\sum\_{i=1}^n e^{\\log Z\_{k,i}}\\right) - \\log k \\\

\\end{aligned}\\end{equation}

其中$\\log Z\_{1,i} = x\_i$，为了减少溢出风险，我们对两端都取了对数。现在只需要迭代$k$步就可以完成$\\mathcal{ST}\_k(\\boldsymbol{x})$的计算，效率上是可以接受的。然而，即便做了对数化处理，上述递归也只能对小方差的$\\boldsymbol{x}$或者比较小的$k$算一下，反之$\\log Z\_{k-1}$与最大的$\\log Z\_{k-1,i}$就会相当接近，当数值上无法区分时就会出现$\\log 0$的Bug，个人认为这是这种递归转化的根本困难。

一个非常简陋的参考实现：

```python

```

## 待定常数 [\#](https://kexue.fm/archives/10373\#%E5%BE%85%E5%AE%9A%E5%B8%B8%E6%95%B0)

上一节通过梯度来构建Top-$k$光滑近似的思路，确实能给人一种高屋建瓴的美感，但可能也会有读者觉得它过于抽象，缺乏一种由表及里的直观感，同时对于大方差的$\\boldsymbol{x}$或者比较大的$k$的数值不稳定性，也让我们难以完全满意现有结果。所以，接下来我们将探究一种自下而上的构建思路。

### 方法大意 [\#](https://kexue.fm/archives/10373\#%E6%96%B9%E6%B3%95%E5%A4%A7%E6%84%8F)

该思路来源于Stack Exchange上的另一个帖子 [《Differentiable top-k function》](https://math.stackexchange.com/a/4506773) 的回复。设$f(x)$是任意$\\mathbb{R}\\mapsto \[0,1\]$的、光滑的、单调递增的函数，并且满足$\\lim\\limits\_{x\\to\\infty}f(x) = 1,\\lim\\limits\_{x\\to-\\infty}f(x) = 0$。看上去条件很多，但实际上这种函数构造起来没有什么难度，比如经典的Sigmoid函数$\\sigma(x)=1/(1+e^{-x})$，还有$\\text{clip}(x,0,1)$、$\\min(1, e^x)$等。接着我们考虑

\\begin{equation}f(\\boldsymbol{x}) = \[f(x\_1),f(x\_2),\\cdots,f(x\_n)\]\\end{equation}

$f(\\boldsymbol{x})$跟我们想要的$\\mathcal{ST}\_k(\\boldsymbol{x})$差多远呢？每个分量都在$\[0,1\]$内肯定是满足了，但是分量之和等于$k$无法保证，所以我们引入一个跟$\\boldsymbol{x}$相关的待定常数$\\lambda(x)$来保证这一点：

\\begin{equation}\\mathcal{ST}\_k(\\boldsymbol{x}) \\triangleq f(\\boldsymbol{x} - \\lambda(\\boldsymbol{x})),\\quad \\sum\_{i=1}^n f(x\_i - \\lambda(\\boldsymbol{x})) = k\\end{equation}

也就是通过分量之和为$k$来反解出$\\lambda(\\boldsymbol{x})$，我们可以称之为“ **ThreTopK**（Threshold-adjusted Soft Top-k operator）”，如果读者已经阅读过 [《通向概率分布之路：盘点Softmax及其替代品》](https://kexue.fm/archives/10145)，就会发现这个做法跟Sparsemax、Entmax-$\\alpha$是一样的。

ThreTopK会是我们理想的$\\mathcal{ST}\_k(\\boldsymbol{x})$吗？还真是！首先我们假设了$f$的单调性，所以单调性是满足的，其次$f(\\boldsymbol{x} - \\lambda(\\boldsymbol{x}))=f(\\boldsymbol{x}+c - (c+\\lambda(\\boldsymbol{x})))$，也就是常数可以收纳到$\\lambda(\\boldsymbol{x})$里边，所以不变性也是满足的。最后，当$\\tau\\to 0^+$时，我们可以找到一个适当的阈值$\\lambda(\\boldsymbol{x}/\\tau)$，使得$\\boldsymbol{x}/\\tau-\\lambda(\\boldsymbol{x}/\\tau)$最大的$k$个分量趋于$\\infty$，剩下的分量趋于$-\\infty$，从而$f(\\boldsymbol{x}/\\tau-\\lambda(\\boldsymbol{x}/\\tau))$就等于$\\mathcal{T}\_k(\\boldsymbol{x})$，也就是说满足趋近性。

### 解析求解 [\#](https://kexue.fm/archives/10373\#%E8%A7%A3%E6%9E%90%E6%B1%82%E8%A7%A3)

既然已经证明了ThreTopK的理论优越性，那么接下来要解决的就是$\\lambda(\\boldsymbol{x})$的计算了，这个大部份情况下都只能诉诸数值计算方法，不过对于$f(x)=\\min(1, e^x)$，我们可以求得一个解析解。

求解的思路跟之前的Sparsemax是一样的。不失一般性，假设$\\boldsymbol{x}$的分量已经从大到小排好顺序，即$x\_1 > x\_2 > \\cdots > x\_n$，接着假设我们已经知道$x\_m \\geq \\lambda(\\boldsymbol{x}) \\geq x\_{m+1}$，那么此时

\\begin{equation}k = \\sum\_{i=1}^n \\min(1, e^{x\_i - \\lambda(\\boldsymbol{x})}) = m + \\sum\_{i=m+1}^n e^{x\_i - \\lambda(\\boldsymbol{x})}\\end{equation}

由此解得

\\begin{equation}\\lambda(\\boldsymbol{x})=\\log\\left(\\sum\_{i=m+1}^n e^{x\_i}\\right) - \\log(k-m)\\end{equation}

由此我们可以看出，当$k=1$时，$m$只能取$0$，此时可以发现ThreTopK正好就是Softmax；当$k > 1$时，我们无法事先确定$m$的值，所以只能遍历$m=0,1,\\cdots,k-1$，根据上式计算$\\lambda(\\boldsymbol{x})$，寻找满足$x\_m \\geq \\lambda(\\boldsymbol{x}) \\geq x\_{m+1}$的$\\lambda(\\boldsymbol{x})$。下面同样是一个非常简陋的参考实现：

```python

```

### 通用结果 [\#](https://kexue.fm/archives/10373\#%E9%80%9A%E7%94%A8%E7%BB%93%E6%9E%9C)

从原理和代码都可以看出，$f(x)=\\min(1, e^x)$时的ThreTopK几乎不会出现数值稳定性问题，并且$k=1$时能退化为Softmax，这些都是它的优势。然而，$\\min(1, e^x)$实际上也算不上完全光滑（除了当$k=1$时$\\min$不起作用），它在$x=0$处是不可导的。如果介意这一点，那么我们就需要选择处处可导的$f(x)$，比如$\\sigma(x)$。

下面我们以$f(x)=\\sigma(x)$为例，此时我们无法求出$\\lambda(\\boldsymbol{x})$的解析解，不过由于$\\sigma(x)$的单调递增性，所以函数

\\begin{equation}F(\\lambda)\\triangleq \\sum\_{i=1}^n \\sigma(x\_i - \\lambda)\\end{equation}

关于$\\lambda$是单调递减的，因此$F(\\lambda(\\boldsymbol{x}))=k$的数值求解并不困难，二分法、牛顿法均可。以二分法为例，不难看出$\\lambda(\\boldsymbol{x})\\in\[x\_{\\min} - \\sigma^{-1}(k/n), x\_{\\max} - \\sigma^{-1}(k/n)\]$，这里$\\sigma^{-1}$是$\\sigma$的逆函数，从这个初始区间出发，逐步二分到指定精度即可：

```python

```

因此，$\\lambda(\\boldsymbol{x})$的数值计算并没有太大困难，真正的困难是当我们用数值方法去计算$\\lambda(\\boldsymbol{x})$时，往往会丢失$\\lambda(\\boldsymbol{x})$关于$\\boldsymbol{x}$的梯度，从而影响端到端的训练。针对这个问题，我们可以手动把$\\nabla\_{\\boldsymbol{x}}\\lambda(\\boldsymbol{x})$算出来，然后自定义反向传播过程。具体来说，我们对

\\begin{equation}\\sum\_{i=1}^n \\sigma(x\_i - \\lambda(\\boldsymbol{x})) = k\\end{equation}

两边求某个$x\_j$的偏导数，得到

\\begin{equation}\\sigma'(x\_j - \\lambda(\\boldsymbol{x}))-\\sum\_{i=1}^n \\sigma'(x\_i - \\lambda(\\boldsymbol{x}))\\frac{\\partial\\lambda(\\boldsymbol{x})}{\\partial x\_j} = 0\\end{equation}

那么

\\begin{equation}\\frac{\\partial\\lambda(\\boldsymbol{x})}{\\partial x\_j} = \\frac{\\sigma'(x\_j - \\lambda(\\boldsymbol{x}))}{\\sum\\limits\_{i=1}^n \\sigma'(x\_i - \\lambda(\\boldsymbol{x}))}\\end{equation}

其中$\\sigma'$是$\\sigma$的导函数。现在我们有了$\\nabla\_{\\boldsymbol{x}}\\lambda(\\boldsymbol{x})$的表达式，它的每一项都是可计算的（$\\lambda(\\boldsymbol{x})$也已经由数值方法求出），我们可以直接指定它作为反向传播的结果。一个比较简单且通用的实现方法是利用$\\text{stop\_gradient}$（下面简称$\\text{sg}$）技巧，即在实现模型时将$\\lambda(\\boldsymbol{x})$替换为

\\begin{equation}\\boldsymbol{x}\\cdot\\text{sg}\[\\nabla\_{\\boldsymbol{x}}\\lambda(\\boldsymbol{x})\] + \\text{sg}\[\\lambda(\\boldsymbol{x}) - \\boldsymbol{x}\\cdot\\nabla\_{\\boldsymbol{x}}\\lambda(\\boldsymbol{x})\]\\end{equation}

其中$\\cdot$是向量的内积。这样一来，前向传播时等价于$\\text{sg}$不存在，所以结果是$\\lambda(\\boldsymbol{x})$，反向传播时被$\\text{sg}$的部份梯度都是零，所以梯度就是给定的$\\nabla\_{\\boldsymbol{x}}\\lambda(\\boldsymbol{x})$，这样我们就自定义了$\\lambda(\\boldsymbol{x})$的梯度，跟$\\lambda(\\boldsymbol{x})$如何计算得到的无关。

### 二者兼之 [\#](https://kexue.fm/archives/10373\#%E4%BA%8C%E8%80%85%E5%85%BC%E4%B9%8B)

现在我们看到，$f(x)=\\min(1,e^x)$有解析解，但它并非全局光滑；$f(x)=\\sigma(x)$倒是足够光滑了，但它求解起来比较复杂。有没有兼顾两者优点的选择呢？还真有，笔者发现下述的$f(x)$是全局光滑并且$\\lambda(\\boldsymbol{x})$可以解析求解的：

\\begin{equation}f(x) = \\left\\{\\begin{aligned}1 - e^{-x}/2,\\quad x\\geq 0 \\\ e^x / 2,\\quad x < 0\\end{aligned}\\right.\\end{equation}

也可以写成$f(x) = (1 - e^{-\|x\|})\\text{sign}(x)/2+1/2$。可以验证$f(x)$其实也是一个S型函数，虽然它是分段函数，但它本身及其导函数在$x=0$处都是连续的，因此足够光滑了。

求解思路跟之前一样，不失一般性设$x\_1 > x\_2 > \\cdots > x\_n$，假设已经知道$x\_m \\geq \\lambda(\\boldsymbol{x}) \\geq x\_{m+1}$，那么

\\begin{equation}\\begin{aligned}

k =&\\, \\sum\_{i=1}^m (1 - e^{-(x\_i - \\lambda(\\boldsymbol{x}))}/2) + \\sum\_{i=m+1}^n e^{x\_i - \\lambda(\\boldsymbol{x})}/2 \\\

=&\\, m - \\frac{1}{2}e^{\\lambda(\\boldsymbol{x})}\\sum\_{i=1}^m e^{-x\_i} + \\frac{1}{2}e^{-\\lambda(\\boldsymbol{x})}\\sum\_{i=m+1}^n e^{x\_i}

\\end{aligned}\\end{equation}

由此解得

\\begin{equation}\\lambda(\\boldsymbol{x})=\\log\\sum\_{i=m+1}^n e^{x\_i} - \\log\\left(\\sqrt{(k-m)^2 + \\left(\\sum\_{i=1}^m e^{-x\_i}\\right)\\left(\\sum\_{i=m+1}^n e^{x\_i}\\right)}+(k-m)\\right)\\end{equation}

然后遍历$m=0,1,\\cdots,n-1$，寻找满足$x\_m \\geq \\lambda(\\boldsymbol{x}) \\geq x\_{m+1}$的$\\lambda(\\boldsymbol{x})$即可。读者还可以尝试证明一下，当$k=1$时此$f(x)$下的ThreTopK也正好退化为Softmax。

参考实现：

```python

```

## 文章小结 [\#](https://kexue.fm/archives/10373\#%E6%96%87%E7%AB%A0%E5%B0%8F%E7%BB%93)

本文探讨了Top-k算子的光滑近似问题，它是Softmax等Top1的光滑近似的一般推广，提出了迭代构造、梯度指引、待定常数三种构造思路，并分析了它们的优缺点。

_**转载到请包括本文地址：** [https://kexue.fm/archives/10373](https://kexue.fm/archives/10373 "Softmax后传：寻找Top-K的光滑近似")_

_**更详细的转载事宜请参考：**_ [《科学空间FAQ》](https://kexue.fm/archives/6508#%E6%96%87%E7%AB%A0%E5%A6%82%E4%BD%95%E8%BD%AC%E8%BD%BD/%E5%BC%95%E7%94%A8 "《科学空间FAQ》")

**如果您还有什么疑惑或建议，欢迎在下方评论区继续讨论。**

**如果您觉得本文还不错，欢迎 [分享](https://kexue.fm/archives/10373#share)/ [打赏](https://kexue.fm/archives/10373#pay) 本文。打赏并非要从中获得收益，而是希望知道科学空间获得了多少读者的真心关注。当然，如果你无视它，也不会影响你的阅读。再次表示欢迎和感谢！**

打赏

![科学空间](https://kexue.fm/usr/themes/geekg/payment/wx.png)

微信打赏

![科学空间](https://kexue.fm/usr/themes/geekg/payment/zfb.png)

支付宝打赏

因为网站后台对打赏并无记录，因此欢迎在打赏时候备注留言。

你还可以 [**点击这里**](http://mail.qq.com/cgi-bin/qm_share?t=qm_mailme&email=tN7d1drY3drrx8H0xcWa19vZ) 或在下方评论区留言来告知你的建议或需求。

**如果您需要引用本文，请参考：**

苏剑林. (Sep. 19, 2024). 《Softmax后传：寻找Top-K的光滑近似 》\[Blog post\]. Retrieved from [https://kexue.fm/archives/10373](https://kexue.fm/archives/10373)

@online{kexuefm-10373,

         title={Softmax后传：寻找Top-K的光滑近似},

         author={苏剑林},

         year={2024},

         month={Sep},

         url={\\url{https://kexue.fm/archives/10373}},

}


分类： [数学研究](https://kexue.fm/category/Mathematics)    标签： [概率](https://kexue.fm/tag/%E6%A6%82%E7%8E%87/), [近似](https://kexue.fm/tag/%E8%BF%91%E4%BC%BC/), [梯度](https://kexue.fm/tag/%E6%A2%AF%E5%BA%A6/), [光滑](https://kexue.fm/tag/%E5%85%89%E6%BB%91/)[32 评论](https://kexue.fm/archives/10373#comments)

< [低秩近似之路（一）：伪逆](https://kexue.fm/archives/10366 "低秩近似之路（一）：伪逆") \| [利用“熄火保护 \+ 通断器”实现燃气灶智能关火](https://kexue.fm/archives/10394 "利用“熄火保护 + 通断器”实现燃气灶智能关火") >

### 你也许还对下面的内容感兴趣

- [让炼丹更科学一些（五）：基于梯度精调学习率](https://kexue.fm/archives/11530 "让炼丹更科学一些（五）：基于梯度精调学习率")
- [滑动平均视角下的权重衰减和学习率](https://kexue.fm/archives/11459 "滑动平均视角下的权重衰减和学习率")
- [AdamW的Weight RMS的渐近估计（下）](https://kexue.fm/archives/11404 "AdamW的Weight RMS的渐近估计（下）")
- [n个正态随机数的最大值的渐近估计](https://kexue.fm/archives/11390 "n个正态随机数的最大值的渐近估计")
- [低精度Attention可能存在有偏的舍入误差](https://kexue.fm/archives/11371 "低精度Attention可能存在有偏的舍入误差")
- [随机矩阵的谱范数的快速估计](https://kexue.fm/archives/11335 "随机矩阵的谱范数的快速估计")
- [DiVeQ：一种非常简洁的VQ训练方案](https://kexue.fm/archives/11328 "DiVeQ：一种非常简洁的VQ训练方案")
- [AdamW的Weight RMS的渐近估计（上）](https://kexue.fm/archives/11307 "AdamW的Weight RMS的渐近估计（上）")
- [为什么Adam的Update RMS是0.2？](https://kexue.fm/archives/11267 "为什么Adam的Update RMS是0.2？")
- [重新思考学习率与Batch Size（一）：现状](https://kexue.fm/archives/11260 "重新思考学习率与Batch Size（一）：现状")

[发表你的看法](https://kexue.fm/archives/10373#comment_form)

杨小江

September 19th, 2024

对于ThreTopK中$f(x)=\\min(1, e^x)$的情形，为什么不直接固定取$\\lambda$使得$m=k-1$，这样f(x)就是k-1个1和一个n-k+1维的Softmax的直积。

[回复评论](https://kexue.fm/archives/10373/comment-page-1?replyTo=25242#respond-post-10373)

[苏剑林](https://kexue.fm/) 发表于
September 20th, 2024

怎么取$m=k-1$？取$m=k-1$就无法保证求和为$k$了啊

[回复评论](https://kexue.fm/archives/10373/comment-page-1?replyTo=25261#respond-post-10373)

杨小江 发表于
September 23rd, 2024

也就是取$\\lambda$ 使得 $x\_{k-1}\\geq \\lambda >x\_k$，则为了保证求和为$k$，有$\\lambda(x)=\\log\\sum\_{i=k}^n e^{x\_i}$。此时$f(x)$的前$k-1$项为1,而第$k\\sim n$项构成了Softmax，和为1,总的和为$k$。这似乎也是个简单粗暴的TopK光滑近似呀？

[回复评论](https://kexue.fm/archives/10373/comment-page-1?replyTo=25278#respond-post-10373)

[苏剑林](https://kexue.fm/) 发表于
September 23rd, 2024

你这个就是Top-(k-1)个直接变为1，剩下的做Softmax，这是属于你自己构造的一个Top-$k$的“伪”光滑近似，不属于$f(x)=\\min(1,e^x)$的ThreTopK解，因为你这样算出来的$\\lambda(\\boldsymbol{x})$，无法保证对于前$k-1$个$x\_i$，都有$x\_i > \\lambda(\\boldsymbol{x})$。

至于为什么是“伪”光滑近似，都直接将Top-(k-1)硬截断为1了，明显的不连续操作。

[回复评论](https://kexue.fm/archives/10373/comment-page-1?replyTo=25294#respond-post-10373)

杨小江 发表于
September 23rd, 2024

我明白我的出发点错在那里了：$x\_{k-1}\\geq \\lambda(x)$和$\\lambda(x)=\\log\\sum\_{i=k}^n e^{x\_i}$并不一定同时成立。但是核心问题似乎仍没被解决：为什么不直接使用前$k-1$项为$1$，$k\\sim n$项为Softmax的$f(x)$作为TopK的光滑近似？它也满足“单调性、不变性和趋近性”。

[回复评论](https://kexue.fm/archives/10373/comment-page-1?replyTo=25291#respond-post-10373)

[苏剑林](https://kexue.fm/) 发表于
September 23rd, 2024

因为不够光滑。如果直接将Top-k/Top-(k-1)选出来做硬截断的操作都允许，那么就没有光滑近似这个问题了。

[回复评论](https://kexue.fm/archives/10373/comment-page-1?replyTo=25295#respond-post-10373)

杨小江 发表于
September 29th, 2024

感谢解答~

[回复评论](https://kexue.fm/archives/10373/comment-page-1?replyTo=25345#respond-post-10373)

GD

September 21st, 2024

感谢分享，确实ThreTopK是个比较巧妙的方法。我们这边之前也在研究相关问题，在分析了很多不同的Differentiable top-k 后最后引用了Stack Exchange这个方法。目前这个算子在做些改动后在剪枝领域效果十分不错，感兴趣的话可以看下我们的paper Smart(https://arxiv.org/abs/2403.19969),当然目前也发现当前算子在深度学习应用过程确实还有一些限制，目前在整理相关问题，之后也会整理成flow work 的paper 分享给大家，并且我们也发现类似的算子在混合量化等领域效果依旧出色。

[回复评论](https://kexue.fm/archives/10373/comment-page-1?replyTo=25270#respond-post-10373)

[苏剑林](https://kexue.fm/) 发表于
September 23rd, 2024

学习了，期待后续工作。Stack Exchange这个思路确实精彩，果然是高手在民间。

[回复评论](https://kexue.fm/archives/10373/comment-page-1?replyTo=25292#respond-post-10373)

alann

September 24th, 2024

三种方法里，第一种缺点是有数值稳定性问题，第二种缺点是不完全光滑的，那第三种方法有缺点吗？

[回复评论](https://kexue.fm/archives/10373/comment-page-1?replyTo=25303#respond-post-10373)

alann 发表于
September 25th, 2024

大概清楚了，第三种方法要自己写反向传播函数

[回复评论](https://kexue.fm/archives/10373/comment-page-1?replyTo=25307#respond-post-10373)

[苏剑林](https://kexue.fm/) 发表于
September 28th, 2024

没有解析解，要自己写反向传播，当$k=1$时无法退化为Softmax，这些都可以说是缺点。

[回复评论](https://kexue.fm/archives/10373/comment-page-1?replyTo=25325#respond-post-10373)

qwe 发表于
August 27th, 2025

k=1是可以退化的吧

当 k = 1 的时候 λ = LSE, S = softmax

[回复评论](https://kexue.fm/archives/10373/comment-page-1?replyTo=28446#respond-post-10373)

[苏剑林](https://kexue.fm/) 发表于
August 31st, 2025

噢，写这个回复的时候，我想的是$f(x)=\\sigma(x)$这个case，它是没法退化为softmax的。

[回复评论](https://kexue.fm/archives/10373/comment-page-1?replyTo=28463#respond-post-10373)

hazdzz

September 27th, 2024

這個東西看起來可以用於 MoE。

[回复评论](https://kexue.fm/archives/10373/comment-page-1?replyTo=25312#respond-post-10373)

[苏剑林](https://kexue.fm/) 发表于
September 28th, 2024

是有这个猜测，所以最近在尝试补充一些sparse training的内容。

[回复评论](https://kexue.fm/archives/10373/comment-page-1?replyTo=25332#respond-post-10373)

hazdzz 发表于
September 29th, 2024

之前有人從最佳傳輸問題的角度提出了一個 Differentiable Top-k \[1\]，或許可以考慮用 Wasserstein distance 來做一個？

[回复评论](https://kexue.fm/archives/10373/comment-page-1?replyTo=25339#respond-post-10373)

hazdzz 发表于
September 30th, 2024

\[1\] Xie, Y., Dai, H., Chen, M., Dai, B., Zhao, T., Zha, H., Wei, W., & Pfister, T. (2020). Differentiable Top-k with Optimal Transport. In Advances in Neural Information Processing Systems (pp. 20520–20531). Curran Associates, Inc.

[回复评论](https://kexue.fm/archives/10373/comment-page-1?replyTo=25347#respond-post-10373)

[苏剑林](https://kexue.fm/) 发表于
October 5th, 2024

看到了，一眼看上去不是很简洁，还没认真看它的必要性。不过从 [@GD\|comment-25270](https://kexue.fm/archives/10373/comment-page-1#comment-25270) 的反馈来看，最后一种方案貌似也够用了。

[回复评论](https://kexue.fm/archives/10373/comment-page-1?replyTo=25382#respond-post-10373)

xlf@deepglint

September 27th, 2024

Softmax 的输入和输出都是向量，它应该看作是 one-hot 的 soft 形式。至于 max 的光滑近似，还是回到 $\\log\\sum\\exp$ 吧。

[回复评论](https://kexue.fm/archives/10373/comment-page-1?replyTo=25318#respond-post-10373)

[苏剑林](https://kexue.fm/) 发表于
September 28th, 2024

括号已经注明了，准确来说是$\\text{argmax}$的光滑近似（更准确来说是$\\text{onehot}(\\text{argmax})$，但一般不对两者做区分了）

[回复评论](https://kexue.fm/archives/10373/comment-page-1?replyTo=25334#respond-post-10373)

Namelessking

November 8th, 2024

请问有没有top k sum的光滑近似的参考呀？

[回复评论](https://kexue.fm/archives/10373/comment-page-1?replyTo=25719#respond-post-10373)

[苏剑林](https://kexue.fm/) 发表于
November 15th, 2024

那不就是$\\sum\\limits\_i \\big(\[\\mathcal{ST}\_k(\\boldsymbol{x})\]\_i \\times x\_i\\big)$？

[回复评论](https://kexue.fm/archives/10373/comment-page-1?replyTo=25765#respond-post-10373)

1ring2rta

January 19th, 2025

STk: A Scalable Module for Solving Top-k Problems

[回复评论](https://kexue.fm/archives/10373/comment-page-1?replyTo=26297#respond-post-10373)

[苏剑林](https://kexue.fm/) 发表于
January 20th, 2025

学习了一下，似乎有点复杂。

[回复评论](https://kexue.fm/archives/10373/comment-page-1?replyTo=26345#respond-post-10373)

1ring2rta 发表于
January 24th, 2025

这是top-k sum的近似算法，核心思想 $\\sum\_{j=1}^k e\_{\[k\]}= argmin\_\\lambda \\sum\_{i=1}^n \[e\_i - \\lambda\]\_+ + k\\lambda$ 承自Ogryczak 2003年的一篇文章。

Ogryczak, Wlodzimierz, and Arie Tamir. "Minimizing the sum of the k largest functions in linear time." Information Processing Letters 85.3 (2003): 117-122.

[回复评论](https://kexue.fm/archives/10373/comment-page-1?replyTo=26371#respond-post-10373)

[苏剑林](https://kexue.fm/) 发表于
January 29th, 2025

这个恒等变换确实挺有意思的，值得学习一番。

[回复评论](https://kexue.fm/archives/10373/comment-page-1?replyTo=26392#respond-post-10373)

Kafka\_on\_Shore

March 27th, 2025

苏神，最后一种方法使用了x\_sort = np.sort(x)，如果在torch框架下，torch.sort也没有梯度，是否需要给sort一个梯度？

[回复评论](https://kexue.fm/archives/10373/comment-page-1?replyTo=27253#respond-post-10373)

[苏剑林](https://kexue.fm/) 发表于
March 30th, 2025

sort是有梯度的，argsort才没梯度。最后一个我尝试过用在MoE上了，效果尚可（不过我是jax）。

[回复评论](https://kexue.fm/archives/10373/comment-page-1?replyTo=27281#respond-post-10373)

eular

April 27th, 2025

ThreTopK中关于$\\lambda(\\boldsymbol{x})$的解是否少考虑了一种情况？对于ThreTopK，$\\lambda(\\boldsymbol{x})$的解是存在的，但是存在两种可能：$\\lambda(\\boldsymbol{x}) \\ge x\_i$以及$\\lambda(\\boldsymbol{x}) \\le x\_i$，$i=0,1,\\cdots,n$。“二者兼之”小节的代码里只考虑到了前者（$\\lambda$遍历项中添加了np.inf），而忽略了后者。所以，实际上$\\lambda(\\boldsymbol{x})$应该有$n+1$个遍历项。可以在$\\boldsymbol{x}$序列中再添加一个-np.inf，以处理这种情况。

[回复评论](https://kexue.fm/archives/10373/comment-page-1?replyTo=27482#respond-post-10373)

[苏剑林](https://kexue.fm/) 发表于
April 27th, 2025

你的意思是$\\lambda(\\boldsymbol{x}) < x\_{\\min}$，一般情况下好像倒是有这种可能，不过可以证明当$k \\leq n / 2$时就不会有这种可能了。

[回复评论](https://kexue.fm/archives/10373/comment-page-1?replyTo=27490#respond-post-10373)

eular 发表于
April 27th, 2025

是的，当$k$比较大时会出现这种情况。

[回复评论](https://kexue.fm/archives/10373/comment-page-1?replyTo=27492#respond-post-10373)

[取消回复](https://kexue.fm/archives/10373#respond-post-10373)

你的大名

电子邮箱

个人网站（选填）

1\. 可以使用LaTeX代码，点击“预览效果”可查看效果；

2\. 可以通过点击评论楼层编号来引用该楼层；

3\. 网站可能会有点卡，如非确认评论失败，请 **不要重复点击提交**。

### 内容速览

[问题描述](https://kexue.fm/archives/10373#%E9%97%AE%E9%A2%98%E6%8F%8F%E8%BF%B0)
[迭代构造](https://kexue.fm/archives/10373#%E8%BF%AD%E4%BB%A3%E6%9E%84%E9%80%A0)
[作为梯度](https://kexue.fm/archives/10373#%E4%BD%9C%E4%B8%BA%E6%A2%AF%E5%BA%A6)
[待定常数](https://kexue.fm/archives/10373#%E5%BE%85%E5%AE%9A%E5%B8%B8%E6%95%B0)
[方法大意](https://kexue.fm/archives/10373#%E6%96%B9%E6%B3%95%E5%A4%A7%E6%84%8F)
[解析求解](https://kexue.fm/archives/10373#%E8%A7%A3%E6%9E%90%E6%B1%82%E8%A7%A3)
[通用结果](https://kexue.fm/archives/10373#%E9%80%9A%E7%94%A8%E7%BB%93%E6%9E%9C)
[二者兼之](https://kexue.fm/archives/10373#%E4%BA%8C%E8%80%85%E5%85%BC%E4%B9%8B)
[文章小结](https://kexue.fm/archives/10373#%E6%96%87%E7%AB%A0%E5%B0%8F%E7%BB%93)

### 智能搜索

支持整句搜索！网站自动使用 [结巴分词](https://github.com/fxsjy/jieba) 进行分词，并结合ngrams排序算法给出合理的搜索结果。

### 热门标签

[生成模型](https://kexue.fm/tag/%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B/) [attention](https://kexue.fm/tag/attention/) [优化](https://kexue.fm/tag/%E4%BC%98%E5%8C%96/) [语言模型](https://kexue.fm/tag/%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/) [模型](https://kexue.fm/tag/%E6%A8%A1%E5%9E%8B/) [梯度](https://kexue.fm/tag/%E6%A2%AF%E5%BA%A6/) [网站](https://kexue.fm/tag/%E7%BD%91%E7%AB%99/) [概率](https://kexue.fm/tag/%E6%A6%82%E7%8E%87/) [矩阵](https://kexue.fm/tag/%E7%9F%A9%E9%98%B5/) [优化器](https://kexue.fm/tag/%E4%BC%98%E5%8C%96%E5%99%A8/) [转载](https://kexue.fm/tag/%E8%BD%AC%E8%BD%BD/) [微分方程](https://kexue.fm/tag/%E5%BE%AE%E5%88%86%E6%96%B9%E7%A8%8B/) [分析](https://kexue.fm/tag/%E5%88%86%E6%9E%90/) [天象](https://kexue.fm/tag/%E5%A4%A9%E8%B1%A1/) [深度学习](https://kexue.fm/tag/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/) [积分](https://kexue.fm/tag/%E7%A7%AF%E5%88%86/) [python](https://kexue.fm/tag/python/) [扩散](https://kexue.fm/tag/%E6%89%A9%E6%95%A3/) [力学](https://kexue.fm/tag/%E5%8A%9B%E5%AD%A6/) [无监督](https://kexue.fm/tag/%E6%97%A0%E7%9B%91%E7%9D%A3/) [几何](https://kexue.fm/tag/%E5%87%A0%E4%BD%95/) [节日](https://kexue.fm/tag/%E8%8A%82%E6%97%A5/) [生活](https://kexue.fm/tag/%E7%94%9F%E6%B4%BB/) [文本生成](https://kexue.fm/tag/%E6%96%87%E6%9C%AC%E7%94%9F%E6%88%90/) [数论](https://kexue.fm/tag/%E6%95%B0%E8%AE%BA/)

### 随机文章

- [“让Keras更酷一些！”：小众的自定义优化器](https://kexue.fm/archives/5879)
- [AdaX优化器浅析（附开源实现）](https://kexue.fm/archives/7387)
- [三百年之谜——费马大定理(历史+证明)](https://kexue.fm/archives/38)
- [生成扩散模型漫谈（二十五）：基于恒等式的蒸馏（上）](https://kexue.fm/archives/10085)
- [更别致的词向量模型(一)：simpler glove](https://kexue.fm/archives/4667)
- [科学空间相册上线,与你分享科学图片](https://kexue.fm/archives/275)
- [ChildTuning：试试把Dropout加到梯度上去？](https://kexue.fm/archives/8764)
- [【通知转载】国家天文台信息技术类人才招聘](https://kexue.fm/archives/557)
- [生成扩散模型漫谈（七）：最优扩散方差估计（上）](https://kexue.fm/archives/9245)
- [MoE环游记：2、不患寡而患不均](https://kexue.fm/archives/10735)

### 最近评论

- [Bin](https://kexue.fm/archives/1990/comment-page-2#comment-29105): 今天偶然从某个论坛看到有人推荐您的博客，定睛一看竟然是华师同院的往届师兄！看到这篇2013年的...
- [Rapture D](https://kexue.fm/archives/11530/comment-page-1#comment-29104): 我有一个问题，为什么不考虑亥姆霍兹定理和斯托克斯公式。
- [mofheka](https://kexue.fm/archives/11390/comment-page-1#comment-29103): 苏神是还在用jax是么？最近在做基于Google Pathway的理念做一个动态版的MPMD框...
- [长琴](https://kexue.fm/archives/11530/comment-page-1#comment-29102): 看懂这篇博客也不是一件容易的事情。
- [AlexLi](https://kexue.fm/archives/9257/comment-page-4#comment-29101): 苏老师，请教一下(7)式中将 $\\mu(x\_t)$ 传给 $p\_o$ 进行推理的操作。 $x\_...
- [tyler\_zxc](https://kexue.fm/archives/7921/comment-page-2#comment-29100): "Performer的思想是将标准的Attention线性化，所以为什么不干脆直接训练一个线性...
- [我](https://kexue.fm/archives/11494/comment-page-1#comment-29099): 似乎并非mHC提出矩阵的思想？之前hyper connection就是了
- [winter](https://kexue.fm/archives/10847/comment-page-1#comment-29098): 苏神您好，假如对于比较均匀的attention weightP，往往呈现long tail分布...
- [苏剑林](https://kexue.fm/archives/8512/comment-page-2#comment-29097): KL散度、JS散度、W距离啥的，都行啊，看你喜欢哪个
- [苏剑林](https://kexue.fm/archives/9119/comment-page-14#comment-29096): 没有绝对公平的对比方法，主要看你关心什么。比如，如果只关心推理成本和推理效果，那么有的方法可以...

### 友情链接

- [Cool Papers](https://papers.cool/)
- [数学研发](https://bbs.emath.ac.cn/)
- [Seatop](http://www.seatop.com.cn/)
- [Xiaoxia](https://xiaoxia.org/)
- [积分表-网络版](https://kexue.fm/sci/integral/index.html)
- [丝路博傲](http://blog.dvxj.com/)
- [数学之家](http://www.2math.cn/)
- [有趣天文奇观](http://interesting-sky.china-vo.org/)
- [TwistedW](http://www.twistedwg.com/)
- [godweiyang](https://godweiyang.com/)
- [AI柠檬](https://blog.ailemon.net/)
- [王登科-DK博客](https://greatdk.com/)
- [ESON](https://blog.eson.org/)
- [枫之羽](https://fzhiy.net/)
- [coding-zuo](https://coding-zuo.github.io/)
- [博科园](https://www.bokeyuan.net/)
- [孔皮皮的博客](https://www.kppkkp.top/)
- [运鹏的博客](https://yunpengtai.top/)
- [jiming.site](https://jiming.site/)
- [OmegaXYZ](https://www.omegaxyz.com/)
- [EAI猩球](https://www.robotech.ink/)
- [文举的博客](https://liwenju0.com/)
- [申请链接](https://kexue.fm/links.html)

[![署名-非商业用途-保持一致](https://kexue.fm/usr/themes/geekg/images/cc.gif)](http://creativecommons.org/licenses/by-nc-nd/2.5/cn/) 本站采用创作共用版权协议，要求署名、非商业用途和保持一致。转载本站内容必须也遵循“ [署名-非商业用途-保持一致](http://creativecommons.org/licenses/by-nc-nd/2.5/cn/)”的创作共用协议。



© 2009-2026 Scientific Spaces. All rights reserved. Theme by [laogui](http://www.laogui.com/). Powered by [Typecho](http://typecho.org/). 备案号: [粤ICP备09093259号-1/2](https://beian.miit.gov.cn/ "粤ICP备09093259号")。