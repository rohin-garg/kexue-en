## SEARCH

## MENU

- [打赏](https://kexue.fm/reward.html)
- [公式](https://kexue.fm/latex.html)
- [天象](https://kexue.fm/ac.html)
- [链接](https://kexue.fm/links.html)
- [时光](https://kexue.fm/me.html)
- [博览](https://kexue.fm/science.html)
- [归档](https://kexue.fm/content.html)

## CATEGORIES

- [千奇百怪](https://kexue.fm/category/Everything)
- [天文探索](https://kexue.fm/category/Astronomy)
- [数学研究](https://kexue.fm/category/Mathematics)
- [物理化学](https://kexue.fm/category/Phy-chem)
- [信息时代](https://kexue.fm/category/Big-Data)
- [生物自然](https://kexue.fm/category/Biology)
- [图片摄影](https://kexue.fm/category/Photograph)
- [问题百科](https://kexue.fm/category/Questions)
- [生活/情感](https://kexue.fm/category/Life-Feeling)
- [资源共享](https://kexue.fm/category/Resources)

## NEWPOSTS

- [重新思考学习率与Batch Siz...](https://kexue.fm/archives/11301)
- [重新思考学习率与Batch Siz...](https://kexue.fm/archives/11285)
- [重新思考学习率与Batch Siz...](https://kexue.fm/archives/11280)
- [为什么Adam的Update RM...](https://kexue.fm/archives/11267)
- [重新思考学习率与Batch Siz...](https://kexue.fm/archives/11260)
- [Cool Papers更新：简单适...](https://kexue.fm/archives/11250)
- [流形上的最速下降：4\. Muon ...](https://kexue.fm/archives/11241)
- [ReLU/GeLU/Swish的一...](https://kexue.fm/archives/11233)
- [流形上的最速下降：3\. Muon ...](https://kexue.fm/archives/11221)
- [流形上的最速下降：2\. Muon ...](https://kexue.fm/archives/11215)

## COMMENTS

- [mp4网: 申请友链\
站名：mp4网\
域名：http://mp4wang....](https://kexue.fm/links.html/comment-page-6#comment-28589)
- [FrankCai: 老师您好， 如果结论是【当ϵ越大，结果越接近SGD，“Surg...](https://kexue.fm/archives/10563/comment-page-1#comment-28588)
- [川zi: 看懂了， 谢谢大佬](https://kexue.fm/archives/9009/comment-page-3#comment-28587)
- [pb: 苏神您好，这两天读到“远程衰减”的部分时觉得有些奇怪，还请指正...](https://kexue.fm/archives/8265/comment-page-8#comment-28586)
- [苏剑林: 这就是diffusion模型的神奇之处，它有点直观，但又不完全...](https://kexue.fm/archives/9119/comment-page-13#comment-28585)
- [苏剑林: 平方或者线性复杂度，都是指关于序列长度$n$的总复杂度，$d$不算。](https://kexue.fm/archives/11033/comment-page-2#comment-28584)
- [苏剑林: cool papers的prompt其实没啥特殊的，关键的技巧...](https://kexue.fm/archives/9907/comment-page-4#comment-28583)
- [苏剑林: 已发](https://kexue.fm/archives/443/comment-page-1#comment-28582)
- [苏剑林: Muon不是“可以”用更大的学习率，它用更大的学习率是因为ms...](https://kexue.fm/archives/10592/comment-page-2#comment-28581)
- [苏剑林: 逻辑是：训练一个模型，约等于把训练集压缩到模型权重中，模型权重...](https://kexue.fm/archives/11033/comment-page-2#comment-28580)

## USERLOGIN

- [登录](https://kexue.fm/admin/login.php)

[科学空间\|Scientific Spaces](https://kexue.fm)

- [登录](https://kexue.fm/admin/login.php)
- [打赏](https://kexue.fm/reward.html)
- [公式](https://kexue.fm/latex.html)
- [天象](https://kexue.fm/ac.html)
- [链接](https://kexue.fm/links.html)
- [时光](https://kexue.fm/me.html)
- [博览](https://kexue.fm/science.html)
- [归档](https://kexue.fm/content.html)

渴望成为一个小飞侠

- [欢迎订阅](https://kexue.fm/feed)
- [个性邮箱](https://kexue.fm/archives/119)
- [天象信息](https://kexue.fm/ac.html)
- [观测ISS](https://kexue.fm/archives/41)
- [LaTeX](https://kexue.fm/latex.html)
- [关于博主](https://kexue.fm/me.html)

欢迎访问“科学空间”，这里将与您共同探讨自然科学，回味人生百态；也期待大家的分享～

- [**千奇百怪** Everything](https://kexue.fm/category/Everything)
- [**天文探索** Astronomy](https://kexue.fm/category/Astronomy)
- [**数学研究** Mathematics](https://kexue.fm/category/Mathematics)
- [**物理化学** Phy-chem](https://kexue.fm/category/Phy-chem)
- [**信息时代** Big-Data](https://kexue.fm/category/Big-Data)
- [**生物自然** Biology](https://kexue.fm/category/Biology)
- [**图片摄影** Photograph](https://kexue.fm/category/Photograph)
- [**问题百科** Questions](https://kexue.fm/category/Questions)
- [**生活/情感** Life-Feeling](https://kexue.fm/category/Life-Feeling)
- [**资源共享** Resources](https://kexue.fm/category/Resources)

- [**千奇百怪**](https://kexue.fm/category/Everything)
- [**天文探索**](https://kexue.fm/category/Astronomy)
- [**数学研究**](https://kexue.fm/category/Mathematics)
- [**物理化学**](https://kexue.fm/category/Phy-chem)
- [**信息时代**](https://kexue.fm/category/Big-Data)
- [**生物自然**](https://kexue.fm/category/Biology)
- [**图片摄影**](https://kexue.fm/category/Photograph)
- [**问题百科**](https://kexue.fm/category/Questions)
- [**生活/情感**](https://kexue.fm/category/Life-Feeling)
- [**资源共享**](https://kexue.fm/category/Resources)

[首页](https://kexue.fm) [信息时代](https://kexue.fm/category/Big-Data) CRF用过了，不妨再了解下更快的MEMM？

24Feb

# [CRF用过了，不妨再了解下更快的MEMM？](https://kexue.fm/archives/7213)

By 苏剑林 \|
2020-02-24 \|
57356位读者\|

HMM、MEMM、CRF被称为是三大经典概率图模型，在深度学习之前的机器学习时代，它们被广泛用于各种序列标注相关的任务中。一个有趣的现象是，到了深度学习时代，HMM和MEMM似乎都“没落”了，舞台上就只留下CRF。相信做NLP的读者朋友们就算没亲自做过也会听说过BiLSTM+CRF做中文分词、命名实体识别等任务，却几乎没有听说过BiLSTM+HMM、BiLSTM+MEMM的，这是为什么呢？

今天就让我们来学习一番MEMM，并且通过与CRF的对比，来让我们更深刻地理解概率图模型的思想与设计。

## 模型推导 [\#](https://kexue.fm/kexue.fm\#%E6%A8%A1%E5%9E%8B%E6%8E%A8%E5%AF%BC)

MEMM全称Maximum Entropy Markov Model，中文名可译为“最大熵马尔可夫模型”。不得不说，这个名字可能会吓退80%的初学者：最大熵还没搞懂，马尔可夫也不认识，这两个合起来怕不是天书？而事实上，不管是MEMM还是CRF，它们的模型都远比它们的名字来得简单，它们的概念和设计都非常朴素自然，并不难理解。

### 回顾CRF [\#](https://kexue.fm/kexue.fm\#%E5%9B%9E%E9%A1%BECRF)

作为对比，我们还是来回顾一下CRF。说是“回顾”，是因为笔者之前已经撰文介绍过CRF了，如果对CRF还不是很了解的读者，可以先去阅读旧作 [《简明条件随机场CRF介绍（附带纯Keras实现）》](https://kexue.fm/archives/5542)。简单起见，本文介绍的CRF和MEMM都是最简单的“线性链”版本。

本文都是以序列标注为例，即输入序列$\\boldsymbol{x}=(x\_1,x\_2,\\dots,x\_n)$，希望输出同样长度的标签序列$\\boldsymbol{y}=(y\_1,y\_2,\\dots,y\_n)$，那么建模的就是概率分布
\\begin{equation}P(\\boldsymbol{y}\|\\boldsymbol{x})=P(y\_1,y\_2,\\dots,y\_n\|\\boldsymbol{x})\\label{eq:target}\\end{equation}
CRF把$\\boldsymbol{y}$看成一个整体，算一个总得分，计算公式如下
\\begin{equation}\\begin{aligned}f(y\_1,y\_2,\\dots,y\_n;\\boldsymbol{x})=&\\,f(y\_1;\\boldsymbol{x})+g(y\_1,y\_2)+\\dots+g(y\_{n-1},y\_n)+f(y\_n;\\boldsymbol{x})\\\
=&\\,f(y\_1;\\boldsymbol{x}) + \\sum\_{k=2}^n \\big(g(y\_{k-1},y\_k)+f(y\_k;\\boldsymbol{x})\\big)\\end{aligned}\\end{equation}
这个打分函数的特点就是显式地考虑了相邻标签的关联，其实$g(y\_{k-1},y\_k)$被称为转移矩阵。现在得分算出来了，概率就是得分的softmax，所以最终概率分布的形式设为
\\begin{equation}P(\\boldsymbol{y}\|\\boldsymbol{x})=\\frac{e^{f(y\_1,y\_2,\\dots,y\_n;\\boldsymbol{x})}}{\\sum\\limits\_{y\_1,y\_2,\\dots,y\_n}e^{f(y\_1,y\_2,\\dots,y\_n;\\boldsymbol{x})}}\\label{eq:crf-p}\\end{equation}

如果仅局限于概念的话，那么CRF的介绍到此就结束了。总的来说，就是将目标序列当成一个整体，先给目标设计一个打分函数，然后对打分函数进行整体的softmax，这个建模理念跟普通的分类问题是一致的。CRF的困难之处在于代码实现，因为上式的分母项包含了所有路径的求和，这并不是一件容易的事情，但在概念理解上，笔者相信并没有什么特别困难之处。

### 更朴素的MEMM [\#](https://kexue.fm/kexue.fm\#%E6%9B%B4%E6%9C%B4%E7%B4%A0%E7%9A%84MEMM)

现在我们来介绍MEMM，它可以看成一个极度简化的seq2seq模型。对于目标$\\eqref{eq:target}$，它考虑分解
\\begin{equation}P(y\_1,y\_2,\\dots,y\_n\|\\boldsymbol{x})=P(y\_1\|\\boldsymbol{x})P(y\_2\|\\boldsymbol{x},y\_1)P(y\_3\|\\boldsymbol{x},y\_1,y\_2)\\dots P(y\_n\|\\boldsymbol{x},y\_1,y\_2,\\dots,y\_{n-1})\\end{equation}
然后假设标签的依赖只发生在相邻位置，所以
\\begin{equation}P(y\_1,y\_2,\\dots,y\_n\|\\boldsymbol{x})=P(y\_1\|\\boldsymbol{x})P(y\_2\|\\boldsymbol{x},y\_1)P(y\_3\|\\boldsymbol{x},y\_2)\\dots P(y\_n\|\\boldsymbol{x},y\_{n-1})\\label{eq:p-f}\\end{equation}
接着仿照线性链CRF的设计，我们可以设
\\begin{equation}P(y\_1\|\\boldsymbol{x})=\\frac{e^{f(y\_1;\\boldsymbol{x})}}{\\sum\\limits\_{y\_1}e^{f(y\_k;\\boldsymbol{x})}},\\quad P(y\_k\|\\boldsymbol{x},y\_{k-1})=\\frac{e^{g(y\_{k-1},y\_k)+f(y\_k;\\boldsymbol{x})}}{\\sum\\limits\_{y\_k}e^{g(y\_{k-1},y\_k)+f(y\_k;\\boldsymbol{x})}}\\label{eq:memm}\\end{equation}
至此，这就得到了MEMM了。由于MEMM已经将整体的概率分布分解为逐步的分布之积了，所以算loss只需要把每一步的交叉熵求和。

### 两者的关系 [\#](https://kexue.fm/kexue.fm\#%E4%B8%A4%E8%80%85%E7%9A%84%E5%85%B3%E7%B3%BB)

将式$\\eqref{eq:memm}$代回式$\\eqref{eq:p-f}$，我们可以得到
\\begin{equation}P(\\boldsymbol{y}\|\\boldsymbol{x})=\\frac{e^{f(y\_1;\\boldsymbol{x})+g(y\_1,y\_2)+\\dots+g(y\_{n-1},y\_n)+f(y\_n;\\boldsymbol{x})}}{\\left(\\sum\\limits\_{y\_1}e^{f(y\_1;\\boldsymbol{x})}\\right)\\left(\\sum\\limits\_{y\_2}e^{g(y\_1,y\_2)+f(y\_2;\\boldsymbol{x})}\\right)\\dots\\left(\\sum\\limits\_{y\_n}e^{g(y\_{n-1},y\_n)+f(y\_n;\\boldsymbol{x})}\\right)}\\label{eq:memm-p}\\end{equation}

对比式$\\eqref{eq:memm-p}$和式$\\eqref{eq:crf-p}$，我们可以发现，MEMM跟CRF的区别仅在于分母（也就是归一化因子）的计算方式不同，CRF的式$\\eqref{eq:crf-p}$我们称之为是全局归一化的，而MEMM的式$\\eqref{eq:memm-p}$我们称之为是局部归一化的。

## 模型分析 [\#](https://kexue.fm/kexue.fm\#%E6%A8%A1%E5%9E%8B%E5%88%86%E6%9E%90)

这一节我们来分析MEMM的优劣、改进与效果。

### MEMM的优劣 [\#](https://kexue.fm/kexue.fm\#MEMM%E7%9A%84%E4%BC%98%E5%8A%A3)

MEMM的一个明显的特点是实现简单、速度快，因为它只需要每一步单独执行softmax，所以MEMM是完全可以并行的，速度跟直接逐步Softmax基本一样。而对于CRF，式$\\eqref{eq:crf-p}$的分母并不是那么容易算的，它最终转化为一个递归计算，可以在$\\mathcal{O}(n)$的时间内算出来（具体细节还请参考 [《简明条件随机场CRF介绍（附带纯Keras实现）》](https://kexue.fm/archives/5542)），递归就意味着是串行的，因此当我们模型的主体部分是高度可并行的架构（比如纯CNN或纯Attention架构）时，CRF会严重拖慢模型的训练速度。后面我们有比较MEMM和CRF的训练速度（当然，仅仅是训练慢了，预测阶段MEMM和CRF的速度都一样）。

至于劣势，自然也是有的。前面我们提到过，MEMM可以看成是一个极度简化的seq2seq模型。既然是这样，那么普通seq2seq模型有的弊端它都有。seq2seq中有一个明显的问题是exposure bias，对应到MEMM中，它被称之为label bias，大概意思是：在训练MEMM的时候，对于当前步的预测，都是假设前一步的真实标签已知，这样一来，如果某个标签$A$后面只能接标签$B$，那么模型只需要通过优化转移矩阵就可以实现这一点，而不需要优化输入$\\boldsymbol{x}$对$B$的影响（即没有优化好$f(B;\\boldsymbol{x})$）；然而，在预测阶段，真实标签是不知道的，我们可能无法以较高的置信度预测前一步的标签$A$，而由于在训练阶段，当前步的$f(B;\\boldsymbol{x})$也没有得到强化，所以当前步$B$也无法准确预测，这就有可能导致错误的预测结果。

### 双向MEMM [\#](https://kexue.fm/kexue.fm\#%E5%8F%8C%E5%90%91MEMM)

label bias可能不好理解，但我们可以从另外一个视角看MEMM的不足：事实上，相比CRF，MEMM明显的一个不够漂亮的地方就是它的不对称性——它是从左往右进行概率分解的。笔者的实验表明，如果能解决这个不对称性，能够稍微提高MEMM的效果。笔者的思路是：将MEMM也从右往左做一遍，这时候对应的概率分布是
\\begin{equation}P(\\boldsymbol{y}\|\\boldsymbol{x})=\\frac{e^{f(y\_1;\\boldsymbol{x})+g(y\_1,y\_2)+\\dots+g(y\_{n-1},y\_n)+f(y\_n;\\boldsymbol{x})}}{\\left(\\sum\\limits\_{y\_n}e^{f(y\_n;\\boldsymbol{x})}\\right)\\left(\\sum\\limits\_{y\_{n-1}}e^{g(y\_n,y\_{n-1})+f(y\_{n-1};\\boldsymbol{x})}\\right)\\dots\\left(\\sum\\limits\_{y\_1}e^{g(y\_2,y\_1)+f(y\_1;\\boldsymbol{x})}\\right)}\\end{equation}
然后也算一个交叉熵，跟从左往右的式$\\eqref{eq:memm-p}$的交叉熵平均，作为最终的loss。这样一来，模型同时考虑了从左往右和从右往左两个方向，并且也不用增加参数，弥补了不对称性的缺陷。作为区分，笔者类比Bi-LSTM的命名，将其称为Bi-MEMM。

> 注：Bi-MEMM这个名词并不是在此处首次出现，据笔者所查，首次提出Bi-MEMM这个概念的，是论文 [《Bidirectional Inference with the Easiest-First Strategy for Tagging Sequence Data》](https://www.aclweb.org/anthology/H05-1059/)，里边的Bi-MEMM是指一种MEMM的双向解码策略，跟本博客的Bi-MEMM含义并不相同。

### 实验结果演示 [\#](https://kexue.fm/kexue.fm\#%E5%AE%9E%E9%AA%8C%E7%BB%93%E6%9E%9C%E6%BC%94%E7%A4%BA)

为了验证和比较MEMM的效果，笔者将CRF和MEMM同时写进了 [bert4keras](https://github.com/bojone/bert4keras) 中，并且写了中文分词（ [task\_sequence\_labeling\_cws\_crf.py](https://github.com/bojone/bert4keras/blob/master/examples/task_sequence_labeling_cws_crf.py)）和中文命名实体识别（ [task\_sequence\_labeling\_ner\_crf.py](https://github.com/bojone/bert4keras/blob/master/examples/task_sequence_labeling_ner_crf.py)）两个脚本。在这两个脚本中，从CRF切换到MEMM非常简单，只需将 `ConditionalRandomField` 替换为 `MaximumEntropyMarkovModel`。

详细的实验数据就不贴出来了，反正就是一些数字罢了，下面直接给出一些相对比较的结果：

> 1、相同的实验参数下，Bi-MEMM总是优于MEMM，MEMM总是优于Softmax；
>
> 2、相同的实验参数下，CRF基本上不差于Bi-MEMM；
>
> 3、当编码模型能力比较强时，CRF与Bi-MEMM效果持平；当编码模型较弱时，CRF优于Bi-MEMM，幅度约为0.5%左右；
>
> 4、用12层bert base模型作为编码模型时，Bi-MEMM比CRF快25%；用2层bert base模型作为编码模型时，Bi-MEMM比CRF快1.5倍。

（注：由于笔者发现Bi-MEMM效果总比MEMM略好，并且两者的训练时间基本无异，所以bert4keras里边的MaximumEntropyMarkovModel默认就是Bi-MEMM。）

## 思考与拓展 [\#](https://kexue.fm/kexue.fm\#%E6%80%9D%E8%80%83%E4%B8%8E%E6%8B%93%E5%B1%95)

根据上面的结论，在深度学习时代，MEMM的“没落”似乎就可以理解了——MEMM除了训练速度快点之外，相比CRF似乎也就没什么好处了，两者的预测速度是一样的，而很多时候我们主要关心预测速度和效果，训练速度稍微慢点也无妨。这两个模型的比较结果是有代表性的，可以说这正是所有全局归一化和局部归一化模型的差异：全局归一化模型效果通常好些，但实现通常相对困难一些；局部归一化模型效果通常不超过全局归一化模型，但胜在易于实现，并与易于拓展。

如何易于拓展？这里举两个方向的例子。

第一个例子，假如标签数很大的时候，比如用序列标注的方式做文本纠错或者文本生成时（相关例子可以参考论文 [《Fast Structured Decoding for Sequence Models》](https://papers.cool/arxiv/1910.11555)），标签的数目就是词表里边的词汇数$\|V\|$，就算用了subword方法，词汇数少说也有一两万，这时候转移矩阵的参数量就达到数亿（$\|V\|^2$），难以训练。读者可能会想到低秩分解，不错，低秩分解可以将转移矩阵的参数量控制为$2d\|V\|$，其中$d$是分解的中间层维度。不幸的是，对于CRF来说，低秩分解不能改变归一化因子计算量大的事实，因为CRF的归一化因子依然需要恢复为$\|V\|\\times\|V\|$的转移矩阵才能计算下去，所以对于标签数目巨大的场景，CRF无法直接使用。但幸运的是，对于MEMM来说，低秩分解可以有效低降低训练时的计算量，从而依然能够有效的使用。bert4keras里边所带的 `MaximumEntropyMarkovModel` 已经把低秩序分解集成进去了，有兴趣的读者可以查看源码了解细节。

第二个例子，上述介绍的CRF和MEMM都只考虑相邻标签之间的关联，如果我要考虑更复杂的邻接关联呢？比如同时考虑$y\_k$跟$y\_{k-1},y\_{k-2}$的关联？这时候CRF的全局归一化思路就很难操作了，归根结底还是归一化因子难算；但如果是MEMM的局部归一化思路就容易进行。事实上，笔者之前设计的 [信息抽取分层标注思路](https://kexue.fm/%E5%9F%BA%E4%BA%8EDGCNN%E5%92%8C%E6%A6%82%E7%8E%87%E5%9B%BE%E7%9A%84%E8%BD%BB%E9%87%8F%E7%BA%A7%E4%BF%A1%E6%81%AF%E6%8A%BD%E5%8F%96%E6%A8%A1%E5%9E%8B)，也可以说是一种跟MEMM类似的局部归一化的概率图模型，它考虑的关联就更复杂化了。

## 文章小结 [\#](https://kexue.fm/kexue.fm\#%E6%96%87%E7%AB%A0%E5%B0%8F%E7%BB%93)

本文介绍并简单推广了跟CRF一样同为概率图经典案例的MEMM，它与CRF的区别主要是归一化方式上的不一样，接着笔者从实验上对两者做了简单的比较，得出MEMM训练更快但效果不优于CRF的结论。尽管如此，笔者认为MEMM仍有可取之处，所以最后构思了MEMM的一些拓展。

_**转载到请包括本文地址：** [https://kexue.fm/archives/7213](https://kexue.fm/archives/7213)_

_**更详细的转载事宜请参考：**_ [《科学空间FAQ》](https://kexue.fm/archives/6508#%E6%96%87%E7%AB%A0%E5%A6%82%E4%BD%95%E8%BD%AC%E8%BD%BD/%E5%BC%95%E7%94%A8)

**如果您还有什么疑惑或建议，欢迎在下方评论区继续讨论。**

**如果您觉得本文还不错，欢迎 [分享](https://kexue.fm/kexue.fm#share)/ [打赏](https://kexue.fm/kexue.fm#pay) 本文。打赏并非要从中获得收益，而是希望知道科学空间获得了多少读者的真心关注。当然，如果你无视它，也不会影响你的阅读。再次表示欢迎和感谢！**

打赏

微信打赏

支付宝打赏

因为网站后台对打赏并无记录，因此欢迎在打赏时候备注留言。你还可以 [**点击这里**](http://mail.qq.com/cgi-bin/qm_share?t=qm_mailme&email=tN7d1drY3drrx8H0xcWa19vZ) 或在下方评论区留言来告知你的建议或需求。

**如果您需要引用本文，请参考：**

苏剑林. (Feb. 24, 2020). 《CRF用过了，不妨再了解下更快的MEMM？ 》\[Blog post\]. Retrieved from [https://kexue.fm/archives/7213](https://kexue.fm/archives/7213)

@online{kexuefm-7213,
        title={CRF用过了，不妨再了解下更快的MEMM？},
        author={苏剑林},
        year={2020},
        month={Feb},
        url={\\url{https://kexue.fm/archives/7213}},
}

分类： [信息时代](https://kexue.fm/category/Big-Data)    标签： [模型](https://kexue.fm/tag/%E6%A8%A1%E5%9E%8B/), [概率图](https://kexue.fm/tag/%E6%A6%82%E7%8E%87%E5%9B%BE/), [crf](https://kexue.fm/tag/crf/)[7 评论](https://kexue.fm/archives/7213#comments)

< [Designing GANs：又一个GAN生产车间](https://kexue.fm/archives/7210) \| [对抗训练浅谈：意义、方法和思考（附Keras实现）](https://kexue.fm/archives/7234) >

### 你也许还对下面的内容感兴趣

- [MoE环游记：1、从几何意义出发](https://kexue.fm/archives/10699)
- [基于量子化假设推导模型的尺度定律（Scaling Law）](https://kexue.fm/archives/9607)
- [Tiger：一个“抠”到极致的优化器](https://kexue.fm/archives/9512)
- [在bert4keras中使用混合精度和XLA加速训练](https://kexue.fm/archives/9059)
- [为什么需要残差？一个来自DeepNet的视角](https://kexue.fm/archives/8994)
- [门控注意力单元（GAU）还需要Warmup吗？](https://kexue.fm/archives/8990)
- [Efficient GlobalPointer：少点参数，多点效果](https://kexue.fm/archives/8877)
- [开局一段扯，数据全靠编？真被一篇“神论文”气到了](https://kexue.fm/archives/8783)
- [Dropout视角下的MLM和MAE：一些新的启发](https://kexue.fm/archives/8770)
- [ChildTuning：试试把Dropout加到梯度上去？](https://kexue.fm/archives/8764)

[发表你的看法](https://kexue.fm/kexue.fm#comment_form)

wendao

February 25th, 2020

非常棒的讲解和例子，感谢苏老师！
PS: 正文里 task\_sequence\_labeling\_cws\_crf.py 的链接似乎贴错了。或许有人在py3里跑，第46行需要convert一下：random\_order = list(range(len(data)))。

[回复评论](https://kexue.fm/archives/7213/comment-page-1?replyTo=12905#respond-post-7213)

[苏剑林](https://kexue.fm) 发表于
February 25th, 2020

好的，感谢指正，已修正了^\_^

[回复评论](https://kexue.fm/archives/7213/comment-page-1?replyTo=12907#respond-post-7213)

[\[白话解析\] 用水浒传为例学习条件随机场 R11; 模范科技](http://118.190.38.122/bai-hua-jie-xi-yong-shui-hu-chuan-wei-li-xue-xi-tiao-jian-sui-ji-chang.html)

March 25th, 2020

\[...\]CRF用过了，不妨再了解下更快的MEMM？\[...\]

[回复评论](https://kexue.fm/archives/7213/comment-page-1?replyTo=13077#respond-post-7213)

Shaohua Yang

April 20th, 2020

"预测阶段MEMM和CRF的速度都一样", 预测阶段crf的viterbi算法应该要比memm的greedy慢不少才对。

[回复评论](https://kexue.fm/archives/7213/comment-page-1?replyTo=13199#respond-post-7213)

[苏剑林](https://kexue.fm) 发表于
April 20th, 2020

预测阶段MEMM也可以用viterbi。

[回复评论](https://kexue.fm/archives/7213/comment-page-1?replyTo=13205#respond-post-7213)

YeLiu

December 28th, 2021

苏老师，有个疑问啊，正向和反向的实际标签转移概率应该是不一样的，为什么能使用一个转移矩阵进行训练呢？

[回复评论](https://kexue.fm/archives/7213/comment-page-1?replyTo=18105#respond-post-7213)

[苏剑林](https://kexue.fm) 发表于
December 29th, 2021

你说的两个事情不矛盾。

[回复评论](https://kexue.fm/archives/7213/comment-page-1?replyTo=18117#respond-post-7213)

[取消回复](https://kexue.fm/archives/7213#respond-post-7213)

你的大名

电子邮箱

个人网站（选填）

1\. 可以使用LaTeX代码，点击“预览效果”可查看效果；2. 可以通过点击评论楼层编号来引用该楼层；3. 网站可能会有点卡，如非确认评论失败，请 **不要重复点击提交**。

### 内容速览

[模型推导](https://kexue.fm/kexue.fm#%E6%A8%A1%E5%9E%8B%E6%8E%A8%E5%AF%BC)
[回顾CRF](https://kexue.fm/kexue.fm#%E5%9B%9E%E9%A1%BECRF)
[更朴素的MEMM](https://kexue.fm/kexue.fm#%E6%9B%B4%E6%9C%B4%E7%B4%A0%E7%9A%84MEMM)
[两者的关系](https://kexue.fm/kexue.fm#%E4%B8%A4%E8%80%85%E7%9A%84%E5%85%B3%E7%B3%BB)
[模型分析](https://kexue.fm/kexue.fm#%E6%A8%A1%E5%9E%8B%E5%88%86%E6%9E%90)
[MEMM的优劣](https://kexue.fm/kexue.fm#MEMM%E7%9A%84%E4%BC%98%E5%8A%A3)
[双向MEMM](https://kexue.fm/kexue.fm#%E5%8F%8C%E5%90%91MEMM)
[实验结果演示](https://kexue.fm/kexue.fm#%E5%AE%9E%E9%AA%8C%E7%BB%93%E6%9E%9C%E6%BC%94%E7%A4%BA)
[思考与拓展](https://kexue.fm/kexue.fm#%E6%80%9D%E8%80%83%E4%B8%8E%E6%8B%93%E5%B1%95)
[文章小结](https://kexue.fm/kexue.fm#%E6%96%87%E7%AB%A0%E5%B0%8F%E7%BB%93)

### 智能搜索

支持整句搜索！网站自动使用 [结巴分词](https://github.com/fxsjy/jieba) 进行分词，并结合ngrams排序算法给出合理的搜索结果。

### 热门标签

[生成模型](https://kexue.fm/tag/%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B/) [attention](https://kexue.fm/tag/attention/) [优化](https://kexue.fm/tag/%E4%BC%98%E5%8C%96/) [语言模型](https://kexue.fm/tag/%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/) [模型](https://kexue.fm/tag/%E6%A8%A1%E5%9E%8B/) [网站](https://kexue.fm/tag/%E7%BD%91%E7%AB%99/) [概率](https://kexue.fm/tag/%E6%A6%82%E7%8E%87/) [梯度](https://kexue.fm/tag/%E6%A2%AF%E5%BA%A6/) [转载](https://kexue.fm/tag/%E8%BD%AC%E8%BD%BD/) [矩阵](https://kexue.fm/tag/%E7%9F%A9%E9%98%B5/) [微分方程](https://kexue.fm/tag/%E5%BE%AE%E5%88%86%E6%96%B9%E7%A8%8B/) [优化器](https://kexue.fm/tag/%E4%BC%98%E5%8C%96%E5%99%A8/) [分析](https://kexue.fm/tag/%E5%88%86%E6%9E%90/) [天象](https://kexue.fm/tag/%E5%A4%A9%E8%B1%A1/) [深度学习](https://kexue.fm/tag/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/) [积分](https://kexue.fm/tag/%E7%A7%AF%E5%88%86/) [python](https://kexue.fm/tag/python/) [力学](https://kexue.fm/tag/%E5%8A%9B%E5%AD%A6/) [无监督](https://kexue.fm/tag/%E6%97%A0%E7%9B%91%E7%9D%A3/) [扩散](https://kexue.fm/tag/%E6%89%A9%E6%95%A3/) [几何](https://kexue.fm/tag/%E5%87%A0%E4%BD%95/) [节日](https://kexue.fm/tag/%E8%8A%82%E6%97%A5/) [生活](https://kexue.fm/tag/%E7%94%9F%E6%B4%BB/) [文本生成](https://kexue.fm/tag/%E6%96%87%E6%9C%AC%E7%94%9F%E6%88%90/) [数论](https://kexue.fm/tag/%E6%95%B0%E8%AE%BA/)

### 随机文章

- [矩阵符号函数mcsgn能计算什么？](https://kexue.fm/archives/11056)
- [差分方程的摄动法](https://kexue.fm/archives/3889)
- [Self-Orthogonality Module：一个即插即用的核正交化模块](https://kexue.fm/archives/7169)
- [宇宙驿站十岁啦](https://kexue.fm/archives/1557)
- [修改Transformer结构，设计一个更快更好的MLM模型](https://kexue.fm/archives/7661)
- [《自然极值》系列——3.平衡态公理](https://kexue.fm/archives/1072)
- [2009年中秋手机拍摄月亮](https://kexue.fm/archives/157)
- [搜狐文本匹配：基于条件LayerNorm的多任务baseline](https://kexue.fm/archives/8337)
- [哥德巴赫猜想浅谈1](https://kexue.fm/archives/1727)
- [Transformer升级之路：17、多模态位置编码的简单思考](https://kexue.fm/archives/10040)

### 最近评论

- [mp4网](https://kexue.fm/links.html/comment-page-6#comment-28589): 申请友链
站名：mp4网
域名：http://mp4wang.cc
描述：在线视频
- [FrankCai](https://kexue.fm/archives/10563/comment-page-1#comment-28588): 老师您好， 如果结论是【当ϵ越大，结果越接近SGD，“Surge现象”出现的概率就越低】，那我...
- [川zi](https://kexue.fm/archives/9009/comment-page-3#comment-28587): 看懂了， 谢谢大佬
- [pb](https://kexue.fm/archives/8265/comment-page-8#comment-28586): 苏神您好，这两天读到“远程衰减”的部分时觉得有些奇怪，还请指正。
文中的 upperbound...
- [苏剑林](https://kexue.fm/archives/9119/comment-page-13#comment-28585): 这就是diffusion模型的神奇之处，它有点直观，但又不完全可以单纯由直觉得到。你说的$\\b...
- [苏剑林](https://kexue.fm/archives/11033/comment-page-2#comment-28584): 平方或者线性复杂度，都是指关于序列长度$n$的总复杂度，$d$不算。
- [苏剑林](https://kexue.fm/archives/9907/comment-page-4#comment-28583): cool papers的prompt其实没啥特殊的，关键的技巧是一个问题一个问题地问，不要一次...
- [苏剑林](https://kexue.fm/archives/443/comment-page-1#comment-28582): 已发
- [苏剑林](https://kexue.fm/archives/10592/comment-page-2#comment-28581): Muon不是“可以”用更大的学习率，它用更大的学习率是因为msign的结果本身就是偏小的。举个...
- [苏剑林](https://kexue.fm/archives/11033/comment-page-2#comment-28580): 逻辑是：训练一个模型，约等于把训练集压缩到模型权重中，模型权重是固定大小的，所以本质上是将数目...

### 友情链接

- [Cool Papers](https://papers.cool)
- [数学研发](https://bbs.emath.ac.cn)
- [Seatop](http://www.seatop.com.cn/)
- [Xiaoxia](https://xiaoxia.org/)
- [积分表-网络版](https://kexue.fm/sci/integral/index.html)
- [丝路博傲](http://blog.dvxj.com/)
- [数学之家](http://www.2math.cn/)
- [有趣天文奇观](http://interesting-sky.china-vo.org/)
- [TwistedW](http://www.twistedwg.com/)
- [godweiyang](https://godweiyang.com/)
- [AI柠檬](https://blog.ailemon.net/)
- [王登科-DK博客](https://greatdk.com)
- [ESON](https://blog.eson.org/)
- [枫之羽](https://fzhiy.net/)
- [Mathor's blog](https://wmathor.com/)
- [coding-zuo](https://coding-zuo.github.io/)
- [博科园](https://www.bokeyuan.net/)
- [孔皮皮的博客](https://www.kppkkp.top/)
- [运鹏的博客](https://yunpengtai.top/)
- [jiming.site](https://jiming.site/)
- [OmegaXYZ](https://www.omegaxyz.com/)
- [EAI猩球](https://www.robotech.ink/)
- [文举的博客](https://liwenju0.com/)
- [用代码打点酱油](https://bruceyuan.com/)
- [Zhang's blog](https://armcvai.cn/)
- [申请链接](https://kexue.fm/links.html)

本站采用创作共用版权协议，要求署名、非商业用途和保持一致。转载本站内容必须也遵循“ [署名-非商业用途-保持一致](http://creativecommons.org/licenses/by-nc-nd/2.5/cn/)”的创作共用协议。
© 2009-2025 Scientific Spaces. All rights reserved. Theme by [laogui](http://www.laogui.com). Powered by [Typecho](http://typecho.org). 备案号: [粤ICP备09093259号-1/2](https://beian.miit.gov.cn/)。