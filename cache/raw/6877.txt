![MobileSideBar](https://kexue.fm/usr/themes/geekg/images/slide-button.png)

## SEARCH

## MENU

- [打赏](https://kexue.fm/reward.html)
- [公式](https://kexue.fm/latex.html)
- [天象](https://kexue.fm/ac.html)
- [链接](https://kexue.fm/links.html)
- [时光](https://kexue.fm/me.html)
- [博览](https://kexue.fm/science.html)
- [归档](https://kexue.fm/content.html)

## CATEGORIES

- [千奇百怪](https://kexue.fm/category/Everything)
- [天文探索](https://kexue.fm/category/Astronomy)
- [数学研究](https://kexue.fm/category/Mathematics)
- [物理化学](https://kexue.fm/category/Phy-chem)
- [信息时代](https://kexue.fm/category/Big-Data)
- [生物自然](https://kexue.fm/category/Biology)
- [图片摄影](https://kexue.fm/category/Photograph)
- [问题百科](https://kexue.fm/category/Questions)
- [生活/情感](https://kexue.fm/category/Life-Feeling)
- [资源共享](https://kexue.fm/category/Resources)

## NEWPOSTS

- [让炼丹更科学一些（五）：基于梯度精...](https://kexue.fm/archives/11530)
- [让炼丹更科学一些（四）：新恒等式，...](https://kexue.fm/archives/11494)
- [为什么DeltaNet要加L2 N...](https://kexue.fm/archives/11486)
- [让炼丹更科学一些（三）：SGD的终...](https://kexue.fm/archives/11480)
- [让炼丹更科学一些（二）：将结论推广...](https://kexue.fm/archives/11469)
- [滑动平均视角下的权重衰减和学习率](https://kexue.fm/archives/11459)
- [生成扩散模型漫谈（三十一）：预测数...](https://kexue.fm/archives/11428)
- [Muon优化器指南：快速上手与关键细节](https://kexue.fm/archives/11416)
- [AdamW的Weight RMS的...](https://kexue.fm/archives/11404)
- [n个正态随机数的最大值的渐近估计](https://kexue.fm/archives/11390)

## COMMENTS

- [Bin: 今天偶然从某个论坛看到有人推荐您的博客，定睛一看竟然是华师同院...](https://kexue.fm/archives/1990/comment-page-2#comment-29105)
- [Rapture D: 我有一个问题，为什么不考虑亥姆霍兹定理和斯托克斯公式。](https://kexue.fm/archives/11530/comment-page-1#comment-29104)
- [mofheka: 苏神是还在用jax是么？最近在做基于Google Pathwa...](https://kexue.fm/archives/11390/comment-page-1#comment-29103)
- [长琴: 看懂这篇博客也不是一件容易的事情。](https://kexue.fm/archives/11530/comment-page-1#comment-29102)
- [AlexLi: 苏老师，请教一下(7)式中将 $\\mu(x\_t)$ 传给 $p...](https://kexue.fm/archives/9257/comment-page-4#comment-29101)
- [tyler\_zxc: "Performer的思想是将标准的Attention线性化，...](https://kexue.fm/archives/7921/comment-page-2#comment-29100)
- [我: 似乎并非mHC提出矩阵的思想？之前hyper connecti...](https://kexue.fm/archives/11494/comment-page-1#comment-29099)
- [winter: 苏神您好，假如对于比较均匀的attention weightP...](https://kexue.fm/archives/10847/comment-page-1#comment-29098)
- [苏剑林: KL散度、JS散度、W距离啥的，都行啊，看你喜欢哪个](https://kexue.fm/archives/8512/comment-page-2#comment-29097)
- [苏剑林: 没有绝对公平的对比方法，主要看你关心什么。比如，如果只关心推理...](https://kexue.fm/archives/9119/comment-page-14#comment-29096)

## USERLOGIN

- [登录](https://kexue.fm/admin/login.php)

[科学空间\|Scientific Spaces](https://kexue.fm/)

- [登录](https://kexue.fm/admin/login.php)
- [打赏](https://kexue.fm/reward.html)
- [公式](https://kexue.fm/latex.html)
- [天象](https://kexue.fm/ac.html)
- [链接](https://kexue.fm/links.html)
- [时光](https://kexue.fm/me.html)
- [博览](https://kexue.fm/science.html)
- [归档](https://kexue.fm/content.html)

渴望成为一个小飞侠

- [![](https://kexue.fm/usr/themes/geekg/images/rss.png)\\
\\
欢迎订阅](https://kexue.fm/feed)
- [![](https://kexue.fm/usr/themes/geekg/images/mail.png)\\
\\
个性邮箱](https://kexue.fm/archives/119)
- [![](https://kexue.fm/usr/themes/geekg/images/Saturn.png)\\
\\
天象信息](https://kexue.fm/ac.html)
- [![](https://kexue.fm/usr/themes/geekg/images/iss.png)\\
\\
观测ISS](https://kexue.fm/archives/41)
- [![](https://kexue.fm/usr/themes/geekg/images/pi.png)\\
\\
LaTeX](https://kexue.fm/latex.html)
- [![](https://kexue.fm/usr/themes/geekg/images/mlogo.png)\\
\\
关于博主](https://kexue.fm/me.html)

欢迎访问“科学空间”，这里将与您共同探讨自然科学，回味人生百态；也期待大家的分享～

- [**千奇百怪** Everything](https://kexue.fm/category/Everything)
- [**天文探索** Astronomy](https://kexue.fm/category/Astronomy)
- [**数学研究** Mathematics](https://kexue.fm/category/Mathematics)
- [**物理化学** Phy-chem](https://kexue.fm/category/Phy-chem)
- [**信息时代** Big-Data](https://kexue.fm/category/Big-Data)
- [**生物自然** Biology](https://kexue.fm/category/Biology)
- [**图片摄影** Photograph](https://kexue.fm/category/Photograph)
- [**问题百科** Questions](https://kexue.fm/category/Questions)
- [**生活/情感** Life-Feeling](https://kexue.fm/category/Life-Feeling)
- [**资源共享** Resources](https://kexue.fm/category/Resources)

- [**千奇百怪**](https://kexue.fm/category/Everything)
- [**天文探索**](https://kexue.fm/category/Astronomy)
- [**数学研究**](https://kexue.fm/category/Mathematics)
- [**物理化学**](https://kexue.fm/category/Phy-chem)
- [**信息时代**](https://kexue.fm/category/Big-Data)
- [**生物自然**](https://kexue.fm/category/Biology)
- [**图片摄影**](https://kexue.fm/category/Photograph)
- [**问题百科**](https://kexue.fm/category/Questions)
- [**生活/情感**](https://kexue.fm/category/Life-Feeling)
- [**资源共享**](https://kexue.fm/category/Resources)

[首页](https://kexue.fm/) [信息时代](https://kexue.fm/category/Big-Data) seq2seq之双向解码

9Aug

# [seq2seq之双向解码](https://kexue.fm/archives/6877)

By 苏剑林 \|
2019-08-09 \|
59162位读者 \|

在文章 [《玩转Keras之seq2seq自动生成标题》](https://kexue.fm/archives/5861) 中我们已经基本探讨过seq2seq，并且给出了参考的Keras实现。

本文则将这个seq2seq再往前推一步，引入双向的解码机制，它在一定程度上能提高生成文本的质量（尤其是生成较长文本时）。本文所介绍的双向解码机制参考自 [《Synchronous Bidirectional Neural Machine Translation》](https://papers.cool/arxiv/1905.04847)，最后笔者也是用Keras实现的。

## 背景介绍 [\#](https://kexue.fm/archives/6877\#%E8%83%8C%E6%99%AF%E4%BB%8B%E7%BB%8D)

研究过seq2seq的读者都知道，常见的seq2seq的解码过程是从左往右逐字（词）生成的，即根据encoder的结果先生成第一个字；然后根据encoder的结果以及已经生成的第一个字，来去生成第二个字；再根据encoder的结果和前两个字，来生成第三个词；依此类推。总的来说，就是在建模如下概率分解

\\begin{equation}p(Y\|X)=p(y\_1\|X)p(y\_2\|X,y\_1)p(y\_3\|X,y\_1,y\_2)\\cdots\\label{eq:p}\\end{equation}

当然，也可以从右往左生成，也就是先生成倒数第一个字，再生成倒数第二个字、倒数第三个字，等等。问题是，不管从哪个方向生成，都会有方向性倾斜的问题。比如，从左往右生成的话，前几个字的生成准确率肯定会比后几个字要高，反之亦然。在 [《Synchronous Bidirectional Neural Machine Translation》](https://papers.cool/arxiv/1905.04847) 给出了如下的在机器翻译任务上的统计结果：

$$\\begin{array}{\|c\|c\|c\|}

\\hline

\\text{Model} & \\text{The first 4 tokens} & \\text{The last 4 tokens}\\\

\\hline

\\text{L2R} & 40.21\\% & 35.10\\%\\\

\\hline

\\text{R2L} & 35.67\\% & 39.47\\%\\\

\\hline

\\end{array}$$

$\\text{L2R}$和$\\text{R2L}$分别是指从左往右和从右往左的解码生成。从表中我们可以看到，如果从左往右解码，那么前四个token的准确率有40%左右，但是最后4个token的准确率只有35%；反过来也差不多。这就反映了解码的不对称性。

为了消除这种不对称性， [《Synchronous Bidirectional Neural Machine Translation》](https://papers.cool/arxiv/1905.04847) 提出了一个双向解码机制，它维护两个方向的解码器，然后通过Attention来进一步对齐生成。

## 双向解码 [\#](https://kexue.fm/archives/6877\#%E5%8F%8C%E5%90%91%E8%A7%A3%E7%A0%81)

虽然本文参考自 [《Synchronous Bidirectional Neural Machine Translation》](https://papers.cool/arxiv/1905.04847)，但我没有完全精读原文，我只是凭自己的直觉粗读了原文，大致理解了原理之后自己实现的模型，所以并不保证跟原文完全一致。此外，这篇论文并不是第一篇做双向解码生成的论文，但它是我看到的双向解码的第一篇论文，所以我就只实现了它，并没有跟其他相关论文进行对比。

### 基本思路 [\#](https://kexue.fm/archives/6877\#%E5%9F%BA%E6%9C%AC%E6%80%9D%E8%B7%AF)

既然叫双向“解码”，那么改动就只是在decoder那里，而不涉及到encoder，所以下面的介绍中也只侧重描述decoder部分。还有，要注意的是双向解码只是一个策略，而下面只是一种参考实现，并不是标准的、唯一的，这就好比我们说的seq2seq也只是序列到序列生成模型的泛指，具体encoder和decoder怎么设计，有很多可调整的地方。

首先，给出一个简单的示意动图，来演示双向解码机制的设计和交互过程：

[Your browser does not support video](https://kexue.fm/usr/uploads/2019/08/3347752814.mp4)

Seq2Seq的双向解码机制图示

如图所示，双向解码基本上可以看成是两个不同方向的解码模块共存，为了便于描述，我们将上方称为L2R模块，而下方称为R2L模块。开始情况下，大家都输入一个起始标记（上图中的S），然后L2R模块负责预测第一个字，而R2L模块负责预测最后一个字；接着，将第一个字（以及历史信息）传入到L2R模块中，来预测第二个字，为了预测第二个字，除了用到L2R模块本身的编码外，还用到R2L模块已有的编码结果；反之，将最后一个字（以及历史信息）传入到R2L模块，再加上L2R模块已有的编码信息，来预测倒数第二个字；依此类推，直到出现了结束标记（上图中的E）。

### 数学描述 [\#](https://kexue.fm/archives/6877\#%E6%95%B0%E5%AD%A6%E6%8F%8F%E8%BF%B0)

换句话说， **每个模块预测每一个字时，除了用到模块内部的信息外，还用到另一模块已经编码好的信息序列**，而这个“用”是通过Attention来实现的。用公式来说，假设当前情况下L2R模块要预测第$n$个字，以及R2L模块要预测倒数第$n$个字。假设经过若干层编码后，得到的R2L向量序列（对应图中左上方的第二行）为：

\\begin{equation}H^{(l2r)}=\\left\[h\_1^{(l2r)},h\_2^{(l2r)},\\dots,h\_n^{(l2r)}\\right\]\\end{equation}

而R2L的向量序列（对应图中左下方的倒数第二行）为：

\\begin{equation}H^{(r2l)}=\\left\[h\_1^{(r2l)},h\_2^{(r2l)},\\dots,h\_n^{(r2l)}\\right\]\\end{equation}

如果是单向解码的话，我们会用$h\_n^{(l2r)}$作为特征来预测第$n$个字，或者用$h\_n^{(r2l)}$作为特征来预测倒数第$n$个字。

在双向解码机制下，我们以$h\_n^{(l2r)}$为query，然后以$H^{(r2l)}$为key和value来做一个Attention，用Attention的输出作为特征来预测第$n$个字，这样在预测第$n$个字的时候，就可以提前“感知”到后面的字了；同样地，我们以$h\_n^{(r2l)}$为query，然后以$H^{(l2r)}$为key和value来做一个Attention，用Attention的输出作为特征来预测倒数第$n$个字，这样在预测倒数第$n$个字的时候，就可以提前“感知”到前面的字了。上面示意图中，上面两层和下面两层之间的交互，就是指Attention。在下面的代码中，用到的是最普通的乘性Attention（参考 [《〈Attention is All You Need〉浅读（简介+代码）》](https://kexue.fm/archives/4765)）。

## 模型实现 [\#](https://kexue.fm/archives/6877\#%E6%A8%A1%E5%9E%8B%E5%AE%9E%E7%8E%B0)

上面就是双向解码的基本原理和做法。可以感觉到，这样一来，seq2seq的decoder也变得对称起来了，这是一个很漂亮的特点。当然，为了完全实现这个模型，还需要思考一些问题：1、怎么训练？2、怎么预测？

### 训练方案 [\#](https://kexue.fm/archives/6877\#%E8%AE%AD%E7%BB%83%E6%96%B9%E6%A1%88)

跟普通的seq2seq一样，基本的训练方案就是用所谓的Teacher-Forcing的方式来进行训练，即L2R方向在预测第$n$个字的时候，假设前$n-1$个字都是准确知道的，而R2L方向在预测倒数第$n$个字的时候，假设倒数第$n-1,n-2,\\dots,1$个字都是准确知道的。最终的loss是两个方向的逐字交叉熵的平均。

不过这样的训练方案实在是无可奈何之举，后面我们会分析它信息泄漏的弊端。

### 双向束搜索 [\#](https://kexue.fm/archives/6877\#%E5%8F%8C%E5%90%91%E6%9D%9F%E6%90%9C%E7%B4%A2)

现在讨论预测过程。

如果是常规的单向解码的seq2seq，我们会使用beam search（束搜索）的算法，给出概率尽可能大的序列。所谓beam search，指的是依次逐字解码，每次只保留概率最大的topk条“临时路径”，直到出现结束标记为止。

到了双向解码这里，情况变得复杂了一些。我们依然用beam search的思路，但是同时缓存两个方向的topk结果，也就是说，L2R和R2L两个方向各存topk条临时路径。此外，由于双向解码时，L2R的解码是要参考R2L已有的解码结果的，所以当我们要预测下一个字时，除了要枚举概率最高的topk个字、枚举topk条L2R的临时路径外，还要枚举topk条R2L的临时路径，所以一共要计算topk3那么多个组合。而计算完成后，采用了一种最简单的思路：对每种“字 \- L2R临时路径”的得分在“R2L临时路径”这一维度上做了平均，使得的分数变回topk2个，作为每种“字 \- L2R临时路径”的得分，再从这topk2个组合中，选出分数最高的topk个。而R2L这边的解码，则要进行反向的、相同的处理。最后，如果L2R和R2L两个方向都解码出了完成的句子，那么就选择概率（得分）最高的那个。

这样的整个过程，我们称之为“双向束搜索（双向beam search）”。如果读者自己比较熟悉单向的beam search，甚至自己都写过beam search的话，上述过程其实不难理解（看看代码就更容易懂了），它算是单向beam search自然延伸。当然，如果对beam search本身不了解的话，看上述搜索的过程应该是云里雾里的。所以想要弄清楚原理的读者，应该要从常规的单向beam search出发，先把它弄懂了，然后再看上述解码过程的描述，最后再看看下面给出的参考代码，就容易弄懂了。

### 代码参考 [\#](https://kexue.fm/archives/6877\#%E4%BB%A3%E7%A0%81%E5%8F%82%E8%80%83)

下面是笔者给出了双向解码的参考实现，整体还是跟之前的 [《玩转Keras之seq2seq自动生成标题》](https://kexue.fm/archives/5861) 一致，只是解码端从双向换成单向了：

> [https://github.com/bojone/seq2seq/blob/master/seq2seq\_bidecoder.py](https://github.com/bojone/seq2seq/blob/master/seq2seq_bidecoder.py)

> **注：** 测试环境还是跟之前差不多，大概是Python 2.7 + Keras 2.2.4 + Tensorflow 1.8。用Python 3.x或者其他环境的朋友，如果你们能自己改，那就做相应的改动，如果你们自己不会改，那也请你们别来问我了，我实在没有空也没有义务帮你们跑通每一个环境。本文只讨论seq2seq技术相关的内容可否？

在这个实现里，我觉得有必要解释一下起始标记和结束标记的事情。在之前的单向解码的例子中，笔者是用2作为起始标记，用3作为结束标记。到了双向解码这里，一个很自然的问题就是：L2R和R2L两个方向是不是应该要用两套起始和结束标记呢？

其实这个应该没有什么标准答案，我觉得不管是共用一套还是维护两套起止标记，结果可能都差不多。至于我在上面的参考代码中，使用的方案有点另类，但我认为比较符合直觉，具体是：依然是只用一套，但是在L2R方向中，用2作为起始标记、3作为结束标记，而在R2L方向中，用3作为起始标记、2作为结束标记。

### 思考分析 [\#](https://kexue.fm/archives/6877\#%E6%80%9D%E8%80%83%E5%88%86%E6%9E%90)

最后，我们进一步思考一下这种双向解码方案。尽管将解码过程对称化是一个很漂亮的特点，但也不代表它完全没有问题了，将它思考得更深入一些，有助于我们更好地理解和使用它。

> 1\. 改进生成的原因

一个有意思的问题是：看上去双向解码确实能提高句子首尾的生成质量，但会不会同时降低中间部分的生成质量？

当然，理论上这是有可能的，但实际测试时不是很严重。一方面，seq2seq架构的信息编码和解码能力还是很强的，所以不会轻易损失信息；另一方面，我们自己去评估一个句子的质量的时候，往往会重点关注首尾部分，如果首尾部分都很合理，而中间部分不至于太糟糕的话，那么我们都认为它是一个合理的句子；反过来，如果首或尾不合理的话，我们会觉得这个句子很糟糕。这样一来，把句子首尾的生成质量提高了，整体的生成质量也就提高了。

[![原论文中双向解码相对其它单向模型带来的提升](https://kexue.fm/usr/uploads/2019/08/909218170.png)](https://kexue.fm/usr/uploads/2019/08/909218170.png "点击查看原图")

原论文中双向解码相对其它单向模型带来的提升

> 2\. 对应不上概率模型

对于单向解码，我们有清晰的概率解释，即在估计条件概率$p(Y\|X)$（也就是$\\eqref{eq:p}$）。但是在双向解码的时候，我们发现压根儿不知道怎么对应上一个概率模型，换句话说，我们感觉我们是在算概率，感觉效果也有了，却不知道真正算得是啥，因为条件概率的条件依赖完全已经被打乱了。

当然，如果真的有实效的话，理论美感差点也无妨，我说的这一点只是理论审美的追求，大家见仁见智就好。

> 3\. 信息提前泄漏

所谓信息泄漏，指的是本来作为预测目标的标签被用来做输入了，从而导致训练阶段的loss虚低（或者准确率虚高）。

由于在双向解码中，L2R端的解码要去读取R2L端已有的向量序列，而在训练阶段，为了预测R2L端的第$n$个字，是需要传入前$n-1$个字的，这样一来，越解码到后面，信息泄漏就越严重。如下图所示：

[![信息泄漏示意图。训练阶段，当L2R端在预测“你”的时候，事实上用到了传入到R2L端的“你”标签；反之，R2L端预测“北”字的时候，同样存在这个问题，即用到了L2R的“北”字标签](https://kexue.fm/usr/uploads/2019/08/443517401.png)](https://kexue.fm/usr/uploads/2019/08/443517401.png "点击查看原图")

信息泄漏示意图。训练阶段，当L2R端在预测“你”的时候，事实上用到了传入到R2L端的“你”标签；反之，R2L端预测“北”字的时候，同样存在这个问题，即用到了L2R的“北”字标签

信息泄漏的一个表观现象是：训练到后期，双向解码中L2R和R2L两个方向的交叉熵之和，比单独训练单向解码模型时的单个交叉熵还要小，这并不是因为双向解码带来多大的拟合提升，而正是信息泄漏的体现。

既然训练过程中把信息泄漏了，那为什么这样的模型还有用呢？我想，大概的原因在文章一开头的表格中就给出了。还是刚才的例子，L2R端在预测最后一个字“你”的时候，会用到了R2L端所有的已知信息；而R2L端是从右往左逐字解码的，按照文章一开头的表格的统计数据，我们不难想象到，对于R2L端来说，倒数第一个字的预测准确率应该是最高的。这样一来，假设R2L的倒数第一个字真的能以很高的准确率预测成功的话，那信息泄漏也变成不泄漏了———因为信息泄漏是因为我们人为地传入了标签，但如果预测的结果本身就跟标签一致，那泄漏也不再是泄漏了。

当然，原论文还提供了一个策略来缓解这个泄漏问题，大概做法是先用上述方式训练一版模型，然后对于每个训练样本，用模型生成对应的预测结果（伪标签），接着再去训练模型，这一次训练模型是传入伪标签来预测正确标签，这样就尽可能地保持了训练和预测的一致性。

## 文章小结 [\#](https://kexue.fm/archives/6877\#%E6%96%87%E7%AB%A0%E5%B0%8F%E7%BB%93)

本文介绍并实现了一种seq2seq的双向解码机制，它将整个解码过程对称化了，从而在一定程度上使得生成质量更高了。个人认为这种改进的尝试还是有一定的价值的，尤其是对于追求形式美的读者来说。所以就将其介绍一番。

除此之外，文章也分析了这种双向解码可能存在的问题，给出了笔者自己的看法。敬请各位读者多多交流直角～

_**转载到请包括本文地址：** [https://kexue.fm/archives/6877](https://kexue.fm/archives/6877 "seq2seq之双向解码")_

_**更详细的转载事宜请参考：**_ [《科学空间FAQ》](https://kexue.fm/archives/6508#%E6%96%87%E7%AB%A0%E5%A6%82%E4%BD%95%E8%BD%AC%E8%BD%BD/%E5%BC%95%E7%94%A8 "《科学空间FAQ》")

**如果您还有什么疑惑或建议，欢迎在下方评论区继续讨论。**

**如果您觉得本文还不错，欢迎 [分享](https://kexue.fm/archives/6877#share)/ [打赏](https://kexue.fm/archives/6877#pay) 本文。打赏并非要从中获得收益，而是希望知道科学空间获得了多少读者的真心关注。当然，如果你无视它，也不会影响你的阅读。再次表示欢迎和感谢！**

打赏

![科学空间](https://kexue.fm/usr/themes/geekg/payment/wx.png)

微信打赏

![科学空间](https://kexue.fm/usr/themes/geekg/payment/zfb.png)

支付宝打赏

因为网站后台对打赏并无记录，因此欢迎在打赏时候备注留言。

你还可以 [**点击这里**](http://mail.qq.com/cgi-bin/qm_share?t=qm_mailme&email=tN7d1drY3drrx8H0xcWa19vZ) 或在下方评论区留言来告知你的建议或需求。

**如果您需要引用本文，请参考：**

苏剑林. (Aug. 09, 2019). 《seq2seq之双向解码 》\[Blog post\]. Retrieved from [https://kexue.fm/archives/6877](https://kexue.fm/archives/6877)

@online{kexuefm-6877,

         title={seq2seq之双向解码},

         author={苏剑林},

         year={2019},

         month={Aug},

         url={\\url{https://kexue.fm/archives/6877}},

}


分类： [信息时代](https://kexue.fm/category/Big-Data)    标签： [模型](https://kexue.fm/tag/%E6%A8%A1%E5%9E%8B/), [NLP](https://kexue.fm/tag/NLP/), [keras](https://kexue.fm/tag/keras/), [文本生成](https://kexue.fm/tag/%E6%96%87%E6%9C%AC%E7%94%9F%E6%88%90/)[6 评论](https://kexue.fm/archives/6877#comments)

< [Keras实现两个优化器：Lookahead和LazyOptimizer](https://kexue.fm/archives/6869 "Keras实现两个优化器：Lookahead和LazyOptimizer") \| [开源一版DGCNN阅读理解问答模型（Keras版）](https://kexue.fm/archives/6906 "开源一版DGCNN阅读理解问答模型（Keras版）") >

### 你也许还对下面的内容感兴趣

- [MoE环游记：1、从几何意义出发](https://kexue.fm/archives/10699 "MoE环游记：1、从几何意义出发")
- [基于量子化假设推导模型的尺度定律（Scaling Law）](https://kexue.fm/archives/9607 "基于量子化假设推导模型的尺度定律（Scaling Law）")
- [《为什么现在的LLM都是Decoder-only的架构？》FAQ](https://kexue.fm/archives/9547 "《为什么现在的LLM都是Decoder-only的架构？》FAQ")
- [为什么现在的LLM都是Decoder-only的架构？](https://kexue.fm/archives/9529 "为什么现在的LLM都是Decoder-only的架构？")
- [Tiger：一个“抠”到极致的优化器](https://kexue.fm/archives/9512 "Tiger：一个“抠”到极致的优化器")
- [在bert4keras中使用混合精度和XLA加速训练](https://kexue.fm/archives/9059 "在bert4keras中使用混合精度和XLA加速训练")
- [为什么需要残差？一个来自DeepNet的视角](https://kexue.fm/archives/8994 "为什么需要残差？一个来自DeepNet的视角")
- [门控注意力单元（GAU）还需要Warmup吗？](https://kexue.fm/archives/8990 "门控注意力单元（GAU）还需要Warmup吗？")
- [GPLinker：基于GlobalPointer的事件联合抽取](https://kexue.fm/archives/8926 "GPLinker：基于GlobalPointer的事件联合抽取")
- [GPLinker：基于GlobalPointer的实体关系联合抽取](https://kexue.fm/archives/8888 "GPLinker：基于GlobalPointer的实体关系联合抽取")

[发表你的看法](https://kexue.fm/archives/6877#comment_form)

张亚超

August 12th, 2019

您好，请问一下，记得你之前有篇文章写的是正则化背后的数据增广，还是两个数据增广的什么...当时只是看了标题，没有细看，现在找不到了呢？

[回复评论](https://kexue.fm/archives/6877/comment-page-1?replyTo=11806#respond-post-6877)

张亚超 发表于
August 12th, 2019

找到了

[回复评论](https://kexue.fm/archives/6877/comment-page-1?replyTo=11807#respond-post-6877)

吴雄青

August 25th, 2019

苏神，请教一个问题。seq2seq有三个数据Input。encoder\_input是question, 但是感觉decoder\_input和decoder\_target都是answer，这两个在数据预处理要怎么处理并输入模型训练？

[回复评论](https://kexue.fm/archives/6877/comment-page-1?replyTo=11865#respond-post-6877)

[苏剑林](https://kexue.fm/) 发表于
August 25th, 2019

看

[https://kexue.fm/archives/5861](https://kexue.fm/archives/5861)

以及读源码。

[回复评论](https://kexue.fm/archives/6877/comment-page-1?replyTo=11866#respond-post-6877)

吴雄青

September 18th, 2019

跑了一遍，预测效果貌似没有单向的好，预测的标题很奇怪

[回复评论](https://kexue.fm/archives/6877/comment-page-1?replyTo=12005#respond-post-6877)

[苏剑林](https://kexue.fm/) 发表于
September 20th, 2019

这个是比较抱歉了，我刚开始试的时候看了几个案例还不错，在另外一个任务上也是如此。但是后来再看时发现会出现一些很奇怪的结果，所以看来这样直接训练是得不到论文所说的提升的，可能还是有必要去二次微调。

[回复评论](https://kexue.fm/archives/6877/comment-page-1?replyTo=12011#respond-post-6877)

[取消回复](https://kexue.fm/archives/6877#respond-post-6877)

你的大名

电子邮箱

个人网站（选填）

1\. 可以使用LaTeX代码，点击“预览效果”可查看效果；

2\. 可以通过点击评论楼层编号来引用该楼层；

3\. 网站可能会有点卡，如非确认评论失败，请 **不要重复点击提交**。

### 内容速览

[背景介绍](https://kexue.fm/archives/6877#%E8%83%8C%E6%99%AF%E4%BB%8B%E7%BB%8D)
[双向解码](https://kexue.fm/archives/6877#%E5%8F%8C%E5%90%91%E8%A7%A3%E7%A0%81)
[基本思路](https://kexue.fm/archives/6877#%E5%9F%BA%E6%9C%AC%E6%80%9D%E8%B7%AF)
[数学描述](https://kexue.fm/archives/6877#%E6%95%B0%E5%AD%A6%E6%8F%8F%E8%BF%B0)
[模型实现](https://kexue.fm/archives/6877#%E6%A8%A1%E5%9E%8B%E5%AE%9E%E7%8E%B0)
[训练方案](https://kexue.fm/archives/6877#%E8%AE%AD%E7%BB%83%E6%96%B9%E6%A1%88)
[双向束搜索](https://kexue.fm/archives/6877#%E5%8F%8C%E5%90%91%E6%9D%9F%E6%90%9C%E7%B4%A2)
[代码参考](https://kexue.fm/archives/6877#%E4%BB%A3%E7%A0%81%E5%8F%82%E8%80%83)
[思考分析](https://kexue.fm/archives/6877#%E6%80%9D%E8%80%83%E5%88%86%E6%9E%90)
[文章小结](https://kexue.fm/archives/6877#%E6%96%87%E7%AB%A0%E5%B0%8F%E7%BB%93)

### 智能搜索

支持整句搜索！网站自动使用 [结巴分词](https://github.com/fxsjy/jieba) 进行分词，并结合ngrams排序算法给出合理的搜索结果。

### 热门标签

[生成模型](https://kexue.fm/tag/%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B/) [attention](https://kexue.fm/tag/attention/) [优化](https://kexue.fm/tag/%E4%BC%98%E5%8C%96/) [语言模型](https://kexue.fm/tag/%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/) [模型](https://kexue.fm/tag/%E6%A8%A1%E5%9E%8B/) [梯度](https://kexue.fm/tag/%E6%A2%AF%E5%BA%A6/) [网站](https://kexue.fm/tag/%E7%BD%91%E7%AB%99/) [概率](https://kexue.fm/tag/%E6%A6%82%E7%8E%87/) [矩阵](https://kexue.fm/tag/%E7%9F%A9%E9%98%B5/) [优化器](https://kexue.fm/tag/%E4%BC%98%E5%8C%96%E5%99%A8/) [转载](https://kexue.fm/tag/%E8%BD%AC%E8%BD%BD/) [微分方程](https://kexue.fm/tag/%E5%BE%AE%E5%88%86%E6%96%B9%E7%A8%8B/) [分析](https://kexue.fm/tag/%E5%88%86%E6%9E%90/) [天象](https://kexue.fm/tag/%E5%A4%A9%E8%B1%A1/) [深度学习](https://kexue.fm/tag/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/) [积分](https://kexue.fm/tag/%E7%A7%AF%E5%88%86/) [python](https://kexue.fm/tag/python/) [扩散](https://kexue.fm/tag/%E6%89%A9%E6%95%A3/) [力学](https://kexue.fm/tag/%E5%8A%9B%E5%AD%A6/) [无监督](https://kexue.fm/tag/%E6%97%A0%E7%9B%91%E7%9D%A3/) [几何](https://kexue.fm/tag/%E5%87%A0%E4%BD%95/) [节日](https://kexue.fm/tag/%E8%8A%82%E6%97%A5/) [生活](https://kexue.fm/tag/%E7%94%9F%E6%B4%BB/) [文本生成](https://kexue.fm/tag/%E6%96%87%E6%9C%AC%E7%94%9F%E6%88%90/) [数论](https://kexue.fm/tag/%E6%95%B0%E8%AE%BA/)

### 随机文章

- [齐次多项式不等式的机器证明（差分代换）](https://kexue.fm/archives/2747)
- [天体力学巨匠——拉普拉斯](https://kexue.fm/archives/1779)
- [如何训练你的准确率？](https://kexue.fm/archives/9098)
- [通过互信息思想来缓解类别不平衡问题](https://kexue.fm/archives/7615)
- [斯特灵(stirling)公式与渐近级数](https://kexue.fm/archives/3731)
- [无穷级数求和的积分审敛法](https://kexue.fm/archives/68)
- [从费马大定理谈起（十）：x^3+y^3=z^3+w^3](https://kexue.fm/archives/2972)
- [再来一顿贺岁宴：从K-Means到Capsule](https://kexue.fm/archives/5112)
- [科学空间：2010年10月重要天象](https://kexue.fm/archives/951)
- [祝大家七夕快乐！](https://kexue.fm/archives/101)

### 最近评论

- [Bin](https://kexue.fm/archives/1990/comment-page-2#comment-29105): 今天偶然从某个论坛看到有人推荐您的博客，定睛一看竟然是华师同院的往届师兄！看到这篇2013年的...
- [Rapture D](https://kexue.fm/archives/11530/comment-page-1#comment-29104): 我有一个问题，为什么不考虑亥姆霍兹定理和斯托克斯公式。
- [mofheka](https://kexue.fm/archives/11390/comment-page-1#comment-29103): 苏神是还在用jax是么？最近在做基于Google Pathway的理念做一个动态版的MPMD框...
- [长琴](https://kexue.fm/archives/11530/comment-page-1#comment-29102): 看懂这篇博客也不是一件容易的事情。
- [AlexLi](https://kexue.fm/archives/9257/comment-page-4#comment-29101): 苏老师，请教一下(7)式中将 $\\mu(x\_t)$ 传给 $p\_o$ 进行推理的操作。 $x\_...
- [tyler\_zxc](https://kexue.fm/archives/7921/comment-page-2#comment-29100): "Performer的思想是将标准的Attention线性化，所以为什么不干脆直接训练一个线性...
- [我](https://kexue.fm/archives/11494/comment-page-1#comment-29099): 似乎并非mHC提出矩阵的思想？之前hyper connection就是了
- [winter](https://kexue.fm/archives/10847/comment-page-1#comment-29098): 苏神您好，假如对于比较均匀的attention weightP，往往呈现long tail分布...
- [苏剑林](https://kexue.fm/archives/8512/comment-page-2#comment-29097): KL散度、JS散度、W距离啥的，都行啊，看你喜欢哪个
- [苏剑林](https://kexue.fm/archives/9119/comment-page-14#comment-29096): 没有绝对公平的对比方法，主要看你关心什么。比如，如果只关心推理成本和推理效果，那么有的方法可以...

### 友情链接

- [Cool Papers](https://papers.cool/)
- [数学研发](https://bbs.emath.ac.cn/)
- [Seatop](http://www.seatop.com.cn/)
- [Xiaoxia](https://xiaoxia.org/)
- [积分表-网络版](https://kexue.fm/sci/integral/index.html)
- [丝路博傲](http://blog.dvxj.com/)
- [数学之家](http://www.2math.cn/)
- [有趣天文奇观](http://interesting-sky.china-vo.org/)
- [TwistedW](http://www.twistedwg.com/)
- [godweiyang](https://godweiyang.com/)
- [AI柠檬](https://blog.ailemon.net/)
- [王登科-DK博客](https://greatdk.com/)
- [ESON](https://blog.eson.org/)
- [枫之羽](https://fzhiy.net/)
- [coding-zuo](https://coding-zuo.github.io/)
- [博科园](https://www.bokeyuan.net/)
- [孔皮皮的博客](https://www.kppkkp.top/)
- [运鹏的博客](https://yunpengtai.top/)
- [jiming.site](https://jiming.site/)
- [OmegaXYZ](https://www.omegaxyz.com/)
- [EAI猩球](https://www.robotech.ink/)
- [文举的博客](https://liwenju0.com/)
- [申请链接](https://kexue.fm/links.html)

[![署名-非商业用途-保持一致](https://kexue.fm/usr/themes/geekg/images/cc.gif)](http://creativecommons.org/licenses/by-nc-nd/2.5/cn/) 本站采用创作共用版权协议，要求署名、非商业用途和保持一致。转载本站内容必须也遵循“ [署名-非商业用途-保持一致](http://creativecommons.org/licenses/by-nc-nd/2.5/cn/)”的创作共用协议。



© 2009-2026 Scientific Spaces. All rights reserved. Theme by [laogui](http://www.laogui.com/). Powered by [Typecho](http://typecho.org/). 备案号: [粤ICP备09093259号-1/2](https://beian.miit.gov.cn/ "粤ICP备09093259号")。