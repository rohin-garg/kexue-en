## SEARCH

## MENU

- [打赏](https://kexue.fm/reward.html)
- [公式](https://kexue.fm/latex.html)
- [天象](https://kexue.fm/ac.html)
- [链接](https://kexue.fm/links.html)
- [时光](https://kexue.fm/me.html)
- [博览](https://kexue.fm/science.html)
- [归档](https://kexue.fm/content.html)

## CATEGORIES

- [千奇百怪](https://kexue.fm/category/Everything)
- [天文探索](https://kexue.fm/category/Astronomy)
- [数学研究](https://kexue.fm/category/Mathematics)
- [物理化学](https://kexue.fm/category/Phy-chem)
- [信息时代](https://kexue.fm/category/Big-Data)
- [生物自然](https://kexue.fm/category/Biology)
- [图片摄影](https://kexue.fm/category/Photograph)
- [问题百科](https://kexue.fm/category/Questions)
- [生活/情感](https://kexue.fm/category/Life-Feeling)
- [资源共享](https://kexue.fm/category/Resources)

## NEWPOSTS

- [“对角+低秩”三角阵的高效求逆方法](https://kexue.fm/archives/11072)
- [通过msign来计算奇异值裁剪mc...](https://kexue.fm/archives/11059)
- [矩阵符号函数mcsgn能计算什么？](https://kexue.fm/archives/11056)
- [线性注意力简史：从模仿、创新到反哺](https://kexue.fm/archives/11033)
- [msign的导数](https://kexue.fm/archives/11025)
- [通过msign来计算奇异值裁剪mc...](https://kexue.fm/archives/11006)
- [msign算子的Newton-Sc...](https://kexue.fm/archives/10996)
- [等值振荡定理：最优多项式逼近的充要条件](https://kexue.fm/archives/10972)
- [生成扩散模型漫谈（三十）：从瞬时速...](https://kexue.fm/archives/10958)
- [MoE环游记：5、均匀分布的反思](https://kexue.fm/archives/10945)

## COMMENTS

- [Truenobility303: 谢谢苏神的详细解答！](https://kexue.fm/archives/10739/comment-page-2#comment-28059)
- [Truenobility303: 不好意思我的表述可能会误导性说错了，核心问题不在2\. 我觉得问...](https://kexue.fm/archives/10795/comment-page-1#comment-28058)
- [kw: 把所有M直接换成全1矩阵就行吧，比如DeltaNet变成$(Q...](https://kexue.fm/archives/11033/comment-page-1#comment-28057)
- [WB: 非常清楚的blog。我有一个小问题想问一下，推导的时候用的是不...](https://kexue.fm/archives/10795/comment-page-1#comment-28056)
- [liangzhh: 谢谢大佬的分享，感觉中间有两个手误敲错，式(9)最后应该是加号...](https://kexue.fm/archives/11072/comment-page-1#comment-28055)
- [lidhrandom: Equation 3的等号右侧第二项的第一个${\\Lambda...](https://kexue.fm/archives/11072/comment-page-1#comment-28054)
- [Kuo: 在 $PaTH$ 论文章节 \`UT Transform for...](https://kexue.fm/archives/11033/comment-page-1#comment-28053)
- [Fanhao: 假定Hessian阵正定，那不是意味着$L(\\theta)$是...](https://kexue.fm/archives/10542/comment-page-1#comment-28052)
- [曲笑一: 对于第一个疑问，我看到分布式的版本已经开源。我在想如果将每个梯...](https://kexue.fm/archives/10739/comment-page-2#comment-28051)
- [曲笑一: 苏老师您好，阅读了您关于Muon系列的博客，受益匪浅。在此有两...](https://kexue.fm/archives/10739/comment-page-2#comment-28050)

## USERLOGIN

- [登录](https://kexue.fm/admin/login.php)

[科学空间\|Scientific Spaces](https://kexue.fm)

- [登录](https://kexue.fm/admin/login.php)
- [打赏](https://kexue.fm/reward.html)
- [公式](https://kexue.fm/latex.html)
- [天象](https://kexue.fm/ac.html)
- [链接](https://kexue.fm/links.html)
- [时光](https://kexue.fm/me.html)
- [博览](https://kexue.fm/science.html)
- [归档](https://kexue.fm/content.html)

渴望成为一个小飞侠

- [欢迎订阅](https://kexue.fm/feed)
- [个性邮箱](https://kexue.fm/archives/119)
- [天象信息](https://kexue.fm/ac.html)
- [观测ISS](https://kexue.fm/archives/41)
- [LaTeX](https://kexue.fm/latex.html)
- [关于博主](https://kexue.fm/me.html)

欢迎访问“科学空间”，这里将与您共同探讨自然科学，回味人生百态；也期待大家的分享～

- [**千奇百怪** Everything](https://kexue.fm/category/Everything)
- [**天文探索** Astronomy](https://kexue.fm/category/Astronomy)
- [**数学研究** Mathematics](https://kexue.fm/category/Mathematics)
- [**物理化学** Phy-chem](https://kexue.fm/category/Phy-chem)
- [**信息时代** Big-Data](https://kexue.fm/category/Big-Data)
- [**生物自然** Biology](https://kexue.fm/category/Biology)
- [**图片摄影** Photograph](https://kexue.fm/category/Photograph)
- [**问题百科** Questions](https://kexue.fm/category/Questions)
- [**生活/情感** Life-Feeling](https://kexue.fm/category/Life-Feeling)
- [**资源共享** Resources](https://kexue.fm/category/Resources)

- [**千奇百怪**](https://kexue.fm/category/Everything)
- [**天文探索**](https://kexue.fm/category/Astronomy)
- [**数学研究**](https://kexue.fm/category/Mathematics)
- [**物理化学**](https://kexue.fm/category/Phy-chem)
- [**信息时代**](https://kexue.fm/category/Big-Data)
- [**生物自然**](https://kexue.fm/category/Biology)
- [**图片摄影**](https://kexue.fm/category/Photograph)
- [**问题百科**](https://kexue.fm/category/Questions)
- [**生活/情感**](https://kexue.fm/category/Life-Feeling)
- [**资源共享**](https://kexue.fm/category/Resources)

[首页](https://kexue.fm) [信息时代](https://kexue.fm/category/Big-Data) Transformer升级之路：18、RoPE的底数选择原则

29May

# [Transformer升级之路：18、RoPE的底数选择原则](https://kexue.fm/archives/10122)

By 苏剑林 \|
2024-05-29 \|
165680位读者\|

我们知道，在 [RoPE](https://kexue.fm/archives/8265) 中频率的计算公式为$\\theta\_i = b^{-2i/d}$，底数$b$默认值为10000。目前Long Context的主流做法之一是，先在$b=10000$上用短文本预训练，然后调大$b$并在长文本微调，其出发点是 [《Transformer升级之路：10、RoPE是一种β进制编码》](https://kexue.fm/archives/9675) 里介绍的NTK-RoPE，它本身有较好长度外推性，换用更大的$b$再微调相比不加改动的微调，起始损失更小，收敛也更快。该过程给人的感觉是：调大$b$完全是因为“先短后长”的训练策略，如果一直都用长文本训练似乎就没必要调大$b$了？

上周的论文 [《Base of RoPE Bounds Context Length》](https://papers.cool/arxiv/2405.14591) 试图回答这个问题，它基于一个期望性质研究了$b$的下界，由此指出更大的训练长度本身就应该选择更大的底数，与训练策略无关。整个分析思路颇有启发性，接下来我们一起来品鉴一番。

## 期望性质 [\#](https://kexue.fm/archives/10122\#%E6%9C%9F%E6%9C%9B%E6%80%A7%E8%B4%A8)

RoPE这里就不再详细介绍了，它本质上是一个分块对角矩阵
\\begin{equation}\\boldsymbol{\\mathcal{R}}\_n = \\scriptsize{\\left(\\begin{array}{cc:cc:cc:cc}
\\cos n\\theta\_0 & -\\sin n\\theta\_0 & 0 & 0 & \\cdots & \\cdots & 0 & 0 \\\
\\sin n\\theta\_0 & \\cos n\\theta\_0 & 0 & 0 & \\cdots & \\cdots & 0 & 0 \\\
\\hdashline
0 & 0 & \\cos n\\theta\_1 & -\\sin n\\theta\_1 & \\cdots & \\cdots & 0 & 0 \\\
0 & 0 & \\sin n\\theta\_1 & \\cos n\\theta\_1 & \\cdots & \\cdots & 0 & 0 \\\
\\hdashline
\\vdots & \\vdots & \\vdots & \\vdots & \\ddots & \\ddots & \\vdots & \\vdots \\\
\\vdots & \\vdots & \\vdots & \\vdots & \\ddots & \\ddots & \\vdots & \\vdots \\\
\\hdashline
0 & 0 & 0 & 0 & \\cdots & \\cdots & \\cos n\\theta\_{d/2-1} & -\\sin n\\theta\_{d/2-1} \\\
0 & 0 & 0 & 0 & \\cdots & \\cdots & \\sin n\\theta\_{d/2-1} & \\cos n\\theta\_{d/2-1} \\\
\\end{array}\\right)}\\end{equation}
然后利用恒等式
\\begin{equation}(\\boldsymbol{\\mathcal{R}}\_m \\boldsymbol{q})^{\\top}(\\boldsymbol{\\mathcal{R}}\_n \\boldsymbol{k}) = \\boldsymbol{q}^{\\top} \\boldsymbol{\\mathcal{R}}\_m^{\\top}\\boldsymbol{\\mathcal{R}}\_n \\boldsymbol{k} = \\boldsymbol{q}^{\\top} \\boldsymbol{\\mathcal{R}}\_{n-m} \\boldsymbol{k}\\end{equation}
给$\\boldsymbol{q},\\boldsymbol{k}$注入绝对位置信息，并自动实现了相对位置的效果。其中$\\theta\_i = b^{-2i/d}$，这里的$b$的取值就是本文要探讨的问题。

除了给模型注入位置信息外，我们期望RoPE能具备两个理想性质，以达到更好的效果：1、 **远程衰减**，即位置相近的Token平均来说获得更多的注意力；2、 **语义聚合**，即语义相似的Token平均来说获得更多的注意力。其中第一点我们早在 [《Transformer升级之路：2、博采众长的旋转式位置编码》](https://kexue.fm/archives/8265) 有过相关讨论，RoPE确实有一定的远程衰减性质。

所以接下来我们来分析第二点。

## 不等关系 [\#](https://kexue.fm/archives/10122\#%E4%B8%8D%E7%AD%89%E5%85%B3%E7%B3%BB)

所谓语义聚合，指的是当$\\boldsymbol{k}$与$\\boldsymbol{q}$相近时，不管它们的相对距离$n-m$多大，其注意力$\\boldsymbol{q}^{\\top} \\boldsymbol{\\mathcal{R}}\_{n-m} \\boldsymbol{k}$平均来说都应该更大（至少要比随机的两个Token更大）。为了得到一个量化的结论，我们进一步简化问题，假设$\\boldsymbol{q}$的每个分量都是独立同分布的，每个分量的均值为$\\mu$，方差为$\\sigma^2$。

现在我们考虑两种不同的$\\boldsymbol{k}$：一种是在$\\boldsymbol{q}$的基础上，加上一个零均值的扰动$\\boldsymbol{\\varepsilon}$，我们记$\\tilde{\\boldsymbol{k}} = \\boldsymbol{q} + \\boldsymbol{\\varepsilon}$，代表跟$\\boldsymbol{q}$语义相近的Token；另一种则是假设$\\boldsymbol{k}$跟$\\boldsymbol{q}$独立同分布，这代表两个随机的Token。根据第二点理想性质，我们希望有
\\begin{equation}\\mathbb{E}\_{\\boldsymbol{q},\\boldsymbol{k},\\boldsymbol{\\varepsilon}}\\big\[\\boldsymbol{q}^{\\top} \\boldsymbol{\\mathcal{R}}\_{n-m} \\tilde{\\boldsymbol{k}} - \\boldsymbol{q}^{\\top} \\boldsymbol{\\mathcal{R}}\_{n-m} \\boldsymbol{k}\\big\] \\geq 0\\end{equation}
注意我们刚才反复强调了“平均来说”，意味着我们只是期望一个平均的趋势，而不是每一点都能严格成立，所以我们在上式加了取数学期望$\\mathbb{E}\_{\\boldsymbol{q},\\boldsymbol{k},\\boldsymbol{\\varepsilon}}$。现在根据假设和RoPE的定义，我们可以把上式具体地算出来：
\\begin{equation}\\begin{aligned}
&\\,\\mathbb{E}\_{\\boldsymbol{q},\\boldsymbol{k},\\boldsymbol{\\varepsilon}}\\big\[\\boldsymbol{q}^{\\top} \\boldsymbol{\\mathcal{R}}\_{n-m} (\\boldsymbol{q} + \\boldsymbol{\\varepsilon}) - \\boldsymbol{q}^{\\top} \\boldsymbol{\\mathcal{R}}\_{n-m} \\boldsymbol{k}\\big\] \\\\[5pt\]
=&\\, \\mathbb{E}\_{\\boldsymbol{q}}\\big\[\\boldsymbol{q}^{\\top} \\boldsymbol{\\mathcal{R}}\_{n-m} \\boldsymbol{q}\\big\] - \\mathbb{E}\_{\\boldsymbol{q},\\boldsymbol{k}}\\big\[\\boldsymbol{q}^{\\top} \\boldsymbol{\\mathcal{R}}\_{n-m} \\boldsymbol{k}\\big\] \\\\[5pt\]
=&\\, \\mathbb{E}\_{\\boldsymbol{q}}\\big\[\\boldsymbol{q}^{\\top} \\boldsymbol{\\mathcal{R}}\_{n-m} \\boldsymbol{q}\\big\] - \\mathbb{E}\_{\\boldsymbol{q}}\[\\boldsymbol{q}\]^{\\top}\\boldsymbol{\\mathcal{R}}\_{n-m} \\mathbb{E}\_{\\boldsymbol{k}}\[\\boldsymbol{k}\] \\\\[5pt\]
=&\\, \\mathbb{E}\_{\\boldsymbol{q}}\\big\[\\boldsymbol{q}^{\\top} \\boldsymbol{\\mathcal{R}}\_{n-m} \\boldsymbol{q}\\big\] - \\mu^2\\boldsymbol{1}^{\\top}\\boldsymbol{\\mathcal{R}}\_{n-m} \\boldsymbol{1} \\\\[5pt\]
=& \\mathbb{E}\_{\\boldsymbol{q}}\\left\[\\sum\_{i=0}^{d/2-1} (q\_{2i}^2 + q\_{2i+1}^2)\\cos (n-m)\\theta\_i\\right\] - \\sum\_{i=0}^{d/2-1} 2\\mu^2\\cos (n-m)\\theta\_i \\\\[5pt\]
=& \\sum\_{i=0}^{d/2-1} 2(\\mu^2 + \\sigma^2)\\cos (n-m)\\theta\_i - \\sum\_{i=0}^{d/2-1} 2\\mu^2\\cos (n-m)\\theta\_i \\\\[5pt\]
=& \\sum\_{i=0}^{d/2-1} 2\\sigma^2\\cos (n-m)\\theta\_i \\\
\\end{aligned}\\end{equation}
如果训练长度最大为$L$，那么$n-m\\leq L-1$，因此第二点理想性质可以用如下不等式近似描述：
\\begin{equation}\\sum\_{i=0}^{d/2-1} \\cos m\\theta\_i \\geq 0,\\quad m\\in\\{0,1,2,\\cdots,L-1\\}\\label{neq:base}\\end{equation}
其中$L$是最大长度，是训练前就要选定的超参，而$d$是模型的head\_size，按照LLAMA的一般设置是$d=128$，这也就意味着，上式的唯一可调参数就是$\\theta\_i = b^{-2i/d}$中的$b$。在 [《Transformer升级之路：1、Sinusoidal位置编码追根溯源》](https://kexue.fm/archives/8231) 中我们就简单探究过这个函数，它整体趋势是衰减的，$b$越大则衰减速度越慢，对应的连续非负区间就越大，所以存在一个最小的$b$使得上述不等式恒成立，即
\\begin{equation}b^\* = \\inf\\left\\{\\,\\,b\\,\\,\\,\\left\|\\,\\,\\,f\_b(m)\\triangleq\\sum\_{i=0}^{d/2-1} \\cos m b^{-2i/d} \\geq 0,\\,\\, m\\in\\{0,1,2,\\cdots,L-1\\}\\right.\\right\\}\\end{equation}

## 数值求解 [\#](https://kexue.fm/archives/10122\#%E6%95%B0%E5%80%BC%E6%B1%82%E8%A7%A3)

由于$f\_b(m)$涉及到多个三角函数的求和，并且$\\theta\_i$关于$i$还是非线性的，很难想象上述问题会有解析解，因此只能诉诸数值求解了。然而，$f\_b(m)$越到后面震荡越频繁且不规律，因此即便数值求解也不是那么简单的事情。

笔者一开始以为，如果$b\_0$使得$f\_{b\_0}(m)\\geq 0$恒成立，那么$\\forall b \\geq b\_0$都恒成立$f\_b(m)\\geq 0$，所以用二分法就可以了。但事实上这个假设并不成立，所以二分法宣告破产。继续想了一段时间，依然没什么优化思路，期间向原论文作者请教过，他们采用的是逆函数法，即给定$b$求使得$f\_b(m)\\geq 0$恒成立的最大$L$是比较简单的，于是我们可以得到很多$(b, L)$对，理论上只要枚举的$b$足够多，那么对于任意$L$都可以找出最小的$b$。然而这里有个精度问题，原论文最大的$L$计算到了$10^6$，$b$至少要枚举到$10^8$，如果枚举间隔小，那么计算成本非常大，如果枚举间隔大，那么可能漏掉很多解。

最后，笔者决定还是用“Jax + GPU”进行暴力搜索，以求得到更高精度的结果，大致流程是：

> 1、初始化$b=1000L$（在$10^6$内$b=1000L$可以使得$f\_b(m)\\geq 0$恒成立）；
>
> 2、遍历$k=1,2,3,4,5$，执行以下操作：
>
> 2.1）将$\[0,b\]$等分为$10^k$份，遍历等分点，判断$f\_b(m)\\geq 0$是否恒成立；
>
> 2.2）取最小的使得$f\_b(m)\\geq 0$恒成立的等分点，更新$b$；
>
> 3、返回最终的$b$。

最终结果普遍要比原论文的更紧一些
$$\\scriptsize\\begin{array}{c\|cccccccccc}
\\hline
L & 1k & 2k & 4k & 8k & 16k & 32k & 64k & 128k & 256k & 512k & 1M \\\
\\hline
b^\*(\\text{原文}) & 4.3e3 & 1.6e4 & 2.7e4 & 8.4e4 & 3.1e5 & 6.4e5 & 2.1e6 & 7.8e6 & 3.6e7 & 6.4e7 & 5.1e8 \\\
b^\*(\\text{本文}) & 4.3e3 & \\color{red}{1.2e4} & 2.7e4 & 8.4e4 & \\color{red}{2.3e5} & \\color{red}{6.3e5} & 2.1e6 & \\color{red}{4.9e6} & \\color{red}{2.4e7} & \\color{red}{5.8e7} & \\color{red}{6.5e7} \\\
\\hline
\\end{array}$$

参考代码：

```
from functools import partial
import numpy as np
import jax.numpy as jnp
import jax

@partial(jax.jit, static_argnums=(2,))
def f(m, b, d=128):
 i = jnp.arange(d / 2)
 return jnp.cos(m[:, None] * b ** (-2 * i[None] / d)).sum(axis=1)

@np.vectorize
def fmin(L, b):
 return f(np.arange(L), b).min()

def bmin(L):
 B = 1000 * L
 for k in range(1, 6):
 bs = np.linspace(0, 1, 10**k + 1)[1:] * B
 ys = fmin(L, bs)
 for b, y in zip(bs, ys):
 if y >= 0:
 B = b
 break
 return B

bmin(1024 * 128)
```

## 渐近估计 [\#](https://kexue.fm/archives/10122\#%E6%B8%90%E8%BF%91%E4%BC%B0%E8%AE%A1)

除了数值求解外，我们也可以通过渐近分析来得到一个解析的估计结果，这个估计比数值结果要小，本质上是$d\\to\\infty$的解，但同样能够得出“$b$应该随着$L$增大而增大”的结论。

渐近估计的思路，是用积分代替求和：
\\begin{equation}f\_b(m) = \\sum\_{i=0}^{d/2-1} \\cos m b^{-2i/d}\\approx \\int\_0^1 \\cos m b^{-s} ds \\xlongequal{\\text{令}t = mb^{-s}} \\int\_{mb^{-1}}^m \\frac{\\cos t}{t \\ln b}dt\\end{equation}
其中我们记
\\begin{equation}\\text{Ci}(x) = -\\int\_x^{\\infty} \\frac{\\cos t}{t} dt\\end{equation}
这是被前人研究过的三角积分（参考 [Trigonometric integral](https://en.wikipedia.org/wiki/Trigonometric_integral) ），利用这个记号，我们可以写出
\\begin{equation}f\_b(m) \\approx \\frac{\\text{Ci}(m) - \\text{Ci}(mb^{-1})}{\\ln b}\\end{equation}
$\\text{Ci}(x)$的图像长这样：

Ci(x)的图像【来自维基百科】

它的第一个零点是$x\_0=0.6165\\cdots$，对于$m\\geq 1$，可以看出$\|\\text{Ci}(m)\|\\leq 1/2$，所以其实$\\text{Ci}(m)$相对来说是小项，对于渐近估计来说可以忽略，那么问题近似地变成了$\\text{Ci}(mb^{-1})\\leq 0$对于$m=1,2,\\cdots,L$恒成立，我们只需要让相应的$mb^{-1}$都落在$\[0,x\_0\]$区间内就可以实现，这意味着$Lb^{-1}\\leq x\_0$，即
\\begin{equation}b \\geq L / x\_0 \\approx 2L\\end{equation}
或者简单点$b^\* = \\mathcal{O}(L)$。不出意料这个结果比精确的数值结果要小，因为它对应于$d\\to\\infty$，无限个三角函数叠加会使得函数图像的震荡更少，看起来更加平稳（相比于有限的$d$），从而对于固定的$b$，$f\_b(m)$的连续非负区间更长，或者反过来，对于固定的$L$，保持$m=0,1,2,\\cdots,L-1$的$f\_b(m)$都非负的$b$更小。

## 相关思考 [\#](https://kexue.fm/archives/10122\#%E7%9B%B8%E5%85%B3%E6%80%9D%E8%80%83)

在 [《Transformer升级之路：10、RoPE是一种β进制编码》](https://kexue.fm/archives/9675) 中，我们将RoPE类比为一种$\\beta$进制表示，其中$\\beta = b^{2/d}$，那么$b - 1= \\beta^{d/2} - 1$正好是$d/2$位$\\beta$进制编码能够表示的最大数字，于是要表示$0,1,2,\\cdots,L-1$这$L$个位置编码，至少有$b \\geq L$，这个朴素的类比再次给出了“$b$应该随着$L$增大而增大”的结论，其结果跟上一节的渐近分析结果更为接近。

另一方面，Meta最新发布的LLAMA3，训练长度为8192，但RoPE的底数选择了惊人的500000（5e5），这比前面的数值结果（8.4e4）还要大将近一个数量级，不管从哪个角度看，这个数值笔者都认为是偏大的，可能LLAMA3的这个底数本就是给更大文本长度预留的。但不论如何，更大的文本长度选择更大的RoPE底数，似乎已经成为了很多训练人员的共识。

其实不管是数值结果还是渐近估计，都只是一个参考值，实际上对于给定的$L$，一个相当大范围内的$b$都应该会有相近的效果。所以具体的数值都不重要，关键是原论文通过语义聚合的出发点和一系列推导，澄清了“$b$应该随着$L$增大而增大”的结论及其原理，这是笔者所认为的原论文的核心贡献。

此外，其实语义聚合的出发点和结论也可以用来解释 [Position Interpolation](https://papers.cool/arxiv/2306.15595)（PI）。刚才我们说了，同一个$b$，$f\_b(m)$的连续非负区间是固定的，如果要使$0,1,2,\\cdots,L-1$都落在非负区间内，就需要随着$L$的增大而相应的增加$b$。但反过来，我们也可以不增加$b$，而是减少相邻位置的间隔（即位置ID改成$0,1/k,2/k,\\cdots$），那么就可以在同样大小的非负区间内表示$k$倍的位置了，这便是语义聚合视角下的Position Interpolation。

## 部分旋转 [\#](https://kexue.fm/archives/10122\#%E9%83%A8%E5%88%86%E6%97%8B%E8%BD%AC)

RoPE提出于2021年，当时只有一篇中文博客，后来得到了EleutherAI组织的认可和实验，继而才逐渐向学术界推广。当时EleutherAI实验发现，如果只对部分维度加RoPE，会取得稍优的结果，相关内容可以参考 [这里](https://github.com/lucidrains/x-transformers/issues/40)、 [这里](https://wandb.ai/eleutherai/neox/reports/Partial-Rotary-Tests--Vmlldzo2MjE1MjY) 和 [这里](https://wandb.ai/eleutherai/neox/reports/Partial-Rotary-Tests-v2--Vmlldzo2MjE4MTQ)，后来这个操作用到了它们的 [GPT-NeoX](https://github.com/EleutherAI/gpt-neox/blob/8b43196fbd832b797be9f3d88d54481171010507/megatron/model/transformer.py#L908) 中。

当然，部分旋转还不是当前LLM的主流选择，但这不妨碍我们研究它，也许它未成为主流选择只是因为我们对它还不够了解。那为什么部分旋转反而可能会更优呢？笔者发现可以用本文的结论来一定程度上解释它。以只旋转一半维度为例，它在数学上等价于选择如下的$\\theta\_i$：
\\begin{equation}\\theta\_i = \\left\\{\\begin{aligned}&b^{-4i/d},& i < d/4 \\\
&0,&i \\geq d/4\\end{aligned}\\right.\\end{equation}
此时我们有
\\begin{equation}\\sum\_{i=0}^{d/2-1} \\cos m\\theta\_i = \\sum\_{i=0}^{d/4-1} (1+\\cos mb^{-4i/d})\\geq 0\\end{equation}
也就是不论$m,b$如何，我们所期望的不等式$\\eqref{neq:base}$都自动成立，这意味着从本文的观点来看，部分旋转在赋予位置信息的同时有更好的语义聚合能力，这对模型的效果也许更加有利。同时，部分旋转对模型的长文本能力或许也更有利，因为不等式恒成立，所以按照本文的观点，不论长短文本训练都不用修改$b$。

值得一提的是，DeepSeek提出的 [MLA](https://kexue.fm/archives/10091) 也应用了部分旋转，虽然在MLA的原始推导中，部分旋转更多是为了整合RoPE的无奈之举，但结合以往的部分旋转实验结果来看，也许MLA的优异效果有部分旋转的一分功劳。

## 文章小结 [\#](https://kexue.fm/archives/10122\#%E6%96%87%E7%AB%A0%E5%B0%8F%E7%BB%93)

本文简单介绍了论文 [《Base of RoPE Bounds Context Length》](https://papers.cool/arxiv/2405.14591)，它从语义聚合的期望性质讨论了RoPE的底数下界，由此指出更大的训练长度应该选择更大的底数，而不单单是为了配合“先短后长”的训练策略、继而利用NTK-RoPE来降低初始损失的折中选择。

_**转载到请包括本文地址：** [https://kexue.fm/archives/10122](https://kexue.fm/archives/10122)_

_**更详细的转载事宜请参考：**_ [《科学空间FAQ》](https://kexue.fm/archives/6508#%E6%96%87%E7%AB%A0%E5%A6%82%E4%BD%95%E8%BD%AC%E8%BD%BD/%E5%BC%95%E7%94%A8)

**如果您还有什么疑惑或建议，欢迎在下方评论区继续讨论。**

**如果您觉得本文还不错，欢迎 [分享](https://kexue.fm/archives/10122#share)/ [打赏](https://kexue.fm/archives/10122#pay) 本文。打赏并非要从中获得收益，而是希望知道科学空间获得了多少读者的真心关注。当然，如果你无视它，也不会影响你的阅读。再次表示欢迎和感谢！**

打赏

微信打赏

支付宝打赏

因为网站后台对打赏并无记录，因此欢迎在打赏时候备注留言。你还可以 [**点击这里**](http://mail.qq.com/cgi-bin/qm_share?t=qm_mailme&email=tN7d1drY3drrx8H0xcWa19vZ) 或在下方评论区留言来告知你的建议或需求。

**如果您需要引用本文，请参考：**

苏剑林. (May. 29, 2024). 《Transformer升级之路：18、RoPE的底数选择原则 》\[Blog post\]. Retrieved from [https://kexue.fm/archives/10122](https://kexue.fm/archives/10122)

@online{kexuefm-10122,
        title={Transformer升级之路：18、RoPE的底数选择原则},
        author={苏剑林},
        year={2024},
        month={May},
        url={\\url{https://kexue.fm/archives/10122}},
}

分类： [信息时代](https://kexue.fm/category/Big-Data)    标签： [不等式](https://kexue.fm/tag/%E4%B8%8D%E7%AD%89%E5%BC%8F/), [attention](https://kexue.fm/tag/attention/), [位置编码](https://kexue.fm/tag/%E4%BD%8D%E7%BD%AE%E7%BC%96%E7%A0%81/), [rope](https://kexue.fm/tag/rope/)[18 评论](https://kexue.fm/archives/10122#comments)

< [重温SSM（一）：线性系统和HiPPO矩阵](https://kexue.fm/archives/10114) \| [重温SSM（二）：HiPPO的一些遗留问题](https://kexue.fm/archives/10137) >

### 你也许还对下面的内容感兴趣

- [“对角+低秩”三角阵的高效求逆方法](https://kexue.fm/archives/11072)
- [线性注意力简史：从模仿、创新到反哺](https://kexue.fm/archives/11033)
- [Transformer升级之路：20、MLA好在哪里?（上）](https://kexue.fm/archives/10907)
- [一道概率不等式：盯着它到显然成立为止！](https://kexue.fm/archives/10902)
- [Transformer升级之路：19、第二类旋转位置编码](https://kexue.fm/archives/10862)
- [细水长flow之TARFLOW：流模型满血归来？](https://kexue.fm/archives/10667)
- [“闭门造车”之多模态思路浅谈（三）：位置编码](https://kexue.fm/archives/10352)
- [Decoder-only的LLM为什么需要位置编码？](https://kexue.fm/archives/10347)
- [缓存与效果的极限拉扯：从MHA、MQA、GQA到MLA](https://kexue.fm/archives/10091)
- [Transformer升级之路：17、多模态位置编码的简单思考](https://kexue.fm/archives/10040)

[发表你的看法](https://kexue.fm/archives/10122#comment_form)

nidhogg

May 30th, 2024

有一个困扰挺久的问题想请教一下苏神，每个位置对应的位置编码都是两两成对的形式（从Sinusoidal到RoPE），而且都建立在emb维度是偶数的基础上，偶数维emb与两两成对的PE组合有什么深意吗

[回复评论](https://kexue.fm/archives/10122/comment-page-1?replyTo=24461#respond-post-10122)

[苏剑林](https://kexue.fm) 发表于
June 2nd, 2024

非要扯故事的话，就是数域直到复数域才彻底完备（对于代数运算），而复数分实部和虚部，复数向量展开为实数向量的话，自然就是有偶数个分量了。

[回复评论](https://kexue.fm/archives/10122/comment-page-1?replyTo=24474#respond-post-10122)

su.zhu

May 30th, 2024

请教苏神一个问题，想听听你的看法。之前的做法普遍是先在小的b上大规模训练，再用大的b做少量训练（也可以解释为插值后训练）。 但是不知道如果一开始的大规模训练就用特别大的b是否对于长度外推有效？（个人感觉应该是无效的）

[回复评论](https://kexue.fm/archives/10122/comment-page-1?replyTo=24463#respond-post-10122)

[苏剑林](https://kexue.fm) 发表于
June 2nd, 2024

按照 [https://kexue.fm/archives/9948](https://kexue.fm/archives/9948) 的转圈解释，那么应该就是无助于长度外推。

[回复评论](https://kexue.fm/archives/10122/comment-page-1?replyTo=24476#respond-post-10122)

lzfang

May 31st, 2024

在之前那个ood转圈的视角下，为了使长度内的所有点都遍历整个圆，需要b越小越好；而在这篇文章的视角下需要b越大越好，这两者是不是出现了冲突？

[回复评论](https://kexue.fm/archives/10122/comment-page-1?replyTo=24465#respond-post-10122)

mdx 发表于
June 1st, 2024

以前的OOD需要b越小越好，这篇文章说的是b存在一个下界，不能无限小。否则的话，b 越小越好，那 b=0.1 好于 b=1000。 另外这篇文章说bound了上下文长度，但是也没说b越大越好，大于下界即可吧

[回复评论](https://kexue.fm/archives/10122/comment-page-1?replyTo=24468#respond-post-10122)

[苏剑林](https://kexue.fm) 发表于
June 2nd, 2024

这里有几个细节：

1、在转圈视角下，我们是得出（理论上）$b$越小越利于长度外推，但长度外推好未必其他一切都好；

2、实际测试显示，当$b$小到一定程度，其实连长度外推都不好，所以Attention堆叠起来后，其工作机制是非常复杂的，很多事后解释都很片面；

3、另一个极端是$b\\to\\infty$，这相当于NoPE，有些文章说它长度外推性很好，我测试起来长度外推也没有，另外就算它真的长度外推好，也只是长度外推好，实际上的效果不如加了位置编码的；

4、而本文说的是，$b$随着长度$L$增大，可能在某些方面（满足两个期望性质）对效果更加好，不同的假设，不同的评估指标，自然有不同的结论。

[回复评论](https://kexue.fm/archives/10122/comment-page-1?replyTo=24478#respond-post-10122)

hannlp

September 4th, 2024

苏神您好，YaRN的论文中使用了rope\_base=10000（静态YaRN），产生了很好的外推效果，想问一下苏神是否尝试过在使用YaRN的同时，将rope\_base调为500000，这样是否会有1+1>2的效果，也就是既好于YaRN(rope\_base=10000)，也好于NTK-aware(rope\_base=500000)？还是说结合了两者会有负面效果呀？

[回复评论](https://kexue.fm/archives/10122/comment-page-1?replyTo=25151#respond-post-10122)

[苏剑林](https://kexue.fm) 发表于
September 6th, 2024

两者结合会有负面效果。YaRN是分析了NTK起作用的原理，然后针对提出更好的策略，所以直接使用的话，YaRN基本都优于NTK。YaRN本身就是改进NTK，不存在两者结合能更优的情况。

[回复评论](https://kexue.fm/archives/10122/comment-page-1?replyTo=25162#respond-post-10122)

hannlp 发表于
September 9th, 2024

感谢苏神回复，我当下的认知是，NTK(rope\_base=500000)和YaRN(rope\_base=10000)效果是好于RoPE(rope\_base=10000)的，而YaRN是在rope\_base=10000的基础上进行的改进，从而产生了\\alpha、\\beta、以及t等参数；但是为什么rope\_base=10000就一定是YaRN最好的参数呢，如果适当的调整YaRN的其他超参数（\\alpha、\\beta），会不会在rope\_base=500000上做到比NTK更好的效果呀

[回复评论](https://kexue.fm/archives/10122/comment-page-1?replyTo=25183#respond-post-10122)

[苏剑林](https://kexue.fm) 发表于
September 9th, 2024

你可能没搞清楚逻辑。

从一个base（比如base=10000）出发，在相对较短的语料上，已经训练好了一个模型。现在想要将这个模型微调成长文本的模型，在微调之前我们需要先对RoPE做一些修改，提高它的长度外推效果，从而以达到更高的微调效率。这个修改有两种做法：NTK和YaRN，它们是有一定重叠的，YaRN分析了NTK为什么有效，然后提出了更好的解决方法，所以YaRN通常更好。

NTK和YaRN的优劣，说的是它们的免训练长度外推效果，并不是说微调后甚至是从零训练的效果优劣。经过训练的效果已经不能简单判断了。至于从零训练为什么也需要调大base，这就是本文的主题，已经不属于NTK、YaRN的范畴了。

[回复评论](https://kexue.fm/archives/10122/comment-page-1?replyTo=25197#respond-post-10122)

froze

January 15th, 2025

我尝试了一下,同时修改yarn的rope的base效果是要优于只使用yarn的

[回复评论](https://kexue.fm/archives/10122/comment-page-1?replyTo=26272#respond-post-10122)

kingdeewang

May 8th, 2025

想请教一下Eq(q)T = u2 I是怎么推导出来的？

[回复评论](https://kexue.fm/archives/10122/comment-page-1?replyTo=27547#respond-post-10122)

[苏剑林](https://kexue.fm) 发表于
May 11th, 2025

翻看了一下，好像没用到这个结论？

[回复评论](https://kexue.fm/archives/10122/comment-page-1?replyTo=27575#respond-post-10122)

kingdeewang

May 11th, 2025

[@苏剑林\|comment-27575](https://kexue.fm/archives/10122/comment-page-1#comment-27575)
就是公式4，从第三步推导到第四步。

[回复评论](https://kexue.fm/archives/10122/comment-page-1?replyTo=27587#respond-post-10122)

[苏剑林](https://kexue.fm) 发表于
May 17th, 2025

你说$\\mathbb{E}\_{\\boldsymbol{q}}\[\\boldsymbol{q}\]=\\mu\\boldsymbol{1}$？不是假设了$\\boldsymbol{q}$的每个分量独立同分布，均值为$\\mu$了吗？这完全就是定义呀。

[回复评论](https://kexue.fm/archives/10122/comment-page-1?replyTo=27615#respond-post-10122)

Grit

June 8th, 2025

苏老师您好，我想请教一下，如果在训练一个context很短的模型时我们用了一个相对于其context来说很大的$b$这会导致性能下降很严重吗？

[回复评论](https://kexue.fm/archives/10122/comment-page-1?replyTo=27822#respond-post-10122)

[苏剑林](https://kexue.fm) 发表于
June 8th, 2025

目前各方面的实测效果看来不会，我觉得本质上就是因为partial rope的实测效果优于rope所致的。

[回复评论](https://kexue.fm/archives/10122/comment-page-1?replyTo=27834#respond-post-10122)

[取消回复](https://kexue.fm/archives/10122#respond-post-10122)

你的大名

电子邮箱

个人网站（选填）

1\. 可以使用LaTeX代码，点击“预览效果”可查看效果；2. 可以通过点击评论楼层编号来引用该楼层；3. 网站可能会有点卡，如非确认评论失败，请 **不要重复点击提交**。

### 内容速览

[期望性质](https://kexue.fm/archives/10122#%E6%9C%9F%E6%9C%9B%E6%80%A7%E8%B4%A8)
[不等关系](https://kexue.fm/archives/10122#%E4%B8%8D%E7%AD%89%E5%85%B3%E7%B3%BB)
[数值求解](https://kexue.fm/archives/10122#%E6%95%B0%E5%80%BC%E6%B1%82%E8%A7%A3)
[渐近估计](https://kexue.fm/archives/10122#%E6%B8%90%E8%BF%91%E4%BC%B0%E8%AE%A1)
[相关思考](https://kexue.fm/archives/10122#%E7%9B%B8%E5%85%B3%E6%80%9D%E8%80%83)
[部分旋转](https://kexue.fm/archives/10122#%E9%83%A8%E5%88%86%E6%97%8B%E8%BD%AC)
[文章小结](https://kexue.fm/archives/10122#%E6%96%87%E7%AB%A0%E5%B0%8F%E7%BB%93)

### 智能搜索

支持整句搜索！网站自动使用 [结巴分词](https://github.com/fxsjy/jieba) 进行分词，并结合ngrams排序算法给出合理的搜索结果。

### 热门标签

[生成模型](https://kexue.fm/tag/%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B/) [attention](https://kexue.fm/tag/attention/) [优化](https://kexue.fm/tag/%E4%BC%98%E5%8C%96/) [语言模型](https://kexue.fm/tag/%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/) [模型](https://kexue.fm/tag/%E6%A8%A1%E5%9E%8B/) [网站](https://kexue.fm/tag/%E7%BD%91%E7%AB%99/) [概率](https://kexue.fm/tag/%E6%A6%82%E7%8E%87/) [梯度](https://kexue.fm/tag/%E6%A2%AF%E5%BA%A6/) [转载](https://kexue.fm/tag/%E8%BD%AC%E8%BD%BD/) [矩阵](https://kexue.fm/tag/%E7%9F%A9%E9%98%B5/) [微分方程](https://kexue.fm/tag/%E5%BE%AE%E5%88%86%E6%96%B9%E7%A8%8B/) [天象](https://kexue.fm/tag/%E5%A4%A9%E8%B1%A1/) [分析](https://kexue.fm/tag/%E5%88%86%E6%9E%90/) [深度学习](https://kexue.fm/tag/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/) [积分](https://kexue.fm/tag/%E7%A7%AF%E5%88%86/) [python](https://kexue.fm/tag/python/) [优化器](https://kexue.fm/tag/%E4%BC%98%E5%8C%96%E5%99%A8/) [力学](https://kexue.fm/tag/%E5%8A%9B%E5%AD%A6/) [无监督](https://kexue.fm/tag/%E6%97%A0%E7%9B%91%E7%9D%A3/) [扩散](https://kexue.fm/tag/%E6%89%A9%E6%95%A3/) [几何](https://kexue.fm/tag/%E5%87%A0%E4%BD%95/) [节日](https://kexue.fm/tag/%E8%8A%82%E6%97%A5/) [生活](https://kexue.fm/tag/%E7%94%9F%E6%B4%BB/) [文本生成](https://kexue.fm/tag/%E6%96%87%E6%9C%AC%E7%94%9F%E6%88%90/) [数论](https://kexue.fm/tag/%E6%95%B0%E8%AE%BA/)

### 随机文章

- [【NASA每日一图】春分时刻的土星](https://kexue.fm/archives/143)
- [矩阵描述三维空间旋转](https://kexue.fm/archives/2224)
- [纠缠的时空（三）：长度收缩和时间延缓](https://kexue.fm/archives/1971)
- [科学空间：2010年10月重要天象](https://kexue.fm/archives/951)
- [2013年全年天象](https://kexue.fm/archives/2249)
- [等值振荡定理：最优多项式逼近的充要条件](https://kexue.fm/archives/10972)
- [今日双“近”！月球、火星齐过近地点!](https://kexue.fm/archives/380)
- [\[更正\]一道经典不等式的美妙证明](https://kexue.fm/archives/1420)
- [VQ的又一技巧：给编码表加一个线性变换](https://kexue.fm/archives/10519)
- [求解微分方程的李对称方法（二）](https://kexue.fm/archives/2185)

### 最近评论

- [Truenobility303](https://kexue.fm/archives/10739/comment-page-2#comment-28059): 谢谢苏神的详细解答！
- [Truenobility303](https://kexue.fm/archives/10795/comment-page-1#comment-28058): 不好意思我的表述可能会误导性说错了，核心问题不在2\. 我觉得问题在于整套论述都基于谱条件满足那...
- [kw](https://kexue.fm/archives/11033/comment-page-1#comment-28057): 把所有M直接换成全1矩阵就行吧，比如DeltaNet变成$(QK^⊤)(I+KK^⊤⊙(1-I...
- [WB](https://kexue.fm/archives/10795/comment-page-1#comment-28056): 非常清楚的blog。我有一个小问题想问一下，推导的时候用的是不等式（10），这里左边O(1)，...
- [liangzhh](https://kexue.fm/archives/11072/comment-page-1#comment-28055): 谢谢大佬的分享，感觉中间有两个手误敲错，式(9)最后应该是加号，另外是chunk而不是chuck吧？
- [lidhrandom](https://kexue.fm/archives/11072/comment-page-1#comment-28054): Equation 3的等号右侧第二项的第一个${\\Lambda^{-1}}$疑似不应取逆
- [Kuo](https://kexue.fm/archives/11033/comment-page-1#comment-28053): 在 $PaTH$ 论文章节 \`UT Transform for Products of Hou...
- [Fanhao](https://kexue.fm/archives/10542/comment-page-1#comment-28052): 假定Hessian阵正定，那不是意味着$L(\\theta)$是$\\theta$的凸函数吗？这一...
- [曲笑一](https://kexue.fm/archives/10739/comment-page-2#comment-28051): 对于第一个疑问，我看到分布式的版本已经开源。我在想如果将每个梯度矩阵G拆分为N\*N,再利用mu...
- [曲笑一](https://kexue.fm/archives/10739/comment-page-2#comment-28050): 苏老师您好，阅读了您关于Muon系列的博客，受益匪浅。在此有两个疑问想请教您：第一个问题是，M...

### 友情链接

- [Cool Papers](https://papers.cool)
- [数学研发](https://bbs.emath.ac.cn)
- [Seatop](http://www.seatop.com.cn/)
- [Xiaoxia](https://xiaoxia.org/)
- [积分表-网络版](https://kexue.fm/sci/integral/index.html)
- [丝路博傲](http://blog.dvxj.com/)
- [ph4ntasy 饭特稀](http://www.ph4ntasy.com/)
- [数学之家](http://www.2math.cn/)
- [有趣天文奇观](http://interesting-sky.china-vo.org/)
- [TwistedW](http://www.twistedwg.com/)
- [godweiyang](https://godweiyang.com/)
- [AI柠檬](https://blog.ailemon.net/)
- [王登科-DK博客](https://greatdk.com)
- [ESON](https://blog.eson.org/)
- [枫之羽](https://fzhiy.net/)
- [Mathor's blog](https://wmathor.com/)
- [coding-zuo](https://coding-zuo.github.io/)
- [博科园](https://www.bokeyuan.net/)
- [孔皮皮的博客](https://www.kppkkp.top/)
- [运鹏的博客](https://yunpengtai.top/)
- [jiming.site](https://jiming.site/)
- [OmegaXYZ](https://www.omegaxyz.com/)
- [Blog by Eacls](https://www.eacls.top/)
- [EAI猩球](https://www.robotech.ink/)
- [文举的博客](https://liwenju0.com/)
- [用代码打点酱油](https://bruceyuan.com/)
- [申请链接](https://kexue.fm/links.html)

本站采用创作共用版权协议，要求署名、非商业用途和保持一致。转载本站内容必须也遵循“ [署名-非商业用途-保持一致](http://creativecommons.org/licenses/by-nc-nd/2.5/cn/)”的创作共用协议。
© 2009-2025 Scientific Spaces. All rights reserved. Theme by [laogui](http://www.laogui.com). Powered by [Typecho](http://typecho.org). 备案号: [粤ICP备09093259号-1/2](https://beian.miit.gov.cn/)。