## SEARCH

## MENU

- [打赏](https://kexue.fm/reward.html)
- [公式](https://kexue.fm/latex.html)
- [天象](https://kexue.fm/ac.html)
- [链接](https://kexue.fm/links.html)
- [时光](https://kexue.fm/me.html)
- [博览](https://kexue.fm/science.html)
- [归档](https://kexue.fm/content.html)

## CATEGORIES

- [千奇百怪](https://kexue.fm/category/Everything)
- [天文探索](https://kexue.fm/category/Astronomy)
- [数学研究](https://kexue.fm/category/Mathematics)
- [物理化学](https://kexue.fm/category/Phy-chem)
- [信息时代](https://kexue.fm/category/Big-Data)
- [生物自然](https://kexue.fm/category/Biology)
- [图片摄影](https://kexue.fm/category/Photograph)
- [问题百科](https://kexue.fm/category/Questions)
- [生活/情感](https://kexue.fm/category/Life-Feeling)
- [资源共享](https://kexue.fm/category/Resources)

## NEWPOSTS

- [让炼丹更科学一些（二）：将结论推广...](https://kexue.fm/archives/11469)
- [滑动平均视角下的权重衰减和学习率](https://kexue.fm/archives/11459)
- [生成扩散模型漫谈（三十一）：预测数...](https://kexue.fm/archives/11428)
- [Muon优化器指南：快速上手与关键细节](https://kexue.fm/archives/11416)
- [AdamW的Weight RMS的...](https://kexue.fm/archives/11404)
- [n个正态随机数的最大值的渐近估计](https://kexue.fm/archives/11390)
- [流形上的最速下降：5\. 对偶梯度下降](https://kexue.fm/archives/11388)
- [低精度Attention可能存在有...](https://kexue.fm/archives/11371)
- [MuP之上：1. 好模型的三个特征](https://kexue.fm/archives/11340)
- [随机矩阵的谱范数的快速估计](https://kexue.fm/archives/11335)

## COMMENTS

- [Yifan GUO: 我打脸了，写了代码快速验证了一下，softmax对应的effi...](https://kexue.fm/archives/7546/comment-page-4#comment-29008)
- [Yifan GUO: 《Efficient Attention: Attention...](https://kexue.fm/archives/7546/comment-page-4#comment-29007)
- [岁月如书: muon怎么就丢了奇异值，奇异值用来做weight decay...](https://kexue.fm/archives/11459/comment-page-1#comment-29006)
- [Yifan GUO: Oh，我貌似理解了，或许我可以这样给自己解释：\
如果分母的作用...](https://kexue.fm/archives/11033/comment-page-3#comment-29005)
- [Yifan GUO: "其中分母的作用主要是保持数值稳定性，另外就是如果我们给O加上...](https://kexue.fm/archives/11033/comment-page-3#comment-29004)
- [苏剑林: 嗯，类似的观点我们在 https://kexue.fm/arc...](https://kexue.fm/archives/10739/comment-page-2#comment-29002)
- [苏剑林: KL散度希望$p(Z\|X)$的log\_var接近于0，但由于重...](https://kexue.fm/archives/5253/comment-page-2#comment-29001)
- [苏剑林: $p(Z)$是标准正态分布，我们才能从标准正态分布中随机采样生成。](https://kexue.fm/archives/5253/comment-page-18#comment-29000)
- [苏剑林: 原则上不必要](https://kexue.fm/archives/9181/comment-page-5#comment-28999)
- [苏剑林: \[comment=28988\]xiaojx\[/comment\]...](https://kexue.fm/archives/9181/comment-page-5#comment-28998)

## USERLOGIN

- [登录](https://kexue.fm/admin/login.php)

[科学空间\|Scientific Spaces](https://kexue.fm)

- [登录](https://kexue.fm/admin/login.php)
- [打赏](https://kexue.fm/reward.html)
- [公式](https://kexue.fm/latex.html)
- [天象](https://kexue.fm/ac.html)
- [链接](https://kexue.fm/links.html)
- [时光](https://kexue.fm/me.html)
- [博览](https://kexue.fm/science.html)
- [归档](https://kexue.fm/content.html)

渴望成为一个小飞侠

- [欢迎订阅](https://kexue.fm/feed)
- [个性邮箱](https://kexue.fm/archives/119)
- [天象信息](https://kexue.fm/ac.html)
- [观测ISS](https://kexue.fm/archives/41)
- [LaTeX](https://kexue.fm/latex.html)
- [关于博主](https://kexue.fm/me.html)

欢迎访问“科学空间”，这里将与您共同探讨自然科学，回味人生百态；也期待大家的分享～

- [**千奇百怪** Everything](https://kexue.fm/category/Everything)
- [**天文探索** Astronomy](https://kexue.fm/category/Astronomy)
- [**数学研究** Mathematics](https://kexue.fm/category/Mathematics)
- [**物理化学** Phy-chem](https://kexue.fm/category/Phy-chem)
- [**信息时代** Big-Data](https://kexue.fm/category/Big-Data)
- [**生物自然** Biology](https://kexue.fm/category/Biology)
- [**图片摄影** Photograph](https://kexue.fm/category/Photograph)
- [**问题百科** Questions](https://kexue.fm/category/Questions)
- [**生活/情感** Life-Feeling](https://kexue.fm/category/Life-Feeling)
- [**资源共享** Resources](https://kexue.fm/category/Resources)

- [**千奇百怪**](https://kexue.fm/category/Everything)
- [**天文探索**](https://kexue.fm/category/Astronomy)
- [**数学研究**](https://kexue.fm/category/Mathematics)
- [**物理化学**](https://kexue.fm/category/Phy-chem)
- [**信息时代**](https://kexue.fm/category/Big-Data)
- [**生物自然**](https://kexue.fm/category/Biology)
- [**图片摄影**](https://kexue.fm/category/Photograph)
- [**问题百科**](https://kexue.fm/category/Questions)
- [**生活/情感**](https://kexue.fm/category/Life-Feeling)
- [**资源共享**](https://kexue.fm/category/Resources)

[首页](https://kexue.fm) [数学研究](https://kexue.fm/category/Mathematics) 滑动平均视角下的权重衰减和学习率

5Dec

# [滑动平均视角下的权重衰减和学习率](https://kexue.fm/archives/11459)

By 苏剑林 \|
2025-12-05 \|
5263位读者\|

权重衰减（Weight Decay）和学习率（Learning Rate）是LLM预训练的重要组成部分，它们的设置是否妥当，是模型最终成败的关键因素之一。自 [AdamW](https://papers.cool/arxiv/1711.05101) 以来，单独分离出Weight Decay来取代传统的L2正则，基本上已经成为了共识，但在此基础上，如何合理地设置Weight Decay和Learning Rate，并没有显著的理论进展。

本文将抛砖引玉，分享笔者关于这个问题的一些新理解：把训练过程看作对训练数据的滑动平均记忆，探讨如何设置Weight Decay和Learning Rate才能让这个记忆更为科学。

## 滑动平均 [\#](https://kexue.fm/kexue.fm\#%E6%BB%91%E5%8A%A8%E5%B9%B3%E5%9D%87)

Weight Decay的一般形式是
\\begin{equation}\\boldsymbol{\\theta}\_t = \\boldsymbol{\\theta}\_{t-1} - \\eta\_t (\\boldsymbol{u}\_t + \\lambda\_t \\boldsymbol{\\theta}\_{t-1})\\end{equation}
其中$\\boldsymbol{\\theta}$是参数，$\\boldsymbol{u}$是优化器给出的更新量，$\\lambda\_t,\\eta\_t$亦即我们说的Weight Decay和Learning Rate，而整个序列$\\{\\lambda\_t\\}$和$\\{\\eta\_t\\}$，我们分别称为“WD Schedule”和“LR Schedule”。引入记号
\\begin{equation}\\begin{aligned}
\\boldsymbol{m}\_t =&\\, \\beta\_1 \\boldsymbol{m}\_{t-1} + \\left(1 - \\beta\_1\\right) \\boldsymbol{g}\_t, & \\hat{\\boldsymbol{m}}\_t =&\\, \\boldsymbol{m}\_t\\left/\\left(1 - \\beta\_1^t\\right)\\right. &\\\\[5pt\]
\\boldsymbol{v}\_t =&\\, \\beta\_2 \\boldsymbol{v}\_{t-1} + \\left(1 - \\beta\_2\\right) \\boldsymbol{g}\_t^2,& \\hat{\\boldsymbol{v}}\_t =&\\, \\boldsymbol{v}\_t\\left/\\left(1 - \\beta\_2^t\\right)\\right. &
\\end{aligned}\\end{equation}
那么对于SGDM来说有$\\boldsymbol{u}\_t=\\boldsymbol{m}\_t$，对于RMSProp来说$\\boldsymbol{u}\_t= \\boldsymbol{g}\_t/(\\sqrt{\\boldsymbol{v}\_t} + \\epsilon)$，对于Adam来说则是$\\boldsymbol{u}\_t=\\hat{\\boldsymbol{m}}\_t\\left/\\left(\\sqrt{\\hat{\\boldsymbol{v}}\_t} + \\epsilon\\right)\\right.$，对于SignSGDM来说则$\\newcommand{sign}{\\mathop{\\text{sign}}}\\boldsymbol{u}\_t=\\sign(\\boldsymbol{m}\_t)$，而Muon则是$\\newcommand{msign}{\\mathop{\\text{msign}}}\\boldsymbol{u}\_t=\\msign(\\boldsymbol{m}\_t)$。这里列举的例子，除了SGDM外，其他都算是某种自适应学习率优化器。

我们的出发点是滑动平均（Exponential Moving Average，EMA）视角，即将Weight Decay写成
\\begin{equation}\\boldsymbol{\\theta}\_t = (1 - \\lambda\_t \\eta\_t)\\boldsymbol{\\theta}\_{t-1} - \\eta\_t \\boldsymbol{u}\_t = (1 - \\lambda\_t \\eta\_t)\\boldsymbol{\\theta}\_{t-1} + \\lambda\_t \\eta\_t ( -\\boldsymbol{u}\_t / \\lambda\_t)\\label{eq:wd-ema}\\end{equation}
此时Weight Decay表现为模型参数与$-\\boldsymbol{u}\_t / \\lambda\_t$的加权平均形式。滑动平均视角不是新的， [《How to set AdamW's weight decay as you scale model and dataset size》](https://papers.cool/arxiv/2405.13698)、 [《Power Lines: Scaling Laws for Weight Decay and Batch Size in LLM Pre-training》](https://papers.cool/arxiv/2505.13738) 等文章已有过讨论，本文则是在这个视角下将各方面算得更仔细一些。

接下来的篇幅我们主要以Adam为例，最后再讨论其他优化器的可用性。计算过程跟 [《AdamW的Weight RMS的渐近估计（上）》](https://kexue.fm/archives/11307) 和 [《AdamW的Weight RMS的渐近估计（下）》](https://kexue.fm/archives/11404) 会有相当一部份重叠之处，读者可以相互参考着阅读。

## 迭代展开 [\#](https://kexue.fm/kexue.fm\#%E8%BF%AD%E4%BB%A3%E5%B1%95%E5%BC%80)

简单起见，我们先考虑常数$\\lambda,\\eta$，记$\\beta\_3 = 1 - \\lambda\\eta$，那么$\\boldsymbol{\\theta}\_t = \\beta\_3 \\boldsymbol{\\theta}\_{t-1} + (1 - \\beta\_3)( -\\boldsymbol{u}\_t / \\lambda)$，这样形式上就跟$\\boldsymbol{m}\_t,\\boldsymbol{v}\_t$一致了。直接迭代展开得
\\begin{equation}\\boldsymbol{\\theta}\_t = \\beta\_3^t \\boldsymbol{\\theta}\_0 + (1 - \\beta\_3)\\sum\_{i=1}^t \\beta\_3^{t-i} (-\\boldsymbol{u}\_i / \\lambda) \\end{equation}
对于Adam有$\\boldsymbol{u}\_t=\\hat{\\boldsymbol{m}}\_t\\left/\\left(\\sqrt{\\hat{\\boldsymbol{v}}\_t} + \\epsilon\\right)\\right.$，一般在训练结束时，$t$都能足够大以至于$\\beta\_1^t,\\beta\_2^t$足够接近于零，所以可以不去区分$\\boldsymbol{m}\_t$和$\\hat{\\boldsymbol{m}}\_t$、$\\boldsymbol{v}\_t$和$\\hat{\\boldsymbol{v}}\_t$，进一步地，简单设$\\epsilon=0$，那么可以简化成$\\boldsymbol{u}\_t=\\boldsymbol{m}\_t / \\sqrt{\\boldsymbol{v}\_t}$，然后来个经典的平均场近似
\\begin{equation}\\underbrace{\\frac{1-\\beta\_3}{1-\\beta\_3^t}\\sum\_{i=1}^t \\beta\_3^{t-i} \\boldsymbol{u}\_i}\_{\\text{记为}\\bar{\\boldsymbol{u}}\_t} = \\frac{1-\\beta\_3}{1-\\beta\_3^t}\\sum\_{i=1}^t \\beta\_3^{t-i} \\frac{\\boldsymbol{m}\_i}{\\sqrt{\\boldsymbol{v}\_i}}\\approx \\frac{\\bar{\\boldsymbol{m}}\_t \\,\\,\\triangleq\\,\\, \\frac{1-\\beta\_3}{1-\\beta\_3^t}\\sum\_{i=1}^t \\beta\_3^{t-i}\\boldsymbol{m}\_i}{\\sqrt{\\bar{\\boldsymbol{v}}\_t \\,\\,\\triangleq\\,\\, \\frac{1-\\beta\_3}{1-\\beta\_3^t}\\sum\_{i=1}^t \\beta\_3^{t-i}\\boldsymbol{v}\_i}}\\label{eq:u-bar}\\end{equation}
展开$\\boldsymbol{m}\_t,\\boldsymbol{v}\_t$得$\\boldsymbol{m}\_t = (1 - \\beta\_1)\\sum\_{i=1}^t \\beta\_1^{t-i}\\boldsymbol{g}\_i$和$\\boldsymbol{v}\_t = (1 - \\beta\_2)\\sum\_{i=1}^t \\beta\_2^{t-i}\\boldsymbol{g}\_i^2$，代入上式
\\begin{gather}
\\bar{\\boldsymbol{m}}\_t = \\frac{(1-\\beta\_3)(1 - \\beta\_1)}{1-\\beta\_3^t}\\sum\_{i=1}^t \\beta\_3^{t-i} \\sum\_{j=1}^i \\beta\_1^{i-j}\\boldsymbol{g}\_j = \\frac{(1-\\beta\_3)(1 - \\beta\_1)}{(1-\\beta\_3^t)(\\beta\_3 - \\beta\_1)}\\sum\_{j=1}^t (\\beta\_3^{t-j+1} - \\beta\_1^{t-j+1})\\boldsymbol{g}\_j\\\\[6pt\]
\\bar{\\boldsymbol{v}}\_t = \\frac{(1-\\beta\_3)(1 - \\beta\_2)}{1-\\beta\_3^t}\\sum\_{i=1}^t \\beta\_3^{t-i} \\sum\_{j=1}^i \\beta\_2^{i-j}\\boldsymbol{g}\_j^2 = \\frac{(1-\\beta\_3)(1 - \\beta\_2)}{(1-\\beta\_3^t)(\\beta\_3 - \\beta\_2)}\\sum\_{j=1}^t (\\beta\_3^{t-j+1} - \\beta\_2^{t-j+1})\\boldsymbol{g}\_j^2
\\end{gather}
交换求和符号基于恒等式$\\sum\_{i=1}^t \\sum\_{j=1}^i a\_i b\_j = \\sum\_{j=1}^t \\sum\_{i=j}^t a\_i b\_j$。综上，我们有
\\begin{equation}\\boldsymbol{\\theta}\_t = \\beta\_3^t \\boldsymbol{\\theta}\_0 + (1 - \\beta\_3^t)(-\\bar{\\boldsymbol{u}}\_t / \\lambda) \\label{eq:theta-0-bar-u}\\end{equation}
权重$\\boldsymbol{\\theta}\_t$是我们想要的训练结果，它被表示为$\\boldsymbol{\\theta}\_0$和$-\\bar{\\boldsymbol{u}}\_t / \\lambda$的加权平均。其中，$\\boldsymbol{\\theta}\_0$是初始权重，$\\bar{\\boldsymbol{u}}\_t$则是数据相关的，在平均场近似下它约等于$\\bar{\\boldsymbol{m}}\_t/\\sqrt{\\bar{\\boldsymbol{v}}\_t}$，而$\\bar{\\boldsymbol{m}}\_t$和$\\bar{\\boldsymbol{v}}\_t$可以表示成每一步梯度的加权求和，以$\\bar{\\boldsymbol{m}}\_t$为例，第$j$步的梯度权重正比于$\\beta\_3^{t-j+1} - \\beta\_1^{t-j+1}$。

## 记忆周期 [\#](https://kexue.fm/kexue.fm\#%E8%AE%B0%E5%BF%86%E5%91%A8%E6%9C%9F)

我们主要关心的是预训练，特点是Single-Epoch，大部分数据都只过一遍，所以训出好效果的关键之一是不要忘掉早期的数据。假设训练数据已经经过全局打乱，那么可以合理地认为每一个Batch的数据都同等重要。

数据是以梯度形式线性叠加到$\\bar{\\boldsymbol{m}}\_t$中的，假设每一步梯度只包含当前Batch的信息，那么某个Batch想要不被遗忘，系数$\\beta\_3^{t-j+1} - \\beta\_1^{t-j+1}$就不能太小。考察函数$f(s) = \\beta\_3^s - \\beta\_1^s$，它是一个先增后减的函数，但因为$\\beta\_3$会比$\\beta\_1$更接近于1，所以增的步数不多，远处更多是接近指数下降，如下图：

梯度权重示意图

总之趋势是距离越远系数越小。那么要想模型不遗忘每一个Batch，最远处的系数就不能太小。假设系数不小于$c \\in (0, 1)$才能被记住，当$s$足够大时$\\beta\_1^s$先趋于0，所以$\\beta\_3^s - \\beta\_1^s\\approx \\beta\_3^s$，由$\\beta\_3^s\\geq c$可以解得$s \\leq \\frac{\\log c}{\\log \\beta\_3} \\approx \\frac{-\\log c}{\\lambda\\eta}$。这表明，模型顶多能记住$\\mathcal{O}(1/\\lambda\\eta)$步的数据，这是它的记忆周期。

那直接无脑$\\lambda=0$，让记忆周期无限大，是否就可以不担心遗忘问题了？理论上是的，然而这并不是一个好选择。Weight Decay还有一个作用是帮助模型忘掉初始化。由式$\\eqref{eq:theta-0-bar-u}$可知初始化$\\boldsymbol{\\theta}\_0$的权重是$\\beta\_3^t$，如果$\\beta\_3$太大或者训练步数$t$太小，那么初始化的占比还很高，模型可能还处于欠拟合阶段。

此外，Weight Decay还有利于稳定模型“内科”。在 [《AdamW的Weight RMS的渐近估计（上）》](https://kexue.fm/archives/11307) 我们已经推导过，AdamW的Weight RMS的渐近结果是$\\sqrt{\\eta/2\\lambda}$，如果$\\lambda=0$，那么Weight RMS会以$\\eta\\sqrt{t}$的速率膨胀。这意味着直接设置$\\lambda=0$，还可能带来权重爆炸等模型内科异常。

因此，$\\beta\_3$不能太小，以免忘记早期数据，同时也不能太大，以免欠拟合或者权重爆炸。比较适合的设置是让$1/\\lambda\\eta$正比于训练步数，如果是Multi-Epoch的训练场景，则考虑让$1/\\lambda\\eta$正比于单个Epoch的训练步数。

## 动态版本 [\#](https://kexue.fm/kexue.fm\#%E5%8A%A8%E6%80%81%E7%89%88%E6%9C%AC)

在实际训练中，我们更多是适用动态变化的LR Schedule，比如Consine Decay、Linear Decay、WSD（Warmup-Stable-Decay）等，所以上述静态Weight Decay和Learning Rate的结果并不完全符合实践，我们需要将它们推广到动态版。

从式$\\eqref{eq:wd-ema}$出发，利用近似$1 - \\lambda\_t \\eta\_t\\approx e^{-\\lambda\_t \\eta\_t}$，并迭代展开，可得
\\begin{equation}\\boldsymbol{\\theta}\_t = (1 - \\lambda\_t \\eta\_t)\\boldsymbol{\\theta}\_{t-1} - \\eta\_t \\boldsymbol{u}\_t \\approx e^{-\\lambda\_t \\eta\_t}\\boldsymbol{\\theta}\_{t-1} - \\eta\_t \\boldsymbol{u}\_t = e^{-\\kappa\_t}\\left(\\boldsymbol{\\theta}\_0 - \\sum\_{i=1}^t e^{\\kappa\_i}\\eta\_i\\boldsymbol{u}\_i\\right)\\end{equation}
其中$\\kappa\_t = \\sum\_{i=1}^t \\eta\_i\\lambda\_i$。继续设$z\_t = \\sum\_{i=1}^t e^{\\kappa\_i}\\eta\_i$，那么可以得到同样的平均场近似
\\begin{equation}\\bar{\\boldsymbol{u}}\_t\\triangleq\\frac{1}{z\_t}\\sum\_{i=1}^t e^{\\kappa\_i}\\eta\_i \\boldsymbol{u}\_i = \\frac{1}{z\_t}\\sum\_{i=1}^t e^{\\kappa\_i}\\eta\_i \\frac{\\boldsymbol{m}\_i}{\\sqrt{\\boldsymbol{v}\_i}}\\approx \\frac{\\bar{\\boldsymbol{m}}\_t \\,\\,\\triangleq\\,\\, \\frac{1}{z\_t}\\sum\_{i=1}^t e^{\\kappa\_i}\\eta\_i\\boldsymbol{m}\_i}{\\sqrt{\\bar{\\boldsymbol{v}}\_t \\,\\,\\triangleq\\,\\, \\frac{1}{z\_t}\\sum\_{i=1}^t e^{\\kappa\_i}\\eta\_i\\boldsymbol{v}\_i}}\\end{equation}
代入$\\boldsymbol{m}\_t,\\boldsymbol{v}\_t$的展开式得
\\begin{gather}
\\bar{\\boldsymbol{m}}\_t = \\frac{1}{z\_t}\\sum\_{i=1}^t e^{\\kappa\_i}\\eta\_i\\boldsymbol{m}\_i = \\frac{1 - \\beta\_1}{z\_t}\\sum\_{i=1}^t e^{\\kappa\_i}\\eta\_i\\sum\_{j=1}^i \\beta\_1^{i-j}\\boldsymbol{g}\_j = \\sum\_{j=1}^t\\boldsymbol{g}\_j\\underbrace{\\frac{1 - \\beta\_1}{z\_t}\\sum\_{i=j}^t e^{\\kappa\_i}\\beta\_1^{i-j}\\eta\_i}\_{\\text{记为}\\bar{\\beta}\_1(j,t)} \\\
\\bar{\\boldsymbol{v}}\_t = \\frac{1}{z\_t}\\sum\_{i=1}^t e^{\\kappa\_i}\\eta\_i\\boldsymbol{v}\_i = \\frac{1 - \\beta\_2}{z\_t}\\sum\_{i=1}^t e^{\\kappa\_i}\\eta\_i\\sum\_{j=1}^i \\beta\_2^{i-j}\\boldsymbol{g}\_j^2 = \\sum\_{j=1}^t\\boldsymbol{g}\_j^2\\underbrace{\\frac{1 - \\beta\_2}{z\_t}\\sum\_{i=j}^t e^{\\kappa\_i}\\beta\_2^{i-j}\\eta\_i}\_{\\text{记为}\\bar{\\beta}\_2(j,t)} \\\
\\end{gather}
可以看到，跟静态Weight Decay和Learning Rate相比，动态版形式上并没有太大变化，只不过梯度的加权系数变成了稍微复杂一点的$\\bar{\\beta}\_1(j,t)$和$\\bar{\\beta}\_2(j,t)$。特别地，当$\\beta\_1,\\beta\_2\\to 0$时，$\\bar{\\beta}\_1(j,t)$和$\\bar{\\beta}\_2(j,t)$简化为
\\begin{equation}\\bar{\\beta}\_1(j,t) = \\bar{\\beta}\_2(j,t) = \\frac{e^{\\kappa\_j}\\eta\_j}{z\_t}\\label{eq:bb1-bb2-0}\\end{equation}

## 最优调度 [\#](https://kexue.fm/kexue.fm\#%E6%9C%80%E4%BC%98%E8%B0%83%E5%BA%A6)

接下来可以做的事情有很多，最基本的就是根据具体的WD Schedule和LR Schedule去算$\\bar{\\beta}\_1(j,t)$和$\\bar{\\beta}\_2(j,t)$、估计记忆周期等。不过，这里我们选择做一件更极致的事情——直接去反推一个最优的WD Schedule和LR Schedule。

具体来说，前面我们假设了数据已经全局打乱，那么每个Batch的数据都同等重要，但静态版得到的系数$\\bar{\\beta}\_1(j,t)\\propto\\beta\_3^{t-j+1} - \\beta\_1^{t-j+1}$并非常数，而是随距离变化，这跟“每个Batch的数据都同等重要”不完全吻合。如果条件允许，那么我们期望它应该恒等于某个常数。根据这个期望，我们就可以反解出对应的$\\lambda\_j,\\eta\_j$。

简单起见，我们从$\\beta\_1,\\beta\_2\\to 0$的特例入手，此时希望求解的方程是$e^{\\kappa\_j}\\eta\_j/z\_t = c\_t$，其中$c\_t$是某个只依赖于$t$的函数。注意上面说的“常数”是对于$j$而言的，$t$是训练的终点，常数可以与它有关。为了简化求解，我们用积分代替求和，即$\\kappa\_s \\approx \\int\_0^s \\lambda\_{\\tau} \\eta\_{\\tau} d\\tau$，那么方程变为$\\exp\\left(\\int\_0^s \\lambda\_{\\tau} \\eta\_{\\tau} d\\tau\\right)\\eta\_s \\approx c\_t z\_t$。两边取对数，然后对$s$求导得
\\begin{equation}\\lambda\_s \\eta\_s + \\frac{\\dot{\\eta}\_s}{\\eta\_s} \\approx 0 \\label{eq:lr-wd-ode}\\end{equation}
如果$\\lambda\_s$取常数$\\lambda$，那么可以解得
\\begin{equation}\\eta\_s \\approx \\frac{\\eta\_{\\max}}{\\lambda\\eta\_{\\max} s + 1}\\label{eq:opt-lrt-wd}\\end{equation}
这便是常数Weight Decay下的最佳LR Schedule，它不需要预设终点$t$和最小学习率$\\eta\_{\\min}$，这意味着它可以无限训练下去，类似于WSD的Stable阶段，但它能自动平衡每一步梯度的系数。不过它也有个缺点：$s\\to\\infty$时它会趋于0。从 [《AdamW的Weight RMS的渐近估计（下）》](https://kexue.fm/archives/11404) 可知Weight RMS会趋于$\\lim\\limits\_{s\\to\\infty} \\frac{\\eta\_s}{2\\lambda\_s}$，所以该缺点可能带来权重坍缩的风险。

为了解决这个问题，我们可以考虑让$\\lambda\_s = \\alpha\\eta\_s$，$\\alpha=\\lambda\_{\\max}/\\eta\_{\\max}$是一个常数，此时可以解得
\\begin{equation}\\eta\_s \\approx \\frac{\\eta\_{\\max}}{\\sqrt{2\\lambda\_{\\max}\\eta\_{\\max} s + 1}},\\qquad \\lambda\_s \\approx \\frac{\\lambda\_{\\max}}{\\sqrt{2\\lambda\_{\\max}\\eta\_{\\max} s + 1}} \\label{eq:opt-lrt-wdt}\\end{equation}
相应的$e^{\\kappa\_s} \\approx \\sqrt{2\\lambda\_{\\max}\\eta\_{\\max} s + 1}, e^{\\kappa\_s}\\eta\_s \\approx \\eta\_{\\max}, z\_t\\approx \\eta\_{\\max} t, \\bar{\\beta}\_1(j,t) = \\bar{\\beta}\_2(j,t) \\approx 1/t$。

## 一般结果 [\#](https://kexue.fm/kexue.fm\#%E4%B8%80%E8%88%AC%E7%BB%93%E6%9E%9C)

目前的结果，比如式$\\eqref{eq:opt-lrt-wd}$和式$\\eqref{eq:opt-lrt-wdt}$，都是基于$\\beta\_1,\\beta\_2=0$的，当它们不等于0时，结果需要变化吗？更一般地，上述结果都是基于Adam优化器的，它们多大程度上可以推广到其他优化器呢？

首先来看$\\beta\_1,\\beta\_2\\neq 0$时的问题，答案是当$t$足够大时，结论并不用大改。以$\\bar{\\beta}\_1(j,t)$为例，在上述最优调度下$e^{\\kappa\_i}\\eta\_i$等于（跟$t$有关的）常数，那么根据定义
\\begin{equation}\\bar{\\beta}\_1(j,t) = \\frac{1 - \\beta\_1}{z\_t}\\sum\_{i=j}^t e^{\\kappa\_i}\\beta\_1^{i-j}\\eta\_i \\propto \\sum\_{i=j}^t \\beta\_1^{i-j} = \\frac{1 - \\beta\_1^{t-j+1}}{1 - \\beta\_1}\\end{equation}
当$t$足够大时$\\beta\_1^{t-j+1}\\to 0$，所以这也可以看成是一个跟$j$无关的常数。前面也说了，对于$\\beta\_1,\\beta\_2$来说，“$t$足够大”这件事情几乎时肯定的，所以直接用$\\beta\_1,\\beta\_2=0$的结果就行。

至于优化器，前面我们提到的优化器有SGDM、RMSProp、Adam、SignSGDM、Muon，它们可以分为两类。其中，SGDM是一类，它的$\\bar{\\boldsymbol{u}}\_t$直接就是$\\bar{\\boldsymbol{m}}\_t$，连平均场近似都不需要用，所以直到式$\\eqref{eq:lr-wd-ode}$的结果都是适用的。不过，式$\\eqref{eq:opt-lrt-wd}$和式$\\eqref{eq:opt-lrt-wdt}$大概不是最适合的了，因为SGDM的渐近Weight RMS还依赖于梯度模长\[ [参考](https://papers.cool/arxiv/2305.17212)\]，所以要把梯度模长考虑进去才行，相对复杂一些。

剩下的RMSProp、Adam、SignSGDM、Muon我们将它归为另一类，都属于自适应学习率优化器，它们的更新规则都具有$\\frac{\\text{梯度}}{\\sqrt{\\text{梯度}{}^2}}$的其次形式，这种情况下，如果我们依旧相信平均场近似，那么就能得到同样的$\\bar{\\boldsymbol{m}}\_t$、同样的$\\beta\_1(j,t)$，所以到式$\\eqref{eq:lr-wd-ode}$的结果也是适用的；并且对于这类齐次型优化器，可以证明Weight RMS同样渐近正比于$\\sqrt{\\eta/\\lambda}$，所以连同式$\\eqref{eq:opt-lrt-wd}$和式$\\eqref{eq:opt-lrt-wdt}$也可以复用。

## 假设讨论 [\#](https://kexue.fm/kexue.fm\#%E5%81%87%E8%AE%BE%E8%AE%A8%E8%AE%BA)

我们的推导暂告一段落，这一节我们来讨论一下推导所依赖的假设。

纵观全文，推导过程中所用到的值得拿出来讨论的大假设主要有两个。第一个假设是平均场近似，首次介绍于 [《重新思考学习率与Batch Size（二）：平均场》](https://kexue.fm/archives/11280)。平均场本身肯定不是新的，它是物理学中的经典近似，但将其用来分析优化器的相关动态，应该是笔者首次引入的，目前已经用它估算过优化器的 [Batch Size](https://kexue.fm/archives/11280)、 [Update RMS](https://kexue.fm/archives/11267)、 [Weight RMS](https://kexue.fm/archives/11307) 等，结果看起来是合理的。

对于平均场近似的有效性，其实我们没法评述太多，它更多体现了一种信仰。一方面，我们根据已有的估算结果的合理性，相信它会继续合理下去，至少能对一些标量指标给出有效的渐近估计。另一方面，对于自适应学习率优化器，由于其更新规则的非线性，分析难度大大增加，除了平均场近似外，我们其实也没什么计算工具能用了。

这其中最典型的例子就是Muon，因为它是非Element-wise的运算，以往像SignSGD那样逐分量的计算手段便失去了作用，而平均场近似依然奏效（参考 [《重新思考学习率与Batch Size（三）：Muon》](https://kexue.fm/archives/11285)）。所以，平均场近似实际上为一大类自适应学习率优化器的分析和估计提供了统一、有效、简洁的计算手段，目前看来似乎没有别的方法有同样的效果，所以我们只能继续相信它。

第二个大的假设是“每一步梯度只包含当前Batch的信息”，这个假设本质上是错误的，因为梯度不仅依赖于当前Batch的数据，还依赖于上一步的参数，而上一步的参数自然是包含了历史信息。不过，我们可以尝试补救一下，因为理论上来说，每个Batch都会带来新的信息，否则这个Batch就没有存在的意义了，所以补救的方法是改为“每一步梯度包含大致相同的增量信息”。

当然，仔细思考之下这个说法也是有争议的，因为学得越多，覆盖面越广，后来Batch的独特信息就越少。不过，还可以挣扎一下，那就是将知识分为“规律”和“事实”两大类，事实型知识——比如某个定理是某个数学家发现的——只能靠记忆，那么可以考虑改为“每一步梯度包含大致相同的事实型知识”。总之，从实践来看，“平等对待每一步梯度”所得的LR Schedule似乎真的是有好处的，所以总可以尝试为它构造一个解释。

最近的论文 [《How Learning Rate Decay Wastes Your Best Data in Curriculum-Based LLM Pretraining》](https://papers.cool/arxiv/2511.18903) 提供了一个间接证据。它考虑了数据质量从低到高的课程学习，发现激进的LR Decay会让课程学习优势全无。而我们的结果是每一Batch的权重是式$\\eqref{eq:bb1-bb2-0}$，正比于Learning Rate，如果LR Decay过于激进，那么后面的高质量数据权重反而过小，因而效果欠佳。能够合理解释这个现象，反过来显示了我们假设的合理性。

## 文章小结 [\#](https://kexue.fm/kexue.fm\#%E6%96%87%E7%AB%A0%E5%B0%8F%E7%BB%93)

本文从滑动平均的视角来理解权重衰减（WD）和学习率（LR），并探讨了在该视角下最优的WD Schedule和LR Schedule。

_**转载到请包括本文地址：** [https://kexue.fm/archives/11459](https://kexue.fm/archives/11459)_

_**更详细的转载事宜请参考：**_ [《科学空间FAQ》](https://kexue.fm/archives/6508#%E6%96%87%E7%AB%A0%E5%A6%82%E4%BD%95%E8%BD%AC%E8%BD%BD/%E5%BC%95%E7%94%A8)

**如果您还有什么疑惑或建议，欢迎在下方评论区继续讨论。**

**如果您觉得本文还不错，欢迎 [分享](https://kexue.fm/kexue.fm#share)/ [打赏](https://kexue.fm/kexue.fm#pay) 本文。打赏并非要从中获得收益，而是希望知道科学空间获得了多少读者的真心关注。当然，如果你无视它，也不会影响你的阅读。再次表示欢迎和感谢！**

打赏

微信打赏

支付宝打赏

因为网站后台对打赏并无记录，因此欢迎在打赏时候备注留言。你还可以 [**点击这里**](http://mail.qq.com/cgi-bin/qm_share?t=qm_mailme&email=tN7d1drY3drrx8H0xcWa19vZ) 或在下方评论区留言来告知你的建议或需求。

**如果您需要引用本文，请参考：**

苏剑林. (Dec. 05, 2025). 《滑动平均视角下的权重衰减和学习率 》\[Blog post\]. Retrieved from [https://kexue.fm/archives/11459](https://kexue.fm/archives/11459)

@online{kexuefm-11459,
        title={滑动平均视角下的权重衰减和学习率},
        author={苏剑林},
        year={2025},
        month={Dec},
        url={\\url{https://kexue.fm/archives/11459}},
}

分类： [数学研究](https://kexue.fm/category/Mathematics)    标签： [最优](https://kexue.fm/tag/%E6%9C%80%E4%BC%98/), [梯度](https://kexue.fm/tag/%E6%A2%AF%E5%BA%A6/), [学习率](https://kexue.fm/tag/%E5%AD%A6%E4%B9%A0%E7%8E%87/), [平均场](https://kexue.fm/tag/%E5%B9%B3%E5%9D%87%E5%9C%BA/)[1 评论](https://kexue.fm/archives/11459#comments)

< [生成扩散模型漫谈（三十一）：预测数据而非噪声](https://kexue.fm/archives/11428) \| [让炼丹更科学一些（二）：将结论推广到无界域](https://kexue.fm/archives/11469) >

### 你也许还对下面的内容感兴趣

- [AdamW的Weight RMS的渐近估计（下）](https://kexue.fm/archives/11404)
- [DiVeQ：一种非常简洁的VQ训练方案](https://kexue.fm/archives/11328)
- [AdamW的Weight RMS的渐近估计（上）](https://kexue.fm/archives/11307)
- [重新思考学习率与Batch Size（四）：EMA](https://kexue.fm/archives/11301)
- [重新思考学习率与Batch Size（三）：Muon](https://kexue.fm/archives/11285)
- [重新思考学习率与Batch Size（二）：平均场](https://kexue.fm/archives/11280)
- [为什么Adam的Update RMS是0.2？](https://kexue.fm/archives/11267)
- [重新思考学习率与Batch Size（一）：现状](https://kexue.fm/archives/11260)
- [msign的导数](https://kexue.fm/archives/11025)
- [等值振荡定理：最优多项式逼近的充要条件](https://kexue.fm/archives/10972)

[发表你的看法](https://kexue.fm/kexue.fm#comment_form)

岁月如书

December 14th, 2025

muon怎么就丢了奇异值，奇异值用来做weight decay没有可行性么

[回复评论](https://kexue.fm/archives/11459/comment-page-1?replyTo=29006#respond-post-11459)

[取消回复](https://kexue.fm/archives/11459#respond-post-11459)

你的大名

电子邮箱

个人网站（选填）

1\. 可以使用LaTeX代码，点击“预览效果”可查看效果；2. 可以通过点击评论楼层编号来引用该楼层；3. 网站可能会有点卡，如非确认评论失败，请 **不要重复点击提交**。

### 内容速览

[滑动平均](https://kexue.fm/kexue.fm#%E6%BB%91%E5%8A%A8%E5%B9%B3%E5%9D%87)
[迭代展开](https://kexue.fm/kexue.fm#%E8%BF%AD%E4%BB%A3%E5%B1%95%E5%BC%80)
[记忆周期](https://kexue.fm/kexue.fm#%E8%AE%B0%E5%BF%86%E5%91%A8%E6%9C%9F)
[动态版本](https://kexue.fm/kexue.fm#%E5%8A%A8%E6%80%81%E7%89%88%E6%9C%AC)
[最优调度](https://kexue.fm/kexue.fm#%E6%9C%80%E4%BC%98%E8%B0%83%E5%BA%A6)
[一般结果](https://kexue.fm/kexue.fm#%E4%B8%80%E8%88%AC%E7%BB%93%E6%9E%9C)
[假设讨论](https://kexue.fm/kexue.fm#%E5%81%87%E8%AE%BE%E8%AE%A8%E8%AE%BA)
[文章小结](https://kexue.fm/kexue.fm#%E6%96%87%E7%AB%A0%E5%B0%8F%E7%BB%93)

### 智能搜索

支持整句搜索！网站自动使用 [结巴分词](https://github.com/fxsjy/jieba) 进行分词，并结合ngrams排序算法给出合理的搜索结果。

### 热门标签

[生成模型](https://kexue.fm/tag/%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B/) [attention](https://kexue.fm/tag/attention/) [优化](https://kexue.fm/tag/%E4%BC%98%E5%8C%96/) [语言模型](https://kexue.fm/tag/%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/) [模型](https://kexue.fm/tag/%E6%A8%A1%E5%9E%8B/) [网站](https://kexue.fm/tag/%E7%BD%91%E7%AB%99/) [梯度](https://kexue.fm/tag/%E6%A2%AF%E5%BA%A6/) [概率](https://kexue.fm/tag/%E6%A6%82%E7%8E%87/) [矩阵](https://kexue.fm/tag/%E7%9F%A9%E9%98%B5/) [转载](https://kexue.fm/tag/%E8%BD%AC%E8%BD%BD/) [优化器](https://kexue.fm/tag/%E4%BC%98%E5%8C%96%E5%99%A8/) [微分方程](https://kexue.fm/tag/%E5%BE%AE%E5%88%86%E6%96%B9%E7%A8%8B/) [分析](https://kexue.fm/tag/%E5%88%86%E6%9E%90/) [天象](https://kexue.fm/tag/%E5%A4%A9%E8%B1%A1/) [深度学习](https://kexue.fm/tag/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/) [积分](https://kexue.fm/tag/%E7%A7%AF%E5%88%86/) [python](https://kexue.fm/tag/python/) [扩散](https://kexue.fm/tag/%E6%89%A9%E6%95%A3/) [力学](https://kexue.fm/tag/%E5%8A%9B%E5%AD%A6/) [无监督](https://kexue.fm/tag/%E6%97%A0%E7%9B%91%E7%9D%A3/) [几何](https://kexue.fm/tag/%E5%87%A0%E4%BD%95/) [节日](https://kexue.fm/tag/%E8%8A%82%E6%97%A5/) [生活](https://kexue.fm/tag/%E7%94%9F%E6%B4%BB/) [文本生成](https://kexue.fm/tag/%E6%96%87%E6%9C%AC%E7%94%9F%E6%88%90/) [数论](https://kexue.fm/tag/%E6%95%B0%E8%AE%BA/)

### 随机文章

- [更别致的词向量模型(四)：模型的求解](https://kexue.fm/archives/4675)
- [再来一顿贺岁宴：从K-Means到Capsule](https://kexue.fm/archives/5112)
- [OCR技术浅探：9. 代码共享(完)](https://kexue.fm/archives/3856)
- [对称多项式不等式的“物理证明”](https://kexue.fm/archives/1460)
- [\[SETI-50周年\]茫茫宇宙觅知音](https://kexue.fm/archives/1205)
- [Cool Papers更新：简单适配Zotero Connector](https://kexue.fm/archives/11250)
- [【通知转载】国家天文台信息技术类人才招聘](https://kexue.fm/archives/557)
- [有限内存下全局打乱几百G文件（Python）](https://kexue.fm/archives/8662)
- [算子与线性常微分方程(上)](https://kexue.fm/archives/1791)
- [Transformer升级之路：5、作为无限维的线性Attention](https://kexue.fm/archives/8601)

### 最近评论

- [Yifan GUO](https://kexue.fm/archives/7546/comment-page-4#comment-29008): 我打脸了，写了代码快速验证了一下，softmax对应的efficient attn这样妙用so...
- [Yifan GUO](https://kexue.fm/archives/7546/comment-page-4#comment-29007): 《Efficient Attention: Attention with Linear Com...
- [岁月如书](https://kexue.fm/archives/11459/comment-page-1#comment-29006): muon怎么就丢了奇异值，奇异值用来做weight decay没有可行性么
- [Yifan GUO](https://kexue.fm/archives/11033/comment-page-3#comment-29005): Oh，我貌似理解了，或许我可以这样给自己解释：
如果分母的作用确实只是保持数值稳定性的话，那这...
- [Yifan GUO](https://kexue.fm/archives/11033/comment-page-3#comment-29004): "其中分母的作用主要是保持数值稳定性，另外就是如果我们给O加上RMSNorm，那么分母也会自动...
- [苏剑林](https://kexue.fm/archives/10739/comment-page-2#comment-29002): 嗯，类似的观点我们在 https://kexue.fm/archives/11126 也分享了。
- [苏剑林](https://kexue.fm/archives/5253/comment-page-2#comment-29001): KL散度希望$p(Z\|X)$的log\_var接近于0，但由于重构项的存在，无法达到这个目标，所...
- [苏剑林](https://kexue.fm/archives/5253/comment-page-18#comment-29000): $p(Z)$是标准正态分布，我们才能从标准正态分布中随机采样生成。
- [苏剑林](https://kexue.fm/archives/9181/comment-page-5#comment-28999): 原则上不必要
- [苏剑林](https://kexue.fm/archives/9181/comment-page-5#comment-28998): \[comment=28988\]xiaojx\[/comment\]适量的噪声其实有平滑作用，$\\s...

### 友情链接

- [Cool Papers](https://papers.cool)
- [数学研发](https://bbs.emath.ac.cn)
- [Seatop](http://www.seatop.com.cn/)
- [Xiaoxia](https://xiaoxia.org/)
- [积分表-网络版](https://kexue.fm/sci/integral/index.html)
- [丝路博傲](http://blog.dvxj.com/)
- [数学之家](http://www.2math.cn/)
- [有趣天文奇观](http://interesting-sky.china-vo.org/)
- [TwistedW](http://www.twistedwg.com/)
- [godweiyang](https://godweiyang.com/)
- [AI柠檬](https://blog.ailemon.net/)
- [王登科-DK博客](https://greatdk.com)
- [ESON](https://blog.eson.org/)
- [枫之羽](https://fzhiy.net/)
- [coding-zuo](https://coding-zuo.github.io/)
- [博科园](https://www.bokeyuan.net/)
- [孔皮皮的博客](https://www.kppkkp.top/)
- [运鹏的博客](https://yunpengtai.top/)
- [jiming.site](https://jiming.site/)
- [OmegaXYZ](https://www.omegaxyz.com/)
- [EAI猩球](https://www.robotech.ink/)
- [文举的博客](https://liwenju0.com/)
- [申请链接](https://kexue.fm/links.html)

本站采用创作共用版权协议，要求署名、非商业用途和保持一致。转载本站内容必须也遵循“ [署名-非商业用途-保持一致](http://creativecommons.org/licenses/by-nc-nd/2.5/cn/)”的创作共用协议。
© 2009-2025 Scientific Spaces. All rights reserved. Theme by [laogui](http://www.laogui.com). Powered by [Typecho](http://typecho.org). 备案号: [粤ICP备09093259号-1/2](https://beian.miit.gov.cn/)。