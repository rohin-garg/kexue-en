## SEARCH

## MENU

- [打赏](https://kexue.fm/reward.html)
- [公式](https://kexue.fm/latex.html)
- [天象](https://kexue.fm/ac.html)
- [链接](https://kexue.fm/links.html)
- [时光](https://kexue.fm/me.html)
- [博览](https://kexue.fm/science.html)
- [归档](https://kexue.fm/content.html)

## CATEGORIES

- [千奇百怪](https://kexue.fm/category/Everything)
- [天文探索](https://kexue.fm/category/Astronomy)
- [数学研究](https://kexue.fm/category/Mathematics)
- [物理化学](https://kexue.fm/category/Phy-chem)
- [信息时代](https://kexue.fm/category/Big-Data)
- [生物自然](https://kexue.fm/category/Biology)
- [图片摄影](https://kexue.fm/category/Photograph)
- [问题百科](https://kexue.fm/category/Questions)
- [生活/情感](https://kexue.fm/category/Life-Feeling)
- [资源共享](https://kexue.fm/category/Resources)

## NEWPOSTS

- [为什么Adam的Update RM...](https://kexue.fm/archives/11267)
- [重新思考学习率与Batch Siz...](https://kexue.fm/archives/11260)
- [Cool Papers更新：简单适...](https://kexue.fm/archives/11250)
- [流形上的最速下降：4\. Muon ...](https://kexue.fm/archives/11241)
- [ReLU/GeLU/Swish的一...](https://kexue.fm/archives/11233)
- [流形上的最速下降：3\. Muon ...](https://kexue.fm/archives/11221)
- [流形上的最速下降：2\. Muon ...](https://kexue.fm/archives/11215)
- [基于树莓派Zero2W搭建一个随身旁路由](https://kexue.fm/archives/11206)
- [流形上的最速下降：1\. SGD ...](https://kexue.fm/archives/11196)
- [矩阵r次方根和逆r次方根的高效计算](https://kexue.fm/archives/11175)

## COMMENTS

- [gapeng: kimi k2形式上推导了一个公式，最后数值模拟在0.23左右...](https://kexue.fm/archives/11267/comment-page-1#comment-28531)
- [Evan1024: 太牛了！](https://kexue.fm/archives/11267/comment-page-1#comment-28530)
- [ameowcat: 苏神您好，有个问题想请教一下，最近扩散模型的推理优化有一篇文章...](https://kexue.fm/archives/10958/comment-page-3#comment-28529)
- [Eliot: 2 实现loss-free with budget应当是在当前...](https://kexue.fm/archives/10815/comment-page-1#comment-28528)
- [Eliot: 继续阅读这2份代码后，大概结论如下\
1 megatron-lm...](https://kexue.fm/archives/10815/comment-page-1#comment-28527)
- [lzyyzl: 帮忙解惑一下\
1 文中提到本文主题是求O=msign(M)的导...](https://kexue.fm/archives/11025/comment-page-1#comment-28525)
- [z: 牛](https://kexue.fm/archives/11267/comment-page-1#comment-28524)
- [Gusto: 按照个人理解将weight decay理解为损失函数中的惩罚项...](https://kexue.fm/archives/10739/comment-page-2#comment-28523)
- [hazdzz: 非常感谢您提到 pbSGD，这启发了我的毕业论文！](https://kexue.fm/archives/11196/comment-page-1#comment-28522)
- [苏剑林: 你是指xml代码？那不是乱码，feed就是xml格式，你要自己...](https://kexue.fm/content.html/comment-page-1#comment-28521)

## USERLOGIN

- [登录](https://kexue.fm/admin/login.php)

[科学空间\|Scientific Spaces](https://kexue.fm)

- [登录](https://kexue.fm/admin/login.php)
- [打赏](https://kexue.fm/reward.html)
- [公式](https://kexue.fm/latex.html)
- [天象](https://kexue.fm/ac.html)
- [链接](https://kexue.fm/links.html)
- [时光](https://kexue.fm/me.html)
- [博览](https://kexue.fm/science.html)
- [归档](https://kexue.fm/content.html)

渴望成为一个小飞侠

- [欢迎订阅](https://kexue.fm/feed)
- [个性邮箱](https://kexue.fm/archives/119)
- [天象信息](https://kexue.fm/ac.html)
- [观测ISS](https://kexue.fm/archives/41)
- [LaTeX](https://kexue.fm/latex.html)
- [关于博主](https://kexue.fm/me.html)

欢迎访问“科学空间”，这里将与您共同探讨自然科学，回味人生百态；也期待大家的分享～

- [**千奇百怪** Everything](https://kexue.fm/category/Everything)
- [**天文探索** Astronomy](https://kexue.fm/category/Astronomy)
- [**数学研究** Mathematics](https://kexue.fm/category/Mathematics)
- [**物理化学** Phy-chem](https://kexue.fm/category/Phy-chem)
- [**信息时代** Big-Data](https://kexue.fm/category/Big-Data)
- [**生物自然** Biology](https://kexue.fm/category/Biology)
- [**图片摄影** Photograph](https://kexue.fm/category/Photograph)
- [**问题百科** Questions](https://kexue.fm/category/Questions)
- [**生活/情感** Life-Feeling](https://kexue.fm/category/Life-Feeling)
- [**资源共享** Resources](https://kexue.fm/category/Resources)

- [**千奇百怪**](https://kexue.fm/category/Everything)
- [**天文探索**](https://kexue.fm/category/Astronomy)
- [**数学研究**](https://kexue.fm/category/Mathematics)
- [**物理化学**](https://kexue.fm/category/Phy-chem)
- [**信息时代**](https://kexue.fm/category/Big-Data)
- [**生物自然**](https://kexue.fm/category/Biology)
- [**图片摄影**](https://kexue.fm/category/Photograph)
- [**问题百科**](https://kexue.fm/category/Questions)
- [**生活/情感**](https://kexue.fm/category/Life-Feeling)
- [**资源共享**](https://kexue.fm/category/Resources)

[首页](https://kexue.fm) [信息时代](https://kexue.fm/category/Big-Data) ChildTuning：试试把Dropout加到梯度上去？

22Nov

# [ChildTuning：试试把Dropout加到梯度上去？](https://kexue.fm/archives/8764)

By 苏剑林 \|
2021-11-22 \|
83295位读者\|

Dropout是经典的防止过拟合的思路了，想必很多读者已经了解过它。有意思的是，最近Dropout有点“老树发新芽”的感觉，出现了一些有趣的新玩法，比如最近引起过热议的 [SimCSE](https://kexue.fm/archives/8348) 和 [R-Drop](https://kexue.fm/archives/8496)，尤其是在文章 [《又是Dropout两次！这次它做到了有监督任务的SOTA》](https://kexue.fm/archives/8496) 中，我们发现简单的R-Drop甚至能媲美对抗训练，不得不说让人意外。

一般来说，Dropout是被加在每一层的输出中，或者是加在模型参数上，这是Dropout的两个经典用法。不过，最近笔者从论文 [《Raise a Child in Large Language Model: Towards Effective and Generalizable Fine-tuning》](https://papers.cool/arxiv/2109.05687) 中学到了一种新颖的用法：加到梯度上面。

梯度加上Dropout？相信大部分读者都是没听说过的。那么效果究竟如何呢？让我们来详细看看。

## 方法大意 [\#](https://kexue.fm/kexue.fm\#%E6%96%B9%E6%B3%95%E5%A4%A7%E6%84%8F)

简单来说，这篇论文主要提出了一种名为“ChildTuning”的思路，来提高预训练模型在finetune时的效果，其中“Child”是“Children Network”的意思，指的是从预训练模型中选择一个子网络进行优化，缓解优化整个模型所带来的过拟合风险。其中，在子网络的选择上，又分为两种方式：ChildTuning-D和ChildTuning-F。

### ChildTuning-D [\#](https://kexue.fm/kexue.fm\#ChildTuning-D)

ChildTuning-D（Task-Dependent）是任务相关的选择方式，它需要下游任务的训练数据来参与计算。具体来说，假设训练数据为$(x\_1,y\_1),(x\_2,y\_2),\\cdots,(x\_n,y\_n)$，模型为$p(y\|x;\\theta)$，其中$\\theta$是模型的所有参数，而$\\theta\_i$则是其中的第$i$个参数，那么我们计算如下形式的Fisher信息作为该参数的重要性：
\\begin{equation}F\_i = \\frac{1}{n}\\sum\_{j=1}^n \\left(\\frac{\\partial \\log p(y\_j\|x\_j;\\theta)}{\\partial\\theta\_i}\\right)^2\\end{equation}
有了重要性指标后，我们就可以对每个参数进行排序，然后选出最重要的top-$p$部分（比如前20%，即$p=0.2$），然后在模型更新的时候只优化这些参数。由于优化的参数变少了，所以过拟合的风险也就降低了。在实际使用时，ChildTuning-D在finetune之前就把要优化的参数确定下来，然后就一直保持不变了。

要注意的是，这里的参数选择是以每个分量为单位的，也就是说，可能一个参数矩阵里边，只有一部分被选择中，所以不能说单独挑出哪些参数矩阵不优化，而是要通过构建对应的0/1矩阵$M$来将对应的梯度mask掉，即$g\\leftarrow g\\otimes M / p$，其中除以$p$是保持整理的更新量级不变。这样没被选中的参数梯度一直是0，所以也就没有更新了。这样一来，虽然理论上更新的参数少了，但它也不能节约计算量，所以作者只是将它定位为提高finetune效果的方法。

### ChildTuning-F [\#](https://kexue.fm/kexue.fm\#ChildTuning-F)

ChildTuning-F（Task-Free）是任务无关的选择方式，其实它可以更形象地称为“梯度Dropout”。对于ChildTuning-D来说，我们就是根据任务数据来构建了固定的0/1矩阵$M$，然后将梯度修改为$g\\otimes M / p$，而ChildTuning-F既然希望与任务无关，那么它每步更新就随机构建一个0/1矩阵$M$，其中1的比例为$p$，然后将梯度修改为$g\\otimes M / p$。可以看到，这本质就是对梯度进行Dropout。

要注意，某个参数当前的梯度为0，也不代表该参数当前的更新量为0，因为我们通常用的都是带有动量的优化器如SGDM和Adam，对于此类优化器，更新量是正比于动量，而动量是历史梯度滑动平均过来的，即$m\_t = \\beta m\_{t-1} + (1-\\beta)g\_t$，所以如果该参数的历史梯度不为0，那么即便当前梯度为0，动量依然很可能不会为0，所以更新量也不为0。

所以在这里笔者就有个疑问，按照ChildTuning的设计思路，它应该是想要每步只选择一个子网络进行更新，说白了就是每一步只更新$p$比例的参数，但根据上面的分析，对梯度进行Dropout其实达不到这个目的，而要实现这个目的，应该要对每步更新量$\\Delta\\theta$进行Dropout才对。但笔者反复看了原论文，甚至对照了论文开源的代码，最终确认论文的意思确实是对梯度进行Dropout。

## 实验结果 [\#](https://kexue.fm/kexue.fm\#%E5%AE%9E%E9%AA%8C%E7%BB%93%E6%9E%9C)

从原论文给出的实验结果来看，ChildTuning的“战绩”是相当耀眼了，几乎都有提升，而且最高提升达到8%～

ChildTuning的“战绩”-1

ChildTuning的“战绩”-2

从表中可以看出，对于ChildTuning-D来说，几乎所有任务上都取得了提升，而ChildTuning-F也在不少任务上有效。另外，看论文描述可以知道上面给出的都是large版本模型的结果，而私下跟作者交流的时候，作者表示base版本的效果也有提升，只是限于论文篇幅，没有贴出来。

## 原理思考 [\#](https://kexue.fm/kexue.fm\#%E5%8E%9F%E7%90%86%E6%80%9D%E8%80%83)

ChildTuning-D基于Fisher信息来对进行参数排序，该思路由来已久，它有效并不让人意外，类似的工作还有 [《Training Neural Networks with Fixed Sparse Masks》](https://papers.cool/arxiv/2111.09839) 等。反倒是任务无关的ChildTuning-F，也就是梯度Dropout，居然也有这么效果，值得我们细细思考。

无独有偶，对梯度进行Dropout的工作，去年也有一篇，名为 [《Regularizing Meta-Learning via Gradient Dropout》](https://papers.cool/arxiv/2004.05859)。这表明，Gradient Dropout应该确实能起到一定效果的。那它究竟为什么有效呢？

### 论文推导 [\#](https://kexue.fm/kexue.fm\#%E8%AE%BA%E6%96%87%E6%8E%A8%E5%AF%BC)

原论文给出一个基于SGD的理解，它指出梯度Dropout能扩大更新过程中的方差，从而有助于模型逃离不那么好的局部最优点。

具体来说，因为我们是用了SGD，所以每步所计算的梯度有一定的随机性，假设它服从均值为$\\mu$、方差为$\\sigma^2$的高斯分布；对于ChildTuning-F来说，引入一个随机变量$\\varepsilon$，有$p$的概率为1，剩下$1-p$的概率为0。那么我们将有
\\begin{equation}\\begin{aligned}&\\mathbb{E}\[g\\varepsilon/p\]=\\mathbb{E}\[g\]\\mathbb{E}\[\\varepsilon\]/p=\\mu \\\
&\\mathbb{E}\[(g\\varepsilon/p)^2\]=\\mathbb{E}\[g^2\]\\mathbb{E}\[\\varepsilon^2\]/p^2 = (\\mu^2+\\sigma^2)/p
\\end{aligned}\\end{equation}
所以
\\begin{equation}\\mathbb{V}ar\[g\\varepsilon/p\] = \\mathbb{E}\[(g\\varepsilon/p)^2\] - \\mathbb{E}\[g\\varepsilon/p\]^2=\\sigma^2 + \\frac{1-p}{p}(\\mu^2+\\sigma^2) > \\sigma^2\\end{equation}
也就是说，梯度Dropout能保持梯度的均值不变，但能扩大方差，而在SGD中，更新量正比于梯度，因此梯度Dropout扩大了更新量的方差，论文认为这有助于模型达到更好的收敛结果。

### 答非所问 [\#](https://kexue.fm/kexue.fm\#%E7%AD%94%E9%9D%9E%E6%89%80%E9%97%AE)

这个解释看上去挺合理的，也符合很多人的直觉，因为很多人的潜意识里觉得随机梯度下降比全量梯度下降好的原因就是因为有噪声。然而，只要我们稍微深入思考一下，就能发现上述解释其实是“答非所问”。

原因很简单，上面分析的是SGD，但实际上在NLP中我们用的都是Adam（或者是它的变种），上述结论还能在Adam中保持吗？很遗憾，不能，甚至刚好相反。在Adam中，长期来看，更新量可以近似看成（$\\eta$是学习率）
\\begin{equation}\\Delta\\theta = \\eta\\frac{\\mathbb{E}\[g\]}{\\sqrt{\\mathbb{E}\[g^2\]}}\\end{equation}
于是加了梯度Dropout后，更新量变为
\\begin{equation}\\eta\\frac{\\mathbb{E}\[g\\varepsilon/p\]}{\\sqrt{\\mathbb{E}\[(g\\varepsilon/p)^2\]}}=\\eta\\sqrt{p}\\frac{\\mathbb{E}\[g\]}{\\sqrt{\\mathbb{E}\[g^2\]}}\\end{equation}
可以看到，长期来看，Adam加上梯度Dropout后，仅仅相当于学习率降低为原来的$\\sqrt{p}$倍！而且由于降低了学习率，也即降低了更新量，从而更新量的方差也随之降低。也就是说，如果你用了Adam优化器，那么实际情况跟论文的解释刚好相反，更新量的方差不仅没有增加，反而是降低了。

出现这个现象的根本原因就是，当我们使用了带有滑动平均的优化器时，更新量通常已经不在正比于梯度了，所以梯度如何变化，跟更新量如何变化，并没有必然的关联。这就回到了笔者前面的疑问了：为什么作者不干脆直接对更新量进行Dropout？如果是更新量Dropout，那么前面基于SGD的推导也能搬过来了。

### 个人理解 [\#](https://kexue.fm/kexue.fm\#%E4%B8%AA%E4%BA%BA%E7%90%86%E8%A7%A3)

不过，笔者认为，就算把优化器限制为SGD，或者直接对更新量进行Dropout，原论文的推导也不能完全解释它的有效性。理由也很简单，能够达到“均值不变、方差扩大”的操作太多了，比如直接往梯度里边加点高斯噪声也可以，难道所有的这些操作都能达到同样的效果？个人感觉不大可能。笔者认为，要解释梯度Dropout或者更新量Dropout的有效性，得着眼于Dropout带来的稀疏性。

在这个问题上，笔者联想到了之前写过的文章 [《从动力学角度看优化算法（七）：SGD ≈ SVM？》](https://kexue.fm/archives/8009)，这篇文章告诉我们，所有SGD出来的模型，其解本质上都类似于SVM模型：
\\begin{equation}f\_{\\theta\_T}(x) = \\beta(x) + \\sum\_i \\alpha\_i (x) K(x, x\_i)\\end{equation}
其中$x\_i$是第$i$个训练样本。它有什么特点呢？$K(x,x\_i)$的表现类似一个“相似度函数”，上述形式意味着模型实际上会以某种形式把训练集“背”下来了，然后预测的时候会以$K(x,x\_i)$为相似度取检索训练集，然后给出预测结果。当然，这只是一个原理性的解释，我们并不是主动将模型设计成这样的形式，我们只是从这个角度看出，梯度下降实际上也是在背样本，然后以类似于KNN的形式给出预测结果，这就不难理解为什么通常来说“训练样本越多，效果越好”的结论了。

回到ChildTuning-F上，我们每次采样一个batch，然后对算出来的梯度或更新量进行Dropout，结合上面的“背样本”解释，我们可以直观地想象，这本质上就是“只用一小部分参数来背诵一小部分样本”，而不是每次都要用全体参数来背诵那一小批样本。所以，这跟“不要将鸡蛋放在同一个篮子里”应该是相似的，将样本更均匀分散在每一个参数中，从而降低了过拟合风险。

## 尝试一下 [\#](https://kexue.fm/kexue.fm\#%E5%B0%9D%E8%AF%95%E4%B8%80%E4%B8%8B)

对于ChildTuning-F来说，如果自己懂得改优化器的话，不管是对梯度Dropout还是对更新量Dropout，都只是一行代码的工作量，因此还是值得尝试一下的。万一真的有用呢？

这里笔者在CLUE的几个任务上做了测试，结果如下表。其中，baseline代码来自 [《bert4keras在手，baseline我有：CLUE基准代码》](https://kexue.fm/archives/8739)，“grad drop”是对梯度进行Dropout，“incre drop”是对更新量进行Dropout，绿色表示相比baseline有提升，红色则表示下降。时间算力都有限，所有结果都只跑了一次，存在一定的随机波动。

$$\\begin{array}{c}
\\text{CLUE分类任务对比实验（验证集）} \\\
{\\begin{array}{c\|ccccccc}
\\hline
& \\text{IFLYTEK} & \\text{TNEWS} & \\text{AFQMC} & \\text{OCNLI} & \\text{CMNLI} & \\text{WSC} & \\text{CSL} \\\
\\hline
\\text{BERT} & 60.06 & 56.80 & 72.41 & 73.93 & 79.56 & 78.62 & 83.93 \\\
\\text{BERT}\_{\\text{-grad drop}} & \\color{green}{60.56} & \\color{green}{56.97} & \\color{red}{72.13} & \\color{green}{74.88} & \\color{green}{80.09} & \\color{red}{75.99} & \\color{red}{83.83} \\\
\\text{BERT}\_{\\text{-incre drop}} & \\color{red}{59.99} & \\color{red}{56.78} & \\color{green}{72.66} & \\color{green}{74.51} & \\color{red}{79.36} & \\color{red}{77.30} & \\color{green}{84.20} \\\
\\hline
\\text{RoBERTa} & 60.64 & 58.06 & 74.05 & 76.00 & 81.24 & 87.50 & 84.50\\\
\\text{RoBERTa}\_{\\text{-grad drop}} & \\color{green}{60.72} & \\color{red}{57.91} & \\color{red}{74.03} & \\color{red}{75.19} & \\color{red}{80.52} & \\color{red}{84.54} & \\color{green}{84.73}\\\
\\text{RoBERTa}\_{\\text{-incre drop}} & \\color{green}{60.87} & \\color{red}{57.99} & \\color{red}{74.03} & \\color{red}{75.97} & \\color{red}{81.02} & \\color{red}{84.87} & \\color{green}{84.73}\\\
\\hline
\\end{array}}
\\end{array}$$

从表格中，我们大致可以看出：

> 1、对梯度Dropout和对更新量进行Dropout，大致上各有优劣；
>
> 2、在BERT上的效果明显一些，在RoBERTa上的效果几乎没有，这跟论文给出的英文实验结果相似。

这结果挺让人无语的，不能说它没效，但正常来说，谁会用速度一样、效果更差的BERT而不用效果更好的RoBERTa呢？那么，如果RoBERTa不怎么work的话，似乎就没啥尝试的价值了？当然，原论文提升最大的是Electra，这个我没尝试过，有兴趣的读者尝试了把结果告诉我一下哈。

另外，笔者对ChildTuning-D没有特别的兴趣，加上ChildTuning-D的实现稍微复杂一点，所以也就没有实验ChildTuning-D了，实验过的读者也欢迎反馈结果哈。

## 文章总结 [\#](https://kexue.fm/kexue.fm\#%E6%96%87%E7%AB%A0%E6%80%BB%E7%BB%93)

本文介绍了往梯度里边加入Dropout来提高finetune效果的做法，并给出了自己的理论分析。总的来说，个人的感觉是：可以尝试，可能有效，但不要期望太高～

_**转载到请包括本文地址：** [https://kexue.fm/archives/8764](https://kexue.fm/archives/8764)_

_**更详细的转载事宜请参考：**_ [《科学空间FAQ》](https://kexue.fm/archives/6508#%E6%96%87%E7%AB%A0%E5%A6%82%E4%BD%95%E8%BD%AC%E8%BD%BD/%E5%BC%95%E7%94%A8)

**如果您还有什么疑惑或建议，欢迎在下方评论区继续讨论。**

**如果您觉得本文还不错，欢迎 [分享](https://kexue.fm/kexue.fm#share)/ [打赏](https://kexue.fm/kexue.fm#pay) 本文。打赏并非要从中获得收益，而是希望知道科学空间获得了多少读者的真心关注。当然，如果你无视它，也不会影响你的阅读。再次表示欢迎和感谢！**

打赏

微信打赏

支付宝打赏

因为网站后台对打赏并无记录，因此欢迎在打赏时候备注留言。你还可以 [**点击这里**](http://mail.qq.com/cgi-bin/qm_share?t=qm_mailme&email=tN7d1drY3drrx8H0xcWa19vZ) 或在下方评论区留言来告知你的建议或需求。

**如果您需要引用本文，请参考：**

苏剑林. (Nov. 22, 2021). 《ChildTuning：试试把Dropout加到梯度上去？ 》\[Blog post\]. Retrieved from [https://kexue.fm/archives/8764](https://kexue.fm/archives/8764)

@online{kexuefm-8764,
        title={ChildTuning：试试把Dropout加到梯度上去？},
        author={苏剑林},
        year={2021},
        month={Nov},
        url={\\url{https://kexue.fm/archives/8764}},
}

分类： [信息时代](https://kexue.fm/category/Big-Data)    标签： [模型](https://kexue.fm/tag/%E6%A8%A1%E5%9E%8B/), [优化](https://kexue.fm/tag/%E4%BC%98%E5%8C%96/), [梯度](https://kexue.fm/tag/%E6%A2%AF%E5%BA%A6/)[21 评论](https://kexue.fm/archives/8764#comments)

< [WGAN新方案：通过梯度归一化来实现L约束](https://kexue.fm/archives/8757) \| [Dropout视角下的MLM和MAE：一些新的启发](https://kexue.fm/archives/8770) >

### 你也许还对下面的内容感兴趣

- [为什么Adam的Update RMS是0.2？](https://kexue.fm/archives/11267)
- [重新思考学习率与Batch Size的关系（一）：现状](https://kexue.fm/archives/11260)
- [QK-Clip：让Muon在Scaleup之路上更进一步](https://kexue.fm/archives/11126)
- [Transformer升级之路：21、MLA好在哪里?（下）](https://kexue.fm/archives/11111)
- [msign的导数](https://kexue.fm/archives/11025)
- [MoE环游记：5、均匀分布的反思](https://kexue.fm/archives/10945)
- [Transformer升级之路：20、MLA好在哪里?（上）](https://kexue.fm/archives/10907)
- [SVD的导数](https://kexue.fm/archives/10878)
- [通过梯度近似寻找Normalization的替代品](https://kexue.fm/archives/10831)
- [MoE环游记：4、难处应当多投入](https://kexue.fm/archives/10815)

[发表你的看法](https://kexue.fm/kexue.fm#comment_form)

1. [«](https://kexue.fm/archives/8764/comment-page-1#comments)
2. [1](https://kexue.fm/archives/8764/comment-page-1#comments)
3. [2](https://kexue.fm/archives/8764/comment-page-2#comments)

qianmo

August 16th, 2022

想请教一下，为什么讨论ChildTuning-F的时候要除p（gε/p）不应该是gε吗

[回复评论](https://kexue.fm/archives/8764/comment-page-2?replyTo=19633#respond-post-8764)

[苏剑林](https://kexue.fm) 发表于
August 17th, 2022

因为Dropout的定义就是要除以$p$的。

[回复评论](https://kexue.fm/archives/8764/comment-page-2?replyTo=19636#respond-post-8764)

1. [«](https://kexue.fm/archives/8764/comment-page-1#comments)
2. [1](https://kexue.fm/archives/8764/comment-page-1#comments)
3. [2](https://kexue.fm/archives/8764/comment-page-2#comments)

[取消回复](https://kexue.fm/archives/8764#respond-post-8764)

你的大名

电子邮箱

个人网站（选填）

1\. 可以使用LaTeX代码，点击“预览效果”可查看效果；2. 可以通过点击评论楼层编号来引用该楼层；3. 网站可能会有点卡，如非确认评论失败，请 **不要重复点击提交**。

### 内容速览

[方法大意](https://kexue.fm/kexue.fm#%E6%96%B9%E6%B3%95%E5%A4%A7%E6%84%8F)
[ChildTuning-D](https://kexue.fm/kexue.fm#ChildTuning-D)
[ChildTuning-F](https://kexue.fm/kexue.fm#ChildTuning-F)
[实验结果](https://kexue.fm/kexue.fm#%E5%AE%9E%E9%AA%8C%E7%BB%93%E6%9E%9C)
[原理思考](https://kexue.fm/kexue.fm#%E5%8E%9F%E7%90%86%E6%80%9D%E8%80%83)
[论文推导](https://kexue.fm/kexue.fm#%E8%AE%BA%E6%96%87%E6%8E%A8%E5%AF%BC)
[答非所问](https://kexue.fm/kexue.fm#%E7%AD%94%E9%9D%9E%E6%89%80%E9%97%AE)
[个人理解](https://kexue.fm/kexue.fm#%E4%B8%AA%E4%BA%BA%E7%90%86%E8%A7%A3)
[尝试一下](https://kexue.fm/kexue.fm#%E5%B0%9D%E8%AF%95%E4%B8%80%E4%B8%8B)
[文章总结](https://kexue.fm/kexue.fm#%E6%96%87%E7%AB%A0%E6%80%BB%E7%BB%93)

### 智能搜索

支持整句搜索！网站自动使用 [结巴分词](https://github.com/fxsjy/jieba) 进行分词，并结合ngrams排序算法给出合理的搜索结果。

### 热门标签

[生成模型](https://kexue.fm/tag/%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B/) [attention](https://kexue.fm/tag/attention/) [优化](https://kexue.fm/tag/%E4%BC%98%E5%8C%96/) [语言模型](https://kexue.fm/tag/%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/) [模型](https://kexue.fm/tag/%E6%A8%A1%E5%9E%8B/) [网站](https://kexue.fm/tag/%E7%BD%91%E7%AB%99/) [概率](https://kexue.fm/tag/%E6%A6%82%E7%8E%87/) [梯度](https://kexue.fm/tag/%E6%A2%AF%E5%BA%A6/) [转载](https://kexue.fm/tag/%E8%BD%AC%E8%BD%BD/) [矩阵](https://kexue.fm/tag/%E7%9F%A9%E9%98%B5/) [微分方程](https://kexue.fm/tag/%E5%BE%AE%E5%88%86%E6%96%B9%E7%A8%8B/) [分析](https://kexue.fm/tag/%E5%88%86%E6%9E%90/) [优化器](https://kexue.fm/tag/%E4%BC%98%E5%8C%96%E5%99%A8/) [天象](https://kexue.fm/tag/%E5%A4%A9%E8%B1%A1/) [深度学习](https://kexue.fm/tag/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/) [积分](https://kexue.fm/tag/%E7%A7%AF%E5%88%86/) [python](https://kexue.fm/tag/python/) [力学](https://kexue.fm/tag/%E5%8A%9B%E5%AD%A6/) [无监督](https://kexue.fm/tag/%E6%97%A0%E7%9B%91%E7%9D%A3/) [扩散](https://kexue.fm/tag/%E6%89%A9%E6%95%A3/) [几何](https://kexue.fm/tag/%E5%87%A0%E4%BD%95/) [节日](https://kexue.fm/tag/%E8%8A%82%E6%97%A5/) [生活](https://kexue.fm/tag/%E7%94%9F%E6%B4%BB/) [文本生成](https://kexue.fm/tag/%E6%96%87%E6%9C%AC%E7%94%9F%E6%88%90/) [数论](https://kexue.fm/tag/%E6%95%B0%E8%AE%BA/)

### 随机文章

- [如何“扒”站？手把手教你爬百度百科～](https://kexue.fm/archives/4385)
- [两千美元的汽车----世界最便宜汽车在印度上路!](https://kexue.fm/archives/27)
- [2010年全年天象](https://kexue.fm/archives/1144)
- [趣题：如何编程列出一个集合的所有子集](https://kexue.fm/archives/3641)
- [备忘：椭圆坐标与复三角函数](https://kexue.fm/archives/1314)
- [又折腾数学公式插件了](https://kexue.fm/archives/1866)
- [如何看费曼的讲义和朗道的教程？](https://kexue.fm/archives/2498)
- [用RecomposIt简单给图片换背景](https://kexue.fm/archives/622)
- [蘑菇的最优形状模型](https://kexue.fm/archives/1339)
- [解答不等式的误区...](https://kexue.fm/archives/644)

### 最近评论

- [gapeng](https://kexue.fm/archives/11267/comment-page-1#comment-28531): kimi k2形式上推导了一个公式，最后数值模拟在0.23左右。偏离$\\beta\_1=0.9,...
- [Evan1024](https://kexue.fm/archives/11267/comment-page-1#comment-28530): 太牛了！
- [ameowcat](https://kexue.fm/archives/10958/comment-page-3#comment-28529): 苏神您好，有个问题想请教一下，最近扩散模型的推理优化有一篇文章也是使用ode：https://...
- [Eliot](https://kexue.fm/archives/10815/comment-page-1#comment-28528): 2 实现loss-free with budget应当是在当前Megatron-LM基础上应当...
- [Eliot](https://kexue.fm/archives/10815/comment-page-1#comment-28527): 继续阅读这2份代码后，大概结论如下
1 megatron-lm应当只实现了经典的loss-fr...
- [lzyyzl](https://kexue.fm/archives/11025/comment-page-1#comment-28525): 帮忙解惑一下
1 文中提到本文主题是求O=msign(M)的导数。将∇ML表示为∇OL的函数也...
- [z](https://kexue.fm/archives/11267/comment-page-1#comment-28524): 牛
- [Gusto](https://kexue.fm/archives/10739/comment-page-2#comment-28523): 按照个人理解将weight decay理解为损失函数中的惩罚项的话，为什么weight dec...
- [hazdzz](https://kexue.fm/archives/11196/comment-page-1#comment-28522): 非常感谢您提到 pbSGD，这启发了我的毕业论文！
- [苏剑林](https://kexue.fm/content.html/comment-page-1#comment-28521): 你是指xml代码？那不是乱码，feed就是xml格式，你要自己找工具订阅。

### 友情链接

- [Cool Papers](https://papers.cool)
- [数学研发](https://bbs.emath.ac.cn)
- [Seatop](http://www.seatop.com.cn/)
- [Xiaoxia](https://xiaoxia.org/)
- [积分表-网络版](https://kexue.fm/sci/integral/index.html)
- [丝路博傲](http://blog.dvxj.com/)
- [数学之家](http://www.2math.cn/)
- [有趣天文奇观](http://interesting-sky.china-vo.org/)
- [TwistedW](http://www.twistedwg.com/)
- [godweiyang](https://godweiyang.com/)
- [AI柠檬](https://blog.ailemon.net/)
- [王登科-DK博客](https://greatdk.com)
- [ESON](https://blog.eson.org/)
- [枫之羽](https://fzhiy.net/)
- [Mathor's blog](https://wmathor.com/)
- [coding-zuo](https://coding-zuo.github.io/)
- [博科园](https://www.bokeyuan.net/)
- [孔皮皮的博客](https://www.kppkkp.top/)
- [运鹏的博客](https://yunpengtai.top/)
- [jiming.site](https://jiming.site/)
- [OmegaXYZ](https://www.omegaxyz.com/)
- [EAI猩球](https://www.robotech.ink/)
- [文举的博客](https://liwenju0.com/)
- [用代码打点酱油](https://bruceyuan.com/)
- [Zhang's blog](https://armcvai.cn/)
- [申请链接](https://kexue.fm/links.html)

本站采用创作共用版权协议，要求署名、非商业用途和保持一致。转载本站内容必须也遵循“ [署名-非商业用途-保持一致](http://creativecommons.org/licenses/by-nc-nd/2.5/cn/)”的创作共用协议。
© 2009-2025 Scientific Spaces. All rights reserved. Theme by [laogui](http://www.laogui.com). Powered by [Typecho](http://typecho.org). 备案号: [粤ICP备09093259号-1/2](https://beian.miit.gov.cn/)。