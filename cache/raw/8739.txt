## SEARCH

## MENU

- [打赏](https://kexue.fm/reward.html)
- [公式](https://kexue.fm/latex.html)
- [天象](https://kexue.fm/ac.html)
- [链接](https://kexue.fm/links.html)
- [时光](https://kexue.fm/me.html)
- [博览](https://kexue.fm/science.html)
- [归档](https://kexue.fm/content.html)

## CATEGORIES

- [千奇百怪](https://kexue.fm/category/Everything)
- [天文探索](https://kexue.fm/category/Astronomy)
- [数学研究](https://kexue.fm/category/Mathematics)
- [物理化学](https://kexue.fm/category/Phy-chem)
- [信息时代](https://kexue.fm/category/Big-Data)
- [生物自然](https://kexue.fm/category/Biology)
- [图片摄影](https://kexue.fm/category/Photograph)
- [问题百科](https://kexue.fm/category/Questions)
- [生活/情感](https://kexue.fm/category/Life-Feeling)
- [资源共享](https://kexue.fm/category/Resources)

## NEWPOSTS

- [MuP之上：1. 好模型的三个特征](https://kexue.fm/archives/11340)
- [随机矩阵的谱范数的快速估计](https://kexue.fm/archives/11335)
- [DiVeQ：一种非常简洁的VQ训练方案](https://kexue.fm/archives/11328)
- [为什么线性注意力要加Short C...](https://kexue.fm/archives/11320)
- [AdamW的Weight RMS的...](https://kexue.fm/archives/11307)
- [重新思考学习率与Batch Siz...](https://kexue.fm/archives/11301)
- [重新思考学习率与Batch Siz...](https://kexue.fm/archives/11285)
- [重新思考学习率与Batch Siz...](https://kexue.fm/archives/11280)
- [为什么Adam的Update RM...](https://kexue.fm/archives/11267)
- [重新思考学习率与Batch Siz...](https://kexue.fm/archives/11260)

## COMMENTS

- [苏剑林: 精度也是一个视角，但感觉这个事情感觉得仔细分析一下，因为理论上...](https://kexue.fm/archives/11340/comment-page-1#comment-28680)
- [苏剑林: 可以这样理解：$t$时刻的$\\boldsymbol{x}\_t$...](https://kexue.fm/archives/9257/comment-page-4#comment-28679)
- [苏剑林: 没有太多技巧了，就是直接代入然后根据$\\bar{\\alpha}...](https://kexue.fm/archives/9181/comment-page-5#comment-28678)
- [苏剑林: 完全懵了...WukT是什么？如果代表C到key的投影矩阵，那...](https://kexue.fm/archives/10862/comment-page-1#comment-28677)
- [Zhan-Wang Mao: 苏老师，请教一下(4)式的泰勒展开式为什么严格来说和$t$有关...](https://kexue.fm/archives/9257/comment-page-4#comment-28676)
- [yzlnew: 可以相呼应的是，这样的好模型能被浮点数以误差比较低的方式表示和...](https://kexue.fm/archives/11340/comment-page-1#comment-28675)
- [Henry: 想请问苏老师，方程7是如何推导到方程10的，是否有化简的一些小技巧？](https://kexue.fm/archives/9181/comment-page-5#comment-28673)
- [szsheep: 牛啊，还可以从这方面推出loss的函数最终式。原本是从KL散度...](https://kexue.fm/archives/9119/comment-page-13#comment-28672)
- [pang: 对于目前的MLA算法softmax(X×WQ×WukT×CjT...](https://kexue.fm/archives/10862/comment-page-1#comment-28671)
- [苏剑林: 你是说 chatglm2-6b 里边的？那个没用，预设的常数是...](https://kexue.fm/archives/11126/comment-page-3#comment-28670)

## USERLOGIN

- [登录](https://kexue.fm/admin/login.php)

[科学空间\|Scientific Spaces](https://kexue.fm)

- [登录](https://kexue.fm/admin/login.php)
- [打赏](https://kexue.fm/reward.html)
- [公式](https://kexue.fm/latex.html)
- [天象](https://kexue.fm/ac.html)
- [链接](https://kexue.fm/links.html)
- [时光](https://kexue.fm/me.html)
- [博览](https://kexue.fm/science.html)
- [归档](https://kexue.fm/content.html)

渴望成为一个小飞侠

- [欢迎订阅](https://kexue.fm/feed)
- [个性邮箱](https://kexue.fm/archives/119)
- [天象信息](https://kexue.fm/ac.html)
- [观测ISS](https://kexue.fm/archives/41)
- [LaTeX](https://kexue.fm/latex.html)
- [关于博主](https://kexue.fm/me.html)

欢迎访问“科学空间”，这里将与您共同探讨自然科学，回味人生百态；也期待大家的分享～

- [**千奇百怪** Everything](https://kexue.fm/category/Everything)
- [**天文探索** Astronomy](https://kexue.fm/category/Astronomy)
- [**数学研究** Mathematics](https://kexue.fm/category/Mathematics)
- [**物理化学** Phy-chem](https://kexue.fm/category/Phy-chem)
- [**信息时代** Big-Data](https://kexue.fm/category/Big-Data)
- [**生物自然** Biology](https://kexue.fm/category/Biology)
- [**图片摄影** Photograph](https://kexue.fm/category/Photograph)
- [**问题百科** Questions](https://kexue.fm/category/Questions)
- [**生活/情感** Life-Feeling](https://kexue.fm/category/Life-Feeling)
- [**资源共享** Resources](https://kexue.fm/category/Resources)

- [**千奇百怪**](https://kexue.fm/category/Everything)
- [**天文探索**](https://kexue.fm/category/Astronomy)
- [**数学研究**](https://kexue.fm/category/Mathematics)
- [**物理化学**](https://kexue.fm/category/Phy-chem)
- [**信息时代**](https://kexue.fm/category/Big-Data)
- [**生物自然**](https://kexue.fm/category/Biology)
- [**图片摄影**](https://kexue.fm/category/Photograph)
- [**问题百科**](https://kexue.fm/category/Questions)
- [**生活/情感**](https://kexue.fm/category/Life-Feeling)
- [**资源共享**](https://kexue.fm/category/Resources)

[首页](https://kexue.fm) [信息时代](https://kexue.fm/category/Big-Data) bert4keras在手，baseline我有：CLUE基准代码

31Oct

# [bert4keras在手，baseline我有：CLUE基准代码](https://kexue.fm/archives/8739)

By 苏剑林 \|
2021-10-31 \|
102127位读者\|

[CLUE（Chinese GLUE）](https://www.cluebenchmarks.com/) 是中文自然语言处理的一个评价基准，目前也已经得到了较多团队的认可。CLUE官方Github提供了tensorflow和pytorch的baseline，但并不易读，而且也不方便调试。事实上，不管是tensorflow还是pytorch，不管是CLUE还是GLUE，笔者认为能找到的baseline代码，都很难称得上人性化，试图去理解它们是一件相当痛苦的事情。

所以，笔者决定基于bert4keras实现一套CLUE的baseline。经过一段时间的测试，基本上复现了官方宣称的基准成绩，并且有些任务还更优。最重要的是，所有代码尽量保持了清晰易读的特点，真·“Deep Learning for Humans”。

> **代码链接： [https://github.com/bojone/CLUE-bert4keras](https://github.com/bojone/CLUE-bert4keras)**

## 代码简介 [\#](https://kexue.fm/kexue.fm\#%E4%BB%A3%E7%A0%81%E7%AE%80%E4%BB%8B)

下面简单介绍一下该代码中各个任务baseline的构建思路。在阅读文章和代码之前，请读者自行先观察一下每个任务的数据格式，这里不对任务数据进行详细介绍。

### 文本分类 [\#](https://kexue.fm/kexue.fm\#%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB)

首先是IFLYTEK和TNEWS两个任务，它们是普通的文本分类问题，所以做法很简单，就是常规的“\[CLS\]+句子+\[SEP\]“传入到BERT中，然后取出\[CLS\]的hidden向量做分类就行。

文本分类模型示意图

另外，代词消歧任务WSC也可以转化为单文本分类任务，原任务是判断一个句子中的两个片段（其中一个是代词）是否指代同一个对象，baseline的做法是在文本中用不同的符号标记出这两个片段，然后就直接将这个标记后的文本传入BERT进行二分类。

### 文本匹配 [\#](https://kexue.fm/kexue.fm\#%E6%96%87%E6%9C%AC%E5%8C%B9%E9%85%8D)

接下来，AFQMC、CMNLI、OCNLI是三个文本匹配任务。所谓文本匹配，简单理解就是句子对的分类任务，比如相似匹配是判断两个句子是否相似；自然语言推理是判断两个句子之间的逻辑关系（蕴含、中立、矛盾）等。在预训练时代，句子匹配任务的标准做法就是将两个句子用\[SEP\]连接起来，然后当成单文本分类任务来做。

文本匹配模型示意图

需要指出的是，在原始的BERT中，两个句子的SegmentID（原始代码叫做token type id）是不一样的，但这里考虑到像RoBERTa这样的模型没有NSP任务，编号为1的SegmentID可能没有被预训练过，所以这里的实现中SegmentID都是用全0。实验结果显示，这样处理并不会降低文本匹配的效果。

类似地，CSL这个任务，是判断摘要描述与所给的4个关键词是否匹配，我们将4个关键词用分号“；”连接起来，作为一个句子对待，这样也就转换成了常规的文本匹配问题了。

### 阅读理解 [\#](https://kexue.fm/kexue.fm\#%E9%98%85%E8%AF%BB%E7%90%86%E8%A7%A3)

阅读理解是指CMRC2018任务，这是一个格式跟SQUAD一样的抽取式阅读理解任务，一个段落会配有多个问题，每个问题必然有答案并且答案是段落的一个片段。一般的做法是将问题与段落用\[SEP\]拼接之后传入到BERT中，然后用两个全连接层分别预测首尾的位置。这样做的问题是割裂了首尾之间的联系，而且使得训练和预测的行为不一致。

这里的baseline使用GlobalPointer作为输出结构，它将首尾组合作为一个整体来进行分类，具体细节可以看 [《GlobalPointer：用统一的方式处理嵌套和非嵌套NER》](https://kexue.fm/archives/8373)。使用GlobalPointer能使得训练和预测的行为完全一致，并明显提高解码速度。

抽取式阅读理解模型示意图

此外，不管是SQUAD还是CMRC2018，段落长度多数是明显超过512的，并且有些问题的答案确实在比较后的位置，直接截断前面的部分可能就没有答案了。如果用NEZHA、RoFormer这样的模型，可以直接处理超过512的文本，但像BERT这样的模型就不好处理。为了保持代码的通用性，这里沿用了BERT原始basleine的滑窗设计，即以128为步长将段落分割为多个子段落，每个段落逐一与问题组合传入到模型中。这样分割之后，那么就允许某些段落对于问题来说是“无答案”的，这时候直接将答案指向\[CLS\]位置$(0,0)$。在预测阶段，长段落也是用同样的方法进行分割，然后逐一回答同一个问题，最后取分数最高的答案。

### 单项选择 [\#](https://kexue.fm/kexue.fm\#%E5%8D%95%E9%A1%B9%E9%80%89%E6%8B%A9)

这里的单项选择指的是C3任务，它也算是一种阅读理解任务，同样是一个段落提了多个问题，问题的答案是4个所给的候选答案之一，但不一定是段落中的片段。这种多项选择的baseline做法可能会出乎很多人的意料，它相当于转化为文本匹配问题，将每个候选答案与段落、问题进行匹配，然后预测时取分数最高的那个。

单项选择模型示意图

这样一来，原来的一个问题就需要拆分为4个样本来处理，需要预测4次才能做出答案，大大增加了计算量。但让人惊奇的是，这种做法是基本是所有直观想到的baseline中效果最好的，比将所有候选答案拼在一起然后做4分类要好得多。英文领域类似的任务是 [DREAM](https://dataset.org/dream/)，其榜单上的模型基本上都是这个思路的变种。

### 成语理解 [\#](https://kexue.fm/kexue.fm\#%E6%88%90%E8%AF%AD%E7%90%86%E8%A7%A3)

成语阅读理解任务CHID，本质上也是一个单项选择阅读理解问题，但它形式上复杂不少，所以单独拿出来介绍。

具体来说，CHID的每个样本有10个候选成语，以及由若干道题目，每道题目有若干个空位，我们就是要决定这些空位最适合填入哪个候选成语。如果每道题只有一个空位，那么就直接套用上一节的单项选择做法就行了；但这里每道题是可能有多个空位的，而用单项选择的做法，每次只能识别一个空位，所以我们用\[unused0\]代替我们要识别的空位，而没被识别的空位（如果有的话）直接用4个\[MASK\]代替，比如：

> \[CLS\] “这其实是个荒唐的闹剧，苹果发现iPad大陆商标的拥有人不属于台湾唯冠而是深圳唯冠后，开始着急了并 \[unused1\] 。”肖才元表示。事实上，两个戏剧性的因素让该案更显得 \[MASK\]\[MASK\]\[MASK\]\[MASK\] 。苹果在香港法院提起的诉讼案件中，所提交的材料显示，IPADL公司实为苹果公司律师操作下成立的具有特殊目的的，旨在用于收购唯冠手中i－Pad商标权的公司。 \[SEP\] 一锤定音 \[SEP\]

也就是说，一道有多个空位的题目将会被拆开为多道小题，而按照前述单项选择的做法，每道小题都需要跟候选答案拼接来预测，所以每道小题的计算成本都相当于普通分类的10个样本了，这确实有点费劲，但为了效果没办法了。为了达到更大的batch\_size效果，通常需要用到梯度累积。另外，有些题目还是比较长的，我们仍然需要截断，截断的方式是以当前要识别的空位为中心，尽量向左向右都延伸同样的距离。

最后，根据题目的设计，每个样本有若干道题目，每道题目的每个空位都共用10个候选成语，但每个空位的答案是不会重复的。如果预测的时候每个空位直接独立地取最大值的答案，那么就可能出现重复的预测结果，与问题设计相违背。

为了使得预测结果不重复，我们需要用到“匈牙利算法”：假设有$m$个空位，每个空位有$n > m$个候选答案，那么我们将得到$m\\times n$的打分句子，我们要为每个空位选择不一样的答案，并且使得总分最大，这在数学上被称为“指派问题”，标准解法就是“匈牙利算法”，我们直接用 `scipy.optimize.linear_sum_assignment` 求解就行了。这样的后处理算法比直接逐项取最大（可能导致重复答案）能提升6%左右的准确率。

### 实体识别 [\#](https://kexue.fm/kexue.fm\#%E5%AE%9E%E4%BD%93%E8%AF%86%E5%88%AB)

最后一个任务是CLUENER，常规的非嵌套命名实体识别任务。常见的baseline是BERT+Softmax或者BERT+CRF，这里用的则是BERT+GlobalPointer，同样可以参考 [《GlobalPointer：用统一的方式处理嵌套和非嵌套NER》](https://kexue.fm/archives/8373)。当GlobalPointer用于NER时，可以统一处理嵌套和非嵌套情况。笔者的多次实验显示，在非嵌套情形，GlobalPointer完全可以取得跟CRF相媲美的效果，并且训练和预测速度都更快。所以用GlobalPointer作为NER的baseline是顺理成章的。

## 效果对比 [\#](https://kexue.fm/kexue.fm\#%E6%95%88%E6%9E%9C%E5%AF%B9%E6%AF%94)

在CLUE的测试集上，各任务效果比较如下表，其中标$\_{\\text{-old}}$的是从CLUE官方找到的结果，标$\_{\\text{-our}}$的是本套代码的复现结果。这里的BERT和RoBERTa都是base版，BERT是Google最开始放出的中文BERT，RoBERTa是哈工大开源的RoBERTa\_wwm\_ext，large版本有算力有时间再测～

$$\\begin{array}{c}
\\text{分类任务} \\\
{\\begin{array}{c\|ccccccc}
\\hline
& \\text{IFLYTEK} & \\text{TNEWS} & \\text{AFQMC} & \\text{CMNLI} & \\text{OCNLI} & \\text{WSC} & \\text{CSL} \\\
\\hline
\\text{BERT}\_{\\text{-old}} & 60.29 & 57.42 & 73.70 & 79.69 & 72.20 & 74.60 & 80.36\\\
\\text{BERT}\_{\\text{-our}} & 61.19 & 56.29 & 73.37 & 79.37 & 71.73 & 73.85 & 84.03 \\\
\\hline
\\text{RoBERTa}\_{\\text{-old}} & 60.31 & \\text{-} & 74.04 & 80.51 & \\text{-} & \\text{-} & 81.00\\\
\\text{RoBERTa}\_{\\text{-our}} & 61.12 & 58.35 & 73.61 & 80.81 & 74.27 & 82.28 & 85.33\\\
\\hline
\\end{array}}
\\end{array}$$

$$\\begin{array}{c}
\\text{阅读理解和NER任务} \\\
{\\begin{array}{c\|cccc}
\\hline
& \\text{CMRC2018} & \\text{C3} & \\text{CHID} & \\text{CLUENER} \\\
\\hline
\\text{BERT}\_{\\text{-old}} & 71.60 & 64.50 & 82.04 & 78.82\\\
\\text{BERT}\_{\\text{-our}} & 72.10 & 61.33 & 85.13 & 78.68\\\
\\hline
\\text{RoBERTa}\_{\\text{-old}} & 75.20 & 66.50 & 83.62 & \\text{-}\\\
\\text{RoBERTa}\_{\\text{-our}} & 75.40 & 67.11 & 86.04 & 79.38\\\
\\hline
\\end{array}}
\\end{array}$$

注：这里TNEWS和WSC为空，是因为它们后来更新了测试集，但是官方Github并没有及时更新它们在RoBERTa上的测试结果；而OCNLI和CLUENER为空则是因为官方只测了BERT base和RoBERTa large的结果，RoBERTa base的结果也没有给出。

## 文章小结 [\#](https://kexue.fm/kexue.fm\#%E6%96%87%E7%AB%A0%E5%B0%8F%E7%BB%93)

本文分享了笔者基于bert4keras构建的CLUE评测基准代码，以及简单介绍了每类任务的建模思路。该套baseline代码有着简单清晰、易于迁移的特点，并且基本能达到CLUE官方宣称的基准成绩，部分任务还更优，因此算是上是及格的基准代码了。

_**转载到请包括本文地址：** [https://kexue.fm/archives/8739](https://kexue.fm/archives/8739)_

_**更详细的转载事宜请参考：**_ [《科学空间FAQ》](https://kexue.fm/archives/6508#%E6%96%87%E7%AB%A0%E5%A6%82%E4%BD%95%E8%BD%AC%E8%BD%BD/%E5%BC%95%E7%94%A8)

**如果您还有什么疑惑或建议，欢迎在下方评论区继续讨论。**

**如果您觉得本文还不错，欢迎 [分享](https://kexue.fm/kexue.fm#share)/ [打赏](https://kexue.fm/kexue.fm#pay) 本文。打赏并非要从中获得收益，而是希望知道科学空间获得了多少读者的真心关注。当然，如果你无视它，也不会影响你的阅读。再次表示欢迎和感谢！**

打赏

微信打赏

支付宝打赏

因为网站后台对打赏并无记录，因此欢迎在打赏时候备注留言。你还可以 [**点击这里**](http://mail.qq.com/cgi-bin/qm_share?t=qm_mailme&email=tN7d1drY3drrx8H0xcWa19vZ) 或在下方评论区留言来告知你的建议或需求。

**如果您需要引用本文，请参考：**

苏剑林. (Oct. 31, 2021). 《bert4keras在手，baseline我有：CLUE基准代码 》\[Blog post\]. Retrieved from [https://kexue.fm/archives/8739](https://kexue.fm/archives/8739)

@online{kexuefm-8739,
        title={bert4keras在手，baseline我有：CLUE基准代码},
        author={苏剑林},
        year={2021},
        month={Oct},
        url={\\url{https://kexue.fm/archives/8739}},
}

分类： [信息时代](https://kexue.fm/category/Big-Data)    标签： [模型](https://kexue.fm/tag/%E6%A8%A1%E5%9E%8B/), [代码](https://kexue.fm/tag/%E4%BB%A3%E7%A0%81/), [keras](https://kexue.fm/tag/keras/)[28 评论](https://kexue.fm/archives/8739#comments)

< [CAN：借助先验分布提升分类性能的简单后处理技巧](https://kexue.fm/archives/8728) \| [模型优化漫谈：BERT的初始标准差为什么是0.02？](https://kexue.fm/archives/8747) >

### 你也许还对下面的内容感兴趣

- [MoE环游记：1、从几何意义出发](https://kexue.fm/archives/10699)
- [旁门左道之如何让Python的重试代码更加优雅](https://kexue.fm/archives/9938)
- [基于量子化假设推导模型的尺度定律（Scaling Law）](https://kexue.fm/archives/9607)
- [Tiger：一个“抠”到极致的优化器](https://kexue.fm/archives/9512)
- [在bert4keras中使用混合精度和XLA加速训练](https://kexue.fm/archives/9059)
- [为什么需要残差？一个来自DeepNet的视角](https://kexue.fm/archives/8994)
- [门控注意力单元（GAU）还需要Warmup吗？](https://kexue.fm/archives/8990)
- [Efficient GlobalPointer：少点参数，多点效果](https://kexue.fm/archives/8877)
- [Seq2Seq+前缀树：检索任务新范式（以KgCLUE为例）](https://kexue.fm/archives/8802)
- [开局一段扯，数据全靠编？真被一篇“神论文”气到了](https://kexue.fm/archives/8783)

[发表你的看法](https://kexue.fm/kexue.fm#comment_form)

Ridethewind

November 2nd, 2021

苏神您好，请问bert4keras model在inference的时候打开dropout呢？

[回复评论](https://kexue.fm/archives/8739/comment-page-1?replyTo=17714#respond-post-8739)

[苏剑林](https://kexue.fm) 发表于
November 3rd, 2021

在加载模型之前，执行K.set\_learning\_phase(1)试试。

[回复评论](https://kexue.fm/archives/8739/comment-page-1?replyTo=17717#respond-post-8739)

enjlife1

November 24th, 2021

苏神，ocnli自然语言推理的训练数据，为什么segment采用的是全零，而不是tokenizer出来的segment\_ids

batch\_segment\_ids.append(\[0\] \* len(segment\_ids))

[回复评论](https://kexue.fm/archives/8739/comment-page-1?replyTo=17886#respond-post-8739)

enjlife1 发表于
November 25th, 2021

是我不仔细看博客了。。

[回复评论](https://kexue.fm/archives/8739/comment-page-1?replyTo=17890#respond-post-8739)

moonfanslth

December 1st, 2021

想问下苏神在公司里模型开发一般用哪个环境？

[回复评论](https://kexue.fm/archives/8739/comment-page-1?replyTo=17940#respond-post-8739)

[苏剑林](https://kexue.fm) 发表于
December 2nd, 2021

指的是什么环境？

[回复评论](https://kexue.fm/archives/8739/comment-page-1?replyTo=17952#respond-post-8739)

柏小琦

August 8th, 2022

请问下苏神 我在运行cluener的时候报错显示"Output tensors to a Model must be the output of a Keras \`Layer\`" 报错语句是"model = keras.models.Model(base.model.input, output)" 不知道错在哪里 请指教

[回复评论](https://kexue.fm/archives/8739/comment-page-1?replyTo=19589#respond-post-8739)

[苏剑林](https://kexue.fm) 发表于
August 8th, 2022

完全没改过模型代码？

[回复评论](https://kexue.fm/archives/8739/comment-page-1?replyTo=19595#respond-post-8739)

柏小琦 发表于
August 9th, 2022

对的 完全没改过 报错信息里有“Found: < bert4keras.layers.GlobalPointer object at 0x000001D578B97208 >” 所以我想问题是不是出在GlobalPointer里面？

[回复评论](https://kexue.fm/archives/8739/comment-page-1?replyTo=19598#respond-post-8739)

[苏剑林](https://kexue.fm) 发表于
August 9th, 2022

keras和tf版本分别是多少？

[回复评论](https://kexue.fm/archives/8739/comment-page-1?replyTo=19601#respond-post-8739)

柏小琦 发表于
August 9th, 2022

回苏神 keras=2.3.1 tf-gpu=2.2.0

[回复评论](https://kexue.fm/archives/8739/comment-page-1?replyTo=19602#respond-post-8739)

[苏剑林](https://kexue.fm) 发表于
August 9th, 2022

[@柏小琦\|comment-19602](https://kexue.fm/archives/8739/comment-page-1#comment-19602)

哪个文件哪一行出现的错误？我暂时还无法理解这个错误...

[回复评论](https://kexue.fm/archives/8739/comment-page-1?replyTo=19603#respond-post-8739)

柏小琦 发表于
August 10th, 2022

cluener文件的第106行，网上说这个报错原因是因为tf和keras混用，需要把一些tf语句用lambda修饰成tf.keras.layer形式。 我已经改了很多了，还是报错，我在想是不是我没找全或者没找到关键语句？ 该怎么找全呢

[回复评论](https://kexue.fm/archives/8739/comment-page-1?replyTo=19609#respond-post-8739)

[苏剑林](https://kexue.fm) 发表于
August 10th, 2022

如果qq群里提问的也是你，那么就是你自行改错了源码。认真对比你改的结果和原始文件的差异，就能发现问题了

刘恩晓

December 15th, 2022

请问哪个版本的bert4keras支持tensorflow-gpu=2.7.0的使用

[回复评论](https://kexue.fm/archives/8739/comment-page-1?replyTo=20557#respond-post-8739)

[苏剑林](https://kexue.fm) 发表于
December 16th, 2022

作者不对任何tf 2.x做主动支持。

[回复评论](https://kexue.fm/archives/8739/comment-page-1?replyTo=20570#respond-post-8739)

Lengyq

February 19th, 2023

苏神你好，想问一下，您的代码针对CLUE这一系列下游任务进行评测时都是对bert进行重新训练的。如果想对一个预训练好的bert进行finetune应该怎么做呢？例如针对分类任务。（主要是想问一下finetune过程中，模型的参数是否是部分更新），谢谢苏神。

[回复评论](https://kexue.fm/archives/8739/comment-page-1?replyTo=20962#respond-post-8739)

Lengyq 发表于
February 19th, 2023

如果您之前的blog实现的demo有实现过finetune的，可以帮我找一下吗，我去详细了解一下。感谢！

[回复评论](https://kexue.fm/archives/8739/comment-page-1?replyTo=20963#respond-post-8739)

Lengyq 发表于
February 19th, 2023

我想明白了，我傻了。做的一直都是finetune，不是重头训练。是我了解不够清晰。---

我最后还是想问一下，按照我的理解，如上这个finetune的过程对整个bert模型的参数都进行了调整更新。但是对于下游任务微调的过程不应该是只更新和具体下游任务相关的最后的全连接层参数或者是附带上最后几层transformer-block的参数吗？

[回复评论](https://kexue.fm/archives/8739/comment-page-1?replyTo=20964#respond-post-8739)

[苏剑林](https://kexue.fm) 发表于
February 20th, 2023

是的，原本就是finetune。一般情况下，为了效果，都是finetune全部参数，而不是部分参数。

[回复评论](https://kexue.fm/archives/8739/comment-page-1?replyTo=20974#respond-post-8739)

Lengyq 发表于
February 21st, 2023

明白了，谢谢苏神。我在看了CMRC任务的demo之后还有两点没相通的地方：

(1)对于阅读理解CMRC2018任务，您在博客写道：
一般的做法是将问题与段落用\[SEP\]拼接之后传入到BERT中，然后用两个全连接层分别预测首尾的位置。
在代码中您定义了一个CustomMasking类，注释为自定义mask（主要用于mask掉question部分）。我想了很久，但是还是不明白为什么对于这个任务需要把question给mask掉呢？
对于seq2seq类型的任务，用UniLM的mask形式进行处理我明白，但是想不通这里是为什么。
(2)并且在之前的demo中，例如在这篇文章\[从语言模型到Seq2Seq：Transformer如戏，全靠Mask\]提到的标题生成任务，就是用UniLM来做的，是根据segment\_id来生成mask矩阵。而在CMRC任务的代码中的数据生成器部分构建了一个batch\_masks，我不知道这个batch\_masks的作用是什么？是通过它来生成mask矩阵吗？假设是这样的话，那和用segment\_id来生成mask矩阵这两个方法是不是都可以？

[回复评论](https://kexue.fm/archives/8739/comment-page-1?replyTo=20982#respond-post-8739)

Lengyq 发表于
February 21st, 2023

我仔细又看了一下代码，捋通了。
(1)之所以把question给mask掉是在传入GP的时候mask掉的，因为GP解码的答案肯定是(2)batch\_masks的作用就是为了在输入GP的时候把question给mask掉，而不是Bert内部的mask，所以上面我说的猜想是错的。是我把这两个mask搞混了。
想了很久没想通，后来又自己豁然开朗了。

[回复评论](https://kexue.fm/archives/8739/comment-page-1?replyTo=20983#respond-post-8739)

[苏剑林](https://kexue.fm) 发表于
February 24th, 2023

恭喜恭喜，自己想通的收获是最大的。

[回复评论](https://kexue.fm/archives/8739/comment-page-1?replyTo=21003#respond-post-8739)

stephen

March 17th, 2023

苏神您好！我在测试CMRC2018任务时（用的是哈工大的Roberta参数），发现globalpointer\_accuracy能跑到0.76，但是验证集的val\_acc却一直在0.5~0.6徘徊，感觉像是完全没训练过一样，请问可能是什么原因导致的呢？

[回复评论](https://kexue.fm/archives/8739/comment-page-1?replyTo=21163#respond-post-8739)

[苏剑林](https://kexue.fm) 发表于
March 17th, 2023

你是不是对“完全没训练过”有什么误解？你觉得抽取式阅读理解随机蒙，准确率能有50%？也就是说你随机从一篇文章中抽一个片段，有50%的概率跟正确答案完全匹配？你这是天选之子的概率吧？

[回复评论](https://kexue.fm/archives/8739/comment-page-1?replyTo=21176#respond-post-8739)

stephen

April 19th, 2023

苏神您好！还是CMRC2018任务的测试得分问题，因为硬件条件有限，我的batch\_size参数只能设置成8，maxlen(512)stride(128)epochs(10)这些参数都没变，然后调参learning\_rate（从e-5到e-4），得到的best\_val\_acc最高是到0.60+，距离博客中的robert分数差了十几个点，实在是没有头绪，只能向苏神请教了

[回复评论](https://kexue.fm/archives/8739/comment-page-1?replyTo=21423#respond-post-8739)

[苏剑林](https://kexue.fm) 发表于
April 20th, 2023

0.6已经接近最优了。训练脚本报告的是acc（或者说em），而本文的表格报告的是f1+em的平均值，这个平均值需要提交到clue上才能获得。

[回复评论](https://kexue.fm/archives/8739/comment-page-1?replyTo=21433#respond-post-8739)

stephen 发表于
April 22nd, 2023

原来如此，了解，感谢！

[回复评论](https://kexue.fm/archives/8739/comment-page-1?replyTo=21443#respond-post-8739)

[取消回复](https://kexue.fm/archives/8739#respond-post-8739)

你的大名

电子邮箱

个人网站（选填）

1\. 可以使用LaTeX代码，点击“预览效果”可查看效果；2. 可以通过点击评论楼层编号来引用该楼层；3. 网站可能会有点卡，如非确认评论失败，请 **不要重复点击提交**。

### 内容速览

[代码简介](https://kexue.fm/kexue.fm#%E4%BB%A3%E7%A0%81%E7%AE%80%E4%BB%8B)
[文本分类](https://kexue.fm/kexue.fm#%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB)
[文本匹配](https://kexue.fm/kexue.fm#%E6%96%87%E6%9C%AC%E5%8C%B9%E9%85%8D)
[阅读理解](https://kexue.fm/kexue.fm#%E9%98%85%E8%AF%BB%E7%90%86%E8%A7%A3)
[单项选择](https://kexue.fm/kexue.fm#%E5%8D%95%E9%A1%B9%E9%80%89%E6%8B%A9)
[成语理解](https://kexue.fm/kexue.fm#%E6%88%90%E8%AF%AD%E7%90%86%E8%A7%A3)
[实体识别](https://kexue.fm/kexue.fm#%E5%AE%9E%E4%BD%93%E8%AF%86%E5%88%AB)
[效果对比](https://kexue.fm/kexue.fm#%E6%95%88%E6%9E%9C%E5%AF%B9%E6%AF%94)
[文章小结](https://kexue.fm/kexue.fm#%E6%96%87%E7%AB%A0%E5%B0%8F%E7%BB%93)

### 智能搜索

支持整句搜索！网站自动使用 [结巴分词](https://github.com/fxsjy/jieba) 进行分词，并结合ngrams排序算法给出合理的搜索结果。

### 热门标签

[生成模型](https://kexue.fm/tag/%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B/) [attention](https://kexue.fm/tag/attention/) [优化](https://kexue.fm/tag/%E4%BC%98%E5%8C%96/) [语言模型](https://kexue.fm/tag/%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/) [模型](https://kexue.fm/tag/%E6%A8%A1%E5%9E%8B/) [网站](https://kexue.fm/tag/%E7%BD%91%E7%AB%99/) [概率](https://kexue.fm/tag/%E6%A6%82%E7%8E%87/) [梯度](https://kexue.fm/tag/%E6%A2%AF%E5%BA%A6/) [矩阵](https://kexue.fm/tag/%E7%9F%A9%E9%98%B5/) [转载](https://kexue.fm/tag/%E8%BD%AC%E8%BD%BD/) [优化器](https://kexue.fm/tag/%E4%BC%98%E5%8C%96%E5%99%A8/) [微分方程](https://kexue.fm/tag/%E5%BE%AE%E5%88%86%E6%96%B9%E7%A8%8B/) [分析](https://kexue.fm/tag/%E5%88%86%E6%9E%90/) [天象](https://kexue.fm/tag/%E5%A4%A9%E8%B1%A1/) [深度学习](https://kexue.fm/tag/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/) [积分](https://kexue.fm/tag/%E7%A7%AF%E5%88%86/) [python](https://kexue.fm/tag/python/) [力学](https://kexue.fm/tag/%E5%8A%9B%E5%AD%A6/) [无监督](https://kexue.fm/tag/%E6%97%A0%E7%9B%91%E7%9D%A3/) [扩散](https://kexue.fm/tag/%E6%89%A9%E6%95%A3/) [几何](https://kexue.fm/tag/%E5%87%A0%E4%BD%95/) [节日](https://kexue.fm/tag/%E8%8A%82%E6%97%A5/) [生活](https://kexue.fm/tag/%E7%94%9F%E6%B4%BB/) [文本生成](https://kexue.fm/tag/%E6%96%87%E6%9C%AC%E7%94%9F%E6%88%90/) [数论](https://kexue.fm/tag/%E6%95%B0%E8%AE%BA/)

### 随机文章

- [Self-Orthogonality Module：一个即插即用的核正交化模块](https://kexue.fm/archives/7169)
- [Adam的epsilon如何影响学习率的Scaling Law？](https://kexue.fm/archives/10563)
- [封闭曲线所围成的面积：一个新技巧](https://kexue.fm/archives/3441)
- [RNN模型中输入的重要性的评估](https://kexue.fm/archives/4582)
- [轻微的扰动——摄动法简介(3)](https://kexue.fm/archives/1929)
- [科学空间：一种有趣的平方数](https://kexue.fm/archives/7)
- [以自然数幂为系数的幂级数](https://kexue.fm/archives/986)
- [\[春礼\]《方程与宇宙》:圆形限制性三体问题(七)](https://kexue.fm/archives/1220)
- [日出东方,重逢,最美的风采](https://kexue.fm/archives/815)
- [数学竞赛广东预赛\|组成三角形的概率](https://kexue.fm/archives/1477)

### 最近评论

- [苏剑林](https://kexue.fm/archives/11340/comment-page-1#comment-28680): 精度也是一个视角，但感觉这个事情感觉得仔细分析一下，因为理论上来讲，整体乘一个大数字，是不改变...
- [苏剑林](https://kexue.fm/archives/9257/comment-page-4#comment-28679): 可以这样理解：$t$时刻的$\\boldsymbol{x}\_t$，和$t-1$时刻的$\\bold...
- [苏剑林](https://kexue.fm/archives/9181/comment-page-5#comment-28678): 没有太多技巧了，就是直接代入然后根据$\\bar{\\alpha}\_t,\\bar{\\beta}\_t...
- [苏剑林](https://kexue.fm/archives/10862/comment-page-1#comment-28677): 完全懵了...WukT是什么？如果代表C到key的投影矩阵，那Wropeq和Wropekt又是...
- [Zhan-Wang Mao](https://kexue.fm/archives/9257/comment-page-4#comment-28676): 苏老师，请教一下(4)式的泰勒展开式为什么严格来说和$t$有关？不是在$x\_t$处关于$x$的...
- [yzlnew](https://kexue.fm/archives/11340/comment-page-1#comment-28675): 可以相呼应的是，这样的好模型能被浮点数以误差比较低的方式表示和训练，并且也易于量化。
- [Henry](https://kexue.fm/archives/9181/comment-page-5#comment-28673): 想请问苏老师，方程7是如何推导到方程10的，是否有化简的一些小技巧？
- [szsheep](https://kexue.fm/archives/9119/comment-page-13#comment-28672): 牛啊，还可以从这方面推出loss的函数最终式。原本是从KL散度入手，没想到作者完全用另外一种方...
- [pang](https://kexue.fm/archives/10862/comment-page-1#comment-28671): 对于目前的MLA算法softmax(X×WQ×WukT×CjT)×Cj×Wuv来说其实X,WQ...
- [苏剑林](https://kexue.fm/archives/11126/comment-page-3#comment-28670): 你是说 chatglm2-6b 里边的？那个没用，预设的常数是压不住的...

### 友情链接

- [Cool Papers](https://papers.cool)
- [数学研发](https://bbs.emath.ac.cn)
- [Seatop](http://www.seatop.com.cn/)
- [Xiaoxia](https://xiaoxia.org/)
- [积分表-网络版](https://kexue.fm/sci/integral/index.html)
- [丝路博傲](http://blog.dvxj.com/)
- [数学之家](http://www.2math.cn/)
- [有趣天文奇观](http://interesting-sky.china-vo.org/)
- [TwistedW](http://www.twistedwg.com/)
- [godweiyang](https://godweiyang.com/)
- [AI柠檬](https://blog.ailemon.net/)
- [王登科-DK博客](https://greatdk.com)
- [ESON](https://blog.eson.org/)
- [枫之羽](https://fzhiy.net/)
- [Mathor's blog](https://wmathor.com/)
- [coding-zuo](https://coding-zuo.github.io/)
- [博科园](https://www.bokeyuan.net/)
- [孔皮皮的博客](https://www.kppkkp.top/)
- [运鹏的博客](https://yunpengtai.top/)
- [jiming.site](https://jiming.site/)
- [OmegaXYZ](https://www.omegaxyz.com/)
- [EAI猩球](https://www.robotech.ink/)
- [文举的博客](https://liwenju0.com/)
- [用代码打点酱油](https://bruceyuan.com/)
- [Zhang's blog](https://armcvai.cn/)
- [申请链接](https://kexue.fm/links.html)

本站采用创作共用版权协议，要求署名、非商业用途和保持一致。转载本站内容必须也遵循“ [署名-非商业用途-保持一致](http://creativecommons.org/licenses/by-nc-nd/2.5/cn/)”的创作共用协议。
© 2009-2025 Scientific Spaces. All rights reserved. Theme by [laogui](http://www.laogui.com). Powered by [Typecho](http://typecho.org). 备案号: [粤ICP备09093259号-1/2](https://beian.miit.gov.cn/)。