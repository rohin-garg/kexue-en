## SEARCH

## MENU

- [打赏](https://kexue.fm/reward.html)
- [公式](https://kexue.fm/latex.html)
- [天象](https://kexue.fm/ac.html)
- [链接](https://kexue.fm/links.html)
- [时光](https://kexue.fm/me.html)
- [博览](https://kexue.fm/science.html)
- [归档](https://kexue.fm/content.html)

## CATEGORIES

- [千奇百怪](https://kexue.fm/category/Everything)
- [天文探索](https://kexue.fm/category/Astronomy)
- [数学研究](https://kexue.fm/category/Mathematics)
- [物理化学](https://kexue.fm/category/Phy-chem)
- [信息时代](https://kexue.fm/category/Big-Data)
- [生物自然](https://kexue.fm/category/Biology)
- [图片摄影](https://kexue.fm/category/Photograph)
- [问题百科](https://kexue.fm/category/Questions)
- [生活/情感](https://kexue.fm/category/Life-Feeling)
- [资源共享](https://kexue.fm/category/Resources)

## NEWPOSTS

- [通过msign来计算mclip（奇...](https://kexue.fm/archives/11006)
- [msign算子的Newton-Sc...](https://kexue.fm/archives/10996)
- [等值振荡定理：最优多项式逼近的充要条件](https://kexue.fm/archives/10972)
- [生成扩散模型漫谈（三十）：从瞬时速...](https://kexue.fm/archives/10958)
- [MoE环游记：5、均匀分布的反思](https://kexue.fm/archives/10945)
- [msign算子的Newton-Sc...](https://kexue.fm/archives/10922)
- [Transformer升级之路：2...](https://kexue.fm/archives/10907)
- [一道概率不等式：盯着它到显然成立为止！](https://kexue.fm/archives/10902)
- [SVD的导数](https://kexue.fm/archives/10878)
- [智能家居之手搓一套能接入米家的零冷水装置](https://kexue.fm/archives/10869)

## COMMENTS

- [Zanwei Zhou: 感谢苏神的回复！我理解您的意思，Meanflow确实给出了一个...](https://kexue.fm/archives/10958/comment-page-1#comment-27861)
- [个个君: 捉虫“按照分布轨迹xt=(1−t)x0+tx1，将x1变到x1...](https://kexue.fm/archives/10958/comment-page-1#comment-27860)
- [Phoenix8215: 苏神QT-ViT: Improving Linear Atte...](https://kexue.fm/archives/8601/comment-page-1#comment-27858)
- [Khazzz1c: Rope-Tie-v2的那个图 右侧是数学计算写错了 还得加上...](https://kexue.fm/archives/10352/comment-page-2#comment-27853)
- [gongxie: 感谢回复。目前实验出来的结果是发现在前50% 加rope 效果...](https://kexue.fm/archives/10907/comment-page-1#comment-27852)
- [苏剑林: 代入各自的概率密度表达式，只看指数项。](https://kexue.fm/archives/9164/comment-page-5#comment-27851)
- [苏剑林: DDPM的推导逻辑是：给定$p(\\boldsymbol{x}\_...](https://kexue.fm/archives/9181/comment-page-5#comment-27850)
- [li--: 苏神，公式5是怎么跳到公式7的](https://kexue.fm/archives/9164/comment-page-5#comment-27849)
- [jsrdcht: 再补充一下，刚刚自己动手算了一下。DDIM计算出的前向后验是D...](https://kexue.fm/archives/9181/comment-page-5#comment-27848)
- [jsrdcht: 感谢回答！我解释一下我这个问题的来源，应该也有不少人有一样的疑...](https://kexue.fm/archives/9181/comment-page-5#comment-27847)

## USERLOGIN

- [登录](https://kexue.fm/admin/login.php)

[科学空间\|Scientific Spaces](https://kexue.fm)

- [登录](https://kexue.fm/admin/login.php)
- [打赏](https://kexue.fm/reward.html)
- [公式](https://kexue.fm/latex.html)
- [天象](https://kexue.fm/ac.html)
- [链接](https://kexue.fm/links.html)
- [时光](https://kexue.fm/me.html)
- [博览](https://kexue.fm/science.html)
- [归档](https://kexue.fm/content.html)

渴望成为一个小飞侠

- [欢迎订阅](https://kexue.fm/feed)
- [个性邮箱](https://kexue.fm/archives/119)
- [天象信息](https://kexue.fm/ac.html)
- [观测ISS](https://kexue.fm/archives/41)
- [LaTeX](https://kexue.fm/latex.html)
- [关于博主](https://kexue.fm/me.html)

欢迎访问“科学空间”，这里将与您共同探讨自然科学，回味人生百态；也期待大家的分享～

- [**千奇百怪** Everything](https://kexue.fm/category/Everything)
- [**天文探索** Astronomy](https://kexue.fm/category/Astronomy)
- [**数学研究** Mathematics](https://kexue.fm/category/Mathematics)
- [**物理化学** Phy-chem](https://kexue.fm/category/Phy-chem)
- [**信息时代** Big-Data](https://kexue.fm/category/Big-Data)
- [**生物自然** Biology](https://kexue.fm/category/Biology)
- [**图片摄影** Photograph](https://kexue.fm/category/Photograph)
- [**问题百科** Questions](https://kexue.fm/category/Questions)
- [**生活/情感** Life-Feeling](https://kexue.fm/category/Life-Feeling)
- [**资源共享** Resources](https://kexue.fm/category/Resources)

- [**千奇百怪**](https://kexue.fm/category/Everything)
- [**天文探索**](https://kexue.fm/category/Astronomy)
- [**数学研究**](https://kexue.fm/category/Mathematics)
- [**物理化学**](https://kexue.fm/category/Phy-chem)
- [**信息时代**](https://kexue.fm/category/Big-Data)
- [**生物自然**](https://kexue.fm/category/Biology)
- [**图片摄影**](https://kexue.fm/category/Photograph)
- [**问题百科**](https://kexue.fm/category/Questions)
- [**生活/情感**](https://kexue.fm/category/Life-Feeling)
- [**资源共享**](https://kexue.fm/category/Resources)

[首页](https://kexue.fm) [信息时代](https://kexue.fm/category/Big-Data) GAU-α：尝鲜体验快好省的下一代Attention

22Apr

# [GAU-α：尝鲜体验快好省的下一代Attention](https://kexue.fm/archives/9052)

By 苏剑林 \|
2022-04-22 \|
57583位读者\|

在 [《FLASH：可能是近来最有意思的高效Transformer设计》](https://kexue.fm/archives/8934) 中，我们介绍了GAU（Gated Attention Unit，门控线性单元），在这里笔者愿意称之为“目前最有潜力的下一代Attention设计”，因为它真正达到了“更快（速度）、更好（效果）、更省（显存）”的特点。

然而，有些读者在自己的测试中得到了相反的结果，比如收敛更慢、效果更差等，这与笔者的测试结果大相径庭。本文就来分享一下笔者自己的训练经验，并且放出一个尝鲜版“GAU-α”供大家测试。

> **开源地址： [https://github.com/ZhuiyiTechnology/GAU-alpha](https://github.com/ZhuiyiTechnology/GAU-alpha)**

## GAU-α [\#](https://kexue.fm/archives/9052\#GAU-%CE%B1)

首先介绍一下开源出来的“GAU-α”在CLUE任务上的成绩单：
$$\\small{\\begin{array}{c\|ccccccccccc}
\\hline
& \\text{iflytek} & \\text{tnews} & \\text{afqmc} & \\text{cmnli} & \\text{ocnli} & \\text{wsc} & \\text{csl} & \\text{cmrc2018} & \\text{c3} & \\text{chid} & \\text{cluener}\\\
\\hline
\\text{BERT} & 60.06 & 56.80 & 72.41 & 79.56 & 73.93 & 78.62 & 83.93 & 56.17 & 60.54 & 85.69 & 79.45 \\\
\\text{RoBERTa} & 60.64 & \\textbf{58.06} & 74.05 & 81.24 & 76.00 & \\textbf{87.50} & 84.50 & 56.54 & 67.66 & 86.71 & 79.47\\\
\\text{RoFormer} & 60.91 & 57.54 & 73.52 & 80.92 & \\textbf{76.07} & 86.84 & 84.63 & 56.26 & 67.24 & 86.57 & 79.72\\\
\\text{RoFormerV2}^\* & 60.87 & 56.54 & 72.75 & 80.34 & 75.36 & 80.92 & 84.67 & 57.91 & 64.62 & 85.09 & \\textbf{81.08}\\\
\\hline
\\text{GAU-}\\alpha & \\textbf{61.41} & 57.76 & \\textbf{74.17} & \\textbf{81.82} & 75.86 & 79.93 & \\textbf{85.67} & \\textbf{58.09} & \\textbf{68.24} & \\textbf{87.91} & 80.01\\\
\\hline
\\end{array}}$$

所有的模型都是Base版，上表显示的是CLUE任务上验证集上的结果，大家的运行方式和比较都是公平的，作为一个相对比较来说是合理的。另外，这里的RoFormerV2\*并非 [《RoFormerV2：自然语言理解的极限探索》](https://kexue.fm/archives/8998) 中的多任务版本，而是仅仅进行了MLM预训练的版本（该版本没开源），这样对比是因为GAU-α也仅仅进行了MLM预训练。

从表中可以看出，除了WSC这个数据量极少的“异类”外，GAU-α在多数任务上都有优势，并且除了WSC外的平均成绩是最好的。其中，RoFormerV2\*与GAU-α的比较是最为公平的，因为它们的训练脚本、训练数据、整体结构都是一样的，唯一不同就是GAU-α是将RoFormerV2\*中的Attention+FFN组合换成了两层GAU，两者对比充分显示出了GAU设计“更好”的特点。

此外，我们在 [《RoFormerV2：自然语言理解的极限探索》](https://kexue.fm/archives/8998) 介绍过RoFormerV2对结构进行了简化，从而获得更快的速度，具有同样整体结构的GAU-α也是如此，所以GAU-α的速度是比表中的BERT、RoBERTa、RoFormer都要快的，但平均效果却更胜一筹。更进一步的测试显示，当序列长度超过512时，GAU-α的速度开始超过同样精简过的RoFormerV2，并且显存占用更低，越长则对GAU-α更有利。

## 训练 [\#](https://kexue.fm/archives/9052\#%E8%AE%AD%E7%BB%83)

现在介绍一下模型的训练细节，完整的代码已经开源到Github中，如有疑惑可以对照着代码来读。

**模型架构**： GAU-α就是将RoFormerV2的Attention+FFN换成了两层GAU，在 [之前的文章](https://kexue.fm/archives/8934) 中我们比较过两层GAU的计算量和参数量大致相当于Attention+FFN组合，所以这样的替换是合理的；RoFormerV2的特点是保留了Post Norm结构，去掉了所有的Bias项，并且Layer Norm换成了RMS Norm的最简单变体，在GAU-α中也是如此。

**归一化**： 在 [《听说Attention与Softmax更配哦～》](https://kexue.fm/archives/9019) 中我们讨论过Attention的归一化问题，GAU-α的Attention归一化选取了其中笔者自行提出的具有较好外推能力的 [熵不变性Softmax](https://kexue.fm/archives/8823)（在bert4keras中暂称为softmax\_plus）。

**训练方式**： 在初始化方面笔者按照 [《训练1000层的Transformer究竟有什么困难？》](https://kexue.fm/archives/8978) 进行了调整，因此无须Wamrup就可以直接训练，优化器用的是LAMB，学习率分段线性衰减；预训练任务用的是全词MLM，分词工具用百度的LAC，这些跟RoFormerV2都是对齐的。

好像值得一提的也就这么多了，确实没进行多大的改变。除了在归一化方式上花了点时间进行测试，其他方面也没多费时间，直接训练就得到了不错的效果。

## 小结 [\#](https://kexue.fm/archives/9052\#%E5%B0%8F%E7%BB%93)

GAU是笔者认为的“目前最有潜力的下一代Attention设计”，本文分享了GAU的一些训练经验，并开源了一个尝鲜版“GAU-α”。

_**转载到请包括本文地址：** [https://kexue.fm/archives/9052](https://kexue.fm/archives/9052)_

_**更详细的转载事宜请参考：**_ [《科学空间FAQ》](https://kexue.fm/archives/6508#%E6%96%87%E7%AB%A0%E5%A6%82%E4%BD%95%E8%BD%AC%E8%BD%BD/%E5%BC%95%E7%94%A8)

**如果您还有什么疑惑或建议，欢迎在下方评论区继续讨论。**

**如果您觉得本文还不错，欢迎 [分享](https://kexue.fm/archives/9052#share)/ [打赏](https://kexue.fm/archives/9052#pay) 本文。打赏并非要从中获得收益，而是希望知道科学空间获得了多少读者的真心关注。当然，如果你无视它，也不会影响你的阅读。再次表示欢迎和感谢！**

打赏

微信打赏

支付宝打赏

因为网站后台对打赏并无记录，因此欢迎在打赏时候备注留言。你还可以 [**点击这里**](http://mail.qq.com/cgi-bin/qm_share?t=qm_mailme&email=tN7d1drY3drrx8H0xcWa19vZ) 或在下方评论区留言来告知你的建议或需求。

**如果您需要引用本文，请参考：**

苏剑林. (Apr. 22, 2022). 《GAU-α：尝鲜体验快好省的下一代Attention 》\[Blog post\]. Retrieved from [https://kexue.fm/archives/9052](https://kexue.fm/archives/9052)

@online{kexuefm-9052,
        title={GAU-α：尝鲜体验快好省的下一代Attention},
        author={苏剑林},
        year={2022},
        month={Apr},
        url={\\url{https://kexue.fm/archives/9052}},
}

分类： [信息时代](https://kexue.fm/category/Big-Data)    标签： [语言模型](https://kexue.fm/tag/%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/), [attention](https://kexue.fm/tag/attention/), [预训练](https://kexue.fm/tag/%E9%A2%84%E8%AE%AD%E7%BB%83/)[13 评论](https://kexue.fm/archives/9052#comments)

< [你的语言模型有没有“无法预测的词”？](https://kexue.fm/archives/9046) \| [在bert4keras中使用混合精度和XLA加速训练](https://kexue.fm/archives/9059) >

### 你也许还对下面的内容感兴趣

- [Transformer升级之路：20、MLA究竟好在哪里？](https://kexue.fm/archives/10907)
- [Transformer升级之路：19、第二类旋转位置编码](https://kexue.fm/archives/10862)
- [细水长flow之TARFLOW：流模型满血归来？](https://kexue.fm/archives/10667)
- [“闭门造车”之多模态思路浅谈（三）：位置编码](https://kexue.fm/archives/10352)
- [Decoder-only的LLM为什么需要位置编码？](https://kexue.fm/archives/10347)
- [Monarch矩阵：计算高效的稀疏型矩阵分解](https://kexue.fm/archives/10249)
- [Transformer升级之路：18、RoPE的底数选择原则](https://kexue.fm/archives/10122)
- [缓存与效果的极限拉扯：从MHA、MQA、GQA到MLA](https://kexue.fm/archives/10091)
- [Transformer升级之路：17、多模态位置编码的简单思考](https://kexue.fm/archives/10040)
- [时空之章：将Attention视为平方复杂度的RNN](https://kexue.fm/archives/10017)

[发表你的看法](https://kexue.fm/archives/9052#comment_form)

橙C

April 22nd, 2022

赞， 后面会有显存和速度相关的评测吗？

[回复评论](https://kexue.fm/archives/9052/comment-page-1?replyTo=19021#respond-post-9052)

[苏剑林](https://kexue.fm) 发表于
April 22nd, 2022

显存和速度之前都评测过了，在序列较短的情况下，GAU没什么优势，但是序列长度较长（超过512），GAU肯定是更省显存的并且更快。

[回复评论](https://kexue.fm/archives/9052/comment-page-1?replyTo=19022#respond-post-9052)

Bo仔很忙

April 25th, 2022

我做了个pytorch实现，https://github.com/Tongjilibo/GAU-alpha-pytorch，再请教苏神一个问题，就是我直接用bert4keras的GAU-alpha，mlm的效果一般

[回复评论](https://kexue.fm/archives/9052/comment-page-1?replyTo=19042#respond-post-9052)

Bo仔很忙 发表于
April 25th, 2022

重新测试了下，长文本效果还可以，短文本效果一般~

[回复评论](https://kexue.fm/archives/9052/comment-page-1?replyTo=19044#respond-post-9052)

[苏剑林](https://kexue.fm) 发表于
April 27th, 2022

感谢你的实现和测试。

[回复评论](https://kexue.fm/archives/9052/comment-page-1?replyTo=19055#respond-post-9052)

altenli

August 31st, 2022

请教苏神，GAU这种模型假如想用作生成模型的话，decoder是用类似t5的decoder还是基于GAU复现一个GAU式的decoder呢？

[回复评论](https://kexue.fm/archives/9052/comment-page-1?replyTo=19699#respond-post-9052)

[苏剑林](https://kexue.fm) 发表于
September 1st, 2022

你喜欢，可以用GAU做decoder，Github上最新版本的bert4keras已经内置cross attention版本的GAU。

[回复评论](https://kexue.fm/archives/9052/comment-page-1?replyTo=19713#respond-post-9052)

welldone

September 25th, 2022

苏神，想请教一下，在给类transformer block加position encoding（包括rope）的时候,您认为哪种方案比较合理：
a.只在第一个block前加一次
b.每个block加不同的
c.每个block加相同的
d.每几层加一次（比如降采样长度发生变化的时候肯定是需要加的）

[回复评论](https://kexue.fm/archives/9052/comment-page-1?replyTo=19893#respond-post-9052)

[苏剑林](https://kexue.fm) 发表于
September 27th, 2022

看什么编码吧，相对位置编码一般都是每个block都加。

[回复评论](https://kexue.fm/archives/9052/comment-page-1?replyTo=19919#respond-post-9052)

[苏神博客阅读记录\_Johngo学长](https://www.johngo689.com/95949/)

September 26th, 2022

\[...\]​​​​​​GAU-α：尝鲜体验快好省的下一代Attention\[...\]

[回复评论](https://kexue.fm/archives/9052/comment-page-1?replyTo=19901#respond-post-9052)

[苏神博客阅读记录\_Johngo学长](https://www.johngo689.com/544848/)

May 30th, 2023

\[...\]​​​​​​GAU-α：尝鲜体验快好省的下一代Attention\[...\]

[回复评论](https://kexue.fm/archives/9052/comment-page-1?replyTo=21809#respond-post-9052)

fvdfggh

August 4th, 2023

大佬对最近出现的TransNormerLLM怎么看owo。

[回复评论](https://kexue.fm/archives/9052/comment-page-1?replyTo=22427#respond-post-9052)

[苏剑林](https://kexue.fm) 发表于
August 7th, 2023

跟RetNet很相似，而且对于大模型只报告了速度和显存，没有报告效果，所以没什么可看的。

[回复评论](https://kexue.fm/archives/9052/comment-page-1?replyTo=22437#respond-post-9052)

[取消回复](https://kexue.fm/archives/9052#respond-post-9052)

你的大名

电子邮箱

个人网站（选填）

1\. 可以使用LaTeX代码，点击“预览效果”可查看效果；2. 可以通过点击评论楼层编号来引用该楼层；3. 网站可能会有点卡，如非确认评论失败，请 **不要重复点击提交**。

### 内容速览

[GAU-α](https://kexue.fm/archives/9052#GAU-%CE%B1)
[训练](https://kexue.fm/archives/9052#%E8%AE%AD%E7%BB%83)
[小结](https://kexue.fm/archives/9052#%E5%B0%8F%E7%BB%93)

### 智能搜索

支持整句搜索！网站自动使用 [结巴分词](https://github.com/fxsjy/jieba) 进行分词，并结合ngrams排序算法给出合理的搜索结果。

### 热门标签

[生成模型](https://kexue.fm/tag/%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B/) [attention](https://kexue.fm/tag/attention/) [优化](https://kexue.fm/tag/%E4%BC%98%E5%8C%96/) [语言模型](https://kexue.fm/tag/%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/) [模型](https://kexue.fm/tag/%E6%A8%A1%E5%9E%8B/) [网站](https://kexue.fm/tag/%E7%BD%91%E7%AB%99/) [概率](https://kexue.fm/tag/%E6%A6%82%E7%8E%87/) [梯度](https://kexue.fm/tag/%E6%A2%AF%E5%BA%A6/) [转载](https://kexue.fm/tag/%E8%BD%AC%E8%BD%BD/) [微分方程](https://kexue.fm/tag/%E5%BE%AE%E5%88%86%E6%96%B9%E7%A8%8B/) [矩阵](https://kexue.fm/tag/%E7%9F%A9%E9%98%B5/) [天象](https://kexue.fm/tag/%E5%A4%A9%E8%B1%A1/) [分析](https://kexue.fm/tag/%E5%88%86%E6%9E%90/) [深度学习](https://kexue.fm/tag/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/) [积分](https://kexue.fm/tag/%E7%A7%AF%E5%88%86/) [python](https://kexue.fm/tag/python/) [优化器](https://kexue.fm/tag/%E4%BC%98%E5%8C%96%E5%99%A8/) [力学](https://kexue.fm/tag/%E5%8A%9B%E5%AD%A6/) [无监督](https://kexue.fm/tag/%E6%97%A0%E7%9B%91%E7%9D%A3/) [扩散](https://kexue.fm/tag/%E6%89%A9%E6%95%A3/) [几何](https://kexue.fm/tag/%E5%87%A0%E4%BD%95/) [节日](https://kexue.fm/tag/%E8%8A%82%E6%97%A5/) [生活](https://kexue.fm/tag/%E7%94%9F%E6%B4%BB/) [文本生成](https://kexue.fm/tag/%E6%96%87%E6%9C%AC%E7%94%9F%E6%88%90/) [数论](https://kexue.fm/tag/%E6%95%B0%E8%AE%BA/)

### 随机文章

- [看完了刘亦菲版《倩女幽魂》](https://kexue.fm/archives/1333)
- [一本对称闯物理：相对论力学(一)](https://kexue.fm/archives/2478)
- [不可能事件——一道经典电磁感应题的错误](https://kexue.fm/archives/1170)
- [与日食失之交臂...](https://kexue.fm/archives/341)
- [T5 PEGASUS：开源一个中文生成式预训练模型](https://kexue.fm/archives/8209)
- [VQ的又一技巧：给编码表加一个线性变换](https://kexue.fm/archives/10519)
- [随机分词浅探：从Viterbi Decoding到Viterbi Sampling](https://kexue.fm/archives/9768)
- [新年快乐！记录一下 Cool Papers 的开发体验](https://kexue.fm/archives/9920)
- [模型优化漫谈：BERT的初始标准差为什么是0.02？](https://kexue.fm/archives/8747)
- [【NASA每日一图】太阳系中的木卫三](https://kexue.fm/archives/130)

### 最近评论

- [Zanwei Zhou](https://kexue.fm/archives/10958/comment-page-1#comment-27861): 感谢苏神的回复！我理解您的意思，Meanflow确实给出了一个清晰明确的优化目标。CTM构造l...
- [个个君](https://kexue.fm/archives/10958/comment-page-1#comment-27860): 捉虫“按照分布轨迹xt=(1−t)x0+tx1，将x1变到x1的ODE形式解是”应该是x1到x...
- [Phoenix8215](https://kexue.fm/archives/8601/comment-page-1#comment-27858): 苏神QT-ViT: Improving Linear Attention in ViT wit...
- [Khazzz1c](https://kexue.fm/archives/10352/comment-page-2#comment-27853): Rope-Tie-v2的那个图 右侧是数学计算写错了 还得加上一个L 才是11.5
- [gongxie](https://kexue.fm/archives/10907/comment-page-1#comment-27852): 感谢回复。目前实验出来的结果是发现在前50% 加rope 效果确实比full rope 好， ...
- [苏剑林](https://kexue.fm/archives/9164/comment-page-5#comment-27851): 代入各自的概率密度表达式，只看指数项。
- [苏剑林](https://kexue.fm/archives/9181/comment-page-5#comment-27850): DDPM的推导逻辑是：给定$p(\\boldsymbol{x}\_t\|\\boldsymbol{x}...
- [li--](https://kexue.fm/archives/9164/comment-page-5#comment-27849): 苏神，公式5是怎么跳到公式7的
- [jsrdcht](https://kexue.fm/archives/9181/comment-page-5#comment-27848): 再补充一下，刚刚自己动手算了一下。DDIM计算出的前向后验是DDPM的推广情况，如果把DDDP...
- [jsrdcht](https://kexue.fm/archives/9181/comment-page-5#comment-27847): 感谢回答！我解释一下我这个问题的来源，应该也有不少人有一样的疑惑。我是发现DDIM大部分的推导...

### 友情链接

- [Cool Papers](https://papers.cool)
- [数学研发](https://bbs.emath.ac.cn)
- [Seatop](http://www.seatop.com.cn/)
- [Xiaoxia](https://xiaoxia.org/)
- [积分表-网络版](https://kexue.fm/sci/integral/index.html)
- [丝路博傲](http://blog.dvxj.com/)
- [ph4ntasy 饭特稀](http://www.ph4ntasy.com/)
- [数学之家](http://www.2math.cn/)
- [有趣天文奇观](http://interesting-sky.china-vo.org/)
- [TwistedW](http://www.twistedwg.com/)
- [godweiyang](https://godweiyang.com/)
- [AI柠檬](https://blog.ailemon.net/)
- [王登科-DK博客](https://greatdk.com)
- [ESON](https://blog.eson.org/)
- [枫之羽](https://fzhiy.net/)
- [Mathor's blog](https://wmathor.com/)
- [coding-zuo](https://coding-zuo.github.io/)
- [博科园](https://www.bokeyuan.net/)
- [孔皮皮的博客](https://www.kppkkp.top/)
- [运鹏的博客](https://yunpengtai.top/)
- [jiming.site](https://jiming.site/)
- [OmegaXYZ](https://www.omegaxyz.com/)
- [Blog by Eacls](https://www.eacls.top/)
- [EAI猩球](https://www.robotech.ink/)
- [文举的博客](https://liwenju0.com/)
- [用代码打点酱油](https://bruceyuan.com/)
- [申请链接](https://kexue.fm/links.html)

本站采用创作共用版权协议，要求署名、非商业用途和保持一致。转载本站内容必须也遵循“ [署名-非商业用途-保持一致](http://creativecommons.org/licenses/by-nc-nd/2.5/cn/)”的创作共用协议。
© 2009-2025 Scientific Spaces. All rights reserved. Theme by [laogui](http://www.laogui.com). Powered by [Typecho](http://typecho.org). 备案号: [粤ICP备09093259号-1/2](https://beian.miit.gov.cn/)。