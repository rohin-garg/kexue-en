Processing math: 0%

![MobileSideBar](https://kexue.fm/usr/themes/geekg/images/slide-button.png)

## SEARCH

## MENU

- [打赏](https://kexue.fm/reward.html)
- [公式](https://kexue.fm/latex.html)
- [天象](https://kexue.fm/ac.html)
- [链接](https://kexue.fm/links.html)
- [时光](https://kexue.fm/me.html)
- [博览](https://kexue.fm/science.html)
- [归档](https://kexue.fm/content.html)

## CATEGORIES

- [千奇百怪](https://kexue.fm/category/Everything)
- [天文探索](https://kexue.fm/category/Astronomy)
- [数学研究](https://kexue.fm/category/Mathematics)
- [物理化学](https://kexue.fm/category/Phy-chem)
- [信息时代](https://kexue.fm/category/Big-Data)
- [生物自然](https://kexue.fm/category/Biology)
- [图片摄影](https://kexue.fm/category/Photograph)
- [问题百科](https://kexue.fm/category/Questions)
- [生活/情感](https://kexue.fm/category/Life-Feeling)
- [资源共享](https://kexue.fm/category/Resources)

## NEWPOSTS

- [让炼丹更科学一些（五）：基于梯度精...](https://kexue.fm/archives/11530)
- [让炼丹更科学一些（四）：新恒等式，...](https://kexue.fm/archives/11494)
- [为什么DeltaNet要加L2 N...](https://kexue.fm/archives/11486)
- [让炼丹更科学一些（三）：SGD的终...](https://kexue.fm/archives/11480)
- [让炼丹更科学一些（二）：将结论推广...](https://kexue.fm/archives/11469)
- [滑动平均视角下的权重衰减和学习率](https://kexue.fm/archives/11459)
- [生成扩散模型漫谈（三十一）：预测数...](https://kexue.fm/archives/11428)
- [Muon优化器指南：快速上手与关键细节](https://kexue.fm/archives/11416)
- [AdamW的Weight RMS的...](https://kexue.fm/archives/11404)
- [n个正态随机数的最大值的渐近估计](https://kexue.fm/archives/11390)

## COMMENTS

- [Bin: 今天偶然从某个论坛看到有人推荐您的博客，定睛一看竟然是华师同院...](https://kexue.fm/archives/1990/comment-page-2#comment-29105)
- [Rapture D: 我有一个问题，为什么不考虑亥姆霍兹定理和斯托克斯公式。](https://kexue.fm/archives/11530/comment-page-1#comment-29104)
- [mofheka: 苏神是还在用jax是么？最近在做基于Google Pathwa...](https://kexue.fm/archives/11390/comment-page-1#comment-29103)
- [长琴: 看懂这篇博客也不是一件容易的事情。](https://kexue.fm/archives/11530/comment-page-1#comment-29102)
- [AlexLi: 苏老师，请教一下(7)式中将 \\mu(x\_t) 传给 $p...](https://kexue.fm/archives/9257/comment-page-4#comment-29101)
- [tyler\_zxc: "Performer的思想是将标准的Attention线性化，...](https://kexue.fm/archives/7921/comment-page-2#comment-29100)
- [我: 似乎并非mHC提出矩阵的思想？之前hyper connecti...](https://kexue.fm/archives/11494/comment-page-1#comment-29099)
- [winter: 苏神您好，假如对于比较均匀的attention weightP...](https://kexue.fm/archives/10847/comment-page-1#comment-29098)
- [苏剑林: KL散度、JS散度、W距离啥的，都行啊，看你喜欢哪个](https://kexue.fm/archives/8512/comment-page-2#comment-29097)
- [苏剑林: 没有绝对公平的对比方法，主要看你关心什么。比如，如果只关心推理...](https://kexue.fm/archives/9119/comment-page-14#comment-29096)

## USERLOGIN

- [登录](https://kexue.fm/admin/login.php)

[科学空间\|Scientific Spaces](https://kexue.fm/)

- [登录](https://kexue.fm/admin/login.php)
- [打赏](https://kexue.fm/reward.html)
- [公式](https://kexue.fm/latex.html)
- [天象](https://kexue.fm/ac.html)
- [链接](https://kexue.fm/links.html)
- [时光](https://kexue.fm/me.html)
- [博览](https://kexue.fm/science.html)
- [归档](https://kexue.fm/content.html)

渴望成为一个小飞侠

- [![](https://kexue.fm/usr/themes/geekg/images/rss.png)\\
\\
欢迎订阅](https://kexue.fm/feed)
- [![](https://kexue.fm/usr/themes/geekg/images/mail.png)\\
\\
个性邮箱](https://kexue.fm/archives/119)
- [![](https://kexue.fm/usr/themes/geekg/images/Saturn.png)\\
\\
天象信息](https://kexue.fm/ac.html)
- [![](https://kexue.fm/usr/themes/geekg/images/iss.png)\\
\\
观测ISS](https://kexue.fm/archives/41)
- [![](https://kexue.fm/usr/themes/geekg/images/pi.png)\\
\\
LaTeX](https://kexue.fm/latex.html)
- [![](https://kexue.fm/usr/themes/geekg/images/mlogo.png)\\
\\
关于博主](https://kexue.fm/me.html)

欢迎访问“科学空间”，这里将与您共同探讨自然科学，回味人生百态；也期待大家的分享～

- [**千奇百怪** Everything](https://kexue.fm/category/Everything)
- [**天文探索** Astronomy](https://kexue.fm/category/Astronomy)
- [**数学研究** Mathematics](https://kexue.fm/category/Mathematics)
- [**物理化学** Phy-chem](https://kexue.fm/category/Phy-chem)
- [**信息时代** Big-Data](https://kexue.fm/category/Big-Data)
- [**生物自然** Biology](https://kexue.fm/category/Biology)
- [**图片摄影** Photograph](https://kexue.fm/category/Photograph)
- [**问题百科** Questions](https://kexue.fm/category/Questions)
- [**生活/情感** Life-Feeling](https://kexue.fm/category/Life-Feeling)
- [**资源共享** Resources](https://kexue.fm/category/Resources)

- [**千奇百怪**](https://kexue.fm/category/Everything)
- [**天文探索**](https://kexue.fm/category/Astronomy)
- [**数学研究**](https://kexue.fm/category/Mathematics)
- [**物理化学**](https://kexue.fm/category/Phy-chem)
- [**信息时代**](https://kexue.fm/category/Big-Data)
- [**生物自然**](https://kexue.fm/category/Biology)
- [**图片摄影**](https://kexue.fm/category/Photograph)
- [**问题百科**](https://kexue.fm/category/Questions)
- [**生活/情感**](https://kexue.fm/category/Life-Feeling)
- [**资源共享**](https://kexue.fm/category/Resources)

[首页](https://kexue.fm/) [信息时代](https://kexue.fm/category/Big-Data) Keras实现两个优化器：Lookahead和LazyOptimizer

30Jul

# [Keras实现两个优化器：Lookahead和LazyOptimizer](https://kexue.fm/archives/6869)

By 苏剑林 \|
2019-07-30 \|
57809位读者 \|

最近用Keras实现了两个优化器，也算是有点实现技巧，遂放在一起写篇文章简介一下（如果只有一个的话我就不写了）。这两个优化器的名字都挺有意思的，一个是look ahead（往前看？），一个是lazy（偷懒？），难道是两个完全不同的优化思路么？非也非也～只能说发明者们起名字太有创意了。

## Lookahead [\#](https://kexue.fm/archives/6869\#Lookahead)

首先登场的是Lookahead优化器，它源于论文 [《Lookahead Optimizer: k steps forward, 1 step back》](https://papers.cool/arxiv/1907.08610)，是最近才提出来的优化器，有意思的是大牛Hinton和Adam的作者之一Jimmy Ba也出现在了论文作者列表当中，有这两个大神加持，这个优化器的出现便吸引了不少目光。

Lookahead的思路很朴素，准确来说它并不是一个优化器，而是一个使用现有优化器的方案。简单来说它就是下面三个步骤的循环执行：

> 1、备份模型现有的权重\\theta；
>
> 2、从\\theta出发，用指定优化器更新k步，得到新权重\\tilde{\\theta}；
>
> 3、更新模型权重为\\theta \\leftarrow \\theta + \\alpha\\left(\\tilde{\\theta} - \\theta\\right)。

下面则是我的Keras实现，写法在之前的 [《“让Keras更酷一些！”：小众的自定义优化器》](https://kexue.fm/archives/5879) 一文中就提到过了，属于一种“侵入式”的写法：

> [https://github.com/bojone/keras\_lookahead](https://github.com/bojone/keras_lookahead)

用法就很简单了：

```python

```

至于效果，原论文中做了不少实验，有些有轻微提高（cifar10和cifar100那两个），有些提升还比较明显（LSTM做语言模型那个），我自己简单实验了一下，结果是没什么变化。我一直觉得优化器是一个很玄乎的存在，有时候非得SGD才能达到最优效果，有时候又非得Adam才能收敛得下去，总之不能指望单靠换一个优化器就能大幅度提升模型效果。Lookahead的出现，也就是让我们多一种选择罢了，训练时间充足的读者，多去尝试一下就好。

附： [《机器之心的Lookahead的介绍》](https://mp.weixin.qq.com/s/3J-28xd0pyToSy8zzKs1RA)

## LazyOptimizer [\#](https://kexue.fm/archives/6869\#LazyOptimizer)

LazyOptimizer优化器基本上就是为NLP准备的，当然更准确来说是为Embedding层准备的。

LazyOptimizer指出所有带动量的优化器（自然也就包括Adam以及带动量的SGD）都存在一个问题：当前batch中没被采样到的词，依然会使用历史动量来更新，这可能导致Embedding层过拟合（参考 [知乎讨论](https://www.zhihu.com/question/265357659/answer/580469438)）。具体来说，当一个词的被采样过后，它的Embedding的梯度不为0，这个梯度也会被记录在动量中，实际更新是用动量去更新的；在后面的batch中，假如该词没有被采样到，它的Embedding的梯度为0，但是它的动量并不为0，所以该词还是被更新了。这样一来就算没有被反复采样的词，对应的Embedding也被反复更新了，就导致了过拟合。

所以，一个改进的方案是只有当该词被采样过才更新，这就是LazyOptimizer的基本原理了。

在实现上，我们要如何判断一个词有没有被采样过呢？当然终极方法肯定是传入被采样过的词的index了，但这使用上不够友好。我这里使用了一个近似的方法：判断该词的Embedding对应的梯度是否为0，如果为0意味着它“很可能”在当前batch没有被采样到。背后的原理在于，如果它没有被采样到，那么梯度一定为0，如果它被采样到了，那么梯度为0的概率是非常小的，毕竟那么多分量，同时为0的可能性很小，所以这样实现也够用了。

我的Keras实现位于：

> [https://github.com/bojone/keras\_lazyoptimizer](https://github.com/bojone/keras_lazyoptimizer)

这个用法也很简单，就是包装一个带动量的优化器，传入所有Embedding层，使得它成为一个新的Lazy版的优化器：

```python

```

Github中还带有一个 [IMDB的例子](https://github.com/bojone/keras_lazyoptimizer/blob/master/imdb_lstm_test.py)，在这个例子中，如果直接用`Adam(1e-3)`做优化器，那么验证准确率最高只有83.7%左右，而如果用`LazyOptimizer(Adam(1e-3), embedding_layers)`，那么基本上最优验证准确率能跑到84.9%以上，效果可见一斑。总的来说，我觉得Embedding层很大的模型（尤其是以词为单位的模型）都可以试一下，总的来说就是因为Embedding层参数量太大了，减少更新频率，让模型重点去优化其余部分。

> 注：这个LazyOptimizer和标准的LazyOptimizer有点不一样。标准的LazyOptimizer对没有采样过的词，所有相关的缓存量（比如动量等）也不去更新，但我这个实现中，就算该词没有被采样到，该词对应的所有缓存量还是会被更新的，有评测说这样做其实效果会更好些。

## 文末小结 [\#](https://kexue.fm/archives/6869\#%E6%96%87%E6%9C%AB%E5%B0%8F%E7%BB%93)

也没啥内容，就用Keras实现了两个优化器，让用Keras的朋友可以及时尝尝鲜，或者用Keras用得更有意思一些。

_**转载到请包括本文地址：** [https://kexue.fm/archives/6869](https://kexue.fm/archives/6869 "Keras实现两个优化器：Lookahead和LazyOptimizer")_

_**更详细的转载事宜请参考：**_ [《科学空间FAQ》](https://kexue.fm/archives/6508#%E6%96%87%E7%AB%A0%E5%A6%82%E4%BD%95%E8%BD%AC%E8%BD%BD/%E5%BC%95%E7%94%A8 "《科学空间FAQ》")

**如果您还有什么疑惑或建议，欢迎在下方评论区继续讨论。**

**如果您觉得本文还不错，欢迎 [分享](https://kexue.fm/archives/6869#share)/ [打赏](https://kexue.fm/archives/6869#pay) 本文。打赏并非要从中获得收益，而是希望知道科学空间获得了多少读者的真心关注。当然，如果你无视它，也不会影响你的阅读。再次表示欢迎和感谢！**

打赏

![科学空间](https://kexue.fm/usr/themes/geekg/payment/wx.png)

微信打赏

![科学空间](https://kexue.fm/usr/themes/geekg/payment/zfb.png)

支付宝打赏

因为网站后台对打赏并无记录，因此欢迎在打赏时候备注留言。

你还可以 [**点击这里**](http://mail.qq.com/cgi-bin/qm_share?t=qm_mailme&email=tN7d1drY3drrx8H0xcWa19vZ) 或在下方评论区留言来告知你的建议或需求。

**如果您需要引用本文，请参考：**

苏剑林. (Jul. 30, 2019). 《Keras实现两个优化器：Lookahead和LazyOptimizer 》\[Blog post\]. Retrieved from [https://kexue.fm/archives/6869](https://kexue.fm/archives/6869)

@online{kexuefm-6869,

         title={Keras实现两个优化器：Lookahead和LazyOptimizer},

         author={苏剑林},

         year={2019},

         month={Jul},

         url={\\url{https://kexue.fm/archives/6869}},

}


分类： [信息时代](https://kexue.fm/category/Big-Data)    标签： [keras](https://kexue.fm/tag/keras/), [优化器](https://kexue.fm/tag/%E4%BC%98%E5%8C%96%E5%99%A8/)[7 评论](https://kexue.fm/archives/6869#comments)

< [为节约而生：从标准Attention到稀疏Attention](https://kexue.fm/archives/6853 "为节约而生：从标准Attention到稀疏Attention") \| [seq2seq之双向解码](https://kexue.fm/archives/6877 "seq2seq之双向解码") >

### 你也许还对下面的内容感兴趣

- [让炼丹更科学一些（五）：基于梯度精调学习率](https://kexue.fm/archives/11530 "让炼丹更科学一些（五）：基于梯度精调学习率")
- [让炼丹更科学一些（四）：新恒等式，新学习率](https://kexue.fm/archives/11494 "让炼丹更科学一些（四）：新恒等式，新学习率")
- [让炼丹更科学一些（三）：SGD的终点损失收敛](https://kexue.fm/archives/11480 "让炼丹更科学一些（三）：SGD的终点损失收敛")
- [让炼丹更科学一些（二）：将结论推广到无界域](https://kexue.fm/archives/11469 "让炼丹更科学一些（二）：将结论推广到无界域")
- [Muon优化器指南：快速上手与关键细节](https://kexue.fm/archives/11416 "Muon优化器指南：快速上手与关键细节")
- [AdamW的Weight RMS的渐近估计（下）](https://kexue.fm/archives/11404 "AdamW的Weight RMS的渐近估计（下）")
- [流形上的最速下降：5\. 对偶梯度下降](https://kexue.fm/archives/11388 "流形上的最速下降：5. 对偶梯度下降")
- [MuP之上：1. 好模型的三个特征](https://kexue.fm/archives/11340 "MuP之上：1. 好模型的三个特征")
- [AdamW的Weight RMS的渐近估计（上）](https://kexue.fm/archives/11307 "AdamW的Weight RMS的渐近估计（上）")
- [重新思考学习率与Batch Size（四）：EMA](https://kexue.fm/archives/11301 "重新思考学习率与Batch Size（四）：EMA")

[发表你的看法](https://kexue.fm/archives/6869#comment_form)

郭宏宽

August 8th, 2019

LazyOptimizer这个在阿里CTR的论文DIN里见到了类似的问题。背景是对高维稀疏的id特征做embedding的时候，如果不对embedding加正则（比如L2），整个模型无法收敛。但是增加正则以后，所有的index不管在当前batch出现过，梯度都不为0。这个只能传入被采样过的词的index了吧？

[回复评论](https://kexue.fm/archives/6869/comment-page-1?replyTo=11781#respond-post-6869)

[苏剑林](https://kexue.fm/) 发表于
August 8th, 2019

办法总比困难多。比如你说的这种情况，可以直接把权重衰减写到优化器里边去，而不是在loss那里加l2，而且AdamW不是也表明直接权重衰减要比loss里加l2更合理了吗？

[回复评论](https://kexue.fm/archives/6869/comment-page-1?replyTo=11783#respond-post-6869)

郭宏宽 发表于
August 9th, 2019

这个衰减是只针对Embedding层的，不是所有参数。

[回复评论](https://kexue.fm/archives/6869/comment-page-1?replyTo=11784#respond-post-6869)

郭宏宽 发表于
August 9th, 2019

好的，我找个时间试一下

[回复评论](https://kexue.fm/archives/6869/comment-page-1?replyTo=11785#respond-post-6869)

[苏剑林](https://kexue.fm/) 发表于
August 9th, 2019

我知道啊，lazy optimizer的实现里不是已经传入了Embedding层了吗？可以只针对Embedding层加衰减呀。

[回复评论](https://kexue.fm/archives/6869/comment-page-1?replyTo=11787#respond-post-6869)

Xu Zhang

September 5th, 2019

What do you mean "有时候非得SGD才能达到最优效果"? Do you mean that we can't use only one optimizer? Is it true that first train the model with Adam for several epochs and then change the optimizer to SGD for the final polishing? Could you give me an example to explain it? Many thanks

[回复评论](https://kexue.fm/archives/6869/comment-page-1?replyTo=11918#respond-post-6869)

[苏剑林](https://kexue.fm/) 发表于
September 5th, 2019

这类工作有很多～SWATS、AdaBound、RAdam这三个优化器，都算是慢慢从Adam切换到SGD的优化器。

[回复评论](https://kexue.fm/archives/6869/comment-page-1?replyTo=11924#respond-post-6869)

[取消回复](https://kexue.fm/archives/6869#respond-post-6869)

你的大名

电子邮箱

个人网站（选填）

1\. 可以使用LaTeX代码，点击“预览效果”可查看效果；

2\. 可以通过点击评论楼层编号来引用该楼层；

3\. 网站可能会有点卡，如非确认评论失败，请 **不要重复点击提交**。

### 内容速览

[Lookahead](https://kexue.fm/archives/6869#Lookahead)
[LazyOptimizer](https://kexue.fm/archives/6869#LazyOptimizer)
[文末小结](https://kexue.fm/archives/6869#%E6%96%87%E6%9C%AB%E5%B0%8F%E7%BB%93)

### 智能搜索

支持整句搜索！网站自动使用 [结巴分词](https://github.com/fxsjy/jieba) 进行分词，并结合ngrams排序算法给出合理的搜索结果。

### 热门标签

[生成模型](https://kexue.fm/tag/%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B/) [attention](https://kexue.fm/tag/attention/) [优化](https://kexue.fm/tag/%E4%BC%98%E5%8C%96/) [语言模型](https://kexue.fm/tag/%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/) [模型](https://kexue.fm/tag/%E6%A8%A1%E5%9E%8B/) [梯度](https://kexue.fm/tag/%E6%A2%AF%E5%BA%A6/) [网站](https://kexue.fm/tag/%E7%BD%91%E7%AB%99/) [概率](https://kexue.fm/tag/%E6%A6%82%E7%8E%87/) [矩阵](https://kexue.fm/tag/%E7%9F%A9%E9%98%B5/) [优化器](https://kexue.fm/tag/%E4%BC%98%E5%8C%96%E5%99%A8/) [转载](https://kexue.fm/tag/%E8%BD%AC%E8%BD%BD/) [微分方程](https://kexue.fm/tag/%E5%BE%AE%E5%88%86%E6%96%B9%E7%A8%8B/) [分析](https://kexue.fm/tag/%E5%88%86%E6%9E%90/) [天象](https://kexue.fm/tag/%E5%A4%A9%E8%B1%A1/) [深度学习](https://kexue.fm/tag/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/) [积分](https://kexue.fm/tag/%E7%A7%AF%E5%88%86/) [python](https://kexue.fm/tag/python/) [扩散](https://kexue.fm/tag/%E6%89%A9%E6%95%A3/) [力学](https://kexue.fm/tag/%E5%8A%9B%E5%AD%A6/) [无监督](https://kexue.fm/tag/%E6%97%A0%E7%9B%91%E7%9D%A3/) [几何](https://kexue.fm/tag/%E5%87%A0%E4%BD%95/) [节日](https://kexue.fm/tag/%E8%8A%82%E6%97%A5/) [生活](https://kexue.fm/tag/%E7%94%9F%E6%B4%BB/) [文本生成](https://kexue.fm/tag/%E6%96%87%E6%9C%AC%E7%94%9F%E6%88%90/) [数论](https://kexue.fm/tag/%E6%95%B0%E8%AE%BA/)

### 随机文章

- [向量结合复数：常曲率曲线(1)](https://kexue.fm/archives/1381)
- [路径积分系列：2.随机游走模型](https://kexue.fm/archives/3750)
- [【理解黎曼几何】8\. 处处皆几何 (力学几何化)](https://kexue.fm/archives/4046)
- [《环球科学》:超越费曼图](https://kexue.fm/archives/1790)
- [第一次拍摄天体（月球）！](https://kexue.fm/archives/377)
- [精确自由落体运动定律的讨论(二)](https://kexue.fm/archives/337)
- [开始学习数学软件Scilab](https://kexue.fm/archives/1720)
- [齐次多项式不等式的机器证明（差分代换）](https://kexue.fm/archives/2747)
- [行动起来！共同应对全球气候变暖](https://kexue.fm/archives/107)
- [隐藏在动量中的梯度累积：少更新几步，效果反而更好？](https://kexue.fm/archives/8634)

### 最近评论

- [Bin](https://kexue.fm/archives/1990/comment-page-2#comment-29105): 今天偶然从某个论坛看到有人推荐您的博客，定睛一看竟然是华师同院的往届师兄！看到这篇2013年的...
- [Rapture D](https://kexue.fm/archives/11530/comment-page-1#comment-29104): 我有一个问题，为什么不考虑亥姆霍兹定理和斯托克斯公式。
- [mofheka](https://kexue.fm/archives/11390/comment-page-1#comment-29103): 苏神是还在用jax是么？最近在做基于Google Pathway的理念做一个动态版的MPMD框...
- [长琴](https://kexue.fm/archives/11530/comment-page-1#comment-29102): 看懂这篇博客也不是一件容易的事情。
- [AlexLi](https://kexue.fm/archives/9257/comment-page-4#comment-29101): 苏老师，请教一下(7)式中将 \\mu(x\_t) 传给 p\_o 进行推理的操作。 $x\_...
- [tyler\_zxc](https://kexue.fm/archives/7921/comment-page-2#comment-29100): "Performer的思想是将标准的Attention线性化，所以为什么不干脆直接训练一个线性...
- [我](https://kexue.fm/archives/11494/comment-page-1#comment-29099): 似乎并非mHC提出矩阵的思想？之前hyper connection就是了
- [winter](https://kexue.fm/archives/10847/comment-page-1#comment-29098): 苏神您好，假如对于比较均匀的attention weightP，往往呈现long tail分布...
- [苏剑林](https://kexue.fm/archives/8512/comment-page-2#comment-29097): KL散度、JS散度、W距离啥的，都行啊，看你喜欢哪个
- [苏剑林](https://kexue.fm/archives/9119/comment-page-14#comment-29096): 没有绝对公平的对比方法，主要看你关心什么。比如，如果只关心推理成本和推理效果，那么有的方法可以...

### 友情链接

- [Cool Papers](https://papers.cool/)
- [数学研发](https://bbs.emath.ac.cn/)
- [Seatop](http://www.seatop.com.cn/)
- [Xiaoxia](https://xiaoxia.org/)
- [积分表-网络版](https://kexue.fm/sci/integral/index.html)
- [丝路博傲](http://blog.dvxj.com/)
- [数学之家](http://www.2math.cn/)
- [有趣天文奇观](http://interesting-sky.china-vo.org/)
- [TwistedW](http://www.twistedwg.com/)
- [godweiyang](https://godweiyang.com/)
- [AI柠檬](https://blog.ailemon.net/)
- [王登科-DK博客](https://greatdk.com/)
- [ESON](https://blog.eson.org/)
- [枫之羽](https://fzhiy.net/)
- [coding-zuo](https://coding-zuo.github.io/)
- [博科园](https://www.bokeyuan.net/)
- [孔皮皮的博客](https://www.kppkkp.top/)
- [运鹏的博客](https://yunpengtai.top/)
- [jiming.site](https://jiming.site/)
- [OmegaXYZ](https://www.omegaxyz.com/)
- [EAI猩球](https://www.robotech.ink/)
- [文举的博客](https://liwenju0.com/)
- [申请链接](https://kexue.fm/links.html)

[![署名-非商业用途-保持一致](https://kexue.fm/usr/themes/geekg/images/cc.gif)](http://creativecommons.org/licenses/by-nc-nd/2.5/cn/) 本站采用创作共用版权协议，要求署名、非商业用途和保持一致。转载本站内容必须也遵循“ [署名-非商业用途-保持一致](http://creativecommons.org/licenses/by-nc-nd/2.5/cn/)”的创作共用协议。



© 2009-2026 Scientific Spaces. All rights reserved. Theme by [laogui](http://www.laogui.com/). Powered by [Typecho](http://typecho.org/). 备案号: [粤ICP备09093259号-1/2](https://beian.miit.gov.cn/ "粤ICP备09093259号")。