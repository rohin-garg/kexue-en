## SEARCH

## MENU

- [打赏](https://kexue.fm/reward.html)
- [公式](https://kexue.fm/latex.html)
- [天象](https://kexue.fm/ac.html)
- [链接](https://kexue.fm/links.html)
- [时光](https://kexue.fm/me.html)
- [博览](https://kexue.fm/science.html)
- [归档](https://kexue.fm/content.html)

## CATEGORIES

- [千奇百怪](https://kexue.fm/category/Everything)
- [天文探索](https://kexue.fm/category/Astronomy)
- [数学研究](https://kexue.fm/category/Mathematics)
- [物理化学](https://kexue.fm/category/Phy-chem)
- [信息时代](https://kexue.fm/category/Big-Data)
- [生物自然](https://kexue.fm/category/Biology)
- [图片摄影](https://kexue.fm/category/Photograph)
- [问题百科](https://kexue.fm/category/Questions)
- [生活/情感](https://kexue.fm/category/Life-Feeling)
- [资源共享](https://kexue.fm/category/Resources)

## NEWPOSTS

- [让炼丹更科学一些（三）：SGD的终...](https://kexue.fm/archives/11480)
- [让炼丹更科学一些（二）：将结论推广...](https://kexue.fm/archives/11469)
- [滑动平均视角下的权重衰减和学习率](https://kexue.fm/archives/11459)
- [生成扩散模型漫谈（三十一）：预测数...](https://kexue.fm/archives/11428)
- [Muon优化器指南：快速上手与关键细节](https://kexue.fm/archives/11416)
- [AdamW的Weight RMS的...](https://kexue.fm/archives/11404)
- [n个正态随机数的最大值的渐近估计](https://kexue.fm/archives/11390)
- [流形上的最速下降：5\. 对偶梯度下降](https://kexue.fm/archives/11388)
- [低精度Attention可能存在有...](https://kexue.fm/archives/11371)
- [MuP之上：1. 好模型的三个特征](https://kexue.fm/archives/11340)

## COMMENTS

- [cmlin: 本人对这方面不太熟悉，想了解这三个条件的意义及动机，且希望这系...](https://kexue.fm/archives/11340/comment-page-1#comment-29031)
- [喝一口可乐: 理解了，感谢苏神回复，数学上给出建模分析确实清晰了很多，再次感...](https://kexue.fm/archives/10958/comment-page-3#comment-29030)
- [CuddleSabe1: 感觉普通的 flow matching 可以看成 degrad...](https://kexue.fm/archives/10958/comment-page-1#comment-29029)
- [岁月如书: 受教了，感谢](https://kexue.fm/archives/11126/comment-page-3#comment-29028)
- [苏剑林: 是](https://kexue.fm/archives/11126/comment-page-3#comment-29027)
- [岁月如书: 哦哦，原来是有实验结论，那是我盲目了。多问一句，你说的atte...](https://kexue.fm/archives/11126/comment-page-3#comment-29026)
- [苏剑林: attention sink指的是第一个token的atten...](https://kexue.fm/archives/11126/comment-page-3#comment-29025)
- [苏剑林: 这也许是好事呢？SGD倒是保留了模长，但它就普遍不如不保留模长...](https://kexue.fm/archives/11459/comment-page-1#comment-29024)
- [岁月如书: maxlogit 是attention qk乘积中出现了大值，...](https://kexue.fm/archives/11126/comment-page-3#comment-29023)
- [岁月如书: \[comment=29016\]苏剑林\[/comment\]他通过...](https://kexue.fm/archives/11459/comment-page-1#comment-29022)

## USERLOGIN

- [登录](https://kexue.fm/admin/login.php)

[科学空间\|Scientific Spaces](https://kexue.fm)

- [登录](https://kexue.fm/admin/login.php)
- [打赏](https://kexue.fm/reward.html)
- [公式](https://kexue.fm/latex.html)
- [天象](https://kexue.fm/ac.html)
- [链接](https://kexue.fm/links.html)
- [时光](https://kexue.fm/me.html)
- [博览](https://kexue.fm/science.html)
- [归档](https://kexue.fm/content.html)

渴望成为一个小飞侠

- [欢迎订阅](https://kexue.fm/feed)
- [个性邮箱](https://kexue.fm/archives/119)
- [天象信息](https://kexue.fm/ac.html)
- [观测ISS](https://kexue.fm/archives/41)
- [LaTeX](https://kexue.fm/latex.html)
- [关于博主](https://kexue.fm/me.html)

欢迎访问“科学空间”，这里将与您共同探讨自然科学，回味人生百态；也期待大家的分享～

- [**千奇百怪** Everything](https://kexue.fm/category/Everything)
- [**天文探索** Astronomy](https://kexue.fm/category/Astronomy)
- [**数学研究** Mathematics](https://kexue.fm/category/Mathematics)
- [**物理化学** Phy-chem](https://kexue.fm/category/Phy-chem)
- [**信息时代** Big-Data](https://kexue.fm/category/Big-Data)
- [**生物自然** Biology](https://kexue.fm/category/Biology)
- [**图片摄影** Photograph](https://kexue.fm/category/Photograph)
- [**问题百科** Questions](https://kexue.fm/category/Questions)
- [**生活/情感** Life-Feeling](https://kexue.fm/category/Life-Feeling)
- [**资源共享** Resources](https://kexue.fm/category/Resources)

- [**千奇百怪**](https://kexue.fm/category/Everything)
- [**天文探索**](https://kexue.fm/category/Astronomy)
- [**数学研究**](https://kexue.fm/category/Mathematics)
- [**物理化学**](https://kexue.fm/category/Phy-chem)
- [**信息时代**](https://kexue.fm/category/Big-Data)
- [**生物自然**](https://kexue.fm/category/Biology)
- [**图片摄影**](https://kexue.fm/category/Photograph)
- [**问题百科**](https://kexue.fm/category/Questions)
- [**生活/情感**](https://kexue.fm/category/Life-Feeling)
- [**资源共享**](https://kexue.fm/category/Resources)

[首页](https://kexue.fm) [数学研究](https://kexue.fm/category/Mathematics) 重新思考学习率与Batch Size（一）：现状

1Sep

# [重新思考学习率与Batch Size（一）：现状](https://kexue.fm/archives/11260)

By 苏剑林 \|
2025-09-01 \|
28448位读者\|

在之前的文章 [《当Batch Size增大时，学习率该如何随之变化？》](https://kexue.fm/archives/10542) 和 [《Adam的epsilon如何影响学习率的Scaling Law？》](https://kexue.fm/archives/10563) 中，我们从理论上讨论了学习率随Batch Size的变化规律，其中比较经典的部分是由OpenAI提出的展开到二阶的分析。然而，当我们要处理非SGD优化器时，这套分析方法的计算过程往往会相当复杂，有种无从下手的感觉。

接下来的几篇文章，笔者将重新整理和思考上述文章中的相关细节，尝试简化其中的一些推导步骤，给出一条更通用、更轻盈的推导路径，并且探讨推广到Muon优化器的可能性。

## 方法大意 [\#](https://kexue.fm/kexue.fm\#%E6%96%B9%E6%B3%95%E5%A4%A7%E6%84%8F)

首先回顾一下之前的分析方法。在 [《当Batch Size增大时，学习率该如何随之变化？》](https://kexue.fm/archives/10542) 中，我们介绍了多种分析学习率与Batch Size规律的思路，其中OpenAI在 [《An Empirical Model of Large-Batch Training》](https://papers.cool/arxiv/1812.06162) 提出的二阶近似分析占了主要篇幅，本文也是沿用同样的思路。

接着需要引入一些记号。设损失函数为$\\mathcal{L}(\\boldsymbol{w})$，$\\boldsymbol{w}\\in\\mathbb{R}^N$是参数向量，$\\boldsymbol{g}$是它的梯度。注意理想的损失函数是在全体训练样本上算的期望，但实际我们只能采样一个Batch来算，这导致梯度也带有随机性，我们将单个样本的梯度记为$\\tilde{\\boldsymbol{g}}$，它的均值就是$\\boldsymbol{g}$，而协方差矩阵记为$\\boldsymbol{\\Sigma}$；当Batch Size为$B$时，梯度记为$\\tilde{\\boldsymbol{g}}\_B$，它的均值还是$\\boldsymbol{g}$，但协方差矩阵变为$\\boldsymbol{\\Sigma}/B$。

进一步地，设当前学习率为$\\eta$，更新向量为$\\tilde{\\boldsymbol{\\varphi}}\_B$，那么更新后的损失函数将是
\\begin{equation}\\begin{aligned}
\\mathcal{L}(\\boldsymbol{w} - \\eta\\tilde{\\boldsymbol{\\varphi}}\_B) \\approx&\\, \\mathcal{L}(\\boldsymbol{w}) - \\eta \\tilde{\\boldsymbol{\\varphi}}\_B^{\\top}\\boldsymbol{g} + \\frac{1}{2}\\eta^2\\tilde{\\boldsymbol{\\varphi}}\_B^{\\top}\\boldsymbol{H}\\tilde{\\boldsymbol{\\varphi}}\_B \\\
=&\\, \\mathcal{L}(\\boldsymbol{w}) - \\eta \\tilde{\\boldsymbol{\\varphi}}\_B^{\\top}\\boldsymbol{g} + \\frac{1}{2}\\eta^2\\newcommand{tr}{\\mathop{\\text{tr}}}\\tr(\\tilde{\\boldsymbol{\\varphi}}\_B\\tilde{\\boldsymbol{\\varphi}}\_B^{\\top}\\boldsymbol{H})
\\end{aligned}\\end{equation}
右侧我们泰勒展开到了二阶，$\\boldsymbol{H}$是Hessian矩阵，$\\tr$是矩阵的迹，第二个等号用到了$\\tr(\\boldsymbol{A}\\boldsymbol{B})=\\tr(\\boldsymbol{B}\\boldsymbol{A})$这个恒等式。为了得到一个确定性的结果，我们对两边求期望：
\\begin{equation}\\mathbb{E}\[\\mathcal{L}(\\boldsymbol{w} - \\eta\\tilde{\\boldsymbol{\\varphi}}\_B)\] \\approx \\mathcal{L}(\\boldsymbol{w}) - \\eta\\mathbb{E}\[\\tilde{\\boldsymbol{\\varphi}}\_B\]^{\\top}\\boldsymbol{g} + \\frac{1}{2}\\eta^2 \\tr(\\mathbb{E}\[\\tilde{\\boldsymbol{\\varphi}}\_B\\tilde{\\boldsymbol{\\varphi}}\_B^{\\top}\]\\boldsymbol{H})\\end{equation}
我们把右端看成是关于$\\eta$的二次函数，并假设二次项系数是正的（更强的假设是$\\boldsymbol{H}$矩阵是正定的），那么可以得到最小值点
\\begin{equation}\\eta^\* \\approx \\frac{\\mathbb{E}\[\\tilde{\\boldsymbol{\\varphi}}\_B\]^{\\top}\\boldsymbol{g}}{\\tr(\\mathbb{E}\[\\tilde{\\boldsymbol{\\varphi}}\_B\\tilde{\\boldsymbol{\\varphi}}\_B^{\\top}\]\\boldsymbol{H})}\\end{equation}
这便是平均来说让损失函数下降最快的学习率，是学习率的理论最优解。我们要做的事情，就是针对具体的$\\tilde{\\boldsymbol{\\varphi}}\_B$算出$\\mathbb{E}\[\\tilde{\\boldsymbol{\\varphi}}\_B\]$和$\\mathbb{E}\[\\tilde{\\boldsymbol{\\varphi}}\_B\\tilde{\\boldsymbol{\\varphi}}\_B^{\\top}\]$，然后从上式析出它与Batch Size（即$B$）的关系。

## 热身练习 [\#](https://kexue.fm/kexue.fm\#%E7%83%AD%E8%BA%AB%E7%BB%83%E4%B9%A0)

作为第一个例子，我们自然是考虑最简单的SGD，此时有$\\tilde{\\boldsymbol{\\varphi}}\_B=\\tilde{\\boldsymbol{g}}\_B$，那么简单可得$\\mathbb{E}\[\\tilde{\\boldsymbol{\\varphi}}\_B\]=\\boldsymbol{g}$以及$\\mathbb{E}\[\\tilde{\\boldsymbol{\\varphi}}\_B\\tilde{\\boldsymbol{\\varphi}}\_B^{\\top}\]=\\boldsymbol{g}\\boldsymbol{g}^{\\top} + \\boldsymbol{\\Sigma}/B$，于是有
\\begin{equation}\\eta^\* \\approx \\frac{\\boldsymbol{g}^{\\top}\\boldsymbol{g}}{\\tr((\\boldsymbol{g}\\boldsymbol{g}^{\\top} + \\boldsymbol{\\Sigma}/B)\\boldsymbol{H})} = \\frac{\\boldsymbol{g}^{\\top}\\boldsymbol{g}}{\\boldsymbol{g}^{\\top}\\boldsymbol{H}\\boldsymbol{g} + \\tr(\\boldsymbol{\\Sigma}\\boldsymbol{H})/B} = \\frac{\\eta\_{\\max}}{1 + \\mathcal{B}\_{\\text{noise}}/B}\\label{eq:eta-sgd}\\end{equation}
其中
\\begin{equation}\\eta\_{\\max} = \\frac{\\boldsymbol{g}^{\\top}\\boldsymbol{g}}{\\boldsymbol{g}^{\\top}\\boldsymbol{H}\\boldsymbol{g}},\\qquad\\mathcal{B}\_{\\text{noise}} = \\frac{\\tr(\\boldsymbol{\\Sigma}\\boldsymbol{H})}{\\boldsymbol{g}^{\\top}\\boldsymbol{H}\\boldsymbol{g}}\\end{equation}

对于结果$\\eqref{eq:eta-sgd}$，我们可以有多种解读方式。首先，它是一个单调递增但有上界的函数，上界为$\\eta\_{\\max}$，这表明学习率不能无限增加，相比简单的线性律或者平方根律，它更符合我们的直觉认知；当$B \\ll \\mathcal{B}\_{\\text{noise}}$时，我们有
\\begin{equation}\\eta^\* \\approx \\frac{\\eta\_{\\max}}{1 + \\mathcal{B}\_{\\text{noise}}/B} \\approx \\frac{\\eta\_{\\max}}{\\mathcal{B}\_{\\text{noise}}/B} = \\eta\_{\\max} B / \\mathcal{B}\_{\\text{noise}}\\end{equation}
这表明在Batch Size比较小时，SGD的学习率与Batch Size确实呈线性关系，同时也暗示了$\\mathcal{B}\_{\\text{noise}}$是一个关键统计量。不过$\\mathcal{B}\_{\\text{noise}}$的定义依赖于Hessian矩阵$\\boldsymbol{H}$，这在LLM中是几乎不可能精确计算的，所以实践中我们通常假设它是单位阵（的若干倍），得到一个简化的形式
\\begin{equation}\\mathcal{B}\_{\\text{simple}} = \\frac{\\tr(\\boldsymbol{\\Sigma})}{\\boldsymbol{g}^{\\top}\\boldsymbol{g}}\\end{equation}
该结果具有噪音强度（$\\tr(\\boldsymbol{\\Sigma})$)除以信号强度（$\\boldsymbol{g}^{\\top}\\boldsymbol{g}$）的形式，它其实就是信噪比的倒数，它表明信噪比越小，那么就需要更大的Batch Size才能用上相同的$\\eta\_{\\max}$，这也跟我们的直觉认知相符。$\\tr(\\boldsymbol{\\Sigma})$只依赖于$\\boldsymbol{\\Sigma}$的对角线元素，这表明我们只需要将每个参数独立地估计均值和方差，这在实践上是可行的。

## 数据效率 [\#](https://kexue.fm/kexue.fm\#%E6%95%B0%E6%8D%AE%E6%95%88%E7%8E%87)

除了学习率与Batch Size的直接关系外，笔者认为由此衍生出来的关于训练数据量和训练步数的渐近关系，也是必须要学习的精彩部分。特别地，这个结论似乎比学习率的关系式$\\eqref{eq:eta-sgd}$更为通用，因为后面我们将会看到，SignSGD也能得到同样形式的结论，但它的学习率规律并不是式$\\eqref{eq:eta-sgd}$。

原论文对这部分的讨论比较复杂，下面的推导是经过笔者简化的。具体来说，我们将$\\eta^\*$代回到$\\mathcal{L}(\\boldsymbol{w} - \\eta\\tilde{\\boldsymbol{g}}\_B)$，将得到
\\begin{equation}\\overline{\\Delta\\mathcal{L}} = \\mathcal{L}(\\boldsymbol{w}) - \\mathbb{E}\[\\mathcal{L}(\\boldsymbol{w} - \\eta^\*\\tilde{\\boldsymbol{g}}\_B)\] \\approx \\frac{\\Delta\\mathcal{L}\_{\\max}}{1 + \\mathcal{B}\_{\\text{noise}}/B}\\end{equation}
其中$\\Delta\\mathcal{L}\_{\\max} = \\frac{(\\boldsymbol{g}^{\\top}\\boldsymbol{g})^2}{2\\boldsymbol{g}^{\\top}\\boldsymbol{H}\\boldsymbol{g}}$。怎么理解这个结果呢？首先，它是关于$B$的单调递增函数，当$B\\to\\infty$时等于$\\Delta\\mathcal{L}\_{\\max}$，换言之如果我们能开无穷大的Batch Size，那么每一步的损失下降量是$\\Delta\\mathcal{L}\_{\\max}$，此时所需的训练步数最少，记为$S\_{\\min}$。

如果Batch Size是有限值，那么每一步的损失下降量平均来说只有$\\overline{\\Delta\\mathcal{L}}$，这意味着平均而言我们要花$1 + \\mathcal{B}\_{\\text{noise}}/B$步，才能达到无穷大Batch Size时1步的下降量，于是为了达到相同的损失，我们要训练$S = (1 + \\mathcal{B}\_{\\text{noise}}/B)S\_{\\min}$步。

由于Batch Size为$B$，所以很容易得出训练消耗的数据总量为$E = BS = (B + \\mathcal{B}\_{\\text{noise}})S\_{\\min}$。从这个结果可以看出，增大Batch Size后，想要达到相同的效果，我们还需要适当增加数据量$E$；当$B\\to 0$时，所需要的数据量最少，为$E\_{\\min} = \\mathcal{B}\_{\\text{noise}}S\_{\\min}$。利用这些记号，我们可以写出
\\begin{equation}\\left(\\frac{S}{S\_{\\min}} - 1\\right)\\left(\\frac{E}{E\_{\\min}} - 1\\right) = 1\\end{equation}
这便是训练数据量和训练步数的经典关系式，它有两个参数$S\_{\\min},E\_{\\min}$，我们也可以通过实验搜索多个$(S,E)$来拟合上式，从而估计$S\_{\\min},E\_{\\min}$，进而可以估算$\\mathcal{B}\_{\\text{noise}} = E\_{\\min} / S\_{\\min}$。更多分析细节请看回之前的文章 [《当Batch Size增大时，学习率该如何随之变化？》](https://kexue.fm/archives/10542) 或OpenAI的原论文 [《An Empirical Model of Large-Batch Training》](https://papers.cool/arxiv/1812.06162)。

## 困难分析 [\#](https://kexue.fm/kexue.fm\#%E5%9B%B0%E9%9A%BE%E5%88%86%E6%9E%90)

前面写了那么多，都还停留在SGD中。从计算角度看，SGD是平凡的，真正复杂的是$\\tilde{\\boldsymbol{\\varphi}}\_B$非线性地依赖于$\\tilde{\\boldsymbol{g}}\_B$的情形，比如SignSGD对应于$\\newcommand{sign}{\\mathop{\\text{sign}}}\\tilde{\\boldsymbol{\\varphi}}\_B=\\sign(\\tilde{\\boldsymbol{g}}\_B)$，在理论分析中它经常用作Adam的近似，更准确的近似则是考虑了$\\epsilon$的SoftSignSGD，我们在 [《Adam的epsilon如何影响学习率的Scaling Law？》](https://kexue.fm/archives/10563) 尝试过分析它。

在这些非线性场景下，$\\mathbb{E}\[\\tilde{\\boldsymbol{\\varphi}}\_B\]$和$\\mathbb{E}\[\\tilde{\\boldsymbol{\\varphi}}\_B\\tilde{\\boldsymbol{\\varphi}}\_B^{\\top}\]$的计算往往是相当困难的，即便我们将$\\tilde{\\boldsymbol{g}}\_B$的分布假设为简单的正态分布也是如此（注意，在SGD的分析中，我们并不需要对它的分布形式做正态假设）。比如，在之前的文章中，对于$\\tilde{\\boldsymbol{\\varphi}}\_B=\\sign(\\tilde{\\boldsymbol{g}}\_B)$的SignSGD，为了计算$\\mathbb{E}\[\\tilde{\\boldsymbol{\\varphi}}\_B\]$，我们经历了如下步骤：

> 1、假设$\\tilde{\\boldsymbol{g}}\_B$的分量相互独立，问题简化为单个分量$\\tilde{\\varphi}\_B=\\sign(\\tilde{g}\_B)$（没有加粗）的期望；
>
> 2、假设$\\tilde{g}\_B$（此时是一个标量）服从正态分布，那么就可以算出$\\mathbb{E}\[\\tilde{\\varphi}\_B\]$，答案要用$\\newcommand{erf}{\\mathop{\\text{erf}}}\\erf$函数来表示；
>
> 3、将$\\erf$函数用$x/\\sqrt{x^2+c}$形式的函数近似，简化结果。

也就是说，我们要经过一堆弯弯绕绕的步骤，才勉强算出一个可以分析下去的近似结果（这个过程首次出现在Tencent的论文 [《Surge Phenomenon in Optimal Learning Rate and Batch Size Scaling》](https://papers.cool/arxiv/2405.14578)），而且这已经算是简单的了，因为如果是SoftSignSGD，则更加复杂：

> 1、假设$\\tilde{\\boldsymbol{g}}\_B$的分量相互独立，问题简化为单个分量$\\tilde{\\varphi}\_B=\\newcommand{softsign}{\\mathop{\\text{softsign}}}\\softsign(\\tilde{g}\_B, \\epsilon)$的期望；
>
> 2、将$\\softsign$函数用分段线性函数近似，这样才能算出下面的积分；
>
> 3、假设$\\tilde{g}\_B$服从正态分布，结合第2步的近似，可以算出$\\mathbb{E}\[\\tilde{\\varphi}\_B\]$，答案是包含$\\erf$的复杂函数；
>
> 4、将复杂函数用$x/\\sqrt{x^2+c}$形式的函数近似，简化结果。

事情还没完。费那么大劲，加那么多假设，我们才堪堪算出$\\mathbb{E}\[\\tilde{\\boldsymbol{\\varphi}}\_B\]$，接着还要算$\\mathbb{E}\[\\tilde{\\boldsymbol{\\varphi}}\_B\\tilde{\\boldsymbol{\\varphi}}\_B^{\\top}\]$，这往往更加复杂（SignSGD是个例外，因为$\\sign(x)^2$一定是1，所以反而简单了）。然而，计算的复杂性还是次要的，主要是这些步骤看上去没有任何能推广的规律，似乎只能具体问题具体分析的样子，这就让人觉得非常心累。

## 未完待续 [\#](https://kexue.fm/kexue.fm\#%E6%9C%AA%E5%AE%8C%E5%BE%85%E7%BB%AD)

为了避免文章过长，本文就先到这里了，主要先简单回顾一下现有的分析结果和计算困难。在下一篇文章中，笔者将会介绍自己为了降低推导过程中的心智负担所做的一些尝试。

_**转载到请包括本文地址：** [https://kexue.fm/archives/11260](https://kexue.fm/archives/11260)_

_**更详细的转载事宜请参考：**_ [《科学空间FAQ》](https://kexue.fm/archives/6508#%E6%96%87%E7%AB%A0%E5%A6%82%E4%BD%95%E8%BD%AC%E8%BD%BD/%E5%BC%95%E7%94%A8)

**如果您还有什么疑惑或建议，欢迎在下方评论区继续讨论。**

**如果您觉得本文还不错，欢迎 [分享](https://kexue.fm/kexue.fm#share)/ [打赏](https://kexue.fm/kexue.fm#pay) 本文。打赏并非要从中获得收益，而是希望知道科学空间获得了多少读者的真心关注。当然，如果你无视它，也不会影响你的阅读。再次表示欢迎和感谢！**

打赏

微信打赏

支付宝打赏

因为网站后台对打赏并无记录，因此欢迎在打赏时候备注留言。你还可以 [**点击这里**](http://mail.qq.com/cgi-bin/qm_share?t=qm_mailme&email=tN7d1drY3drrx8H0xcWa19vZ) 或在下方评论区留言来告知你的建议或需求。

**如果您需要引用本文，请参考：**

苏剑林. (Sep. 01, 2025). 《重新思考学习率与Batch Size（一）：现状 》\[Blog post\]. Retrieved from [https://kexue.fm/archives/11260](https://kexue.fm/archives/11260)

@online{kexuefm-11260,
        title={重新思考学习率与Batch Size（一）：现状},
        author={苏剑林},
        year={2025},
        month={Sep},
        url={\\url{https://kexue.fm/archives/11260}},
}

分类： [数学研究](https://kexue.fm/category/Mathematics)    标签： [梯度](https://kexue.fm/tag/%E6%A2%AF%E5%BA%A6/), [学习率](https://kexue.fm/tag/%E5%AD%A6%E4%B9%A0%E7%8E%87/), [优化器](https://kexue.fm/tag/%E4%BC%98%E5%8C%96%E5%99%A8/), [尺度定律](https://kexue.fm/tag/%E5%B0%BA%E5%BA%A6%E5%AE%9A%E5%BE%8B/)[抢沙发](https://kexue.fm/archives/11260#comments)

< [Cool Papers更新：简单适配Zotero Connector](https://kexue.fm/archives/11250) \| [为什么Adam的Update RMS是0.2？](https://kexue.fm/archives/11267) >

### 你也许还对下面的内容感兴趣

- [让炼丹更科学一些（三）：SGD的终点损失收敛](https://kexue.fm/archives/11480)
- [让炼丹更科学一些（二）：将结论推广到无界域](https://kexue.fm/archives/11469)
- [滑动平均视角下的权重衰减和学习率](https://kexue.fm/archives/11459)
- [Muon优化器指南：快速上手与关键细节](https://kexue.fm/archives/11416)
- [AdamW的Weight RMS的渐近估计（下）](https://kexue.fm/archives/11404)
- [流形上的最速下降：5\. 对偶梯度下降](https://kexue.fm/archives/11388)
- [MuP之上：1. 好模型的三个特征](https://kexue.fm/archives/11340)
- [DiVeQ：一种非常简洁的VQ训练方案](https://kexue.fm/archives/11328)
- [AdamW的Weight RMS的渐近估计（上）](https://kexue.fm/archives/11307)
- [重新思考学习率与Batch Size（四）：EMA](https://kexue.fm/archives/11301)

[发表你的看法](https://kexue.fm/kexue.fm#comment_form)

[取消回复](https://kexue.fm/archives/11260#respond-post-11260)

你的大名

电子邮箱

个人网站（选填）

1\. 可以使用LaTeX代码，点击“预览效果”可查看效果；2. 可以通过点击评论楼层编号来引用该楼层；3. 网站可能会有点卡，如非确认评论失败，请 **不要重复点击提交**。

### 内容速览

[方法大意](https://kexue.fm/kexue.fm#%E6%96%B9%E6%B3%95%E5%A4%A7%E6%84%8F)
[热身练习](https://kexue.fm/kexue.fm#%E7%83%AD%E8%BA%AB%E7%BB%83%E4%B9%A0)
[数据效率](https://kexue.fm/kexue.fm#%E6%95%B0%E6%8D%AE%E6%95%88%E7%8E%87)
[困难分析](https://kexue.fm/kexue.fm#%E5%9B%B0%E9%9A%BE%E5%88%86%E6%9E%90)
[未完待续](https://kexue.fm/kexue.fm#%E6%9C%AA%E5%AE%8C%E5%BE%85%E7%BB%AD)

### 智能搜索

支持整句搜索！网站自动使用 [结巴分词](https://github.com/fxsjy/jieba) 进行分词，并结合ngrams排序算法给出合理的搜索结果。

### 热门标签

[生成模型](https://kexue.fm/tag/%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B/) [attention](https://kexue.fm/tag/attention/) [优化](https://kexue.fm/tag/%E4%BC%98%E5%8C%96/) [语言模型](https://kexue.fm/tag/%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/) [模型](https://kexue.fm/tag/%E6%A8%A1%E5%9E%8B/) [网站](https://kexue.fm/tag/%E7%BD%91%E7%AB%99/) [梯度](https://kexue.fm/tag/%E6%A2%AF%E5%BA%A6/) [概率](https://kexue.fm/tag/%E6%A6%82%E7%8E%87/) [矩阵](https://kexue.fm/tag/%E7%9F%A9%E9%98%B5/) [优化器](https://kexue.fm/tag/%E4%BC%98%E5%8C%96%E5%99%A8/) [转载](https://kexue.fm/tag/%E8%BD%AC%E8%BD%BD/) [微分方程](https://kexue.fm/tag/%E5%BE%AE%E5%88%86%E6%96%B9%E7%A8%8B/) [分析](https://kexue.fm/tag/%E5%88%86%E6%9E%90/) [天象](https://kexue.fm/tag/%E5%A4%A9%E8%B1%A1/) [深度学习](https://kexue.fm/tag/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/) [积分](https://kexue.fm/tag/%E7%A7%AF%E5%88%86/) [python](https://kexue.fm/tag/python/) [扩散](https://kexue.fm/tag/%E6%89%A9%E6%95%A3/) [力学](https://kexue.fm/tag/%E5%8A%9B%E5%AD%A6/) [无监督](https://kexue.fm/tag/%E6%97%A0%E7%9B%91%E7%9D%A3/) [几何](https://kexue.fm/tag/%E5%87%A0%E4%BD%95/) [节日](https://kexue.fm/tag/%E8%8A%82%E6%97%A5/) [生活](https://kexue.fm/tag/%E7%94%9F%E6%B4%BB/) [文本生成](https://kexue.fm/tag/%E6%96%87%E6%9C%AC%E7%94%9F%E6%88%90/) [数论](https://kexue.fm/tag/%E6%95%B0%E8%AE%BA/)

### 随机文章

- [生成扩散模型漫谈（十六）：W距离 ≤ 得分匹配](https://kexue.fm/archives/9467)
- [网站PR升到3了！](https://kexue.fm/archives/335)
- [【NASA每日一图】超新星遗骸E0102-72](https://kexue.fm/archives/112)
- [生成扩散模型漫谈（十九）：作为扩散ODE的GAN](https://kexue.fm/archives/9662)
- [《新理解矩阵2》：矩阵是什么？](https://kexue.fm/archives/1768)
- [线性注意力简史：从模仿、创新到反哺](https://kexue.fm/archives/11033)
- [一篇费曼的介绍](https://kexue.fm/archives/1686)
- [第一学期结束了](https://kexue.fm/archives/352)
- [\[欧拉数学\]凸多面体的面、顶、棱公式](https://kexue.fm/archives/1496)
- [一维弹簧的运动（下）](https://kexue.fm/archives/2434)

### 最近评论

- [cmlin](https://kexue.fm/archives/11340/comment-page-1#comment-29031): 本人对这方面不太熟悉，想了解这三个条件的意义及动机，且希望这系列可以继续写下去。以下想发表一些...
- [喝一口可乐](https://kexue.fm/archives/10958/comment-page-3#comment-29030): 理解了，感谢苏神回复，数学上给出建模分析确实清晰了很多，再次感谢苏神回复！
- [CuddleSabe1](https://kexue.fm/archives/10958/comment-page-1#comment-29029): 感觉普通的 flow matching 可以看成 degrade-aware image de...
- [岁月如书](https://kexue.fm/archives/11126/comment-page-3#comment-29028): 受教了，感谢
- [苏剑林](https://kexue.fm/archives/11126/comment-page-3#comment-29027): 是
- [岁月如书](https://kexue.fm/archives/11126/comment-page-3#comment-29026): 哦哦，原来是有实验结论，那是我盲目了。多问一句，你说的attention + output g...
- [苏剑林](https://kexue.fm/archives/11126/comment-page-3#comment-29025): attention sink指的是第一个token的attention普遍不可忽略，不一定是爆...
- [苏剑林](https://kexue.fm/archives/11459/comment-page-1#comment-29024): 这也许是好事呢？SGD倒是保留了模长，但它就普遍不如不保留模长的SignSGD或者Normal...
- [岁月如书](https://kexue.fm/archives/11126/comment-page-3#comment-29023): maxlogit 是attention qk乘积中出现了大值，attention sink等于...
- [岁月如书](https://kexue.fm/archives/11459/comment-page-1#comment-29022): \[comment=29016\]苏剑林\[/comment\]他通过Newton-schulz迭代近...

### 友情链接

- [Cool Papers](https://papers.cool)
- [数学研发](https://bbs.emath.ac.cn)
- [Seatop](http://www.seatop.com.cn/)
- [Xiaoxia](https://xiaoxia.org/)
- [积分表-网络版](https://kexue.fm/sci/integral/index.html)
- [丝路博傲](http://blog.dvxj.com/)
- [数学之家](http://www.2math.cn/)
- [有趣天文奇观](http://interesting-sky.china-vo.org/)
- [TwistedW](http://www.twistedwg.com/)
- [godweiyang](https://godweiyang.com/)
- [AI柠檬](https://blog.ailemon.net/)
- [王登科-DK博客](https://greatdk.com)
- [ESON](https://blog.eson.org/)
- [枫之羽](https://fzhiy.net/)
- [coding-zuo](https://coding-zuo.github.io/)
- [博科园](https://www.bokeyuan.net/)
- [孔皮皮的博客](https://www.kppkkp.top/)
- [运鹏的博客](https://yunpengtai.top/)
- [jiming.site](https://jiming.site/)
- [OmegaXYZ](https://www.omegaxyz.com/)
- [EAI猩球](https://www.robotech.ink/)
- [文举的博客](https://liwenju0.com/)
- [申请链接](https://kexue.fm/links.html)

本站采用创作共用版权协议，要求署名、非商业用途和保持一致。转载本站内容必须也遵循“ [署名-非商业用途-保持一致](http://creativecommons.org/licenses/by-nc-nd/2.5/cn/)”的创作共用协议。
© 2009-2025 Scientific Spaces. All rights reserved. Theme by [laogui](http://www.laogui.com). Powered by [Typecho](http://typecho.org). 备案号: [粤ICP备09093259号-1/2](https://beian.miit.gov.cn/)。