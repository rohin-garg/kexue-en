## SEARCH

## MENU

- [打赏](https://kexue.fm/reward.html)
- [公式](https://kexue.fm/latex.html)
- [天象](https://kexue.fm/ac.html)
- [链接](https://kexue.fm/links.html)
- [时光](https://kexue.fm/me.html)
- [博览](https://kexue.fm/science.html)
- [归档](https://kexue.fm/content.html)

## CATEGORIES

- [千奇百怪](https://kexue.fm/category/Everything)
- [天文探索](https://kexue.fm/category/Astronomy)
- [数学研究](https://kexue.fm/category/Mathematics)
- [物理化学](https://kexue.fm/category/Phy-chem)
- [信息时代](https://kexue.fm/category/Big-Data)
- [生物自然](https://kexue.fm/category/Biology)
- [图片摄影](https://kexue.fm/category/Photograph)
- [问题百科](https://kexue.fm/category/Questions)
- [生活/情感](https://kexue.fm/category/Life-Feeling)
- [资源共享](https://kexue.fm/category/Resources)

## NEWPOSTS

- [msign算子的Newton-Sc...](https://kexue.fm/archives/10996)
- [从无穷范数求导到等值振荡定理](https://kexue.fm/archives/10972)
- [生成扩散模型漫谈（三十）：从瞬时速...](https://kexue.fm/archives/10958)
- [MoE环游记：5、均匀分布的反思](https://kexue.fm/archives/10945)
- [msign算子的Newton-Sc...](https://kexue.fm/archives/10922)
- [Transformer升级之路：2...](https://kexue.fm/archives/10907)
- [一道概率不等式：盯着它到显然成立为止！](https://kexue.fm/archives/10902)
- [SVD的导数](https://kexue.fm/archives/10878)
- [智能家居之手搓一套能接入米家的零冷水装置](https://kexue.fm/archives/10869)
- [Transformer升级之路：1...](https://kexue.fm/archives/10862)

## COMMENTS

- [贵阳机场接机: 怎么不更新啦](https://kexue.fm/archives/1490/comment-page-1#comment-27807)
- [czvzb: 具身智能模型目前主流也是在使用扩散和流匹配这类方法来预测动作。...](https://kexue.fm/archives/10958/comment-page-1#comment-27806)
- [Shawn\_yang: 不好意思，以为网页卡了0.0点了三下](https://kexue.fm/archives/10945/comment-page-1#comment-27805)
- [Shawn\_yang: 苏神，关于您所说的：“推理阶段可以事先预估Routed Exp...](https://kexue.fm/archives/10945/comment-page-1#comment-27804)
- [Shawn\_yang: 苏神，关于您所说的：“推理阶段可以事先预估Routed Exp...](https://kexue.fm/archives/10945/comment-page-1#comment-27803)
- [Shawn\_yang: 苏神，关于您所说的：“推理阶段可以事先预估Routed Exp...](https://kexue.fm/archives/10945/comment-page-1#comment-27802)
- [OceanYU: 您好，关于由式（7）推导出高斯分布，我这里有一点问题，式（7）...](https://kexue.fm/archives/9164/comment-page-4#comment-27801)
- [jorjiang: 训练和prefill这个compute-bound阶段不做矩阵...](https://kexue.fm/archives/10907/comment-page-2#comment-27800)
- [amy: 苏老师，您有关注傅里叶旋转位置编码这篇工作吗，想知道您对这篇工...](https://kexue.fm/archives/10907/comment-page-2#comment-27799)
- [jiurizz: 在2\*shared experts + 160\*routed ...](https://kexue.fm/archives/10945/comment-page-1#comment-27798)

## USERLOGIN

- [登录](https://kexue.fm/admin/login.php)

[科学空间\|Scientific Spaces](https://kexue.fm)

- [登录](https://kexue.fm/admin/login.php)
- [打赏](https://kexue.fm/reward.html)
- [公式](https://kexue.fm/latex.html)
- [天象](https://kexue.fm/ac.html)
- [链接](https://kexue.fm/links.html)
- [时光](https://kexue.fm/me.html)
- [博览](https://kexue.fm/science.html)
- [归档](https://kexue.fm/content.html)

渴望成为一个小飞侠

- [欢迎订阅](https://kexue.fm/feed)
- [个性邮箱](https://kexue.fm/archives/119)
- [天象信息](https://kexue.fm/ac.html)
- [观测ISS](https://kexue.fm/archives/41)
- [LaTeX](https://kexue.fm/latex.html)
- [关于博主](https://kexue.fm/me.html)

欢迎访问“科学空间”，这里将与您共同探讨自然科学，回味人生百态；也期待大家的分享～

- [**千奇百怪** Everything](https://kexue.fm/category/Everything)
- [**天文探索** Astronomy](https://kexue.fm/category/Astronomy)
- [**数学研究** Mathematics](https://kexue.fm/category/Mathematics)
- [**物理化学** Phy-chem](https://kexue.fm/category/Phy-chem)
- [**信息时代** Big-Data](https://kexue.fm/category/Big-Data)
- [**生物自然** Biology](https://kexue.fm/category/Biology)
- [**图片摄影** Photograph](https://kexue.fm/category/Photograph)
- [**问题百科** Questions](https://kexue.fm/category/Questions)
- [**生活/情感** Life-Feeling](https://kexue.fm/category/Life-Feeling)
- [**资源共享** Resources](https://kexue.fm/category/Resources)

- [**千奇百怪**](https://kexue.fm/category/Everything)
- [**天文探索**](https://kexue.fm/category/Astronomy)
- [**数学研究**](https://kexue.fm/category/Mathematics)
- [**物理化学**](https://kexue.fm/category/Phy-chem)
- [**信息时代**](https://kexue.fm/category/Big-Data)
- [**生物自然**](https://kexue.fm/category/Biology)
- [**图片摄影**](https://kexue.fm/category/Photograph)
- [**问题百科**](https://kexue.fm/category/Questions)
- [**生活/情感**](https://kexue.fm/category/Life-Feeling)
- [**资源共享**](https://kexue.fm/category/Resources)

[首页](https://kexue.fm) [信息时代](https://kexue.fm/category/Big-Data) 再谈类别不平衡问题：调节权重与魔改Loss的对比联系

31Aug

# [再谈类别不平衡问题：调节权重与魔改Loss的对比联系](https://kexue.fm/archives/7708)

By 苏剑林 \|
2020-08-31 \|
93766位读者\|

类别不平衡问题，也称为长尾分布问题，在本博客里已经有好几次相关讨论了，比如 [《从loss的硬截断、软化到focal loss》](https://kexue.fm/archives/4733)、 [《将“Softmax+交叉熵”推广到多标签分类问题》](https://kexue.fm/archives/7359)、 [《通过互信息思想来缓解类别不平衡问题》](https://kexue.fm/archives/7615)。对于缓解类别不平衡，比较基本的方法就是调节样本权重，看起来“高端”一点的方法则是各种魔改loss了（比如Focal Loss、Dice Loss、Logits Adjustment等），本文希望比较系统地理解一下它们之间的联系。

长尾分布：少数类别的样本数目非常多，多数类别的样本数目非常少。

## 从光滑准确率到交叉熵 [\#](https://kexue.fm/archives/7708\#%E4%BB%8E%E5%85%89%E6%BB%91%E5%87%86%E7%A1%AE%E7%8E%87%E5%88%B0%E4%BA%A4%E5%8F%89%E7%86%B5)

这里的分析主要以sigmoid的2分类为主，但多数结论可以平行推广到softmax的多分类。设$x$为输入，$y\\in\\{0,1\\}$为目标，$p\_{\\theta}(x) \\in \[0, 1\]$为模型。理想情况下，当然是要评测什么指标，我们就去优化那个指标。对于分类问题来说，最朴素的指标当然就是准确率，但准确率并没有办法提供有效的梯度，所以不能直接来训练。

为此，我们一个光滑化的指标。从之前的文章 [《函数光滑化杂谈：不可导函数的可导逼近》](https://kexue.fm/archives/6620)，准确率的光滑化近似是
\\begin{equation}\\text{ACC}\_{\\text{smooth}}=\\mathbb{E}\_{(x,y)\\sim\\mathcal{D}}\\big\[y p\_{\\theta}(x) + (1 - y)(1 - p\_{\\theta}(x))\\big\]\\end{equation}
其中$\\mathcal{D}$是训练数据集合。所以按道理，我们应该以$-\\text{ACC}\_{\\text{smooth}}$为最小化的目标。但事实上，直接优化这个目标的效果并不好，更好的是去优化交叉熵
\\begin{equation}\\text{cross\_entropy}=\\mathbb{E}\_{(x,y)\\sim\\mathcal{D}}\\big\[-y \\log p\_{\\theta}(x) - (1 - y)\\log(1 - p\_{\\theta}(x))\\big\]\\end{equation}
这就有点耐人寻味了，明明$\\text{ACC}\_{\\text{smooth}}$更接近我们的评测指标，为什么用交叉熵反而对评测指标更有利呢？

这需要用梯度来解释。对于$p\_{\\theta}(x)$，它通常是经过了sigmoid激活的，也就是$p\_{\\theta}(x)=\\sigma(z\_{\\theta}(x))$，其中$\\sigma(t)=\\frac{1}{1+e^{-t}}$，它的导数$\\sigma'(t)=\\sigma(t)(1 - \\sigma(t))$，而$z\_{\\theta}(x)$就是我们通常称的“logits”。

假设$y$是1，那么对应的$-\\text{ACC}\_{\\text{smooth}}$就是$-p\_{\\theta}(x)=-\\sigma(z\_{\\theta}(x))$，它的梯度是
\\begin{equation}-\\nabla\_{\\theta} p\_{\\theta}(x) = - p\_{\\theta}(x) (1 - p\_{\\theta}(x))\\nabla\_{\\theta}z\_{\\theta}(x)\\end{equation}
刚才说了，$y$是1，所以训练目标是$p\_{\\theta}(x)\\to 1$，因此我们期望当$p\_{\\theta}(x)$接近于0时（误差较大），会带来一个较大的梯度，当$p\_{\\theta}(x)$接近于1时（误差较小），会带来一个较小的梯度。但上述$-\\nabla\_{\\theta} p\_{\\theta}(x)$显然不是如此，它的调节项$p\_{\\theta}(x) (1 - p\_{\\theta}(x))$在0.5处取到最大值，至于0和1都是最小值，这就意味着如果误差太大了，梯度反而也小，这就带来优化效率的低下，最终导致整体效果不好。相反，对于交叉熵来说，有
\\begin{equation}-\\nabla\_{\\theta} \\log p\_{\\theta}(x) = - (1 - p\_{\\theta}(x))\\nabla\_{\\theta}z\_{\\theta}(x)\\end{equation}
刚好把梯度里边带来负面作用的$p\_{\\theta}(x)$因子去掉了，因此优化效率更高，最终效果也好些。上述分析针对的是$y=1$，如果$y=0$，那么结论也是一样的。

## 从光滑F1到加权交叉熵 [\#](https://kexue.fm/archives/7708\#%E4%BB%8E%E5%85%89%E6%BB%91F1%E5%88%B0%E5%8A%A0%E6%9D%83%E4%BA%A4%E5%8F%89%E7%86%B5)

从这个过程中，我们可以感觉到，对loss的各种魔改，本质上来说都只是在调整梯度，得到更合理的梯度，我们就能实现更有效的优化，得到更好的模型。此外，我们再思考上述转换过程，本来近似目标的梯度是$-\\nabla\_{\\theta}p\_{\\theta}(x)$，结果$-\\nabla\_{\\theta}\\log p\_{\\theta}(x)$效果更好。如果我们不去仔细分析背后的原因，直接把$p\\to \\log p$当作一个“公理”来使用，那能否成立呢？会不会带来一些有意思的结果呢？

举个例子，当负样本远远多于正样本时，我们的评测指标通常都不再是准确率了（不然直接全部输出0准确率就很高了），我们通常关心正类的F1，而F1的直接优化也是不容易的，所以我们也需要一个光滑版，文章 [《函数光滑化杂谈：不可导函数的可导逼近》](https://kexue.fm/archives/6620) 同样也给出了结果：
\\begin{equation}\\text{F1}\_{\\text{smooth}}=\\frac{2 \\mathbb{E}\_{(x,y)\\sim\\mathcal{D}}\\big\[y p\_{\\theta}(x)\\big\]}{\\mathbb{E}\_{(x,y)\\sim\\mathcal{D}}\\big\[y + p\_{\\theta}(x)\\big\]}\\end{equation}
所以我们的最小化目标原本是$-\\text{F1}\_{\\text{smooth}}$。根据上述“公理”，我们先直接对$-\\text{F1}\_{\\text{smooth}}$求梯度：
\\begin{equation}\\begin{aligned}&-\\nabla\_{\\theta}\\frac{2 \\mathbb{E}\_{(x,y)\\sim\\mathcal{D}}\\big\[y p\_{\\theta}(x)\\big\]}{\\mathbb{E}\_{(x,y)\\sim\\mathcal{D}}\\big\[y + p\_{\\theta}(x)\\big\]}\\\
=&-2\\frac{\\mathbb{E}\_{(x,y)\\sim\\mathcal{D}}\\big\[y \\nabla\_{\\theta}p\_{\\theta}(x)\\big\]}{\\mathbb{E}\_{(x,y)\\sim\\mathcal{D}}\\big\[y + p\_{\\theta}(x)\\big\]} + 2\\frac{\\mathbb{E}\_{(x,y)\\sim\\mathcal{D}}\\big\[y p\_{\\theta}(x)\\big\]\\mathbb{E}\_{(x,y)\\sim\\mathcal{D}}\\big\[\\nabla\_{\\theta}p\_{\\theta}(x)\\big\]}{\\left(\\mathbb{E}\_{(x,y)\\sim\\mathcal{D}}\\big\[y + p\_{\\theta}(x)\\big\]\\right)^2}\\\
=&-\\frac{2\\mathbb{E}\_{(x,y)\\sim\\mathcal{D}}\\big\[\\big(y-\\text{F1}\_{\\text{smooth}}/2\\big)\\nabla\_{\\theta}p\_{\\theta}(x)\\big\]}{\\mathbb{E}\_{(x,y)\\sim\\mathcal{D}}\\big\[y + p\_{\\theta}(x)\\big\]}
\\end{aligned}\\end{equation}
其中$\\frac{2}{\\mathbb{E}\_{(x,y)\\sim\\mathcal{D}}\[y + p\_{\\theta}(x)\]}$是整体的一个缩放因子，我们主要关心的还是每个样本的梯度，所以结果是
\\begin{equation}-\\mathbb{E}\_{(x,y)\\sim\\mathcal{D}}\\big\[\\big(y-\\text{F1}\_{\\text{smooth}}/2\\big)\\nabla\_{\\theta}p\_{\\theta}(x)\\big\]\\end{equation}
根据$p\\to \\log p$“公理”（负样本则是$-p\\to\\log(1-p)$），我们得到最后的梯度为
\\begin{equation}-\\mathbb{E}\_{(x,y)\\sim\\mathcal{D}}\\big\[y\\cdot\\big(1-\\text{F1}\_{\\text{smooth}}/2\\big)\\cdot\\nabla\_{\\theta}\\log p\_{\\theta}(x) + (1 - y)\\cdot\\text{F1}\_{\\text{smooth}}/2\\cdot\\nabla\_{\\theta}\\log (1-p\_{\\theta}(x))\\big\]\\end{equation}
这等价于优化目标
\\begin{equation}-\\mathbb{E}\_{(x,y)\\sim\\mathcal{D}}\\big\[y\\cdot\\big(1-\\text{F1}\_{\\text{smooth}}/2\\big)\\cdot\\log p\_{\\theta}(x) + (1 - y)\\cdot\\text{F1}\_{\\text{smooth}}/2\\cdot\\log (1-p\_{\\theta}(x))\\big\]\\end{equation}
的梯度（其中$\\text{F1}\_{\\text{smooth}}$不求梯度），所以这其实就是用$1-\\text{F1}\_{\\text{smooth}}/2$调节正样本的交叉熵，用$\\text{F1}\_{\\text{smooth}}/2$调节负样本的交叉熵。

## 从扩大边界到Logits调整 [\#](https://kexue.fm/archives/7708\#%E4%BB%8E%E6%89%A9%E5%A4%A7%E8%BE%B9%E7%95%8C%E5%88%B0Logits%E8%B0%83%E6%95%B4)

其实无论评测指标是什么，我们肯定都是希望每一个样本都尽可能预测对。问题在于，样本数目比较少的类别，因为学习得不够充分，所以泛化性能不会太好。

让我们从几何角度来思考这个问题。理想情况下，在编码空间里边，每一类样本都占据着自己的一个“地盘”，不同类的“地盘”是互不相交的。样本数目较少的类别泛化性能不大好，主要就体现为其类别所占据的“地盘”比较小，而且往往还会受到类别数目较多的样本的“打压”，因此“生存”几乎都成了问题，更不用说照顾到训练集没有出现过的新样本了。

怎么解决这个问题呢？其实也很形象，如果样本数目少的类别，里边的样本个个都是“大佬”，一个打十个的那种，那么就算样本少，也能在“地盘之争”中不落下风。让我们考虑一个$n$分类问题，某个样本的编码向量为$f\_{\\theta}(x)$，类别向量为$u\_y$，那么该样本与类别向量的相似度，一般用内积$\\langle f\_{\\theta}(x), u\_y\\rangle$来度量。假设每个样本能占据半径为$r\_y$的“地盘”，这样就是说，满足$\\Vert z - f\_{\\theta}(x)\\Vert \\leq r\_y$的任意$z$都算是该样本的编码向量，这也就意味着，满足这个条件的任意$z$，它跟$u\_y$的相似度都应该大于它跟其他类别的相似度。

现在我们考虑
\\begin{equation}\\langle z, u\_y\\rangle = \\langle f\_{\\theta}(x), u\_y\\rangle + \\langle z - f\_{\\theta}(x), u\_y\\rangle\\end{equation}
由于$\\Vert z - f\_{\\theta}(x)\\Vert \\leq r\_y$，所以显然有
\\begin{equation}\\langle f\_{\\theta}(x), u\_y\\rangle - r\_y\\Vert u\_y\\Vert\\leq\\langle z, u\_y\\rangle \\leq \\langle f\_{\\theta}(x), u\_y\\rangle + r\_y\\Vert u\_y\\Vert\\end{equation}
所以，为了达到“$z$跟$u\_y$的相似度都应该大于它跟其他类别的相似度”这个目的，只需要“$z$跟$u\_y$的最小相似度都应该大于它跟其他类别的最大相似度”，因此我们的优化目标变为
\\begin{equation}-\\log\\frac{e^{\\langle f\_{\\theta}(x), u\_y\\rangle - r\_y\\Vert u\_y\\Vert}}{e^{\\langle f\_{\\theta}(x), u\_y\\rangle - r\_y\\Vert u\_y\\Vert}+\\sum\\limits\_{i\\neq y} e^{\\langle f\_{\\theta}(x), u\_i\\rangle + r\_y\\Vert u\_i\\Vert}}\\end{equation}
可以看到，这其实就相当于am-softmax、circle loss等带有margin的softmax变种，具体形式其实不重要，只需要为类别小的类设置更大的margin就好（样本少的类别每个样本都更“能打”）。那怎么设计每个类的margin呢？之前的文章 [《通过互信息思想来缓解类别不平衡问题》](https://kexue.fm/archives/7615) 就提供了一个方案：$m\_y=-\\tau\\log p(y)$，这里的$p(y)$是先验分布，那么就有
\\begin{equation}-\\log\\frac{e^{\\langle f\_{\\theta}(x), u\_y\\rangle + \\tau \\log p(y)}}{\\sum\\limits\_{i} e^{\\langle f\_{\\theta}(x), u\_i\\rangle + \\tau \\log p(i)}}\\end{equation}
这样我们就联系到了logit adjustment loss了，或者说给logit adjustment loss提供了一种几何直观理解。本质上来说，logit adjustment也是在调节权重，只不过一般的调节权重是在损失函数的$\\log$之后调整，而logit adjustment则是在$\\log$之前调整。

## 感觉上可以小结一下了 [\#](https://kexue.fm/archives/7708\#%E6%84%9F%E8%A7%89%E4%B8%8A%E5%8F%AF%E4%BB%A5%E5%B0%8F%E7%BB%93%E4%B8%80%E4%B8%8B%E4%BA%86)

本文就类别不平衡现象及其对策做了一些思考，主要是希望通过一些相对直观的引导，来揭示一些魔改loss的思路，从中我们也可以发现，其实这些方案本质上都算是在调节样本权重或者类权重。本文的分析思路相对来说比较散漫，基本上是笔者的头脑风暴内容，如果错漏之处，请读者见谅并指出。

_**转载到请包括本文地址：** [https://kexue.fm/archives/7708](https://kexue.fm/archives/7708)_

_**更详细的转载事宜请参考：**_ [《科学空间FAQ》](https://kexue.fm/archives/6508#%E6%96%87%E7%AB%A0%E5%A6%82%E4%BD%95%E8%BD%AC%E8%BD%BD/%E5%BC%95%E7%94%A8)

**如果您还有什么疑惑或建议，欢迎在下方评论区继续讨论。**

**如果您觉得本文还不错，欢迎 [分享](https://kexue.fm/archives/7708#share)/ [打赏](https://kexue.fm/archives/7708#pay) 本文。打赏并非要从中获得收益，而是希望知道科学空间获得了多少读者的真心关注。当然，如果你无视它，也不会影响你的阅读。再次表示欢迎和感谢！**

打赏

微信打赏

支付宝打赏

因为网站后台对打赏并无记录，因此欢迎在打赏时候备注留言。你还可以 [**点击这里**](http://mail.qq.com/cgi-bin/qm_share?t=qm_mailme&email=tN7d1drY3drrx8H0xcWa19vZ) 或在下方评论区留言来告知你的建议或需求。

**如果您需要引用本文，请参考：**

苏剑林. (Aug. 31, 2020). 《再谈类别不平衡问题：调节权重与魔改Loss的对比联系 》\[Blog post\]. Retrieved from [https://kexue.fm/archives/7708](https://kexue.fm/archives/7708)

@online{kexuefm-7708,
        title={再谈类别不平衡问题：调节权重与魔改Loss的对比联系},
        author={苏剑林},
        year={2020},
        month={Aug},
        url={\\url{https://kexue.fm/archives/7708}},
}

分类： [信息时代](https://kexue.fm/category/Big-Data)    标签： [优化](https://kexue.fm/tag/%E4%BC%98%E5%8C%96/), [损失函数](https://kexue.fm/tag/%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0/), [光滑](https://kexue.fm/tag/%E5%85%89%E6%BB%91/)[22 评论](https://kexue.fm/archives/7708#comments)

< [最小熵原理（六）：词向量的维度应该怎么选择？](https://kexue.fm/archives/7695) \| [动手做个DialoGPT：基于LM的生成式多轮对话模型](https://kexue.fm/archives/7718) >

### 你也许还对下面的内容感兴趣

- [MoE环游记：5、均匀分布的反思](https://kexue.fm/archives/10945)
- [Transformer升级之路：20、MLA究竟好在哪里？](https://kexue.fm/archives/10907)
- [通过梯度近似寻找Normalization的替代品](https://kexue.fm/archives/10831)
- [MoE环游记：4、难处应当多投入](https://kexue.fm/archives/10815)
- [MoE环游记：3、换个思路来分配](https://kexue.fm/archives/10757)
- [MoE环游记：2、不患寡而患不均](https://kexue.fm/archives/10735)
- [为什么梯度裁剪的默认模长是1？](https://kexue.fm/archives/10657)
- [从谱范数梯度到新式权重衰减的思考](https://kexue.fm/archives/10648)
- [从Hessian近似看自适应学习率优化器](https://kexue.fm/archives/10588)
- [Softmax后传：寻找Top-K的光滑近似](https://kexue.fm/archives/10373)

[发表你的看法](https://kexue.fm/archives/7708#comment_form)

jamesp

September 1st, 2020

交叉熵就是$Hy\|x$。Ix,y=Hy - Hy\|x，因为Hy是常量，所以最小化交叉熵就是最大化互信息量Ix,y，所以比优化准确率更好吧。

[回复评论](https://kexue.fm/archives/7708/comment-page-1?replyTo=14220#respond-post-7708)

tiandiweizun

September 3rd, 2020

你好，我对本站很感兴趣，也阅读了不少文章，发现写的非常棒，但是对部分文章有些疑惑并写在评论区，但是我发现未回复（这不符合你的性格和行为），所以是不是系统问题，导致你这边看不到历史文章的评论呢？比如：《“让Keras更酷一些！”：分层的学习率和自由的梯度》，最后一个tiandi的评论

[回复评论](https://kexue.fm/archives/7708/comment-page-1?replyTo=14239#respond-post-7708)

[苏剑林](https://kexue.fm) 发表于
September 4th, 2020

原话是：
“SetLearningRate添加后，尽管效果会提升，但是发现正确的类别预测得分很低（低于0.1，但是如果直接用softmax的话，会很高，我在最后dense分类层也加了SetLearningRate），虽然也是最高分”

很明显这是一个陈述句不是疑问句，既然不是疑问，那我回不回复均可吧。

[回复评论](https://kexue.fm/archives/7708/comment-page-1?replyTo=14244#respond-post-7708)

tiandi 发表于
September 4th, 2020

可以的，我以为是看不见，那我下次使用疑问句，改变一下评论方式

[回复评论](https://kexue.fm/archives/7708/comment-page-1?replyTo=14245#respond-post-7708)

yudake

September 16th, 2020

这里p->log(p)的转化不是很明白。
1.是由于log(p)的导数更利于优化，从而导致我们使用log(p)；
2.还是我们使用log(p)，并且发现log(p)的导数利于优化模型？

从之前看的资料，应该是分类问题优化交叉熵与优化相对熵等价。还是说数学上有更好的解释？

[回复评论](https://kexue.fm/archives/7708/comment-page-1?replyTo=14334#respond-post-7708)

[苏剑林](https://kexue.fm) 发表于
September 17th, 2020

这两种应该也没多大区别吧。对于“为什么使用交叉熵”（也就是为什么用$\\log p$而不用$p$）这个问题来说，你说的两种解释其实都存在。交叉熵可以来源于最大似然，然后大家进一步发现了它的好处，这对应你说的第二点；也有人从设计梯度的角度出发，反过来推出了交叉熵，这对应你说的第一点。

[回复评论](https://kexue.fm/archives/7708/comment-page-1?replyTo=14335#respond-post-7708)

yudake 发表于
September 17th, 2020

我的疑惑点是后面推出f1的优化也沿用p->log(p)这种替代。这个时候是单纯从求导发现有好处呢，还是像交叉熵一样是有物理意义在呢？
如果只是求导发现有好处，那么就有可能存在更好的公式

[回复评论](https://kexue.fm/archives/7708/comment-page-1?replyTo=14336#respond-post-7708)

[苏剑林](https://kexue.fm) 发表于
September 17th, 2020

是可能存在更好的公式啊，这个只是建立在$p\\to \\log p$这个“假设”下所推导出来的结果。

[回复评论](https://kexue.fm/archives/7708/comment-page-1?replyTo=14341#respond-post-7708)

zw

November 10th, 2020

请问（10）是如何得到的吗，为什么用这个呢

[回复评论](https://kexue.fm/archives/7708/comment-page-1?replyTo=14746#respond-post-7708)

[苏剑林](https://kexue.fm) 发表于
November 10th, 2020

$(10)$的左右两端相等不是显然的吗？至于为什么写成$(10)$的形式，因为后面的推导要用呀。

[回复评论](https://kexue.fm/archives/7708/comment-page-1?replyTo=14750#respond-post-7708)

xxw

February 17th, 2021

"假设y是1，那么对应的−smooth\_accuracy就是−pθ(x)=σ(zθ(x))"
这里σ前面是不是缺了个负号?

[回复评论](https://kexue.fm/archives/7708/comment-page-1?replyTo=15563#respond-post-7708)

[苏剑林](https://kexue.fm) 发表于
February 19th, 2021

是的，已修正，谢谢指出。

[回复评论](https://kexue.fm/archives/7708/comment-page-1?replyTo=15569#respond-post-7708)

travis

April 7th, 2021

公式(6)(7)中, 是不是应该修改为(y- smooth\_f1/2)?

[回复评论](https://kexue.fm/archives/7708/comment-page-1?replyTo=16031#respond-post-7708)

[苏剑林](https://kexue.fm) 发表于
April 8th, 2021

为什么呢？

[回复评论](https://kexue.fm/archives/7708/comment-page-1?replyTo=16039#respond-post-7708)

travis 发表于
April 10th, 2021

(6)中, 我觉得第二个等号的内容 并不等于 第三个等号的内容.

[回复评论](https://kexue.fm/archives/7708/comment-page-1?replyTo=16066#respond-post-7708)

travis 发表于
April 10th, 2021

$\\frac{-2yp\\prime}{\\sum(y+p)} + \\frac{2ypp\\prime}{(\\sum y+p)^2} = -2\\frac{(y-0.5\*\\mathrm{smooth-f1})p\\prime}{\\sum(y+p)}$

[回复评论](https://kexue.fm/archives/7708/comment-page-1?replyTo=16067#respond-post-7708)

[苏剑林](https://kexue.fm) 发表于
April 12th, 2021

检验了一下，你的确是对的，当初推导的时候可能我也犯糊涂了，已经修正，感谢指出。

[回复评论](https://kexue.fm/archives/7708/comment-page-1?replyTo=16078#respond-post-7708)

sgdjh

March 19th, 2022

"本质上来说，logit adjustment也是在调节权重，只不过一般的调节权重是在损失函数的log之后调整，而logit adjustment则是在log之前调整"

为什么logit adjustment这种调节权重的方式要好于一般的在log之后调整的呢？

[回复评论](https://kexue.fm/archives/7708/comment-page-1?replyTo=18727#respond-post-7708)

[苏剑林](https://kexue.fm) 发表于
March 21st, 2022

这个或许可以解释为互信息更能揭示本质？

[回复评论](https://kexue.fm/archives/7708/comment-page-1?replyTo=18739#respond-post-7708)

steven

April 10th, 2022

苏神，有点想不明白，公式(7)到(8)中的负样本转换，为什么能从（y-smooth/2)转换成smooth/2？

[回复评论](https://kexue.fm/archives/7708/comment-page-1?replyTo=18911#respond-post-7708)

[苏剑林](https://kexue.fm) 发表于
April 11th, 2022

负样本$y=0$。

[回复评论](https://kexue.fm/archives/7708/comment-page-1?replyTo=18918#respond-post-7708)

[EMO：基于最优传输思想设计的分类损失函数 R11; AI 資訊](https://news.aitime.space/2023/10/60815/)

October 13th, 2023

\[...\]众所周知，分类任务的标准损失是交叉熵（Cross Entropy，等价于最大似然MLE，即Maximum Likelihood Estimation），它有着简单高效的特点，但在某些场景下也暴露出一些问题，如偏离评价指标、过度自信等，相应的改进工作也有很多，此前我们也介绍过一些，比如《再谈类别不平衡问题：调节权重与魔改Loss的对比联系》、《如何训练你的准确率？》、《缓解交叉熵过度自信的一个简明方\[...\]

[回复评论](https://kexue.fm/archives/7708/comment-page-1?replyTo=22884#respond-post-7708)

[取消回复](https://kexue.fm/archives/7708#respond-post-7708)

你的大名

电子邮箱

个人网站（选填）

1\. 可以使用LaTeX代码，点击“预览效果”可查看效果；2. 可以通过点击评论楼层编号来引用该楼层；3. 网站可能会有点卡，如非确认评论失败，请不要重复点击提交。

### 内容速览

[从光滑准确率到交叉熵](https://kexue.fm/archives/7708#%E4%BB%8E%E5%85%89%E6%BB%91%E5%87%86%E7%A1%AE%E7%8E%87%E5%88%B0%E4%BA%A4%E5%8F%89%E7%86%B5)
[从光滑F1到加权交叉熵](https://kexue.fm/archives/7708#%E4%BB%8E%E5%85%89%E6%BB%91F1%E5%88%B0%E5%8A%A0%E6%9D%83%E4%BA%A4%E5%8F%89%E7%86%B5)
[从扩大边界到Logits调整](https://kexue.fm/archives/7708#%E4%BB%8E%E6%89%A9%E5%A4%A7%E8%BE%B9%E7%95%8C%E5%88%B0Logits%E8%B0%83%E6%95%B4)
[感觉上可以小结一下了](https://kexue.fm/archives/7708#%E6%84%9F%E8%A7%89%E4%B8%8A%E5%8F%AF%E4%BB%A5%E5%B0%8F%E7%BB%93%E4%B8%80%E4%B8%8B%E4%BA%86)

### 智能搜索

支持整句搜索！网站自动使用 [结巴分词](https://github.com/fxsjy/jieba) 进行分词，并结合ngrams排序算法给出合理的搜索结果。

### 热门标签

[生成模型](https://kexue.fm/tag/%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B/) [attention](https://kexue.fm/tag/attention/) [优化](https://kexue.fm/tag/%E4%BC%98%E5%8C%96/) [语言模型](https://kexue.fm/tag/%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/) [模型](https://kexue.fm/tag/%E6%A8%A1%E5%9E%8B/) [网站](https://kexue.fm/tag/%E7%BD%91%E7%AB%99/) [概率](https://kexue.fm/tag/%E6%A6%82%E7%8E%87/) [梯度](https://kexue.fm/tag/%E6%A2%AF%E5%BA%A6/) [转载](https://kexue.fm/tag/%E8%BD%AC%E8%BD%BD/) [微分方程](https://kexue.fm/tag/%E5%BE%AE%E5%88%86%E6%96%B9%E7%A8%8B/) [天象](https://kexue.fm/tag/%E5%A4%A9%E8%B1%A1/) [矩阵](https://kexue.fm/tag/%E7%9F%A9%E9%98%B5/) [分析](https://kexue.fm/tag/%E5%88%86%E6%9E%90/) [深度学习](https://kexue.fm/tag/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/) [积分](https://kexue.fm/tag/%E7%A7%AF%E5%88%86/) [python](https://kexue.fm/tag/python/) [优化器](https://kexue.fm/tag/%E4%BC%98%E5%8C%96%E5%99%A8/) [力学](https://kexue.fm/tag/%E5%8A%9B%E5%AD%A6/) [无监督](https://kexue.fm/tag/%E6%97%A0%E7%9B%91%E7%9D%A3/) [扩散](https://kexue.fm/tag/%E6%89%A9%E6%95%A3/) [几何](https://kexue.fm/tag/%E5%87%A0%E4%BD%95/) [节日](https://kexue.fm/tag/%E8%8A%82%E6%97%A5/) [生活](https://kexue.fm/tag/%E7%94%9F%E6%B4%BB/) [文本生成](https://kexue.fm/tag/%E6%96%87%E6%9C%AC%E7%94%9F%E6%88%90/) [数论](https://kexue.fm/tag/%E6%95%B0%E8%AE%BA/)

### 随机文章

- [世界最复杂的机器11月重启,温度宇宙最低](https://kexue.fm/archives/207)
- [正十七边形的尺规作图存在之证明](https://kexue.fm/archives/133)
- [今日双“近”！月球、火星齐过近地点!](https://kexue.fm/archives/380)
- [寒假结束，今天上学了](https://kexue.fm/archives/470)
- [2009诺贝尔奖揭晓时间表](https://kexue.fm/archives/155)
- [天文望远镜拍到宇宙最美部分(图)](https://kexue.fm/archives/24)
- [微型博客在中国](https://kexue.fm/archives/189)
- [相对位置编码Transformer的一个理论缺陷与对策](https://kexue.fm/archives/9105)
- [爱恩斯坦的狭义相对论论文(中文/图片)](https://kexue.fm/archives/259)
- [生成扩散模型漫谈（五）：一般框架之SDE篇](https://kexue.fm/archives/9209)

### 最近评论

- [贵阳机场接机](https://kexue.fm/archives/1490/comment-page-1#comment-27807): 怎么不更新啦
- [czvzb](https://kexue.fm/archives/10958/comment-page-1#comment-27806): 具身智能模型目前主流也是在使用扩散和流匹配这类方法来预测动作。
苏神推荐你看这几篇文章：
1....
- [Shawn\_yang](https://kexue.fm/archives/10945/comment-page-1#comment-27805): 不好意思，以为网页卡了0.0点了三下
- [Shawn\_yang](https://kexue.fm/archives/10945/comment-page-1#comment-27804): 苏神，关于您所说的：“推理阶段可以事先预估Routed Expert的实际分布，只要细致地进行...
- [Shawn\_yang](https://kexue.fm/archives/10945/comment-page-1#comment-27803): 苏神，关于您所说的：“推理阶段可以事先预估Routed Expert的实际分布，只要细致地进行...
- [Shawn\_yang](https://kexue.fm/archives/10945/comment-page-1#comment-27802): 苏神，关于您所说的：“推理阶段可以事先预估Routed Expert的实际分布，只要细致地进行...
- [OceanYU](https://kexue.fm/archives/9164/comment-page-4#comment-27801): 您好，关于由式（7）推导出高斯分布，我这里有一点问题，式（7）只能保证关于x\_t-1是二次函数...
- [jorjiang](https://kexue.fm/archives/10907/comment-page-2#comment-27800): 训练和prefill这个compute-bound阶段不做矩阵吸收，这个用我这个解释更好理解了...
- [amy](https://kexue.fm/archives/10907/comment-page-2#comment-27799): 苏老师，您有关注傅里叶旋转位置编码这篇工作吗，想知道您对这篇工作的看法是什么，这篇工作可以wo...
- [jiurizz](https://kexue.fm/archives/10945/comment-page-1#comment-27798): 在2\*shared experts + 160\*routed expert + top6的配置...

### 友情链接

- [Cool Papers](https://papers.cool)
- [数学研发](https://bbs.emath.ac.cn)
- [Seatop](http://www.seatop.com.cn/)
- [Xiaoxia](https://xiaoxia.org/)
- [积分表-网络版](https://kexue.fm/sci/integral/index.html)
- [丝路博傲](http://blog.dvxj.com/)
- [ph4ntasy 饭特稀](http://www.ph4ntasy.com/)
- [数学之家](http://www.2math.cn/)
- [有趣天文奇观](http://interesting-sky.china-vo.org/)
- [TwistedW](http://www.twistedwg.com/)
- [godweiyang](https://godweiyang.com/)
- [AI柠檬](https://blog.ailemon.net/)
- [王登科-DK博客](https://greatdk.com)
- [ESON](https://blog.eson.org/)
- [枫之羽](https://fzhiy.net/)
- [Mathor's blog](https://wmathor.com/)
- [coding-zuo](https://coding-zuo.github.io/)
- [博科园](https://www.bokeyuan.net/)
- [孔皮皮的博客](https://www.kppkkp.top/)
- [运鹏的博客](https://yunpengtai.top/)
- [jiming.site](https://jiming.site/)
- [OmegaXYZ](https://www.omegaxyz.com/)
- [Blog by Eacls](https://www.eacls.top/)
- [EAI猩球](https://www.robotech.ink/)
- [文举的博客](https://liwenju0.com/)
- [用代码打点酱油](https://bruceyuan.com/)
- [申请链接](https://kexue.fm/links.html)

本站采用创作共用版权协议，要求署名、非商业用途和保持一致。转载本站内容必须也遵循“ [署名-非商业用途-保持一致](http://creativecommons.org/licenses/by-nc-nd/2.5/cn/)”的创作共用协议。
© 2009-2025 Scientific Spaces. All rights reserved. Theme by [laogui](http://www.laogui.com). Powered by [Typecho](http://typecho.org). 备案号: [粤ICP备09093259号-1/2](https://beian.miit.gov.cn/)。