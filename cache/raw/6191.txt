## SEARCH

## MENU

- [打赏](https://kexue.fm/reward.html)
- [公式](https://kexue.fm/latex.html)
- [天象](https://kexue.fm/ac.html)
- [链接](https://kexue.fm/links.html)
- [时光](https://kexue.fm/me.html)
- [博览](https://kexue.fm/science.html)
- [归档](https://kexue.fm/content.html)

## CATEGORIES

- [千奇百怪](https://kexue.fm/category/Everything)
- [天文探索](https://kexue.fm/category/Astronomy)
- [数学研究](https://kexue.fm/category/Mathematics)
- [物理化学](https://kexue.fm/category/Phy-chem)
- [信息时代](https://kexue.fm/category/Big-Data)
- [生物自然](https://kexue.fm/category/Biology)
- [图片摄影](https://kexue.fm/category/Photograph)
- [问题百科](https://kexue.fm/category/Questions)
- [生活/情感](https://kexue.fm/category/Life-Feeling)
- [资源共享](https://kexue.fm/category/Resources)

## NEWPOSTS

- [msign的导数](https://kexue.fm/archives/11025)
- [通过msign来计算mclip（奇...](https://kexue.fm/archives/11006)
- [msign算子的Newton-Sc...](https://kexue.fm/archives/10996)
- [等值振荡定理：最优多项式逼近的充要条件](https://kexue.fm/archives/10972)
- [生成扩散模型漫谈（三十）：从瞬时速...](https://kexue.fm/archives/10958)
- [MoE环游记：5、均匀分布的反思](https://kexue.fm/archives/10945)
- [msign算子的Newton-Sc...](https://kexue.fm/archives/10922)
- [Transformer升级之路：2...](https://kexue.fm/archives/10907)
- [一道概率不等式：盯着它到显然成立为止！](https://kexue.fm/archives/10902)
- [SVD的导数](https://kexue.fm/archives/10878)

## COMMENTS

- [tyj: 感觉和之前的一篇文章很像，应该算是concurrent wor...](https://kexue.fm/archives/10958/comment-page-2#comment-27894)
- [li6626: 苏老师，Normalizing Flow有了新进展，论文链接:...](https://kexue.fm/archives/10667/comment-page-1#comment-27893)
- [tesslqy: Evans那本书感觉就挺好的，不过长了一点且有点难（姜萍拿来装...](https://kexue.fm/archives/4718/comment-page-1#comment-27891)
- [Bauree: 渴望大图，万分感谢！](https://kexue.fm/archives/443/comment-page-1#comment-27890)
- [tesslqy: 感觉应该是狄拉克测度，不过我也没系统学过Lebesgue以外的...](https://kexue.fm/archives/4187/comment-page-1#comment-27888)
- [tesslqy: 假设只有作者懂，但我不是作者，我懂了，假设不成立](https://kexue.fm/archives/4187/comment-page-1#comment-27887)
- [苏剑林: 我们的目标是1，$f^∗\_t(0)=0$，如果$x=0$处还单...](https://kexue.fm/archives/10996/comment-page-1#comment-27886)
- [苏剑林: 最后的观点我是同意的。MeanFlow作为后来的工作，不管是形...](https://kexue.fm/archives/10958/comment-page-1#comment-27885)
- [Kai Liu: 我可能错过了最重要的点，为啥在x=0处是单调递增的？](https://kexue.fm/archives/10996/comment-page-1#comment-27884)
- [苏剑林: 如果$l\_t$处是最大值，那么$l\_t$右侧就是下降对吧？但$...](https://kexue.fm/archives/10996/comment-page-1#comment-27883)

## USERLOGIN

- [登录](https://kexue.fm/admin/login.php)

[科学空间\|Scientific Spaces](https://kexue.fm)

- [登录](https://kexue.fm/admin/login.php)
- [打赏](https://kexue.fm/reward.html)
- [公式](https://kexue.fm/latex.html)
- [天象](https://kexue.fm/ac.html)
- [链接](https://kexue.fm/links.html)
- [时光](https://kexue.fm/me.html)
- [博览](https://kexue.fm/science.html)
- [归档](https://kexue.fm/content.html)

渴望成为一个小飞侠

- [欢迎订阅](https://kexue.fm/feed)
- [个性邮箱](https://kexue.fm/archives/119)
- [天象信息](https://kexue.fm/ac.html)
- [观测ISS](https://kexue.fm/archives/41)
- [LaTeX](https://kexue.fm/latex.html)
- [关于博主](https://kexue.fm/me.html)

欢迎访问“科学空间”，这里将与您共同探讨自然科学，回味人生百态；也期待大家的分享～

- [**千奇百怪** Everything](https://kexue.fm/category/Everything)
- [**天文探索** Astronomy](https://kexue.fm/category/Astronomy)
- [**数学研究** Mathematics](https://kexue.fm/category/Mathematics)
- [**物理化学** Phy-chem](https://kexue.fm/category/Phy-chem)
- [**信息时代** Big-Data](https://kexue.fm/category/Big-Data)
- [**生物自然** Biology](https://kexue.fm/category/Biology)
- [**图片摄影** Photograph](https://kexue.fm/category/Photograph)
- [**问题百科** Questions](https://kexue.fm/category/Questions)
- [**生活/情感** Life-Feeling](https://kexue.fm/category/Life-Feeling)
- [**资源共享** Resources](https://kexue.fm/category/Resources)

- [**千奇百怪**](https://kexue.fm/category/Everything)
- [**天文探索**](https://kexue.fm/category/Astronomy)
- [**数学研究**](https://kexue.fm/category/Mathematics)
- [**物理化学**](https://kexue.fm/category/Phy-chem)
- [**信息时代**](https://kexue.fm/category/Big-Data)
- [**生物自然**](https://kexue.fm/category/Biology)
- [**图片摄影**](https://kexue.fm/category/Photograph)
- [**问题百科**](https://kexue.fm/category/Questions)
- [**生活/情感**](https://kexue.fm/category/Life-Feeling)
- [**资源共享**](https://kexue.fm/category/Resources)

[首页](https://kexue.fm) [信息时代](https://kexue.fm/category/Big-Data) 最小熵原理（四）：“物以类聚”之从图书馆到词向量

2Dec

# [最小熵原理（四）：“物以类聚”之从图书馆到词向量](https://kexue.fm/archives/6191)

By 苏剑林 \|
2018-12-02 \|
108971位读者\|

从第一篇看下来到这里，我们知道所谓“最小熵原理”就是致力于降低学习成本，试图用最小的成本完成同样的事情。所以整个系列就是一个“偷懒攻略”。那偷懒的秘诀是什么呢？答案是“套路”，所以本系列又称为“套路宝典”。

本篇我们介绍图书馆里边的套路。

先抛出一个问题： **词向量出现在什么时候？** 是2013年Mikolov的Word2Vec？还是是2003年Bengio大神的神经语言模型？都不是，其实词向量可以追溯到千年以前，在那古老的图书馆中...

图书馆一角（图片来源于百度搜索）

## 走进图书馆 [\#](https://kexue.fm/archives/6191\#%E8%B5%B0%E8%BF%9B%E5%9B%BE%E4%B9%A6%E9%A6%86)

图书馆里有词向量？还是千年以前？在哪本书？我去借来看看。

### 放书的套路 [\#](https://kexue.fm/archives/6191\#%E6%94%BE%E4%B9%A6%E7%9A%84%E5%A5%97%E8%B7%AF)

其实不是哪本书，而是放书的套路。

很明显，图书馆中书的摆放是有“套路”的：它们不是随机摆放的，而是分门别类地放置的，比如数学类放一个区，文学类放一个区，计算机类也放一个区；同一个类也有很多子类，比如数学类中，数学分析放一个子区，代数放一个子区，几何放一个子区，等等。读者是否思考过，为什么要这么分类放置？分类放置有什么好处？跟最小熵又有什么关系？

有的读者可能觉得很简单：不就是为了便于查找吗？这个答案其实不大准确。如果只是为了方便找书，那很简单，只要在数据库上记录好每一本书的坐标，然后在地面上也注明当前坐标，这样需要借哪本书，在数据库一查坐标，然后就可以去找到那本书了，整个过程不需要用到“图书分类”这一点。所以，如果单纯考虑找书的难易程度，是无法很好的解释这个现象。

### 省力地借书 [\#](https://kexue.fm/archives/6191\#%E7%9C%81%E5%8A%9B%E5%9C%B0%E5%80%9F%E4%B9%A6)

其实原因的核心在于： **我们通常不只是借一本书。**

前面说了，只要建好索引，在图书馆里找一本书是不难的，问题是：如果找两本呢？一般情况下，每个人的兴趣和研究是比较集中的，因此，如果我要到图书馆借两本书，那么可以合理地假设你要借的这两本书是相近的，比如借了一本《神经网络》，那么再借一本《深度学习》的概率是挺大的，但再借一本《红楼梦》的概率就很小了。借助于数据库，我可以很快找到《神经网络》，那么《深度学习》呢？如果这本书在附近，那么我只需要再走几步就可以找到它了，如果图书是随机打乱放置的，我可能要从东南角走到西北角，才找到我想要的另一本书《深度学习》，再多借几本，我不是要在图书馆里跑几圈我才能借齐我要的书？

这样一来，图书分类的作用就很明显了。图书分类就是把相近的书放在一起，而每个人同一次要借的书也会相近的，所以图书分类会让大多数人的找书、借书过程更加省力！这又是一个“偷懒攻略”。也就是说，将我们要处理的东西分类放好，相近的放在一起，这也是满足最小熵原理的。生活中我们会将常用的东西分类放在触手可及的地方，也是基于同样的原理。

## 图书馆规划 [\#](https://kexue.fm/archives/6191\#%E5%9B%BE%E4%B9%A6%E9%A6%86%E8%A7%84%E5%88%92)

下面我们再来从数学角度，更仔细地考察这个过程。

### 简化的借书模型 [\#](https://kexue.fm/archives/6191\#%E7%AE%80%E5%8C%96%E7%9A%84%E5%80%9F%E4%B9%A6%E6%A8%A1%E5%9E%8B)

假如我们到图书馆去借两本书，分别记为$i,j$，假设借第一本书的成本是$d(i)$，两本书之间的成本函数为$d(i,j)$，这也就是说，找到第一本书$i$后，我就要再花$d(i,j)$那么多力气才能找到第二本书$j$。我们可以考虑这个过程对所有人的平均，即
\\begin{equation}S = \\sum\_{i,j} p(i)p(j\|i) \[d(i)+d(i,j)\] = \\sum\_{i,j} p(i,j) \[d(i)+d(i,j)\]\\end{equation}
其中$p(i)$是$i$这本书被借的概率，$p(j\|i)$就是借了$i$之后还会再借$j$的概率。图书馆的要把书放好，那么就要使得$S$最小化。

现在我们以图书馆入口为原点，在图书馆建立一个三维坐标系，那么每本书的位置都可以用一个向量$\\boldsymbol{v}$来表示，不失一般性，我们可以简单考虑$d(i)$为这本书到图书馆原点的欧氏距离，$d(i,j)$为两本书的欧氏距离，那么$S$的表达式变为：
\\begin{equation}S = \\sum\_{i,j} p(i)p(j\|i) \\left\[\\Vert \\boldsymbol{v}\_i\\Vert ＋ \\Vert \\boldsymbol{v}\_i - \\boldsymbol{v}\_j\\Vert\\right\] = \\sum\_{i,j} p(i,j) \\left\[\\Vert \\boldsymbol{v}\_i\\Vert ＋ \\Vert \\boldsymbol{v}\_i - \\boldsymbol{v}\_j\\Vert\\right\] \\label{eq:chengben}\\end{equation}

让我们再来解释一下各项的含义， **其中$(i,j)$代表着一种借书习惯，即借了书$i$还借书$j$，$p(i,j)$代表着这种借书习惯出现的概率，实际生活中可以通过图书馆的借书记录去估算它；$\\Vert \\boldsymbol{v}\_i\\Vert ＋ \\Vert \\boldsymbol{v}\_i - \\boldsymbol{v}\_j\\Vert$则代表着先借$i$再借$j$的总成本。其中$\\Vert \\boldsymbol{v}\_i\\Vert$这一项要尽量小，意味着我们要将热门的书放在靠近出口（原点）的地方；而$\\Vert \\boldsymbol{v}\_i - \\boldsymbol{v}\_j\\Vert$要尽量小，则告诉我们要把相近的书放在一起。**

### 约束优化规划 [\#](https://kexue.fm/archives/6191\#%E7%BA%A6%E6%9D%9F%E4%BC%98%E5%8C%96%E8%A7%84%E5%88%92)

假如我们拿到了图书馆的借书记录，也就是说已知$p(i,j)$了，那么是不是可以通过最小化$\\eqref{eq:chengben}$来得到图书馆的“最佳排书方案”了呢？思想对了，但还不完整，因为很显然式$\\eqref{eq:chengben}$的最小值是0，只需要让所有的$\\boldsymbol{v}$都等于0，也就是说，所有的书都挤在出口的位置。

显然这是不可能的，因为实际上书不是无穷小的，两本书之间有一个最小间距$d\_{\\min} > 0$，所以完整的提法应该是：
\\begin{equation}\\begin{aligned}S =& \\min\_{\\boldsymbol{v}}\\sum\_{i,j} p(i)p(j\|i) \\left\[\\Vert \\boldsymbol{v}\_i\\Vert ＋ \\Vert \\boldsymbol{v}\_i - \\boldsymbol{v}\_j\\Vert\\right\] = \\sum\_{i,j} p(i,j) \\left\[\\Vert \\boldsymbol{v}\_i\\Vert ＋ \\Vert \\boldsymbol{v}\_i - \\boldsymbol{v}\_j\\Vert\\right\] \\\
&\\text{s.t.}\\quad\\forall i\\neq j,\\, \\Vert \\boldsymbol{v}\_i - \\boldsymbol{v}\_j\\Vert \\geq d\_{\\min}
\\end{aligned}\\label{eq:chengben-2}\\end{equation}
也就是说，这是一个带约束的极值问题，解决了这个问题，我们就可以得到图书馆对图书的最合理安排了（理论上）。当然，如果真的去给图书馆做规划，我们还要根据图书馆的实际情况引入更多的约束，比如图书馆的形状、过道的设置等，但$\\eqref{eq:chengben-2}$已经不妨碍我们理解其中的根本思想了。

## 一般成本最小化 [\#](https://kexue.fm/archives/6191\#%E4%B8%80%E8%88%AC%E6%88%90%E6%9C%AC%E6%9C%80%E5%B0%8F%E5%8C%96)

现在我们再将问题一般化，从更抽象的视角来观察问题，能得到更深刻的认识。

### 均匀化与去约束 [\#](https://kexue.fm/archives/6191\#%E5%9D%87%E5%8C%80%E5%8C%96%E4%B8%8E%E5%8E%BB%E7%BA%A6%E6%9D%9F)

我们先将成本函数$\\Vert \\boldsymbol{v}\_i\\Vert ＋ \\Vert \\boldsymbol{v}\_i - \\boldsymbol{v}\_j\\Vert$代换为一般的$f(\\boldsymbol{v}\_i,\\boldsymbol{v}\_j)$，即考虑
\\begin{equation}S = \\sum\_{i,j} p(i)p(j\|i) f(\\boldsymbol{v}\_i,\\boldsymbol{v}\_j) = \\sum\_{i,j} p(i,j) f(\\boldsymbol{v}\_i,\\boldsymbol{v}\_j)\\label{eq:yibanchengben}\\end{equation}
同时$\\boldsymbol{v}$可以不再局限为3维向量，可以是一般的$n$维向量。我们依旧是希望成本最低，但是我们不喜欢诸如$\\Vert \\boldsymbol{v}\_i - \\boldsymbol{v}\_j\\Vert \\geq d\_{\\min}$的约束条件，因为带约束的优化问题往往不容易求解，所以如果能把这个约束直接体现在$f$的选择中，那么就是一个漂亮的“去约束”方案了。

怎么实现这个目的呢？回到图书馆的问题上，如果没有约束的话，理论最优解就是把所有图书都挤在出口的位置，为了防止这个不合理的解的出现，我们加了个约束“两本书之间有一个最小间距$d\_{\\min} > 0$”，防止了解的坍缩。其实有很多其他约束可以考虑，比如可以要求所有图书必须尽量均匀地放满图书馆，在这个希望之下，也能够得到合理的解。

“尽量均匀”其实可以理解为某种归一化约束，因为归一，所以不能全部集中在一点，因为只有一点就不归一了。“归一”启发我们可以往概率的方向想，也就是说，先构造概率分布，然后作为成本函数的度量。在这里就不做太多牵强的引导了，直接给出其中一个选择：
\\begin{equation}f(\\boldsymbol{v}\_i,\\boldsymbol{v}\_j)=-\\log\\frac{e^{-\\left\\Vert\\boldsymbol{v}\_i-\\boldsymbol{v}\_j\\right\\Vert^2}}{Z\_i},\\quad Z\_i = \\sum\_j e^{-\\left\\Vert\\boldsymbol{v}\_i-\\boldsymbol{v}\_j\\right\\Vert^2}\\label{eq:chengben-l2}\\end{equation}

### 最小熵=最大似然 [\#](https://kexue.fm/archives/6191\#%E6%9C%80%E5%B0%8F%E7%86%B5=%E6%9C%80%E5%A4%A7%E4%BC%BC%E7%84%B6)

让我们来分步理解一下这个式子。首先如果不看分母$Z\_i$，那么结果就是
\\begin{equation}-\\log \\left(e^{-\\left\\Vert\\boldsymbol{v}\_i-\\boldsymbol{v}\_j\\right\\Vert^2}\\right) =\\left\\Vert\\boldsymbol{v}\_i-\\boldsymbol{v}\_j\\right\\Vert^2\\end{equation}
也就是说，这个$f$相当于成本函数为$\\left\\Vert\\boldsymbol{v}\_i-\\boldsymbol{v}\_j\\right\\Vert^2$。然后，由于分母的存在，我们知道
\\begin{equation}\\sum\_j\\frac{e^{-\\left\\Vert\\boldsymbol{v}\_i-\\boldsymbol{v}\_j\\right\\Vert^2}}{Z\_i}=1\\end{equation}
所以$e^{-\\left\\Vert\\boldsymbol{v}\_i-\\boldsymbol{v}\_j\\right\\Vert^2}/Z\_i$实际上定义了一个待定的条件概率分布$q(j\|i)$，说白了，这实际上就是对$-\\left\\Vert\\boldsymbol{v}\_i-\\boldsymbol{v}\_j\\right\\Vert^2$的一个softmax操作，而此时$\\eqref{eq:yibanchengben}$实际上就是：
\\begin{equation}S = -\\sum\_{i,j} p(i)p(j\|i) \\log q(j\|i)\\label{eq:gailvchengben}\\end{equation}
对于固定的$i$而言，最小化上式这不就是相当于最大对数似然了吗？所以结果就是$q(j\|i)$会尽量接近$p(j\|i)$，从而全部取0不一定就是最优解的，因为全部取0对应着均匀分布，而真实的$p(j\|i)$却不一定是均匀分布。

**现在再来想想，我们从最小成本的思想出发，设计了一个具有概率的负对数形式的$f(\\boldsymbol{v}\_i,\\boldsymbol{v}\_j)$，然后发现最后的结果是最大似然。这个结果可以说是意料之外、情理之中，因为$-\\log q(j\|i)$的含义就是熵，我们说要最大似然，就是要最小化式$\\eqref{eq:gailvchengben}$，其含义就是最小熵了。最大似然跟最小熵其实具有相同的含义。**

## Word2Vec [\#](https://kexue.fm/archives/6191\#Word2Vec)

只要稍微将对象一转变，Word2Vec就出来了，甚至everything2vec～

### 多样的度量 [\#](https://kexue.fm/archives/6191\#%E5%A4%9A%E6%A0%B7%E7%9A%84%E5%BA%A6%E9%87%8F)

纯粹形式地看，式$\\eqref{eq:chengben-l2}$的选择虽然很直观，但并不是唯一的，可取的选择还有
\\begin{equation}f(\\boldsymbol{v}\_i,\\boldsymbol{v}\_j)=-\\log\\frac{e^{\\left\\langle\\boldsymbol{v}\_i,\\boldsymbol{v}\_j\\right\\rangle}}{Z\_i},\\quad Z\_i = \\sum\_j e^{\\left\\langle\\boldsymbol{v}\_i,\\boldsymbol{v}\_j\\right\\rangle}\\label{eq:chengben-dot}\\end{equation}
这以内积为距离度量，希望相近的对象内积越小越好。

### Skip Gram [\#](https://kexue.fm/archives/6191\#Skip%20Gram)

事实上，如果$i,j$分别代表句子窗口里边的一个词，那么式$\\eqref{eq:chengben-dot}$就对应了著名的词向量模型——Word2Vec的Skip Gram模型了，也就是说，最小化
\\begin{equation}S = -\\sum\_{i,j} p(i)p(j\|i)\\log\\frac{e^{\\left\\langle\\boldsymbol{v}\_i,\\boldsymbol{v}\_j\\right\\rangle}}{Z\_i} = -\\sum\_{i,j} p(i,j) \\log\\frac{e^{\\left\\langle\\boldsymbol{v}\_i,\\boldsymbol{v}\_j\\right\\rangle}}{Z\_i}\\label{eq:word2vec}\\end{equation}
这正好是Word2Vec的Skip Gram模型的优化目标。

> 注：Word2Vec实际上对上下文向量和中心词向量做了区分，也就是用了两套词向量，但这里为了直观理解其中的思想，我们就不区别这一点。

### 原理类比分析 [\#](https://kexue.fm/archives/6191\#%E5%8E%9F%E7%90%86%E7%B1%BB%E6%AF%94%E5%88%86%E6%9E%90)

等等，怎么突然就出来词向量了？？

我们再重新捋一下思路：是这样的，我们把每个词当作一本书，每个句子都可以看成每个人的“借书记录”，这样我们就能知道哪两本“书”经常被一起借了是吧？按照我们前面讨论了一大通的图书馆最佳放书方案，我们就可以把“书”的最佳位置找出来，理论上用$\\eqref{eq:chengben-2},\\eqref{eq:chengben-l2}$或$\\eqref{eq:chengben-dot}$都可以，这就是词向量了～如果用式$\\eqref{eq:chengben-dot}$，就是Word2Vec了。

反过来，找出一个最佳放书方案也就简单了，把图书馆的每个人的借书记录都当成一个句子，每本书当成一个词，设置词向量维度为3，送入Word2Vec训练一下，出来的词向量，就是最佳放书方案了。

那些doc2vec、node2vec、everything2vec，基本上都是这样做的。

所以，开始的问题就很清晰了：将图书馆的每本书的三维坐标记录下来，这不就是一个实实在在的“book embedding”？相近的书的向量也相近呀，跟词向量的特性完美对应～所以，自从有了图书馆，就有了embedding，尽管那时候还没有坐标系，当然也没有计算机。

## 再来看看t-SNE [\#](https://kexue.fm/archives/6191\#%E5%86%8D%E6%9D%A5%E7%9C%8B%E7%9C%8Bt-SNE)

有了“借书记录”，也就是$p(j\|i),p(i)$，我们就可以照搬上述过程，得到一个“最佳位置规划”，这就是向量化的过程。

如果没有呢？

### SNE [\#](https://kexue.fm/archives/6191\#SNE)

那就造一个出来呀！比如我们已经有了一堆高维样本$\\boldsymbol{x}\_1,\\boldsymbol{x}\_2,\\dots,\\boldsymbol{x}\_N$，它们可以是一堆图像数据集，我们想要得到一个低维表示$\\boldsymbol{z}\_1,\\boldsymbol{z}\_2,\\dots,\\boldsymbol{z}\_N$。我们构造一个
\\begin{equation}p(\\boldsymbol{x}\_j\|\\boldsymbol{x}\_i)=\\frac{e^{-\\Vert \\boldsymbol{x}\_i-\\boldsymbol{x}\_j\\Vert^2/2\\sigma^2}}{\\sum\\limits\_{j}^{j\\neq i}e^{-\\Vert \\boldsymbol{x}\_i-\\boldsymbol{x}\_j\\Vert^2/2\\sigma^2}}\\label{eq:pij}\\end{equation}
然后还是用式$\\eqref{eq:chengben-l2}$作为成本函数（假设$p(i)$是常数，即均匀分布，同时求和不对自身进行），去优化$\\eqref{eq:yibanchengben}$，即
\\begin{equation}S=-\\sum\_{i,j}^{i\\neq j}p(\\boldsymbol{x}\_j\|\\boldsymbol{x}\_i)\\log q(j\|i),\\quad q(j\|i)=\\frac{e^{-\\left\\Vert\\boldsymbol{z}\_i-\\boldsymbol{z}\_j\\right\\Vert^2}}{\\sum\\limits\_{j}^{j\\neq i}e^{-\\left\\Vert\\boldsymbol{z}\_i-\\boldsymbol{z}\_j\\right\\Vert^2}}\\end{equation}
这便是称为SNE的降维方法了。

一般来说它还有一些变种，我们就不细抠了，这也不是本文的重点，我们只需要理解基本思想。SNE本质上就是尽量保持相对距离的一种降维方案。因为它保持的是相对距离，保持了基本的形状不变，所以降维效果比PCA等方法要好。原因是PCA等方法仅仅保留主成分，只适用于比较规则的数据（比如具有中心聚拢特性、各向同性的），SNE的思想可以适用于任意连通形状。

### t-SNE [\#](https://kexue.fm/archives/6191\#t-SNE)

前面说得SNE已经体现出降维思想了。但是它会有一些问题，主要的就是“Crowding问题”。这个“Crowding问题”，简单来看，就是因为低维分布$\\eqref{eq:chengben-l2}$也是距离的负指数形式，负指数的问题就是在远处迅速衰减到0，而$\\eqref{eq:chengben-l2}$中的$\\boldsymbol{v}$是我们要求解的目标，这样一来优化结果是所有的点几乎都拥挤（Crowding）在某处附近（因为指数衰减，距离较远的点几乎不会出现），效果就不够好了。

为了解决这个问题，我们可以把式$\\eqref{eq:chengben-l2}$换成衰减没那么快的函数，比如说简单的分式：
\\begin{equation}f(\\boldsymbol{z}\_i,\\boldsymbol{z}\_j)=-\\log\\frac{(1+\\left\\Vert\\boldsymbol{z}\_i-\\boldsymbol{z}\_j\\right\\Vert^2)^{-1}}{Z\_i},\\quad Z\_i = \\sum\_{j}^{j\\neq i} (1+\\left\\Vert\\boldsymbol{z}\_i-\\boldsymbol{z}\_j\\right\\Vert^2)^{-1}\\label{eq:t}\\end{equation}
这称为t分布。式$\\eqref{eq:t}$、式$\\eqref{eq:pij}$和式$\\eqref{eq:yibanchengben}$结合，就是称为t-SNE的降维方法，相比SNE，它改善了Crowding问题。

当然，t-SNE与SNE的差别，其实已经不是本文的重点了，本文的重点是揭示SNE这类降维算法与Word2Vec的异曲同工之处。

虽然在深度学习中，我们直接用t-SNE这类降维手段的场景并不多，哪怕降维、聚类都有很多更漂亮的方案了，比如降维可以看这篇 [《深度学习的互信息：无监督提取特征》](https://kexue.fm/archives/6024)、聚类可以看这个 [《变分自编码器（四）：一步到位的聚类方案》](https://kexue.fm/archives/5887)。但是t-SNE的本质思想在很多场景都有体现，所以挖掘并体味其中的原理，并与其它知识点联系起来，融汇成自己的知识体系，是一件值得去做的事情。

## 本文总结 [\#](https://kexue.fm/archives/6191\#%E6%9C%AC%E6%96%87%E6%80%BB%E7%BB%93)

本文基于最小成本的思想，构建了一个比较理想化的模型来分析图书馆的图书安排原理，进而联系到了最小熵原理，并且思考了它跟Word2Vec、t-SNE之间的联系。就这样，又构成了最小熵原理的一个个鲜活例子：物以类聚、分门别类，都能降低成本。比如我们现在可以理解为什么预训练词向量能够加快NLP任务的收敛、有时还能提升最终效果了，因为词向量事先将词摆在了适合的位置，它的构造原理本身就是为了降低成本呀。

同时，将很多看似没有关联的东西联系在一起，能够相互促进各自的理解，达到尽可能融会贯通的效果，其妙不言而喻～

_**转载到请包括本文地址：** [https://kexue.fm/archives/6191](https://kexue.fm/archives/6191)_

_**更详细的转载事宜请参考：**_ [《科学空间FAQ》](https://kexue.fm/archives/6508#%E6%96%87%E7%AB%A0%E5%A6%82%E4%BD%95%E8%BD%AC%E8%BD%BD/%E5%BC%95%E7%94%A8)

**如果您还有什么疑惑或建议，欢迎在下方评论区继续讨论。**

**如果您觉得本文还不错，欢迎 [分享](https://kexue.fm/archives/6191#share)/ [打赏](https://kexue.fm/archives/6191#pay) 本文。打赏并非要从中获得收益，而是希望知道科学空间获得了多少读者的真心关注。当然，如果你无视它，也不会影响你的阅读。再次表示欢迎和感谢！**

打赏

微信打赏

支付宝打赏

因为网站后台对打赏并无记录，因此欢迎在打赏时候备注留言。你还可以 [**点击这里**](http://mail.qq.com/cgi-bin/qm_share?t=qm_mailme&email=tN7d1drY3drrx8H0xcWa19vZ) 或在下方评论区留言来告知你的建议或需求。

**如果您需要引用本文，请参考：**

苏剑林. (Dec. 02, 2018). 《最小熵原理（四）：“物以类聚”之从图书馆到词向量 》\[Blog post\]. Retrieved from [https://kexue.fm/archives/6191](https://kexue.fm/archives/6191)

@online{kexuefm-6191,
        title={最小熵原理（四）：“物以类聚”之从图书馆到词向量},
        author={苏剑林},
        year={2018},
        month={Dec},
        url={\\url{https://kexue.fm/archives/6191}},
}

分类： [信息时代](https://kexue.fm/category/Big-Data)    标签： [熵](https://kexue.fm/tag/%E7%86%B5/), [聚类](https://kexue.fm/tag/%E8%81%9A%E7%B1%BB/), [词向量](https://kexue.fm/tag/%E8%AF%8D%E5%90%91%E9%87%8F/), [无监督](https://kexue.fm/tag/%E6%97%A0%E7%9B%91%E7%9D%A3/), [最小熵](https://kexue.fm/tag/%E6%9C%80%E5%B0%8F%E7%86%B5/)[20 评论](https://kexue.fm/archives/6191#comments)

< [从变分编码、信息瓶颈到正态分布：论遗忘的重要性](https://kexue.fm/archives/6181) \| [BiGAN-QP：简单清晰的编码&生成模型](https://kexue.fm/archives/6214) >

### 你也许还对下面的内容感兴趣

- [矩阵的有效秩（Effective Rank）](https://kexue.fm/archives/10847)
- [生成扩散模型漫谈（二十三）：信噪比与大图生成（下）](https://kexue.fm/archives/10055)
- [注意力机制真的可以“集中注意力”吗？](https://kexue.fm/archives/9889)
- [BytePiece：更纯粹、更高压缩率的Tokenizer](https://kexue.fm/archives/9752)
- [如何度量数据的稀疏程度？](https://kexue.fm/archives/9595)
- [注意力和Softmax的两点有趣发现：鲁棒性和信息量](https://kexue.fm/archives/9593)
- [从JL引理看熵不变性Attention](https://kexue.fm/archives/9588)
- [用热传导方程来指导自监督学习](https://kexue.fm/archives/9359)
- [熵不变性Softmax的一个快速推导](https://kexue.fm/archives/9034)
- [听说Attention与Softmax更配哦～](https://kexue.fm/archives/9019)

[发表你的看法](https://kexue.fm/archives/6191#comment_form)

膜拜苏神

December 3rd, 2018

苏神，感觉有些牵强了，还以为你要追根溯源呢

[回复评论](https://kexue.fm/archives/6191/comment-page-1?replyTo=10251#respond-post-6191)

[苏剑林](https://kexue.fm) 发表于
December 3rd, 2018

不牵强呀，很直观的几何解释。

[回复评论](https://kexue.fm/archives/6191/comment-page-1?replyTo=10254#respond-post-6191)

小马

December 3rd, 2018

苏神又写了篇好文 例子举得通俗易懂
但是还是有个问题想请教苏神。
对于Skip Gram模型，我的理解是：其优化目标是：通过中心词，预测它上下文词。以图书馆来类比的话，就是将中心词上下文的词尽量放在同一个“书架”，而中心词本身不一定在这个“书架”，也就是说中心词不一定和上下文的距离很近。
在本文中，看公式（10），似乎是希望同时出现的词的内积要小，那就是不断优化以后，一起出现的词语，希望它们都尽可能放在一个“书架”上面，与我的理解有所出入。
是否是我的理解哪里出现了偏颇，希望苏神赐教。

[回复评论](https://kexue.fm/archives/6191/comment-page-1?replyTo=10252#respond-post-6191)

[苏剑林](https://kexue.fm) 发表于
December 3rd, 2018

这个问题是这样的：标准的word2vec实际上用了两份词向量，区分了上下文词向量和中心词向量，这样一来词的相似性实际上不是通过“出现在同一个句子”来体现，而是通过传递性来体现，即“A和B出现在同一个句子中，C和B出现在另一个句子中，那么A和C可能是相似的，A和B、C和B却不一定相似”。

如果上下文和中心词共用一份词向量，那就完全对应着本文所描述的“书架模型”了。这种模型的结论是怎样的呢？“同一个句子的词尽量放在同一个书架上”，这个原理没有错，但问题是，像“的”这些高频词，几乎出现在所有的句子中，这本“书”总不能跟其他所有的“书”放在一起吧？

我们知道对于任意两个词$p(i\|j)$不一定等于$p(j\|i)$，$p(i\|j)$很大不一定$p(j\|i)$就很大，所以这个书架模型实际上将$p(i\|j)$很大且$p(j\|i)$也很大的“书”放在同一个书架上了，而“$p(i\|j)$很大且$p(j\|i)$也很大”实际上就意味着它们的互信息很大。所以结论就是：最终并不总是同一个句子的词放在同一个书架上，而是互信息很大的词会放在同一个书架上。这跟我之前提出的simpler glove的词向量模型是很像的。（ [https://kexue.fm/archives/4667](https://kexue.fm/archives/4667)）

[回复评论](https://kexue.fm/archives/6191/comment-page-1?replyTo=10255#respond-post-6191)

小马 发表于
December 4th, 2018

感谢苏神的指点，这下清楚了很多

[回复评论](https://kexue.fm/archives/6191/comment-page-1?replyTo=10263#respond-post-6191)

jamesp

December 15th, 2018

博主有了解信息瓶颈方法吗？

[回复评论](https://kexue.fm/archives/6191/comment-page-1?replyTo=10353#respond-post-6191)

[苏剑林](https://kexue.fm) 发表于
December 16th, 2018

[https://kexue.fm/archives/6181](https://kexue.fm/archives/6181)
这个？

[回复评论](https://kexue.fm/archives/6191/comment-page-1?replyTo=10355#respond-post-6191)

核子

January 15th, 2019

兄弟。我想说 从房间里找一本书和从一张纸上找书名是一个问题。

[回复评论](https://kexue.fm/archives/6191/comment-page-1?replyTo=10548#respond-post-6191)

核子

January 15th, 2019

网上查到的名字似乎是 最小耗能原理（即新最小熵产生原理）

[回复评论](https://kexue.fm/archives/6191/comment-page-1?replyTo=10550#respond-post-6191)

核子

January 15th, 2019

{我们把每个词当作一本书，每个句子都可以看成每个人的“借书记录”}，这句话是比较核心的，问题是 你的假设是 {借书记录}中的书是相似的，但相邻的词语不能说相似吧？

[回复评论](https://kexue.fm/archives/6191/comment-page-1?replyTo=10552#respond-post-6191)

[苏剑林](https://kexue.fm) 发表于
January 15th, 2019

[@苏剑林\|comment-10255](https://kexue.fm/archives/6191/comment-page-1#comment-10255) 这个楼层有讨论过这个问题，这种情况下事实上捕捉的是互信息比较大的词对。然后可以参考这里： [https://kexue.fm/archives/4677](https://kexue.fm/archives/4677) ，进一步看到基于互信息的词向量模型如何捕捉到相似性。（内积为互信息，cos为相似）

[回复评论](https://kexue.fm/archives/6191/comment-page-1?replyTo=10553#respond-post-6191)

核子 发表于
January 16th, 2019

好的，我赞同。那么你的文章就有逻辑问题了，那么应该用词向量（互信息原理）来推到图书问题（相似性），而不应该反过来（从图书到词向量），尽管二者形式上一致。

[回复评论](https://kexue.fm/archives/6191/comment-page-1?replyTo=10562#respond-post-6191)

[苏剑林](https://kexue.fm) 发表于
January 16th, 2019

两者形式上一致，内容上也一致。

我可以假设“同一人同一次借的书是相似的”，我也可以假设“出现在同一个句子的词是相似的“。当然你或许马上就反对说同一个句子的词不一定相似，但事实上同一人同一次借的书也未必相似，因为可能有一些书很热门，人人都想看，这些书就相当于语言中的高频词（停用词、常用词），也就是这些高频词的存在，导致同一个句子的词不一定相似。

所以，从类比角度，两者是相当吻合的，不只是形式一样。

从逻辑上来看，本文的逻辑是：
1、图书馆的书为什么要把相似的书放在一起？
2、假设“同一人同一次借的书是相似的”，那么可以发现这种放书方案实际上能让借书的人更省力；
3、抛开“同一人同一次借的书是相似的”这个假设，直接根据借书记录和目标$(3)$，也可以优化出一套放书方案出来；
4、更抽象化；
5、发现”借书记录“跟”句子“有相似性，进而可以给出word2vec的新视角。

对于word2vec的词向量，我们认为质量是很不错的，相似的词确实放在相近的位置了，所以我们也有理由相信，步骤3给出来的放书方案，也能够把相近的书放在相近的位置。

[回复评论](https://kexue.fm/archives/6191/comment-page-1?replyTo=10563#respond-post-6191)

核子 发表于
January 21st, 2019

这个逻辑挺简练，那么‘同一人同一次借书相似’这个假设就没必要存在了，直接面向借书更省力（即目标3）就可以了

[回复评论](https://kexue.fm/archives/6191/comment-page-1?replyTo=10589#respond-post-6191)

[苏剑林](https://kexue.fm) 发表于
January 21st, 2019

假设“同一人同一次借书相似”会使得分析问题更简单，本文也是遵循从简单入手的思路。然后再去观察不完全满足这个条件的时候，会发现什么情况。

[回复评论](https://kexue.fm/archives/6191/comment-page-1?replyTo=10591#respond-post-6191)

homehehe

May 30th, 2019

对于固定的ii而言，最小化上式这不就是相当于最大对数似然了吗？所以结果就是q(j\|i)会尽量接近p(j\|i) 。
苏神， 这是为什么了？？？ 为什么 q(j\|i)接近p(j\|i) 就能让对数似然增大了？

[回复评论](https://kexue.fm/archives/6191/comment-page-1?replyTo=11269#respond-post-6191)

[苏剑林](https://kexue.fm) 发表于
October 12th, 2019

这可以自己证明。

[回复评论](https://kexue.fm/archives/6191/comment-page-1?replyTo=12172#respond-post-6191)

宇航员

October 13th, 2021

「最大似然跟最小熵其实具有相同的含义。」——这句话正巧解答了我在苏神其他文章留言提的问题。

[回复评论](https://kexue.fm/archives/6191/comment-page-1?replyTo=17553#respond-post-6191)

逍遥游

November 2nd, 2021

公式12中的
$$
q(j\|i)=\\frac{e^{-\\left\\Vert\\boldsymbol{v}\_i-\\boldsymbol{v}\_j\\right\\Vert^2}}{\\sum\\limits\_{j}^{j\\neq i}e^{-\\left\\Vert\\boldsymbol{v}\_i-\\boldsymbol{v}\_j\\right\\Vert^2}}
$$
是不是应该改为
$$
q(z\_j\|z\_i)=\\frac{e^{-\\left\\Vert\\boldsymbol{z}\_i-\\boldsymbol{z}\_j\\right\\Vert^2}}{\\sum\\limits\_{j}^{j\\neq i}e^{-\\left\\Vert\\boldsymbol{z}\_i-\\boldsymbol{z}\_j\\right\\Vert^2}}
$$
？

如果博主觉得文中的写法是对的，那能否顺便解释下SNE中的$\\boldsymbol{z}\_1,\\boldsymbol{z}\_2,\\dots,\\boldsymbol{z}\_N$怎样在优化中体现？

[回复评论](https://kexue.fm/archives/6191/comment-page-1?replyTo=17701#respond-post-6191)

[苏剑林](https://kexue.fm) 发表于
November 2nd, 2021

是你说的记号，已修正，谢谢反馈～

[回复评论](https://kexue.fm/archives/6191/comment-page-1?replyTo=17709#respond-post-6191)

[取消回复](https://kexue.fm/archives/6191#respond-post-6191)

你的大名

电子邮箱

个人网站（选填）

1\. 可以使用LaTeX代码，点击“预览效果”可查看效果；2. 可以通过点击评论楼层编号来引用该楼层；3. 网站可能会有点卡，如非确认评论失败，请 **不要重复点击提交**。

### 内容速览

[走进图书馆](https://kexue.fm/archives/6191#%E8%B5%B0%E8%BF%9B%E5%9B%BE%E4%B9%A6%E9%A6%86)
[放书的套路](https://kexue.fm/archives/6191#%E6%94%BE%E4%B9%A6%E7%9A%84%E5%A5%97%E8%B7%AF)
[省力地借书](https://kexue.fm/archives/6191#%E7%9C%81%E5%8A%9B%E5%9C%B0%E5%80%9F%E4%B9%A6)
[图书馆规划](https://kexue.fm/archives/6191#%E5%9B%BE%E4%B9%A6%E9%A6%86%E8%A7%84%E5%88%92)
[简化的借书模型](https://kexue.fm/archives/6191#%E7%AE%80%E5%8C%96%E7%9A%84%E5%80%9F%E4%B9%A6%E6%A8%A1%E5%9E%8B)
[约束优化规划](https://kexue.fm/archives/6191#%E7%BA%A6%E6%9D%9F%E4%BC%98%E5%8C%96%E8%A7%84%E5%88%92)
[一般成本最小化](https://kexue.fm/archives/6191#%E4%B8%80%E8%88%AC%E6%88%90%E6%9C%AC%E6%9C%80%E5%B0%8F%E5%8C%96)
[均匀化与去约束](https://kexue.fm/archives/6191#%E5%9D%87%E5%8C%80%E5%8C%96%E4%B8%8E%E5%8E%BB%E7%BA%A6%E6%9D%9F)
[最小熵=最大似然](https://kexue.fm/archives/6191#%E6%9C%80%E5%B0%8F%E7%86%B5=%E6%9C%80%E5%A4%A7%E4%BC%BC%E7%84%B6)
[Word2Vec](https://kexue.fm/archives/6191#Word2Vec)
[多样的度量](https://kexue.fm/archives/6191#%E5%A4%9A%E6%A0%B7%E7%9A%84%E5%BA%A6%E9%87%8F)
[Skip Gram](https://kexue.fm/archives/6191#Skip%20Gram)
[原理类比分析](https://kexue.fm/archives/6191#%E5%8E%9F%E7%90%86%E7%B1%BB%E6%AF%94%E5%88%86%E6%9E%90)
[再来看看t-SNE](https://kexue.fm/archives/6191#%E5%86%8D%E6%9D%A5%E7%9C%8B%E7%9C%8Bt-SNE)
[SNE](https://kexue.fm/archives/6191#SNE)
[t-SNE](https://kexue.fm/archives/6191#t-SNE)
[本文总结](https://kexue.fm/archives/6191#%E6%9C%AC%E6%96%87%E6%80%BB%E7%BB%93)

### 智能搜索

支持整句搜索！网站自动使用 [结巴分词](https://github.com/fxsjy/jieba) 进行分词，并结合ngrams排序算法给出合理的搜索结果。

### 热门标签

[生成模型](https://kexue.fm/tag/%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B/) [attention](https://kexue.fm/tag/attention/) [优化](https://kexue.fm/tag/%E4%BC%98%E5%8C%96/) [语言模型](https://kexue.fm/tag/%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/) [模型](https://kexue.fm/tag/%E6%A8%A1%E5%9E%8B/) [网站](https://kexue.fm/tag/%E7%BD%91%E7%AB%99/) [概率](https://kexue.fm/tag/%E6%A6%82%E7%8E%87/) [梯度](https://kexue.fm/tag/%E6%A2%AF%E5%BA%A6/) [转载](https://kexue.fm/tag/%E8%BD%AC%E8%BD%BD/) [微分方程](https://kexue.fm/tag/%E5%BE%AE%E5%88%86%E6%96%B9%E7%A8%8B/) [矩阵](https://kexue.fm/tag/%E7%9F%A9%E9%98%B5/) [天象](https://kexue.fm/tag/%E5%A4%A9%E8%B1%A1/) [分析](https://kexue.fm/tag/%E5%88%86%E6%9E%90/) [深度学习](https://kexue.fm/tag/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/) [积分](https://kexue.fm/tag/%E7%A7%AF%E5%88%86/) [python](https://kexue.fm/tag/python/) [优化器](https://kexue.fm/tag/%E4%BC%98%E5%8C%96%E5%99%A8/) [力学](https://kexue.fm/tag/%E5%8A%9B%E5%AD%A6/) [无监督](https://kexue.fm/tag/%E6%97%A0%E7%9B%91%E7%9D%A3/) [扩散](https://kexue.fm/tag/%E6%89%A9%E6%95%A3/) [几何](https://kexue.fm/tag/%E5%87%A0%E4%BD%95/) [节日](https://kexue.fm/tag/%E8%8A%82%E6%97%A5/) [生活](https://kexue.fm/tag/%E7%94%9F%E6%B4%BB/) [文本生成](https://kexue.fm/tag/%E6%96%87%E6%9C%AC%E7%94%9F%E6%88%90/) [数论](https://kexue.fm/tag/%E6%95%B0%E8%AE%BA/)

### 随机文章

- [最近广告特别多...（严厉声明）](https://kexue.fm/archives/411)
- [变分自编码器（七）：球面上的VAE（vMF-VAE）](https://kexue.fm/archives/8404)
- [科学空间：2010年3月重要天象](https://kexue.fm/archives/488)
- [如何在科学空间输入数学公式？——LaTeX帮助](https://kexue.fm/archives/83)
- [祝大家端午节快乐！](https://kexue.fm/archives/681)
- [纠缠的时空（一）：洛仑兹变换的矩阵](https://kexue.fm/archives/1889)
- [一道概率不等式：盯着它到显然成立为止！](https://kexue.fm/archives/10902)
- [生成扩散模型漫谈（十九）：作为扩散ODE的GAN](https://kexue.fm/archives/9662)
- [抛物线内一根定长的弦](https://kexue.fm/archives/1639)
- [今天把Blog升级了](https://kexue.fm/archives/21)

### 最近评论

- [tyj](https://kexue.fm/archives/10958/comment-page-2#comment-27894): 感觉和之前的一篇文章很像，应该算是concurrent work： https://arxiv...
- [li6626](https://kexue.fm/archives/10667/comment-page-1#comment-27893): 苏老师，Normalizing Flow有了新进展，论文链接:https://arxiv.or...
- [tesslqy](https://kexue.fm/archives/4718/comment-page-1#comment-27891): Evans那本书感觉就挺好的，不过长了一点且有点难（姜萍拿来装的就是这本书，可见流传度之广
- [Bauree](https://kexue.fm/archives/443/comment-page-1#comment-27890): 渴望大图，万分感谢！
- [tesslqy](https://kexue.fm/archives/4187/comment-page-1#comment-27888): 感觉应该是狄拉克测度，不过我也没系统学过Lebesgue以外的测度只是有这个感觉
- [tesslqy](https://kexue.fm/archives/4187/comment-page-1#comment-27887): 假设只有作者懂，但我不是作者，我懂了，假设不成立
- [苏剑林](https://kexue.fm/archives/10996/comment-page-1#comment-27886): 我们的目标是1，$f^∗\_t(0)=0$，如果$x=0$处还单调递减，而$x=l\_t$处大于零...
- [苏剑林](https://kexue.fm/archives/10958/comment-page-1#comment-27885): 最后的观点我是同意的。MeanFlow作为后来的工作，不管是形式上还是调参上，肯定参考了不少前...
- [Kai Liu](https://kexue.fm/archives/10996/comment-page-1#comment-27884): 我可能错过了最重要的点，为啥在x=0处是单调递增的？
- [苏剑林](https://kexue.fm/archives/10996/comment-page-1#comment-27883): 如果$l\_t$处是最大值，那么$l\_t$右侧就是下降对吧？但$x=0$处是单调递增的，这意味着...

### 友情链接

- [Cool Papers](https://papers.cool)
- [数学研发](https://bbs.emath.ac.cn)
- [Seatop](http://www.seatop.com.cn/)
- [Xiaoxia](https://xiaoxia.org/)
- [积分表-网络版](https://kexue.fm/sci/integral/index.html)
- [丝路博傲](http://blog.dvxj.com/)
- [ph4ntasy 饭特稀](http://www.ph4ntasy.com/)
- [数学之家](http://www.2math.cn/)
- [有趣天文奇观](http://interesting-sky.china-vo.org/)
- [TwistedW](http://www.twistedwg.com/)
- [godweiyang](https://godweiyang.com/)
- [AI柠檬](https://blog.ailemon.net/)
- [王登科-DK博客](https://greatdk.com)
- [ESON](https://blog.eson.org/)
- [枫之羽](https://fzhiy.net/)
- [Mathor's blog](https://wmathor.com/)
- [coding-zuo](https://coding-zuo.github.io/)
- [博科园](https://www.bokeyuan.net/)
- [孔皮皮的博客](https://www.kppkkp.top/)
- [运鹏的博客](https://yunpengtai.top/)
- [jiming.site](https://jiming.site/)
- [OmegaXYZ](https://www.omegaxyz.com/)
- [Blog by Eacls](https://www.eacls.top/)
- [EAI猩球](https://www.robotech.ink/)
- [文举的博客](https://liwenju0.com/)
- [用代码打点酱油](https://bruceyuan.com/)
- [申请链接](https://kexue.fm/links.html)

本站采用创作共用版权协议，要求署名、非商业用途和保持一致。转载本站内容必须也遵循“ [署名-非商业用途-保持一致](http://creativecommons.org/licenses/by-nc-nd/2.5/cn/)”的创作共用协议。
© 2009-2025 Scientific Spaces. All rights reserved. Theme by [laogui](http://www.laogui.com). Powered by [Typecho](http://typecho.org). 备案号: [粤ICP备09093259号-1/2](https://beian.miit.gov.cn/)。