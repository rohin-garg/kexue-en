## SEARCH

## MENU

- [打赏](https://kexue.fm/reward.html)
- [公式](https://kexue.fm/latex.html)
- [天象](https://kexue.fm/ac.html)
- [链接](https://kexue.fm/links.html)
- [时光](https://kexue.fm/me.html)
- [博览](https://kexue.fm/science.html)
- [归档](https://kexue.fm/content.html)

## CATEGORIES

- [千奇百怪](https://kexue.fm/category/Everything)
- [天文探索](https://kexue.fm/category/Astronomy)
- [数学研究](https://kexue.fm/category/Mathematics)
- [物理化学](https://kexue.fm/category/Phy-chem)
- [信息时代](https://kexue.fm/category/Big-Data)
- [生物自然](https://kexue.fm/category/Biology)
- [图片摄影](https://kexue.fm/category/Photograph)
- [问题百科](https://kexue.fm/category/Questions)
- [生活/情感](https://kexue.fm/category/Life-Feeling)
- [资源共享](https://kexue.fm/category/Resources)

## NEWPOSTS

- [随机矩阵的谱范数的快速估计](https://kexue.fm/archives/11335)
- [DiVeQ：一种非常简洁的VQ训练方案](https://kexue.fm/archives/11328)
- [为什么线性注意力要加Short C...](https://kexue.fm/archives/11320)
- [AdamW的Weight RMS的...](https://kexue.fm/archives/11307)
- [重新思考学习率与Batch Siz...](https://kexue.fm/archives/11301)
- [重新思考学习率与Batch Siz...](https://kexue.fm/archives/11285)
- [重新思考学习率与Batch Siz...](https://kexue.fm/archives/11280)
- [为什么Adam的Update RM...](https://kexue.fm/archives/11267)
- [重新思考学习率与Batch Siz...](https://kexue.fm/archives/11260)
- [Cool Papers更新：简单适...](https://kexue.fm/archives/11250)

## COMMENTS

- [QiYang: (27) 式求解出了 \\Phi 之后，(22) 式里的 \\De...](https://kexue.fm/archives/10592/comment-page-2#comment-28661)
- [论文机器: 苏老师，有论文通过实验验证过Muon下使用MuP能否迁移超参数...](https://kexue.fm/archives/11241/comment-page-1#comment-28660)
- [CuddleSabe: 并且 softmax(KV) 也是可以让秩提升的， QK 和 ...](https://kexue.fm/archives/7546/comment-page-4#comment-28659)
- [CuddleSabe: 大佬，如果在 self-attention 中使用 linea...](https://kexue.fm/archives/7546/comment-page-4#comment-28658)
- [大力: 你好，苏老师，请问这些式子中的~w都是近似看做广义迁移核吗？如...](https://kexue.fm/archives/4598/comment-page-1#comment-28656)
- [小熊: 苏老师，可以求个图集吗？邮箱 492860400@qq.co...](https://kexue.fm/archives/443/comment-page-1#comment-28655)
- [且寻: 嗷，理解苏老师您的意思了，不加0均值约束就是公式$\\eqref...](https://kexue.fm/archives/10815/comment-page-1#comment-28654)
- [且寻: 公式$\\eqref{eq:aux-loss-free-3}$。](https://kexue.fm/archives/10815/comment-page-1#comment-28653)
- [pang: 您好，关于式子7那里提到的第二类旋转位置编码只需要再给Outp...](https://kexue.fm/archives/10862/comment-page-1#comment-28652)
- [苏剑林: 你是说公式$\\eqref{eq:aux-loss-free-5}$？](https://kexue.fm/archives/10815/comment-page-1#comment-28651)

## USERLOGIN

- [登录](https://kexue.fm/admin/login.php)

[科学空间\|Scientific Spaces](https://kexue.fm)

- [登录](https://kexue.fm/admin/login.php)
- [打赏](https://kexue.fm/reward.html)
- [公式](https://kexue.fm/latex.html)
- [天象](https://kexue.fm/ac.html)
- [链接](https://kexue.fm/links.html)
- [时光](https://kexue.fm/me.html)
- [博览](https://kexue.fm/science.html)
- [归档](https://kexue.fm/content.html)

渴望成为一个小飞侠

- [欢迎订阅](https://kexue.fm/feed)
- [个性邮箱](https://kexue.fm/archives/119)
- [天象信息](https://kexue.fm/ac.html)
- [观测ISS](https://kexue.fm/archives/41)
- [LaTeX](https://kexue.fm/latex.html)
- [关于博主](https://kexue.fm/me.html)

欢迎访问“科学空间”，这里将与您共同探讨自然科学，回味人生百态；也期待大家的分享～

- [**千奇百怪** Everything](https://kexue.fm/category/Everything)
- [**天文探索** Astronomy](https://kexue.fm/category/Astronomy)
- [**数学研究** Mathematics](https://kexue.fm/category/Mathematics)
- [**物理化学** Phy-chem](https://kexue.fm/category/Phy-chem)
- [**信息时代** Big-Data](https://kexue.fm/category/Big-Data)
- [**生物自然** Biology](https://kexue.fm/category/Biology)
- [**图片摄影** Photograph](https://kexue.fm/category/Photograph)
- [**问题百科** Questions](https://kexue.fm/category/Questions)
- [**生活/情感** Life-Feeling](https://kexue.fm/category/Life-Feeling)
- [**资源共享** Resources](https://kexue.fm/category/Resources)

- [**千奇百怪**](https://kexue.fm/category/Everything)
- [**天文探索**](https://kexue.fm/category/Astronomy)
- [**数学研究**](https://kexue.fm/category/Mathematics)
- [**物理化学**](https://kexue.fm/category/Phy-chem)
- [**信息时代**](https://kexue.fm/category/Big-Data)
- [**生物自然**](https://kexue.fm/category/Biology)
- [**图片摄影**](https://kexue.fm/category/Photograph)
- [**问题百科**](https://kexue.fm/category/Questions)
- [**生活/情感**](https://kexue.fm/category/Life-Feeling)
- [**资源共享**](https://kexue.fm/category/Resources)

[首页](https://kexue.fm) [信息时代](https://kexue.fm/category/Big-Data) 用变分推断统一理解生成模型（VAE、GAN、AAE、ALI）

18Jul

# [用变分推断统一理解生成模型（VAE、GAN、AAE、ALI）](https://kexue.fm/archives/5716)

By 苏剑林 \|
2018-07-18 \|
435420位读者\|

> 前言：我小学开始就喜欢纯数学，后来也喜欢上物理，还学习过一段时间的理论物理，直到本科毕业时，我才慢慢进入机器学习领域。所以，哪怕在机器学习领域中，我的研究习惯还保留着数学和物理的风格：企图从最少的原理出发，理解、推导尽可能多的东西。这篇文章是我这个理念的结果之一，试图以变分推断作为出发点，来统一地理解深度学习中的各种模型，尤其是各种让人眼花缭乱的GAN。本文已经挂到arxiv上，需要读英文原稿的可以移步到 [《Variational Inference: A Unified Framework of Generative Models and Some Revelations》](https://papers.cool/arxiv/1807.05936)。
>
> 下面是文章的介绍。其实，中文版的信息可能还比英文版要稍微丰富一些，原谅我这蹩脚的英语...

> **摘要：** 本文从一种新的视角阐述了变分推断，并证明了EM算法、VAE、GAN、AAE、ALI(BiGAN)都可以作为变分推断的某个特例。其中，论文也表明了标准的GAN的优化目标是不完备的，这可以解释为什么GAN的训练需要谨慎地选择各个超参数。最后，文中给出了一个可以改善这种不完备性的正则项，实验表明该正则项能增强GAN训练的稳定性。

近年来，深度生成模型，尤其是GAN，取得了巨大的成功。现在我们已经可以找到数十个乃至上百个GAN的变种。然而，其中的大部分都是凭着经验改进的，鲜有比较完备的理论指导。

本文的目标是通过变分推断来给这些生成模型建立一个统一的框架。首先，本文先介绍了变分推断的一个新形式，这个新形式其实在博客以前的文章中就已经介绍过，它可以让我们在几行字之内导出变分自编码器（VAE）和EM算法。然后，利用这个新形式，我们能直接导出GAN，并且发现标准GAN的loss实则是不完备的，缺少了一个正则项。如果没有这个正则项，我们就需要谨慎地调整超参数，才能使得模型收敛。

实际上，本文这个工作的初衷，就是要将GAN纳入到变分推断的框架下。目前看来，最初的意图已经达到了，结果让人欣慰。新导出的正则项实际上是一个副产品，并且幸运的是，在我们的实验中这个副产品生效了。

## 变分推断新解 [\#](https://kexue.fm/kexue.fm\#%E5%8F%98%E5%88%86%E6%8E%A8%E6%96%AD%E6%96%B0%E8%A7%A3)

假设$x$为显变量，$z$为隐变量，$\\tilde{p}(x)$为$x$的证据分布，并且有
$$\\begin{equation}q(x)=q\_{\\theta}(x)=\\int q\_{\\theta}(x,z)dz\\end{equation}$$
我们希望$q\_{\\theta}(x)$能逼近$\\tilde{p}(x)$，所以一般情况下我们会去最大化似然函数
$$\\begin{equation}\\theta = \\mathop{\\text{argmax}}\_{\\theta}\\, \\int \\tilde{p}(x)\\log q(x) dx\\end{equation}$$
这也等价于最小化KL散度$KL(\\tilde{p}(x)\\Vert q(x))$：
$$\\begin{equation}KL(\\tilde{p}(x)\\Vert q(x)) = \\int \\tilde{p}(x) \\log \\frac{\\tilde{p}(x)}{q(x)}dx\\end{equation}$$
但是由于积分可能难以计算，因此大多数情况下都难以直接优化。

变分推断中，首先引入联合分布$p(x,z)$使得$\\tilde{p}(x)=\\int p(x,z)dz$，而变分推断的本质，就是将边际分布的KL散度$KL(\\tilde{p}(x)\\Vert q(x))$改为联合分布的KL散度$KL(p(x,z)\\Vert q(x,z))$或$KL(q(x,z)\\Vert p(x,z))$，而
$$\\begin{equation}\\begin{aligned}KL(p(x,z)\\Vert q(x,z)) &= KL(\\tilde{p}(x)\\Vert q(x)) + \\int \\tilde{p}(x) KL(p(z\|x)\\Vert q(z\|x)) dx\\\
&\\geq KL(\\tilde{p}(x)\\Vert q(x))\\end{aligned}\\end{equation}$$
意味着联合分布的KL散度是一个更强的条件（上界）。所以一旦优化成功，那么我们就得到$q(x,z)\\to p(x,z)$，从而$\\int q(x,z)dz \\to \\int p(x,z)dz = \\tilde{p}(x)$，即$\\int q(x,z)dz$成为了真实分布$\\tilde{p}(x)$的一个近似。

当然，我们本身不是为了加强条件而加强，而是因为在很多情况下，$KL(p(x,z)\\Vert q(x,z))$或$KL(q(x,z)\\Vert p(x,z))$往往比$KL(\\tilde{p}(x)\\Vert q(x))$更加容易计算。所以变分推断是提供了一个可计算的方案。

## VAE和EM算法 [\#](https://kexue.fm/kexue.fm\#VAE%E5%92%8CEM%E7%AE%97%E6%B3%95)

由上述关于变分推断的新理解，我们可以在几句话内导出两个基本结果：变分自编码器和EM算法。这部分内容，实际上在 [《从最大似然到EM算法：一致的理解方式》](https://kexue.fm/archives/5239) 和 [《变分自编码器（二）：从贝叶斯观点出发》](https://kexue.fm/archives/5343) 已经详细介绍过了。这里用简单几句话重提一下。

### VAE [\#](https://kexue.fm/kexue.fm\#VAE)

在VAE中，我们设$q(x,z)=q(x\|z)q(z), p(x,z)=\\tilde{p}(x) p(z\|x)$，其中$q(x\|z),p(z\|x)$带有未知参数的高斯分布而$q(z)$是标准高斯分布。最小化的目标是
$$\\begin{equation}\\label{eq:kl-oo}KL\\left(p(x,z)\\Vert q(x,z) \\right)=\\iint \\tilde{p}(x) p(z\|x) \\log \\frac{\\tilde{p}(x) p(z\|x)}{q(x\|z)q(z)}dxdz\\end{equation}$$
其中$\\log \\tilde{p}(x)$没有包含优化目标，可以视为常数，而对$\\tilde{p}(x)$的积分则转化为对样本的采样，从而
$$\\begin{equation}\\mathbb{E}\_{x\\sim \\tilde{p}(x)}\\left\[-\\int p(z\|x)\\log q(x\|z)dz + KL(p(z\|x)\\Vert q(z))\\right\]\\end{equation}$$
因为$q(x\|z),p(z\|x)$为带有神经网络的高斯分布，这时候$KL\\left(p(z\|x)\\Vert q(z)\\right)$可以显式地算出，而通过重参数技巧来采样一个点完成积分$\\int p(z\|x) \\log q(x\|z)dz$的估算，可以得到VAE最终要最小化的loss：
$$\\begin{equation}\\mathbb{E}\_{x\\sim \\tilde{p}(x)}\\Big\[-\\log q(x\|z) + KL(p(z\|x)\\Vert q(z))\\Big\]\\end{equation}$$

### EM算法 [\#](https://kexue.fm/kexue.fm\#EM%E7%AE%97%E6%B3%95)

在VAE中我们对后验分布做了约束，仅假设它是高斯分布，所以我们优化的是高斯分布的参数。如果不作此假设，那么直接优化原始目标$\\eqref{eq:kl-oo}$，在某些情况下也是可操作的，但这时候只能采用交替优化的方式：先固定$p(z\|x)$，优化$q(x\|z)$，那么就有
$$\\begin{equation}\\label{eq:em-1}q(x\|z) = \\mathop{\\text{argmax}}\_{q(x\|z)} \\,\\mathbb{E}\_{x\\sim \\tilde{p}(x)}\\left\[\\int p(z\|x) \\log q(x,z) dz\\right\]\\end{equation}$$
完成这一步后，我们固定$q(x,z)$，优化$p(z\|x)$，先将$q(x\|z)q(z)$写成$q(z\|x)q(x)$的形式：
$$\\begin{equation}q(x)=\\int q(x\|z)q(z)dz,\\quad q(z\|x)=\\frac{q(x\|z)q(z)}{q(x)}\\end{equation}$$
那么有
$$\\begin{equation}\\begin{aligned}p(z\|x) =& \\mathop{\\text{argmin}}\_{p(z\|x)} \\,\\mathbb{E}\_{x\\sim \\tilde{p}(x)}\\left\[\\int p(z\|x) \\log \\frac{p(z\|x)}{q(z\|x)q(x)} dz\\right\]\\\
=& \\mathop{\\text{argmin}}\_{p(z\|x)} \\,\\mathbb{E}\_{x\\sim \\tilde{p}(x)}\\left\[KL\\left(p(z\|x)\\Vert q(z\|x)\\right)-\\log q(x)\\right\]\\\
=& \\mathop{\\text{argmin}}\_{p(z\|x)} \\,\\mathbb{E}\_{x\\sim \\tilde{p}(x)} \\left\[KL\\left(p(z\|x)\\Vert q(z\|x)\\right)\\right\]
\\end{aligned}\\end{equation}$$
由于现在对$p(z\|x)$没有约束，因此可以直接让$p(z\|x)=q(z\|x)$使得loss等于0。也就是说，$p(z\|x)$有理论最优解：
$$\\begin{equation}\\label{eq:em-2}p(z\|x) = \\frac{q(x\|z)q(z)}{\\int q(x\|z)q(z)dz}\\end{equation}$$
$\\eqref{eq:em-1},\\eqref{eq:em-2}$的交替执行，构成了EM算法的求解步骤。这样，我们从变分推断框架中快速得到了EM算法。

## 变分推断下的GAN [\#](https://kexue.fm/kexue.fm\#%E5%8F%98%E5%88%86%E6%8E%A8%E6%96%AD%E4%B8%8B%E7%9A%84GAN)

在这部分内容中，我们介绍了一般化的将GAN纳入到变分推断中的方法，这将引导我们得到GAN的新理解，以及一个有效的正则项。

### 一般框架 [\#](https://kexue.fm/kexue.fm\#%E4%B8%80%E8%88%AC%E6%A1%86%E6%9E%B6)

同VAE一样，GAN也希望能训练一个生成模型$q(x\|z)$，来将$q(z)=N(z;0,I)$映射为数据集分布$\\tilde{p}(x)$，不同于VAE中将$q(x\|z)$选择为高斯分布，GAN的选择是
$$\\begin{equation}q(x\|z)=\\delta\\left(x - G(z)\\right),\\quad q(x)=\\int q(x\|z)q(z)dz\\end{equation}$$
其中$\\delta(x)$是狄拉克$\\delta$函数，$G(z)$即为生成器的神经网络。

一般我们会认为$z$是一个隐变量，但由于$\\delta$函数实际上意味着单点分布，因此可以认为$x$与$z$的关系已经是一一对应的，所以$z$与$x$的关系已经“不够随机”，在GAN中我们认为它不是隐变量（意味着我们不需要考虑后验分布$p(z\|x)$）。

事实上，在GAN中仅仅引入了一个二元的隐变量$y$来构成联合分布
$$\\begin{equation}q(x,y)=\\left\\{\\begin{aligned}&\\tilde{p}(x)p\_1,\\,y=1\\\&q(x)p\_0,\\,y=0\\end{aligned}\\right.\\end{equation}$$
这里$p\_1 = 1-p\_0$描述了一个二元概率分布，我们直接取$p\_1=p\_0=1/2$。另一方面，我们设$p(x,y)=p(y\|x) \\tilde{p}(x)$，$p(y\|x)$是一个条件伯努利分布。而优化目标是另一方向的$KL\\left(q(x,y)\\Vert p(x,y) \\right)$：
$$\\begin{equation}\\begin{aligned}KL\\left(q(x,y)\\Vert p(x,y) \\right)=&\\int \\tilde{p}(x)p\_1\\log \\frac{\\tilde{p}(x)p\_1}{p(1\|x)\\tilde{p}(x)}dx+\\int q(x)p\_0\\log \\frac{q(x)p\_0}{p(0\|x)\\tilde{p}(x)}dx\\\
\\sim&\\int \\tilde{p}(x)\\log \\frac{1}{p(1\|x)}dx+\\int q(x)\\log \\frac{q(x)}{p(0\|x)\\tilde{p}(x)}dx\\end{aligned}\\end{equation}$$
一旦成功优化，那么就有$q(x,y)\\to p(x,y)$，那么
$$\\begin{equation}p\_1 \\tilde{p}(x) + p\_0 q(x) = \\sum\_y q(x,y) \\to \\sum\_y p(x,y) = \\tilde{p}(x)\\end{equation}$$
从而$q(x)\\to\\tilde{p}(x)$，完成了生成模型的构建。

现在我们优化对象有$p(y\|x)$和$G(x)$，记$p(1\|x)=D(x)$，这就是判别器。类似EM算法，我们进行交替优化：先固定$G(z)$，这也意味着$q(x)$固定了，然后优化$p(y\|x)$，这时候略去常量，得到优化目标为：
$$\\begin{equation}D = \\mathop{\\text{argmin}}\_{D} -\\mathbb{E}\_{x\\sim\\tilde{p}(x)}\\left\[\\log D(x)\\right\]-\\mathbb{E}\_{x\\sim q(x)}\\left\[\\log (1-D(x))\\right\]\\end{equation}$$
然后固定$D(x)$来优化$G(x)$，这时候相关的loss为：
$$\\begin{equation}\\label{eq:gan-g-loss}G = \\mathop{\\text{argmin}}\_{G}\\int q(x)\\log \\frac{q(x)}{(1-D(x)) \\tilde{p}(x)}dx\\end{equation}$$
这里包含了我们不知道的$\\tilde{p}(x)$，但是假如$D(x)$模型具有足够的拟合能力，那么跟$\\eqref{eq:em-2}$式同理，$D(x)$的最优解应该是
$$\\begin{equation}D(x)=\\frac{\\tilde{p}(x)}{\\tilde{p}(x)+q^{o}(x)}\\end{equation}$$
这里的$q^{o}(x)$是前一阶段的$q(x)$。从中解出$\\tilde{p}(x)$，代入$\\eqref{eq:gan-g-loss}$得到
$$\\begin{equation}\\begin{aligned}\\int q(x)\\log \\frac{q(x)}{D(x) q^{o}(x)}dx=&-\\mathbb{E}\_{x\\sim q(x)}\\log D(x) + KL\\left(q(x)\\Vert q^{o}(x)\\right)\\\
=&-\\mathbb{E}\_{z\\sim q(z)}\\log D(G(z)) + KL\\left(q(x)\\Vert q^{o}(x)\\right)
\\end{aligned}\\end{equation}$$

### 基本分析 [\#](https://kexue.fm/kexue.fm\#%E5%9F%BA%E6%9C%AC%E5%88%86%E6%9E%90)

可以看到，第一项就是标准的GAN生成器所采用的loss之一。
$$\\begin{equation}-\\mathbb{E}\_{z\\sim q(z)}\\log D(G(z))\\end{equation}$$
多出来的第二项，描述了新分布与旧分布之间的距离。这两项loss是对抗的，因为$KL\\left(q(x)\\Vert q^{o}(x)\\right)$希望新旧分布尽量一致，但是如果判别器充分优化的话，对于旧分布$q^{o}(x)$中的样本，$D(x)$都很小（几乎都被识别为负样本），所以$-\\log D(x)$会相当大，反之亦然。这样一来，整个loss一起优化的话，模型既要“传承”旧分布$q^{o}(x)$，同时要在往新方向$p(1\|y)$探索，在新旧之间插值。

我们知道，目前标准的GAN的生成器loss都不包含$KL\\left(q(x)\\Vert q^{o}(x)\\right)$，这事实上造成了loss的不完备。假设有一个优化算法总能找到$G(z)$的理论最优解、并且$G(z)$具有无限的拟合能力，那么$G(z)$只需要生成唯一一个使得$D(x)$最大的样本（不管输入的$z$是什么），这就是模型坍缩。这样说的话，理论上它一定会发生。

那么，$KL\\left(q(x)\\Vert q^{o}(x)\\right)$给我们的启发是什么呢？我们设
$$\\begin{equation}q^{o}(x)=q\_{\\theta-\\Delta \\theta}(x),\\quad q(x)=q\_{\\theta}(x)\\end{equation}$$
也就是说，假设当前模型的参数改变量为$\\Delta\\theta$，那么展开到二阶得到
$$\\begin{equation}KL\\left(q(x)\\Vert q^{o}(x)\\right)\\approx \\int\\frac{\\left(\\Delta\\theta\\cdot \\nabla\_{\\theta}q\_{\\theta}(x)\\right)^2}{2q\_{\\theta}(x)} dx \\approx \\left(\\Delta\\theta\\cdot c\\right)^2\\end{equation}$$

我们已经指出一个完备的GAN生成器的损失函数应该要包含$KL\\left(q(x)\\Vert q^{o}(x)\\right)$，如果不包含的话，那么就要通过各种间接手段达到这个效果，上述近似表明额外的损失约为$\\left(\\Delta\\theta\\cdot c\\right)^2$，这就要求我们不能使得它过大，也就是不能使得$\\Delta\\theta$过大（在每个阶段$c$可以近似认为是一个常数）。而我们用的是基于梯度下降的优化算法，所以$\\Delta\\theta$正比于梯度，因此标准GAN训练时的很多trick，比如梯度裁剪、用adam优化器、用BN，都可以解释得通了，它们都是为了稳定梯度，使得$\\theta$不至于过大，同时，$G(z)$的迭代次数也不能过多，因为过多同样会导致$\\Delta\\theta$过大。

还有，这部分的分析只适用于生成器，而判别器本身并不受约束，因此判别器可以训练到最优。

### 正则项 [\#](https://kexue.fm/kexue.fm\#%E6%AD%A3%E5%88%99%E9%A1%B9)

现在我们从中算出一些真正有用的内容，直接对$KL\\left(q(x)\\Vert q^{o}(x)\\right)$进行估算，以得到一个可以在实际训练中使用的正则项。直接计算是难以进行的，但我们可以用$KL\\left(q(x,z)\\Vert \\tilde{q}(x,z)\\right)$去估算它：
$$\\begin{equation}\\begin{aligned}KL\\left(q(x,z)\\Vert \\tilde{q}(x,z)\\right)=&\\iint q(x\|z)q(z)\\log \\frac{q(x\|z)q(z)}{\\tilde{q}(x\|z)q(z)}dxdz\\\
=&\\iint \\delta\\left(x-G(z)\\right)q(z)\\log \\frac{\\delta\\left(x-G(z)\\right)}{\\delta\\left(x-G^{o}(z)\\right)}dxdz\\\
=&\\int q(z)\\log \\frac{\\delta(0)}{\\delta\\left(G(z)-G^{o}(z)\\right)}dz
\\end{aligned}\\end{equation}$$
因为有极限
$$\\begin{equation}\\delta(x)=\\lim\_{\\sigma\\to 0}\\frac{1}{(2\\pi\\sigma^2)^{d/2}}\\exp\\left(-\\frac{x^2}{2\\sigma^2}\\right)\\end{equation}$$
所以可以将$\\delta(x)$看成是小方差的高斯分布，代入算得也就是我们有
$$\\begin{equation}KL\\left(q(x)\\Vert q^{o}(x)\\right)\\sim \\lambda \\int q(z)\\Vert G(z) - G^{o}(z)\\Vert^2 dz\\end{equation}$$
所以完整生成器的loss可以选为
$$\\begin{equation}\\mathbb{E}\_{z\\sim q(z)}\\left\[-\\log D(G(z))+\\lambda \\Vert G(z) - G^{o}(z)\\Vert^2\\right\] \\end{equation}$$
也就是说，可以用新旧生成样本的距离作为正则项，正则项保证模型不会过于偏离旧分布。

下面的两个在人脸数据CelebA上的实验表明这个正则项是生效的。实验代码修改自 [这里](https://github.com/LynnHo/DCGAN-LSGAN-WGAN-WGAN-GP-Tensorflow)，目前放在 [我的github](https://github.com/bojone/gan/tree/master/vgan) 上。

**实验一：普通的DCGAN网络，每次迭代生成器和判别器各训练一个batch。**

不带正则项，在25个epoch之后模型开始坍缩

带有正则项，模型能一直稳定训练

**实验二：普通的DCGAN网络，但去掉BN，每次迭代生成器和判别器各训练五个batch。**

不带正则项，模型收敛速度比较慢

带有正则项，模型更快“步入正轨”

## GAN相关模型 [\#](https://kexue.fm/kexue.fm\#GAN%E7%9B%B8%E5%85%B3%E6%A8%A1%E5%9E%8B)

对抗自编码器（Adversarial Autoencoders，AAE）和对抗推断学习（Adversarially Learned Inference，ALI）这两个模型是GAN的变种之一，也可以被纳入到变分推断中。当然，有了前述准备后，这仅仅就像两道作业题罢了。

有意思的是，在ALI之中，我们有一些反直觉的结果。

### GAN视角下的AAE [\#](https://kexue.fm/kexue.fm\#GAN%E8%A7%86%E8%A7%92%E4%B8%8B%E7%9A%84AAE)

事实上，只需要在GAN的论述中，将$x,z$的位置交换，就得到了AAE的框架。

具体来说，AAE希望能训练一个编码模型$p(z\|x)$，来将真实分布$\\tilde{q}(x)$映射为标准高斯分布$q(z)=N(z;0,I)$，而
$$\\begin{equation}p(z\|x)=\\delta\\left(z - E(x)\\right),\\quad p(z)=\\int p(z\|x)\\tilde{q}(x)dx\\end{equation}$$
其中$E(x)$即为编码器的神经网络。

同GAN一样，AAE引入了一个二元的隐变量$y$，并有
$$\\begin{equation}p(z,y)=\\left\\{\\begin{aligned}&p(z)p\_1,\\,y=1\\\&q(z)p\_0,\\,y=0\\end{aligned}\\right.\\end{equation}$$
同样直接取$p\_1=p\_0=1/2$。另一方面，我们设$q(z,y)=q(y\|z) q(z)$，这里的后验分布$p(y\|z)$是一个输入为$z$的二元分布，然后去优化$KL\\left(p(z,y)\\Vert q(z,y) \\right)$：
$$\\begin{equation}\\begin{aligned}KL\\left(p(z,y)\\Vert q(z,y) \\right)=&\\int p(z)p\_1\\log \\frac{p(z)p\_1}{q(1\|z)q(z)}dz+\\int q(z)p\_0\\log \\frac{q(z)p\_0}{q(0\|z)q(z)}dz\\\
\\sim&\\int p(z)\\log \\frac{p(z)}{q(1\|z)q(z)}dz+\\int q(z)\\log \\frac{1}{q(0\|z)}dz\\end{aligned}\\end{equation}$$

现在我们优化对象有$q(y\|z)$和$E(x)$，记$q(0\|z)=D(z)$，依然交替优化：先固定$E(x)$，这也意味着$p(z)$固定了，然后优化$q(y\|z)$，这时候略去常量，得到优化目标为：
$$\\begin{equation}\\begin{aligned}D=\\mathop{\\text{argmin}}\_D &-\\mathbb{E}\_{z\\sim p(z)}\\left\[\\log (1-D(z))\\right\]-\\mathbb{E}\_{z\\sim q(z)}\\left\[\\log D(z)\\right\]\\\
=\\mathop{\\text{argmin}}\_D &-\\mathbb{E}\_{z\\sim \\tilde{p}(x)}\\left\[\\log (1-D(E(x)))\\right\]-\\mathbb{E}\_{z\\sim q(z)}\\left\[\\log D(z)\\right\]\\end{aligned}\\end{equation}$$
然后固定$D(z)$来优化$E(x)$，这时候相关的loss为：
$$\\begin{equation}E = \\mathop{\\text{argmin}}\_E \\int p(z)\\log \\frac{p(z) }{(1-D(z)) q(z)}dz\\end{equation}$$
利用$D(z)$的理论最优解$D(z)=q(z)/\[p^{o}(z)+q(z)\]$，代入loss得到
$$\\begin{equation}\\mathbb{E}\_{x\\sim \\tilde{p}(x)}\[-\\log D(E(x))\] + KL\\left(p(z)\\Vert p^{o}(z)\\right)\\end{equation}$$
一方面，同标准GAN一样，谨慎地训练，我们可以去掉第二项，得到
$$\\begin{equation}\\mathbb{E}\_{x\\sim \\tilde{p}(x)}\[-\\log D(E(x))\]\\end{equation}$$
另外一方面，我们可以得到编码器后再训练一个解码器$G(z)$，但是如果所假设的$E(x),G(z)$的拟合能力是充分的，重构误差可以足够小，那么将$G(z)$加入到上述loss中并不会干扰GAN的训练，因此可以联合训练：
$$\\begin{equation}G,E = \\mathop{\\text{argmin}}\_{G,E}\\mathbb{E}\_{x\\sim \\tilde{p}(x)}\\left\[-\\log D(E(x))+\\lambda\\Vert x - G(E(x))\\Vert^2\\right\]\\end{equation}$$

### 反直觉的ALI版本 [\#](https://kexue.fm/kexue.fm\#%E5%8F%8D%E7%9B%B4%E8%A7%89%E7%9A%84ALI%E7%89%88%E6%9C%AC)

ALI像是GAN和AAE的融合，另一个几乎一样的工作是Bidirectional GAN (BiGAN)。相比于GAN，它将$z$也作为隐变量纳入到变分推断中。具体来说，在ALI中有
$$\\begin{equation}q(x,z,y)=\\left\\{\\begin{aligned}&p(z\|x)\\tilde{p}(x) p\_1,\\,y=1\\\&q(x\|z)q(z)p\_0,\\,y=0\\end{aligned}\\right.\\end{equation}$$
以及$p(x,z,y)=p(y\|x,z) p(z\|x) \\tilde{p}(x)$，然后去优化$KL\\left(q(x,z,y)\\Vert p(x,z,y) \\right)$：
$$\\begin{equation}\\begin{aligned}&\\iint p(z\|x)\\tilde{p}(x) p\_1\\log \\frac{p(z\|x)\\tilde{p}(x) p\_1}{p(1\|x,z) p(z\|x) \\tilde{p}(x)}dxdz\\\
+&\\iint q(x\|z)q(z)p\_0\\log \\frac{q(x\|z)q(z)p\_0}{p(0\|x,z) p(z\|x) \\tilde{p}(x)}dxdz\\end{aligned}\\end{equation}$$
等价于最小化
$$\\begin{equation}\\label{eq: ori-loss-ali}\\iint p(z\|x)\\tilde{p}(x)\\log \\frac{1}{p(1\|x,z)}dxdz+\\iint q(x\|z)q(z)\\log \\frac{q(x\|z)q(z)}{p(0\|x,z) p(z\|x) \\tilde{p}(x)}dxdz\\end{equation}$$
现在优化的对象有$p(y\|x,z),p(z\|x),q(x\|z)$，记$p(1\|x,z)=D(x,z)$，而$p(z\|x)$是一个带有编码器$E$的高斯分布或狄拉克分布，$q(x\|z)$是一个带有生成器$G$的高斯分布或狄拉克分布。依然交替优化：先固定$E,G$，那么与$D$相关的loss为
$$\\begin{equation}D=\\mathop{\\text{argmin}}\_D -\\mathbb{E}\_{x\\sim\\tilde{p}(x),z\\sim p(z\|x)} \\log D(x,z) - \\mathbb{E}\_{z\\sim q(z),x\\sim q(x\|z)} \\log (1-D(x,z))\\end{equation}$$
跟VAE一样，对$p(z\|x)$和$q(x\|z)$的期望可以通过“重参数”技巧完成。接着固定$D$来优化$G,E$，因为这时候有$E$又有$G$，整个loss没得化简，还是$\\eqref{eq: ori-loss-ali}$那样。但利用$D$的最优解
$$\\begin{equation}D(x,z)=\\frac{p^{o}(z\|x)\\tilde{p}(x)}{p^{o}(z\|x)\\tilde{p}(x)+q^{o}(x\|z)q(z)}\\end{equation}$$
可以转化为
$$\\begin{equation}\\begin{aligned}-\\iint p(z\|x)\\tilde{p}(x)\\log D(x, z) dxdz -\\iint q(x\|z) q(z)\\log D(x, z) dxdz\\\
+\\int q(z) KL(q(x\|z)\\Vert q^o(x\|z)) dz + \\iint q(x\|z) q(z)\\log \\frac{p^o(z\|x)}{p(z\|x)}dxdz\\end{aligned}\\end{equation}$$
由于$q(x\|z),p(x\|z)$都是高斯分布，事实上后两项我们可以具体地算出来（配合重参数技巧），但同标准GAN一样，谨慎地训练，我们可以简单地去掉后面两项，得到
$$\\begin{equation}\\label{eq:our-ali-g}-\\iint p(z\|x)\\tilde{p}(x)\\log D(x, z) dxdz -\\iint q(x\|z) q(z)\\log D(x, z) dxdz\\end{equation}$$
这就是我们导出的ALI的生成器和编码器的loss，它跟标准的ALI结果有所不同。标准的ALI（包括普通的GAN）将其视为一个极大极小问题，所以生成器和编码器的loss为
$$\\begin{equation}\\label{eq:our-ali-g-o1}\\iint p(z\|x)\\tilde{p}(x)\\log D(x, z) dxdz + \\iint q(x\|z) q(z)\\log (1-D(x, z)) dxdz\\end{equation}$$
或
$$\\begin{equation}\\label{eq:our-ali-g-o2}-\\iint p(z\|x)\\tilde{p}(x)\\log (1-D(x, z)) dxdz -\\iint q(x\|z) q(z)\\log D(x, z) dxdz\\end{equation}$$
它们都不等价于$\\eqref{eq:our-ali-g}$。针对这个差异，事实上笔者也做了实验，结果表明这里的ALI有着和标准的ALI同样的表现，甚至可能稍好一些（可能是我的自我良好的错觉，所以就没有放图了）。这说明，将对抗网络视为一个极大极小问题仅仅是一个直觉行为，并非总应该如此。

## 结论综述 [\#](https://kexue.fm/kexue.fm\#%E7%BB%93%E8%AE%BA%E7%BB%BC%E8%BF%B0)

本文的结果表明了变分推断确实是一个推导和解释生成模型的统一框架，包括VAE和GAN。通过变分推断的新诠释，我们介绍了变分推断是如何达到这个目的的。

当然，本文不是第一篇提出用变分推断研究GAN这个想法的文章。在 [《On Unifying Deep Generative Models》](https://papers.cool/arxiv/1706.00550) 一文中，其作者也试图用变分推断统一VAE和GAN，也得到了一些启发性的结果。但笔者觉得那不够清晰。事实上，我并没有完全读懂这篇文章，我不大确定，这篇文章究竟是将GAN纳入到了变分推断中了，还是将VAE纳入到了GAN中～相对而言，我觉得本文的论述更加清晰、明确一些。

看起来变分推断还有很大的挖掘空间，等待着我们去探索。

_**转载到请包括本文地址：** [https://kexue.fm/archives/5716](https://kexue.fm/archives/5716)_

_**更详细的转载事宜请参考：**_ [《科学空间FAQ》](https://kexue.fm/archives/6508#%E6%96%87%E7%AB%A0%E5%A6%82%E4%BD%95%E8%BD%AC%E8%BD%BD/%E5%BC%95%E7%94%A8)

**如果您还有什么疑惑或建议，欢迎在下方评论区继续讨论。**

**如果您觉得本文还不错，欢迎 [分享](https://kexue.fm/kexue.fm#share)/ [打赏](https://kexue.fm/kexue.fm#pay) 本文。打赏并非要从中获得收益，而是希望知道科学空间获得了多少读者的真心关注。当然，如果你无视它，也不会影响你的阅读。再次表示欢迎和感谢！**

打赏

微信打赏

支付宝打赏

因为网站后台对打赏并无记录，因此欢迎在打赏时候备注留言。你还可以 [**点击这里**](http://mail.qq.com/cgi-bin/qm_share?t=qm_mailme&email=tN7d1drY3drrx8H0xcWa19vZ) 或在下方评论区留言来告知你的建议或需求。

**如果您需要引用本文，请参考：**

苏剑林. (Jul. 18, 2018). 《用变分推断统一理解生成模型（VAE、GAN、AAE、ALI） 》\[Blog post\]. Retrieved from [https://kexue.fm/archives/5716](https://kexue.fm/archives/5716)

@online{kexuefm-5716,
        title={用变分推断统一理解生成模型（VAE、GAN、AAE、ALI）},
        author={苏剑林},
        year={2018},
        month={Jul},
        url={\\url{https://kexue.fm/archives/5716}},
}

分类： [信息时代](https://kexue.fm/category/Big-Data)    标签： [变分](https://kexue.fm/tag/%E5%8F%98%E5%88%86/), [深度学习](https://kexue.fm/tag/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/), [推断](https://kexue.fm/tag/%E6%8E%A8%E6%96%AD/)[124 评论](https://kexue.fm/archives/5716#comments)

< [从SamplePairing到mixup：神奇的正则项](https://kexue.fm/archives/5693) \| [基于GRU和AM-Softmax的句子相似度模型](https://kexue.fm/archives/5743) >

### 你也许还对下面的内容感兴趣

- [为什么需要残差？一个来自DeepNet的视角](https://kexue.fm/archives/8994)
- [多任务学习漫谈（三）：分主次之序](https://kexue.fm/archives/8907)
- [多任务学习漫谈（二）：行梯度之事](https://kexue.fm/archives/8896)
- [多任务学习漫谈（一）：以损失之名](https://kexue.fm/archives/8870)
- [变分自编码器（八）：估计样本概率密度](https://kexue.fm/archives/8791)
- [UniVAE：基于Transformer的单模型、多尺度的VAE模型](https://kexue.fm/archives/8475)
- [变分自编码器（七）：球面上的VAE（vMF-VAE）](https://kexue.fm/archives/8404)
- [也来谈谈RNN的梯度消失/爆炸问题](https://kexue.fm/archives/7888)
- [变分自编码器（六）：从几何视角来理解VAE的尝试](https://kexue.fm/archives/7725)
- [L2正则没有想象那么好？可能是“权重尺度偏移”惹的祸](https://kexue.fm/archives/7681)

[发表你的看法](https://kexue.fm/kexue.fm#comment_form)

1. [«](https://kexue.fm/archives/5716/comment-page-4#comments)
2. [1](https://kexue.fm/archives/5716/comment-page-1#comments)
3. [2](https://kexue.fm/archives/5716/comment-page-2#comments)
4. [3](https://kexue.fm/archives/5716/comment-page-3#comments)
5. [4](https://kexue.fm/archives/5716/comment-page-4#comments)
6. [5](https://kexue.fm/archives/5716/comment-page-5#comments)

observer

March 21st, 2022

想请教下各位大佬，(4)式是怎么推出来的。目前看ELBO是证据下界，那么这个上界是怎么推出来的？谢谢

[回复评论](https://kexue.fm/archives/5716/comment-page-5?replyTo=18737#respond-post-5716)

[苏剑林](https://kexue.fm) 发表于
March 21st, 2022

$(4)$式本身就是推导过程了。第一个等号是恒等的，如果看不懂可以从右往左化简；然后由于$KL(p(z\|x)\\Vert q(z\|x))\\geq 0$，所以不等号是显然成立的。

[回复评论](https://kexue.fm/archives/5716/comment-page-5?replyTo=18744#respond-post-5716)

Dzr

November 24th, 2022

你好，按照公式13的建模方式，可得$q(x) = \\sum\_y q(x,y) = p\_1 \\tilde{p}(x) + p\_0 q(x)$，进而可得$(1-p\_0)q(x) = p\_1 \\tilde{p}(x)$，由于$p\_0+p\_1=1$，这也意味着$q(x)= \\tilde{p}(x)$。这是否说明公式13的建模方式不当？

[回复评论](https://kexue.fm/archives/5716/comment-page-5?replyTo=20456#respond-post-5716)

[苏剑林](https://kexue.fm) 发表于
November 24th, 2022

可是我没说$q(x) = \\sum\_y q(x, y)$呀。

[回复评论](https://kexue.fm/archives/5716/comment-page-5?replyTo=20457#respond-post-5716)

Dzr 发表于
November 24th, 2022

按照公式15，以及“从而$q(x)\\to\\tilde{p}(x)$，完成了生成模型的构建”，似乎是隐含这一点的？

[回复评论](https://kexue.fm/archives/5716/comment-page-5?replyTo=20458#respond-post-5716)

[苏剑林](https://kexue.fm) 发表于
November 30th, 2022

公式$(15)$说的是“一旦成功优化”的前提下，有$p\_1 \\tilde{p}(x) + p\_0 q(x) \\to \\tilde{p}(x)$，从而可以推出$ q(x) \\to \\tilde{p}(x)$，这很明显说的是目标，而不是假设。

[回复评论](https://kexue.fm/archives/5716/comment-page-5?replyTo=20483#respond-post-5716)

hqr

May 24th, 2023

苏神，请问公式(4)中，第一项中$\\iint \\widetilde{p} (x)p(z\|x)\\log \\frac{\\widetilde{p} (x)}{q(x)}dz\\;dx$ 怎么会等于$KL(\\widetilde{p}\|\|q(x))$？其中存在$p(z\|x)$呀。

[回复评论](https://kexue.fm/archives/5716/comment-page-5?replyTo=21747#respond-post-5716)

[苏剑林](https://kexue.fm) 发表于
May 24th, 2023

不是还有对$z$的积分吗？$z$的积分难道不是1吗？

[回复评论](https://kexue.fm/archives/5716/comment-page-5?replyTo=21752#respond-post-5716)

hqr 发表于
June 14th, 2023

感谢，明白了

[回复评论](https://kexue.fm/archives/5716/comment-page-5?replyTo=21976#respond-post-5716)

蛤蛤蛤

July 11th, 2023

苏神，我想问一下关于GAN中狄克拉函数的选择，在哪篇论文里有提到呢，我在GAN最初的论文里没找到呢

[回复评论](https://kexue.fm/archives/5716/comment-page-5?replyTo=22229#respond-post-5716)

[苏剑林](https://kexue.fm) 发表于
July 17th, 2023

确定性变换对应的分布就是狄拉克分布，这个算是狄拉克函数的基本性质吧，应该不需要论文特别提。你如果不了解的话，单独去学习一下狄拉克函数即可。

[回复评论](https://kexue.fm/archives/5716/comment-page-5?replyTo=22252#respond-post-5716)

龙行 发表于
July 19th, 2023

原始GAN是基于博弈论的，DCGAN才提出鉴别器+生成器架构。在这个架构里，生成器可以建模为delta函数，这个是生成器数学模型吧。

[回复评论](https://kexue.fm/archives/5716/comment-page-5?replyTo=22274#respond-post-5716)

蛤啦

July 12th, 2023

苏神，公式（16）上面那段话里，优化对象有G(x),是G(X)还是G(Z)呀

[回复评论](https://kexue.fm/archives/5716/comment-page-5?replyTo=22230#respond-post-5716)

[苏剑林](https://kexue.fm) 发表于
July 17th, 2023

不是写得很清楚了么？固定$G(z)$，优化$p(y\|x)$，就是优化$D(x)$。

[回复评论](https://kexue.fm/archives/5716/comment-page-5?replyTo=22253#respond-post-5716)

龙行

July 19th, 2023

看了苏神的这篇博客，有融汇贯通之感。之前留言的好多问题都迎刃而解啦。感谢苏神输出了这么多优质内容，让我们小白能够快速上手！

[回复评论](https://kexue.fm/archives/5716/comment-page-5?replyTo=22275#respond-post-5716)

龙行 发表于
July 19th, 2023

对了，可能也可以换个角度理解，VAE是将变分（噪声、隐变量）下放到了编码中，图像变换中，而GAN则是将变分上提到了分类中，或者距离判别中。VAE与精确的数学模型更加贴近一些，GAN则与ANN的逼近拟合更加贴近一些。
但是GAN的变分过度上提，造成拟合不够好，数学模型不够精确，生成器就不够好，所以需要修正项。这里苏神直接用VAE的原理逼近了GAN修正项的数学表达式，加入损失中，得到了很好的效果。
那能不能调节建模的精确程度呢？将变分再往下走，或者再往上走？类似与元学习一样，得到元生成模型？感觉很有意思。

[回复评论](https://kexue.fm/archives/5716/comment-page-5?replyTo=22276#respond-post-5716)

龙行 发表于
July 19th, 2023

最后苏神说并不总应该视为极大极小值问题，因为这个是直觉上的优化，是有瑕疵的，是没啥道理可讲的。从公式上讲，应该是一个EM算法解决的问题，因此通过EM算法导出的损失才是有道理的。

[回复评论](https://kexue.fm/archives/5716/comment-page-5?replyTo=22277#respond-post-5716)

[苏剑林](https://kexue.fm) 发表于
July 20th, 2023

很惭愧，后面没做下去了。从事后来看，当时的我也没能力继续做下去。

[回复评论](https://kexue.fm/archives/5716/comment-page-5?replyTo=22298#respond-post-5716)

资料的小田

October 21st, 2023

1、为什么狄拉克函数能够描述GAN等生成模型的生成过程？感觉只是在开头引入了狄拉克函数但没有说明为什么？（比如哪些性质符合等）

[回复评论](https://kexue.fm/archives/5716/comment-page-5?replyTo=22917#respond-post-5716)

[苏剑林](https://kexue.fm) 发表于
October 21st, 2023

这是狄拉克函数的定义？

[回复评论](https://kexue.fm/archives/5716/comment-page-5?replyTo=22930#respond-post-5716)

资料的小田

October 21st, 2023

2、用狄拉克函数和传统的通过高斯分布解释的生成有什么关联？这两个问题比较基础抽象，求苏神解答！

[回复评论](https://kexue.fm/archives/5716/comment-page-5?replyTo=22918#respond-post-5716)

[苏剑林](https://kexue.fm) 发表于
October 21st, 2023

“传统”什么时候用高斯分布来解释GAN了？有什么参考文献吗？

[回复评论](https://kexue.fm/archives/5716/comment-page-5?replyTo=22931#respond-post-5716)

1. [«](https://kexue.fm/archives/5716/comment-page-4#comments)
2. [1](https://kexue.fm/archives/5716/comment-page-1#comments)
3. [2](https://kexue.fm/archives/5716/comment-page-2#comments)
4. [3](https://kexue.fm/archives/5716/comment-page-3#comments)
5. [4](https://kexue.fm/archives/5716/comment-page-4#comments)
6. [5](https://kexue.fm/archives/5716/comment-page-5#comments)

[取消回复](https://kexue.fm/archives/5716#respond-post-5716)

你的大名

电子邮箱

个人网站（选填）

1\. 可以使用LaTeX代码，点击“预览效果”可查看效果；2. 可以通过点击评论楼层编号来引用该楼层；3. 网站可能会有点卡，如非确认评论失败，请 **不要重复点击提交**。

### 内容速览

[变分推断新解](https://kexue.fm/kexue.fm#%E5%8F%98%E5%88%86%E6%8E%A8%E6%96%AD%E6%96%B0%E8%A7%A3)
[VAE和EM算法](https://kexue.fm/kexue.fm#VAE%E5%92%8CEM%E7%AE%97%E6%B3%95)
[VAE](https://kexue.fm/kexue.fm#VAE)
[EM算法](https://kexue.fm/kexue.fm#EM%E7%AE%97%E6%B3%95)
[变分推断下的GAN](https://kexue.fm/kexue.fm#%E5%8F%98%E5%88%86%E6%8E%A8%E6%96%AD%E4%B8%8B%E7%9A%84GAN)
[一般框架](https://kexue.fm/kexue.fm#%E4%B8%80%E8%88%AC%E6%A1%86%E6%9E%B6)
[基本分析](https://kexue.fm/kexue.fm#%E5%9F%BA%E6%9C%AC%E5%88%86%E6%9E%90)
[正则项](https://kexue.fm/kexue.fm#%E6%AD%A3%E5%88%99%E9%A1%B9)
[GAN相关模型](https://kexue.fm/kexue.fm#GAN%E7%9B%B8%E5%85%B3%E6%A8%A1%E5%9E%8B)
[GAN视角下的AAE](https://kexue.fm/kexue.fm#GAN%E8%A7%86%E8%A7%92%E4%B8%8B%E7%9A%84AAE)
[反直觉的ALI版本](https://kexue.fm/kexue.fm#%E5%8F%8D%E7%9B%B4%E8%A7%89%E7%9A%84ALI%E7%89%88%E6%9C%AC)
[结论综述](https://kexue.fm/kexue.fm#%E7%BB%93%E8%AE%BA%E7%BB%BC%E8%BF%B0)

### 智能搜索

支持整句搜索！网站自动使用 [结巴分词](https://github.com/fxsjy/jieba) 进行分词，并结合ngrams排序算法给出合理的搜索结果。

### 热门标签

[生成模型](https://kexue.fm/tag/%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B/) [attention](https://kexue.fm/tag/attention/) [优化](https://kexue.fm/tag/%E4%BC%98%E5%8C%96/) [语言模型](https://kexue.fm/tag/%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/) [模型](https://kexue.fm/tag/%E6%A8%A1%E5%9E%8B/) [网站](https://kexue.fm/tag/%E7%BD%91%E7%AB%99/) [概率](https://kexue.fm/tag/%E6%A6%82%E7%8E%87/) [梯度](https://kexue.fm/tag/%E6%A2%AF%E5%BA%A6/) [矩阵](https://kexue.fm/tag/%E7%9F%A9%E9%98%B5/) [转载](https://kexue.fm/tag/%E8%BD%AC%E8%BD%BD/) [优化器](https://kexue.fm/tag/%E4%BC%98%E5%8C%96%E5%99%A8/) [微分方程](https://kexue.fm/tag/%E5%BE%AE%E5%88%86%E6%96%B9%E7%A8%8B/) [分析](https://kexue.fm/tag/%E5%88%86%E6%9E%90/) [天象](https://kexue.fm/tag/%E5%A4%A9%E8%B1%A1/) [深度学习](https://kexue.fm/tag/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/) [积分](https://kexue.fm/tag/%E7%A7%AF%E5%88%86/) [python](https://kexue.fm/tag/python/) [力学](https://kexue.fm/tag/%E5%8A%9B%E5%AD%A6/) [无监督](https://kexue.fm/tag/%E6%97%A0%E7%9B%91%E7%9D%A3/) [扩散](https://kexue.fm/tag/%E6%89%A9%E6%95%A3/) [几何](https://kexue.fm/tag/%E5%87%A0%E4%BD%95/) [节日](https://kexue.fm/tag/%E8%8A%82%E6%97%A5/) [生活](https://kexue.fm/tag/%E7%94%9F%E6%B4%BB/) [文本生成](https://kexue.fm/tag/%E6%96%87%E6%9C%AC%E7%94%9F%E6%88%90/) [数论](https://kexue.fm/tag/%E6%95%B0%E8%AE%BA/)

### 随机文章

- [重温SSM（二）：HiPPO的一些遗留问题](https://kexue.fm/archives/10137)
- [高斯型积分的微扰展开（三）](https://kexue.fm/archives/3280)
- [从动力学角度看优化算法（五）：为什么学习率不宜过小？](https://kexue.fm/archives/7787)
- [均匀球状星团内恒星的运动](https://kexue.fm/archives/1394)
- [更换了一个相册程序](https://kexue.fm/archives/1622)
- [费曼路径积分思想的发展(四)](https://kexue.fm/archives/1850)
- [ReLU/GeLU/Swish的一个恒等式](https://kexue.fm/archives/11233)
- [科学空间：2011年1月重要天象](https://kexue.fm/archives/1148)
- [从Wasserstein距离、对偶理论到WGAN](https://kexue.fm/archives/6280)
- [Ladder Side-Tuning：预训练模型的“过墙梯”](https://kexue.fm/archives/9138)

### 最近评论

- [QiYang](https://kexue.fm/archives/10592/comment-page-2#comment-28661): (27) 式求解出了 \\Phi 之后，(22) 式里的 \\Delta W 不应该是 \\gamm...
- [论文机器](https://kexue.fm/archives/11241/comment-page-1#comment-28660): 苏老师，有论文通过实验验证过Muon下使用MuP能否迁移超参数吗，以及是否可以按照您RMS对齐...
- [CuddleSabe](https://kexue.fm/archives/7546/comment-page-4#comment-28659): 并且 softmax(KV) 也是可以让秩提升的， QK 和 KV 的秩都是
- [CuddleSabe](https://kexue.fm/archives/7546/comment-page-4#comment-28658): 大佬，如果在 self-attention 中使用 linear-attentio 的话， 是...
- [大力](https://kexue.fm/archives/4598/comment-page-1#comment-28656): 你好，苏老师，请问这些式子中的~w都是近似看做广义迁移核吗？如果是，为什么这样
- [小熊](https://kexue.fm/archives/443/comment-page-1#comment-28655): 苏老师，可以求个图集吗？邮箱 492860400@qq.com 非常感谢！！
- [且寻](https://kexue.fm/archives/10815/comment-page-1#comment-28654): 嗷，理解苏老师您的意思了，不加0均值约束就是公式$\\eqref{eq:aux-loss-fre...
- [且寻](https://kexue.fm/archives/10815/comment-page-1#comment-28653): 公式$\\eqref{eq:aux-loss-free-3}$。
- [pang](https://kexue.fm/archives/10862/comment-page-1#comment-28652): 您好，关于式子7那里提到的第二类旋转位置编码只需要再给Output加一次O-RoPE就行了，有...
- [苏剑林](https://kexue.fm/archives/10815/comment-page-1#comment-28651): 你是说公式$\\eqref{eq:aux-loss-free-5}$？

### 友情链接

- [Cool Papers](https://papers.cool)
- [数学研发](https://bbs.emath.ac.cn)
- [Seatop](http://www.seatop.com.cn/)
- [Xiaoxia](https://xiaoxia.org/)
- [积分表-网络版](https://kexue.fm/sci/integral/index.html)
- [丝路博傲](http://blog.dvxj.com/)
- [数学之家](http://www.2math.cn/)
- [有趣天文奇观](http://interesting-sky.china-vo.org/)
- [TwistedW](http://www.twistedwg.com/)
- [godweiyang](https://godweiyang.com/)
- [AI柠檬](https://blog.ailemon.net/)
- [王登科-DK博客](https://greatdk.com)
- [ESON](https://blog.eson.org/)
- [枫之羽](https://fzhiy.net/)
- [Mathor's blog](https://wmathor.com/)
- [coding-zuo](https://coding-zuo.github.io/)
- [博科园](https://www.bokeyuan.net/)
- [孔皮皮的博客](https://www.kppkkp.top/)
- [运鹏的博客](https://yunpengtai.top/)
- [jiming.site](https://jiming.site/)
- [OmegaXYZ](https://www.omegaxyz.com/)
- [EAI猩球](https://www.robotech.ink/)
- [文举的博客](https://liwenju0.com/)
- [用代码打点酱油](https://bruceyuan.com/)
- [Zhang's blog](https://armcvai.cn/)
- [申请链接](https://kexue.fm/links.html)

本站采用创作共用版权协议，要求署名、非商业用途和保持一致。转载本站内容必须也遵循“ [署名-非商业用途-保持一致](http://creativecommons.org/licenses/by-nc-nd/2.5/cn/)”的创作共用协议。
© 2009-2025 Scientific Spaces. All rights reserved. Theme by [laogui](http://www.laogui.com). Powered by [Typecho](http://typecho.org). 备案号: [粤ICP备09093259号-1/2](https://beian.miit.gov.cn/)。