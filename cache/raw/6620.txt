## SEARCH

## MENU

- [打赏](https://kexue.fm/reward.html)
- [公式](https://kexue.fm/latex.html)
- [天象](https://kexue.fm/ac.html)
- [链接](https://kexue.fm/links.html)
- [时光](https://kexue.fm/me.html)
- [博览](https://kexue.fm/science.html)
- [归档](https://kexue.fm/content.html)

## CATEGORIES

- [千奇百怪](https://kexue.fm/category/Everything)
- [天文探索](https://kexue.fm/category/Astronomy)
- [数学研究](https://kexue.fm/category/Mathematics)
- [物理化学](https://kexue.fm/category/Phy-chem)
- [信息时代](https://kexue.fm/category/Big-Data)
- [生物自然](https://kexue.fm/category/Biology)
- [图片摄影](https://kexue.fm/category/Photograph)
- [问题百科](https://kexue.fm/category/Questions)
- [生活/情感](https://kexue.fm/category/Life-Feeling)
- [资源共享](https://kexue.fm/category/Resources)

## NEWPOSTS

- [msign的导数](https://kexue.fm/archives/11025)
- [通过msign来计算mclip（奇...](https://kexue.fm/archives/11006)
- [msign算子的Newton-Sc...](https://kexue.fm/archives/10996)
- [等值振荡定理：最优多项式逼近的充要条件](https://kexue.fm/archives/10972)
- [生成扩散模型漫谈（三十）：从瞬时速...](https://kexue.fm/archives/10958)
- [MoE环游记：5、均匀分布的反思](https://kexue.fm/archives/10945)
- [msign算子的Newton-Sc...](https://kexue.fm/archives/10922)
- [Transformer升级之路：2...](https://kexue.fm/archives/10907)
- [一道概率不等式：盯着它到显然成立为止！](https://kexue.fm/archives/10902)
- [SVD的导数](https://kexue.fm/archives/10878)

## COMMENTS

- [1: 1\[comment=14986\]Daniel Cheng\[/c...](https://kexue.fm/reward.html/comment-page-1#comment-27905)
- [1: 1\[comment=14986\]Daniel Cheng\[/c...](https://kexue.fm/reward.html/comment-page-1#comment-27904)
- [1: 1\[comment=25585\]Evan-wyl\[/comme...](https://kexue.fm/links.html/comment-page-6#comment-27903)
- [1: 1\[comment=25585\]Evan-wyl\[/comme...](https://kexue.fm/links.html/comment-page-6#comment-27902)
- [tll1945tll1937: 真心实意的向大家请教问题：看了文章“对齐全量微调！这是我看过最...](https://kexue.fm/archives/10266/comment-page-1#comment-27901)
- [oYo\_logan: \[comment=27017\]苏剑林\[/comment\]苏神，...](https://kexue.fm/archives/10757/comment-page-1#comment-27897)
- [z123: 在参数矩阵较多的CNN小模型上，Muon会明显慢于Adam，这...](https://kexue.fm/archives/10592/comment-page-1#comment-27896)
- [dry: 苏神好，一直有个疑问，ReFlow构建的ODE是$dx\_t/d...](https://kexue.fm/archives/10958/comment-page-2#comment-27895)
- [tyj: 感觉和之前的一篇文章很像，应该算是concurrent wor...](https://kexue.fm/archives/10958/comment-page-2#comment-27894)
- [li6626: 苏老师，Normalizing Flow有了新进展，论文链接:...](https://kexue.fm/archives/10667/comment-page-1#comment-27893)

## USERLOGIN

- [登录](https://kexue.fm/admin/login.php)

[科学空间\|Scientific Spaces](https://kexue.fm)

- [登录](https://kexue.fm/admin/login.php)
- [打赏](https://kexue.fm/reward.html)
- [公式](https://kexue.fm/latex.html)
- [天象](https://kexue.fm/ac.html)
- [链接](https://kexue.fm/links.html)
- [时光](https://kexue.fm/me.html)
- [博览](https://kexue.fm/science.html)
- [归档](https://kexue.fm/content.html)

渴望成为一个小飞侠

- [欢迎订阅](https://kexue.fm/feed)
- [个性邮箱](https://kexue.fm/archives/119)
- [天象信息](https://kexue.fm/ac.html)
- [观测ISS](https://kexue.fm/archives/41)
- [LaTeX](https://kexue.fm/latex.html)
- [关于博主](https://kexue.fm/me.html)

欢迎访问“科学空间”，这里将与您共同探讨自然科学，回味人生百态；也期待大家的分享～

- [**千奇百怪** Everything](https://kexue.fm/category/Everything)
- [**天文探索** Astronomy](https://kexue.fm/category/Astronomy)
- [**数学研究** Mathematics](https://kexue.fm/category/Mathematics)
- [**物理化学** Phy-chem](https://kexue.fm/category/Phy-chem)
- [**信息时代** Big-Data](https://kexue.fm/category/Big-Data)
- [**生物自然** Biology](https://kexue.fm/category/Biology)
- [**图片摄影** Photograph](https://kexue.fm/category/Photograph)
- [**问题百科** Questions](https://kexue.fm/category/Questions)
- [**生活/情感** Life-Feeling](https://kexue.fm/category/Life-Feeling)
- [**资源共享** Resources](https://kexue.fm/category/Resources)

- [**千奇百怪**](https://kexue.fm/category/Everything)
- [**天文探索**](https://kexue.fm/category/Astronomy)
- [**数学研究**](https://kexue.fm/category/Mathematics)
- [**物理化学**](https://kexue.fm/category/Phy-chem)
- [**信息时代**](https://kexue.fm/category/Big-Data)
- [**生物自然**](https://kexue.fm/category/Biology)
- [**图片摄影**](https://kexue.fm/category/Photograph)
- [**问题百科**](https://kexue.fm/category/Questions)
- [**生活/情感**](https://kexue.fm/category/Life-Feeling)
- [**资源共享**](https://kexue.fm/category/Resources)

[首页](https://kexue.fm) [数学研究](https://kexue.fm/category/Mathematics) 函数光滑化杂谈：不可导函数的可导逼近

20May

# [函数光滑化杂谈：不可导函数的可导逼近](https://kexue.fm/archives/6620)

By 苏剑林 \|
2019-05-20 \|
144950位读者\|

一般来说，神经网络处理的东西都是连续的浮点数，标准的输出也是连续型的数字。但实际问题中，我们很多时候都需要一个离散的结果，比如分类问题中我们希望输出正确的类别，“类别”是离散的，“类别的概率”才是连续的；又比如我们很多任务的评测指标实际上都是离散的，比如分类问题的正确率和F1、机器翻译中的BLEU，等等。

还是以分类问题为例，常见的评测指标是正确率，而常见的损失函数是交叉熵。交叉熵的降低与正确率的提升确实会有一定的关联，但它们不是绝对的单调相关关系。换句话说，交叉熵下降了，正确率不一定上升。显然，如果能用正确率的相反数做损失函数，那是最理想的，但正确率是不可导的（涉及到$\\text{argmax}$等操作），所以没法直接用。

这时候一般有两种解决方案；一是动用强化学习，将正确率设为奖励函数，这是“用牛刀杀鸡”的方案；另外一种是试图给正确率找一个光滑可导的近似公式。本文就来探讨一下常见的不可导函数的光滑近似，有时候我们称之为“光滑化”，有时候我们也称之为“软化”。

## max [\#](https://kexue.fm/archives/6620\#max)

后面谈到的大部分内容，基础点就是$\\max$操作的光滑近似，我们有：
\\begin{equation}\\max(x\_1,x\_2,\\dots,x\_n) = \\lim\_{K\\to +\\infty}\\frac{1}{K}\\log\\left(\\sum\_{i=1}^n e^{K x\_i}\\right)\\end{equation}
所以选定常数$K$，我们就有近似：
\\begin{equation}\\max(x\_1,x\_2,\\dots,x\_n) \\approx \\frac{1}{K}\\log\\left(\\sum\_{i=1}^n e^{K x\_i}\\right)\\end{equation}
在模型中，很多时候可以设$K=1$，这等价于把$K$融合到模型自身之中，所以最简单地有：
\\begin{equation}\\begin{aligned}\\max(x\_1,x\_2,\\dots,x\_n) \\approx&\\, \\log\\left(\\sum\_{i=1}^n e^{x\_i}\\right) \\\
\\triangleq&\\, \\text{logsumexp}(x\_1, x\_2, \\dots, x\_n)\\end{aligned}\\label{eq:max-approx}\\end{equation}

这里出现了$\\text{logsumexp}$，这是一个很常见的算子，在这里它是$\\max$函数的光滑近似。没错，$\\max$函数的光滑近似其实是$\\text{logsumexp}$，而不是字面上很像的$\\text{softmax}$。相关推导还可以参考我之前写的 [《寻求一个光滑的最大值函数》](https://kexue.fm/archives/3290)。

## softmax [\#](https://kexue.fm/archives/6620\#softmax)

刚才说$\\text{softmax}$不是$\\max$的光滑近似，那它是谁的光滑近似呢？它事实上是$\\text{onehot}(\\text{argmax}(\\boldsymbol{x}))$的光滑近似，即先求出最大值所在的位置，然后生成一个等长的向量，最大值那一位置1，其它位置都置0，比如：
\\begin{equation}\[2, 1, 4, 5, 3\]\\quad \\to \\quad \[0, 0, 0, 1, 0\]\\end{equation}
我们可以简单地给出一个从$\\text{logsumexp}$到$\\text{softmax}$的推导过程。考虑向量$\\boldsymbol{x}=\[x\_1, x\_2, \\dots, x\_n\]$，然后考虑
\\begin{equation}\\boldsymbol{x}'=\[x\_1, x\_2, \\dots, x\_n\] - \\max(x\_1, x\_2, \\dots, x\_n)\\end{equation}
即每一位都减去整体的最大值，这样的新向量与原向量最大值所在的位置是一样的，即$\\text{onehot}(\\text{argmax}(\\boldsymbol{x}))=\\text{onehot}(\\text{argmax}(\\boldsymbol{x}'))$。不失一般性，考虑$x\_1,x\_2,\\dots,x\_n$两两不相等的情况，那么新向量的最大值显然为0，并且除去最大值外，其余各位都是负数。这样一来，我们可以考虑
\\begin{equation}e^{\\boldsymbol{x}'}=\[e^{x\_1 - \\max(x\_1, x\_2, \\dots, x\_n)}, e^{x\_2 - \\max(x\_1, x\_2, \\dots, x\_n)}, \\dots, e^{x\_n - \\max(x\_1, x\_2, \\dots, x\_n)}\]\\end{equation}
作为$\\text{onehot}(\\text{argmax}(\\boldsymbol{x}'))$的近似，因为最大值为0，所以对应的位置是$e^0=1$，而其余为负，取指数后会比较接近于0。

最后将近似$\\eqref{eq:max-approx}$代入上式，化简，就可以得到
\\begin{equation}\\begin{aligned}\\text{onehot}(\\text{argmax}(\\boldsymbol{x}))=&\\,\\text{onehot}(\\text{argmax}(\\boldsymbol{x}'))\\\
\\approx&\\, \\left(\\frac{e^{x\_1}}{\\sum\\limits\_{i=1}^n e^{x\_i}}, \\frac{e^{x\_2}}{\\sum\\limits\_{i=1}^n e^{x\_i}}, \\dots, \\frac{e^{x\_n}}{\\sum\\limits\_{i=1}^n e^{x\_i}}\\right)\\\
\\triangleq&\\,\\text{softmax}(x\_1, x\_2, \\dots, x\_n)
\\end{aligned}\\end{equation}

## argmax [\#](https://kexue.fm/archives/6620\#argmax)

$\\text{argmax}$是指直接给出向量最大值所在的下标（一个整数），比如
\\begin{equation}\[2, 1, 4, 5, 3\]\\quad \\to \\quad 4\\end{equation}
这里我们遵循平时的使用习惯，下标是从1开始的，所以返回结果是4；但在编程语言中一般是从0开始的，所以在编程语言中返回结果一般是3。

如果是$\\text{argmax}$的光滑近似，自然是希望输出一个接近4的浮点数。为了构造这样的近似，我们先观察到$\\text{argmax}$实际上等于
\\begin{equation}\\text{sum}\\Big(\\underbrace{\[1, 2, 3, 4, 5\]}\_{\\text{序向量\[1, 2, ..., n\]}}\\,\\, \\otimes\\,\\,\\underbrace{\[0, 0, 0, 1, 0\]}\_{\\text{onehot}(\\text{argmax}(\\boldsymbol{x}))}\\Big)\\end{equation}
即数组$\[1, 2, \\dots, n\]$与$\\text{onehot}(\\text{argmax}(\\boldsymbol{x}))$的内积。那构造$\\text{argmax}$的软化版就简单了，将$\\text{onehot}(\\text{argmax}(\\boldsymbol{x}))$换成$\\text{softmax}(\\boldsymbol{x})$就行了，即
\\begin{equation}\\text{argmax} (\\boldsymbol{x}) \\approx \\sum\_{i=1}^n i\\times \\text{softmax}(\\boldsymbol{x})\_i\\end{equation}

## 正确率 [\#](https://kexue.fm/archives/6620\#%E6%AD%A3%E7%A1%AE%E7%8E%87)

上述讨论的若干个近似，基本都是在one hot向量的基础上推导出正确形式，然后用softmax近似one hot，从而得到光滑近似。用这种思想，还可以导出很多算子的光滑近似，比如正确率。

简单起见，引入记号$\\boldsymbol{1}\_k$，它表示第$k$位为1的one hot向量。假设在分类问题中，目标类别是$i$，预测类别是$j$，那么可以考虑one hot向量$\\boldsymbol{1}\_i$和$\\boldsymbol{1}\_j$，然后考虑内积
\\begin{equation}\\langle \\boldsymbol{1}\_i, \\boldsymbol{1}\_j\\rangle = \\left\\{\\begin{aligned}&1,\\,\\,(i=j)\\\ &0,\\,\\,(i\\neq j)\\end{aligned}\\right.\\end{equation}
也就是说两个类别一样时，内积刚好为1，而两个类别不一样时，内积刚好为0，所以目标类别和预测类别对应的one hot向量的内积，刚好定义了一个“预测正确”的计数函数，而有了计数函数，就可以计算正确率：
\\begin{equation}\\text{正确率}=\\frac{1}{\|\\mathcal{B}\|}\\sum\_{\\boldsymbol{x}\\in\\mathcal{B}}\\langle \\boldsymbol{1}\_i(\\boldsymbol{x}), \\boldsymbol{1}\_j(\\boldsymbol{x})\\rangle\\end{equation}
其中$\\mathcal{B}$表示当前batch，上式正是计算一个batch内正确率的函数。而在神经网络中，为了保证可导性，最后的输出只能是一个概率分布（softmax后的结果），所以正确率的光滑近似，就是将预测类别的one hot向量，换成概率分布：
\\begin{equation}\\text{正确率}\\approx \\frac{1}{\|\\mathcal{B}\|}\\sum\_{\\boldsymbol{x}\\in\\mathcal{B}}\\langle \\boldsymbol{1}\_i(\\boldsymbol{x}), p(\\boldsymbol{x})\\rangle\\end{equation}
类似地，可以导出recall、f1等指标的光滑近似。以二分类为例，假设$p(\\boldsymbol{x})$是正类的概率，$t(\\boldsymbol{x})$是样本$\\boldsymbol{x}$的标签（0或1），则正类的f1的光滑近似是：
\\begin{equation}\\text{正类F1}\\approx\\frac{2 \\sum\\limits\_{\\boldsymbol{x}\\in\\mathcal{B}}t(\\boldsymbol{x}) p(\\boldsymbol{x})}{\\sum\\limits\_{\\boldsymbol{x}\\in\\mathcal{B}}\\big\[t(\\boldsymbol{x}) + p(\\boldsymbol{x})\\big\]}\\end{equation}

这样导出来的正确率近似公式是可导的，可以直接将它的相反数作为loss。但事实上在采样估计过程中，它是f1的有偏估计（分母也有对batch的求和），有时候会影响优化轨迹甚至导致发散，所以一般情况下最好不要直接用，而是先用普通的交叉熵训练到差不多了，然后再用f1的相反数作为loss来微调。

## softkmax [\#](https://kexue.fm/archives/6620\#softkmax)

$\\text{softmax}$是“最大值那一位置1、其余置0”的光滑近似，那如果是“最大的$k$个值的位置都置1、其余置0”的光滑近似呢？我们可以称之为$\\text{soft-}k\\text{-max}$。

我没有构造出$\\text{soft-}k\\text{-max}$的简单形式，但可以利用递归地构造出来：

> 输入为$\\boldsymbol{x}$，初始化$\\boldsymbol{p}^{(0)}$为全0向量；
> 执行$\\boldsymbol{x} = \\boldsymbol{x} - \\min(\\boldsymbol{x})$（保证所有元素非负）
>
> 对于$i=1,2,\\dots,k$，执行：
>      $\\boldsymbol{y} = (1 - \\boldsymbol{p}^{(i-1)})\\otimes\\boldsymbol{x}$;
>      $\\boldsymbol{p}^{(i)} = \\boldsymbol{p}^{(i-1)} + \\text{softmax}(\\boldsymbol{y})$
>
> 返回$\\boldsymbol{p}^{(k)}$。

至于原理，将$\\text{softmax}(\\boldsymbol{y})$换成$\\text{onehot}(\\text{argmax}(\\boldsymbol{y}))$然后递归下去就明白了，其实就是先$\\max$，然后把将$\\max$那一位减掉，这样次最大值就变成了最大值，然后重新$\\text{softmax}$，递归下去即可。

## 总结 [\#](https://kexue.fm/archives/6620\#%E6%80%BB%E7%BB%93)

函数光滑化是一个比较有意思的数学内容，在机器学习中也经常出现。一方面，它是处理将某些操作可导化的技巧，使得模型可以直接使用反向传播求解，而不需要“动用”强化学习；另一方面，在某些情况下它也能增强模型的解释性，因为往往对应的不可导函数解释性是比较好的，将光滑化版本代入训练完成后，也许可能恢复为不可导版本来解释模型的输出结果。

当然，作为纯粹的数学之美来欣赏，也是相当赏心悦目的～

_**转载到请包括本文地址：** [https://kexue.fm/archives/6620](https://kexue.fm/archives/6620)_

_**更详细的转载事宜请参考：**_ [《科学空间FAQ》](https://kexue.fm/archives/6508#%E6%96%87%E7%AB%A0%E5%A6%82%E4%BD%95%E8%BD%AC%E8%BD%BD/%E5%BC%95%E7%94%A8)

**如果您还有什么疑惑或建议，欢迎在下方评论区继续讨论。**

**如果您觉得本文还不错，欢迎 [分享](https://kexue.fm/archives/6620#share)/ [打赏](https://kexue.fm/archives/6620#pay) 本文。打赏并非要从中获得收益，而是希望知道科学空间获得了多少读者的真心关注。当然，如果你无视它，也不会影响你的阅读。再次表示欢迎和感谢！**

打赏

微信打赏

支付宝打赏

因为网站后台对打赏并无记录，因此欢迎在打赏时候备注留言。你还可以 [**点击这里**](http://mail.qq.com/cgi-bin/qm_share?t=qm_mailme&email=tN7d1drY3drrx8H0xcWa19vZ) 或在下方评论区留言来告知你的建议或需求。

**如果您需要引用本文，请参考：**

苏剑林. (May. 20, 2019). 《函数光滑化杂谈：不可导函数的可导逼近 》\[Blog post\]. Retrieved from [https://kexue.fm/archives/6620](https://kexue.fm/archives/6620)

@online{kexuefm-6620,
        title={函数光滑化杂谈：不可导函数的可导逼近},
        author={苏剑林},
        year={2019},
        month={May},
        url={\\url{https://kexue.fm/archives/6620}},
}

分类： [数学研究](https://kexue.fm/category/Mathematics)    标签： [导数](https://kexue.fm/tag/%E5%AF%BC%E6%95%B0/), [函数](https://kexue.fm/tag/%E5%87%BD%E6%95%B0/), [近似](https://kexue.fm/tag/%E8%BF%91%E4%BC%BC/), [光滑](https://kexue.fm/tag/%E5%85%89%E6%BB%91/)[42 评论](https://kexue.fm/archives/6620#comments)

< [能量视角下的GAN模型（三）：生成模型=能量模型](https://kexue.fm/archives/6612) \| [ON-LSTM：用有序神经元表达层次结构](https://kexue.fm/archives/6621) >

### 你也许还对下面的内容感兴趣

- [通过msign来计算mclip（奇异值裁剪）](https://kexue.fm/archives/11006)
- [msign算子的Newton-Schulz迭代（下）](https://kexue.fm/archives/10996)
- [等值振荡定理：最优多项式逼近的充要条件](https://kexue.fm/archives/10972)
- [msign算子的Newton-Schulz迭代（上）](https://kexue.fm/archives/10922)
- [通过梯度近似寻找Normalization的替代品](https://kexue.fm/archives/10831)
- [低秩近似之路（五）：CUR](https://kexue.fm/archives/10662)
- [低秩近似之路（四）：ID](https://kexue.fm/archives/10501)
- [低秩近似之路（三）：CR](https://kexue.fm/archives/10427)
- [低秩近似之路（二）：SVD](https://kexue.fm/archives/10407)
- [Softmax后传：寻找Top-K的光滑近似](https://kexue.fm/archives/10373)

[发表你的看法](https://kexue.fm/archives/6620#comment_form)

1. [«](https://kexue.fm/archives/6620/comment-page-1#comments)
2. [1](https://kexue.fm/archives/6620/comment-page-1#comments)
3. [2](https://kexue.fm/archives/6620/comment-page-2#comments)

不知道

April 23rd, 2022

Auc的光滑近似要怎么推导

[回复评论](https://kexue.fm/archives/6620/comment-page-2?replyTo=19029#respond-post-6620)

[苏剑林](https://kexue.fm) 发表于
April 25th, 2022

这个没想过，对AUC不大熟悉。

[回复评论](https://kexue.fm/archives/6620/comment-page-2?replyTo=19038#respond-post-6620)

tsotfsk 发表于
April 28th, 2024

AUC的光滑近似就是pairwise的loss。
《MBA: Mini-Batch AUC Optimization》

[回复评论](https://kexue.fm/archives/6620/comment-page-2?replyTo=24214#respond-post-6620)

[苏剑林](https://kexue.fm) 发表于
May 1st, 2024

谢谢，感谢告知。主要是对我自己来说AUC这个指标几乎没用过，所以很少关注到相关内容。

[回复评论](https://kexue.fm/archives/6620/comment-page-2?replyTo=24235#respond-post-6620)

[为什么max 的光滑近似是logsumexp？ R11; aigonna](https://tt.aigonna.com/2023/11/26/%e4%b8%ba%e4%bb%80%e4%b9%88max-%e7%9a%84%e5%85%89%e6%bb%91%e8%bf%91%e4%bc%bc%e6%98%aflogsumexp%ef%bc%9f/)

November 26th, 2023

\[...\]\[1\] 函数光滑化杂谈：不可导函数的可导逼近\[...\]

[回复评论](https://kexue.fm/archives/6620/comment-page-2?replyTo=23174#respond-post-6620)

RanX

September 13th, 2024

有什么更好的方式能光滑进行$k\\max$么？虽然循环去用$\\text{softmax}$可以达到这个目标。但计算效率低，也不容易分析。如果有一个函数解析式可以达到这个目标就更好了。

[回复评论](https://kexue.fm/archives/6620/comment-page-2?replyTo=25228#respond-post-6620)

[苏剑林](https://kexue.fm) 发表于
September 20th, 2024

来了： [https://kexue.fm/archives/10373](https://kexue.fm/archives/10373)

[回复评论](https://kexue.fm/archives/6620/comment-page-2?replyTo=25252#respond-post-6620)

flyer

December 22nd, 2024

soft-k-max中$x - \\min(x)$这一步可以不要吗？感觉不要也可以

[回复评论](https://kexue.fm/archives/6620/comment-page-2?replyTo=26062#respond-post-6620)

[苏剑林](https://kexue.fm) 发表于
December 26th, 2024

不可以，这一步是用来保证$\\boldsymbol{x}$非负的。这里的思想是通过置零让原本的最大值变为最小值，如果$\\boldsymbol{x}$有负数分量，那么0就不是最小值了。

[回复评论](https://kexue.fm/archives/6620/comment-page-2?replyTo=26093#respond-post-6620)

1. [«](https://kexue.fm/archives/6620/comment-page-1#comments)
2. [1](https://kexue.fm/archives/6620/comment-page-1#comments)
3. [2](https://kexue.fm/archives/6620/comment-page-2#comments)

[取消回复](https://kexue.fm/archives/6620#respond-post-6620)

你的大名

电子邮箱

个人网站（选填）

1\. 可以使用LaTeX代码，点击“预览效果”可查看效果；2. 可以通过点击评论楼层编号来引用该楼层；3. 网站可能会有点卡，如非确认评论失败，请 **不要重复点击提交**。

### 内容速览

[max](https://kexue.fm/archives/6620#max)
[softmax](https://kexue.fm/archives/6620#softmax)
[argmax](https://kexue.fm/archives/6620#argmax)
[正确率](https://kexue.fm/archives/6620#%E6%AD%A3%E7%A1%AE%E7%8E%87)
[softkmax](https://kexue.fm/archives/6620#softkmax)
[总结](https://kexue.fm/archives/6620#%E6%80%BB%E7%BB%93)

### 智能搜索

支持整句搜索！网站自动使用 [结巴分词](https://github.com/fxsjy/jieba) 进行分词，并结合ngrams排序算法给出合理的搜索结果。

### 热门标签

[生成模型](https://kexue.fm/tag/%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B/) [attention](https://kexue.fm/tag/attention/) [优化](https://kexue.fm/tag/%E4%BC%98%E5%8C%96/) [语言模型](https://kexue.fm/tag/%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/) [模型](https://kexue.fm/tag/%E6%A8%A1%E5%9E%8B/) [网站](https://kexue.fm/tag/%E7%BD%91%E7%AB%99/) [概率](https://kexue.fm/tag/%E6%A6%82%E7%8E%87/) [梯度](https://kexue.fm/tag/%E6%A2%AF%E5%BA%A6/) [转载](https://kexue.fm/tag/%E8%BD%AC%E8%BD%BD/) [微分方程](https://kexue.fm/tag/%E5%BE%AE%E5%88%86%E6%96%B9%E7%A8%8B/) [矩阵](https://kexue.fm/tag/%E7%9F%A9%E9%98%B5/) [天象](https://kexue.fm/tag/%E5%A4%A9%E8%B1%A1/) [分析](https://kexue.fm/tag/%E5%88%86%E6%9E%90/) [深度学习](https://kexue.fm/tag/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/) [积分](https://kexue.fm/tag/%E7%A7%AF%E5%88%86/) [python](https://kexue.fm/tag/python/) [优化器](https://kexue.fm/tag/%E4%BC%98%E5%8C%96%E5%99%A8/) [力学](https://kexue.fm/tag/%E5%8A%9B%E5%AD%A6/) [无监督](https://kexue.fm/tag/%E6%97%A0%E7%9B%91%E7%9D%A3/) [扩散](https://kexue.fm/tag/%E6%89%A9%E6%95%A3/) [几何](https://kexue.fm/tag/%E5%87%A0%E4%BD%95/) [节日](https://kexue.fm/tag/%E8%8A%82%E6%97%A5/) [生活](https://kexue.fm/tag/%E7%94%9F%E6%B4%BB/) [文本生成](https://kexue.fm/tag/%E6%96%87%E6%9C%AC%E7%94%9F%E6%88%90/) [数论](https://kexue.fm/tag/%E6%95%B0%E8%AE%BA/)

### 随机文章

- [费曼积分法——积分符号内取微分(3)](https://kexue.fm/archives/1629)
- [通过互信息思想来缓解类别不平衡问题](https://kexue.fm/archives/7615)
- [天文望远镜拍到宇宙最美部分(图)](https://kexue.fm/archives/24)
- [《教材如何写》:BoJone的粗浅看法](https://kexue.fm/archives/1329)
- [【外微分浅谈】4\. 微分不微](https://kexue.fm/archives/4059)
- [OCR技术浅探：3. 特征提取(2)](https://kexue.fm/archives/3802)
- [一道级数求和证明题(非数学归纳法)](https://kexue.fm/archives/49)
- [已知中心五边形，作五边形](https://kexue.fm/archives/753)
- [科学空间：2010年11月重要天象](https://kexue.fm/archives/998)
- [《自然极值》系列——4.费马点问题](https://kexue.fm/archives/1076)

### 最近评论

- [1](https://kexue.fm/reward.html/comment-page-1#comment-27905): 1\[comment=14986\]Daniel Cheng\[/comment\]\[comment=...
- [1](https://kexue.fm/reward.html/comment-page-1#comment-27904): 1\[comment=14986\]Daniel Cheng\[/comment\]\[comment=...
- [1](https://kexue.fm/links.html/comment-page-6#comment-27903): 1\[comment=25585\]Evan-wyl\[/comment\]\[comment=2564...
- [1](https://kexue.fm/links.html/comment-page-6#comment-27902): 1\[comment=25585\]Evan-wyl\[/comment\]\[comment=2564...
- [tll1945tll1937](https://kexue.fm/archives/10266/comment-page-1#comment-27901): 真心实意的向大家请教问题：看了文章“对齐全量微调！这是我看过最精彩的LoRA改进（二）”，我实...
- [oYo\_logan](https://kexue.fm/archives/10757/comment-page-1#comment-27897): \[comment=27017\]苏剑林\[/comment\]苏神，想请教一下，我理解在一个batc...
- [z123](https://kexue.fm/archives/10592/comment-page-1#comment-27896): 在参数矩阵较多的CNN小模型上，Muon会明显慢于Adam，这方面有什么优化提速的方案吗？
- [dry](https://kexue.fm/archives/10958/comment-page-2#comment-27895): 苏神好，一直有个疑问，ReFlow构建的ODE是$dx\_t/dt=x\_1-x\_0$，为什么这并...
- [tyj](https://kexue.fm/archives/10958/comment-page-2#comment-27894): 感觉和之前的一篇文章很像，应该算是concurrent work： https://arxiv...
- [li6626](https://kexue.fm/archives/10667/comment-page-1#comment-27893): 苏老师，Normalizing Flow有了新进展，论文链接:https://arxiv.or...

### 友情链接

- [Cool Papers](https://papers.cool)
- [数学研发](https://bbs.emath.ac.cn)
- [Seatop](http://www.seatop.com.cn/)
- [Xiaoxia](https://xiaoxia.org/)
- [积分表-网络版](https://kexue.fm/sci/integral/index.html)
- [丝路博傲](http://blog.dvxj.com/)
- [ph4ntasy 饭特稀](http://www.ph4ntasy.com/)
- [数学之家](http://www.2math.cn/)
- [有趣天文奇观](http://interesting-sky.china-vo.org/)
- [TwistedW](http://www.twistedwg.com/)
- [godweiyang](https://godweiyang.com/)
- [AI柠檬](https://blog.ailemon.net/)
- [王登科-DK博客](https://greatdk.com)
- [ESON](https://blog.eson.org/)
- [枫之羽](https://fzhiy.net/)
- [Mathor's blog](https://wmathor.com/)
- [coding-zuo](https://coding-zuo.github.io/)
- [博科园](https://www.bokeyuan.net/)
- [孔皮皮的博客](https://www.kppkkp.top/)
- [运鹏的博客](https://yunpengtai.top/)
- [jiming.site](https://jiming.site/)
- [OmegaXYZ](https://www.omegaxyz.com/)
- [Blog by Eacls](https://www.eacls.top/)
- [EAI猩球](https://www.robotech.ink/)
- [文举的博客](https://liwenju0.com/)
- [用代码打点酱油](https://bruceyuan.com/)
- [申请链接](https://kexue.fm/links.html)

本站采用创作共用版权协议，要求署名、非商业用途和保持一致。转载本站内容必须也遵循“ [署名-非商业用途-保持一致](http://creativecommons.org/licenses/by-nc-nd/2.5/cn/)”的创作共用协议。
© 2009-2025 Scientific Spaces. All rights reserved. Theme by [laogui](http://www.laogui.com). Powered by [Typecho](http://typecho.org). 备案号: [粤ICP备09093259号-1/2](https://beian.miit.gov.cn/)。