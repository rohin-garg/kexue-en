## SEARCH

## MENU

- [打赏](https://kexue.fm/reward.html)
- [公式](https://kexue.fm/latex.html)
- [天象](https://kexue.fm/ac.html)
- [链接](https://kexue.fm/links.html)
- [时光](https://kexue.fm/me.html)
- [博览](https://kexue.fm/science.html)
- [归档](https://kexue.fm/content.html)

## CATEGORIES

- [千奇百怪](https://kexue.fm/category/Everything)
- [天文探索](https://kexue.fm/category/Astronomy)
- [数学研究](https://kexue.fm/category/Mathematics)
- [物理化学](https://kexue.fm/category/Phy-chem)
- [信息时代](https://kexue.fm/category/Big-Data)
- [生物自然](https://kexue.fm/category/Biology)
- [图片摄影](https://kexue.fm/category/Photograph)
- [问题百科](https://kexue.fm/category/Questions)
- [生活/情感](https://kexue.fm/category/Life-Feeling)
- [资源共享](https://kexue.fm/category/Resources)

## NEWPOSTS

- [通过msign来计算mclip（奇...](https://kexue.fm/archives/11006)
- [msign算子的Newton-Sc...](https://kexue.fm/archives/10996)
- [从无穷范数求导到等值振荡定理](https://kexue.fm/archives/10972)
- [生成扩散模型漫谈（三十）：从瞬时速...](https://kexue.fm/archives/10958)
- [MoE环游记：5、均匀分布的反思](https://kexue.fm/archives/10945)
- [msign算子的Newton-Sc...](https://kexue.fm/archives/10922)
- [Transformer升级之路：2...](https://kexue.fm/archives/10907)
- [一道概率不等式：盯着它到显然成立为止！](https://kexue.fm/archives/10902)
- [SVD的导数](https://kexue.fm/archives/10878)
- [智能家居之手搓一套能接入米家的零冷水装置](https://kexue.fm/archives/10869)

## COMMENTS

- [PengchengMa: 牛啊](https://kexue.fm/archives/10996/comment-page-1#comment-27811)
- [xczh: 已使用mean flow policy，一步推理效果确实惊人，...](https://kexue.fm/archives/10958/comment-page-1#comment-27810)
- [Cosine: 是不是因为shared experts每次都激活，而route...](https://kexue.fm/archives/10945/comment-page-1#comment-27809)
- [rpsun: 这样似乎与传统的经验正交函数之类的有相似之处。把样本的平均值减...](https://kexue.fm/archives/10699/comment-page-1#comment-27808)
- [贵阳机场接机: 怎么不更新啦](https://kexue.fm/archives/1490/comment-page-1#comment-27807)
- [czvzb: 具身智能模型目前主流也是在使用扩散和流匹配这类方法来预测动作。...](https://kexue.fm/archives/10958/comment-page-1#comment-27806)
- [Shawn\_yang: 苏神，关于您所说的：“推理阶段可以事先预估Routed Exp...](https://kexue.fm/archives/10945/comment-page-1#comment-27802)
- [OceanYU: 您好，关于由式（7）推导出高斯分布，我这里有一点问题，式（7）...](https://kexue.fm/archives/9164/comment-page-4#comment-27801)
- [jorjiang: 训练和prefill这个compute-bound阶段不做矩阵...](https://kexue.fm/archives/10907/comment-page-2#comment-27800)
- [amy: 苏老师，您有关注傅里叶旋转位置编码这篇工作吗，想知道您对这篇工...](https://kexue.fm/archives/10907/comment-page-2#comment-27799)

## USERLOGIN

- [登录](https://kexue.fm/admin/login.php)

[科学空间\|Scientific Spaces](https://kexue.fm)

- [登录](https://kexue.fm/admin/login.php)
- [打赏](https://kexue.fm/reward.html)
- [公式](https://kexue.fm/latex.html)
- [天象](https://kexue.fm/ac.html)
- [链接](https://kexue.fm/links.html)
- [时光](https://kexue.fm/me.html)
- [博览](https://kexue.fm/science.html)
- [归档](https://kexue.fm/content.html)

渴望成为一个小飞侠

- [欢迎订阅](https://kexue.fm/feed)
- [个性邮箱](https://kexue.fm/archives/119)
- [天象信息](https://kexue.fm/ac.html)
- [观测ISS](https://kexue.fm/archives/41)
- [LaTeX](https://kexue.fm/latex.html)
- [关于博主](https://kexue.fm/me.html)

欢迎访问“科学空间”，这里将与您共同探讨自然科学，回味人生百态；也期待大家的分享～

- [**千奇百怪** Everything](https://kexue.fm/category/Everything)
- [**天文探索** Astronomy](https://kexue.fm/category/Astronomy)
- [**数学研究** Mathematics](https://kexue.fm/category/Mathematics)
- [**物理化学** Phy-chem](https://kexue.fm/category/Phy-chem)
- [**信息时代** Big-Data](https://kexue.fm/category/Big-Data)
- [**生物自然** Biology](https://kexue.fm/category/Biology)
- [**图片摄影** Photograph](https://kexue.fm/category/Photograph)
- [**问题百科** Questions](https://kexue.fm/category/Questions)
- [**生活/情感** Life-Feeling](https://kexue.fm/category/Life-Feeling)
- [**资源共享** Resources](https://kexue.fm/category/Resources)

- [**千奇百怪**](https://kexue.fm/category/Everything)
- [**天文探索**](https://kexue.fm/category/Astronomy)
- [**数学研究**](https://kexue.fm/category/Mathematics)
- [**物理化学**](https://kexue.fm/category/Phy-chem)
- [**信息时代**](https://kexue.fm/category/Big-Data)
- [**生物自然**](https://kexue.fm/category/Biology)
- [**图片摄影**](https://kexue.fm/category/Photograph)
- [**问题百科**](https://kexue.fm/category/Questions)
- [**生活/情感**](https://kexue.fm/category/Life-Feeling)
- [**资源共享**](https://kexue.fm/category/Resources)

[首页](https://kexue.fm) [信息时代](https://kexue.fm/category/Big-Data) 缓存与效果的极限拉扯：从MHA、MQA、GQA到MLA

13May

# [缓存与效果的极限拉扯：从MHA、MQA、GQA到MLA](https://kexue.fm/archives/10091)

By 苏剑林 \|
2024-05-13 \|
228060位读者\|

前几天，幻方发布的 [DeepSeek-V2](https://papers.cool/arxiv/2405.04434) 引起了大家的热烈讨论。首先，最让人哗然的是1块钱100万token的价格，普遍比现有的各种竞品API便宜了两个数量级，以至于有人调侃“这个价格哪怕它输出乱码，我也会认为这个乱码是一种艺术”；其次，从模型的技术报告看，如此便宜的价格背后的关键技术之一是它新提出的MLA（ **M** ulti-head **L** atent **A** ttention），这是对GQA的改进，据说能比GQA更省更好，也引起了读者的广泛关注。

接下来，本文将跟大家一起梳理一下从MHA、MQA、GQA到MLA的演变历程，并着重介绍一下MLA的设计思路。

## MHA [\#](https://kexue.fm/archives/10091\#MHA)

MHA（ **M** ulti- **H** ead **A** ttention），也就是多头注意力，是开山之作 [《Attention is all you need》](https://kexue.fm/archives/4765) 所提出的一种Attention形式，可以说它是当前主流LLM的基础工作。在数学上，多头注意力MHA等价于多个独立的单头注意力的拼接，假设输入的（行）向量序列为$\\boldsymbol{x}\_1,\\boldsymbol{x}\_2,\\cdots,\\boldsymbol{x}\_l$，其中$\\boldsymbol{x}\_i\\in\\mathbb{R}^d$，那么MHA可以形式地记为
\\begin{equation}
\\begin{gathered}
\\boldsymbol{o}\_t = \\left\[\\boldsymbol{o}\_t^{(1)}, \\boldsymbol{o}\_t^{(2)}, \\cdots, \\boldsymbol{o}\_t^{(h)}\\right\] \\\\[10pt\]
\\boldsymbol{o}\_t^{(s)} = Attention\\left(\\boldsymbol{q}\_t^{(s)}, \\boldsymbol{k}\_{\\leq t}^{(s)} ,\\boldsymbol{v}\_{\\leq t}^{(s)}\\right)\\triangleq\\frac{\\sum\_{i\\leq t}\\exp\\left(\\boldsymbol{q}\_t^{(s)} \\boldsymbol{k}\_i^{(s)}{}^{\\top}\\right)\\boldsymbol{v}\_i^{(s)}}{\\sum\_{i\\leq t}\\exp\\left(\\boldsymbol{q}\_t^{(s)} \\boldsymbol{k}\_i^{(s)}{}^{\\top}\\right)} \\\\[15pt\]
\\boldsymbol{q}\_i^{(s)} = \\boldsymbol{x}\_i\\boldsymbol{W}\_q^{(s)}\\in\\mathbb{R}^{d\_k},\\quad \\boldsymbol{W}\_q^{(s)}\\in\\mathbb{R}^{d\\times d\_k}\\\
\\boldsymbol{k}\_i^{(s)} = \\boldsymbol{x}\_i\\boldsymbol{W}\_k^{(s)}\\in\\mathbb{R}^{d\_k},\\quad \\boldsymbol{W}\_k^{(s)}\\in\\mathbb{R}^{d\\times d\_k} \\\
\\boldsymbol{v}\_i^{(s)} = \\boldsymbol{x}\_i\\boldsymbol{W}\_v^{(s)}\\in\\mathbb{R}^{d\_v},\\quad \\boldsymbol{W}\_v^{(s)}\\in\\mathbb{R}^{d\\times d\_v}
\\end{gathered}
\\end{equation}
简单起见，这里省略了Attention矩阵的缩放因子。实践上，常见的设置是$d\_k = d\_v = d / h$，对于LLAMA2-7b有$d=4096, h=32, d\_k = d\_v = 128$，LLAMA2-70b则是$d=8192,h=64, d\_k = d\_v = 128$

由于这里只考虑了主流的自回归LLM所用的Causal Attention，因此在token by token递归生成时，新预测出来的第$t+1$个token，并不会影响到已经算好的$\\boldsymbol{k}\_{\\leq t}^{(s)} ,\\boldsymbol{v}\_{\\leq t}^{(s)}$，因此这部分结果我们可以缓存下来供后续生成调用，避免不必要的重复计算，这就是所谓的KV Cache。

而后面的MQA、GQA、MLA，都是围绕“如何减少KV Cache同时尽可能地保证效果”这个主题发展而来的产物。

## 瓶颈 [\#](https://kexue.fm/archives/10091\#%E7%93%B6%E9%A2%88)

一个自然的问题是：为什么降低KV Cache的大小如此重要？

众所周知，一般情况下LLM的推理都是在GPU上进行，单张GPU的显存是有限的，一部分我们要用来存放模型的参数和前向计算的激活值，这部分依赖于模型的体量，选定模型后它就是个常数；另外一部分我们要用来存放模型的KV Cache，这部分不仅依赖于模型的体量，还依赖于模型的输入长度，也就是在推理过程中是动态增长的，当Context长度足够长时，它的大小就会占主导地位，可能超出一张卡甚至一台机（8张卡）的总显存量。

在GPU上部署模型的原则是：能一张卡部署的，就不要跨多张卡；能一台机部署的，就不要跨多台机。这是因为“卡内通信带宽 > 卡间通信带宽 > 机间通信带宽”，由于“木桶效应”，模型部署时跨的设备越多，受设备间通信带宽的的“拖累”就越大，事实上即便是单卡H100内SRAM与HBM的带宽已经达到了3TB/s，但对于Short Context来说这个速度依然还是推理的瓶颈，更不用说更慢的卡间、机间通信了。

所以，减少KV Cache的目的就是要实现在更少的设备上推理更长的Context，或者在相同的Context长度下让推理的batch size更大，从而实现更快的推理速度或者更大的吞吐总量。当然，最终目的都是为了实现更低的推理成本。

要想更详细地了解这个问题，读者可以进一步阅读 [《FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness》](https://papers.cool/arxiv/2205.14135)、 [《A guide to LLM inference and performance》](https://www.baseten.co/blog/llm-transformer-inference-guide/)、 [《LLM inference speed of light》](https://zeux.io/2024/03/15/llm-inference-sol/) 等文章，这里就不继续展开了（主要是笔者水平也有限，唯恐说多错多）。

## MQA [\#](https://kexue.fm/archives/10091\#MQA)

MQA，即“ **M** ulti- **Q** uery **A** ttention”，是减少KV Cache的一次非常朴素的尝试，首次提出自 [《Fast Transformer Decoding: One Write-Head is All You Need》](https://papers.cool/arxiv/1911.02150)，这已经是2019年的论文了，这也意味着早在LLM火热之前，减少KV Cache就已经是研究人员非常关注的一个课题了。

MQA的思路很简单，直接让所有Attention Head共享同一个K、V，用公式来说，就是取消MHA所有的$\\boldsymbol{k},\\boldsymbol{v}$的上标${}^{(s)}$：
\\begin{equation}\\require{cancel}
\\begin{gathered}
\\boldsymbol{o}\_t = \\left\[\\boldsymbol{o}\_t^{(1)}, \\boldsymbol{o}\_t^{(2)}, \\cdots, \\boldsymbol{o}\_t^{(h)}\\right\] \\\\[10pt\]
\\boldsymbol{o}\_t^{(s)} = Attention\\left(\\boldsymbol{q}\_t^{(s)}, \\boldsymbol{k}\_{\\leq t}^{\\color{#ccc}{\\smash{\\bcancel{(s)}}}} ,\\boldsymbol{v}\_{\\leq t}^{\\color{#ccc}{\\smash{\\bcancel{(s)}}}}\\right)\\triangleq\\frac{\\sum\_{i\\leq t}\\exp\\left(\\boldsymbol{q}\_t^{(s)} \\boldsymbol{k}\_i^{\\color{#ccc}{\\smash{\\bcancel{(s)}}}}{}^{\\top}\\right)\\boldsymbol{v}\_i^{\\color{#ccc}{\\smash{\\bcancel{(s)}}}}}{\\sum\_{i\\leq t}\\exp\\left(\\boldsymbol{q}\_t^{(s)} \\boldsymbol{k}\_i^{\\color{#ccc}{\\smash{\\bcancel{(s)}}}}{}^{\\top}\\right)} \\\\[15pt\]
\\boldsymbol{q}\_i^{(s)} = \\boldsymbol{x}\_i\\boldsymbol{W}\_q^{(s)}\\in\\mathbb{R}^{d\_k},\\quad \\boldsymbol{W}\_q^{(s)}\\in\\mathbb{R}^{d\\times d\_k}\\\
\\boldsymbol{k}\_i^{\\color{#ccc}{\\smash{\\bcancel{(s)}}}} = \\boldsymbol{x}\_i\\boldsymbol{W}\_k^{\\color{#ccc}{\\smash{\\bcancel{(s)}}}}\\in\\mathbb{R}^{d\_k},\\quad \\boldsymbol{W}\_k^{\\color{#ccc}{\\smash{\\bcancel{(s)}}}}\\in\\mathbb{R}^{d\\times d\_k} \\\
\\boldsymbol{v}\_i^{\\color{#ccc}{\\smash{\\bcancel{(s)}}}} = \\boldsymbol{x}\_i\\boldsymbol{W}\_v^{\\color{#ccc}{\\smash{\\bcancel{(s)}}}}\\in\\mathbb{R}^{d\_v},\\quad \\boldsymbol{W}\_v^{\\color{#ccc}{\\smash{\\bcancel{(s)}}}}\\in\\mathbb{R}^{d\\times d\_v}
\\end{gathered}
\\end{equation}
使用MQA的模型包括 [PaLM](https://arxiv.org/pdf/2204.02311)、 [StarCoder](https://papers.cool/arxiv/2305.06161)、 [Gemini](https://papers.cool/arxiv/2312.11805) 等。很明显，MQA直接将KV Cache减少到了原来的$1/h$，这是非常可观的，单从节省显存角度看已经是天花板了。

效果方面，目前看来大部分任务的损失都比较有限，且MQA的支持者相信这部分损失可以通过进一步训练来弥补回。此外，注意到MQA由于共享了K、V，将会导致Attention的参数量减少了将近一半，而为了模型总参数量的不变，通常会相应地增大FFN/GLU的规模，这也能弥补一部分效果损失。

## GQA [\#](https://kexue.fm/archives/10091\#GQA)

然而，也有人担心MQA对KV Cache的压缩太严重，以至于会影响模型的学习效率以及最终效果。为此，一个MHA与MQA之间的过渡版本GQA（ **G** rouped- **Q** uery **A** ttention）应运而生，出自论文 [《GQA: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints》](https://papers.cool/arxiv/2305.13245)，是去年的工作。

事后看来，GQA的思想也很朴素，它就是将所有Head分为$g$个组（$g$可以整除$h$），每组共享同一对K、V，用数学公式表示为
\\begin{equation}
\\begin{gathered}
\\boldsymbol{o}\_t = \\left\[\\boldsymbol{o}\_t^{(1)}, \\boldsymbol{o}\_t^{(2)}, \\cdots, \\boldsymbol{o}\_t^{(h)}\\right\] \\\\[10pt\]
\\boldsymbol{o}\_t^{(s)} = Attention\\left(\\boldsymbol{q}\_t^{(s)}, \\boldsymbol{k}\_{\\leq t}^{\\color{red}{(\\lceil sg/h\\rceil)}} ,\\boldsymbol{v}\_{\\leq t}^{\\color{red}{(\\lceil sg/h\\rceil)}}\\right)\\triangleq\\frac{\\sum\_{i\\leq t}\\exp\\left(\\boldsymbol{q}\_t^{(s)} \\boldsymbol{k}\_i^{\\color{red}{(\\lceil sg/h\\rceil)}}{}^{\\top}\\right)\\boldsymbol{v}\_i^{\\color{red}{(\\lceil sg/h\\rceil)}}}{\\sum\_{i\\leq t}\\exp\\left(\\boldsymbol{q}\_t^{(s)} \\boldsymbol{k}\_i^{\\color{red}{(\\lceil sg/h\\rceil)}}{}^{\\top}\\right)} \\\\[15pt\]
\\boldsymbol{q}\_i^{(s)} = \\boldsymbol{x}\_i\\boldsymbol{W}\_q^{(s)}\\in\\mathbb{R}^{d\_k},\\quad \\boldsymbol{W}\_q^{(s)}\\in\\mathbb{R}^{d\\times d\_k}\\\
\\boldsymbol{k}\_i^{\\color{red}{(\\lceil sg/h\\rceil)}} = \\boldsymbol{x}\_i\\boldsymbol{W}\_k^{\\color{red}{(\\lceil sg/h\\rceil)}}\\in\\mathbb{R}^{d\_k},\\quad \\boldsymbol{W}\_k^{\\color{red}{(\\lceil sg/h\\rceil)}}\\in\\mathbb{R}^{d\\times d\_k} \\\
\\boldsymbol{v}\_i^{\\color{red}{(\\lceil sg/h\\rceil)}} = \\boldsymbol{x}\_i\\boldsymbol{W}\_v^{\\color{red}{(\\lceil sg/h\\rceil)}}\\in\\mathbb{R}^{d\_v},\\quad \\boldsymbol{W}\_v^{\\color{red}{(\\lceil sg/h\\rceil)}}\\in\\mathbb{R}^{d\\times d\_v}
\\end{gathered}
\\end{equation}
这里的$\\lceil\\cdot\\rceil$是上取整符号。GQA提供了MHA到MQA的自然过渡，当$g=h$时就是MHA，$g=1$时就是MQA，当$1 < g < h$时，它只将KV Cache压缩到$g/h$，压缩率不如MQA，但同时也提供了更大的自由度，效果上更有保证。GQA最知名的使用者，大概是Meta开源的 [LLAMA2-70B](https://llama.meta.com/llama2/)，以及 [LLAMA3](https://llama.meta.com/llama3/) 全系列，此外使用GQA的模型还有 [TigerBot](https://papers.cool/arxiv/2312.08688)、 [DeepSeek-V1](https://papers.cool/arxiv/2401.02954)、 [StarCoder2](https://papers.cool/arxiv/2402.19173)、 [Yi](https://papers.cool/arxiv/2403.04652)、 [ChatGLM2](https://github.com/THUDM/ChatGLM2-6B)、 [ChatGLM3](https://github.com/THUDM/ChatGLM3) 等，相比使用MQA的模型更多（ChatGLM虽然在它的介绍中说自己是MQA，但实际是$g=2$的GQA）。

在llama2/3-70B中，GQA的$g=8$，其他用了GQA的同体量模型基本上也保持了这个设置，这并非偶然，而是同样出于推理效率的考虑。我们知道，70B这个体量的模型，如果不进行极端的量化，那么不可能部署到单卡（A100/H100 80G）上。单卡不行，那么就能单机了，一般情况下一台机可以装8张卡，刚才我们说了，Attention的每个Head实际上是独立运算然后拼接起来的，当$g=8$时，正好可以每张卡负责计算一组K、V对应的Attention Head，这样可以在尽可能保证K、V多样性的同时最大程度上减少卡间通信。

## MLA [\#](https://kexue.fm/archives/10091\#MLA)

有了MHA、MQA、GQA的铺垫，我们理解MLA（ **M** ulti-head **L** atent **A** ttention）就相对容易一些了。DeepSeek-V2的技术报告里是从低秩投影的角度引入MLA的，以至于有部分读者提出“为什么LoRA提出这么久了，直到MLA才提出对KV Cache低秩分解的做法”之类的疑问。

然而，笔者认为低秩投影这个角度并不贴近本质，因为要说低秩投影的话，事实上只要我们将GQA的所有K、V叠在一起，就会发现GQA也相当于在做低秩投影：
\\begin{equation}\\underbrace{\\left\[\\boldsymbol{k}\_i^{(1)},\\cdots,\\boldsymbol{k}\_i^{(g)},\\boldsymbol{v}\_i^{(1)},\\cdots,\\boldsymbol{v}\_i^{(g)}\\right\]}\_{\\boldsymbol{c}\_i\\in\\mathbb{R}^{g(d\_k+d\_v)}} = \\boldsymbol{x}\_i \\underbrace{\\left\[\\boldsymbol{W}\_k^{(1)},\\cdots,\\boldsymbol{W}\_k^{(g)},\\boldsymbol{W}\_v^{(1)},\\cdots,\\boldsymbol{W}\_v^{(g)}\\right\]}\_{\\boldsymbol{W}\_c\\in\\mathbb{R}^{d\\times g(d\_k+d\_v)}}\\end{equation}
这里我们将所有$\\boldsymbol{k}\_i^{(s)},\\boldsymbol{v}\_i^{(s)}$拼在一起记为$\\boldsymbol{c}\_i$，相应的投影矩阵也拼在一起记为$\\boldsymbol{W}\_c$，注意到一般都有$d\_c = g(d\_k+d\_v) < d$，所以$\\boldsymbol{x}\_i$到$\\boldsymbol{c}\_i$的变换就是一个低秩投影。所以，MLA的本质改进不是低秩投影，而是低秩投影之后的工作。

### Part 1 [\#](https://kexue.fm/archives/10091\#Part%201)

GQA在投影之后做了什么呢？首先它将向量对半分为两份分别作为K、V，然后每一份又均分为$g$份，每一份复制$h/g$次，以此来“凑”够$h$个Attention Head所需要的K、V。我们知道分割、复制都是简单的线性变换，所以MLA的第一个想法是将这些简单的线性变换换成一般的线性变换，以增强模型的能力：
\\begin{equation}
\\begin{gathered}
\\boldsymbol{o}\_t = \\left\[\\boldsymbol{o}\_t^{(1)}, \\boldsymbol{o}\_t^{(2)}, \\cdots, \\boldsymbol{o}\_t^{(h)}\\right\] \\\\[10pt\]
\\boldsymbol{o}\_t^{(s)} = Attention\\left(\\boldsymbol{q}\_t^{(s)}, \\boldsymbol{k}\_{\\leq t}^{(s)} ,\\boldsymbol{v}\_{\\leq t}^{(s)}\\right)\\triangleq\\frac{\\sum\_{i\\leq t}\\exp\\left(\\boldsymbol{q}\_t^{(s)} \\boldsymbol{k}\_i^{(s)}{}^{\\top}\\right)\\boldsymbol{v}\_i^{(s)}}{\\sum\_{i\\leq t}\\exp\\left(\\boldsymbol{q}\_t^{(s)} \\boldsymbol{k}\_i^{(s)}{}^{\\top}\\right)} \\\\[15pt\]
\\boldsymbol{q}\_i^{(s)} = \\boldsymbol{x}\_i\\boldsymbol{W}\_q^{(s)}\\in\\mathbb{R}^{d\_k},\\quad \\boldsymbol{W}\_q^{(s)}\\in\\mathbb{R}^{d\\times d\_k}\\\
\\boldsymbol{k}\_i^{(s)} = \\boldsymbol{c}\_i\\boldsymbol{W}\_k^{(s)}\\in\\mathbb{R}^{d\_k},\\quad \\boldsymbol{W}\_k^{(s)}\\in\\mathbb{R}^{d\_c\\times d\_k} \\\
\\boldsymbol{v}\_i^{(s)} = \\boldsymbol{c}\_i\\boldsymbol{W}\_v^{(s)}\\in\\mathbb{R}^{d\_v},\\quad \\boldsymbol{W}\_v^{(s)}\\in\\mathbb{R}^{d\_c\\times d\_v} \\\\[10pt\]
\\boldsymbol{c}\_i = \\boldsymbol{x}\_i \\boldsymbol{W}\_c\\in\\mathbb{R}^{d\_c},\\quad \\boldsymbol{W}\_c\\in\\mathbb{R}^{d\\times d\_c}
\\end{gathered}
\\end{equation}
然而，理论上这样是能增加模型能力，但别忘了GQA的主要目的是减少KV Cache，出于节省计算和通信成本的考虑，我们一般会缓存的是投影后的$\\boldsymbol{k}\_i, \\boldsymbol{v}\_i$而不是投影前的$\\boldsymbol{c}\_i$或$\\boldsymbol{x}\_i$，而MLA的这个做法，通过不同的投影矩阵再次让所有的K、V Head都变得各不相同，那么KV Cache的大小就恢复成跟MHA一样大了，违背了GQA的初衷。

对此，MLA发现，我们可以结合Dot-Attention的具体形式，通过一个简单但不失巧妙的恒等变换来规避这个问题。首先，在训练阶段还是照常进行，此时优化空间不大；然后，在推理阶段，我们利用
\\begin{equation}\\boldsymbol{q}\_t^{(s)} \\boldsymbol{k}\_i^{(s)}{}^{\\top} = \\left(\\boldsymbol{x}\_t\\boldsymbol{W}\_q^{(s)}\\right) \\left(\\boldsymbol{c}\_i\\boldsymbol{W}\_k^{(s)}\\right){}^{\\top} = \\boldsymbol{x}\_t\\left(\\boldsymbol{W}\_q^{(s)}\\boldsymbol{W}\_k^{(s)}{}^{\\top}\\right)\\boldsymbol{c}\_i^{\\top} \\end{equation}
这意味着推理阶段，我们可以将$\\boldsymbol{W}\_q^{(s)}\\boldsymbol{W}\_k^{(s)}{}^{\\top}$合并起来作为Q的投影矩阵，那么$\\boldsymbol{c}\_i$则取代了原本的$\\boldsymbol{k}\_i$，同理，在$\\boldsymbol{o}\_t$后面我们还有一个投影矩阵，于是$\\boldsymbol{v}\_i^{(s)} = \\boldsymbol{c}\_i\\boldsymbol{W}\_v^{(s)}$的$\\boldsymbol{W}\_v^{(s)}$也可以吸收到后面的投影矩阵中去，于是等效地$\\boldsymbol{v}\_i$也可以用$\\boldsymbol{c}\_i$代替，也就是说此时KV Cache只需要存下所有的$\\boldsymbol{c}\_i$就行，而不至于存下所有的$\\boldsymbol{k}\_i^{(s)}$、$\\boldsymbol{v}\_i^{(s)}$。注意到$\\boldsymbol{c}\_i$跟${}^{(s)}$无关，也就是说是所有头共享的，即MLA在推理阶段它可以恒等变换为一个MQA。

再次强调，本文的主题是一直都是减少KV Cache，那到目前为止，MLA做到了什么呢？答案是通过不同的投影矩阵来增强了GQA的能力，并且推理时可以保持同样大小的KV Cache。那么反过来，如果我们只需要跟GQA相近的能力，那么是不是就可以再次减少KV Cache了？换言之，$d\_c$没必要取$g(d\_k+d\_v)$，而是取更小的值（DeepSeek-V2取了512），从而进一步压缩KV Cache，这就是MLA的核心思想。

> **补充说明：**
>
> 1、$\\boldsymbol{W}\_q^{(s)}\\boldsymbol{W}\_k^{(s)}{}^{\\top}$合并成一个矩阵的恒等变换，理论上只有在无限精度下才成立，实际上如果我们使用单精度尤其是BF16的话，经过变换后的精度损失往往还是挺明显的，经过多层累积后可能放大到比较可观的程度；
>
> 2、实际上我们一般不按照$\\boldsymbol{x}\_t\\left(\\boldsymbol{W}\_q^{(s)}\\boldsymbol{W}\_k^{(s)}{}^{\\top}\\right)$来计算Q，而是按照$\\left(\\boldsymbol{x}\_t\\boldsymbol{W}\_q^{(s)}\\right)\\boldsymbol{W}\_k^{(s)}{}^{\\top}$来计算，这样虽然是串行的，但在低秩假设下计算量更少，并且理论精度的损失也更少，不过在文章中，我们仍按照$\\boldsymbol{W}\_q^{(s)}\\boldsymbol{W}\_k^{(s)}{}^{\\top}$合并成一个矩阵来介绍。

### Part 2 [\#](https://kexue.fm/archives/10091\#Part%202)

一切似乎都很完美，看上去一个又好又省的理想设计就要出炉了。不过别急，当我们再深入思考一下就会发现，到目前为止的MLA有一个难以绕开的缺陷——不兼容 [RoPE（旋转位置编码）](https://kexue.fm/archives/8265)。

刚才我们说了，MLA之所以能保持跟GQA一样大小的KV Cache，其关键一步是“将$\\boldsymbol{W}\_q^{(s)}\\boldsymbol{W}\_k^{(s)}{}^{\\top}$合并成一个（跟位置无关的）矩阵作为Q的投影矩阵”，但如果加了RoPE的话，这一步就无法实现了。这是因为RoPE是一个跟位置相关的、$d\_k\\times d\_k$的分块对角矩阵$\\boldsymbol{\\mathcal{R}}\_m$，满足$\\boldsymbol{\\mathcal{R}}\_m\\boldsymbol{\\mathcal{R}}\_n^{\\top}=\\boldsymbol{\\mathcal{R}}\_{m-n}$，MLA加入RoPE之后会让$\\boldsymbol{W}\_q^{(s)}\\boldsymbol{W}\_k^{(s)}{}^{\\top}$之间多插入了一项$\\boldsymbol{\\mathcal{R}}\_{t-i}$：
\\begin{equation}
\\boldsymbol{q}\_i^{(s)} = \\boldsymbol{x}\_i\\boldsymbol{W}\_q^{(s)}\\color{#3ce2f7}{\\boldsymbol{\\mathcal{R}}\_i}\\quad,\\quad\\boldsymbol{k}\_i^{(s)} = \\boldsymbol{c}\_i\\boldsymbol{W}\_k^{(s)}\\color{#3ce2f7}{\\boldsymbol{\\mathcal{R}}\_i} \\\
\\boldsymbol{q}\_t^{(s)} \\boldsymbol{k}\_i^{(s)}{}^{\\top} = \\left(\\boldsymbol{x}\_t\\boldsymbol{W}\_q^{(s)}\\color{#3ce2f7}{\\boldsymbol{\\mathcal{R}}\_t}\\right) \\left(\\boldsymbol{c}\_i\\boldsymbol{W}\_k^{(s)}\\color{#3ce2f7}{\\boldsymbol{\\mathcal{R}}\_i}\\right){}^{\\top} = \\boldsymbol{x}\_t\\left(\\boldsymbol{W}\_q^{(s)}\\color{#3ce2f7}{\\boldsymbol{\\mathcal{R}}\_{t-i}}\\boldsymbol{W}\_k^{(s)}{}^{\\top}\\right)\\boldsymbol{c}\_i^{\\top} \\end{equation}
这里的$\\boldsymbol{W}\_q^{(s)}\\color{#3ce2f7}{\\boldsymbol{\\mathcal{R}}\_{t-i}}\\boldsymbol{W}\_k^{(s)}{}^{\\top}$就无法合并为一个固定的投影矩阵了（跟位置差$t-i$相关），从而MLA的想法无法结合RoPE实现。

前段时间，笔者也很荣幸跟DeepSeek团队讨论过这个问题，但这个问题可以说非常本质，所以当时笔者实际上也没能提出什么有效的建议。最简单的方式是放弃RoPE，换用其他基于Attention Bias的位置编码，如 [ALIBI](https://kexue.fm/archives/9431#ALIBI)，但DeepSeek的实验显示它明显不如RoPE（注意，MLA不是不能加RoPE，而是加了RoPE之后无法用恒等变换技巧来减少KV Cache），笔者也提议过换 [Sandwich](https://kexue.fm/archives/9431#Sandwich)，它不像ALIBI单调衰减到负无穷，估计效果会好些，但感觉是治标不治本。还有一个折中的办法是将$\\boldsymbol{q}\_i$的输入也改为$\\boldsymbol{c}\_i$，然后RoPE加在$\\boldsymbol{c}\_i$之后，即
\\begin{equation}\\boldsymbol{q}\_i^{(s)} = \\boldsymbol{c}\_i\\color{#3ce2f7}{\\boldsymbol{\\mathcal{R}}\_i}\\boldsymbol{W}\_q^{(s)},\\quad\\boldsymbol{k}\_i^{(s)} = \\boldsymbol{c}\_i\\color{#3ce2f7}{\\boldsymbol{\\mathcal{R}}\_i}\\boldsymbol{W}\_k^{(s)}\\end{equation}
这样$\\boldsymbol{\\mathcal{R}}\_i$就可以吸收到$\\boldsymbol{c}\_i$中去，但这样就没有$\\boldsymbol{\\mathcal{R}}\_m\\boldsymbol{\\mathcal{R}}\_n^{\\top}=\\boldsymbol{\\mathcal{R}}\_{m-n}$的运算了，此时的RoPE不再是通过绝对位置实现相对位置，而单纯是在Q、K上加绝对位置，让模型自己想办法提炼相对位置信息。

最后发布的MLA，采取了一种混合的方法——每个Attention Head的Q、K新增$d\_r$个维度用来添加RoPE，其中K新增的维度每个Head共享：
\\begin{equation}
\\begin{gathered}
\\boldsymbol{o}\_t = \\left\[\\boldsymbol{o}\_t^{(1)}, \\boldsymbol{o}\_t^{(2)}, \\cdots, \\boldsymbol{o}\_t^{(h)}\\right\] \\\\[10pt\]
\\boldsymbol{o}\_t^{(s)} = Attention\\left(\\boldsymbol{q}\_t^{(s)}, \\boldsymbol{k}\_{\\leq t}^{(s)} ,\\boldsymbol{v}\_{\\leq t}^{(s)}\\right)\\triangleq\\frac{\\sum\_{i\\leq t}\\exp\\left(\\boldsymbol{q}\_t^{(s)} \\boldsymbol{k}\_i^{(s)}{}^{\\top}\\right)\\boldsymbol{v}\_i^{(s)}}{\\sum\_{i\\leq t}\\exp\\left(\\boldsymbol{q}\_t^{(s)} \\boldsymbol{k}\_i^{(s)}{}^{\\top}\\right)} \\\\[15pt\]
\\boldsymbol{q}\_i^{(s)} = \\left\[\\boldsymbol{x}\_i\\boldsymbol{W}\_{qc}^{(s)}, \\boldsymbol{x}\_i\\boldsymbol{W}\_{qr}^{(s)}\\color{#3ce2f7}{\\boldsymbol{\\mathcal{R}}\_i}\\right\]\\in\\mathbb{R}^{d\_k + d\_r},\\quad \\boldsymbol{W}\_{qc}^{(s)}\\in\\mathbb{R}^{d\\times d\_k},\\boldsymbol{W}\_{qr}^{(s)}\\in\\mathbb{R}^{d\\times d\_r}\\\
\\boldsymbol{k}\_i^{(s)} = \\left\[\\boldsymbol{c}\_i\\boldsymbol{W}\_{kc}^{(s)}, \\boldsymbol{x}\_i\\boldsymbol{W}\_{kr}^{\\color{#ccc}{\\smash{\\bcancel{(s)}}}}\\color{#3ce2f7}{\\boldsymbol{\\mathcal{R}}\_i}\\right\]\\in\\mathbb{R}^{d\_k+d\_r},\\quad \\boldsymbol{W}\_{kc}^{(s)}\\in\\mathbb{R}^{d\_c\\times d\_k}, \\boldsymbol{W}\_{kr}^{\\color{#ccc}{\\smash{\\bcancel{(s)}}}}\\in\\mathbb{R}^{d\\times d\_r} \\\
\\boldsymbol{v}\_i^{(s)} = \\boldsymbol{c}\_i\\boldsymbol{W}\_v^{(s)}\\in\\mathbb{R}^{d\_v},\\quad \\boldsymbol{W}\_v^{(s)}\\in\\mathbb{R}^{d\_c\\times d\_v} \\\\[10pt\]
\\boldsymbol{c}\_i = \\boldsymbol{x}\_i \\boldsymbol{W}\_c\\in\\mathbb{R}^{d\_c},\\quad \\boldsymbol{W}\_c\\in\\mathbb{R}^{d\\times d\_c}
\\end{gathered}
\\end{equation}
这样一来，没有RoPE的维度就可以重复“Part 1”的操作，在推理时KV Cache只需要存$\\boldsymbol{c}\_i$，新增的带RoPE的维度就可以用来补充位置信息，并且由于所有Head共享，所以也就只有在K Cache这里增加了$d\_r$个维度，原论文取了$d\_r = d\_k / 2 = 64$，相比原本的$d\_c=512$，增加的幅度不大。

### Part 3 [\#](https://kexue.fm/archives/10091\#Part%203)

最后有一个细节，就是MLA的最终版本，还将Q的输入也改为了低秩投影形式，这与减少KV Cache无关，主要是为了减少训练期间参数量和相应的梯度（原论文说的是激活值，个人表示不大理解）所占的显存：
\\begin{equation}
\\begin{gathered}
\\boldsymbol{o}\_t = \\left\[\\boldsymbol{o}\_t^{(1)}, \\boldsymbol{o}\_t^{(2)}, \\cdots, \\boldsymbol{o}\_t^{(h)}\\right\] \\\\[10pt\]
\\boldsymbol{o}\_t^{(s)} = Attention\\left(\\boldsymbol{q}\_t^{(s)}, \\boldsymbol{k}\_{\\leq t}^{(s)} ,\\boldsymbol{v}\_{\\leq t}^{(s)}\\right)\\triangleq\\frac{\\sum\_{i\\leq t}\\exp\\left(\\boldsymbol{q}\_t^{(s)} \\boldsymbol{k}\_i^{(s)}{}^{\\top}\\right)\\boldsymbol{v}\_i^{(s)}}{\\sum\_{i\\leq t}\\exp\\left(\\boldsymbol{q}\_t^{(s)} \\boldsymbol{k}\_i^{(s)}{}^{\\top}\\right)} \\\\[15pt\]
\\boldsymbol{q}\_i^{(s)} = \\left\[\\boldsymbol{c}\_i'\\boldsymbol{W}\_{qc}^{(s)}, \\boldsymbol{c}\_i'\\boldsymbol{W}\_{qr}^{(s)}\\color{#3ce2f7}{\\boldsymbol{\\mathcal{R}}\_i}\\right\]\\in\\mathbb{R}^{d\_k + d\_r},\\quad \\boldsymbol{W}\_{qc}^{(s)}\\in\\mathbb{R}^{d\_c'\\times d\_k},\\boldsymbol{W}\_{qr}^{(s)}\\in\\mathbb{R}^{d\_c'\\times d\_r}\\\
\\boldsymbol{k}\_i^{(s)} = \\left\[\\boldsymbol{c}\_i\\boldsymbol{W}\_{kc}^{(s)}, \\boldsymbol{x}\_i\\boldsymbol{W}\_{kr}^{\\color{#ccc}{\\smash{\\bcancel{(s)}}}}\\color{#3ce2f7}{\\boldsymbol{\\mathcal{R}}\_i}\\right\]\\in\\mathbb{R}^{d\_k+d\_r},\\quad \\boldsymbol{W}\_{kc}^{(s)}\\in\\mathbb{R}^{d\_c\\times d\_k}, \\boldsymbol{W}\_{kr}^{\\color{#ccc}{\\smash{\\bcancel{(s)}}}}\\in\\mathbb{R}^{d\\times d\_r} \\\
\\boldsymbol{v}\_i^{(s)} = \\boldsymbol{c}\_i\\boldsymbol{W}\_v^{(s)}\\in\\mathbb{R}^{d\_v},\\quad \\boldsymbol{W}\_v^{(s)}\\in\\mathbb{R}^{d\_c\\times d\_v} \\\\[10pt\]
\\boldsymbol{c}\_i' = \\boldsymbol{x}\_i \\boldsymbol{W}\_c'\\in\\mathbb{R}^{d\_c'},\\quad \\boldsymbol{W}\_c'\\in\\mathbb{R}^{d\\times d\_c'} \\\
\\boldsymbol{c}\_i = \\boldsymbol{x}\_i \\boldsymbol{W}\_c\\in\\mathbb{R}^{d\_c},\\quad \\boldsymbol{W}\_c\\in\\mathbb{R}^{d\\times d\_c} \\\
\\end{gathered}
\\label{eq:mla-mha}\\end{equation}
注意$\\boldsymbol{k}\_i^{(s)}$中的第二项，带RoPE的部分，其输入还是$\\boldsymbol{x}\_i$而不是$\\boldsymbol{c}\_i$，这里保持了原论文的设置，不是笔误，$d\_c'$原论文的取值是1536，跟$d\_c=512$不同。同时，我们把带RoPE的MHA放在下面，方便大家对比：
\\begin{equation}
\\begin{gathered}
\\boldsymbol{o}\_t = \\left\[\\boldsymbol{o}\_t^{(1)}, \\boldsymbol{o}\_t^{(2)}, \\cdots, \\boldsymbol{o}\_t^{(h)}\\right\] \\\\[10pt\]
\\boldsymbol{o}\_t^{(s)} = Attention\\left(\\boldsymbol{q}\_t^{(s)}, \\boldsymbol{k}\_{\\leq t}^{(s)} ,\\boldsymbol{v}\_{\\leq t}^{(s)}\\right)\\triangleq\\frac{\\sum\_{i\\leq t}\\exp\\left(\\boldsymbol{q}\_t^{(s)} \\boldsymbol{k}\_i^{(s)}{}^{\\top}\\right)\\boldsymbol{v}\_i^{(s)}}{\\sum\_{i\\leq t}\\exp\\left(\\boldsymbol{q}\_t^{(s)} \\boldsymbol{k}\_i^{(s)}{}^{\\top}\\right)} \\\\[15pt\]
\\boldsymbol{q}\_i^{(s)} = \\boldsymbol{x}\_i\\boldsymbol{W}\_q^{(s)}\\color{#3ce2f7}{\\boldsymbol{\\mathcal{R}}\_i}\\in\\mathbb{R}^{d\_k},\\quad \\boldsymbol{W}\_q^{(s)}\\in\\mathbb{R}^{d\\times d\_k}\\\
\\boldsymbol{k}\_i^{(s)} = \\boldsymbol{x}\_i\\boldsymbol{W}\_k^{(s)}\\color{#3ce2f7}{\\boldsymbol{\\mathcal{R}}\_i}\\in\\mathbb{R}^{d\_k},\\quad \\boldsymbol{W}\_k^{(s)}\\in\\mathbb{R}^{d\\times d\_k} \\\
\\boldsymbol{v}\_i^{(s)} = \\boldsymbol{x}\_i\\boldsymbol{W}\_v^{(s)}\\in\\mathbb{R}^{d\_v},\\quad \\boldsymbol{W}\_v^{(s)}\\in\\mathbb{R}^{d\\times d\_v}
\\end{gathered}
\\end{equation}
可以发现，其实在训练阶段，除了多了一步低秩投影以及只在部分维度加RoPE外，MLA与Q、K的Head Size由$d\_k$换成$d\_k + d\_r$的MHA基本无异。

解码阶段的MLA则改为MQA形式
\\begin{equation}
\\begin{gathered}
\\boldsymbol{o}\_t = \\left\[\\boldsymbol{o}\_t^{(1)}\\boldsymbol{W}\_v^{(1)}, \\boldsymbol{o}\_t^{(2)}\\boldsymbol{W}\_v^{(2)}, \\cdots, \\boldsymbol{o}\_t^{(h)}\\boldsymbol{W}\_v^{(h)}\\right\] \\\\[10pt\]
\\boldsymbol{o}\_t^{(s)} = Attention\\left(\\boldsymbol{q}\_t^{(s)}, \\boldsymbol{k}\_{\\leq t}^{\\color{#ccc}{\\smash{\\bcancel{(s)}}}} ,\\boldsymbol{c}\_{\\leq t}\\right)\\triangleq\\frac{\\sum\_{i\\leq t}\\exp\\left(\\boldsymbol{q}\_t^{(s)} \\boldsymbol{k}\_i^{\\color{#ccc}{\\smash{\\bcancel{(s)}}}}{}^{\\top}\\right)\\boldsymbol{c}\_i}{\\sum\_{i\\leq t}\\exp\\left(\\boldsymbol{q}\_t^{(s)} \\boldsymbol{k}\_i^{\\color{#ccc}{\\smash{\\bcancel{(s)}}}}{}^{\\top}\\right)} \\\\[15pt\]
\\boldsymbol{q}\_i^{(s)} = \\left\[\\boldsymbol{c}\_i'\\boldsymbol{W}\_{qc}^{(s)}\\boldsymbol{W}\_{kc}^{(s)}{}^{\\top}, \\boldsymbol{c}\_i'\\boldsymbol{W}\_{qr}^{(s)}\\color{#3ce2f7}{\\boldsymbol{\\mathcal{R}}\_i}\\right\]\\in\\mathbb{R}^{d\_c + d\_r}\\\
\\boldsymbol{k}\_i^{\\color{#ccc}{\\smash{\\bcancel{(s)}}}} = \\left\[\\boldsymbol{c}\_i, \\boldsymbol{x}\_i\\boldsymbol{W}\_{kr}^{\\color{#ccc}{\\smash{\\bcancel{(s)}}}}\\color{#3ce2f7}{\\boldsymbol{\\mathcal{R}}\_i}\\right\]\\in\\mathbb{R}^{d\_c+d\_r}\\\
\\boldsymbol{W}\_{qc}^{(s)}\\in\\mathbb{R}^{d\_c'\\times d\_k},\\boldsymbol{W}\_{kc}^{(s)}\\in\\mathbb{R}^{d\_c\\times d\_k},\\boldsymbol{W}\_{qr}^{(s)}\\in\\mathbb{R}^{d\_c'\\times d\_r},\\boldsymbol{W}\_{kr}^{\\color{#ccc}{\\smash{\\bcancel{(s)}}}}\\in\\mathbb{R}^{d\\times d\_r} \\\\[10pt\]
\\boldsymbol{c}\_i' = \\boldsymbol{x}\_i \\boldsymbol{W}\_c'\\in\\mathbb{R}^{d\_c'},\\quad \\boldsymbol{W}\_c'\\in\\mathbb{R}^{d\\times d\_c'} \\\
\\boldsymbol{c}\_i = \\boldsymbol{x}\_i \\boldsymbol{W}\_c\\in\\mathbb{R}^{d\_c},\\quad \\boldsymbol{W}\_c\\in\\mathbb{R}^{d\\times d\_c} \\\
\\end{gathered}
\\label{eq:mla-mqa}\\end{equation}
此时Q、K的Head Size变成了$d\_c + d\_r$，V的Head Size 则变成了$d\_c$，按照原论文的设置，这是$d\_k$、$d\_v$的4倍。所以实际上MLA在解码阶段做的这个转换，虽然能有效减少KV Cache，但其解码的计算量是增加的。

那为什么还能提高推理效率呢？这又回到“瓶颈”一节所讨论的问题了，我们可以将LLM的推理分两部分：第一个Token的生成（Prefill）和后续每个Token的生成（Generation），Prefill阶段涉及到对输入所有Token的并行计算，然后把对应的KV Cache存下来，这部分对于计算、带宽和显存都是瓶颈，我们可以用MLA的MHA形式$\\eqref{eq:mla-mha}$来算；但是Generation阶段由于每步只计算一个Token，实际上它更多的是带宽瓶颈和显存瓶颈，此时我们可以用MLA的MQA形式$\\eqref{eq:mla-mqa}$来算，从而明显提高Generation的速度。

还有一个细节充分体现了这个特性。一般的LLM架构参数满足$h \\times d\_k = d$，即num\_heads \* head\_size = hidden\_size，但DeepSeek-V2不一样，它$d\_k=128,d=5120$，但$h=128$，是一般设置的3倍！这是因为MLA的KV Cache大小跟$h$无关，增大$h$只会增加计算量和提升模型能力，但不会增加KV Cache，所以不会带来速度瓶颈。

## 小结 [\#](https://kexue.fm/archives/10091\#%E5%B0%8F%E7%BB%93)

本文简单概述了多头注意力的演变历程，特别是从MHA向MQA、GQA，最终到MLA的变化理念，最后详细展开了对MLA的介绍。在本文中，MLA被视为GQA的一般化，它用投影矩阵的方式替代了GQA的分割、重复，并引入了一个恒等变换技巧来可以进一步压缩KV Cache，同时采用了一种混合方法来兼容RoPE。总的来说，MLA称得上是一种非常实用的注意力变体。

_**转载到请包括本文地址：** [https://kexue.fm/archives/10091](https://kexue.fm/archives/10091)_

_**更详细的转载事宜请参考：**_ [《科学空间FAQ》](https://kexue.fm/archives/6508#%E6%96%87%E7%AB%A0%E5%A6%82%E4%BD%95%E8%BD%AC%E8%BD%BD/%E5%BC%95%E7%94%A8)

**如果您还有什么疑惑或建议，欢迎在下方评论区继续讨论。**

**如果您觉得本文还不错，欢迎 [分享](https://kexue.fm/archives/10091#share)/ [打赏](https://kexue.fm/archives/10091#pay) 本文。打赏并非要从中获得收益，而是希望知道科学空间获得了多少读者的真心关注。当然，如果你无视它，也不会影响你的阅读。再次表示欢迎和感谢！**

打赏

微信打赏

支付宝打赏

因为网站后台对打赏并无记录，因此欢迎在打赏时候备注留言。你还可以 [**点击这里**](http://mail.qq.com/cgi-bin/qm_share?t=qm_mailme&email=tN7d1drY3drrx8H0xcWa19vZ) 或在下方评论区留言来告知你的建议或需求。

**如果您需要引用本文，请参考：**

苏剑林. (May. 13, 2024). 《缓存与效果的极限拉扯：从MHA、MQA、GQA到MLA 》\[Blog post\]. Retrieved from [https://kexue.fm/archives/10091](https://kexue.fm/archives/10091)

@online{kexuefm-10091,
        title={缓存与效果的极限拉扯：从MHA、MQA、GQA到MLA},
        author={苏剑林},
        year={2024},
        month={May},
        url={\\url{https://kexue.fm/archives/10091}},
}

分类： [信息时代](https://kexue.fm/category/Big-Data)    标签： [优化](https://kexue.fm/tag/%E4%BC%98%E5%8C%96/), [语言模型](https://kexue.fm/tag/%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/), [生成模型](https://kexue.fm/tag/%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B/), [attention](https://kexue.fm/tag/attention/)[153 评论](https://kexue.fm/archives/10091#comments)

< [Cool Papers更新：简单搭建了一个站内检索系统](https://kexue.fm/archives/10088) \| [重温SSM（一）：线性系统和HiPPO矩阵](https://kexue.fm/archives/10114) >

### 你也许还对下面的内容感兴趣

- [生成扩散模型漫谈（三十）：从瞬时速度到平均速度](https://kexue.fm/archives/10958)
- [MoE环游记：5、均匀分布的反思](https://kexue.fm/archives/10945)
- [Transformer升级之路：20、MLA究竟好在哪里？](https://kexue.fm/archives/10907)
- [Transformer升级之路：19、第二类旋转位置编码](https://kexue.fm/archives/10862)
- [MoE环游记：4、难处应当多投入](https://kexue.fm/archives/10815)
- [生成扩散模型漫谈（二十九）：用DDPM来离散编码](https://kexue.fm/archives/10711)
- [细水长flow之TARFLOW：流模型满血归来？](https://kexue.fm/archives/10667)
- [为什么梯度裁剪的默认模长是1？](https://kexue.fm/archives/10657)
- [从谱范数梯度到新式权重衰减的思考](https://kexue.fm/archives/10648)
- [生成扩散模型漫谈（二十八）：分步理解一致性模型](https://kexue.fm/archives/10633)

[发表你的看法](https://kexue.fm/archives/10091#comment_form)

1. [«](https://kexue.fm/archives/10091/comment-page-5#comments)
2. [1](https://kexue.fm/archives/10091/comment-page-1#comments)
3. ...
4. [3](https://kexue.fm/archives/10091/comment-page-3#comments)
5. [4](https://kexue.fm/archives/10091/comment-page-4#comments)
6. [5](https://kexue.fm/archives/10091/comment-page-5#comments)
7. [6](https://kexue.fm/archives/10091/comment-page-6#comments)

王阳阳

February 23rd, 2025

MLA+RoPE是额外增加了一些维度（concat）表示位置编码，那么在计算attention的时候分子分母的子项应该就是$exp(qk^T+q\_{pos}k\_{pos}^T)$。第一项为不包含位置信息的embedding，第二项是额外增加的包含位置信息的embedding。那么是不是可以理解为在无位置信息的内积上加一个相对位置偏置？

[回复评论](https://kexue.fm/archives/10091/comment-page-6?replyTo=26719#respond-post-10091)

[苏剑林](https://kexue.fm) 发表于
February 27th, 2025

如果你这里的“一个相对位置偏置”是依赖于输入的，那就没问题，可以这样理解。

[回复评论](https://kexue.fm/archives/10091/comment-page-6?replyTo=26794#respond-post-10091)

kks 发表于
March 23rd, 2025

苏神，这种方式实现的rope是否会比直接qk做rope的外推能力会受影响，毕竟这样的化实际上有相对位置信息的只有后面的相对位置qk，同时该部分参数量比前面的要少，而前面部分是不包含位置信息的。

[回复评论](https://kexue.fm/archives/10091/comment-page-6?replyTo=27208#respond-post-10091)

[苏剑林](https://kexue.fm) 发表于
March 23rd, 2025

按找NTK-RoPE、YaRN的操作，NoPE占比高其实更容易外推，当然实际上长度外推的规律我也没摸清楚，所以不大好判断这个事情，因为按照我自己的实验结果，纯NoPE没什么外推能力。

另一方面，真追求效果的话，一般不敢直接将外推的模型上线，多少都要用长文本微调一下，所以长度外推这个事情的实践意义也不是很大。

[回复评论](https://kexue.fm/archives/10091/comment-page-6?replyTo=27232#respond-post-10091)

ito

March 3rd, 2025

请教苏神两个细节：
1\. 在part1中您提到：“于是$v\_i^{(s)}=c\_iW\_v^{(s)}$也可以吸收到后面的投影矩阵当中去”。这里的投影矩阵是原论文中的首次在(8)式中出现的矩阵$W^O$吗？是否因为$W^O$的存在，才能使得限制$h\\times d\_k = d$被突破，从而设置更大的$h$以提高LLM的性能？此外，相应的残差运算$o:=x+o$应该是在矩阵$W^O$之后发生的吧？
2\. 在您给出的折中方案(8)中，与原RoPE的区别就是旋转矩阵$\\mathcal{R}\_i$用于旋转隐向量$c\_i$而非$q\_i^{(s)}$和$k\_i^{(s)}$吗？看起来您的折中方案和实际的MLA的KV cache是相同的，都需要保存$c\_i$和被$\\mathcal{R}\_i$旋转后的向量。

[回复评论](https://kexue.fm/archives/10091/comment-page-6?replyTo=26918#respond-post-10091)

[苏剑林](https://kexue.fm) 发表于
March 6th, 2025

1、是的；

2、折中方案只需要存RoPE之后的就行了啊。

[回复评论](https://kexue.fm/archives/10091/comment-page-6?replyTo=26987#respond-post-10091)

Peng0625

March 3rd, 2025

苏神，没有理解为什么K Cache要缓存多份？由于需要降低K VCache，所以用了降维，这点是毫无疑问的。但是为什么旋转位置编码不能直接用于Q和K呢。因为矩阵吸收并不能带来参数量和计算量的降低。

[回复评论](https://kexue.fm/archives/10091/comment-page-6?replyTo=26919#respond-post-10091)

ito 发表于
March 3rd, 2025

我的理解是这样的：不同的head也就是上标$(s)$的信息通过矩阵吸收，被整合进了Q的投影矩阵$W^{(s)}\_q$中，从而只要缓存1份向量$c\_i$即可；而如果将RoPE直接用于Q和K，则会使得投影矩阵$W^{(s)}\_q$与相对位置$i-j$有关，它就不再是一组固定的参数了，在推理时会很麻烦（有点类似于不使用KV cache时的计算过程）。

[回复评论](https://kexue.fm/archives/10091/comment-page-6?replyTo=26920#respond-post-10091)

[苏剑林](https://kexue.fm) 发表于
March 6th, 2025

你说的是推理阶段实时投影到multi-head，然后实时加RoPE？理论上没问题，但延迟可能有点严重。

[回复评论](https://kexue.fm/archives/10091/comment-page-6?replyTo=26988#respond-post-10091)

WLB

March 19th, 2025

麻烦问一下为什么说训练阶段优化不大，训练和推理的计算过程为什么不一样呢

[回复评论](https://kexue.fm/archives/10091/comment-page-6?replyTo=27169#respond-post-10091)

[苏剑林](https://kexue.fm) 发表于
March 23rd, 2025

要不你先认真看看全文？

[回复评论](https://kexue.fm/archives/10091/comment-page-6?replyTo=27218#respond-post-10091)

JHzzzzZ

May 11th, 2025

苏神，文中关于第(9)式有一个这样的描述：“每个Attention Head的Q、K新增$d\_{r}$
个维度用来添加RoPE，其中K新增的维度每个Head共享”；请教一下为何Q的RoPE没有将每个Head共享呢？

[回复评论](https://kexue.fm/archives/10091/comment-page-6?replyTo=27574#respond-post-10091)

[苏剑林](https://kexue.fm) 发表于
May 11th, 2025

Q RoPE共享，除了节省一点参数量外没有实质价值。

[回复评论](https://kexue.fm/archives/10091/comment-page-6?replyTo=27584#respond-post-10091)

1. [«](https://kexue.fm/archives/10091/comment-page-5#comments)
2. [1](https://kexue.fm/archives/10091/comment-page-1#comments)
3. ...
4. [3](https://kexue.fm/archives/10091/comment-page-3#comments)
5. [4](https://kexue.fm/archives/10091/comment-page-4#comments)
6. [5](https://kexue.fm/archives/10091/comment-page-5#comments)
7. [6](https://kexue.fm/archives/10091/comment-page-6#comments)

[取消回复](https://kexue.fm/archives/10091#respond-post-10091)

你的大名

电子邮箱

个人网站（选填）

1\. 可以使用LaTeX代码，点击“预览效果”可查看效果；2. 可以通过点击评论楼层编号来引用该楼层；3. 网站可能会有点卡，如非确认评论失败，请不要重复点击提交。

### 内容速览

[MHA](https://kexue.fm/archives/10091#MHA)
[瓶颈](https://kexue.fm/archives/10091#%E7%93%B6%E9%A2%88)
[MQA](https://kexue.fm/archives/10091#MQA)
[GQA](https://kexue.fm/archives/10091#GQA)
[MLA](https://kexue.fm/archives/10091#MLA)
[Part 1](https://kexue.fm/archives/10091#Part%201)
[Part 2](https://kexue.fm/archives/10091#Part%202)
[Part 3](https://kexue.fm/archives/10091#Part%203)
[小结](https://kexue.fm/archives/10091#%E5%B0%8F%E7%BB%93)

### 智能搜索

支持整句搜索！网站自动使用 [结巴分词](https://github.com/fxsjy/jieba) 进行分词，并结合ngrams排序算法给出合理的搜索结果。

### 热门标签

[生成模型](https://kexue.fm/tag/%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B/) [attention](https://kexue.fm/tag/attention/) [优化](https://kexue.fm/tag/%E4%BC%98%E5%8C%96/) [语言模型](https://kexue.fm/tag/%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/) [模型](https://kexue.fm/tag/%E6%A8%A1%E5%9E%8B/) [网站](https://kexue.fm/tag/%E7%BD%91%E7%AB%99/) [概率](https://kexue.fm/tag/%E6%A6%82%E7%8E%87/) [梯度](https://kexue.fm/tag/%E6%A2%AF%E5%BA%A6/) [转载](https://kexue.fm/tag/%E8%BD%AC%E8%BD%BD/) [微分方程](https://kexue.fm/tag/%E5%BE%AE%E5%88%86%E6%96%B9%E7%A8%8B/) [矩阵](https://kexue.fm/tag/%E7%9F%A9%E9%98%B5/) [天象](https://kexue.fm/tag/%E5%A4%A9%E8%B1%A1/) [分析](https://kexue.fm/tag/%E5%88%86%E6%9E%90/) [深度学习](https://kexue.fm/tag/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/) [积分](https://kexue.fm/tag/%E7%A7%AF%E5%88%86/) [python](https://kexue.fm/tag/python/) [优化器](https://kexue.fm/tag/%E4%BC%98%E5%8C%96%E5%99%A8/) [力学](https://kexue.fm/tag/%E5%8A%9B%E5%AD%A6/) [无监督](https://kexue.fm/tag/%E6%97%A0%E7%9B%91%E7%9D%A3/) [扩散](https://kexue.fm/tag/%E6%89%A9%E6%95%A3/) [几何](https://kexue.fm/tag/%E5%87%A0%E4%BD%95/) [节日](https://kexue.fm/tag/%E8%8A%82%E6%97%A5/) [生活](https://kexue.fm/tag/%E7%94%9F%E6%B4%BB/) [文本生成](https://kexue.fm/tag/%E6%96%87%E6%9C%AC%E7%94%9F%E6%88%90/) [数论](https://kexue.fm/tag/%E6%95%B0%E8%AE%BA/)

### 随机文章

- [基于遗忘假设的平滑公式](https://kexue.fm/archives/4182)
- [庆祝圆周率(π)节！](https://kexue.fm/archives/524)
- [【理解黎曼几何】4\. 联络和协变导数](https://kexue.fm/archives/3998)
- [训练集、验证集和测试集的意义](https://kexue.fm/archives/4638)
- [生成扩散模型漫谈（二十五）：基于恒等式的蒸馏（上）](https://kexue.fm/archives/10085)
- [“让Keras更酷一些！”：层与模型的重用技巧](https://kexue.fm/archives/6985)
- [SVD分解(二)：为什么SVD意味着聚类？](https://kexue.fm/archives/4216)
- [OCR技术浅探：3. 特征提取(1)](https://kexue.fm/archives/3785)
- [采样定理：有限个点构建出整个函数](https://kexue.fm/archives/3266)
- [Keras：Tensorflow的黄金标准](https://kexue.fm/archives/7055)

### 最近评论

- [PengchengMa](https://kexue.fm/archives/10996/comment-page-1#comment-27811): 牛啊
- [xczh](https://kexue.fm/archives/10958/comment-page-1#comment-27810): 已使用mean flow policy，一步推理效果确实惊人，性能跟多步推理的diffusio...
- [Cosine](https://kexue.fm/archives/10945/comment-page-1#comment-27809): 是不是因为shared experts每次都激活，而routed experts是依概率被选中...
- [rpsun](https://kexue.fm/archives/10699/comment-page-1#comment-27808): 这样似乎与传统的经验正交函数之类的有相似之处。把样本的平均值减掉之后做正交分解。那么如果单纯地...
- [贵阳机场接机](https://kexue.fm/archives/1490/comment-page-1#comment-27807): 怎么不更新啦
- [czvzb](https://kexue.fm/archives/10958/comment-page-1#comment-27806): 具身智能模型目前主流也是在使用扩散和流匹配这类方法来预测动作。
苏神推荐你看这几篇文章：
1....
- [Shawn\_yang](https://kexue.fm/archives/10945/comment-page-1#comment-27802): 苏神，关于您所说的：“推理阶段可以事先预估Routed Expert的实际分布，只要细致地进行...
- [OceanYU](https://kexue.fm/archives/9164/comment-page-4#comment-27801): 您好，关于由式（7）推导出高斯分布，我这里有一点问题，式（7）只能保证关于x\_t-1是二次函数...
- [jorjiang](https://kexue.fm/archives/10907/comment-page-2#comment-27800): 训练和prefill这个compute-bound阶段不做矩阵吸收，这个用我这个解释更好理解了...
- [amy](https://kexue.fm/archives/10907/comment-page-2#comment-27799): 苏老师，您有关注傅里叶旋转位置编码这篇工作吗，想知道您对这篇工作的看法是什么，这篇工作可以wo...

### 友情链接

- [Cool Papers](https://papers.cool)
- [数学研发](https://bbs.emath.ac.cn)
- [Seatop](http://www.seatop.com.cn/)
- [Xiaoxia](https://xiaoxia.org/)
- [积分表-网络版](https://kexue.fm/sci/integral/index.html)
- [丝路博傲](http://blog.dvxj.com/)
- [ph4ntasy 饭特稀](http://www.ph4ntasy.com/)
- [数学之家](http://www.2math.cn/)
- [有趣天文奇观](http://interesting-sky.china-vo.org/)
- [TwistedW](http://www.twistedwg.com/)
- [godweiyang](https://godweiyang.com/)
- [AI柠檬](https://blog.ailemon.net/)
- [王登科-DK博客](https://greatdk.com)
- [ESON](https://blog.eson.org/)
- [枫之羽](https://fzhiy.net/)
- [Mathor's blog](https://wmathor.com/)
- [coding-zuo](https://coding-zuo.github.io/)
- [博科园](https://www.bokeyuan.net/)
- [孔皮皮的博客](https://www.kppkkp.top/)
- [运鹏的博客](https://yunpengtai.top/)
- [jiming.site](https://jiming.site/)
- [OmegaXYZ](https://www.omegaxyz.com/)
- [Blog by Eacls](https://www.eacls.top/)
- [EAI猩球](https://www.robotech.ink/)
- [文举的博客](https://liwenju0.com/)
- [用代码打点酱油](https://bruceyuan.com/)
- [申请链接](https://kexue.fm/links.html)

本站采用创作共用版权协议，要求署名、非商业用途和保持一致。转载本站内容必须也遵循“ [署名-非商业用途-保持一致](http://creativecommons.org/licenses/by-nc-nd/2.5/cn/)”的创作共用协议。
© 2009-2025 Scientific Spaces. All rights reserved. Theme by [laogui](http://www.laogui.com). Powered by [Typecho](http://typecho.org). 备案号: [粤ICP备09093259号-1/2](https://beian.miit.gov.cn/)。