MoE环游记：4、难处应当多投入 - 科学空间|Scientific Spaces
![MobileSideBar](https://kexue.fm/usr/themes/geekg/images/slide-button.png "MobileSideBar")
## SEARCH
## MENU
* [打赏](https://kexue.fm/reward.html)
* [公式](https://kexue.fm/latex.html)
* [天象](https://kexue.fm/ac.html)
* [链接](https://kexue.fm/links.html)
* [时光](https://kexue.fm/me.html)
* [博览](https://kexue.fm/science.html)
* [归档](https://kexue.fm/content.html)
## CATEGORIES
* [千奇百怪](https://kexue.fm/category/Everything)
* [天文探索](https://kexue.fm/category/Astronomy)
* [数学研究](https://kexue.fm/category/Mathematics)
* [物理化学](https://kexue.fm/category/Phy-chem)
* [信息时代](https://kexue.fm/category/Big-Data)
* [生物自然](https://kexue.fm/category/Biology)
* [图片摄影](https://kexue.fm/category/Photograph)
* [问题百科](https://kexue.fm/category/Questions)
* [生活/情感](https://kexue.fm/category/Life-Feeling)
* [资源共享](https://kexue.fm/category/Resources)
## NEWPOSTS
* [让炼丹更科学一些（五）：基于梯度精...](https://kexue.fm/archives/11530)
* [让炼丹更科学一些（四）：新恒等式，...](https://kexue.fm/archives/11494)
* [为什么DeltaNet要加L2 N...](https://kexue.fm/archives/11486)
* [让炼丹更科学一些（三）：SGD的终...](https://kexue.fm/archives/11480)
* [让炼丹更科学一些（二）：将结论推广...](https://kexue.fm/archives/11469)
* [滑动平均视角下的权重衰减和学习率](https://kexue.fm/archives/11459)
* [生成扩散模型漫谈（三十一）：预测数...](https://kexue.fm/archives/11428)
* [Muon优化器指南：快速上手与关键细节](https://kexue.fm/archives/11416)
* [AdamW的Weight RMS的...](https://kexue.fm/archives/11404)
* [n个正态随机数的最大值的渐近估计](https://kexue.fm/archives/11390)
## COMMENTS
* [Rapture D: 我有一个问题，为什么不考虑亥姆霍兹定理和斯托克斯公式。](https://kexue.fm/archives/11530/comment-page-1#comment-29104)
* [mofheka: 苏神是还在用jax是么？最近在做基于Google Pathwa...](https://kexue.fm/archives/11390/comment-page-1#comment-29103)
* [长琴: 看懂这篇博客也不是一件容易的事情。](https://kexue.fm/archives/11530/comment-page-1#comment-29102)
* [AlexLi: 苏老师，请教一下(7)式中将 $\\mu(x\_t)$ 传给$p...](https://kexue.fm/archives/9257/comment-page-4#comment-29101)
* [tyler\_zxc: "Performer的思想是将标准的Attention线性化，...](https://kexue.fm/archives/7921/comment-page-2#comment-29100)
* [我: 似乎并非mHC提出矩阵的思想？之前hyper connecti...](https://kexue.fm/archives/11494/comment-page-1#comment-29099)
* [winter: 苏神您好，假如对于比较均匀的attention weightP...](https://kexue.fm/archives/10847/comment-page-1#comment-29098)
* [苏剑林: KL散度、JS散度、W距离啥的，都行啊，看你喜欢哪个](https://kexue.fm/archives/8512/comment-page-2#comment-29097)
* [苏剑林: 没有绝对公平的对比方法，主要看你关心什么。比如，如果只关心推理...](https://kexue.fm/archives/9119/comment-page-14#comment-29096)
* [苏剑林: 如果我有时间重新搭建博客，应该会用python自己写了，而不用...](https://kexue.fm/links.html/comment-page-6#comment-29095)
## USERLOGIN
* [登录](https://kexue.fm/admin/login.php)
[科学空间|Scientific Spaces](https://kexue.fm)
* [登录](https://kexue.fm/admin/login.php)
* [打赏](https://kexue.fm/reward.html)
* [公式](https://kexue.fm/latex.html)
* [天象](https://kexue.fm/ac.html)
* [链接](https://kexue.fm/links.html)
* [时光](https://kexue.fm/me.html)
* [博览](https://kexue.fm/science.html)
* [归档](https://kexue.fm/content.html)
渴望成为一个小飞侠* [![](https://kexue.fm/usr/themes/geekg/images/rss.png)
欢迎订阅](https://kexue.fm/feed)
* [![](https://kexue.fm/usr/themes/geekg/images/mail.png)
个性邮箱](https://kexue.fm/archives/119)
* [![](https://kexue.fm/usr/themes/geekg/images/Saturn.png)
天象信息](https://kexue.fm/ac.html)
* [![](https://kexue.fm/usr/themes/geekg/images/iss.png)
观测ISS](https://kexue.fm/archives/41)
* [![](https://kexue.fm/usr/themes/geekg/images/pi.png)
LaTeX](https://kexue.fm/latex.html)
* [![](https://kexue.fm/usr/themes/geekg/images/mlogo.png)
关于博主](https://kexue.fm/me.html)
欢迎访问“科学空间”，这里将与您共同探讨自然科学，回味人生百态；也期待大家的分享～* [**千奇百怪**Everything](https://kexue.fm/category/Everything)
* [**天文探索**Astronomy](https://kexue.fm/category/Astronomy)
* [**数学研究**Mathematics](https://kexue.fm/category/Mathematics)
* [**物理化学**Phy-chem](https://kexue.fm/category/Phy-chem)
* [**信息时代**Big-Data](https://kexue.fm/category/Big-Data)
* [**生物自然**Biology](https://kexue.fm/category/Biology)
* [**图片摄影**Photograph](https://kexue.fm/category/Photograph)
* [**问题百科**Questions](https://kexue.fm/category/Questions)
* [**生活/情感**Life-Feeling](https://kexue.fm/category/Life-Feeling)
* [**资源共享**Resources](https://kexue.fm/category/Resources)
* [**千奇百怪**](https://kexue.fm/category/Everything)
* [**天文探索**](https://kexue.fm/category/Astronomy)
* [**数学研究**](https://kexue.fm/category/Mathematics)
* [**物理化学**](https://kexue.fm/category/Phy-chem)
* [**信息时代**](https://kexue.fm/category/Big-Data)
* [**生物自然**](https://kexue.fm/category/Biology)
* [**图片摄影**](https://kexue.fm/category/Photograph)
* [**问题百科**](https://kexue.fm/category/Questions)
* [**生活/情感**](https://kexue.fm/category/Life-Feeling)
* [**资源共享**](https://kexue.fm/category/Resources)
[首页](https://kexue.fm)[信息时代](https://kexue.fm/category/Big-Data)MoE环游记：4、难处应当多投入
28Mar
# [MoE环游记：4、难处应当多投入](https://kexue.fm/archives/10815)
By苏剑林|2025-03-28|46964位读者|:
前两篇文章我们都在讨论负载均衡，其中在[《MoE环游记：3、换个思路来分配》](https://kexue.fm/archives/10757)介绍Loss-Free方案时，笔者留了一个悬念：它引入的Bias项有一个冗余的自由度，这个自由度可以用来做另外有趣的事情。这篇文章我们就来讨论这件事。
我们知道，MoE是为每个Token只选择最匹配的$k$个Expert来进行计算，从而在增大参数量的同时还节省了计算量。然而，当我们仔细思考就会发现，这个策略实际上有明显的可改进之处：直观来看，每个Token的难度并不一样，所以更合理的方案应该是难的Token分配更多的计算资源，简单的token分配更少的资源，这样或许能在同样有限的资源下将效果最大化。
而刚才提到的Bias的额外自由度，恰好可以用来简单地实现这个目标。
## 设计思想[#](#设计思想)
首先，我们回顾一下，MoE的基本形式是
\\begin{equation}\\boldsymbol{y} = \\sum\_{i\\in \\mathop{\\text{argtop}}\_k \\boldsymbol{\\rho}} \\rho\_i \\boldsymbol{e}\_i\\end{equation}
负载不均衡是MoE训练常见的问题，对此研究人员提出了Aux Loss，这部分工作我们介绍于[《MoE环游记：2、不患寡而患不均》](https://kexue.fm/archives/10735)。此外，在[《MoE环游记：3、换个思路来分配》](https://kexue.fm/archives/10757)我们介绍了DeepSeek提出的Loss-Free方案，它将MoE改为
\\begin{equation}\\boldsymbol{y} = \\sum\_{i\\in \\mathop{\\text{argtop}}\_k \\boldsymbol{\\rho} + \\boldsymbol{b}} \\rho\_i \\boldsymbol{e}\_i\\end{equation}
然后通过调节新引入的Bias项$\\boldsymbol{b}$来实现负载均衡。为了实现每个Token可以选择动态数量的Expert，笔者提出的做法是将Loss-Free的形式稍微修改一下：
\\begin{equation}\\boldsymbol{y} = \\sum\_{i\\in \\mathop{\\text{argwhere}} \\boldsymbol{\\rho} + \\boldsymbol{b} \> 0} \\rho\_i \\boldsymbol{e}\_i\\end{equation}
即只要满足$\\rho\_i + b\_i \> 0$的Expert就被选中，这样每个Token选出的Expert数量自然是动态的，并且免除了排序的需求，某种程度上看还变得更简化了。
## 优化目标[#](#优化目标)
$\\boldsymbol{b}$的优化目标有两个：一是跟Loss-Free一样，要实现**负载均匀**；二是要控制每个Token被选中的**平均**Expert数为$k$，这我们可以称为**预算控制**，要不然直接$b\_i = \\infty$将所有Expert都选出来就行了，但这不是我们想要的。
负载均衡依然采样Loss-Free的训练方式。定义记号$\\boldsymbol{f} = [f\_1, f\_2, \\cdots, f\_n]$
\\begin{equation}f\_i = \\left\\{\\begin{aligned}1, \\quad \\rho\_i + b\_i \> 0 \\\\
0, \\quad \\rho\_i + b\_i \\leq 0\\end{aligned}\\right.\\end{equation}
然后记$\\tilde{\\boldsymbol{F}}=\\mathbb{E}[\\boldsymbol{f}]$，那么$\\boldsymbol{F} = \\tilde{\\boldsymbol{F}}/|\\tilde{\\boldsymbol{F}}|$就是当前Expert分布，其中$|\\tilde{\\boldsymbol{F}}|$是$\\tilde{\\boldsymbol{F}}$的各分量之和。Loss-Free提出的更新公式是：
\\begin{equation}\\boldsymbol{b}\\leftarrow \\boldsymbol{b} - \\alpha \\mathop{\\text{sign}}(\\boldsymbol{F} - \\boldsymbol{Q})\\label{eq:aux-loss-free}\\end{equation}
其中$\\boldsymbol{Q}=(1/n, 1/n, \\cdots, 1/n)$是目标的均匀分布。我们提到多次，$\\boldsymbol{b}$存在一个冗余的自由度，体现在对$\\boldsymbol{b}$所有分量加上同一个常数，排序结果不变。这样一来，我们可以把更新规则$\\eqref{eq:aux-loss-free}$改为
\\begin{equation}\\boldsymbol{b}\\leftarrow \\boldsymbol{b} - \\alpha \\left[\\mathop{\\text{sign}}(\\boldsymbol{F} - \\boldsymbol{Q}) - \\overline{\\mathop{\\text{sign}}(\\boldsymbol{F} - \\boldsymbol{Q})}\\right]\\label{eq:aux-loss-free-2}\\end{equation}
这里向量上面加一横代表该向量的全体分量的均值，是一个标量，向量减标量代表每个分量都减去这个标量。这样一来出来的$\\boldsymbol{b}$必然满足$\\overline{\\boldsymbol{b}}=0$，但不改变负载均衡的效果。于是我们可以$\\overline{\\boldsymbol{b}}$这个自由度留给预算控制。
怎么理解呢？很明显，如果给全体$b\_i$都加上同一个正数，那么满足$\\rho\_i + b\_i \> 0$的几率将会变大，从而总预算也会增大。所以做法很简单，先算出当前平均预算，不难发现正好是$|\\tilde{\\boldsymbol{F}}|$，如果它大于$k$，那么就调小一点$\\boldsymbol{b}$，反之则增大。整合到式$\\eqref{eq:aux-loss-free-2}$是
\\begin{equation}\\boldsymbol{b}\\leftarrow \\boldsymbol{b} - \\alpha \\left[\\mathop{\\text{sign}}(\\boldsymbol{F} - \\boldsymbol{Q}) - \\overline{\\mathop{\\text{sign}}(\\boldsymbol{F} - \\boldsymbol{Q})} + \\mathop{\\text{sign}}(|\\tilde{\\boldsymbol{F}}|- k)\\right]\\label{eq:aux-loss-free-3}\\end{equation}
如果只想保证预算不超过$k$，而不非要等于$k$，那么可以改为当$|\\tilde{\\boldsymbol{F}}|\ 0).sum(1).mean() if -eps\ k: b2 = b else: b1 = b b\_init(32, 4, 1024, 6e-3)`
```
代码中考虑的是Sigmoid激活，所以搜索区间是$[-1, 0]$，如果是其他激活函数请自行调整。不过这里的建议跟[《MoE环游记：3、换个思路来分配》](https://kexue.fm/archives/10757)一文是相同的，即加$\\boldsymbol{b}$的$\\boldsymbol{\\rho}$可以统一用Sigmoid激活，乘上Expert的$\\boldsymbol{\\rho}$才考虑用别的激活函数。
## 相关工作[#](#相关工作)
这篇文章之前，已经有一些工作尝试过动态选择Expert数目的MoE设计，下面简单列举一些笔者搜到的工作，并从个人的审美角度做一些简单的评析。
比较朴素的做法是[AdaMoE](https://papers.cool/arxiv/2406.13233)和[MoE++](https://papers.cool/arxiv/2410.07348)，它们在Expert中混入了一些低计算成本的Expert，如空白Expert、复制Expert、常数Expert，同时也鼓励负载均衡，这样当Token选中这些简单Expert时，等价于少选择了其他标准的Expert，从而间接地实现了动态数目。这样做的好处是可以复用原本Top-$k$ MoE的基建，但同时也欠缺了一些灵活性。
另外一个朴素的想法是将Top-$k$选择改为Top-$p$，出自[《Harder Tasks Need More Experts: Dynamic Routing in MoE Models》](https://papers.cool/arxiv/2403.07652)。这个转换看上去很自然，但实际上有颇多问题，比如无法准确控制平均预算，因为当$\\boldsymbol{\\rho}$接近均匀分布时Top-$p$的比例会非常大，所以原论文又新增了一项熵损失来让$\\boldsymbol{\\rho}$远离均匀分布。总的来说，个人感觉它引入的问题比收益更明显。
一个比较独特的做法是[Ada-K Routing](https://papers.cool/arxiv/2410.10456)，它新增一个模块来预测要激活的Expert数，然后用强化学习来训练，这样做在原理上没问题，但引入强化学习无疑会增加训练复杂性。[DA-MoE](https://papers.cool/arxiv/2409.06669)则利用Attention分数来识别重要Token，为其分配更多Expert，但感觉不够本质，因为“MoE”原则上不局限于FFN层，一旦用到Attention上，不就没有Attention分数可用了？
形式上跟本文做法最相似的可能是[ReMoE](https://papers.cool/arxiv/2412.14711)，它同样是基于零阈值来选择Expert，但选择了Aux Loss的方式来实现负载均匀以及预算控制，同时又混合了手搓梯度的思想来控制Aux Loss权重，总体来看多了点糅合感。本文则延续了Loss-Free的思想，利用$\\boldsymbol{b}$的额外自由度来调控这个阈值，从而以最小的改动实现了动态Expert数目。
## 文章小结[#](#文章小结)
本文提出了一种动态选择Expert数目的MoE设计，主要思想是对Loss-Free的MoE形式稍作修改，然后调整Bias项的更新规则，利用它的额外自由度来同时实现负载均衡和预算控制。
***转载到请包括本文地址：** [https://kexue.fm/archives/10815](https://kexue.fm/archives/10815)*
***更详细的转载事宜请参考：*** [《科学空间FAQ》](https://kexue.fm/archives/6508#文章如何转载/引用)
**如果您还有什么疑惑或建议，欢迎在下方评论区继续讨论。**
**如果您觉得本文还不错，欢迎[分享](#share)/[打赏](#pay)本文。打赏并非要从中获得收益，而是希望知道科学空间获得了多少读者的真心关注。当然，如果你无视它，也不会影响你的阅读。再次表示欢迎和感谢！**
打赏![科学空间](https://kexue.fm/usr/themes/geekg/payment/wx.png)
微信打赏![科学空间](https://kexue.fm/usr/themes/geekg/payment/zfb.png)
支付宝打赏因为网站后台对打赏并无记录，因此欢迎在打赏时候备注留言。 你还可以[**点击这里**](http://mail.qq.com/cgi-bin/qm_share?t=qm_mailme&email=tN7d1drY3drrx8H0xcWa19vZ)或在下方评论区留言来告知你的建议或需求。
**如果您需要引用本文，请参考：**
苏剑林. (Mar. 28, 2025). 《MoE环游记：4、难处应当多投入 》[Blog post]. Retrieved from[https://kexue.fm/archives/10815](https://kexue.fm/archives/10815)
@online{kexuefm-10815,
title={MoE环游记：4、难处应当多投入},
author={苏剑林},
year={2025},
month={Mar},
url={\\url{https://kexue.fm/archives/10815}},
}
分类：[信息时代](https://kexue.fm/category/Big-Data) 标签：[优化](https://kexue.fm/tag/优化/),[梯度](https://kexue.fm/tag/梯度/),[moe](https://kexue.fm/tag/moe/),[动态](https://kexue.fm/tag/动态/)[28 评论](https://kexue.fm/archives/10815#comments)
&lt;[高阶MuP：更简明但更高明的谱条件缩放](https://kexue.fm/archives/10795)|[通过梯度近似寻找Normalization的替代品](https://kexue.fm/archives/10831)&gt;
### 你也许还对下面的内容感兴趣* [让炼丹更科学一些（五）：基于梯度精调学习率](https://kexue.fm/archives/11530)
* [滑动平均视角下的权重衰减和学习率](https://kexue.fm/archives/11459)
* [Muon优化器指南：快速上手与关键细节](https://kexue.fm/archives/11416)
* [AdamW的Weight RMS的渐近估计（下）](https://kexue.fm/archives/11404)
* [低精度Attention可能存在有偏的舍入误差](https://kexue.fm/archives/11371)
* [MuP之上：1. 好模型的三个特征](https://kexue.fm/archives/11340)
* [DiVeQ：一种非常简洁的VQ训练方案](https://kexue.fm/archives/11328)
* [AdamW的Weight RMS的渐近估计（上）](https://kexue.fm/archives/11307)
* [为什么Adam的Update RMS是0.2？](https://kexue.fm/archives/11267)
* [重新思考学习率与Batch Size（一）：现状](https://kexue.fm/archives/11260)
[发表你的看法](#comment_form)
[oneko](https://zhenglin-cheng.com)
April 1st, 2025
苏神可以看看我们被ICLR'25接收的DynMoE (\\url{https://arxiv.org/abs/2405.14297})，支持动态路由(top\_k)和最大专家数(max\_k)
[回复评论](https://kexue.fm/archives/10815/comment-page-1?replyTo=27296#respond-post-10815)
[苏剑林](https://kexue.fm)发表于 April 3rd, 2025
欢迎自荐！粗扫了一下，Dynamic选择方面似乎没什么启发性（当然也没啥问题），主要是感觉会不会Dynamic过度了？我看好像会自动新增和删除expert，实现的复杂性如何呢？
[回复评论](https://kexue.fm/archives/10815/comment-page-1?replyTo=27313#respond-post-10815)
elk
April 10th, 2025
请问，为什么公式(6)不会改变“负载均衡”的效果。
我理解现在选择expert的方式是$\\rho\_i + b\_i \> 0$，不再是$\\rho\_i + b\_i$的相对大小，而是绝对大小。那么公式(6)对所有的$b\_i$加上同一个标量后，不就破坏了负载均衡的更新方式（公式(5)）了吗。
[回复评论](https://kexue.fm/archives/10815/comment-page-1?replyTo=27352#respond-post-10815)
[苏剑林](https://kexue.fm)发表于 April 13th, 2025
那又如何？负载均衡只取决于$\\boldsymbol{\\rho}+\\boldsymbol{b}$的各分量的序，我们又没修改它的序。
[回复评论](https://kexue.fm/archives/10815/comment-page-1?replyTo=27374#respond-post-10815)
lsdsjy 发表于August 3rd, 2025
但本文提出的负载均衡不是已经从依赖排序（argtop）变成了依赖绝对值（argwhere\>0）吗
[回复评论](https://kexue.fm/archives/10815/comment-page-1?replyTo=28281#respond-post-10815)
[苏剑林](https://kexue.fm)发表于 August 3rd, 2025
绝对值或者说均值，影响的是“预算”；“均衡”还是只受相对大小影响。[回复评论](https://kexue.fm/archives/10815/comment-page-1?replyTo=28292#respond-post-10815)
carol
August 1st, 2025
请问公式6这里为什么需要减去sign(F−Q)的均值呢
[回复评论](https://kexue.fm/archives/10815/comment-page-1?replyTo=28277#respond-post-10815)
[苏剑林](https://kexue.fm)发表于 August 3rd, 2025
将均值这个自由度留给预算控制[回复评论](https://kexue.fm/archives/10815/comment-page-1?replyTo=28289#respond-post-10815)
且寻发表于October 11th, 2025
有点困惑，b的优化目标只有均衡负载和预算均值为k，减去$\\overline{sign(F-Q)}$（0均值约束）的操作感觉是加了一个不相关的约束条件，甚至0均值约束和预算均值约束似乎还有点冲突。请问一下苏老师试过不加0均值约束的效果吗？
[回复评论](https://kexue.fm/archives/10815/comment-page-1?replyTo=28643#respond-post-10815)
[苏剑林](https://kexue.fm)发表于 October 13th, 2025
你是说公式$\\eqref{eq:aux-loss-free-5}$？
[回复评论](https://kexue.fm/archives/10815/comment-page-1?replyTo=28651#respond-post-10815)
且寻发表于October 13th, 2025
公式$\\eqref{eq:aux-loss-free-3}$。
[回复评论](https://kexue.fm/archives/10815/comment-page-1?replyTo=28653#respond-post-10815)
且寻发表于October 13th, 2025
嗷，理解苏老师您的意思了，不加0均值约束就是公式$\\eqref{eq:aux-loss-free-5}$。感谢。
[回复评论](https://kexue.fm/archives/10815/comment-page-1?replyTo=28654#respond-post-10815)
开水August 5th, 2025
苏老师，从muon is scalable那篇论文上看kimi似乎是用的式(6)? 如果是的话，想请教下为什么没有用式(7)或者其他dynamic k式(8,9,10)呢
[回复评论](https://kexue.fm/archives/10815/comment-page-1?replyTo=28301#respond-post-10815)
[苏剑林](https://kexue.fm)发表于 August 6th, 2025
后面的都是动态$k$的方案啊，K2是静态$k$。
[回复评论](https://kexue.fm/archives/10815/comment-page-1?replyTo=28314#respond-post-10815)
开水发表于August 9th, 2025
谢谢！还有一个问题想请教下，假设有一部分token在dynamics k的时候没有被选中，shared experts应该直接处理所有token，还是应该只处理dynamic k当中被选中处理的token呢
[回复评论](https://kexue.fm/archives/10815/comment-page-1?replyTo=28336#respond-post-10815)
[苏剑林](https://kexue.fm)发表于 August 12th, 2025
shared expert跟routed expert是独立的，shared expert处理所有的token
[回复评论](https://kexue.fm/archives/10815/comment-page-1?replyTo=28364#respond-post-10815)
Eliot 发表于August 10th, 2025
你好，muon is scalable论文提交的https://github.com/NVIDIA/Megatron-LM/pull/1428 上是有关于式(6)的梯度更新代码吗 扫了一下似乎没有看到，请两位赐教了[回复评论](https://kexue.fm/archives/10815/comment-page-1?replyTo=28342#respond-post-10815)
开水发表于August 11th, 2025
我没有看过这个PR，但式6或者其他式都是基于loss free的，并没有梯度，所以应该不会出现在muon或者其他的optimizer的PR里面
[回复评论](https://kexue.fm/archives/10815/comment-page-1?replyTo=28352#respond-post-10815)
Eliot 发表于August 11th, 2025
谢谢回复了，意思是关于bias的参数训练优化求出的grad 这块好像没有看到比较权威的开源实现的[回复评论](https://kexue.fm/archives/10815/comment-page-1?replyTo=28354#respond-post-10815)
[苏剑林](https://kexue.fm)发表于 August 12th, 2025
“关于bias的参数训练优化求出的grad”是啥意思
[回复评论](https://kexue.fm/archives/10815/comment-page-1?replyTo=28370#respond-post-10815)
Eliot 发表于September 3rd, 2025
抱歉说的过于不专业，意思就是想看看loss-free的moe训练代码是哪里有的，目前是大致检索到在Megatron中有 先再仔细看看能不能找到代码对上苏神在blog中那些loss函数
[回复评论](https://kexue.fm/archives/10815/comment-page-1?replyTo=28500#respond-post-10815)
[苏剑林](https://kexue.fm)发表于 September 5th, 2025
loss-free属于训练部分，不在K2的开源范围内。但弄懂原理后实现应该不算难。
[苏剑林](https://kexue.fm)发表于 August 12th, 2025
这个PR只是优化器PR。loss-free不在这里边
[回复评论](https://kexue.fm/archives/10815/comment-page-1?replyTo=28368#respond-post-10815)
Eliot
September 3rd, 2025
苏神你好，阅读了https://huggingface.co/moonshotai/Kimi-K2-Base/blob/main/modeling\_deepseek.py的class MoE的源码，发现这里是先对专家分组，然后根据每组的专家中前2位分数之和作为该专家组分数，由此给专家组排序选择topk(=n\_groups(=1,根据config.json))的专家组，然后对这些专家组内的所有专家分数进一步排序获取到topk(=num\_experts\_per\_tok).
我是按照苏神你MoE环游记的论述阅读这份源码
1. 关于MoE-3换个思路来分配中的loss-free操作，我是知道对应这行，其实也对应着负载均衡的实现
"""
scores\_for\_choice = scores.view(bsz \* seq\_len, -1) + self.e\_score\_correction\_bias.unsqueeze(0)
"""
2. 关于MoE-4的难处应该多投入这节中的预算控制思想，其中的每个token分配k个专家 我理解应该是config.json中的num\_experts\_per\_tok参数吧，对应代码为
"""
self.top\_k = num\_experts\_per\_tok
\_, topk\_idx = torch.topk(
tmp\_scores, k=self.top\_k, dim=-1, sorted=False)
"""
3. 最后想问一下，modeling\_deepseek.py代码对应的是推理阶段的MoE实现，如果想看训练阶段的MoE为负责均衡和预算控制的源码，应该看哪里的代码会比较权威，我目前准备是看一下官方Megatron的MoE训练框架 去找寻下是否有对应的代码因为我发现有些MoE的训练实现好像都是魔改megatron比如下面这份MoE代码https://github.com/thu-ml/ReMoE
读源码的主要目的是希望除了理论学习之外更熟悉工程上是怎么实现的为了学习的质量所以比较希望找到权威高质量的源码实现希望不吝赐教[回复评论](https://kexue.fm/archives/10815/comment-page-1?replyTo=28499#respond-post-10815)
Eliot 发表于September 3rd, 2025
关于3，大概检索到了这2处，阅读之后再与苏神交流
1. https://github1s.com/NVIDIA/Megatron-LM/blob/main/megatron/core/transformer/moe/router.py#L145-L146
2. https://github1s.com/NVIDIA/Megatron-LM/blob/main/megatron/core/transformer/transformer\_config.py#L468
[回复评论](https://kexue.fm/archives/10815/comment-page-1?replyTo=28501#respond-post-10815)
Eliot 发表于September 6th, 2025
继续阅读这2份代码后，大概结论如下
1 megatron-lm应当只实现了经典的loss-free 也就是b ←b −αsign(F −Q)；具体的代码可见
- def \_update\_router\_expert\_bias -\> https://github1s.com/NVIDIA/Megatron-LM/blob/main/megatron/core/distributed/finalize\_model\_grads.py#L280
- def get\_updated\_expert\_bias -\> https://github1s.com/NVIDIA/Megatron-LM/blob/main/megatron/core/transformer/moe/moe\_utils.py#L830
[回复评论](https://kexue.fm/archives/10815/comment-page-1?replyTo=28527#respond-post-10815)
Eliot 发表于September 6th, 2025
2 实现loss-free with budget应当是在当前Megatron-LM基础上应当是较为直接的，但细节在于魔鬼，确实需要对于分布式细节了解清楚才能bug-free改出；下面是个最初步的修改思路
- 定义budget\_control参数
```python
#[class TopKRouter](https://github1s.com/NVIDIA/Megatron-LM/blob/main/megatron/core/transformer/moe/router.py#L146-L164)
self.enable\_expert\_bias = self.config.moe\_router\_enable\_expert\_bias
self.enable\_expert\_budget\_control = self.config.moe\_router\_enable\_expert\_budget\_control
if self.enable\_expert\_bias:
if self.enable\_expert\_budget\_control:
self.register\_buffer(
'local\_expert\_budget',
torch.zeros(
self.config.num\_moe\_experts,
dtype=torch.float32,
device=torch.cuda.current\_device(),
),
persistent=False,
)
```
- expert\_bias更新: 遵循b←b−α[sign(F−Q)−sign(F−Q)+sign(max( F\~ −k,0))]
```python
#[class TopKRouter](https://github1s.com/NVIDIA/Megatron-LM/blob/main/megatron/core/distributed/finalize\_model\_grads.py#L280)
def get\_updated\_expert\_bias\_with\_budget\_control(tokens\_per\_expert, expert\_bias, expert\_bias\_update\_rate):
```
[回复评论](https://kexue.fm/archives/10815/comment-page-1?replyTo=28528#respond-post-10815)
[苏剑林](https://kexue.fm)发表于 September 13th, 2025
噢，本文的dynamic-k思路，之前我没看到过完全相同实现的论文，所以如无意外应该属于新的，没有公开实现是正常的。至于怎么改，megatron我也不大懂，没法给参考意见，但我自己用jax写的话，其实dynamic-k比静态选top-k，代码还稍微简单一点。
[回复评论](https://kexue.fm/archives/10815/comment-page-1?replyTo=28547#respond-post-10815)
[取消回复](https://kexue.fm/archives/10815#respond-post-10815)
你的大名电子邮箱个人网站（选填）1. 可以使用LaTeX代码，点击“预览效果”可查看效果；
2. 可以通过点击评论楼层编号来引用该楼层；3. 网站可能会有点卡，如非确认评论失败，请**不要重复点击提交**。
********************
### 内容速览* [设计思想](#设计思想)
* [优化目标](#优化目标)
* [尝试简化](#尝试简化)
* [初始方式](#初始方式)
* [相关工作](#相关工作)
* [文章小结](#文章小结)
********************
### 智能搜索支持整句搜索！网站自动使用[结巴分词](https://github.com/fxsjy/jieba)进行分词，并结合ngrams排序算法给出合理的搜索结果。
********************
### 热门标签[生成模型](https://kexue.fm/tag/生成模型/)[attention](https://kexue.fm/tag/attention/)[优化](https://kexue.fm/tag/优化/)[语言模型](https://kexue.fm/tag/语言模型/)[模型](https://kexue.fm/tag/模型/)[梯度](https://kexue.fm/tag/梯度/)[网站](https://kexue.fm/tag/网站/)[概率](https://kexue.fm/tag/概率/)[矩阵](https://kexue.fm/tag/矩阵/)[优化器](https://kexue.fm/tag/优化器/)[转载](https://kexue.fm/tag/转载/)[微分方程](https://kexue.fm/tag/微分方程/)[分析](https://kexue.fm/tag/分析/)[天象](https://kexue.fm/tag/天象/)[深度学习](https://kexue.fm/tag/深度学习/)[积分](https://kexue.fm/tag/积分/)[python](https://kexue.fm/tag/python/)[扩散](https://kexue.fm/tag/扩散/)[力学](https://kexue.fm/tag/力学/)[无监督](https://kexue.fm/tag/无监督/)[几何](https://kexue.fm/tag/几何/)[节日](https://kexue.fm/tag/节日/)[生活](https://kexue.fm/tag/生活/)[文本生成](https://kexue.fm/tag/文本生成/)[数论](https://kexue.fm/tag/数论/)
********************
********************
### 随机文章* [桃花开放了](https://kexue.fm/archives/1548)
* [分享：孟岩的《理解矩阵》一文](https://kexue.fm/archives/1754)
* [班门弄斧：Python的代码能有多简洁？](https://kexue.fm/archives/2971)
* [四次方程的根式求解（通俗版）](https://kexue.fm/archives/114)
* [三月二十七日](https://kexue.fm/archives/2516)
* [三角函数幂的定积分](https://kexue.fm/archives/2192)
* [2009.7.22日全食各地区模拟(Flash)](https://kexue.fm/archives/18)
* [迟到一年的建模：再探碎纸复原](https://kexue.fm/archives/3134)
* [“战神”升空看它到底有多神？](https://kexue.fm/archives/229)
* [再谈非方阵的行列式](https://kexue.fm/archives/6096)
********************
********************
### 最近评论* [Rapture D](https://kexue.fm/archives/11530/comment-page-1#comment-29104): 我有一个问题，为什么不考虑亥姆霍兹定理和斯托克斯公式。* [mofheka](https://kexue.fm/archives/11390/comment-page-1#comment-29103): 苏神是还在用jax是么？最近在做基于Google Pathway的理念做一个动态版的MPMD框...
* [长琴](https://kexue.fm/archives/11530/comment-page-1#comment-29102): 看懂这篇博客也不是一件容易的事情。* [AlexLi](https://kexue.fm/archives/9257/comment-page-4#comment-29101): 苏老师，请教一下(7)式中将 $\\mu(x\_t)$ 传给$p\_o$ 进行推理的操作。$x\_...
* [tyler\_zxc](https://kexue.fm/archives/7921/comment-page-2#comment-29100): "Performer的思想是将标准的Attention线性化，所以为什么不干脆直接训练一个线性...
* [我](https://kexue.fm/archives/11494/comment-page-1#comment-29099): 似乎并非mHC提出矩阵的思想？之前hyper connection就是了
* [winter](https://kexue.fm/archives/10847/comment-page-1#comment-29098): 苏神您好，假如对于比较均匀的attention weightP，往往呈现long tail分布...
* [苏剑林](https://kexue.fm/archives/8512/comment-page-2#comment-29097): KL散度、JS散度、W距离啥的，都行啊，看你喜欢哪个
* [苏剑林](https://kexue.fm/archives/9119/comment-page-14#comment-29096): 没有绝对公平的对比方法，主要看你关心什么。比如，如果只关心推理成本和推理效果，那么有的方法可以...
* [苏剑林](https://kexue.fm/links.html/comment-page-6#comment-29095): 如果我有时间重新搭建博客，应该会用python自己写了，而不用第三方架构，这样可玩性好很多。事...
********************
********************
### 友情链接* [Cool Papers](https://papers.cool)
* [数学研发](https://bbs.emath.ac.cn)
* [Seatop](http://www.seatop.com.cn/)
* [Xiaoxia](https://xiaoxia.org/)
* [积分表-网络版](https://kexue.fm/sci/integral/index.html)
* [丝路博傲](http://blog.dvxj.com/)
* [数学之家](http://www.2math.cn/)
* [有趣天文奇观](http://interesting-sky.china-vo.org/)
* [TwistedW](http://www.twistedwg.com/)
* [godweiyang](https://godweiyang.com/)
* [AI柠檬](https://blog.ailemon.net/)
* [王登科-DK博客](https://greatdk.com)
* [ESON](https://blog.eson.org/)
* [枫之羽](https://fzhiy.net/)
* [coding-zuo](https://coding-zuo.github.io/)
* [博科园](https://www.bokeyuan.net/)
* [孔皮皮的博客](https://www.kppkkp.top/)
* [运鹏的博客](https://yunpengtai.top/)
* [jiming.site](https://jiming.site/)
* [OmegaXYZ](https://www.omegaxyz.com/)
* [EAI猩球](https://www.robotech.ink/)
* [文举的博客](https://liwenju0.com/)
* [申请链接](https://kexue.fm/links.html)
********************
[![署名-非商业用途-保持一致](https://kexue.fm/usr/themes/geekg/images/cc.gif)](http://creativecommons.org/licenses/by-nc-nd/2.5/cn/)本站采用创作共用版权协议，要求署名、非商业用途和保持一致。转载本站内容必须也遵循“[署名-非商业用途-保持一致](http://creativecommons.org/licenses/by-nc-nd/2.5/cn/)”的创作共用协议。
©2009-2026 Scientific Spaces. All rights reserved. Theme by[laogui](http://www.laogui.com). Powered by[Typecho](http://typecho.org). 备案号:[粤ICP备09093259号-1/2](https://beian.miit.gov.cn/)。