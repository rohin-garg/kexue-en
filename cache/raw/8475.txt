## SEARCH

## MENU

- [打赏](https://kexue.fm/reward.html)
- [公式](https://kexue.fm/latex.html)
- [天象](https://kexue.fm/ac.html)
- [链接](https://kexue.fm/links.html)
- [时光](https://kexue.fm/me.html)
- [博览](https://kexue.fm/science.html)
- [归档](https://kexue.fm/content.html)

## CATEGORIES

- [千奇百怪](https://kexue.fm/category/Everything)
- [天文探索](https://kexue.fm/category/Astronomy)
- [数学研究](https://kexue.fm/category/Mathematics)
- [物理化学](https://kexue.fm/category/Phy-chem)
- [信息时代](https://kexue.fm/category/Big-Data)
- [生物自然](https://kexue.fm/category/Biology)
- [图片摄影](https://kexue.fm/category/Photograph)
- [问题百科](https://kexue.fm/category/Questions)
- [生活/情感](https://kexue.fm/category/Life-Feeling)
- [资源共享](https://kexue.fm/category/Resources)

## NEWPOSTS

- [“对角+低秩”三角阵的高效求逆方法](https://kexue.fm/archives/11072)
- [通过msign来计算奇异值裁剪mc...](https://kexue.fm/archives/11059)
- [矩阵符号函数mcsgn能计算什么？](https://kexue.fm/archives/11056)
- [线性注意力简史：从模仿、创新到反哺](https://kexue.fm/archives/11033)
- [msign的导数](https://kexue.fm/archives/11025)
- [通过msign来计算奇异值裁剪mc...](https://kexue.fm/archives/11006)
- [msign算子的Newton-Sc...](https://kexue.fm/archives/10996)
- [等值振荡定理：最优多项式逼近的充要条件](https://kexue.fm/archives/10972)
- [生成扩散模型漫谈（三十）：从瞬时速...](https://kexue.fm/archives/10958)
- [MoE环游记：5、均匀分布的反思](https://kexue.fm/archives/10945)

## COMMENTS

- [silver: 求问文中的$e\_i$是啥？是ds论文的“Algorithm 1...](https://kexue.fm/archives/10757/comment-page-3#comment-28060)
- [Truenobility303: 谢谢苏神的详细解答！](https://kexue.fm/archives/10739/comment-page-2#comment-28059)
- [Truenobility303: 不好意思我的表述可能会误导性说错了，核心问题不在2\. 我觉得问...](https://kexue.fm/archives/10795/comment-page-1#comment-28058)
- [kw: 把所有M直接换成全1矩阵就行吧，比如DeltaNet变成$(Q...](https://kexue.fm/archives/11033/comment-page-1#comment-28057)
- [WB: 非常清楚的blog。我有一个小问题想问一下，推导的时候用的是不...](https://kexue.fm/archives/10795/comment-page-1#comment-28056)
- [liangzhh: 谢谢大佬的分享，感觉中间有两个手误敲错，式(9)最后应该是加号...](https://kexue.fm/archives/11072/comment-page-1#comment-28055)
- [lidhrandom: Equation 3的等号右侧第二项的第一个${\\Lambda...](https://kexue.fm/archives/11072/comment-page-1#comment-28054)
- [Kuo: 在 $PaTH$ 论文章节 \`UT Transform for...](https://kexue.fm/archives/11033/comment-page-1#comment-28053)
- [Fanhao: 假定Hessian阵正定，那不是意味着$L(\\theta)$是...](https://kexue.fm/archives/10542/comment-page-1#comment-28052)
- [曲笑一: 对于第一个疑问，我看到分布式的版本已经开源。我在想如果将每个梯...](https://kexue.fm/archives/10739/comment-page-2#comment-28051)

## USERLOGIN

- [登录](https://kexue.fm/admin/login.php)

[科学空间\|Scientific Spaces](https://kexue.fm)

- [登录](https://kexue.fm/admin/login.php)
- [打赏](https://kexue.fm/reward.html)
- [公式](https://kexue.fm/latex.html)
- [天象](https://kexue.fm/ac.html)
- [链接](https://kexue.fm/links.html)
- [时光](https://kexue.fm/me.html)
- [博览](https://kexue.fm/science.html)
- [归档](https://kexue.fm/content.html)

渴望成为一个小飞侠

- [欢迎订阅](https://kexue.fm/feed)
- [个性邮箱](https://kexue.fm/archives/119)
- [天象信息](https://kexue.fm/ac.html)
- [观测ISS](https://kexue.fm/archives/41)
- [LaTeX](https://kexue.fm/latex.html)
- [关于博主](https://kexue.fm/me.html)

欢迎访问“科学空间”，这里将与您共同探讨自然科学，回味人生百态；也期待大家的分享～

- [**千奇百怪** Everything](https://kexue.fm/category/Everything)
- [**天文探索** Astronomy](https://kexue.fm/category/Astronomy)
- [**数学研究** Mathematics](https://kexue.fm/category/Mathematics)
- [**物理化学** Phy-chem](https://kexue.fm/category/Phy-chem)
- [**信息时代** Big-Data](https://kexue.fm/category/Big-Data)
- [**生物自然** Biology](https://kexue.fm/category/Biology)
- [**图片摄影** Photograph](https://kexue.fm/category/Photograph)
- [**问题百科** Questions](https://kexue.fm/category/Questions)
- [**生活/情感** Life-Feeling](https://kexue.fm/category/Life-Feeling)
- [**资源共享** Resources](https://kexue.fm/category/Resources)

- [**千奇百怪**](https://kexue.fm/category/Everything)
- [**天文探索**](https://kexue.fm/category/Astronomy)
- [**数学研究**](https://kexue.fm/category/Mathematics)
- [**物理化学**](https://kexue.fm/category/Phy-chem)
- [**信息时代**](https://kexue.fm/category/Big-Data)
- [**生物自然**](https://kexue.fm/category/Biology)
- [**图片摄影**](https://kexue.fm/category/Photograph)
- [**问题百科**](https://kexue.fm/category/Questions)
- [**生活/情感**](https://kexue.fm/category/Life-Feeling)
- [**资源共享**](https://kexue.fm/category/Resources)

[首页](https://kexue.fm) [信息时代](https://kexue.fm/category/Big-Data) UniVAE：基于Transformer的单模型、多尺度的VAE模型

29Jun

# [UniVAE：基于Transformer的单模型、多尺度的VAE模型](https://kexue.fm/archives/8475)

By 苏剑林 \|
2021-06-29 \|
91032位读者\|

大家都知道，Transformer的$\\mathcal{O}(n^2)$复杂度是它的“硬伤”之一。不过凡事有弊亦有利，$\\mathcal{O}(n^2)$的复杂度也为Transformer带来很大的折腾空间，我们可以灵活地定制不同的attention mask，来设计出不同用途的Transformer模型来，比如 [UniLM](https://kexue.fm/archives/6933)、 [K-BERT](https://papers.cool/arxiv/1909.07606) 等。

本文介绍笔者构思的一个能用于文本的UniVAE模型，它沿用类似UniLM的思路，将VAE做到了一个Transformer模型里边，并且还具备多尺度特性～

## UniAE [\#](https://kexue.fm/archives/8475\#UniAE)

VAE（Variational Autoencoder）这里就不科普了，本站已经有多篇文章进行介绍，大家自行搜索就好。VAE可以理解为带有正则项的AE（Autoencoder），一般情况下，Encoder负责将输入编码为一个向量，并且满足一定的分布，而Decoder则负责将编码向量重构为输入。所以很显然，要实现UniVAE，首先要实现对应的UniAE。

在 [《从语言模型到Seq2Seq：Transformer如戏，全靠Mask》](https://kexue.fm/archives/6933) 中，我们已经介绍了UniLM（Uni是Unified的缩写），它通过下图左的Attention Mask来使得Transformer能完成Seq2Seq任务。然而UniLM并不是我们要寻找的UniAE，因为UniLM的Decoder部分关联到的是输入的整个编码序列，而不是单个向量。

UniLM式Attention Mask

UniAE式Attention Mask

不过，我们可以在UniLM的基础上，进一步调整Attention Mask为上图右的模式，这样一来，解码的时候只能依赖于编码部分的\[CLS\]向量以及当前已完成的解码结果，这就是我们要找的UniAE式Attention Mask了。因为对于输入来说，它只依赖于\[CLS\]向量，而\[CLS\]向量的大小是固定的，所以相当于说生成过程中的源信息只是一个固定大小的向量，而输入也被编码成这个固定大小的向量，这就是AE功能了。

UniAE式Attention关联示意图

## 多尺度 [\#](https://kexue.fm/archives/8475\#%E5%A4%9A%E5%B0%BA%E5%BA%A6)

也就是说，通过UniAE式Attention Mask，我们可以实现类似UniLM的Seq2Seq模型，它等效于Encoder将输入编码为固定长度的向量，然后Decoder对该向量进行解码。如果还觉得不够清晰，我们还可以分拆为Encoder-Decoder架构来理解，如下图所示：

分拆为Encoder-Decoder结构来理解

跟常规的Seq2Seq架构不同的地方在于，这里的Encoder和Decoder的权重是共享的。从上图还可以看出，如果我们每一层Attention都加上这种Mask，那么Decoder将依赖于每一层输入的\[CLS\]向量，这也就意味如果有$L$层Attention，那么这$L$层Attention的输入序列的所有\[CLS\]向量拼接起来，才是输入文本的完整的编码向量（当然，第一层可以去掉，因为第一层的\[CLS\]是其Embedding向量，对于每个输入来说它都是常向量），单独某一层的\[CLS\]向量，并不是完整编码向量。

对于Decoder来说，每一层Attention都有一个\[CLS\]向量传入，这其实就形成了一种多尺度结构。在CV中，最先进的生成模型基本上都是多尺度结构了，如 [StyleGAN](https://papers.cool/arxiv/1812.04948)、 [Glow](https://kexue.fm/archives/5807)、 [NVAE](https://kexue.fm/archives/7574) 等，但是NLP中似乎还不多见。不难想象，在多尺度结构中，不同层次的输入对生成结果的调控程度也是不同的，越靠近输入层的变量，控制的部分越是“无伤大雅”，而越靠近输出层的变量，则控制着生成结果的关键信息。所以理想情况下，训练好一个多尺度模型后，我们可以通过编辑不同层级的输入变量，来实现对生成结果的不同层次的控制。

## 降低维度 [\#](https://kexue.fm/archives/8475\#%E9%99%8D%E4%BD%8E%E7%BB%B4%E5%BA%A6)

有些读者可能会想到，要是每层的维度是$d$，共有$L$层，那么全部\[CLS\]向量拼接起来就是$Ld$维了，对于BERT base来说就是$12\\times 768 = 9216$维了，这编码向量维度是不是太大了？确实如此，对于一个普通的AE或者VAE来说，近万维的编码向量是太大了。

降维过程示意图

其实解决方法很简单，我们只需要将每层的\[CLS\]向量用一个全连接层先降维，然后再用另一个全连接层升维，最后拼接到剩下的$(L-1)$个$d$维向量就行了，如上图所示。这样的话，虽然输入序列还是$L\\times d$大小，但事实上\[CLS\]向量可以用一个更低维的向量表达出来，我们只需要把每一层的这个更低维向量拼接起来，作为总的编码向量就行了。

降维后的Encoder-Decoder示意图

## 解耦能力 [\#](https://kexue.fm/archives/8475\#%E8%A7%A3%E8%80%A6%E8%83%BD%E5%8A%9B)

前面的设计和讨论还只是针对普通的AE的，对于VAE来说，就是往AE的编码向量里边加入重参数操作，然后损失函数里边加入KL散度项，所以，设计好UniAE后，理论上就已经设计好UniVAE了。

不过，实际操作的时候，我们还有改进的空间。理论上来说，训练好VAE是具有一定的解耦（Disentanglement）能力的，也就是说，隐变量的每个维度是独立无关的，它们分别控制生成结果的某一方面，可以随机调节。不难理解，解耦是一件非常有挑战性的事情，所以如果VAE的Encoder能编码出解耦的编码向量，那么其拟合能力必然也是比较强的，换言之，其结构需要有一定的复杂了。

我们再来看UniAE的Encoder，它的编码向量是每一层的\[CLS\]向量（或者对应的低维向量）的拼接，对于前面的层来说，它们的\[CLS\]向量仅仅是有限几层的Transformer的输出，它们的编码能力是很弱的，并不足以编码出解耦的向量，因此将它们作为VAE的隐变量是不合适的。

所以，在实际设计UniVAE的时候，我们不能使用UniAE的所有\[CLS\]向量作为编码向量，应该设置一个起始层数，Decoder只使用大于这个层数的\[CLS\]向量，而小于等于这个层数的\[CLS\]向量则不使用，此时相对于使用下图右的Attention Mask：

靠近输出层，使用UniAE式Attention Mask

靠近输入层，使用独立式Attention Mask

此时它等效于如下的Encoder-Decoder结构：

前两层Attention使用独立式Mask的效果示意图

## 其他细节 [\#](https://kexue.fm/archives/8475\#%E5%85%B6%E4%BB%96%E7%BB%86%E8%8A%82)

至此，UniVAE的关键部分已经介绍完毕了，下面分享一下在实现过程中一些比较重要的细节。

首先是长度泄漏问题。不管是UniLM还是UniVAE，因为Encoder和Decoder整合成了一个模型，所以我们都是将输入输出拼接起来作为单个样本训练的，这样的话每个样本在Decoder部分的起始位置就不一样了，取决于输入文本的长度，这就意味着输入长度是也是作为了输入条件传入到了Decoder中，这就是长度泄漏。

这个问题有两个解决方案：第一个就是所有输入都通过截断或者填充来变为同一长度，这就不会造成长度泄漏了；第二个就更简单了，干脆啥都不做，即确实把长度当成条件输入，解码时通过控制起始位置来控制生成长度，但这样可能带来的问题是长度信息可能没有跟编码向量完全解耦，因此同一编码向量配上不同的长度可能会得到不合理的结果。

然后是层数和维度的选择问题。前面说了，为了让隐变量具有较好的解耦能力，我们将前$k$层的Attention加上独立式Attention Mask，剩下的$L-k$层则加上UniAE式Attention Mask。那么这个$k$怎么选择呢？这是一个需要仔细调整的超参数，比较小的$k$能保留更多的信息，有利于重构，但不利于解耦；反之较大的$k$则更有利于解耦，但是不利于重构。在笔者的实验中，使用的是$k=8$。

类似的问题出现在降维的维度选择上，较大的维度自然是有利于重构的，但也不利于解耦，反之则利于解耦而有损重构性能。这个参数需要根据任务本身的复杂度来具体调整，调整的大致方向是观察随机采样效果和重构效果，如果随机采样出来的样本多数可读、自然句子的重构效果也不错，那么说明这个维度适中了，否则则需要相应地调整。

最后，值得一提的是，UniAE的设计不单单可以用来做VAE，还可以用于构建 [VQ-VAE](https://kexue.fm/archives/6760)，只需要对每个\[CLS\]向量做一下量化，就成为了一个将不定长句子编码为定长离散序列的VQ-VAE模型了。

## 参考实现 [\#](https://kexue.fm/archives/8475\#%E5%8F%82%E8%80%83%E5%AE%9E%E7%8E%B0)

这里给出一个UniVAE参考实现：

> **Github： [https://github.com/bojone/univae](https://github.com/bojone/univae)**

代码里使用的是 [vMF-VAE](https://kexue.fm/archives/8404) 变体，基于bert4keras实现，基础架构是RoFormer，当然也可以换成BERT。下面演示的是用问句训练的UniVAE的效果。

随机采样效果：

> 我在steam下载的游戏，怎样能在电脑上玩啊？？？
> 呼市男科医院哪家比较好实惠
> 我血压高，我妈妈手脚麻木，是怎么回事呀
> 怎样查询交通违章记录和处罚
> 为什么我提问的问题有点卡顿
> 小米2s用的是移动卡还是联通卡
> 幼儿园怎么发展幼儿教育
> 英国读研学校排名对于英国留学生来说重要吗
> 有专业的关于excel表格数据库的培训机构吗？
> 为什么一到晚上就容易咳嗽，不睡觉就不咳

重构效果：

> 原句：数字电视机顶盒坏了，可以免费维修吗
> 重构：数字电视机顶盒坏了可以换吗?
>
> 原句：青椒跟什么炒好吃
> 重构：青椒跟什么炒好吃
>
> 原句：王者荣耀carryyou什么意思
> 重构：王者荣耀carry芈月什么意思
>
> 原句：没感冒老是咳嗽要吃什么药好
> 重构：没感冒老是咳嗽要吃什么药好
>
> 原句：沁园（金科西城大院店）怎么样，好不好的默认点评
> 重构：沁园（金源店）怎么样，好不好的默认点评

随机替换前32维隐变量：

> 原句：牙龈出血要吃什么药？
> 结果：牙龈出血还出血吃什么消炎药好
> 牙龈出血吃阿莫西林有效吗
> 牙龈出血是肝火旺吗？
> 牙龈出血去医院检查大概要多少钱？
> 牙龈出血去牙科看什么科室
> 牙龈出血去深圳哪里看牙科好
>
> 原句：广州和深圳哪个更好玩？
> 结果：广州和深圳哪个城市发展得好? 薪资高?
> 广州和深圳，哪个发达？深圳到广州的飞机票贵吗？
> 广州和深圳比哪个好
> 广州和深圳哪个人均gdp高
> 广州和深圳房价涨幅
> 广州和深圳自考一样吗

随机替换后16维隐变量：

> 原句：牙龈出血要吃什么药？
> 结果：未来21年做什么生意好？
> 湿疹给身体有什么伤害？
> 朗逸现在要买什么配置？
> 马来西亚签证要多少钱？
> 早上给孩子吃什么水果好？
> 头晕发热去医院看什么科？
>
> 原句：广州和深圳哪个更好玩？
> 结果：99和98相差多少呢？
> 微信和支付宝怎么更换手机号
> 我的指甲和肉很不一样怎么回事？
> 吃了甲硝唑多久才能喝酒？
> 桂圆和红枣可以一起泡茶吗？
> 小米和华为哪个更好点？

可以看到，随机采样和重构的效果都不错的，而通过随机替换不同维度的隐变量，我们可以大致观察到多尺度结构的效果：替换前面部分维度的隐变量，大致上保持了主题词不变；替换后面部分维度的隐变量，大致上保持了句式不变。当然，自然语言的结构性本身就很弱，因此例子中通常也夹杂了一些例外情况。

## 文章小结 [\#](https://kexue.fm/archives/8475\#%E6%96%87%E7%AB%A0%E5%B0%8F%E7%BB%93)

本文介绍了笔者构思的UniVAE设计，它沿用类似UniLM的思路，通过特定的Attention Mask将VAE做到了一个Transformer模型里边，并且还具备多尺度特性。除了常规的VAE模型外，该设计还可以用于VQ-VAE等模型。

_**转载到请包括本文地址：** [https://kexue.fm/archives/8475](https://kexue.fm/archives/8475)_

_**更详细的转载事宜请参考：**_ [《科学空间FAQ》](https://kexue.fm/archives/6508#%E6%96%87%E7%AB%A0%E5%A6%82%E4%BD%95%E8%BD%AC%E8%BD%BD/%E5%BC%95%E7%94%A8)

**如果您还有什么疑惑或建议，欢迎在下方评论区继续讨论。**

**如果您觉得本文还不错，欢迎 [分享](https://kexue.fm/archives/8475#share)/ [打赏](https://kexue.fm/archives/8475#pay) 本文。打赏并非要从中获得收益，而是希望知道科学空间获得了多少读者的真心关注。当然，如果你无视它，也不会影响你的阅读。再次表示欢迎和感谢！**

打赏

微信打赏

支付宝打赏

因为网站后台对打赏并无记录，因此欢迎在打赏时候备注留言。你还可以 [**点击这里**](http://mail.qq.com/cgi-bin/qm_share?t=qm_mailme&email=tN7d1drY3drrx8H0xcWa19vZ) 或在下方评论区留言来告知你的建议或需求。

**如果您需要引用本文，请参考：**

苏剑林. (Jun. 29, 2021). 《UniVAE：基于Transformer的单模型、多尺度的VAE模型 》\[Blog post\]. Retrieved from [https://kexue.fm/archives/8475](https://kexue.fm/archives/8475)

@online{kexuefm-8475,
        title={UniVAE：基于Transformer的单模型、多尺度的VAE模型},
        author={苏剑林},
        year={2021},
        month={Jun},
        url={\\url{https://kexue.fm/archives/8475}},
}

分类： [信息时代](https://kexue.fm/category/Big-Data)    标签： [变分](https://kexue.fm/tag/%E5%8F%98%E5%88%86/), [无监督](https://kexue.fm/tag/%E6%97%A0%E7%9B%91%E7%9D%A3/), [vae](https://kexue.fm/tag/vae/), [attention](https://kexue.fm/tag/attention/)[27 评论](https://kexue.fm/archives/8475#comments)

< [对比学习可以使用梯度累积吗？](https://kexue.fm/archives/8471) \| [又是Dropout两次！这次它做到了有监督任务的SOTA](https://kexue.fm/archives/8496) >

### 你也许还对下面的内容感兴趣

- [“对角+低秩”三角阵的高效求逆方法](https://kexue.fm/archives/11072)
- [线性注意力简史：从模仿、创新到反哺](https://kexue.fm/archives/11033)
- [Transformer升级之路：20、MLA好在哪里?（上）](https://kexue.fm/archives/10907)
- [Transformer升级之路：19、第二类旋转位置编码](https://kexue.fm/archives/10862)
- [细水长flow之TARFLOW：流模型满血归来？](https://kexue.fm/archives/10667)
- [“闭门造车”之多模态思路浅谈（三）：位置编码](https://kexue.fm/archives/10352)
- [Decoder-only的LLM为什么需要位置编码？](https://kexue.fm/archives/10347)
- [Transformer升级之路：18、RoPE的底数选择原则](https://kexue.fm/archives/10122)
- [缓存与效果的极限拉扯：从MHA、MQA、GQA到MLA](https://kexue.fm/archives/10091)
- [生成扩散模型漫谈（二十三）：信噪比与大图生成（下）](https://kexue.fm/archives/10055)

[发表你的看法](https://kexue.fm/archives/8475#comment_form)

1. [«](https://kexue.fm/archives/8475/comment-page-1#comments)
2. [1](https://kexue.fm/archives/8475/comment-page-1#comments)
3. [2](https://kexue.fm/archives/8475/comment-page-2#comments)

陈泽龙

May 5th, 2022

如果我们每一层Attention都加上这种Mask，那么Decoder将依赖于每一层输入的\[CLS\]向量
\-\-\--------
苏神，这句话怎么理解呢？为什么“Decoder将依赖于每一层输入的\[CLS\]向量”，而不是最后一层的\[CLS\]向量？

[回复评论](https://kexue.fm/archives/8475/comment-page-2?replyTo=19071#respond-post-8475)

[苏剑林](https://kexue.fm) 发表于
May 7th, 2022

因为它就是读取到了每一层的\[CLS\]向量

[回复评论](https://kexue.fm/archives/8475/comment-page-2?replyTo=19072#respond-post-8475)

cy

July 17th, 2024

请问UniVAE设置为单模型结构的优势是什么呢？不知道我理解的对不对，看起来单模型的特点在于1.输入和输出拼接和2. encoder/decoder参数共享。其中第二点在分离的结构下选择共享/不共享都是可以的。也没有理解第一点的必要性，而且文中提到的长度泄露的问题似乎就是由第一点带来的？

[回复评论](https://kexue.fm/archives/8475/comment-page-2?replyTo=24857#respond-post-8475)

[苏剑林](https://kexue.fm) 发表于
July 18th, 2024

单模型主要是当时比较流行的就是单模型（BERT系列），以及受到UniLM的影响吧。非要说什么特别的好处，应该是没有的。

[回复评论](https://kexue.fm/archives/8475/comment-page-2?replyTo=24869#respond-post-8475)

1. [«](https://kexue.fm/archives/8475/comment-page-1#comments)
2. [1](https://kexue.fm/archives/8475/comment-page-1#comments)
3. [2](https://kexue.fm/archives/8475/comment-page-2#comments)

[取消回复](https://kexue.fm/archives/8475#respond-post-8475)

你的大名

电子邮箱

个人网站（选填）

1\. 可以使用LaTeX代码，点击“预览效果”可查看效果；2. 可以通过点击评论楼层编号来引用该楼层；3. 网站可能会有点卡，如非确认评论失败，请 **不要重复点击提交**。

### 内容速览

[UniAE](https://kexue.fm/archives/8475#UniAE)
[多尺度](https://kexue.fm/archives/8475#%E5%A4%9A%E5%B0%BA%E5%BA%A6)
[降低维度](https://kexue.fm/archives/8475#%E9%99%8D%E4%BD%8E%E7%BB%B4%E5%BA%A6)
[解耦能力](https://kexue.fm/archives/8475#%E8%A7%A3%E8%80%A6%E8%83%BD%E5%8A%9B)
[其他细节](https://kexue.fm/archives/8475#%E5%85%B6%E4%BB%96%E7%BB%86%E8%8A%82)
[参考实现](https://kexue.fm/archives/8475#%E5%8F%82%E8%80%83%E5%AE%9E%E7%8E%B0)
[文章小结](https://kexue.fm/archives/8475#%E6%96%87%E7%AB%A0%E5%B0%8F%E7%BB%93)

### 智能搜索

支持整句搜索！网站自动使用 [结巴分词](https://github.com/fxsjy/jieba) 进行分词，并结合ngrams排序算法给出合理的搜索结果。

### 热门标签

[生成模型](https://kexue.fm/tag/%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B/) [attention](https://kexue.fm/tag/attention/) [优化](https://kexue.fm/tag/%E4%BC%98%E5%8C%96/) [语言模型](https://kexue.fm/tag/%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/) [模型](https://kexue.fm/tag/%E6%A8%A1%E5%9E%8B/) [网站](https://kexue.fm/tag/%E7%BD%91%E7%AB%99/) [概率](https://kexue.fm/tag/%E6%A6%82%E7%8E%87/) [梯度](https://kexue.fm/tag/%E6%A2%AF%E5%BA%A6/) [转载](https://kexue.fm/tag/%E8%BD%AC%E8%BD%BD/) [矩阵](https://kexue.fm/tag/%E7%9F%A9%E9%98%B5/) [微分方程](https://kexue.fm/tag/%E5%BE%AE%E5%88%86%E6%96%B9%E7%A8%8B/) [天象](https://kexue.fm/tag/%E5%A4%A9%E8%B1%A1/) [分析](https://kexue.fm/tag/%E5%88%86%E6%9E%90/) [深度学习](https://kexue.fm/tag/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/) [积分](https://kexue.fm/tag/%E7%A7%AF%E5%88%86/) [python](https://kexue.fm/tag/python/) [优化器](https://kexue.fm/tag/%E4%BC%98%E5%8C%96%E5%99%A8/) [力学](https://kexue.fm/tag/%E5%8A%9B%E5%AD%A6/) [无监督](https://kexue.fm/tag/%E6%97%A0%E7%9B%91%E7%9D%A3/) [扩散](https://kexue.fm/tag/%E6%89%A9%E6%95%A3/) [几何](https://kexue.fm/tag/%E5%87%A0%E4%BD%95/) [节日](https://kexue.fm/tag/%E8%8A%82%E6%97%A5/) [生活](https://kexue.fm/tag/%E7%94%9F%E6%B4%BB/) [文本生成](https://kexue.fm/tag/%E6%96%87%E6%9C%AC%E7%94%9F%E6%88%90/) [数论](https://kexue.fm/tag/%E6%95%B0%E8%AE%BA/)

### 随机文章

- [智能家居之手搓一套能接入米家的零冷水装置](https://kexue.fm/archives/10869)
- [利用“熄火保护 \+ 通断器”实现燃气灶智能关火](https://kexue.fm/archives/10394)
- [【外微分浅谈】2\. 反对称的威力](https://kexue.fm/archives/4054)
- [N体问题的30个周期性解](https://kexue.fm/archives/1123)
- [脑洞大开：非线性RNN居然也可以并行计算？](https://kexue.fm/archives/9783)
- [提速不掉点：基于词颗粒度的中文WoBERT](https://kexue.fm/archives/7758)
- [德国女作家摘得2009诺贝尔文学奖](https://kexue.fm/archives/183)
- [《新理解矩阵2》：矩阵是什么？](https://kexue.fm/archives/1768)
- [世界最复杂的机器11月重启,温度宇宙最低](https://kexue.fm/archives/207)
- [一道级数求和证明题(非数学归纳法)](https://kexue.fm/archives/49)

### 最近评论

- [silver](https://kexue.fm/archives/10757/comment-page-3#comment-28060): 求问文中的$e\_i$是啥？是ds论文的“Algorithm 1”中提到的“violation ...
- [Truenobility303](https://kexue.fm/archives/10739/comment-page-2#comment-28059): 谢谢苏神的详细解答！
- [Truenobility303](https://kexue.fm/archives/10795/comment-page-1#comment-28058): 不好意思我的表述可能会误导性说错了，核心问题不在2\. 我觉得问题在于整套论述都基于谱条件满足那...
- [kw](https://kexue.fm/archives/11033/comment-page-1#comment-28057): 把所有M直接换成全1矩阵就行吧，比如DeltaNet变成$(QK^⊤)(I+KK^⊤⊙(1-I...
- [WB](https://kexue.fm/archives/10795/comment-page-1#comment-28056): 非常清楚的blog。我有一个小问题想问一下，推导的时候用的是不等式（10），这里左边O(1)，...
- [liangzhh](https://kexue.fm/archives/11072/comment-page-1#comment-28055): 谢谢大佬的分享，感觉中间有两个手误敲错，式(9)最后应该是加号，另外是chunk而不是chuck吧？
- [lidhrandom](https://kexue.fm/archives/11072/comment-page-1#comment-28054): Equation 3的等号右侧第二项的第一个${\\Lambda^{-1}}$疑似不应取逆
- [Kuo](https://kexue.fm/archives/11033/comment-page-1#comment-28053): 在 $PaTH$ 论文章节 \`UT Transform for Products of Hou...
- [Fanhao](https://kexue.fm/archives/10542/comment-page-1#comment-28052): 假定Hessian阵正定，那不是意味着$L(\\theta)$是$\\theta$的凸函数吗？这一...
- [曲笑一](https://kexue.fm/archives/10739/comment-page-2#comment-28051): 对于第一个疑问，我看到分布式的版本已经开源。我在想如果将每个梯度矩阵G拆分为N\*N,再利用mu...

### 友情链接

- [Cool Papers](https://papers.cool)
- [数学研发](https://bbs.emath.ac.cn)
- [Seatop](http://www.seatop.com.cn/)
- [Xiaoxia](https://xiaoxia.org/)
- [积分表-网络版](https://kexue.fm/sci/integral/index.html)
- [丝路博傲](http://blog.dvxj.com/)
- [ph4ntasy 饭特稀](http://www.ph4ntasy.com/)
- [数学之家](http://www.2math.cn/)
- [有趣天文奇观](http://interesting-sky.china-vo.org/)
- [TwistedW](http://www.twistedwg.com/)
- [godweiyang](https://godweiyang.com/)
- [AI柠檬](https://blog.ailemon.net/)
- [王登科-DK博客](https://greatdk.com)
- [ESON](https://blog.eson.org/)
- [枫之羽](https://fzhiy.net/)
- [Mathor's blog](https://wmathor.com/)
- [coding-zuo](https://coding-zuo.github.io/)
- [博科园](https://www.bokeyuan.net/)
- [孔皮皮的博客](https://www.kppkkp.top/)
- [运鹏的博客](https://yunpengtai.top/)
- [jiming.site](https://jiming.site/)
- [OmegaXYZ](https://www.omegaxyz.com/)
- [Blog by Eacls](https://www.eacls.top/)
- [EAI猩球](https://www.robotech.ink/)
- [文举的博客](https://liwenju0.com/)
- [用代码打点酱油](https://bruceyuan.com/)
- [申请链接](https://kexue.fm/links.html)

本站采用创作共用版权协议，要求署名、非商业用途和保持一致。转载本站内容必须也遵循“ [署名-非商业用途-保持一致](http://creativecommons.org/licenses/by-nc-nd/2.5/cn/)”的创作共用协议。
© 2009-2025 Scientific Spaces. All rights reserved. Theme by [laogui](http://www.laogui.com). Powered by [Typecho](http://typecho.org). 备案号: [粤ICP备09093259号-1/2](https://beian.miit.gov.cn/)。