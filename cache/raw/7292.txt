现在可以用Keras玩中文GPT2了（GPT2\_ML） - 科学空间|Scientific Spaces
![MobileSideBar](https://kexue.fm/usr/themes/geekg/images/slide-button.png "MobileSideBar")
## SEARCH
## MENU
* [打赏](https://kexue.fm/reward.html)
* [公式](https://kexue.fm/latex.html)
* [天象](https://kexue.fm/ac.html)
* [链接](https://kexue.fm/links.html)
* [时光](https://kexue.fm/me.html)
* [博览](https://kexue.fm/science.html)
* [归档](https://kexue.fm/content.html)
## CATEGORIES
* [千奇百怪](https://kexue.fm/category/Everything)
* [天文探索](https://kexue.fm/category/Astronomy)
* [数学研究](https://kexue.fm/category/Mathematics)
* [物理化学](https://kexue.fm/category/Phy-chem)
* [信息时代](https://kexue.fm/category/Big-Data)
* [生物自然](https://kexue.fm/category/Biology)
* [图片摄影](https://kexue.fm/category/Photograph)
* [问题百科](https://kexue.fm/category/Questions)
* [生活/情感](https://kexue.fm/category/Life-Feeling)
* [资源共享](https://kexue.fm/category/Resources)
## NEWPOSTS
* [让炼丹更科学一些（五）：基于梯度精...](https://kexue.fm/archives/11530)
* [让炼丹更科学一些（四）：新恒等式，...](https://kexue.fm/archives/11494)
* [为什么DeltaNet要加L2 N...](https://kexue.fm/archives/11486)
* [让炼丹更科学一些（三）：SGD的终...](https://kexue.fm/archives/11480)
* [让炼丹更科学一些（二）：将结论推广...](https://kexue.fm/archives/11469)
* [滑动平均视角下的权重衰减和学习率](https://kexue.fm/archives/11459)
* [生成扩散模型漫谈（三十一）：预测数...](https://kexue.fm/archives/11428)
* [Muon优化器指南：快速上手与关键细节](https://kexue.fm/archives/11416)
* [AdamW的Weight RMS的...](https://kexue.fm/archives/11404)
* [n个正态随机数的最大值的渐近估计](https://kexue.fm/archives/11390)
## COMMENTS
* [Rapture D: 我有一个问题，为什么不考虑亥姆霍兹定理和斯托克斯公式。](https://kexue.fm/archives/11530/comment-page-1#comment-29104)
* [mofheka: 苏神是还在用jax是么？最近在做基于Google Pathwa...](https://kexue.fm/archives/11390/comment-page-1#comment-29103)
* [长琴: 看懂这篇博客也不是一件容易的事情。](https://kexue.fm/archives/11530/comment-page-1#comment-29102)
* [AlexLi: 苏老师，请教一下(7)式中将 $\\mu(x\_t)$ 传给$p...](https://kexue.fm/archives/9257/comment-page-4#comment-29101)
* [tyler\_zxc: "Performer的思想是将标准的Attention线性化，...](https://kexue.fm/archives/7921/comment-page-2#comment-29100)
* [我: 似乎并非mHC提出矩阵的思想？之前hyper connecti...](https://kexue.fm/archives/11494/comment-page-1#comment-29099)
* [winter: 苏神您好，假如对于比较均匀的attention weightP...](https://kexue.fm/archives/10847/comment-page-1#comment-29098)
* [苏剑林: KL散度、JS散度、W距离啥的，都行啊，看你喜欢哪个](https://kexue.fm/archives/8512/comment-page-2#comment-29097)
* [苏剑林: 没有绝对公平的对比方法，主要看你关心什么。比如，如果只关心推理...](https://kexue.fm/archives/9119/comment-page-14#comment-29096)
* [苏剑林: 如果我有时间重新搭建博客，应该会用python自己写了，而不用...](https://kexue.fm/links.html/comment-page-6#comment-29095)
## USERLOGIN
* [登录](https://kexue.fm/admin/login.php)
[科学空间|Scientific Spaces](https://kexue.fm)
* [登录](https://kexue.fm/admin/login.php)
* [打赏](https://kexue.fm/reward.html)
* [公式](https://kexue.fm/latex.html)
* [天象](https://kexue.fm/ac.html)
* [链接](https://kexue.fm/links.html)
* [时光](https://kexue.fm/me.html)
* [博览](https://kexue.fm/science.html)
* [归档](https://kexue.fm/content.html)
渴望成为一个小飞侠* [![](https://kexue.fm/usr/themes/geekg/images/rss.png)
欢迎订阅](https://kexue.fm/feed)
* [![](https://kexue.fm/usr/themes/geekg/images/mail.png)
个性邮箱](https://kexue.fm/archives/119)
* [![](https://kexue.fm/usr/themes/geekg/images/Saturn.png)
天象信息](https://kexue.fm/ac.html)
* [![](https://kexue.fm/usr/themes/geekg/images/iss.png)
观测ISS](https://kexue.fm/archives/41)
* [![](https://kexue.fm/usr/themes/geekg/images/pi.png)
LaTeX](https://kexue.fm/latex.html)
* [![](https://kexue.fm/usr/themes/geekg/images/mlogo.png)
关于博主](https://kexue.fm/me.html)
欢迎访问“科学空间”，这里将与您共同探讨自然科学，回味人生百态；也期待大家的分享～* [**千奇百怪**Everything](https://kexue.fm/category/Everything)
* [**天文探索**Astronomy](https://kexue.fm/category/Astronomy)
* [**数学研究**Mathematics](https://kexue.fm/category/Mathematics)
* [**物理化学**Phy-chem](https://kexue.fm/category/Phy-chem)
* [**信息时代**Big-Data](https://kexue.fm/category/Big-Data)
* [**生物自然**Biology](https://kexue.fm/category/Biology)
* [**图片摄影**Photograph](https://kexue.fm/category/Photograph)
* [**问题百科**Questions](https://kexue.fm/category/Questions)
* [**生活/情感**Life-Feeling](https://kexue.fm/category/Life-Feeling)
* [**资源共享**Resources](https://kexue.fm/category/Resources)
* [**千奇百怪**](https://kexue.fm/category/Everything)
* [**天文探索**](https://kexue.fm/category/Astronomy)
* [**数学研究**](https://kexue.fm/category/Mathematics)
* [**物理化学**](https://kexue.fm/category/Phy-chem)
* [**信息时代**](https://kexue.fm/category/Big-Data)
* [**生物自然**](https://kexue.fm/category/Biology)
* [**图片摄影**](https://kexue.fm/category/Photograph)
* [**问题百科**](https://kexue.fm/category/Questions)
* [**生活/情感**](https://kexue.fm/category/Life-Feeling)
* [**资源共享**](https://kexue.fm/category/Resources)
[首页](https://kexue.fm)[信息时代](https://kexue.fm/category/Big-Data)现在可以用Keras玩中文GPT2了（GPT2\_ML）
16Mar
# [现在可以用Keras玩中文GPT2了（GPT2\_ML）](https://kexue.fm/archives/7292)
By苏剑林|2020-03-16|125009位读者|:
前段时间留意到有大牛开源了一个中文的GPT2模型，是最大的15亿参数规模的，看作者给的demo，生成效果还是蛮惊艳的，就想着加载到自己的[bert4keras](https://github.com/bojone/bert4keras)来玩玩。不过早期的bert4keras整体架构写得比较“死”，集成多个不同的模型很不方便。前两周终于看不下去了，把bert4keras的整体结构重写了一遍，现在的bert4keras总能算比较灵活地编写各种Transformer结构的模型了，比如**GPT2**、**T5**等都已经集成在里边了。
## GPT2科普[#](#GPT2科普)
GPT，相信很多读者都听说过它了，简单来说，它就是一个基于Transformer结构的语言模型，源自论文[《GPT：Improving Language Understanding by Generative Pre-Training》](https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf)，但它又不是为了做语言模型而生，它是通过语言模型来预训练自身，然后在下游任务微调，提高下游任务的表现。它是“Transformer + 预训练+ 微调”这种模式的先驱者，相对而言，BERT都算是它的“后辈”，而GPT2，则是GPT的升级版——模型更大，训练数据更多——模型最大版的参数量达到了15亿。
## 中文版[#](#中文版)
看过GPT2的科普推文的读者，多数都会被它的生成效果所惊艳。不过再好也是别人家的语言，OpenAI并没有帮忙训练中文版。不过好消息是，一个叫[GPT2\_ML](https://github.com/imcaspar/gpt2-ml)的项目开源了一个中文版的GPT2，而且还是最大的15亿参数级别的模型。
目前bert4keras集成的GPT2，正是GPT2\_ML项目给出的，而不是OpenAI的那个，毕竟bert4keras优先服务中文版。值得指出的是，GPT2\_ML的模型结构，跟OpenAI版的GPT2结构并不一样，也跟BERT的结构有所不同，三者的Block对比如下：
[![官方GPT2的Block示意图](https://kexue.fm/usr/uploads/2020/03/2257367993.png)](https://kexue.fm/usr/uploads/2020/03/2257367993.png)
官方GPT2的Block示意图
[![BERT的Block示意图](https://kexue.fm/usr/uploads/2020/03/3372936810.png)](https://kexue.fm/usr/uploads/2020/03/3372936810.png)
BERT的Block示意图
[![GPT2_ML的Block示意图](https://kexue.fm/usr/uploads/2020/03/2373952967.png)](https://kexue.fm/usr/uploads/2020/03/2373952967.png)
GPT2\_ML的Block示意图
## 测试一下[#](#测试一下)
首先，下载模型权重，地址为：> 链接: [> https://pan.baidu.com/s/1OXBd16o82SpIzu57kwA8Mg
](https://pan.baidu.com/s/1OXBd16o82SpIzu57kwA8Mg)> 提取码: q79r
 > 其中，主文件“model.ckpt-100000.data-00000-of-00001”还可以从
[> Google Drive
](https://drive.google.com/file/d/1IzWpQ6I2IgfV7CldZvFJnZ9byNDZdO4n)> 下载。下载完成后请检查一下model.ckpt-100000.data-00000-of-00001的SHA256（4a6e5124df8db7ac2bdd902e6191b807a6983a7f5d09fb10ce011f9a073b183e）。
然后安装不低于0.6.0版本的bert4keras（当前最新版），就可以跑下述测试代码了（如果因为版本迭代原因导致下述代码已经过期，那么请到[basic\_language\_model\_gpt2\_ml.py](https://github.com/bojone/bert4keras/blob/master/examples/basic_language_model_gpt2_ml.py)查看最新版本）：
```
`#! -\*- coding: utf-8 -\*- # 基本测试：中文GPT2模型 # 介绍链接：https://kexue.fm/archives/7292 import numpy as np from bert4keras.models import build\_transformer\_model from bert4keras.tokenizers import Tokenizer from bert4keras.snippets import AutoRegressiveDecoder from bert4keras.snippets import uniout config\_path = '/root/gpt2/config.json' checkpoint\_path = '/root/gpt2/model.ckpt-100000' dict\_path = '/root/gpt2/vocab.txt' tokenizer = Tokenizer(dict\_path, token\_start=None, token\_end=None, do\_lower\_case=True) # 建立分词器model = build\_transformer\_model(config\_path=config\_path, checkpoint\_path=checkpoint\_path, model='gpt2\_ml') # 建立模型，加载权重class ArticleCompletion(AutoRegressiveDecoder): """基于随机采样的文章续写 """ @AutoRegressiveDecoder.set\_rtype('probas') def predict(self, inputs, output\_ids, step): token\_ids = np.concatenate([inputs[0], output\_ids], 1) return model.predict(token\_ids)[:, -1] def generate(self, text, n=1, topk=5): token\_ids, \_ = tokenizer.encode(text) results = self.random\_sample([token\_ids], n, topk) # 基于随机采样return [text + tokenizer.decode(ids) for ids in results] article\_completion = ArticleCompletion(start\_id=None, end\_id=511, # 511是中文句号 maxlen=256, minlen=128) print(article\_completion.generate(u'今天天气不错'))`
```
部分结果：> >>> article_completion.generate(u'今天天气不错')
> [u'今天天气不错，可以去跑步。昨晚看了一个关于跑步的纪录片，里面的女主讲述的是一个女孩子的成长，很励志，也很美丽。我也想跑，但是我不知道跑步要穿运动鞋，所以就买了一双运动鞋。这个纪录片是关于运动鞋的，有一 集讲了一个女孩子，从小学开始就没有穿过运动鞋，到了高中才开始尝试跑步。']
> >>> article_completion.generate(u'双十一')
> [u'双十一马上就要到了！你还在为双11的物流配送而担心吗？你还在为没时间去仓库取货而发愁吗？你还在为不知道怎么买到便宜货而发愁吗？你还在为买不到心仪的产品而懊恼吗？那么，双十一就来了！今天小编带你来看看这些 快递，都是怎么送货的！1. 物流配送快递公司的配送，主要是由快递公司负责，快递公司负责派件，物流服务。']
> >>> article_completion.generate(u'科学空间')
> [u'科学空间站科学空间站（英文：science space station），是中华人民共和国的一个空间站。该空间站是中国科学院大连物理研究所研制，主要研发和使用中国科学院大连物理研究所的核能动力空间站。科学空间站位于北京市海淀区，距离地面393米，总建筑面积约为1万平方米，总投资约为5亿元人民币。科学空间站于2018年12月26日开始动工，2021年6月建成并投入使用。']
是不是感觉还挺好的？## 想Finetune？[#](#想Finetune？)
看到这效果，估计不少读者的想法就是：怎么用到我自己的模型上，能不能在我自己的任务上finetune一下？
不好意思，告诉大家一个比较悲观的结果：笔者用Adam优化器在公司的22G显存的TITAN RTX上测试finetune这个参数规模接近15亿的GPT2，发现batch\_size=1都没法跑起来...最后发现，只有在AdaFactor优化器下，才能Finetune得动它。
关于AdaFactor，后面有机会再写文章讨论。
***转载到请包括本文地址：** [https://kexue.fm/archives/7292](https://kexue.fm/archives/7292)*
***更详细的转载事宜请参考：*** [《科学空间FAQ》](https://kexue.fm/archives/6508#文章如何转载/引用)
**如果您还有什么疑惑或建议，欢迎在下方评论区继续讨论。**
**如果您觉得本文还不错，欢迎[分享](#share)/[打赏](#pay)本文。打赏并非要从中获得收益，而是希望知道科学空间获得了多少读者的真心关注。当然，如果你无视它，也不会影响你的阅读。再次表示欢迎和感谢！**
打赏![科学空间](https://kexue.fm/usr/themes/geekg/payment/wx.png)
微信打赏![科学空间](https://kexue.fm/usr/themes/geekg/payment/zfb.png)
支付宝打赏因为网站后台对打赏并无记录，因此欢迎在打赏时候备注留言。 你还可以[**点击这里**](http://mail.qq.com/cgi-bin/qm_share?t=qm_mailme&email=tN7d1drY3drrx8H0xcWa19vZ)或在下方评论区留言来告知你的建议或需求。
**如果您需要引用本文，请参考：**
苏剑林. (Mar. 16, 2020). 《现在可以用Keras玩中文GPT2了（GPT2\_ML） 》[Blog post]. Retrieved from[https://kexue.fm/archives/7292](https://kexue.fm/archives/7292)
@online{kexuefm-7292,
title={现在可以用Keras玩中文GPT2了（GPT2\_ML）},
author={苏剑林},
year={2020},
month={Mar},
url={\\url{https://kexue.fm/archives/7292}},
}
分类：[信息时代](https://kexue.fm/category/Big-Data) 标签：[语言模型](https://kexue.fm/tag/语言模型/),[NLP](https://kexue.fm/tag/NLP/),[文本生成](https://kexue.fm/tag/文本生成/),[attention](https://kexue.fm/tag/attention/)[31 评论](https://kexue.fm/archives/7292#comments)
&lt;[Seq2Seq中Exposure Bias现象的浅析与对策](https://kexue.fm/archives/7259)|[AdaFactor优化器浅析（附开源实现）](https://kexue.fm/archives/7302)&gt;
### 你也许还对下面的内容感兴趣* [为什么DeltaNet要加L2 Normalize？](https://kexue.fm/archives/11486)
* [低精度Attention可能存在有偏的舍入误差](https://kexue.fm/archives/11371)
* [为什么线性注意力要加Short Conv？](https://kexue.fm/archives/11320)
* [QK-Clip：让Muon在Scaleup之路上更进一步](https://kexue.fm/archives/11126)
* [Transformer升级之路：21、MLA好在哪里?（下）](https://kexue.fm/archives/11111)
* [“对角+低秩”三角阵的高效求逆方法](https://kexue.fm/archives/11072)
* [线性注意力简史：从模仿、创新到反哺](https://kexue.fm/archives/11033)
* [Transformer升级之路：20、MLA好在哪里?（上）](https://kexue.fm/archives/10907)
* [Transformer升级之路：19、第二类旋转位置编码](https://kexue.fm/archives/10862)
* [细水长flow之TARFLOW：流模型满血归来？](https://kexue.fm/archives/10667)
[发表你的看法](#comment_form)
1. [&laquo;](https://kexue.fm/archives/7292/comment-page-1#comments)
2. [1](https://kexue.fm/archives/7292/comment-page-1#comments)
3. [2](https://kexue.fm/archives/7292/comment-page-2#comments)
dakkor
September 7th, 2020
大佬你好，能不能介绍一下怎么finetune？
[回复评论](https://kexue.fm/archives/7292/comment-page-2?replyTo=14265#respond-post-7292)
[苏剑林](https://kexue.fm)发表于 September 7th, 2020
稍微学点keras就知道怎么finetune了。finetune这个gpt2的核心难度不是在于技术，而是在于硬件，文章已经谈及了一下了。
[回复评论](https://kexue.fm/archives/7292/comment-page-2?replyTo=14266#respond-post-7292)
zhe wang
March 6th, 2021
苏老师您好，学习了，非常感谢。请问一下，如果我希望能从您这个函数，改成一个可以实时调用的http服务，应该如何实现呢？比如基于Flask，谢谢啊！
[回复评论](https://kexue.fm/archives/7292/comment-page-2?replyTo=15696#respond-post-7292)
[苏剑林](https://kexue.fm)发表于 March 8th, 2021
bert4keras自带了个简单的web sever的demo，可以参考：https://github.com/bojone/bert4keras/blob/master/examples/basic\_simple\_web\_serving\_simbert.py
如果你需要其他的，请自行学习和实现，我也没法教你...
[回复评论](https://kexue.fm/archives/7292/comment-page-2?replyTo=15697#respond-post-7292)
1. [&laquo;](https://kexue.fm/archives/7292/comment-page-1#comments)
2. [1](https://kexue.fm/archives/7292/comment-page-1#comments)
3. [2](https://kexue.fm/archives/7292/comment-page-2#comments)
[取消回复](https://kexue.fm/archives/7292#respond-post-7292)
你的大名电子邮箱个人网站（选填）1. 可以使用LaTeX代码，点击“预览效果”可查看效果；
2. 可以通过点击评论楼层编号来引用该楼层；3. 网站可能会有点卡，如非确认评论失败，请**不要重复点击提交**。
********************
### 内容速览4. [GPT2科普](#GPT2科普)
5. [中文版](#中文版)
6. [测试一下](#测试一下)
7. [想Finetune？](#想Finetune？)
********************
### 智能搜索支持整句搜索！网站自动使用[结巴分词](https://github.com/fxsjy/jieba)进行分词，并结合ngrams排序算法给出合理的搜索结果。
********************
### 热门标签[生成模型](https://kexue.fm/tag/生成模型/)[attention](https://kexue.fm/tag/attention/)[优化](https://kexue.fm/tag/优化/)[语言模型](https://kexue.fm/tag/语言模型/)[模型](https://kexue.fm/tag/模型/)[梯度](https://kexue.fm/tag/梯度/)[网站](https://kexue.fm/tag/网站/)[概率](https://kexue.fm/tag/概率/)[矩阵](https://kexue.fm/tag/矩阵/)[优化器](https://kexue.fm/tag/优化器/)[转载](https://kexue.fm/tag/转载/)[微分方程](https://kexue.fm/tag/微分方程/)[分析](https://kexue.fm/tag/分析/)[天象](https://kexue.fm/tag/天象/)[深度学习](https://kexue.fm/tag/深度学习/)[积分](https://kexue.fm/tag/积分/)[python](https://kexue.fm/tag/python/)[扩散](https://kexue.fm/tag/扩散/)[力学](https://kexue.fm/tag/力学/)[无监督](https://kexue.fm/tag/无监督/)[几何](https://kexue.fm/tag/几何/)[节日](https://kexue.fm/tag/节日/)[生活](https://kexue.fm/tag/生活/)[文本生成](https://kexue.fm/tag/文本生成/)[数论](https://kexue.fm/tag/数论/)
********************
********************
### 随机文章* [【备忘】维基百科与DNSCrypt](https://kexue.fm/archives/3327)
* [当Bert遇上Keras：这可能是Bert最简单的打开姿势](https://kexue.fm/archives/6736)
* [谷歌搜索退出中国内地](https://kexue.fm/archives/556)
* [fashion-mnist的gan玩具](https://kexue.fm/archives/4540)
* [深度学习的互信息：无监督提取特征](https://kexue.fm/archives/6024)
* [两个多元正态分布的KL散度、巴氏距离和W距离](https://kexue.fm/archives/8512)
* [2012北约自主招生数学](https://kexue.fm/archives/1551)
* [域名Sci-Cn.cn转让...](https://kexue.fm/archives/301)
* [开学啦！咱们来做完形填空～（讯飞杯）](https://kexue.fm/archives/4564)
* [【搜出来的文本】⋅（二）从MCMC到模拟退火](https://kexue.fm/archives/8084)
********************
********************
### 最近评论* [Rapture D](https://kexue.fm/archives/11530/comment-page-1#comment-29104): 我有一个问题，为什么不考虑亥姆霍兹定理和斯托克斯公式。* [mofheka](https://kexue.fm/archives/11390/comment-page-1#comment-29103): 苏神是还在用jax是么？最近在做基于Google Pathway的理念做一个动态版的MPMD框...
* [长琴](https://kexue.fm/archives/11530/comment-page-1#comment-29102): 看懂这篇博客也不是一件容易的事情。* [AlexLi](https://kexue.fm/archives/9257/comment-page-4#comment-29101): 苏老师，请教一下(7)式中将 $\\mu(x\_t)$ 传给$p\_o$ 进行推理的操作。$x\_...
* [tyler\_zxc](https://kexue.fm/archives/7921/comment-page-2#comment-29100): "Performer的思想是将标准的Attention线性化，所以为什么不干脆直接训练一个线性...
* [我](https://kexue.fm/archives/11494/comment-page-1#comment-29099): 似乎并非mHC提出矩阵的思想？之前hyper connection就是了
* [winter](https://kexue.fm/archives/10847/comment-page-1#comment-29098): 苏神您好，假如对于比较均匀的attention weightP，往往呈现long tail分布...
* [苏剑林](https://kexue.fm/archives/8512/comment-page-2#comment-29097): KL散度、JS散度、W距离啥的，都行啊，看你喜欢哪个
* [苏剑林](https://kexue.fm/archives/9119/comment-page-14#comment-29096): 没有绝对公平的对比方法，主要看你关心什么。比如，如果只关心推理成本和推理效果，那么有的方法可以...
* [苏剑林](https://kexue.fm/links.html/comment-page-6#comment-29095): 如果我有时间重新搭建博客，应该会用python自己写了，而不用第三方架构，这样可玩性好很多。事...
********************
********************
### 友情链接* [Cool Papers](https://papers.cool)
* [数学研发](https://bbs.emath.ac.cn)
* [Seatop](http://www.seatop.com.cn/)
* [Xiaoxia](https://xiaoxia.org/)
* [积分表-网络版](https://kexue.fm/sci/integral/index.html)
* [丝路博傲](http://blog.dvxj.com/)
* [数学之家](http://www.2math.cn/)
* [有趣天文奇观](http://interesting-sky.china-vo.org/)
* [TwistedW](http://www.twistedwg.com/)
* [godweiyang](https://godweiyang.com/)
* [AI柠檬](https://blog.ailemon.net/)
* [王登科-DK博客](https://greatdk.com)
* [ESON](https://blog.eson.org/)
* [枫之羽](https://fzhiy.net/)
* [coding-zuo](https://coding-zuo.github.io/)
* [博科园](https://www.bokeyuan.net/)
* [孔皮皮的博客](https://www.kppkkp.top/)
* [运鹏的博客](https://yunpengtai.top/)
* [jiming.site](https://jiming.site/)
* [OmegaXYZ](https://www.omegaxyz.com/)
* [EAI猩球](https://www.robotech.ink/)
* [文举的博客](https://liwenju0.com/)
* [申请链接](https://kexue.fm/links.html)
********************
[![署名-非商业用途-保持一致](https://kexue.fm/usr/themes/geekg/images/cc.gif)](http://creativecommons.org/licenses/by-nc-nd/2.5/cn/)本站采用创作共用版权协议，要求署名、非商业用途和保持一致。转载本站内容必须也遵循“[署名-非商业用途-保持一致](http://creativecommons.org/licenses/by-nc-nd/2.5/cn/)”的创作共用协议。
©2009-2026 Scientific Spaces. All rights reserved. Theme by[laogui](http://www.laogui.com). Powered by[Typecho](http://typecho.org). 备案号:[粤ICP备09093259号-1/2](https://beian.miit.gov.cn/)。