![MobileSideBar](https://kexue.fm/usr/themes/geekg/images/slide-button.png)

## SEARCH

## MENU

- [打赏](https://kexue.fm/reward.html)
- [公式](https://kexue.fm/latex.html)
- [天象](https://kexue.fm/ac.html)
- [链接](https://kexue.fm/links.html)
- [时光](https://kexue.fm/me.html)
- [博览](https://kexue.fm/science.html)
- [归档](https://kexue.fm/content.html)

## CATEGORIES

- [千奇百怪](https://kexue.fm/category/Everything)
- [天文探索](https://kexue.fm/category/Astronomy)
- [数学研究](https://kexue.fm/category/Mathematics)
- [物理化学](https://kexue.fm/category/Phy-chem)
- [信息时代](https://kexue.fm/category/Big-Data)
- [生物自然](https://kexue.fm/category/Biology)
- [图片摄影](https://kexue.fm/category/Photograph)
- [问题百科](https://kexue.fm/category/Questions)
- [生活/情感](https://kexue.fm/category/Life-Feeling)
- [资源共享](https://kexue.fm/category/Resources)

## NEWPOSTS

- [让炼丹更科学一些（五）：基于梯度精...](https://kexue.fm/archives/11530)
- [让炼丹更科学一些（四）：新恒等式，...](https://kexue.fm/archives/11494)
- [为什么DeltaNet要加L2 N...](https://kexue.fm/archives/11486)
- [让炼丹更科学一些（三）：SGD的终...](https://kexue.fm/archives/11480)
- [让炼丹更科学一些（二）：将结论推广...](https://kexue.fm/archives/11469)
- [滑动平均视角下的权重衰减和学习率](https://kexue.fm/archives/11459)
- [生成扩散模型漫谈（三十一）：预测数...](https://kexue.fm/archives/11428)
- [Muon优化器指南：快速上手与关键细节](https://kexue.fm/archives/11416)
- [AdamW的Weight RMS的...](https://kexue.fm/archives/11404)
- [n个正态随机数的最大值的渐近估计](https://kexue.fm/archives/11390)

## COMMENTS

- [Bin: 今天偶然从某个论坛看到有人推荐您的博客，定睛一看竟然是华师同院...](https://kexue.fm/archives/1990/comment-page-2#comment-29105)
- [Rapture D: 我有一个问题，为什么不考虑亥姆霍兹定理和斯托克斯公式。](https://kexue.fm/archives/11530/comment-page-1#comment-29104)
- [mofheka: 苏神是还在用jax是么？最近在做基于Google Pathwa...](https://kexue.fm/archives/11390/comment-page-1#comment-29103)
- [长琴: 看懂这篇博客也不是一件容易的事情。](https://kexue.fm/archives/11530/comment-page-1#comment-29102)
- [AlexLi: 苏老师，请教一下(7)式中将 $\\mu(x\_t)$ 传给 $p...](https://kexue.fm/archives/9257/comment-page-4#comment-29101)
- [tyler\_zxc: "Performer的思想是将标准的Attention线性化，...](https://kexue.fm/archives/7921/comment-page-2#comment-29100)
- [我: 似乎并非mHC提出矩阵的思想？之前hyper connecti...](https://kexue.fm/archives/11494/comment-page-1#comment-29099)
- [winter: 苏神您好，假如对于比较均匀的attention weightP...](https://kexue.fm/archives/10847/comment-page-1#comment-29098)
- [苏剑林: KL散度、JS散度、W距离啥的，都行啊，看你喜欢哪个](https://kexue.fm/archives/8512/comment-page-2#comment-29097)
- [苏剑林: 没有绝对公平的对比方法，主要看你关心什么。比如，如果只关心推理...](https://kexue.fm/archives/9119/comment-page-14#comment-29096)

## USERLOGIN

- [登录](https://kexue.fm/admin/login.php)

[科学空间\|Scientific Spaces](https://kexue.fm/)

- [登录](https://kexue.fm/admin/login.php)
- [打赏](https://kexue.fm/reward.html)
- [公式](https://kexue.fm/latex.html)
- [天象](https://kexue.fm/ac.html)
- [链接](https://kexue.fm/links.html)
- [时光](https://kexue.fm/me.html)
- [博览](https://kexue.fm/science.html)
- [归档](https://kexue.fm/content.html)

渴望成为一个小飞侠

- [![](https://kexue.fm/usr/themes/geekg/images/rss.png)\\
\\
欢迎订阅](https://kexue.fm/feed)
- [![](https://kexue.fm/usr/themes/geekg/images/mail.png)\\
\\
个性邮箱](https://kexue.fm/archives/119)
- [![](https://kexue.fm/usr/themes/geekg/images/Saturn.png)\\
\\
天象信息](https://kexue.fm/ac.html)
- [![](https://kexue.fm/usr/themes/geekg/images/iss.png)\\
\\
观测ISS](https://kexue.fm/archives/41)
- [![](https://kexue.fm/usr/themes/geekg/images/pi.png)\\
\\
LaTeX](https://kexue.fm/latex.html)
- [![](https://kexue.fm/usr/themes/geekg/images/mlogo.png)\\
\\
关于博主](https://kexue.fm/me.html)

欢迎访问“科学空间”，这里将与您共同探讨自然科学，回味人生百态；也期待大家的分享～

- [**千奇百怪** Everything](https://kexue.fm/category/Everything)
- [**天文探索** Astronomy](https://kexue.fm/category/Astronomy)
- [**数学研究** Mathematics](https://kexue.fm/category/Mathematics)
- [**物理化学** Phy-chem](https://kexue.fm/category/Phy-chem)
- [**信息时代** Big-Data](https://kexue.fm/category/Big-Data)
- [**生物自然** Biology](https://kexue.fm/category/Biology)
- [**图片摄影** Photograph](https://kexue.fm/category/Photograph)
- [**问题百科** Questions](https://kexue.fm/category/Questions)
- [**生活/情感** Life-Feeling](https://kexue.fm/category/Life-Feeling)
- [**资源共享** Resources](https://kexue.fm/category/Resources)

- [**千奇百怪**](https://kexue.fm/category/Everything)
- [**天文探索**](https://kexue.fm/category/Astronomy)
- [**数学研究**](https://kexue.fm/category/Mathematics)
- [**物理化学**](https://kexue.fm/category/Phy-chem)
- [**信息时代**](https://kexue.fm/category/Big-Data)
- [**生物自然**](https://kexue.fm/category/Biology)
- [**图片摄影**](https://kexue.fm/category/Photograph)
- [**问题百科**](https://kexue.fm/category/Questions)
- [**生活/情感**](https://kexue.fm/category/Life-Feeling)
- [**资源共享**](https://kexue.fm/category/Resources)

[首页](https://kexue.fm/) [信息时代](https://kexue.fm/category/Big-Data) 【不可思议的Word2Vec】 2.训练好的模型

3Apr

# [【不可思议的Word2Vec】 2.训练好的模型](https://kexue.fm/archives/4304)

By 苏剑林 \|
2017-04-03 \|
570805位读者 \|

由于后面几篇要讲解Word2Vec怎么用，因此笔者先训练好了一个Word2Vec模型。为了节约读者的时间，并且保证读者可以复现后面的结果，笔者决定把这个训练好的模型分享出来，用Gensim训练的。单纯的词向量并不大，但第一篇已经说了，我们要用到完整的Word2Vec模型，因此我将完整的模型分享出来了，包含四个文件，所以文件相对大一些。

提醒读者的是，如果你想获取完整的Word2Vec模型，又不想改源代码，那么Python的Gensim库应该是你唯一的选择，据我所知，其他版本的Word2Vec最后都是只提供词向量给我们，没有完整的模型。

对于做知识挖掘来说，显然用知识库语料（如百科语料）训练的Word2Vec效果会更好。但百科语料我还在爬取中，爬完了我再训练一个模型，到时再分享。

## 模型概况 [\#](https://kexue.fm/archives/4304\#%E6%A8%A1%E5%9E%8B%E6%A6%82%E5%86%B5)

这个模型的大概情况如下：

$$\\begin{array}{c\|c}

\\hline

\\text{训练语料} & \\text{微信公众号的文章，多领域，属于中文平衡语料}\\\

\\hline

\\text{语料数量} & \\text{800万篇，总词数达到650亿}\\\

\\hline

\\text{模型词数} & \\text{共352196词，基本是中文词，包含常见英文词}\\\

\\hline

\\text{模型结构} & \\text{Skip-Gram + Huffman Softmax}\\\

\\hline

\\text{向量维度} & \\text{256维}\\\

\\hline

\\text{分词工具} & \\text{结巴分词，加入了有50万词条的词典，关闭了新词发现}\\\

\\hline

\\text{训练工具} & \\text{Gensim的Word2Vec，服务器训练了7天}\\\

\\hline

\\text{其他情况} & \\text{窗口大小为10，最小词频是64，迭代了10次}\\\

\\hline

\\end{array}$$

需要特别说明的是：公众号文章属于比较“现代”的文章，反映了近来的网络热点内容，覆盖面也比较广，因此文章相对来说还是比较典型的。对于分词，我用的是结巴分词，并且关闭了新词发现，这是宁可分少一些，也要分准一些。当然，自带的词典是不够的，笔者自己还整理了50万词条，词条来源有两部分：1、网络收集的词典合并；2、在公众号文章上做新词发现，人工筛选后加入到词典中。因此，分词的结果还算靠谱，而且包含了比较多的流行词，可用性较高。

## 训练代码 [\#](https://kexue.fm/archives/4304\#%E8%AE%AD%E7%BB%83%E4%BB%A3%E7%A0%81)

大家可以参考着改写，要注意，这里引入hashlib.md5是为了对文章进行去重（本来1000万篇文章，去重后得到800万），而这个步骤不是必要的。

```python

```

## 下载链接 [\#](https://kexue.fm/archives/4304\#%E4%B8%8B%E8%BD%BD%E9%93%BE%E6%8E%A5)

> 链接: [https://pan.baidu.com/s/1htC495U](https://pan.baidu.com/s/1htC495U) 密码: 4ff8
>
> 包含文件：word2vec\_wx, word2vec\_wx.syn1neg.npy, word2vec\_wx.syn1.npy, word2vec\_wx.wv.syn0.npy，4个文件都是Gensim加载模型所必需的。具体每个文件的含义我也没弄清楚，word2vec\_wx大概是模型声明，word2vec\_wx.wv.syn0.npy应该就是我们所说的词向量表，word2vec\_wx.syn1.npy是隐层到输出层的参数（Huffman树的参数），word2vec\_wx.syn1neg.npy就不大清楚了～
>
> 如果你只关心词向量，也可以下载C版本的格式（跟C版本Word2Vec兼容，只包含词向量）：
>
> 链接: [https://pan.baidu.com/s/1nv3ANLB](https://pan.baidu.com/s/1nv3ANLB) 密码: dgfw

## 一些演示 [\#](https://kexue.fm/archives/4304\#%E4%B8%80%E4%BA%9B%E6%BC%94%E7%A4%BA)

主要随便演示一下该模型找近义词的结果。欢迎大家提出改进建议。

> >>\> import gensim
>
> >>\> model = gensim.models.Word2Vec.load('word2vec\_wx')
>
> >>\> pd.Series(model.most\_similar(u'微信'))
>
> 0 (QQ, 0.752506196499)
>
> 1 (订阅号, 0.714340209961)
>
> 2 (QQ号, 0.695577561855)
>
> 3 (扫一扫, 0.695488214493)
>
> 4 (微信公众号, 0.694692015648)
>
> 5 (私聊, 0.681655049324)
>
> 6 (微信公众平台, 0.674170553684)
>
> 7 (私信, 0.65382117033)
>
> 8 (微信平台, 0.65175652504)
>
> 9 (官方, 0.643620729446)
>
> >>\> pd.Series(model.most\_similar(u'公众号'))
>
> 0 (订阅号, 0.782696723938)
>
> 1 (微信公众号, 0.760639667511)
>
> 2 (微信公众账号, 0.73489522934)
>
> 3 (公众平台, 0.716173946857)
>
> 4 (扫一扫, 0.697836577892)
>
> 5 (微信公众平台, 0.696847081184)
>
> 6 (置顶, 0.666775584221)
>
> 7 (公共账号, 0.665741920471)
>
> 8 (微信平台, 0.661035299301)
>
> 9 (菜单栏, 0.65234708786)
>
> >>\> pd.Series(model.most\_similar(u'牛逼'))
>
> 0 (牛掰, 0.701575636864)
>
> 1 (厉害, 0.619165301323)
>
> 2 (靠谱, 0.588266670704)
>
> 3 (苦逼, 0.586573541164)
>
> 4 (吹牛逼, 0.569260418415)
>
> 5 (了不起, 0.565731525421)
>
> 6 (牛叉, 0.563843131065)
>
> 7 (绝逼, 0.549570798874)
>
> 8 (说真的, 0.549259066582)
>
> 9 (两把刷子, 0.545115828514)
>
> >>\> pd.Series(model.most\_similar(u'广州'))
>
> 0 (东莞, 0.840889930725)
>
> 1 (深圳, 0.799216389656)
>
> 2 (佛山, 0.786817133427)
>
> 3 (惠州, 0.779960036278)
>
> 4 (珠海, 0.73523247242)
>
> 5 (厦门, 0.72509008646)
>
> 6 (武汉, 0.724122405052)
>
> 7 (汕头, 0.719602584839)
>
> 8 (增城, 0.713532209396)
>
> 9 (上海, 0.710560560226)
>
> >>\> pd.Series(model.most\_similar(u'朱元璋'))
>
> 0 (朱棣, 0.857951819897)
>
> 1 (燕王, 0.853199958801)
>
> 2 (朝廷, 0.847517609596)
>
> 3 (明太祖朱元璋, 0.837111353874)
>
> 4 (赵匡胤, 0.835654854774)
>
> 5 (称帝, 0.835589051247)
>
> 6 (起兵, 0.833530187607)
>
> 7 (明太祖, 0.829249799252)
>
> 8 (太祖, 0.826784193516)
>
> 9 (丞相, 0.826457977295)
>
> >>\> pd.Series(model.most\_similar(u'微积分'))
>
> 0 (线性代数, 0.808522999287)
>
> 1 (数学分析, 0.791161835194)
>
> 2 (高等数学, 0.786414265633)
>
> 3 (数学, 0.758676528931)
>
> 4 (概率论, 0.747221827507)
>
> 5 (高等代数, 0.737897276878)
>
> 6 (解析几何, 0.730488717556)
>
> 7 (复变函数, 0.715447306633)
>
> 8 (微分方程, 0.71503329277)
>
> 9 (微积分学, 0.704192101955)
>
> >>\> pd.Series(model.most\_similar(u'apple'))
>
> 0 (banana, 0.79927945137)
>
> 1 (pineapple, 0.789698243141)
>
> 2 (pen, 0.779583632946)
>
> 3 (orange, 0.769554674625)
>
> 4 (sweet, 0.721074819565)
>
> 5 (fruit, 0.71402490139)
>
> 6 (pie, 0.711439430714)
>
> 7 (watermelon, 0.700904607773)
>
> 8 (apples, 0.697601020336)
>
> 9 (juice, 0.694036960602)
>
> >>\> pd.Series(model.most\_similar(u'企鹅'))
>
> 0 (海豹, 0.665253281593)
>
> 1 (帝企鹅, 0.645192623138)
>
> 2 (北极熊, 0.619929730892)
>
> 3 (大象, 0.618502140045)
>
> 4 (鲸鱼, 0.606555819511)
>
> 5 (猫, 0.591019570827)
>
> 6 (蜥蜴, 0.584576964378)
>
> 7 (蓝鲸, 0.572826981544)
>
> 8 (海豚, 0.566122889519)
>
> 9 (猩猩, 0.563284397125)
>
> >>\> pd.Series(model.most\_similar(u'足球'))
>
> 0 (篮球, 0.842746257782)
>
> 1 (足球运动, 0.819511592388)
>
> 2 (青训, 0.793446540833)
>
> 3 (排球, 0.774085760117)
>
> 4 (乒乓球, 0.760577201843)
>
> 5 (足球赛事, 0.758624792099)
>
> 6 (棒垒球, 0.750351667404)
>
> 7 (篮球运动, 0.746055066586)
>
> 8 (足球队, 0.74296438694)
>
> 9 (网球, 0.742858171463)
>
> >>\> pd.Series(model.most\_similar(u'爸爸'))
>
> 0 (妈妈, 0.779690504074)
>
> 1 (儿子, 0.752222895622)
>
> 2 (奶奶, 0.70418381691)
>
> 3 (妈, 0.693783283234)
>
> 4 (爷爷, 0.683066487312)
>
> 5 (父亲, 0.673043072224)
>
> 6 (女儿, 0.670304119587)
>
> 7 (爸妈, 0.669358253479)
>
> 8 (爸, 0.663688421249)
>
> 9 (外婆, 0.652905225754)
>
> >>\> pd.Series(model.most\_similar(u'淘宝'))
>
> 0 (淘, 0.770935535431)
>
> 1 (店铺, 0.739198565483)
>
> 2 (手机端, 0.728774428368)
>
> 3 (天猫店, 0.725838780403)
>
> 4 (口令, 0.721312999725)
>
> 5 (登录淘宝, 0.717839717865)
>
> 6 (淘宝店, 0.71473968029)
>
> 7 (淘宝搜, 0.697688698769)
>
> 8 (天猫, 0.690212249756)
>
> 9 (网店, 0.6820114851)

_**转载到请包括本文地址：** [https://kexue.fm/archives/4304](https://kexue.fm/archives/4304 "【不可思议的Word2Vec】 2.训练好的模型")_

_**更详细的转载事宜请参考：**_ [《科学空间FAQ》](https://kexue.fm/archives/6508#%E6%96%87%E7%AB%A0%E5%A6%82%E4%BD%95%E8%BD%AC%E8%BD%BD/%E5%BC%95%E7%94%A8 "《科学空间FAQ》")

**如果您还有什么疑惑或建议，欢迎在下方评论区继续讨论。**

**如果您觉得本文还不错，欢迎 [分享](https://kexue.fm/archives/4304#share)/ [打赏](https://kexue.fm/archives/4304#pay) 本文。打赏并非要从中获得收益，而是希望知道科学空间获得了多少读者的真心关注。当然，如果你无视它，也不会影响你的阅读。再次表示欢迎和感谢！**

打赏

![科学空间](https://kexue.fm/usr/themes/geekg/payment/wx.png)

微信打赏

![科学空间](https://kexue.fm/usr/themes/geekg/payment/zfb.png)

支付宝打赏

因为网站后台对打赏并无记录，因此欢迎在打赏时候备注留言。

你还可以 [**点击这里**](http://mail.qq.com/cgi-bin/qm_share?t=qm_mailme&email=tN7d1drY3drrx8H0xcWa19vZ) 或在下方评论区留言来告知你的建议或需求。

**如果您需要引用本文，请参考：**

苏剑林. (Apr. 03, 2017). 《【不可思议的Word2Vec】 2.训练好的模型 》\[Blog post\]. Retrieved from [https://kexue.fm/archives/4304](https://kexue.fm/archives/4304)

@online{kexuefm-4304,

         title={【不可思议的Word2Vec】 2.训练好的模型},

         author={苏剑林},

         year={2017},

         month={Apr},

         url={\\url{https://kexue.fm/archives/4304}},

}


分类： [信息时代](https://kexue.fm/category/Big-Data)    标签： [词向量](https://kexue.fm/tag/%E8%AF%8D%E5%90%91%E9%87%8F/), [Word2Vec](https://kexue.fm/tag/Word2Vec/), [语言模型](https://kexue.fm/tag/%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/)[144 评论](https://kexue.fm/archives/4304#comments)

< [【不可思议的Word2Vec】 1.数学原理](https://kexue.fm/archives/4299 "【不可思议的Word2Vec】 1.数学原理") \| [【不可思议的Word2Vec】 3.提取关键词](https://kexue.fm/archives/4316 "【不可思议的Word2Vec】 3.提取关键词") >

### 你也许还对下面的内容感兴趣

- [Transformer升级之路：21、MLA好在哪里?（下）](https://kexue.fm/archives/11111 "Transformer升级之路：21、MLA好在哪里?（下）")
- [Transformer升级之路：20、MLA好在哪里?（上）](https://kexue.fm/archives/10907 "Transformer升级之路：20、MLA好在哪里?（上）")
- [Transformer升级之路：19、第二类旋转位置编码](https://kexue.fm/archives/10862 "Transformer升级之路：19、第二类旋转位置编码")
- [Decoder-only的LLM为什么需要位置编码？](https://kexue.fm/archives/10347 "Decoder-only的LLM为什么需要位置编码？")
- [Monarch矩阵：计算高效的稀疏型矩阵分解](https://kexue.fm/archives/10249 "Monarch矩阵：计算高效的稀疏型矩阵分解")
- [缓存与效果的极限拉扯：从MHA、MQA、GQA到MLA](https://kexue.fm/archives/10091 "缓存与效果的极限拉扯：从MHA、MQA、GQA到MLA")
- [时空之章：将Attention视为平方复杂度的RNN](https://kexue.fm/archives/10017 "时空之章：将Attention视为平方复杂度的RNN")
- [我在Performer中发现了Transformer-VQ的踪迹](https://kexue.fm/archives/9862 "我在Performer中发现了Transformer-VQ的踪迹")
- [预训练一下，Transformer的长序列成绩还能涨不少！](https://kexue.fm/archives/9787 "预训练一下，Transformer的长序列成绩还能涨不少！")
- [脑洞大开：非线性RNN居然也可以并行计算？](https://kexue.fm/archives/9783 "脑洞大开：非线性RNN居然也可以并行计算？")

[发表你的看法](https://kexue.fm/archives/4304#comment_form)

1. [«](https://kexue.fm/archives/4304/comment-page-6#comments)
2. [1](https://kexue.fm/archives/4304/comment-page-1#comments)
3. ...
4. [4](https://kexue.fm/archives/4304/comment-page-4#comments)
5. [5](https://kexue.fm/archives/4304/comment-page-5#comments)
6. [6](https://kexue.fm/archives/4304/comment-page-6#comments)
7. [7](https://kexue.fm/archives/4304/comment-page-7#comments)

Robin003

February 3rd, 2021

非常感谢！！！

[回复评论](https://kexue.fm/archives/4304/comment-page-7?replyTo=15459#respond-post-4304)

fener

February 22nd, 2021

苏神，这是我使用关键词：哈佛，使用你的word2vec预训练模型得到的相似词结果，如下图1所示：

斯坦福大学 0.7545563578605652

哥伦比亚大学 0.7417672872543335

宾夕法尼亚大学 0.7209892272949219

耶鲁大学 0.7185845375061035

波士顿大学 0.7175740003585815

加利福尼亚大学 0.7091684937477112

英国剑桥大学 0.7083733081817627

麻省理工学院 0.7058111429214478

普林斯顿大学 0.7057685256004333

牛津大学 0.7045313119888306

美国斯坦福大学 0.7040293216705322

俄亥俄州立大学 0.7002373933792114

芝加哥大学 0.6980818510055542

约翰霍普金斯大学 0.6949770450592041

杜克大学 0.6942733526229858

而我增量训练后，通过关键词：哈佛，得到的相似词为：

哈佛大学医学院 0.45154106616973877

哈佛大学 0.441599577665329

捐款 0.408735990524292

博士学位 0.40859535336494446

博士后 0.4049009680747986

拒 0.39807987213134766

捐 0.3902815878391266

卡内基梅隆大学 0.38287100195884705

潘石屹 0.38206493854522705

哈佛大学商学院 0.3776305913925171

母校 0.37715545296669006

盖茨 0.3751351833343506

斯坦福大学 0.369379460811615

宾夕法尼亚大学 0.3688274025917053

医学院 0.3683770000934601

我增量训练后，发现和未增量的相比，一些相关的学校没有了，还没有你未增量训练的好，我使用了你的nlp\_zero里面的dict分词的。

这个不知道是什么原因造成的，我是希望它能在你的基础上可以得到我数据里的一些词，但是，结果是增加了一些和我数据相关的词，但是，已有的一些相似词却没有了，有什么办法可以做到吗？望指点一下。

[回复评论](https://kexue.fm/archives/4304/comment-page-7?replyTo=15582#respond-post-4304)

[苏剑林](https://kexue.fm/) 发表于
February 22nd, 2021

有办法可以实现（比如只训练新增的词），但是word2vec自带的增量训练实现不了

[回复评论](https://kexue.fm/archives/4304/comment-page-7?replyTo=15583#respond-post-4304)

fener 发表于
February 22nd, 2021

我用的word2vec自带的增量训练做的，自带的增量训练这么鸡肋吗，好像并没有什么用【捂脸哭】。

只训练新增的词，这个怎么操作哈，我还没有见过这种操作，可以简单说下吗

[回复评论](https://kexue.fm/archives/4304/comment-page-7?replyTo=15584#respond-post-4304)

[苏剑林](https://kexue.fm/) 发表于
February 22nd, 2021

我想到的是自己实现word2vec...

[回复评论](https://kexue.fm/archives/4304/comment-page-7?replyTo=15585#respond-post-4304)

fener 发表于
February 24th, 2021

嗯嗯，了解了，感谢

[回复评论](https://kexue.fm/archives/4304/comment-page-7?replyTo=15599#respond-post-4304)

lamcrazy

October 7th, 2021

运行WordToVec = gensim.models.Word2Vec.load("word2vec\_wx")报错了

报的错是AttributeError: Can't get attribute 'Vocab' on

[回复评论](https://kexue.fm/archives/4304/comment-page-7?replyTo=17496#respond-post-4304)

lamcrazy 发表于
October 8th, 2021

发现是gensim库版本问题，降到3.8版本就可以跑通了

[回复评论](https://kexue.fm/archives/4304/comment-page-7?replyTo=17497#respond-post-4304)

瓦力

June 24th, 2022

您好，word2vec模型中的sample参数取值范围是（0，1e-5），模型默认参数是1e-3,但是1e-3是大于1e-5的，并不在（0，1e-5）范围内啊

[回复评论](https://kexue.fm/archives/4304/comment-page-7?replyTo=19347#respond-post-4304)

[苏剑林](https://kexue.fm/) 发表于
June 27th, 2022

这是个谜，我也不知道

[回复评论](https://kexue.fm/archives/4304/comment-page-7?replyTo=19360#respond-post-4304)

蓝朋友

January 29th, 2023

苏神，想求一个英文库有吗呜呜呜

[回复评论](https://kexue.fm/archives/4304/comment-page-7?replyTo=20815#respond-post-4304)

[苏剑林](https://kexue.fm/) 发表于
January 30th, 2023

我不做非中文任务，爱莫能助，抱歉。

[回复评论](https://kexue.fm/archives/4304/comment-page-7?replyTo=20826#respond-post-4304)

王震

April 23rd, 2023

! C:\\Users\\11765\\Desktop\\word2vec\\word2vec\_from\_weixin.7z: 文件 C:\\Users\\11765\\Desktop\\word2vec\_wx.syn1.npy 里出现校验和错误。该文件已损坏。

[回复评论](https://kexue.fm/archives/4304/comment-page-7?replyTo=21446#respond-post-4304)

[苏剑林](https://kexue.fm/) 发表于
April 25th, 2023

重新下载试试。

[回复评论](https://kexue.fm/archives/4304/comment-page-7?replyTo=21464#respond-post-4304)

1. [«](https://kexue.fm/archives/4304/comment-page-6#comments)
2. [1](https://kexue.fm/archives/4304/comment-page-1#comments)
3. ...
4. [4](https://kexue.fm/archives/4304/comment-page-4#comments)
5. [5](https://kexue.fm/archives/4304/comment-page-5#comments)
6. [6](https://kexue.fm/archives/4304/comment-page-6#comments)
7. [7](https://kexue.fm/archives/4304/comment-page-7#comments)

[取消回复](https://kexue.fm/archives/4304#respond-post-4304)

你的大名

电子邮箱

个人网站（选填）

1\. 可以使用LaTeX代码，点击“预览效果”可查看效果；

2\. 可以通过点击评论楼层编号来引用该楼层；

3\. 网站可能会有点卡，如非确认评论失败，请 **不要重复点击提交**。

### 内容速览

[模型概况](https://kexue.fm/archives/4304#%E6%A8%A1%E5%9E%8B%E6%A6%82%E5%86%B5)
[训练代码](https://kexue.fm/archives/4304#%E8%AE%AD%E7%BB%83%E4%BB%A3%E7%A0%81)
[下载链接](https://kexue.fm/archives/4304#%E4%B8%8B%E8%BD%BD%E9%93%BE%E6%8E%A5)
[一些演示](https://kexue.fm/archives/4304#%E4%B8%80%E4%BA%9B%E6%BC%94%E7%A4%BA)

### 智能搜索

支持整句搜索！网站自动使用 [结巴分词](https://github.com/fxsjy/jieba) 进行分词，并结合ngrams排序算法给出合理的搜索结果。

### 热门标签

[生成模型](https://kexue.fm/tag/%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B/) [attention](https://kexue.fm/tag/attention/) [优化](https://kexue.fm/tag/%E4%BC%98%E5%8C%96/) [语言模型](https://kexue.fm/tag/%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/) [模型](https://kexue.fm/tag/%E6%A8%A1%E5%9E%8B/) [梯度](https://kexue.fm/tag/%E6%A2%AF%E5%BA%A6/) [网站](https://kexue.fm/tag/%E7%BD%91%E7%AB%99/) [概率](https://kexue.fm/tag/%E6%A6%82%E7%8E%87/) [矩阵](https://kexue.fm/tag/%E7%9F%A9%E9%98%B5/) [优化器](https://kexue.fm/tag/%E4%BC%98%E5%8C%96%E5%99%A8/) [转载](https://kexue.fm/tag/%E8%BD%AC%E8%BD%BD/) [微分方程](https://kexue.fm/tag/%E5%BE%AE%E5%88%86%E6%96%B9%E7%A8%8B/) [分析](https://kexue.fm/tag/%E5%88%86%E6%9E%90/) [天象](https://kexue.fm/tag/%E5%A4%A9%E8%B1%A1/) [深度学习](https://kexue.fm/tag/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/) [积分](https://kexue.fm/tag/%E7%A7%AF%E5%88%86/) [python](https://kexue.fm/tag/python/) [扩散](https://kexue.fm/tag/%E6%89%A9%E6%95%A3/) [力学](https://kexue.fm/tag/%E5%8A%9B%E5%AD%A6/) [无监督](https://kexue.fm/tag/%E6%97%A0%E7%9B%91%E7%9D%A3/) [几何](https://kexue.fm/tag/%E5%87%A0%E4%BD%95/) [节日](https://kexue.fm/tag/%E8%8A%82%E6%97%A5/) [生活](https://kexue.fm/tag/%E7%94%9F%E6%B4%BB/) [文本生成](https://kexue.fm/tag/%E6%96%87%E6%9C%AC%E7%94%9F%E6%88%90/) [数论](https://kexue.fm/tag/%E6%95%B0%E8%AE%BA/)

### 随机文章

- [分享：孟岩的《理解矩阵》一文](https://kexue.fm/archives/1754)
- [局部余弦相似度大，全局余弦相似度一定也大吗？](https://kexue.fm/archives/9931)
- [CDS星表库](https://kexue.fm/archives/364)
- [百科翻译：盐酸的历史（氯化氢，HCl）](https://kexue.fm/archives/9)
- [桃花开放了](https://kexue.fm/archives/1548)
- [“战神”升空看它到底有多神？](https://kexue.fm/archives/229)
- [变分与理论力学略览](https://kexue.fm/archives/1304)
- [概率分布的熵归一化（Entropy Normalization）](https://kexue.fm/archives/8829)
- [从梯度最大化看Attention的Scale操作](https://kexue.fm/archives/9812)
- [费曼积分法——积分符号内取微分(3)](https://kexue.fm/archives/1629)

### 最近评论

- [Bin](https://kexue.fm/archives/1990/comment-page-2#comment-29105): 今天偶然从某个论坛看到有人推荐您的博客，定睛一看竟然是华师同院的往届师兄！看到这篇2013年的...
- [Rapture D](https://kexue.fm/archives/11530/comment-page-1#comment-29104): 我有一个问题，为什么不考虑亥姆霍兹定理和斯托克斯公式。
- [mofheka](https://kexue.fm/archives/11390/comment-page-1#comment-29103): 苏神是还在用jax是么？最近在做基于Google Pathway的理念做一个动态版的MPMD框...
- [长琴](https://kexue.fm/archives/11530/comment-page-1#comment-29102): 看懂这篇博客也不是一件容易的事情。
- [AlexLi](https://kexue.fm/archives/9257/comment-page-4#comment-29101): 苏老师，请教一下(7)式中将 $\\mu(x\_t)$ 传给 $p\_o$ 进行推理的操作。 $x\_...
- [tyler\_zxc](https://kexue.fm/archives/7921/comment-page-2#comment-29100): "Performer的思想是将标准的Attention线性化，所以为什么不干脆直接训练一个线性...
- [我](https://kexue.fm/archives/11494/comment-page-1#comment-29099): 似乎并非mHC提出矩阵的思想？之前hyper connection就是了
- [winter](https://kexue.fm/archives/10847/comment-page-1#comment-29098): 苏神您好，假如对于比较均匀的attention weightP，往往呈现long tail分布...
- [苏剑林](https://kexue.fm/archives/8512/comment-page-2#comment-29097): KL散度、JS散度、W距离啥的，都行啊，看你喜欢哪个
- [苏剑林](https://kexue.fm/archives/9119/comment-page-14#comment-29096): 没有绝对公平的对比方法，主要看你关心什么。比如，如果只关心推理成本和推理效果，那么有的方法可以...

### 友情链接

- [Cool Papers](https://papers.cool/)
- [数学研发](https://bbs.emath.ac.cn/)
- [Seatop](http://www.seatop.com.cn/)
- [Xiaoxia](https://xiaoxia.org/)
- [积分表-网络版](https://kexue.fm/sci/integral/index.html)
- [丝路博傲](http://blog.dvxj.com/)
- [数学之家](http://www.2math.cn/)
- [有趣天文奇观](http://interesting-sky.china-vo.org/)
- [TwistedW](http://www.twistedwg.com/)
- [godweiyang](https://godweiyang.com/)
- [AI柠檬](https://blog.ailemon.net/)
- [王登科-DK博客](https://greatdk.com/)
- [ESON](https://blog.eson.org/)
- [枫之羽](https://fzhiy.net/)
- [coding-zuo](https://coding-zuo.github.io/)
- [博科园](https://www.bokeyuan.net/)
- [孔皮皮的博客](https://www.kppkkp.top/)
- [运鹏的博客](https://yunpengtai.top/)
- [jiming.site](https://jiming.site/)
- [OmegaXYZ](https://www.omegaxyz.com/)
- [EAI猩球](https://www.robotech.ink/)
- [文举的博客](https://liwenju0.com/)
- [申请链接](https://kexue.fm/links.html)

[![署名-非商业用途-保持一致](https://kexue.fm/usr/themes/geekg/images/cc.gif)](http://creativecommons.org/licenses/by-nc-nd/2.5/cn/) 本站采用创作共用版权协议，要求署名、非商业用途和保持一致。转载本站内容必须也遵循“ [署名-非商业用途-保持一致](http://creativecommons.org/licenses/by-nc-nd/2.5/cn/)”的创作共用协议。



© 2009-2026 Scientific Spaces. All rights reserved. Theme by [laogui](http://www.laogui.com/). Powered by [Typecho](http://typecho.org/). 备案号: [粤ICP备09093259号-1/2](https://beian.miit.gov.cn/ "粤ICP备09093259号")。