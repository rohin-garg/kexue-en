## SEARCH

## MENU

- [打赏](https://kexue.fm/reward.html)
- [公式](https://kexue.fm/latex.html)
- [天象](https://kexue.fm/ac.html)
- [链接](https://kexue.fm/links.html)
- [时光](https://kexue.fm/me.html)
- [博览](https://kexue.fm/science.html)
- [归档](https://kexue.fm/content.html)

## CATEGORIES

- [千奇百怪](https://kexue.fm/category/Everything)
- [天文探索](https://kexue.fm/category/Astronomy)
- [数学研究](https://kexue.fm/category/Mathematics)
- [物理化学](https://kexue.fm/category/Phy-chem)
- [信息时代](https://kexue.fm/category/Big-Data)
- [生物自然](https://kexue.fm/category/Biology)
- [图片摄影](https://kexue.fm/category/Photograph)
- [问题百科](https://kexue.fm/category/Questions)
- [生活/情感](https://kexue.fm/category/Life-Feeling)
- [资源共享](https://kexue.fm/category/Resources)

## NEWPOSTS

- [MuP之上：1. 好模型的三个特征](https://kexue.fm/archives/11340)
- [随机矩阵的谱范数的快速估计](https://kexue.fm/archives/11335)
- [DiVeQ：一种非常简洁的VQ训练方案](https://kexue.fm/archives/11328)
- [为什么线性注意力要加Short C...](https://kexue.fm/archives/11320)
- [AdamW的Weight RMS的...](https://kexue.fm/archives/11307)
- [重新思考学习率与Batch Siz...](https://kexue.fm/archives/11301)
- [重新思考学习率与Batch Siz...](https://kexue.fm/archives/11285)
- [重新思考学习率与Batch Siz...](https://kexue.fm/archives/11280)
- [为什么Adam的Update RM...](https://kexue.fm/archives/11267)
- [重新思考学习率与Batch Siz...](https://kexue.fm/archives/11260)

## COMMENTS

- [苏剑林: 精度也是一个视角，但感觉这个事情感觉得仔细分析一下，因为理论上...](https://kexue.fm/archives/11340/comment-page-1#comment-28680)
- [苏剑林: 可以这样理解：$t$时刻的$\\boldsymbol{x}\_t$...](https://kexue.fm/archives/9257/comment-page-4#comment-28679)
- [苏剑林: 没有太多技巧了，就是直接代入然后根据$\\bar{\\alpha}...](https://kexue.fm/archives/9181/comment-page-5#comment-28678)
- [苏剑林: 完全懵了...WukT是什么？如果代表C到key的投影矩阵，那...](https://kexue.fm/archives/10862/comment-page-1#comment-28677)
- [Zhan-Wang Mao: 苏老师，请教一下(4)式的泰勒展开式为什么严格来说和$t$有关...](https://kexue.fm/archives/9257/comment-page-4#comment-28676)
- [yzlnew: 可以相呼应的是，这样的好模型能被浮点数以误差比较低的方式表示和...](https://kexue.fm/archives/11340/comment-page-1#comment-28675)
- [Henry: 想请问苏老师，方程7是如何推导到方程10的，是否有化简的一些小技巧？](https://kexue.fm/archives/9181/comment-page-5#comment-28673)
- [szsheep: 牛啊，还可以从这方面推出loss的函数最终式。原本是从KL散度...](https://kexue.fm/archives/9119/comment-page-13#comment-28672)
- [pang: 对于目前的MLA算法softmax(X×WQ×WukT×CjT...](https://kexue.fm/archives/10862/comment-page-1#comment-28671)
- [苏剑林: 你是说 chatglm2-6b 里边的？那个没用，预设的常数是...](https://kexue.fm/archives/11126/comment-page-3#comment-28670)

## USERLOGIN

- [登录](https://kexue.fm/admin/login.php)

[科学空间\|Scientific Spaces](https://kexue.fm)

- [登录](https://kexue.fm/admin/login.php)
- [打赏](https://kexue.fm/reward.html)
- [公式](https://kexue.fm/latex.html)
- [天象](https://kexue.fm/ac.html)
- [链接](https://kexue.fm/links.html)
- [时光](https://kexue.fm/me.html)
- [博览](https://kexue.fm/science.html)
- [归档](https://kexue.fm/content.html)

渴望成为一个小飞侠

- [欢迎订阅](https://kexue.fm/feed)
- [个性邮箱](https://kexue.fm/archives/119)
- [天象信息](https://kexue.fm/ac.html)
- [观测ISS](https://kexue.fm/archives/41)
- [LaTeX](https://kexue.fm/latex.html)
- [关于博主](https://kexue.fm/me.html)

欢迎访问“科学空间”，这里将与您共同探讨自然科学，回味人生百态；也期待大家的分享～

- [**千奇百怪** Everything](https://kexue.fm/category/Everything)
- [**天文探索** Astronomy](https://kexue.fm/category/Astronomy)
- [**数学研究** Mathematics](https://kexue.fm/category/Mathematics)
- [**物理化学** Phy-chem](https://kexue.fm/category/Phy-chem)
- [**信息时代** Big-Data](https://kexue.fm/category/Big-Data)
- [**生物自然** Biology](https://kexue.fm/category/Biology)
- [**图片摄影** Photograph](https://kexue.fm/category/Photograph)
- [**问题百科** Questions](https://kexue.fm/category/Questions)
- [**生活/情感** Life-Feeling](https://kexue.fm/category/Life-Feeling)
- [**资源共享** Resources](https://kexue.fm/category/Resources)

- [**千奇百怪**](https://kexue.fm/category/Everything)
- [**天文探索**](https://kexue.fm/category/Astronomy)
- [**数学研究**](https://kexue.fm/category/Mathematics)
- [**物理化学**](https://kexue.fm/category/Phy-chem)
- [**信息时代**](https://kexue.fm/category/Big-Data)
- [**生物自然**](https://kexue.fm/category/Biology)
- [**图片摄影**](https://kexue.fm/category/Photograph)
- [**问题百科**](https://kexue.fm/category/Questions)
- [**生活/情感**](https://kexue.fm/category/Life-Feeling)
- [**资源共享**](https://kexue.fm/category/Resources)

[首页](https://kexue.fm) [信息时代](https://kexue.fm/category/Big-Data) 【中文分词系列】 1\. 基于AC自动机的快速分词

17Aug

# [【中文分词系列】 1\. 基于AC自动机的快速分词](https://kexue.fm/archives/3908)

By 苏剑林 \|
2016-08-17 \|
122183位读者\|

前言：这个暑假花了不少时间在中文分词和语言模型上面，碰了无数次壁，也得到了零星收获。打算写一个专题，分享一下心得体会。虽说是专题，但仅仅是一些笔记式的集合，并非系统的教程，请读者见谅。

### 中文分词 [\#](https://kexue.fm/kexue.fm\#%E4%B8%AD%E6%96%87%E5%88%86%E8%AF%8D)

关于中文分词的介绍和重要性，我就不多说了， [matrix67这里](http://www.matrix67.com/blog/archives/4212) 有一篇关于分词和分词算法很清晰的介绍，值得一读。在文本挖掘中，虽然已经有不少文章探索了不分词的处理方法，如本博客的 [《文本情感分类（三）：分词 OR 不分词》](https://kexue.fm/archives/3863/)，但在一般场合都会将分词作为文本挖掘的第一步，因此，一个有效的分词算法是很重要的。当然，中文分词作为第一步，已经被探索很久了，目前做的很多工作，都是总结性质的，最多是微弱的改进，并不会有很大的变化了。

目前中文分词主要有两种思路：查词典和字标注。首先，查词典的方法有：机械的最大匹配法、最少词数法，以及基于有向无环图的最大概率组合，还有基于语言模型的最大概率组合，等等。查词典的方法简单高效（得益于动态规划的思想），尤其是结合了语言模型的最大概率法，能够很好地解决歧义问题，但对于中文分词一大难度——未登录词（中文分词有两大难度：歧义和未登录词），则无法解决；为此，人们也提出了基于字标注的思路，所谓字标注，就是通过几个标记（比如4标注的是：single，单字成词；begin，多字词的开头；middle，三字以上词语的中间部分；end，多字词的结尾），把句子的正确分词法表示出来。这是一个序列（输入句子）到序列（标记序列）的过程，能够较好地解决未登录词的问题，但速度较慢，而且对于已经有了完备词典的场景下，字标注的分词效果可能也不如查词典方法。总之，各有优缺点（似乎是废话～），实际使用可能会结合两者，像结巴分词，用的是有向无环图的最大概率组合，而对于连续的单字，则使用字标注的HMM模型来识别。

### AC自动机 [\#](https://kexue.fm/kexue.fm\#AC%E8%87%AA%E5%8A%A8%E6%9C%BA)

本文首先要实现的是查词典方法的分词。查词典的过程是：1、给定一批词，查找给定句子中是不是含有这个词；2、如果有的话，怎么解决歧义性问题。其中，第1步，在计算机中称为“多模式匹配”。这步看上去简单，但事实上要高效地实现并不容易。要知道，一个完备的词典，少说也有十几万词语，如果一个个枚举查找，那计算量是吃不消的，事实上我们人也不是这样做的，我们在查字典的时候，会首先看首字母，然后只在首字母相同的那一块找，然后又比较下一个字母，依此下去。这需要两个条件：1、一个做好特殊排序的词典；2、有效的查找技巧，对于第1个条件，我们有所谓的前缀树（trie），第2个条件，我们有一些经典的算法，比如AC自动机（Aho and Corasick）。

对于这两个条件，我也不多评价什么了，不是不想说，而是我的了解也到此为止了——对于AC自动机，我的认识就是一个使用了trie数据结构的高效多模式匹配算法。我也不用亲自实现它，因为Python已经有对应的库了： [pyahocorasick](https://pypi.python.org/pypi/pyahocorasick/)。因此，我们只需要关心怎么使用它就行了。官方的教程已经很详细地介绍了pyahocorasick的基本使用方法了，这里也不赘述。（遗憾的是，虽然pyahocorasick已经同时支持python2和python3了，但是在python2中，它只支持bytes字符串不支持unicode字符串，而在python3中，则默认使用unicode编码，这对我们写程度会带来一点困惑，当然，不是本质性的。本文使用的是 **python 2.7**。）

构建一个基于AC自动机的分词系统，首先需要有一个文本词典，假设词典有两列，每一行是词和对应的频数，用空格分开。那么就可以用下面的代码构建一个AC自动机。

```
import ahocorasick

def load_dic(dicfile):
 from math import log
 dic = ahocorasick.Automaton()
 total = 0.0
 with open(dicfile) as dicfile:
 words = []
 for line in dicfile:
 line = line.split(' ')
 words.append((line[0], int(line[1])))
 total += int(line[1])
 for i,j in words:
 dic.add_word(i, (i, log(j/total))) #这里使用了对数概率，防止溢出
 dic.make_automaton()
 return dic

dic = load_dic('me.dic')
```

pyahocorasick构建AC自动机有一个很人性化的地方，它能够以“键-注释”这样成对的形式添加词汇（请留意dic.add\_word(i, (i, log(j/total)))这一句），这样，我们可以在注释这里，添加我们想要的信息，比如频数、词性等，然后在查找的时候会一并返回。有了上述AC自动机，我们就能很方便地构建一个全模式分词，也就是把词典中有的词都扫描出来（其实这本来就是AC自动机的本职工作）。

```
def all_cut(sentence):
 words = []
 for i,j in dic.iter(sentence):
 words.append(j[0])
 return words
```

对于一个长句子，这可能会返回很多词，请慎用。

### 最大匹配法 [\#](https://kexue.fm/kexue.fm\#%E6%9C%80%E5%A4%A7%E5%8C%B9%E9%85%8D%E6%B3%95)

当然，上述所谓的全模式分词，根本就算不上什么分词，只是简单的查找罢了，下面我们来实现一个经典的分词算法：最大匹配法。

最大匹配法是指从左到右逐渐匹配词库中的词语，匹配到最长的词语为止。这是一种比较粗糙的分词方法，在matrix67的文章中也有说到，构造反例很简单，如果词典中有“不”、“不可”、“能”、“可能”四个词，但没有“不可能”这个词，那么“不可能”就会被切分为“不可/能”了。虽然如此，在精度要求不高的情况下，这种分词算法还是可以接受的，毕竟速度很快～下面是基于AC自动机的最大匹配法的实现：

```
def max_match_cut(sentence):
 sentence = sentence.decode('utf-8')
 words = ['']
 for i in sentence:
 i = i.encode('utf-8')
 if dic.match(words[-1] + i):
 words[-1] += i
 else:
 words.append(i)
 return words
```

代码很短，也挺清晰的，主要用到了pyahocorasick的match函数。在我的机器上测试，这个算法的效率大概是4M/s，根据hanlp的作者的描述，用JAVA做类似的事情，可以达到20M/s的速度！而用python做，则有两个限制，一个是python本身速度的限制，另外一个是pyahocorasick的限制，导致上面的实现其实并非是最高效率的，因为pyahocorasick不支持unicode编码，所以汉字的编码长度不一，要不断通过转编码的方式来获取汉字长度。

上面说的最大匹配法，准确来说是“正向最大匹配法”，类似地，还有“逆向最大匹配法”，顾名思义，是从右到左扫描句子进行最大匹配，效果一般比正向最大匹配要好些。如果用AC自动机来实现，唯一的办法就是对词典所有的词都反序存储，然后对输入的句子也反序，然后进行正向最大匹配了。

### 最大概率组合 [\#](https://kexue.fm/kexue.fm\#%E6%9C%80%E5%A4%A7%E6%A6%82%E7%8E%87%E7%BB%84%E5%90%88)

基于最大概率组合的方法，是目前兼顾了速度和准确率的比较优秀的方法。它说的是：对于一个句子，如果切分为词语$w\_1,w\_2,\\dots,w\_n$是最优的切分方案，那么应该使得下述概率最大：
$$P(w\_1,w\_2,\\dots,w\_n)$$
直接估计这概率是不容易的，一般用一些近似方案，比如
$$P(w\_1,w\_2,\\dots,w\_n)\\approx P(w\_1)P(w\_2\|w\_1)P(w\_3\|w\_2)\\dots P(w\_n\|w\_{n-1})$$
这里$P(w\_k\|w\_{k-1})$就称为语言模型，它已经初步地考虑了语义了。当然，普通分词工具是很难估计$P(w\_k\|w\_{k-1})$的，一般采用更加简单的近似方案。
$$P(w\_1,w\_2,\\dots,w\_n)\\approx P(w\_1)P(w\_2)P(w\_3)\\dots P(w\_n)$$
放到图论来看，这就是有向无环图里边的最大概率路径了。

下面介绍用AC自动机，结合动态规划，来实现后一种方案。

```
def max_proba_cut(sentence):
 paths = {0:([], 0)}
 end = 0
 for i,j in dic.iter(sentence):
 start,end = 1+i-len(j[0]), i+1
 if start not in paths:
 last = max([i for i in paths if i < start])
 paths[start] = (paths[last][0]+[sentence[last:start]], paths[last][1]-10)
 proba = paths[start][1]+j[1]
 if end not in paths or proba > paths[end][1]:
 paths[end] = (paths[start][0]+[j[0]], proba)
 if end < len(sentence):
 return paths[end][0] + [sentence[end:]]
 else:
 return paths[end][0]
```

代码还是很简短清晰，这里假设了不匹配部分的频率是$e^{-10}$，这个可以修改。只是要注意的是，由于使用的思路不同，因此这里的动态规划方案与一般的有向无环图的动态规划不一样，但是思路是很自然的。要注意，如果直接用这个函数对长度为上万字的句子进行分词，会比较慢，而且耗内存比较大，这是因为我通过字典把动态规划过程中所有的临时方案都保留了下来。幸好，中文句子中还是有很多天然的断句标记的，比如标点符号、换行符，我们可以用这些标记把句子分成很多部分，然后逐步分词，如下。

```
to_break = ahocorasick.Automaton()
for i in ['，', '。', '！', '、', '？', ' ', '\n']:
 to_break.add_word(i, i)

to_break.make_automaton()

def map_cut(sentence):
 start = 0
 words = []
 for i in to_break.iter(sentence):
 words.extend(max_proba_cut(sentence[start:i[0]+1]))
 start = i[0]+1
 words.extend(max_proba_cut(sentence[start:]))
 return words
```

在服务器上，我抽了10万篇文章出来（1亿多字），对比了结巴分词的速度，发现在使用相同词典的情况下，并且关闭结巴分词的新词发现，用AC自动机实现的这个map\_cut的分词速度，大概是结巴分词的2～3倍，大约有1M/s。

最后，值得一提的是，max\_proba\_cut这个函数的实现思路，可以用于其他涉及到动态规划的分词方法，比如最少词数分词：

```
def min_words_cut(sentence):
 paths = {0:([], 0)}
 end = 0
 for i,j in dic.iter(sentence):
 start,end = 1+i-len(j[0]), i+1
 if start not in paths:
 last = max([i for i in paths if i < start])
 paths[start] = (paths[last][0]+[sentence[last:start]], paths[last][1]+1)
 num = paths[start][1]+1
 if end not in paths or num < paths[end][1]:
 paths[end] = (paths[start][0]+[j[0]], num)
 if end < len(sentence):
 return paths[end][0] + [sentence[end:]]
 else:
 return paths[end][0]
```

这里采取了罚分规则：有多少个词罚多少分，未登录词再罚一分，最后罚分最少的胜出。

### 总结 [\#](https://kexue.fm/kexue.fm\#%E6%80%BB%E7%BB%93)

事实上，只要涉及到查词典的操作，AC自动机都会有一定的用武之地。将AC自动机用于分词，事实上是一个非常自然的应用。我们期待有更多对中文支持更好的数据结构和算法出现，这样我们就有可能设计出更高效率的算法了。

_**转载到请包括本文地址：** [https://kexue.fm/archives/3908](https://kexue.fm/archives/3908)_

_**更详细的转载事宜请参考：**_ [《科学空间FAQ》](https://kexue.fm/archives/6508#%E6%96%87%E7%AB%A0%E5%A6%82%E4%BD%95%E8%BD%AC%E8%BD%BD/%E5%BC%95%E7%94%A8)

**如果您还有什么疑惑或建议，欢迎在下方评论区继续讨论。**

**如果您觉得本文还不错，欢迎 [分享](https://kexue.fm/kexue.fm#share)/ [打赏](https://kexue.fm/kexue.fm#pay) 本文。打赏并非要从中获得收益，而是希望知道科学空间获得了多少读者的真心关注。当然，如果你无视它，也不会影响你的阅读。再次表示欢迎和感谢！**

打赏

微信打赏

支付宝打赏

因为网站后台对打赏并无记录，因此欢迎在打赏时候备注留言。你还可以 [**点击这里**](http://mail.qq.com/cgi-bin/qm_share?t=qm_mailme&email=tN7d1drY3drrx8H0xcWa19vZ) 或在下方评论区留言来告知你的建议或需求。

**如果您需要引用本文，请参考：**

苏剑林. (Aug. 17, 2016). 《【中文分词系列】 1. 基于AC自动机的快速分词 》\[Blog post\]. Retrieved from [https://kexue.fm/archives/3908](https://kexue.fm/archives/3908)

@online{kexuefm-3908,
        title={【中文分词系列】 1. 基于AC自动机的快速分词},
        author={苏剑林},
        year={2016},
        month={Aug},
        url={\\url{https://kexue.fm/archives/3908}},
}

分类： [信息时代](https://kexue.fm/category/Big-Data)    标签： [动态规划](https://kexue.fm/tag/%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92/), [AC自动机](https://kexue.fm/tag/AC%E8%87%AA%E5%8A%A8%E6%9C%BA/), [分词](https://kexue.fm/tag/%E5%88%86%E8%AF%8D/), [自然语言处理](https://kexue.fm/tag/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/), [词库](https://kexue.fm/tag/%E8%AF%8D%E5%BA%93/)[26 评论](https://kexue.fm/archives/3908#comments)

< [两个惊艳的python库：tqdm和retry](https://kexue.fm/archives/3902) \| [【中文分词系列】 2\. 基于切分的新词发现](https://kexue.fm/archives/3913) >

### 你也许还对下面的内容感兴趣

- [随机分词再探：从Viterbi Sampling到完美采样算法](https://kexue.fm/archives/9811)
- [随机分词浅探：从Viterbi Decoding到Viterbi Sampling](https://kexue.fm/archives/9768)
- [BytePiece：更纯粹、更高压缩率的Tokenizer](https://kexue.fm/archives/9752)
- [一个二值化词向量模型，是怎么跟果蝇搭上关系的？](https://kexue.fm/archives/8159)
- [JoSE：球面上的词向量和句向量](https://kexue.fm/archives/7063)
- [重新写了之前的新词发现算法：更快更好的新词发现](https://kexue.fm/archives/6920)
- [分享一次专业领域词汇的无监督挖掘](https://kexue.fm/archives/6540)
- [【分享】千万级百度知道语料](https://kexue.fm/archives/5067)
- [分享一个slide：花式自然语言处理](https://kexue.fm/archives/4823)
- [【中文分词系列】 8\. 更好的新词发现算法](https://kexue.fm/archives/4256)

[发表你的看法](https://kexue.fm/kexue.fm#comment_form)

1. [«](https://kexue.fm/archives/3908/comment-page-1#comments)
2. [1](https://kexue.fm/archives/3908/comment-page-1#comments)
3. [2](https://kexue.fm/archives/3908/comment-page-2#comments)

enjlife

January 12th, 2021

学习了，原来分词也在用字典树

[回复评论](https://kexue.fm/archives/3908/comment-page-2?replyTo=15249#respond-post-3908)

[基于双向BiLstm神经网络的中文分词详解及源码 \| newbie](https://new-bie.xyz/p/16493/)

April 9th, 2022

\[...\]在自然语言处理中（NLP，Natural Language ProcessingNLP，Natural Language Processing），分词是一个较为简单也基础的基本技术。常用的分词方法包括这两种：基于字典的机械分词 和 基于统计序列标注的分词。对于基于字典的机械分词本文不再赘述，可看字典分词方法。在本文中主要讲解基于深度学习的分词方法及原理，包括一下几个步骤：1标注序列，2双向LSTM\[...\]

[回复评论](https://kexue.fm/archives/3908/comment-page-2?replyTo=18906#respond-post-3908)

dan

October 21st, 2022

这里使用最大概率的话，对于停止词是不是会比较倾向于单独切分开来呢，因为停止词的概率太大了。

[回复评论](https://kexue.fm/archives/3908/comment-page-2?replyTo=20157#respond-post-3908)

[基于双向BiLstm神经网络的中文分词详解及源码 R11; 94007技术网](https://www.94007.cc/nodejs/115633)

March 31st, 2023

\[...\]在自然语言处理中（NLP，Natural Language ProcessingNLP，Natural Language Processing），分词是一个较为简单也基础的基本技术。常用的分词方法包括这两种：基于字典的机械分词 和 基于统计序列标注的分词。对于基于字典的机械分词本文不再赘述，可看字典分词方法。在本文中主要讲解基于深度学习的分词方法及原理，包括一下几个步骤：1标注序列，2双向LSTM\[...\]

[回复评论](https://kexue.fm/archives/3908/comment-page-2?replyTo=21277#respond-post-3908)

[基于双向BiLstm神经网络的中文分词详解及源码 R11; 94007技术网](https://www.94007.cc/python/116251)

March 31st, 2023

\[...\]在自然语言处理中（NLP，Natural Language ProcessingNLP，Natural Language Processing），分词是一个较为简单也基础的基本技术。常用的分词方法包括这两种：基于字典的机械分词 和 基于统计序列标注的分词。对于基于字典的机械分词本文不再赘述，可看字典分词方法。在本文中主要讲解基于深度学习的分词方法及原理，包括一下几个步骤：1标注序列，2双向LSTM\[...\]

[回复评论](https://kexue.fm/archives/3908/comment-page-2?replyTo=21278#respond-post-3908)

[基于双向BiLstm神经网络的中文分词详解及源码 R11; 94007技术网](https://www.94007.cc/ios/266788)

April 1st, 2023

\[...\]在自然语言处理中（NLP，Natural Language ProcessingNLP，Natural Language Processing），分词是一个较为简单也基础的基本技术。常用的分词方法包括这两种：基于字典的机械分词 和 基于统计序列标注的分词。对于基于字典的机械分词本文不再赘述，可看字典分词方法。在本文中主要讲解基于深度学习的分词方法及原理，包括一下几个步骤：1标注序列，2双向LSTM\[...\]

[回复评论](https://kexue.fm/archives/3908/comment-page-2?replyTo=21297#respond-post-3908)

zcj5918

June 25th, 2024

假设要构造一个自动机，如果我有一个单词表，$\\{ab,bc,abc,a,b,c\\}$，然后我去匹配abc的时候，最好返回表中有的所有子串的组合也就是这6个都要返回，有没有什么$O(n)$的算法可以满足要求啊？

[回复评论](https://kexue.fm/archives/3908/comment-page-2?replyTo=24603#respond-post-3908)

zcj5918 发表于
June 25th, 2024

如果没有办法构造出来，能不能从数学上证明不存在$O(n)$的算法可以满足要求？

[回复评论](https://kexue.fm/archives/3908/comment-page-2?replyTo=24604#respond-post-3908)

[苏剑林](https://kexue.fm) 发表于
June 27th, 2024

我理解这可以分为两步实现：
1、以abc为输入，用AC自动机搜索所有在词表中的子串（一般的AC自动机都有这个函数）；
2、以第1步搜索到的所有子串为关键词，去搜索指定输入文本。

[回复评论](https://kexue.fm/archives/3908/comment-page-2?replyTo=24618#respond-post-3908)

zcj5918 发表于
July 1st, 2024

1、以abc为输入，用AC自动机搜索所有在词表中的子串（一般的AC自动机都有这个函数）；

这一步如果只是前缀匹配是$O(n)$，如果要在子串有abc的情况下，去匹配是否有b和c这种全部匹配，就相当于每走到一个节点，都需要去递归找他的failure\_link，最多用一个set标记是否访问到过，这样实现最坏情况是$O(n^2)$的代码。。。就想知道有没有更优的。。。

[回复评论](https://kexue.fm/archives/3908/comment-page-2?replyTo=24644#respond-post-3908)

[苏剑林](https://kexue.fm) 发表于
July 2nd, 2024

这个搜索算法本身就是从左往右扫描序列的，一般情况下是线性，但确实存在你说的最坏的情况，然而此时返回的数量也是$O(n^2)$了，既然返回的数量都是$O(n^2)$，那么就不可能有比$O(n^2)$更快的方法。

[回复评论](https://kexue.fm/archives/3908/comment-page-2?replyTo=24661#respond-post-3908)

1. [«](https://kexue.fm/archives/3908/comment-page-1#comments)
2. [1](https://kexue.fm/archives/3908/comment-page-1#comments)
3. [2](https://kexue.fm/archives/3908/comment-page-2#comments)

[取消回复](https://kexue.fm/archives/3908#respond-post-3908)

你的大名

电子邮箱

个人网站（选填）

1\. 可以使用LaTeX代码，点击“预览效果”可查看效果；2. 可以通过点击评论楼层编号来引用该楼层；3. 网站可能会有点卡，如非确认评论失败，请 **不要重复点击提交**。

### 内容速览

[中文分词](https://kexue.fm/kexue.fm#%E4%B8%AD%E6%96%87%E5%88%86%E8%AF%8D)
[AC自动机](https://kexue.fm/kexue.fm#AC%E8%87%AA%E5%8A%A8%E6%9C%BA)
[最大匹配法](https://kexue.fm/kexue.fm#%E6%9C%80%E5%A4%A7%E5%8C%B9%E9%85%8D%E6%B3%95)
[最大概率组合](https://kexue.fm/kexue.fm#%E6%9C%80%E5%A4%A7%E6%A6%82%E7%8E%87%E7%BB%84%E5%90%88)
[总结](https://kexue.fm/kexue.fm#%E6%80%BB%E7%BB%93)

### 智能搜索

支持整句搜索！网站自动使用 [结巴分词](https://github.com/fxsjy/jieba) 进行分词，并结合ngrams排序算法给出合理的搜索结果。

### 热门标签

[生成模型](https://kexue.fm/tag/%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B/) [attention](https://kexue.fm/tag/attention/) [优化](https://kexue.fm/tag/%E4%BC%98%E5%8C%96/) [语言模型](https://kexue.fm/tag/%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/) [模型](https://kexue.fm/tag/%E6%A8%A1%E5%9E%8B/) [网站](https://kexue.fm/tag/%E7%BD%91%E7%AB%99/) [概率](https://kexue.fm/tag/%E6%A6%82%E7%8E%87/) [梯度](https://kexue.fm/tag/%E6%A2%AF%E5%BA%A6/) [矩阵](https://kexue.fm/tag/%E7%9F%A9%E9%98%B5/) [转载](https://kexue.fm/tag/%E8%BD%AC%E8%BD%BD/) [优化器](https://kexue.fm/tag/%E4%BC%98%E5%8C%96%E5%99%A8/) [微分方程](https://kexue.fm/tag/%E5%BE%AE%E5%88%86%E6%96%B9%E7%A8%8B/) [分析](https://kexue.fm/tag/%E5%88%86%E6%9E%90/) [天象](https://kexue.fm/tag/%E5%A4%A9%E8%B1%A1/) [深度学习](https://kexue.fm/tag/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/) [积分](https://kexue.fm/tag/%E7%A7%AF%E5%88%86/) [python](https://kexue.fm/tag/python/) [力学](https://kexue.fm/tag/%E5%8A%9B%E5%AD%A6/) [无监督](https://kexue.fm/tag/%E6%97%A0%E7%9B%91%E7%9D%A3/) [扩散](https://kexue.fm/tag/%E6%89%A9%E6%95%A3/) [几何](https://kexue.fm/tag/%E5%87%A0%E4%BD%95/) [节日](https://kexue.fm/tag/%E8%8A%82%E6%97%A5/) [生活](https://kexue.fm/tag/%E7%94%9F%E6%B4%BB/) [文本生成](https://kexue.fm/tag/%E6%96%87%E6%9C%AC%E7%94%9F%E6%88%90/) [数论](https://kexue.fm/tag/%E6%95%B0%E8%AE%BA/)

### 随机文章

- [【搜出来的文本】⋅（四）通过增、删、改来用词造句](https://kexue.fm/archives/8194)
- [当概率遇上复变：随机游走与路径积分](https://kexue.fm/archives/2609)
- [素数之美2：Bertrand假设的证明](https://kexue.fm/archives/2800)
- [从“0.999...等于1”说开来](https://kexue.fm/archives/3402)
- [有限素域上的乘法群是循环群](https://kexue.fm/archives/3200)
- [我的大学，我的未来](https://kexue.fm/archives/1677)
- [BN究竟起了什么作用？一个闭门造车的分析](https://kexue.fm/archives/6992)
- [费曼积分法(6)：教科书上的两道练习题](https://kexue.fm/archives/1944)
- [Designing GANs：又一个GAN生产车间](https://kexue.fm/archives/7210)
- [【搜出来的文本】⋅（二）从MCMC到模拟退火](https://kexue.fm/archives/8084)

### 最近评论

- [苏剑林](https://kexue.fm/archives/11340/comment-page-1#comment-28680): 精度也是一个视角，但感觉这个事情感觉得仔细分析一下，因为理论上来讲，整体乘一个大数字，是不改变...
- [苏剑林](https://kexue.fm/archives/9257/comment-page-4#comment-28679): 可以这样理解：$t$时刻的$\\boldsymbol{x}\_t$，和$t-1$时刻的$\\bold...
- [苏剑林](https://kexue.fm/archives/9181/comment-page-5#comment-28678): 没有太多技巧了，就是直接代入然后根据$\\bar{\\alpha}\_t,\\bar{\\beta}\_t...
- [苏剑林](https://kexue.fm/archives/10862/comment-page-1#comment-28677): 完全懵了...WukT是什么？如果代表C到key的投影矩阵，那Wropeq和Wropekt又是...
- [Zhan-Wang Mao](https://kexue.fm/archives/9257/comment-page-4#comment-28676): 苏老师，请教一下(4)式的泰勒展开式为什么严格来说和$t$有关？不是在$x\_t$处关于$x$的...
- [yzlnew](https://kexue.fm/archives/11340/comment-page-1#comment-28675): 可以相呼应的是，这样的好模型能被浮点数以误差比较低的方式表示和训练，并且也易于量化。
- [Henry](https://kexue.fm/archives/9181/comment-page-5#comment-28673): 想请问苏老师，方程7是如何推导到方程10的，是否有化简的一些小技巧？
- [szsheep](https://kexue.fm/archives/9119/comment-page-13#comment-28672): 牛啊，还可以从这方面推出loss的函数最终式。原本是从KL散度入手，没想到作者完全用另外一种方...
- [pang](https://kexue.fm/archives/10862/comment-page-1#comment-28671): 对于目前的MLA算法softmax(X×WQ×WukT×CjT)×Cj×Wuv来说其实X,WQ...
- [苏剑林](https://kexue.fm/archives/11126/comment-page-3#comment-28670): 你是说 chatglm2-6b 里边的？那个没用，预设的常数是压不住的...

### 友情链接

- [Cool Papers](https://papers.cool)
- [数学研发](https://bbs.emath.ac.cn)
- [Seatop](http://www.seatop.com.cn/)
- [Xiaoxia](https://xiaoxia.org/)
- [积分表-网络版](https://kexue.fm/sci/integral/index.html)
- [丝路博傲](http://blog.dvxj.com/)
- [数学之家](http://www.2math.cn/)
- [有趣天文奇观](http://interesting-sky.china-vo.org/)
- [TwistedW](http://www.twistedwg.com/)
- [godweiyang](https://godweiyang.com/)
- [AI柠檬](https://blog.ailemon.net/)
- [王登科-DK博客](https://greatdk.com)
- [ESON](https://blog.eson.org/)
- [枫之羽](https://fzhiy.net/)
- [Mathor's blog](https://wmathor.com/)
- [coding-zuo](https://coding-zuo.github.io/)
- [博科园](https://www.bokeyuan.net/)
- [孔皮皮的博客](https://www.kppkkp.top/)
- [运鹏的博客](https://yunpengtai.top/)
- [jiming.site](https://jiming.site/)
- [OmegaXYZ](https://www.omegaxyz.com/)
- [EAI猩球](https://www.robotech.ink/)
- [文举的博客](https://liwenju0.com/)
- [用代码打点酱油](https://bruceyuan.com/)
- [Zhang's blog](https://armcvai.cn/)
- [申请链接](https://kexue.fm/links.html)

本站采用创作共用版权协议，要求署名、非商业用途和保持一致。转载本站内容必须也遵循“ [署名-非商业用途-保持一致](http://creativecommons.org/licenses/by-nc-nd/2.5/cn/)”的创作共用协议。
© 2009-2025 Scientific Spaces. All rights reserved. Theme by [laogui](http://www.laogui.com). Powered by [Typecho](http://typecho.org). 备案号: [粤ICP备09093259号-1/2](https://beian.miit.gov.cn/)。