## SEARCH

## MENU

- [打赏](https://kexue.fm/reward.html)
- [公式](https://kexue.fm/latex.html)
- [天象](https://kexue.fm/ac.html)
- [链接](https://kexue.fm/links.html)
- [时光](https://kexue.fm/me.html)
- [博览](https://kexue.fm/science.html)
- [归档](https://kexue.fm/content.html)

## CATEGORIES

- [千奇百怪](https://kexue.fm/category/Everything)
- [天文探索](https://kexue.fm/category/Astronomy)
- [数学研究](https://kexue.fm/category/Mathematics)
- [物理化学](https://kexue.fm/category/Phy-chem)
- [信息时代](https://kexue.fm/category/Big-Data)
- [生物自然](https://kexue.fm/category/Biology)
- [图片摄影](https://kexue.fm/category/Photograph)
- [问题百科](https://kexue.fm/category/Questions)
- [生活/情感](https://kexue.fm/category/Life-Feeling)
- [资源共享](https://kexue.fm/category/Resources)

## NEWPOSTS

- [Transformer升级之路：2...](https://kexue.fm/archives/11111)
- [“对角+低秩”三角阵的高效求逆方法](https://kexue.fm/archives/11072)
- [通过msign来计算奇异值裁剪mc...](https://kexue.fm/archives/11059)
- [矩阵符号函数mcsgn能计算什么？](https://kexue.fm/archives/11056)
- [线性注意力简史：从模仿、创新到反哺](https://kexue.fm/archives/11033)
- [msign的导数](https://kexue.fm/archives/11025)
- [通过msign来计算奇异值裁剪mc...](https://kexue.fm/archives/11006)
- [msign算子的Newton-Sc...](https://kexue.fm/archives/10996)
- [等值振荡定理：最优多项式逼近的充要条件](https://kexue.fm/archives/10972)
- [生成扩散模型漫谈（三十）：从瞬时速...](https://kexue.fm/archives/10958)

## COMMENTS

- [Laity: 苏老师好，有个问题想请教，这里的meanflow前提是使用了线...](https://kexue.fm/archives/10958/comment-page-2#comment-28067)
- [bingps: 苏神怎么看Tri Dao最近提的GTA和GLA https:/...](https://kexue.fm/archives/11111/comment-page-1#comment-28066)
- [YyWangCS: 从剑林老师关于MLA的两篇文章学习到了很多，我想从工程框架的视...](https://kexue.fm/archives/11111/comment-page-1#comment-28064)
- [Namoe: typo: 总而言之一节的总结第4点 通过LoRA来金可能地兼...](https://kexue.fm/archives/11111/comment-page-1#comment-28062)
- [阿呱: 你好我也碰到了类似的问题，请问您是怎么解决的呀？想参考一下方法](https://kexue.fm/archives/7359/comment-page-8#comment-28061)
- [silver: 求问文中的$e\_i$是啥？是ds论文的“Algorithm 1...](https://kexue.fm/archives/10757/comment-page-3#comment-28060)
- [Truenobility303: 谢谢苏神的详细解答！](https://kexue.fm/archives/10739/comment-page-2#comment-28059)
- [Truenobility303: 不好意思我的表述可能会误导性说错了，核心问题不在2\. 我觉得问...](https://kexue.fm/archives/10795/comment-page-1#comment-28058)
- [kw: 把所有M直接换成全1矩阵就行吧，比如DeltaNet变成$(Q...](https://kexue.fm/archives/11033/comment-page-1#comment-28057)
- [WB: 非常清楚的blog。我有一个小问题想问一下，推导的时候用的是不...](https://kexue.fm/archives/10795/comment-page-1#comment-28056)

## USERLOGIN

- [登录](https://kexue.fm/admin/login.php)

[科学空间\|Scientific Spaces](https://kexue.fm)

- [登录](https://kexue.fm/admin/login.php)
- [打赏](https://kexue.fm/reward.html)
- [公式](https://kexue.fm/latex.html)
- [天象](https://kexue.fm/ac.html)
- [链接](https://kexue.fm/links.html)
- [时光](https://kexue.fm/me.html)
- [博览](https://kexue.fm/science.html)
- [归档](https://kexue.fm/content.html)

渴望成为一个小飞侠

- [欢迎订阅](https://kexue.fm/feed)
- [个性邮箱](https://kexue.fm/archives/119)
- [天象信息](https://kexue.fm/ac.html)
- [观测ISS](https://kexue.fm/archives/41)
- [LaTeX](https://kexue.fm/latex.html)
- [关于博主](https://kexue.fm/me.html)

欢迎访问“科学空间”，这里将与您共同探讨自然科学，回味人生百态；也期待大家的分享～

- [**千奇百怪** Everything](https://kexue.fm/category/Everything)
- [**天文探索** Astronomy](https://kexue.fm/category/Astronomy)
- [**数学研究** Mathematics](https://kexue.fm/category/Mathematics)
- [**物理化学** Phy-chem](https://kexue.fm/category/Phy-chem)
- [**信息时代** Big-Data](https://kexue.fm/category/Big-Data)
- [**生物自然** Biology](https://kexue.fm/category/Biology)
- [**图片摄影** Photograph](https://kexue.fm/category/Photograph)
- [**问题百科** Questions](https://kexue.fm/category/Questions)
- [**生活/情感** Life-Feeling](https://kexue.fm/category/Life-Feeling)
- [**资源共享** Resources](https://kexue.fm/category/Resources)

- [**千奇百怪**](https://kexue.fm/category/Everything)
- [**天文探索**](https://kexue.fm/category/Astronomy)
- [**数学研究**](https://kexue.fm/category/Mathematics)
- [**物理化学**](https://kexue.fm/category/Phy-chem)
- [**信息时代**](https://kexue.fm/category/Big-Data)
- [**生物自然**](https://kexue.fm/category/Biology)
- [**图片摄影**](https://kexue.fm/category/Photograph)
- [**问题百科**](https://kexue.fm/category/Questions)
- [**生活/情感**](https://kexue.fm/category/Life-Feeling)
- [**资源共享**](https://kexue.fm/category/Resources)

[首页](https://kexue.fm) [信息时代](https://kexue.fm/category/Big-Data) Bias项的神奇作用：RoPE + Bias = 更好的长度外推性

3Apr

# [Bias项的神奇作用：RoPE + Bias = 更好的长度外推性](https://kexue.fm/archives/9577)

By 苏剑林 \|
2023-04-03 \|
56120位读者\|

万万没想到，Bias项能跟Transformer的长度外推性联系在一起！

长度外推性是我们希望Transformer具有的一个理想性质，笔者曾在 [《Transformer升级之路：7、长度外推性与局部注意力》](https://kexue.fm/archives/9431)、 [《Transformer升级之路：8、长度外推性与位置鲁棒性》](https://kexue.fm/archives/9444) 系统地介绍过这一问题。至于Bias项（偏置项），目前的主流观点是当模型足够大时，Bias项不会有什么特别的作用，所以很多模型选择去掉Bias项，其中代表是Google的 [T5](https://kexue.fm/archives/7867) 和 [PaLM](https://papers.cool/arxiv/2204.02311)，我们后面做的 [RoFormerV2](https://kexue.fm/archives/8998) 和 [GAU-α](https://kexue.fm/archives/9052) 也沿用了这个做法。

那么，这两个看上去“风牛马不相及”的东西，究竟是怎么联系起来的呢？Bias项真的可以增强Transformer的长度外推性？且听笔者慢慢道来。

## 隐藏彩蛋 [\#](https://kexue.fm/archives/9577\#%E9%9A%90%E8%97%8F%E5%BD%A9%E8%9B%8B)

首先，为什么会想到考察Bias项和长度外推性的联系呢？这是因为笔者前几天在重温GAU的论文 [《Transformer Quality in Linear Time》](https://papers.cool/arxiv/2202.10447) 时，发现了之前没有在意的一个“隐藏彩蛋”——加性相对位置编码，其伪代码为

GAU的加性相对位置编码的伪代码

这里我们主要看$n\\geq 512$的部分，如果写成公式，大致是
\\begin{equation}\\boldsymbol{q}\_m^{\\top}\\boldsymbol{\\mathcal{R}}\_m^{\\top}\\boldsymbol{\\mathcal{R}}\_n\\boldsymbol{k}\_n \\quad\\to\\quad \\boldsymbol{q}\_m^{\\top}\\boldsymbol{\\mathcal{R}}\_m^{\\top}\\boldsymbol{\\mathcal{R}}\_n\\boldsymbol{k}\_n+ \\boldsymbol{a}^{\\top}\\boldsymbol{\\mathcal{R}}\_m^{\\top}\\boldsymbol{\\mathcal{R}}\_n\\boldsymbol{b}\\label{eq:rel-bias}\\end{equation}
其中$\\boldsymbol{\\mathcal{R}}\_m,\\boldsymbol{\\mathcal{R}}\_n$是RoPE的旋转矩阵，$\\boldsymbol{a},\\boldsymbol{b}$是两个可学习参数。

这个加性相对位置编码其实之前也留意到了，但当时的评价只是“不理解为什么同时用几种位置编码”，而最近笔者一直在思考长度外推性问题，所以对这个形式就比较敏感了。可以证明，当$\\boldsymbol{a}=\\boldsymbol{b}=\[\\sqrt{\\lambda},0,\\sqrt{\\lambda},0,\\cdots,\\sqrt{\\lambda},0\]^{\\top}$时，结果正好是 [《Transformer升级之路：7、长度外推性与局部注意力》](https://kexue.fm/archives/9431) 介绍的能改善长度外推性的Sandwich ，其原理就是$\\boldsymbol{a}^{\\top}\\boldsymbol{\\mathcal{R}}\_m^{\\top}\\boldsymbol{\\mathcal{R}}\_n\\boldsymbol{b}$呈现出关于$\|m-n\|$递减的趋势，加到注意力矩阵上后，能够起到局部化注意力的作用，而根据 [《Transformer升级之路：7、长度外推性与局部注意力》](https://kexue.fm/archives/9431)，注意力局部化是语言模型外推性的关键。

所以笔者不禁猜测，难道原论文中的这个加性相对位置编码，就是用来增强长度外推性的？GAU的作者竟然如此有先见之明，早在Sandwich之前就提出了类似的想法来解决长度外推性问题？

## 换成偏置 [\#](https://kexue.fm/archives/9577\#%E6%8D%A2%E6%88%90%E5%81%8F%E7%BD%AE)

不过，对于笔者来说，这种往Attention矩阵上额外加上一项来增强长度外推性的方案都显得不够优雅，所以不管原作者意图如何以及实际效果如何，笔者都不倾向这样做。有什么类似的但几乎“无感”的方案呢？笔者考虑到，如果$\\boldsymbol{a}$、$\\boldsymbol{b}$分别是$\\boldsymbol{q}\_m,\\boldsymbol{k}\_n$的Bias项，或许可以起到类似的效果，即考虑
\\begin{equation}\\boldsymbol{q}\_m^{\\top}\\boldsymbol{\\mathcal{R}}\_m^{\\top}\\boldsymbol{\\mathcal{R}}\_n\\boldsymbol{k}\_n \\quad\\to\\quad (\\boldsymbol{q}\_m + \\boldsymbol{a})^{\\top}\\boldsymbol{\\mathcal{R}}\_m^{\\top}\\boldsymbol{\\mathcal{R}}\_n(\\boldsymbol{k}\_n + \\boldsymbol{b})\\end{equation}
很明显，单纯增加一个Bias项，不管从形式上还是计算量上看都几乎是“无感”的，如果这样就能增强长度外推性，无疑是一个很漂亮的方案。是否可行呢？我们先来看展开后的结果：
\\begin{equation}\\boldsymbol{q}\_m^{\\top}\\boldsymbol{\\mathcal{R}}\_m^{\\top}\\boldsymbol{\\mathcal{R}}\_n\\boldsymbol{k}\_n + \\boldsymbol{a}^{\\top}\\boldsymbol{\\mathcal{R}}\_m^{\\top}\\boldsymbol{\\mathcal{R}}\_n\\boldsymbol{k}\_n + \\boldsymbol{q}\_m^{\\top}\\boldsymbol{\\mathcal{R}}\_m^{\\top}\\boldsymbol{\\mathcal{R}}\_n\\boldsymbol{b} + \\boldsymbol{a}^{\\top}\\boldsymbol{\\mathcal{R}}\_m^{\\top}\\boldsymbol{\\mathcal{R}}\_n\\boldsymbol{b} \\label{eq:bias}\\end{equation}
其中第一项和第四项正好对应公式$\\eqref{eq:rel-bias}$，它们都是我们想要的，所以我们想看看第二项和第三项起到什么作用，如果它们不会有什么明显的效应，那么直接加上Bias项的做法，至少是“有希望”能够取得跟式$\\eqref{eq:rel-bias}$或者Sandwich相似的外推效果。

笔者是这样想的：作为Attention的Query和Key，$\\boldsymbol{q}\_m$、$\\boldsymbol{k}\_n$应该是比较“各向同性”的，即它们的方向比较均匀，接近球面上均匀采样，而$\\boldsymbol{\\mathcal{R}}\_m^{\\top}\\boldsymbol{\\mathcal{R}}\_n=\\boldsymbol{\\mathcal{R}}\_{n-m}$只是一个正交变换，它不改变$\\boldsymbol{q}\_m$、$\\boldsymbol{k}\_n$的各向同性性质，那么$\\boldsymbol{a}^{\\top}\\boldsymbol{\\mathcal{R}}\_m^{\\top}\\boldsymbol{\\mathcal{R}}\_n\\boldsymbol{k}\_n $、$\\boldsymbol{q}\_m^{\\top}\\boldsymbol{\\mathcal{R}}\_m^{\\top}\\boldsymbol{\\mathcal{R}}\_n\\boldsymbol{b}$这两项，就相当于从各向同性分布采样出来的向量，跟一个固定向量的内积，根据我们在 [《n维空间下两个随机向量的夹角分布》](https://kexue.fm/archives/7076) 中的讨论，这样的两个向量夹角应该是很接近90度的，换言之这个内积的期望应该是0，所以第二项和第三项的效应理论上没有剩余两项那么强。

当然，这仅仅是猜测，实际它会训练成怎样，只能通过实验来确定。所以事不宜迟，笔者立刻进行了实验。

## 实验结果 [\#](https://kexue.fm/archives/9577\#%E5%AE%9E%E9%AA%8C%E7%BB%93%E6%9E%9C)

这次笔者选了语言模型任务进行实验，模型架构还是之前的 [GAU-α](https://kexue.fm/archives/9052)，训练长度和batch\_size都是512，优化器是 [Tiger](https://kexue.fm/archives/9512)，两个模型的唯一差别就是Q、K的Bias是否开启（其他Bias仍被去掉）。

外推效果上的对比：
$$\\begin{array}{c}
\\text{不同测试长度下的LM准确率} \\\
{\\begin{array}{c\|cccc}
\\hline
& 512 & 1024 & 2048 & 4096 \\\
\\hline
\\text{w/o Bias} & 52.37\\% & 33.15\\% & 22.85\\% & 17.87\\% \\\
\\text{w/ Bias} & 52.75\\% & 50.99\\% & 45.25\\% & 39.55\\% \\\
\\hline
\\end{array}}
\\end{array}$$
可以看到，Bias项确实不怎么影响训练效果（512长度），但却在长度外推性上面明显拉开了差距，看似毫无存在感的Bias项居然有此神奇作用！当然，要是重跑几次实验，外推性的结果可能会有明显的波动，毕竟长度外推性属于“赠送功能”，并不是我们主动触发的。

为了验证剩下生效机制是否如我们猜测，笔者可视化了式$\\eqref{eq:bias}$的四项在某个样本某一层的变化规律：

加上Bias后四项内积对比

可以看到，第4项确确实实呈现衰减趋势，并且其大小占据了主导地位，将这四项叠加起来，与没有加Bias的模型对比如下：

有无Bias的Attention矩阵对比

没有Bias的模型（蓝色），Attention在训练长度（512）范围内确实也呈现出衰减趋势，但长度增加之后就上升了，没有明显的局部性，这就是它外推性不够好的原因；相反，跟前面的猜测一致，带有Bias项的模型（橙色）的注意力矩阵呈现更明显的衰减趋势，换言之它的局部化效应更加强，从而有更好的外推性能。需要指出的是，加上Bias的模型并不是每一层的Attention都有这么明显的衰减趋势，总体来说前面的层衰减趋势更明显些，后面的层衰减趋势更弱些，说明越靠近输入的层越关注局部信息，这跟 [《The Devil in Linear Transformer》](https://papers.cool/arxiv/2210.10340) 的结论一致。

**【注：后来经过反复测试发现，发现此篇文章的长度外推结果可复现性比较不稳定（可能跟模型结构、超参数等紧密相关），请自行斟酌使用。】**

## 延伸思考 [\#](https://kexue.fm/archives/9577\#%E5%BB%B6%E4%BC%B8%E6%80%9D%E8%80%83)

这时候问题就来了：之前做长度外推性的工作不是都验证了RoPE的外推性不大好了吗？难道它们都没加Bias？为此，笔者特意去考证了一下，果然”不出所料”：“开山之作”ALIBI和最近的XPOS都是没有加Bias项的，而KERPLE和Sandwich则是加了Bias项的。之前笔者在读论文的时候，就一直感觉KERPLE和Sandwich中的RoPE外推效果似乎比ALIBI和XPOS中的好，现在可以肯定这应该不是错觉了，既然KERPLE和Sandwich都加了Bias，那么根据本文的结论，RoPE是可能呈现出更好的长度外推性的。

可能有读者想起，之前不是说Attention的Key的Bias可以去掉吗？难道这里也可以去掉？关于这个问题，可以参考知乎的提问 [《为什么有的 Vision Transformer 中的 key 不需要 bias ？》](https://www.zhihu.com/question/506218961)，事实上，“可以去掉Key的Bias”这个结论，是针对没有RoPE的Attention的，由于Softmax的存在，加上的bias可以约掉：
\\begin{equation}\\frac{e^{\\boldsymbol{q}\\cdot(\\boldsymbol{k}\_n + \\boldsymbol{b})}}{\\sum\\limits\_n e^{\\boldsymbol{q}\\cdot(\\boldsymbol{k}\_n + \\boldsymbol{b})}} = \\frac{e^{\\boldsymbol{q}\\cdot\\boldsymbol{k}\_n}e^{\\boldsymbol{q}\\cdot\\boldsymbol{b}}}{\\sum\\limits\_n e^{\\boldsymbol{q}\\cdot\\boldsymbol{k}\_n} e^{\\boldsymbol{q}\\cdot\\boldsymbol{b}}}= \\frac{e^{\\boldsymbol{q}\\cdot\\boldsymbol{k}\_n}}{\\sum\\limits\_n e^{\\boldsymbol{q}\\cdot\\boldsymbol{k}\_n}}\\end{equation}
然而，这个“可以约掉”依赖于$\\boldsymbol{b}$跟$n$无关，但从式$\\eqref{eq:bias}$我们就知道，经过RoPE后，$\\boldsymbol{b}$也算是$m,n$的函数了，实际上是无法约掉的，因此对于加了RoPE的模型，Bias项去掉前后会有不一样的效果。

还有一个问题，就是为什么要费力探索长度外推性呢？直接在更长的样本下微调模型不行吗？事实上，即便是对于抱有这样想法的读者，长度外推性也是有好处的。抛开算力不说，更好的长度外推性意味着在微调的时候与预训练差距更小，于是微调更不容易发生灾难性遗忘，这对于当前的LLM更为重要了。当然，还可以发散一下，最理想的结果是：在短文本学习的模型，能够切换到长文本场景而无损效果甚至效果更优。

## 文章小结 [\#](https://kexue.fm/archives/9577\#%E6%96%87%E7%AB%A0%E5%B0%8F%E7%BB%93)

本文分享了笔者发现的一个“万万没想到”的有趣结论：Bias项能增强RoPE模型的长度外推性！看上去毫无存在感的Bias项，居然能跟Transformer的长度外推性联系在一起，让人不得不感叹细节的重要性——细枝末节有时候也能发挥关键作用。

_**转载到请包括本文地址：** [https://kexue.fm/archives/9577](https://kexue.fm/archives/9577)_

_**更详细的转载事宜请参考：**_ [《科学空间FAQ》](https://kexue.fm/archives/6508#%E6%96%87%E7%AB%A0%E5%A6%82%E4%BD%95%E8%BD%AC%E8%BD%BD/%E5%BC%95%E7%94%A8)

**如果您还有什么疑惑或建议，欢迎在下方评论区继续讨论。**

**如果您觉得本文还不错，欢迎 [分享](https://kexue.fm/archives/9577#share)/ [打赏](https://kexue.fm/archives/9577#pay) 本文。打赏并非要从中获得收益，而是希望知道科学空间获得了多少读者的真心关注。当然，如果你无视它，也不会影响你的阅读。再次表示欢迎和感谢！**

打赏

微信打赏

支付宝打赏

因为网站后台对打赏并无记录，因此欢迎在打赏时候备注留言。你还可以 [**点击这里**](http://mail.qq.com/cgi-bin/qm_share?t=qm_mailme&email=tN7d1drY3drrx8H0xcWa19vZ) 或在下方评论区留言来告知你的建议或需求。

**如果您需要引用本文，请参考：**

苏剑林. (Apr. 03, 2023). 《Bias项的神奇作用：RoPE + Bias = 更好的长度外推性 》\[Blog post\]. Retrieved from [https://kexue.fm/archives/9577](https://kexue.fm/archives/9577)

@online{kexuefm-9577,
        title={Bias项的神奇作用：RoPE + Bias = 更好的长度外推性},
        author={苏剑林},
        year={2023},
        month={Apr},
        url={\\url{https://kexue.fm/archives/9577}},
}

分类： [信息时代](https://kexue.fm/category/Big-Data)    标签： [语言模型](https://kexue.fm/tag/%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/), [attention](https://kexue.fm/tag/attention/), [位置编码](https://kexue.fm/tag/%E4%BD%8D%E7%BD%AE%E7%BC%96%E7%A0%81/), [外推](https://kexue.fm/tag/%E5%A4%96%E6%8E%A8/), [rope](https://kexue.fm/tag/rope/)[15 评论](https://kexue.fm/archives/9577#comments)

< [Google新作试图“复活”RNN：RNN能否再次辉煌？](https://kexue.fm/archives/9554) \| [从JL引理看熵不变性Attention](https://kexue.fm/archives/9588) >

### 你也许还对下面的内容感兴趣

- [Transformer升级之路：21、MLA好在哪里?（下）](https://kexue.fm/archives/11111)
- [“对角+低秩”三角阵的高效求逆方法](https://kexue.fm/archives/11072)
- [线性注意力简史：从模仿、创新到反哺](https://kexue.fm/archives/11033)
- [Transformer升级之路：20、MLA好在哪里?（上）](https://kexue.fm/archives/10907)
- [Transformer升级之路：19、第二类旋转位置编码](https://kexue.fm/archives/10862)
- [细水长flow之TARFLOW：流模型满血归来？](https://kexue.fm/archives/10667)
- [“闭门造车”之多模态思路浅谈（三）：位置编码](https://kexue.fm/archives/10352)
- [Decoder-only的LLM为什么需要位置编码？](https://kexue.fm/archives/10347)
- [Monarch矩阵：计算高效的稀疏型矩阵分解](https://kexue.fm/archives/10249)
- [Transformer升级之路：18、RoPE的底数选择原则](https://kexue.fm/archives/10122)

[发表你的看法](https://kexue.fm/archives/9577#comment_form)

心狠手辣小王

April 3rd, 2023

非常有意思的结论！不愧是苏神！
之前的实验中用dropout也可以有类似的效果 如果现在来看 把它从bias的角度就非常有趣了

[回复评论](https://kexue.fm/archives/9577/comment-page-1?replyTo=21305#respond-post-9577)

[tellw](https://cnblogs.com/tellw)

April 4th, 2023

太强了

[回复评论](https://kexue.fm/archives/9577/comment-page-1?replyTo=21308#respond-post-9577)

zw

April 4th, 2023

感觉bias也有两种：
1\. 输入特征在线性映射成Q和K的时候，linear层的bias为True，这个时候bias的维度是m\*d
2\. linear层的bias为False，Q和K分别reshape成多头的Qm和Km之后再加bias，这个时候bias的维度是d，多头共享bias

不知道苏神文中是指哪一种，以及这两种的外推性表现是否有区别？

[回复评论](https://kexue.fm/archives/9577/comment-page-1?replyTo=21314#respond-post-9577)

zw 发表于
April 4th, 2023

还有几种：
3\. 对所有位置，只用一个标量作为bias
4\. 每个位置上的特征用一个标量，不同位置不同标量

[回复评论](https://kexue.fm/archives/9577/comment-page-1?replyTo=21315#respond-post-9577)

zw 发表于
April 4th, 2023

总结来说就是bias形状的问题。

[回复评论](https://kexue.fm/archives/9577/comment-page-1?replyTo=21316#respond-post-9577)

[苏剑林](https://kexue.fm) 发表于
April 4th, 2023

不知道你这里的$m$指的是head的数目，还是token的数目（序列长度）？

本文所实验的bias，就是Q、K的Linear层的bias设为True，但是对于Transformer来说，这样做的结果是所有位置的token共用同一个bias，而不是每个位置不同的bias，事实上一般也没有每个位置用不同bias的做法。

另外，还可以考虑的做法是：1、Q和K共用同一个向量bias；2、Q和K共用同一个标量bias。这些还在实验当中，有新结果了会继续分享，敬请期待。

另外，本文实验的GAU结构，只有一个head

[回复评论](https://kexue.fm/archives/9577/comment-page-1?replyTo=21339#respond-post-9577)

zz

April 11th, 2023

印象中刷到过几篇论文，他们在attention的Softmax里给注意力分数加上了一个可学习的量 softmax(QK + B)，但时间太久了找不到原文了，忘了他们是什么motivation

[回复评论](https://kexue.fm/archives/9577/comment-page-1?replyTo=21384#respond-post-9577)

[苏剑林](https://kexue.fm) 发表于
April 15th, 2023

这些研究不少，T5的位置编码也差不多是这一个形式。只不过本文讨论的是$Q,K$的bias，并非$A$的bias。

[回复评论](https://kexue.fm/archives/9577/comment-page-1?replyTo=21404#respond-post-9577)

[羡鱼](https://www.zhihu.com/people/hai-tan-shang-chong-hua-47)

May 8th, 2023

这真是万万没想到了~

[回复评论](https://kexue.fm/archives/9577/comment-page-1?replyTo=21546#respond-post-9577)

longlongman

May 11th, 2023

提供一个相关论文 TRAIN SHORT, TEST LONG: ATTENTION WITH LINEAR
BIASES ENABLES INPUT LENGTH EXTRAPOLATION （https://arxiv.org/pdf/2108.12409.pdf）

[回复评论](https://kexue.fm/archives/9577/comment-page-1?replyTo=21580#respond-post-9577)

[苏剑林](https://kexue.fm) 发表于
May 12th, 2023

这个不大相关。

[回复评论](https://kexue.fm/archives/9577/comment-page-1?replyTo=21611#respond-post-9577)

何以琛

July 18th, 2023

[@longlongman\|comment-21580](https://kexue.fm/archives/9577/comment-page-1#comment-21580)

这个不就是alibi的论文吗

[回复评论](https://kexue.fm/archives/9577/comment-page-1?replyTo=22271#respond-post-9577)

[探秘Transformer系列之（23）R12; 长度外推 \| 呱唧呱唧网](http://www.itfaba.com/jishufenxian/211006.html)

April 6th, 2025

\[...\]Bias项的神奇作用：RoPE + Bias = 更好的长度外推性\[...\]

[回复评论](https://kexue.fm/archives/9577/comment-page-1?replyTo=27327#respond-post-9577)

[Rui Wang](http://ruiwang1998.com)

May 19th, 2025

苏神update说了不太可复现，在这篇https://arxiv.org/pdf/2410.06205之后看一下有没有可能是因为bias能让qk更像paper中figure 2的左边而非右边？

[回复评论](https://kexue.fm/archives/9577/comment-page-1?replyTo=27633#respond-post-9577)

[苏剑林](https://kexue.fm) 发表于
May 28th, 2025

如果你没说错的话，像左边不是更好？

[回复评论](https://kexue.fm/archives/9577/comment-page-1?replyTo=27696#respond-post-9577)

[取消回复](https://kexue.fm/archives/9577#respond-post-9577)

你的大名

电子邮箱

个人网站（选填）

1\. 可以使用LaTeX代码，点击“预览效果”可查看效果；2. 可以通过点击评论楼层编号来引用该楼层；3. 网站可能会有点卡，如非确认评论失败，请 **不要重复点击提交**。

### 内容速览

[隐藏彩蛋](https://kexue.fm/archives/9577#%E9%9A%90%E8%97%8F%E5%BD%A9%E8%9B%8B)
[换成偏置](https://kexue.fm/archives/9577#%E6%8D%A2%E6%88%90%E5%81%8F%E7%BD%AE)
[实验结果](https://kexue.fm/archives/9577#%E5%AE%9E%E9%AA%8C%E7%BB%93%E6%9E%9C)
[延伸思考](https://kexue.fm/archives/9577#%E5%BB%B6%E4%BC%B8%E6%80%9D%E8%80%83)
[文章小结](https://kexue.fm/archives/9577#%E6%96%87%E7%AB%A0%E5%B0%8F%E7%BB%93)

### 智能搜索

支持整句搜索！网站自动使用 [结巴分词](https://github.com/fxsjy/jieba) 进行分词，并结合ngrams排序算法给出合理的搜索结果。

### 热门标签

[生成模型](https://kexue.fm/tag/%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B/) [attention](https://kexue.fm/tag/attention/) [优化](https://kexue.fm/tag/%E4%BC%98%E5%8C%96/) [语言模型](https://kexue.fm/tag/%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/) [模型](https://kexue.fm/tag/%E6%A8%A1%E5%9E%8B/) [网站](https://kexue.fm/tag/%E7%BD%91%E7%AB%99/) [概率](https://kexue.fm/tag/%E6%A6%82%E7%8E%87/) [梯度](https://kexue.fm/tag/%E6%A2%AF%E5%BA%A6/) [转载](https://kexue.fm/tag/%E8%BD%AC%E8%BD%BD/) [矩阵](https://kexue.fm/tag/%E7%9F%A9%E9%98%B5/) [微分方程](https://kexue.fm/tag/%E5%BE%AE%E5%88%86%E6%96%B9%E7%A8%8B/) [天象](https://kexue.fm/tag/%E5%A4%A9%E8%B1%A1/) [分析](https://kexue.fm/tag/%E5%88%86%E6%9E%90/) [深度学习](https://kexue.fm/tag/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/) [积分](https://kexue.fm/tag/%E7%A7%AF%E5%88%86/) [python](https://kexue.fm/tag/python/) [优化器](https://kexue.fm/tag/%E4%BC%98%E5%8C%96%E5%99%A8/) [力学](https://kexue.fm/tag/%E5%8A%9B%E5%AD%A6/) [无监督](https://kexue.fm/tag/%E6%97%A0%E7%9B%91%E7%9D%A3/) [扩散](https://kexue.fm/tag/%E6%89%A9%E6%95%A3/) [几何](https://kexue.fm/tag/%E5%87%A0%E4%BD%95/) [节日](https://kexue.fm/tag/%E8%8A%82%E6%97%A5/) [生活](https://kexue.fm/tag/%E7%94%9F%E6%B4%BB/) [文本生成](https://kexue.fm/tag/%E6%96%87%E6%9C%AC%E7%94%9F%E6%88%90/) [数论](https://kexue.fm/tag/%E6%95%B0%E8%AE%BA/)

### 随机文章

- [科学空间：2009年12月重要天象](https://kexue.fm/archives/297)
- [生成扩散模型漫谈（十）：统一扩散模型（理论篇）](https://kexue.fm/archives/9262)
- [为什么梯度裁剪能加速训练过程？一个简明的分析](https://kexue.fm/archives/7469)
- [OCR技术浅探：3. 特征提取(2)](https://kexue.fm/archives/3802)
- [开学啦！咱们来做完形填空～（讯飞杯）](https://kexue.fm/archives/4564)
- [为方轮自行车铺路](https://kexue.fm/archives/1630)
- [你所没有思考过的平行线问题](https://kexue.fm/archives/3243)
- [Cool Papers更新：简单搭建了一个站内检索系统](https://kexue.fm/archives/10088)
- [高斯型积分的微扰展开（二）](https://kexue.fm/archives/3241)
- [星座计划“破产”，重返月球搁浅](https://kexue.fm/archives/388)

### 最近评论

- [Laity](https://kexue.fm/archives/10958/comment-page-2#comment-28067): 苏老师好，有个问题想请教，这里的meanflow前提是使用了线性插补，基于线性插补来说平均速度...
- [bingps](https://kexue.fm/archives/11111/comment-page-1#comment-28066): 苏神怎么看Tri Dao最近提的GTA和GLA https://arxiv.org/pdf/2...
- [YyWangCS](https://kexue.fm/archives/11111/comment-page-1#comment-28064): 从剑林老师关于MLA的两篇文章学习到了很多，我想从工程框架的视角简单补充一下信息：
在上半篇文...
- [Namoe](https://kexue.fm/archives/11111/comment-page-1#comment-28062): typo: 总而言之一节的总结第4点 通过LoRA来金可能地兼顾->通过LoRA来尽可能地兼顾
- [阿呱](https://kexue.fm/archives/7359/comment-page-8#comment-28061): 你好我也碰到了类似的问题，请问您是怎么解决的呀？想参考一下方法
- [silver](https://kexue.fm/archives/10757/comment-page-3#comment-28060): 求问文中的$e\_i$是啥？是ds论文的“Algorithm 1”中提到的“violation ...
- [Truenobility303](https://kexue.fm/archives/10739/comment-page-2#comment-28059): 谢谢苏神的详细解答！
- [Truenobility303](https://kexue.fm/archives/10795/comment-page-1#comment-28058): 不好意思我的表述可能会误导性说错了，核心问题不在2\. 我觉得问题在于整套论述都基于谱条件满足那...
- [kw](https://kexue.fm/archives/11033/comment-page-1#comment-28057): 把所有M直接换成全1矩阵就行吧，比如DeltaNet变成$(QK^⊤)(I+KK^⊤⊙(1-I...
- [WB](https://kexue.fm/archives/10795/comment-page-1#comment-28056): 非常清楚的blog。我有一个小问题想问一下，推导的时候用的是不等式（10），这里左边O(1)，...

### 友情链接

- [Cool Papers](https://papers.cool)
- [数学研发](https://bbs.emath.ac.cn)
- [Seatop](http://www.seatop.com.cn/)
- [Xiaoxia](https://xiaoxia.org/)
- [积分表-网络版](https://kexue.fm/sci/integral/index.html)
- [丝路博傲](http://blog.dvxj.com/)
- [ph4ntasy 饭特稀](http://www.ph4ntasy.com/)
- [数学之家](http://www.2math.cn/)
- [有趣天文奇观](http://interesting-sky.china-vo.org/)
- [TwistedW](http://www.twistedwg.com/)
- [godweiyang](https://godweiyang.com/)
- [AI柠檬](https://blog.ailemon.net/)
- [王登科-DK博客](https://greatdk.com)
- [ESON](https://blog.eson.org/)
- [枫之羽](https://fzhiy.net/)
- [Mathor's blog](https://wmathor.com/)
- [coding-zuo](https://coding-zuo.github.io/)
- [博科园](https://www.bokeyuan.net/)
- [孔皮皮的博客](https://www.kppkkp.top/)
- [运鹏的博客](https://yunpengtai.top/)
- [jiming.site](https://jiming.site/)
- [OmegaXYZ](https://www.omegaxyz.com/)
- [Blog by Eacls](https://www.eacls.top/)
- [EAI猩球](https://www.robotech.ink/)
- [文举的博客](https://liwenju0.com/)
- [用代码打点酱油](https://bruceyuan.com/)
- [申请链接](https://kexue.fm/links.html)

本站采用创作共用版权协议，要求署名、非商业用途和保持一致。转载本站内容必须也遵循“ [署名-非商业用途-保持一致](http://creativecommons.org/licenses/by-nc-nd/2.5/cn/)”的创作共用协议。
© 2009-2025 Scientific Spaces. All rights reserved. Theme by [laogui](http://www.laogui.com). Powered by [Typecho](http://typecho.org). 备案号: [粤ICP备09093259号-1/2](https://beian.miit.gov.cn/)。