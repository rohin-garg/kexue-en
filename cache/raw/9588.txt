从JL引理看熵不变性Attention - 科学空间|Scientific Spaces
![MobileSideBar](https://kexue.fm/usr/themes/geekg/images/slide-button.png "MobileSideBar")
## SEARCH
## MENU
* [打赏](https://kexue.fm/reward.html)
* [公式](https://kexue.fm/latex.html)
* [天象](https://kexue.fm/ac.html)
* [链接](https://kexue.fm/links.html)
* [时光](https://kexue.fm/me.html)
* [博览](https://kexue.fm/science.html)
* [归档](https://kexue.fm/content.html)
## CATEGORIES
* [千奇百怪](https://kexue.fm/category/Everything)
* [天文探索](https://kexue.fm/category/Astronomy)
* [数学研究](https://kexue.fm/category/Mathematics)
* [物理化学](https://kexue.fm/category/Phy-chem)
* [信息时代](https://kexue.fm/category/Big-Data)
* [生物自然](https://kexue.fm/category/Biology)
* [图片摄影](https://kexue.fm/category/Photograph)
* [问题百科](https://kexue.fm/category/Questions)
* [生活/情感](https://kexue.fm/category/Life-Feeling)
* [资源共享](https://kexue.fm/category/Resources)
## NEWPOSTS
* [让炼丹更科学一些（五）：基于梯度精...](https://kexue.fm/archives/11530)
* [让炼丹更科学一些（四）：新恒等式，...](https://kexue.fm/archives/11494)
* [为什么DeltaNet要加L2 N...](https://kexue.fm/archives/11486)
* [让炼丹更科学一些（三）：SGD的终...](https://kexue.fm/archives/11480)
* [让炼丹更科学一些（二）：将结论推广...](https://kexue.fm/archives/11469)
* [滑动平均视角下的权重衰减和学习率](https://kexue.fm/archives/11459)
* [生成扩散模型漫谈（三十一）：预测数...](https://kexue.fm/archives/11428)
* [Muon优化器指南：快速上手与关键细节](https://kexue.fm/archives/11416)
* [AdamW的Weight RMS的...](https://kexue.fm/archives/11404)
* [n个正态随机数的最大值的渐近估计](https://kexue.fm/archives/11390)
## COMMENTS
* [Rapture D: 我有一个问题，为什么不考虑亥姆霍兹定理和斯托克斯公式。](https://kexue.fm/archives/11530/comment-page-1#comment-29104)
* [mofheka: 苏神是还在用jax是么？最近在做基于Google Pathwa...](https://kexue.fm/archives/11390/comment-page-1#comment-29103)
* [长琴: 看懂这篇博客也不是一件容易的事情。](https://kexue.fm/archives/11530/comment-page-1#comment-29102)
* [AlexLi: 苏老师，请教一下(7)式中将 $\\mu(x\_t)$ 传给$p...](https://kexue.fm/archives/9257/comment-page-4#comment-29101)
* [tyler\_zxc: "Performer的思想是将标准的Attention线性化，...](https://kexue.fm/archives/7921/comment-page-2#comment-29100)
* [我: 似乎并非mHC提出矩阵的思想？之前hyper connecti...](https://kexue.fm/archives/11494/comment-page-1#comment-29099)
* [winter: 苏神您好，假如对于比较均匀的attention weightP...](https://kexue.fm/archives/10847/comment-page-1#comment-29098)
* [苏剑林: KL散度、JS散度、W距离啥的，都行啊，看你喜欢哪个](https://kexue.fm/archives/8512/comment-page-2#comment-29097)
* [苏剑林: 没有绝对公平的对比方法，主要看你关心什么。比如，如果只关心推理...](https://kexue.fm/archives/9119/comment-page-14#comment-29096)
* [苏剑林: 如果我有时间重新搭建博客，应该会用python自己写了，而不用...](https://kexue.fm/links.html/comment-page-6#comment-29095)
## USERLOGIN
* [登录](https://kexue.fm/admin/login.php)
[科学空间|Scientific Spaces](https://kexue.fm)
* [登录](https://kexue.fm/admin/login.php)
* [打赏](https://kexue.fm/reward.html)
* [公式](https://kexue.fm/latex.html)
* [天象](https://kexue.fm/ac.html)
* [链接](https://kexue.fm/links.html)
* [时光](https://kexue.fm/me.html)
* [博览](https://kexue.fm/science.html)
* [归档](https://kexue.fm/content.html)
渴望成为一个小飞侠* [![](https://kexue.fm/usr/themes/geekg/images/rss.png)
欢迎订阅](https://kexue.fm/feed)
* [![](https://kexue.fm/usr/themes/geekg/images/mail.png)
个性邮箱](https://kexue.fm/archives/119)
* [![](https://kexue.fm/usr/themes/geekg/images/Saturn.png)
天象信息](https://kexue.fm/ac.html)
* [![](https://kexue.fm/usr/themes/geekg/images/iss.png)
观测ISS](https://kexue.fm/archives/41)
* [![](https://kexue.fm/usr/themes/geekg/images/pi.png)
LaTeX](https://kexue.fm/latex.html)
* [![](https://kexue.fm/usr/themes/geekg/images/mlogo.png)
关于博主](https://kexue.fm/me.html)
欢迎访问“科学空间”，这里将与您共同探讨自然科学，回味人生百态；也期待大家的分享～* [**千奇百怪**Everything](https://kexue.fm/category/Everything)
* [**天文探索**Astronomy](https://kexue.fm/category/Astronomy)
* [**数学研究**Mathematics](https://kexue.fm/category/Mathematics)
* [**物理化学**Phy-chem](https://kexue.fm/category/Phy-chem)
* [**信息时代**Big-Data](https://kexue.fm/category/Big-Data)
* [**生物自然**Biology](https://kexue.fm/category/Biology)
* [**图片摄影**Photograph](https://kexue.fm/category/Photograph)
* [**问题百科**Questions](https://kexue.fm/category/Questions)
* [**生活/情感**Life-Feeling](https://kexue.fm/category/Life-Feeling)
* [**资源共享**Resources](https://kexue.fm/category/Resources)
* [**千奇百怪**](https://kexue.fm/category/Everything)
* [**天文探索**](https://kexue.fm/category/Astronomy)
* [**数学研究**](https://kexue.fm/category/Mathematics)
* [**物理化学**](https://kexue.fm/category/Phy-chem)
* [**信息时代**](https://kexue.fm/category/Big-Data)
* [**生物自然**](https://kexue.fm/category/Biology)
* [**图片摄影**](https://kexue.fm/category/Photograph)
* [**问题百科**](https://kexue.fm/category/Questions)
* [**生活/情感**](https://kexue.fm/category/Life-Feeling)
* [**资源共享**](https://kexue.fm/category/Resources)
[首页](https://kexue.fm)[数学研究](https://kexue.fm/category/Mathematics)[信息时代](https://kexue.fm/category/Big-Data)从JL引理看熵不变性Attention
10Apr
# [从JL引理看熵不变性Attention](https://kexue.fm/archives/9588)
By苏剑林|2023-04-10|53061位读者|:
在[《从熵不变性看Attention的Scale操作》](https://kexue.fm/archives/8823)、[《熵不变性Softmax的一个快速推导》](https://kexue.fm/archives/9034)中笔者提出了熵不变性Softmax，简单来说就是往Softmax之前的Attention矩阵多乘上一个$\\log n$，理论上有助于增强长度外推性，其中$n$是序列长度。$\\log n$这个因子让笔者联系到了JL引理（[Johnson-Lindenstrauss引理](https://kexue.fm/archives/8679)），因为JL引理告诉我们编码$n$个向量只需要$\\mathcal{O}(\\log n)$的维度就行了，大家都是$\\log n$，这两者有没有什么关联呢？
## 熵不变性[#](#熵不变性)
我们知道，熵是不确定性的度量，用在注意力机制中，我们将它作为“集中注意力的程度”。所谓熵不变性，指的是不管序列长度$n$是多少，我们都要将注意力集中在关键的几个token上，而不要太过分散。为此，我们提出的熵不变性Attention形式为
\\begin{equation}Attention(Q,K,V) = softmax\\left(\\frac{\\log\_{512} n}{\\sqrt{d}}QK^{\\top}\\right)V\\label{eq:core}\\end{equation}
这里$Q,K\\in\\mathbb{R}^{n\\times d}$。跟常规的Attention相比，就是scale的因子多了个$\\log\_{512} n$，其中底数取512，是假设我们所有的超参数（比如$d$）都是为训练长度512调好的。当然，即便你计划中的预训练长度不是512，底数也可以直接无脑取512，结果基本不会有什么影响。
这个形式的原理也很直观，当$n$增大时，意味着有更多的token去平摊了注意力，导致注意力不集中，此时我们乘上一个关于$n$单调递增的因子，softmax之后它实际上就相当于原来概率的幂运算，由于概率都小于1，所以概率越小幂运算之后会变得更小，这样注意力重新变得集中起来。至于这个因子为什么是对数的形式，那就需要看开头文章的推导过程了。
## JL引理[#](#JL引理)
JL引理，全称“Johnson-Lindenstrauss引理”，是关于向量嵌入的一个重要结论，简单来说它就是告诉我们“要塞下$n$个向量，只需$\\mathcal{O}(\\log n)$维空间”（这里的$\\log$没有写出底数，默认都是以自然对数$e$为底），详细介绍可以参考[《让人惊叹的Johnson-Lindenstrauss引理：理论篇》](https://kexue.fm/archives/8679)。
有意思的是，早在笔者知道JL引理之前，就在[《最小熵原理（六）：词向量的维度应该怎么选择？》](https://kexue.fm/archives/7695)推导过同样的、甚至更具体的结果——嵌入$n$个词向量，大致上需要$8\\log n$维空间就行了。这个估计跟实际使用的维度很接近，比如$n$等于10万时，$8\\log n$算出来大概是92，而我们经常用的词向量维度也是一两百维这个量级。
另外，JL引理还可以用来解释注意力机制的多头性。如果代入$n=512$，那么$8\\log n\\approx 50$，这跟Attention的Q、K常用的投影维度（也就是key\_size，BERT里边是64，参考[这里](https://kexue.fm/archives/7325#Attention%E9%87%8C%E6%9C%89%E4%B8%AA%E7%93%B6%E9%A2%88)）很接近，这就告诉我们，如果序列长度时512，那么算Attention的Q、K的维度在50这个量级就够了，没必要用全部的hidden\_size（BERT base是768），省下来的维度可以转而用来做多头注意力。
更多相关讨论可以参考[《关于维度公式“n \> 8.33 log N”的可用性分析》](https://kexue.fm/archives/8711)、[《让人惊叹的Johnson-Lindenstrauss引理：应用篇》](https://kexue.fm/archives/8706)。
## 联系起来[#](#联系起来)
现在，我们就可以尝试JL引理跟熵不变性Attention联系起来了。
我们将Q、K的key\_size记为$d$，那么JL引理告诉我们，$d$的最佳选择应该是$d\_n=\\lambda \\log n$，这里的$\\lambda$是比例常数，具体是多少不重要。也就是说，理想情况下，$d$应该随着$n$的变化而变化，但很显然这样的设计并不容易实现，也不利于计算的并行化，所以实际情况下我们都只能使用固定的$d$。
假设我们选定了一个固定的$d$，并且假设这个$d$是为训练长度512设计的，那么我们可以得出$d = \\lambda \\log 512$，也就是$\\lambda = \\frac{d}{\\log 512}$，以及
\\begin{equation}d\_n = \\frac{d}{\\log 512}\\log n=d\\log\_{512} n\\end{equation}
对于$n\\neq 512$，理想情况下应该用$d\_n$维的投影维度，但实际用了$d$维，根据内积的定义$\\langle q,k\\rangle = \\sum\\limits\_{i=1}^d q\_i k\_i$，求和的项数正好等于维度数$d$，也就是说，理想情况下应该是$d\_n$项求和，但实际上变为了$d$项求和，那么直觉上来看，如果每一项的贡献接近，那么我们将结果乘以$\\frac{d\_n}{d}$后，能够让结果更接近$d\_n$项求和的理想情况，所以我们就得出，应当往$\\langle q,k\\rangle$中乘上因子
\\begin{equation}\\frac{d\_n}{d} = \\log\_{512} n\\end{equation}
来弥补实际情况与理想情况的差距。而常规的Scaled-Dot Attention乘上$\\log\_{512} n$后，正好是熵不变性Attention，也就是式$\\eqref{eq:core}$。
这样，我们就将JL引理跟熵不变性Attention联系了起来。注意这只是个直观的、定性的理解过程，很难从定量角度将它进一步严格化，事实上也没有必要进一步定量化了，因为JL引理本身更多也只是一个定性的结论。
## 文章小结[#](#文章小结)
本文构建了JL引理与熵不变性Attention之间的一个简单联系。
***转载到请包括本文地址：** [https://kexue.fm/archives/9588](https://kexue.fm/archives/9588)*
***更详细的转载事宜请参考：*** [《科学空间FAQ》](https://kexue.fm/archives/6508#文章如何转载/引用)
**如果您还有什么疑惑或建议，欢迎在下方评论区继续讨论。**
**如果您觉得本文还不错，欢迎[分享](#share)/[打赏](#pay)本文。打赏并非要从中获得收益，而是希望知道科学空间获得了多少读者的真心关注。当然，如果你无视它，也不会影响你的阅读。再次表示欢迎和感谢！**
打赏![科学空间](https://kexue.fm/usr/themes/geekg/payment/wx.png)
微信打赏![科学空间](https://kexue.fm/usr/themes/geekg/payment/zfb.png)
支付宝打赏因为网站后台对打赏并无记录，因此欢迎在打赏时候备注留言。 你还可以[**点击这里**](http://mail.qq.com/cgi-bin/qm_share?t=qm_mailme&email=tN7d1drY3drrx8H0xcWa19vZ)或在下方评论区留言来告知你的建议或需求。
**如果您需要引用本文，请参考：**
苏剑林. (Apr. 10, 2023). 《从JL引理看熵不变性Attention 》[Blog post]. Retrieved from[https://kexue.fm/archives/9588](https://kexue.fm/archives/9588)
@online{kexuefm-9588,
title={从JL引理看熵不变性Attention},
author={苏剑林},
year={2023},
month={Apr},
url={\\url{https://kexue.fm/archives/9588}},
}
分类：[数学研究](https://kexue.fm/category/Mathematics),[信息时代](https://kexue.fm/category/Big-Data) 标签：[熵](https://kexue.fm/tag/熵/),[attention](https://kexue.fm/tag/attention/)[14 评论](https://kexue.fm/archives/9588#comments)
&lt;[Bias项的神奇作用：RoPE + Bias = 更好的长度外推性](https://kexue.fm/archives/9577)|[梯度视角下的LoRA：简介、分析、猜测及推广](https://kexue.fm/archives/9590)&gt;
### 你也许还对下面的内容感兴趣* [为什么DeltaNet要加L2 Normalize？](https://kexue.fm/archives/11486)
* [低精度Attention可能存在有偏的舍入误差](https://kexue.fm/archives/11371)
* [为什么线性注意力要加Short Conv？](https://kexue.fm/archives/11320)
* [QK-Clip：让Muon在Scaleup之路上更进一步](https://kexue.fm/archives/11126)
* [Transformer升级之路：21、MLA好在哪里?（下）](https://kexue.fm/archives/11111)
* [“对角+低秩”三角阵的高效求逆方法](https://kexue.fm/archives/11072)
* [线性注意力简史：从模仿、创新到反哺](https://kexue.fm/archives/11033)
* [Transformer升级之路：20、MLA好在哪里?（上）](https://kexue.fm/archives/10907)
* [Transformer升级之路：19、第二类旋转位置编码](https://kexue.fm/archives/10862)
* [矩阵的有效秩（Effective Rank）](https://kexue.fm/archives/10847)
[发表你的看法](#comment_form)
syxue3
April 11th, 2023
6666
[回复评论](https://kexue.fm/archives/9588/comment-page-1?replyTo=21386#respond-post-9588)
nepro
April 18th, 2023
为什么GPT之类的LLM一般习惯于一步到位取和inner\_dim一样的word\_emb呢？出于工程上的方便优化么？
[回复评论](https://kexue.fm/archives/9588/comment-page-1?replyTo=21419#respond-post-9588)
[苏剑林](https://kexue.fm)发表于 April 20th, 2023
这个没有太多的工作去考证过它的优异性，不过早年ALBERT的结果显示对token embedding进行低秩分解基本不会损失性能。
[回复评论](https://kexue.fm/archives/9588/comment-page-1?replyTo=21431#respond-post-9588)
nepro 发表于April 23rd, 2023
感谢！我自己在尝试visual embedding接入LLM的时候，因为维数差异较大，纠结了很久。最后发现bridge这部分对于最终结果影响远没有数据大。
[回复评论](https://kexue.fm/archives/9588/comment-page-1?replyTo=21448#respond-post-9588)
[苏剑林](https://kexue.fm)发表于 April 25th, 2023
数据为王。[回复评论](https://kexue.fm/archives/9588/comment-page-1?replyTo=21469#respond-post-9588)
hazdzz
May 20th, 2023
「所谓熵不变性，指的是不管序列长度$n$ 是多少，我们都要将注意力集中在关键的几个token 上，而不要太过分散」[回复评论](https://kexue.fm/archives/9588/comment-page-1?replyTo=21683#respond-post-9588)
hazdzz 发表于May 20th, 2023
這樣做到目的，是不是想要得到一個low-rank 的attention map？
[回复评论](https://kexue.fm/archives/9588/comment-page-1?replyTo=21684#respond-post-9588)
[苏剑林](https://kexue.fm)发表于 May 20th, 2023
不是low-rank，而是sparse。
[回复评论](https://kexue.fm/archives/9588/comment-page-1?replyTo=21691#respond-post-9588)
hazdzz 发表于May 21st, 2023
既然如此，一個簡單的方法是可以使用$\\mathrm{Sparsemax}$。
[回复评论](https://kexue.fm/archives/9588/comment-page-1?replyTo=21694#respond-post-9588)
[苏剑林](https://kexue.fm)发表于 May 23rd, 2023
Sparsemax及其后来改进我都略有关注，存在复杂度略高的问题。其实现在主要的问题不是如何实现sparse，而是要sparsity不变性（不随着长度的变化而变化）
[回复评论](https://kexue.fm/archives/9588/comment-page-1?replyTo=21711#respond-post-9588)
EarthsonLu
June 8th, 2023
苏神好:)
当n增大时，意味着有更多的token去平摊了注意力，导致注意力不集中。这句真是醍醐灌顶。
我觉得主要还是文本token的注意力存在局部性。所以当n增大，新增的token必然会稀释局部的注意力。
我在一个无向图场景下测了下熵不变性attention，表现还有所下降了。我是采样做的context。预测使用完整的context，并不会因为n增大而稀释注意力，因为新增的部分和原来的是同分布的，并不像文本那样（新增的都是远端token）。不知道我的理解是不是对。
[回复评论](https://kexue.fm/archives/9588/comment-page-1?replyTo=21906#respond-post-9588)
[苏剑林](https://kexue.fm)发表于 June 12th, 2023
“我在一个无向图场景下测了下熵不变性attention”，是训练阶段就加入，还是后处理加入呢？
独立同分布情况下，可能确实不进行log n缩放比较好，这种情况下有点像In-Context Learning，需要平均多个结果来做出准确预测，而不是单独关注某个case。
[回复评论](https://kexue.fm/archives/9588/comment-page-1?replyTo=21940#respond-post-9588)
EarthsonLu 发表于June 29th, 2023
训练的时候我抽的固定大小的context。
我这两天又想了一下，应该还是有一个被稀释的问题，不过和seq场景不一样。被稀释的是self attention的self，我猜测自身的attention肯定会被稀释（至少和其它样本不一样），相当于是一个大小为1的窗口邻域（只包含它自身）。我试试把Attention矩阵的diag扣掉看看（用mask=\~torch.eye(n, dtype=bool)）
[回复评论](https://kexue.fm/archives/9588/comment-page-1?replyTo=22107#respond-post-9588)
[苏剑林](https://kexue.fm)发表于 July 1st, 2023
是因为你的场景下self特别重要吗？但即便如此，似乎也没法解释log n缩放会下降的问题。
[回复评论](https://kexue.fm/archives/9588/comment-page-1?replyTo=22123#respond-post-9588)
[取消回复](https://kexue.fm/archives/9588#respond-post-9588)
你的大名电子邮箱个人网站（选填）1. 可以使用LaTeX代码，点击“预览效果”可查看效果；
2. 可以通过点击评论楼层编号来引用该楼层；3. 网站可能会有点卡，如非确认评论失败，请**不要重复点击提交**。
********************
### 内容速览* [熵不变性](#熵不变性)
* [JL引理](#JL引理)
* [联系起来](#联系起来)
* [文章小结](#文章小结)
********************
### 智能搜索支持整句搜索！网站自动使用[结巴分词](https://github.com/fxsjy/jieba)进行分词，并结合ngrams排序算法给出合理的搜索结果。
********************
### 热门标签[生成模型](https://kexue.fm/tag/生成模型/)[attention](https://kexue.fm/tag/attention/)[优化](https://kexue.fm/tag/优化/)[语言模型](https://kexue.fm/tag/语言模型/)[模型](https://kexue.fm/tag/模型/)[梯度](https://kexue.fm/tag/梯度/)[网站](https://kexue.fm/tag/网站/)[概率](https://kexue.fm/tag/概率/)[矩阵](https://kexue.fm/tag/矩阵/)[优化器](https://kexue.fm/tag/优化器/)[转载](https://kexue.fm/tag/转载/)[微分方程](https://kexue.fm/tag/微分方程/)[分析](https://kexue.fm/tag/分析/)[天象](https://kexue.fm/tag/天象/)[深度学习](https://kexue.fm/tag/深度学习/)[积分](https://kexue.fm/tag/积分/)[python](https://kexue.fm/tag/python/)[扩散](https://kexue.fm/tag/扩散/)[力学](https://kexue.fm/tag/力学/)[无监督](https://kexue.fm/tag/无监督/)[几何](https://kexue.fm/tag/几何/)[节日](https://kexue.fm/tag/节日/)[生活](https://kexue.fm/tag/生活/)[文本生成](https://kexue.fm/tag/文本生成/)[数论](https://kexue.fm/tag/数论/)
********************
********************
### 随机文章* [未来的天地枢纽——太空天梯](https://kexue.fm/archives/994)
* [背景资料：从数字看诺贝尔物理学奖](https://kexue.fm/archives/166)
* [自然数集中 N = ab + c 时a + b + c 的最小值](https://kexue.fm/archives/9775)
* [当概率遇上复变：从二项分布到泊松分布](https://kexue.fm/archives/3188)
* [美华裔教授破百年物理定律 获国际同行喝彩(图)](https://kexue.fm/archives/52)
* [泛化性乱弹：从随机噪声、梯度惩罚到虚拟对抗训练](https://kexue.fm/archives/7466)
* [力学系统及其对偶性（一）](https://kexue.fm/archives/2121)
* [科学空间：2010年11月重要天象](https://kexue.fm/archives/998)
* [基于双向LSTM和迁移学习的seq2seq核心实体识别](https://kexue.fm/archives/3942)
* [《哈勃太空望远镜超高清原始片源》VeryCD资源](https://kexue.fm/archives/173)
********************
********************
### 最近评论* [Rapture D](https://kexue.fm/archives/11530/comment-page-1#comment-29104): 我有一个问题，为什么不考虑亥姆霍兹定理和斯托克斯公式。* [mofheka](https://kexue.fm/archives/11390/comment-page-1#comment-29103): 苏神是还在用jax是么？最近在做基于Google Pathway的理念做一个动态版的MPMD框...
* [长琴](https://kexue.fm/archives/11530/comment-page-1#comment-29102): 看懂这篇博客也不是一件容易的事情。* [AlexLi](https://kexue.fm/archives/9257/comment-page-4#comment-29101): 苏老师，请教一下(7)式中将 $\\mu(x\_t)$ 传给$p\_o$ 进行推理的操作。$x\_...
* [tyler\_zxc](https://kexue.fm/archives/7921/comment-page-2#comment-29100): "Performer的思想是将标准的Attention线性化，所以为什么不干脆直接训练一个线性...
* [我](https://kexue.fm/archives/11494/comment-page-1#comment-29099): 似乎并非mHC提出矩阵的思想？之前hyper connection就是了
* [winter](https://kexue.fm/archives/10847/comment-page-1#comment-29098): 苏神您好，假如对于比较均匀的attention weightP，往往呈现long tail分布...
* [苏剑林](https://kexue.fm/archives/8512/comment-page-2#comment-29097): KL散度、JS散度、W距离啥的，都行啊，看你喜欢哪个
* [苏剑林](https://kexue.fm/archives/9119/comment-page-14#comment-29096): 没有绝对公平的对比方法，主要看你关心什么。比如，如果只关心推理成本和推理效果，那么有的方法可以...
* [苏剑林](https://kexue.fm/links.html/comment-page-6#comment-29095): 如果我有时间重新搭建博客，应该会用python自己写了，而不用第三方架构，这样可玩性好很多。事...
********************
********************
### 友情链接* [Cool Papers](https://papers.cool)
* [数学研发](https://bbs.emath.ac.cn)
* [Seatop](http://www.seatop.com.cn/)
* [Xiaoxia](https://xiaoxia.org/)
* [积分表-网络版](https://kexue.fm/sci/integral/index.html)
* [丝路博傲](http://blog.dvxj.com/)
* [数学之家](http://www.2math.cn/)
* [有趣天文奇观](http://interesting-sky.china-vo.org/)
* [TwistedW](http://www.twistedwg.com/)
* [godweiyang](https://godweiyang.com/)
* [AI柠檬](https://blog.ailemon.net/)
* [王登科-DK博客](https://greatdk.com)
* [ESON](https://blog.eson.org/)
* [枫之羽](https://fzhiy.net/)
* [coding-zuo](https://coding-zuo.github.io/)
* [博科园](https://www.bokeyuan.net/)
* [孔皮皮的博客](https://www.kppkkp.top/)
* [运鹏的博客](https://yunpengtai.top/)
* [jiming.site](https://jiming.site/)
* [OmegaXYZ](https://www.omegaxyz.com/)
* [EAI猩球](https://www.robotech.ink/)
* [文举的博客](https://liwenju0.com/)
* [申请链接](https://kexue.fm/links.html)
********************
[![署名-非商业用途-保持一致](https://kexue.fm/usr/themes/geekg/images/cc.gif)](http://creativecommons.org/licenses/by-nc-nd/2.5/cn/)本站采用创作共用版权协议，要求署名、非商业用途和保持一致。转载本站内容必须也遵循“[署名-非商业用途-保持一致](http://creativecommons.org/licenses/by-nc-nd/2.5/cn/)”的创作共用协议。
©2009-2026 Scientific Spaces. All rights reserved. Theme by[laogui](http://www.laogui.com). Powered by[Typecho](http://typecho.org). 备案号:[粤ICP备09093259号-1/2](https://beian.miit.gov.cn/)。