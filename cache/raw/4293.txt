## SEARCH

## MENU

- [打赏](https://kexue.fm/reward.html)
- [公式](https://kexue.fm/latex.html)
- [天象](https://kexue.fm/ac.html)
- [链接](https://kexue.fm/links.html)
- [时光](https://kexue.fm/me.html)
- [博览](https://kexue.fm/science.html)
- [归档](https://kexue.fm/content.html)

## CATEGORIES

- [千奇百怪](https://kexue.fm/category/Everything)
- [天文探索](https://kexue.fm/category/Astronomy)
- [数学研究](https://kexue.fm/category/Mathematics)
- [物理化学](https://kexue.fm/category/Phy-chem)
- [信息时代](https://kexue.fm/category/Big-Data)
- [生物自然](https://kexue.fm/category/Biology)
- [图片摄影](https://kexue.fm/category/Photograph)
- [问题百科](https://kexue.fm/category/Questions)
- [生活/情感](https://kexue.fm/category/Life-Feeling)
- [资源共享](https://kexue.fm/category/Resources)

## NEWPOSTS

- [为什么DeltaNet要加L2 N...](https://kexue.fm/archives/11486)
- [让炼丹更科学一些（三）：SGD的终...](https://kexue.fm/archives/11480)
- [让炼丹更科学一些（二）：将结论推广...](https://kexue.fm/archives/11469)
- [滑动平均视角下的权重衰减和学习率](https://kexue.fm/archives/11459)
- [生成扩散模型漫谈（三十一）：预测数...](https://kexue.fm/archives/11428)
- [Muon优化器指南：快速上手与关键细节](https://kexue.fm/archives/11416)
- [AdamW的Weight RMS的...](https://kexue.fm/archives/11404)
- [n个正态随机数的最大值的渐近估计](https://kexue.fm/archives/11390)
- [流形上的最速下降：5\. 对偶梯度下降](https://kexue.fm/archives/11388)
- [低精度Attention可能存在有...](https://kexue.fm/archives/11371)

## COMMENTS

- [kaiyuan: 看了“Linear Transformers Are Secr...](https://kexue.fm/archives/11486/comment-page-1#comment-29036)
- [sog: 好的，符号相同，搞混了呃](https://kexue.fm/archives/11469/comment-page-1#comment-29035)
- [kerry: 还没有通读完后面的系列，提出一些拙见。\
降低方差这一节把原本的...](https://kexue.fm/archives/9119/comment-page-14#comment-29034)
- [Kevin Yin: I wrote https://research.novela...](https://kexue.fm/archives/11158/comment-page-1#comment-29033)
- [罗: 公式(6)显示出来是不是有点小问题？](https://kexue.fm/archives/11480/comment-page-1#comment-29032)
- [cmlin: 本人对这方面不太熟悉，想了解这三个条件的意义及动机，且希望这系...](https://kexue.fm/archives/11340/comment-page-1#comment-29031)
- [喝一口可乐: 理解了，感谢苏神回复，数学上给出建模分析确实清晰了很多，再次感...](https://kexue.fm/archives/10958/comment-page-3#comment-29030)
- [CuddleSabe1: 感觉普通的 flow matching 可以看成 degrad...](https://kexue.fm/archives/10958/comment-page-1#comment-29029)
- [岁月如书: 受教了，感谢](https://kexue.fm/archives/11126/comment-page-3#comment-29028)
- [苏剑林: 是](https://kexue.fm/archives/11126/comment-page-3#comment-29027)

## USERLOGIN

- [登录](https://kexue.fm/admin/login.php)

[科学空间\|Scientific Spaces](https://kexue.fm)

- [登录](https://kexue.fm/admin/login.php)
- [打赏](https://kexue.fm/reward.html)
- [公式](https://kexue.fm/latex.html)
- [天象](https://kexue.fm/ac.html)
- [链接](https://kexue.fm/links.html)
- [时光](https://kexue.fm/me.html)
- [博览](https://kexue.fm/science.html)
- [归档](https://kexue.fm/content.html)

渴望成为一个小飞侠

- [欢迎订阅](https://kexue.fm/feed)
- [个性邮箱](https://kexue.fm/archives/119)
- [天象信息](https://kexue.fm/ac.html)
- [观测ISS](https://kexue.fm/archives/41)
- [LaTeX](https://kexue.fm/latex.html)
- [关于博主](https://kexue.fm/me.html)

欢迎访问“科学空间”，这里将与您共同探讨自然科学，回味人生百态；也期待大家的分享～

- [**千奇百怪** Everything](https://kexue.fm/category/Everything)
- [**天文探索** Astronomy](https://kexue.fm/category/Astronomy)
- [**数学研究** Mathematics](https://kexue.fm/category/Mathematics)
- [**物理化学** Phy-chem](https://kexue.fm/category/Phy-chem)
- [**信息时代** Big-Data](https://kexue.fm/category/Big-Data)
- [**生物自然** Biology](https://kexue.fm/category/Biology)
- [**图片摄影** Photograph](https://kexue.fm/category/Photograph)
- [**问题百科** Questions](https://kexue.fm/category/Questions)
- [**生活/情感** Life-Feeling](https://kexue.fm/category/Life-Feeling)
- [**资源共享** Resources](https://kexue.fm/category/Resources)

- [**千奇百怪**](https://kexue.fm/category/Everything)
- [**天文探索**](https://kexue.fm/category/Astronomy)
- [**数学研究**](https://kexue.fm/category/Mathematics)
- [**物理化学**](https://kexue.fm/category/Phy-chem)
- [**信息时代**](https://kexue.fm/category/Big-Data)
- [**生物自然**](https://kexue.fm/category/Biology)
- [**图片摄影**](https://kexue.fm/category/Photograph)
- [**问题百科**](https://kexue.fm/category/Questions)
- [**生活/情感**](https://kexue.fm/category/Life-Feeling)
- [**资源共享**](https://kexue.fm/category/Resources)

[首页](https://kexue.fm) [信息时代](https://kexue.fm/category/Big-Data) 文本情感分类（四）：更好的损失函数

30Mar

# [文本情感分类（四）：更好的损失函数](https://kexue.fm/archives/4293)

By 苏剑林 \|
2017-03-30 \|
162230位读者\|

文本情感分类其实就是一个二分类问题，事实上，对于分类模型，都会存在这样一个毛病：优化目标跟考核指标不一致。通常来说，对于分类（包括多分类），我们都会采用交叉熵作为损失函数，它的来源就是最大似然估计（参考 [《梯度下降和EM算法：系出同源，一脉相承》](https://kexue.fm/archives/4277/)）。但是，我们最后的评估目标，并非要看交叉熵有多小，而是看模型的准确率。一般来说，交叉熵很小，准确率也会很高，但这个关系并非必然的。

## 要平均，不一定要拔尖 [\#](https://kexue.fm/kexue.fm\#%E8%A6%81%E5%B9%B3%E5%9D%87%EF%BC%8C%E4%B8%8D%E4%B8%80%E5%AE%9A%E8%A6%81%E6%8B%94%E5%B0%96)

> 一个更通俗的例子是：一个数学老师，在努力提高同学们的平均分，但期末考核的指标却是及格率（60分及格）。假如平均分是100分（也就意味着所有同学都考到了100分），那么自然及格率是100%，这是最理想的。但现实不一定这么美好，平均分越高，只要平均分还没有达到100，那么及格率却不一定越高，比如两个人分别考40和90，那么平均分就是65，及格率只有50%；如果两个人的成绩都是60，平均分就是60，及格率却有100%。这也就是说，平均分可以作为一个目标，但这个目标并不直接跟考核目标挂钩。
>
> 那么，为了提升最后的考核目标，这个老师应该怎么做呢？很显然，首先看看所有学生中，哪些同学已经及格了，及格的同学先不管他们，而针对不及格的同学进行补课加强，这样一来，原则上来说有很多不及格的同学都能考上60分了，也有可能一些本来及格的同学考不够60分了，但这个过程可以迭代，最终使得大家都在60分以上，当然，最终的平均分不一定很高，但没办法，谁叫考核目标是及格率呢？

## 更好的更新方案 [\#](https://kexue.fm/kexue.fm\#%E6%9B%B4%E5%A5%BD%E7%9A%84%E6%9B%B4%E6%96%B0%E6%96%B9%E6%A1%88)

对于二分类模型，我们总希望模型能够给正样本输出1，负样本输出0，但限于模型的拟合能力等问题，一般来说做不到这一点。而事实上在预测中，我们也是认为大于0.5的就是正样本了，小于0.5的就是负样本。这样就意味着，我们可以“有选择”地更新模型，比如，设定一个阈值为0.6，那么模型对某个正样本的输出大于0.6，我就不根据这个样本来更新模型了，模型对某个负样本的输出小于0.4，我也不根据这个样本来更新模型了，只有在0.4~0.6之间的，才让模型更新，这时候模型会更“集中精力”去关心那些“模凌两可”的样本，从而使得分类效果更好，这跟传统的SVM思想是一致的。

不仅如此，这样的做法理论上还能防止过拟合，因为它防止了模型专门挑那些容易拟合的样本来“拼命”拟合（使得损失函数下降），这就好比老师只关心优生，希望优生能从80分提高到90分，而不想办法提高差生的成绩，这显然不是一个好老师。

## 修正的交叉熵损失 [\#](https://kexue.fm/kexue.fm\#%E4%BF%AE%E6%AD%A3%E7%9A%84%E4%BA%A4%E5%8F%89%E7%86%B5%E6%8D%9F%E5%A4%B1)

怎样才能达到我们上面说的目的呢？很简单，调整损失函数即可，这里主要借鉴了hinge loss和triplet loss的思想。一般常用的交叉熵损失函数是：
$$L\_{old} = -\\sum\_y y\_{true} \\log y\_{pred}$$

选定一个阈值$m=0.6$，这个阈值原则上大于0.5均可。引入单位阶跃函数$\\theta(x)$：
$$\\theta(x) = \\left\\{\\begin{aligned}&1, x > 0\\\
&\\frac{1}{2}, x = 0\\\
&0, x < 0\\end{aligned}\\right.$$

那么，考虑新的损失函数：
$$L\_{new} = -\\sum\_y \\lambda(y\_{true}, y\_{pred}) y\_{true}\\log y\_{pred}$$
其中
$$\\lambda(y\_{true}, y\_{pred}) = 1-\\theta(y\_{true}-m)\\theta(y\_{pred}-m)-\\theta(1-m-y\_{true})\\theta(1-m-y\_{pred})$$
$L\_{new}$就是在交叉熵的基础上加入了修正项$\\lambda(y\_{true}, y\_{pred})$，这一项意味着什么呢？当进入一个正样本时，那么$y\_{true}=1$，显然
$$\\lambda(1, y\_{pred})=1-\\theta(y\_{pred}-m)$$
这时候，要是$y\_{pred} > m$，那么$\\lambda(1, y\_{pred})=0$，这时候交叉熵自动为0（达到最小值），反之，$y\_{pred} < m$则有$\\lambda(1, y\_{pred})=1$，这时候保持交叉熵，也就是说，正样本如果输出已经大于$m$了，那就不更新了（因为达到了最小值，可以认为最小值梯度是0），小于$m$才继续更新；类似地可以分析负样本的情形，结论是负样本如果输出已经小于$1-m$了，那就不更新了，大于$1-m$才继续更新。

这样一来， **只要将原始的交叉熵损失，换成修正的交叉熵$L\_{new}$，就可以达到我们开始设计的目的了**。

## 基于IMDB的实验测试 [\#](https://kexue.fm/kexue.fm\#%E5%9F%BA%E4%BA%8EIMDB%E7%9A%84%E5%AE%9E%E9%AA%8C%E6%B5%8B%E8%AF%95)

理论看上去很美好，实测效果是不是如想象一样呢？马上来实验。

为了使得结果更具有对比性，这次我选择了文本情感分类中的一个标准任务：IMDB电影评论的分类来进行评测，使用的工具是Keras最新版（2.0）。大部分的代码在 [Keras的examples中](https://github.com/fchollet/keras/tree/master/examples) 都可以找到，LSTM和CNN等等都有。

首先是LSTM的：

```
from keras.preprocessing import sequence
from keras.models import Sequential
from keras.layers import Embedding, LSTM, Dense
from keras.datasets import imdb
from keras import backend as K

margin = 0.6
theta = lambda t: (K.sign(t)+1.)/2.

max_features = 20000
maxlen = 80
batch_size = 32

(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)

x_train = sequence.pad_sequences(x_train, maxlen=maxlen)
x_test = sequence.pad_sequences(x_test, maxlen=maxlen)

model = Sequential()
model.add(Embedding(max_features, 128))
model.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))
model.add(Dense(1, activation='sigmoid'))

def loss(y_true, y_pred):
 return - (1 - theta(y_true - margin) * theta(y_pred - margin)
 - theta(1 - margin - y_true) * theta(1 - margin - y_pred)
 ) * (y_true * K.log(y_pred + 1e-8) + (1 - y_true) * K.log(1 - y_pred + 1e-8))

model.compile(loss=loss,
 optimizer='adam',
 metrics=['accuracy'])

model.fit(x_train, y_train,
 batch_size=batch_size,
 epochs=15,
 validation_data=(x_test, y_test))
```

代码基本上照搬官方的，不解释～运行结束后，模型得到了99.01%的训练准确率和82.26%的测试准确率，如果把loss直接改为binary\_crossentropy（其它都不改变），那么得到99.56%的训练准确率和81.02%的测试准确率，说明新的loss函数确实有助于防止过拟合，提升准确率。当然，这个测试可能会有随机误差，但多次平均的结果仍然显示，新的loss函数大概能带来0.5%～1%的准确率提升（当然，仅仅稍微改变了loss函数，你不能指望有跨越性的提升。）。

再看看CNN的：

```
from keras.preprocessing import sequence
from keras.models import Sequential
from keras.layers import Embedding, Dense, Dropout, Activation
from keras.layers import Conv1D, GlobalMaxPooling1D
from keras.datasets import imdb
from keras import backend as K

margin = 0.6
theta = lambda t: (K.sign(t)+1.)/2.

max_features = 5000
maxlen = 400
batch_size = 32
embedding_dims = 50
filters = 250
kernel_size = 3
hidden_dims = 250
epochs = 10

(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)
x_train = sequence.pad_sequences(x_train, maxlen=maxlen)
x_test = sequence.pad_sequences(x_test, maxlen=maxlen)

model = Sequential()
model.add(Embedding(max_features,
 embedding_dims,
 input_length=maxlen))
model.add(Dropout(0.2))
model.add(Conv1D(filters,
 kernel_size,
 padding='valid',
 activation='relu',
 strides=1))
model.add(GlobalMaxPooling1D())
model.add(Dense(hidden_dims))
model.add(Dropout(0.2))
model.add(Activation('relu'))
model.add(Dense(1))
model.add(Activation('sigmoid'))

def loss(y_true, y_pred):
 return - (1 - theta(y_true - margin) * theta(y_pred - margin)
 - theta(1 - margin - y_true) * theta(1 - margin - y_pred)
 ) * (y_true * K.log(y_pred + 1e-8) + (1 - y_true) * K.log(1 - y_pred + 1e-8))

model.compile(loss=loss,
 optimizer='adam',
 metrics=['accuracy'])

model.fit(x_train, y_train,
 batch_size=batch_size,
 epochs=epochs,
 validation_data=(x_test, y_test))
```

运行结束后，模型得到了98.66%的训练准确率和88.24%的测试准确率，纯粹的binary\_crossentropy的结果为98.90%的训练准确率和88.14%的测试准确率，两个结果基本一致，在波动范围内。但是，在训练过程中，用新loss函数的测试结果，一直保持在88.2%左右不变，而使用交叉熵时，一会跳到89%，一会跳到87%，一会又跳回88%，也就是说，虽然最终大家的准确率一致，但用交叉熵的时候，波动更大，我们有理由相信，新loss函数训练出来的模型，泛化能力更加好。

## 简而言之 [\#](https://kexue.fm/kexue.fm\#%E7%AE%80%E8%80%8C%E8%A8%80%E4%B9%8B)

本文主要借鉴了hinge loss和triplet loss的思想，对二分类所用的交叉熵损失做了一个调整，使得它更加有效地去拟合预测错误的样本，实验也表明某种意义上新的损失函数确实能带来一点小提升。

另外，这种思想事实上还可以用于多分类甚至回归问题，这里就不详细举例了，只能是遇到分析再跟大家分享了。

_**转载到请包括本文地址：** [https://kexue.fm/archives/4293](https://kexue.fm/archives/4293)_

_**更详细的转载事宜请参考：**_ [《科学空间FAQ》](https://kexue.fm/archives/6508#%E6%96%87%E7%AB%A0%E5%A6%82%E4%BD%95%E8%BD%AC%E8%BD%BD/%E5%BC%95%E7%94%A8)

**如果您还有什么疑惑或建议，欢迎在下方评论区继续讨论。**

**如果您觉得本文还不错，欢迎 [分享](https://kexue.fm/kexue.fm#share)/ [打赏](https://kexue.fm/kexue.fm#pay) 本文。打赏并非要从中获得收益，而是希望知道科学空间获得了多少读者的真心关注。当然，如果你无视它，也不会影响你的阅读。再次表示欢迎和感谢！**

打赏

微信打赏

支付宝打赏

因为网站后台对打赏并无记录，因此欢迎在打赏时候备注留言。你还可以 [**点击这里**](http://mail.qq.com/cgi-bin/qm_share?t=qm_mailme&email=tN7d1drY3drrx8H0xcWa19vZ) 或在下方评论区留言来告知你的建议或需求。

**如果您需要引用本文，请参考：**

苏剑林. (Mar. 30, 2017). 《文本情感分类（四）：更好的损失函数 》\[Blog post\]. Retrieved from [https://kexue.fm/archives/4293](https://kexue.fm/archives/4293)

@online{kexuefm-4293,
        title={文本情感分类（四）：更好的损失函数},
        author={苏剑林},
        year={2017},
        month={Mar},
        url={\\url{https://kexue.fm/archives/4293}},
}

分类： [信息时代](https://kexue.fm/category/Big-Data)    标签： [深度学习](https://kexue.fm/tag/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/), [文本挖掘](https://kexue.fm/tag/%E6%96%87%E6%9C%AC%E6%8C%96%E6%8E%98/), [损失函数](https://kexue.fm/tag/%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0/)[29 评论](https://kexue.fm/archives/4293#comments)

< [梯度下降和EM算法：系出同源，一脉相承](https://kexue.fm/archives/4277) \| [【不可思议的Word2Vec】 1.数学原理](https://kexue.fm/archives/4299) >

### 你也许还对下面的内容感兴趣

- [生成扩散模型漫谈（三十一）：预测数据而非噪声](https://kexue.fm/archives/11428)
- [MoE环游记：3、换个思路来分配](https://kexue.fm/archives/10757)
- [MoE环游记：2、不患寡而患不均](https://kexue.fm/archives/10735)
- [通向概率分布之路：盘点Softmax及其替代品](https://kexue.fm/archives/10145)
- [生成扩散模型漫谈（二十二）：信噪比与大图生成（上）](https://kexue.fm/archives/10047)
- [EMO：基于最优传输思想设计的分类损失函数](https://kexue.fm/archives/9797)
- [缓解交叉熵过度自信的一个简明方案](https://kexue.fm/archives/9526)
- [不成功的尝试：将多标签交叉熵推广到“n个m分类”上去](https://kexue.fm/archives/9158)
- [如何训练你的准确率？](https://kexue.fm/archives/9098)
- [多标签“Softmax+交叉熵”的软标签版本](https://kexue.fm/archives/9064)

[发表你的看法](https://kexue.fm/kexue.fm#comment_form)

1. [«](https://kexue.fm/archives/4293/comment-page-1#comments)
2. [1](https://kexue.fm/archives/4293/comment-page-1#comments)
3. [2](https://kexue.fm/archives/4293/comment-page-2#comments)

[『NLP学习笔记』多标签分类损失总结\_Johngo学长](https://www.johngo689.com/215955/)

November 27th, 2022

\[...\]苏建林-文本情感分类（四）：更好的损失函数 [https://kexue.fm/archives/4293](https://kexue.fm/archives/4293)\[...\]

[回复评论](https://kexue.fm/archives/4293/comment-page-2?replyTo=20471#respond-post-4293)

1. [«](https://kexue.fm/archives/4293/comment-page-1#comments)
2. [1](https://kexue.fm/archives/4293/comment-page-1#comments)
3. [2](https://kexue.fm/archives/4293/comment-page-2#comments)

[取消回复](https://kexue.fm/archives/4293#respond-post-4293)

你的大名

电子邮箱

个人网站（选填）

1\. 可以使用LaTeX代码，点击“预览效果”可查看效果；2. 可以通过点击评论楼层编号来引用该楼层；3. 网站可能会有点卡，如非确认评论失败，请 **不要重复点击提交**。

### 内容速览

[要平均，不一定要拔尖](https://kexue.fm/kexue.fm#%E8%A6%81%E5%B9%B3%E5%9D%87%EF%BC%8C%E4%B8%8D%E4%B8%80%E5%AE%9A%E8%A6%81%E6%8B%94%E5%B0%96)
[更好的更新方案](https://kexue.fm/kexue.fm#%E6%9B%B4%E5%A5%BD%E7%9A%84%E6%9B%B4%E6%96%B0%E6%96%B9%E6%A1%88)
[修正的交叉熵损失](https://kexue.fm/kexue.fm#%E4%BF%AE%E6%AD%A3%E7%9A%84%E4%BA%A4%E5%8F%89%E7%86%B5%E6%8D%9F%E5%A4%B1)
[基于IMDB的实验测试](https://kexue.fm/kexue.fm#%E5%9F%BA%E4%BA%8EIMDB%E7%9A%84%E5%AE%9E%E9%AA%8C%E6%B5%8B%E8%AF%95)
[简而言之](https://kexue.fm/kexue.fm#%E7%AE%80%E8%80%8C%E8%A8%80%E4%B9%8B)

### 智能搜索

支持整句搜索！网站自动使用 [结巴分词](https://github.com/fxsjy/jieba) 进行分词，并结合ngrams排序算法给出合理的搜索结果。

### 热门标签

[生成模型](https://kexue.fm/tag/%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B/) [attention](https://kexue.fm/tag/attention/) [优化](https://kexue.fm/tag/%E4%BC%98%E5%8C%96/) [语言模型](https://kexue.fm/tag/%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/) [模型](https://kexue.fm/tag/%E6%A8%A1%E5%9E%8B/) [网站](https://kexue.fm/tag/%E7%BD%91%E7%AB%99/) [梯度](https://kexue.fm/tag/%E6%A2%AF%E5%BA%A6/) [概率](https://kexue.fm/tag/%E6%A6%82%E7%8E%87/) [矩阵](https://kexue.fm/tag/%E7%9F%A9%E9%98%B5/) [优化器](https://kexue.fm/tag/%E4%BC%98%E5%8C%96%E5%99%A8/) [转载](https://kexue.fm/tag/%E8%BD%AC%E8%BD%BD/) [微分方程](https://kexue.fm/tag/%E5%BE%AE%E5%88%86%E6%96%B9%E7%A8%8B/) [分析](https://kexue.fm/tag/%E5%88%86%E6%9E%90/) [天象](https://kexue.fm/tag/%E5%A4%A9%E8%B1%A1/) [深度学习](https://kexue.fm/tag/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/) [积分](https://kexue.fm/tag/%E7%A7%AF%E5%88%86/) [python](https://kexue.fm/tag/python/) [扩散](https://kexue.fm/tag/%E6%89%A9%E6%95%A3/) [力学](https://kexue.fm/tag/%E5%8A%9B%E5%AD%A6/) [无监督](https://kexue.fm/tag/%E6%97%A0%E7%9B%91%E7%9D%A3/) [几何](https://kexue.fm/tag/%E5%87%A0%E4%BD%95/) [节日](https://kexue.fm/tag/%E8%8A%82%E6%97%A5/) [生活](https://kexue.fm/tag/%E7%94%9F%E6%B4%BB/) [文本生成](https://kexue.fm/tag/%E6%96%87%E6%9C%AC%E7%94%9F%E6%88%90/) [数论](https://kexue.fm/tag/%E6%95%B0%E8%AE%BA/)

### 随机文章

- [【分享】兴隆山的双子座流星雨](https://kexue.fm/archives/3580)
- [路径积分系列：5.例子和综述](https://kexue.fm/archives/3766)
- [2010年诺贝尔化学奖出炉,美日科学家分享](https://kexue.fm/archives/979)
- [《量子力学与路径积分》习题解答V0.4](https://kexue.fm/archives/3582)
- [重新思考学习率与Batch Size（三）：Muon](https://kexue.fm/archives/11285)
- [【中文分词系列】 4\. 基于双向LSTM的seq2seq字标注](https://kexue.fm/archives/3924)
- [大气光学质量(Airmass)](https://kexue.fm/archives/396)
- [细水长flow之TARFLOW：流模型满血归来？](https://kexue.fm/archives/10667)
- [\[欧拉数学\]凸多面体的面、顶、棱公式](https://kexue.fm/archives/1496)
- [【生活杂记】用电饭锅来煮米汤](https://kexue.fm/archives/10240)

### 最近评论

- [kaiyuan](https://kexue.fm/archives/11486/comment-page-1#comment-29036): 看了“Linear Transformers Are Secretly Fast Weight...
- [sog](https://kexue.fm/archives/11469/comment-page-1#comment-29035): 好的，符号相同，搞混了呃
- [kerry](https://kexue.fm/archives/9119/comment-page-14#comment-29034): 还没有通读完后面的系列，提出一些拙见。
降低方差这一节把原本的目标“预测单步的noise”变成...
- [Kevin Yin](https://kexue.fm/archives/11158/comment-page-1#comment-29033): I wrote https://research.novelai.net/muonscale/...
- [罗](https://kexue.fm/archives/11480/comment-page-1#comment-29032): 公式(6)显示出来是不是有点小问题？
- [cmlin](https://kexue.fm/archives/11340/comment-page-1#comment-29031): 本人对这方面不太熟悉，想了解这三个条件的意义及动机，且希望这系列可以继续写下去。以下想发表一些...
- [喝一口可乐](https://kexue.fm/archives/10958/comment-page-3#comment-29030): 理解了，感谢苏神回复，数学上给出建模分析确实清晰了很多，再次感谢苏神回复！
- [CuddleSabe1](https://kexue.fm/archives/10958/comment-page-1#comment-29029): 感觉普通的 flow matching 可以看成 degrade-aware image de...
- [岁月如书](https://kexue.fm/archives/11126/comment-page-3#comment-29028): 受教了，感谢
- [苏剑林](https://kexue.fm/archives/11126/comment-page-3#comment-29027): 是

### 友情链接

- [Cool Papers](https://papers.cool)
- [数学研发](https://bbs.emath.ac.cn)
- [Seatop](http://www.seatop.com.cn/)
- [Xiaoxia](https://xiaoxia.org/)
- [积分表-网络版](https://kexue.fm/sci/integral/index.html)
- [丝路博傲](http://blog.dvxj.com/)
- [数学之家](http://www.2math.cn/)
- [有趣天文奇观](http://interesting-sky.china-vo.org/)
- [TwistedW](http://www.twistedwg.com/)
- [godweiyang](https://godweiyang.com/)
- [AI柠檬](https://blog.ailemon.net/)
- [王登科-DK博客](https://greatdk.com)
- [ESON](https://blog.eson.org/)
- [枫之羽](https://fzhiy.net/)
- [coding-zuo](https://coding-zuo.github.io/)
- [博科园](https://www.bokeyuan.net/)
- [孔皮皮的博客](https://www.kppkkp.top/)
- [运鹏的博客](https://yunpengtai.top/)
- [jiming.site](https://jiming.site/)
- [OmegaXYZ](https://www.omegaxyz.com/)
- [EAI猩球](https://www.robotech.ink/)
- [文举的博客](https://liwenju0.com/)
- [申请链接](https://kexue.fm/links.html)

本站采用创作共用版权协议，要求署名、非商业用途和保持一致。转载本站内容必须也遵循“ [署名-非商业用途-保持一致](http://creativecommons.org/licenses/by-nc-nd/2.5/cn/)”的创作共用协议。
© 2009-2025 Scientific Spaces. All rights reserved. Theme by [laogui](http://www.laogui.com). Powered by [Typecho](http://typecho.org). 备案号: [粤ICP备09093259号-1/2](https://beian.miit.gov.cn/)。