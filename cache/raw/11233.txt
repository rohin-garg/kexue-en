## SEARCH

## MENU

- [打赏](https://kexue.fm/reward.html)
- [公式](https://kexue.fm/latex.html)
- [天象](https://kexue.fm/ac.html)
- [链接](https://kexue.fm/links.html)
- [时光](https://kexue.fm/me.html)
- [博览](https://kexue.fm/science.html)
- [归档](https://kexue.fm/content.html)

## CATEGORIES

- [千奇百怪](https://kexue.fm/category/Everything)
- [天文探索](https://kexue.fm/category/Astronomy)
- [数学研究](https://kexue.fm/category/Mathematics)
- [物理化学](https://kexue.fm/category/Phy-chem)
- [信息时代](https://kexue.fm/category/Big-Data)
- [生物自然](https://kexue.fm/category/Biology)
- [图片摄影](https://kexue.fm/category/Photograph)
- [问题百科](https://kexue.fm/category/Questions)
- [生活/情感](https://kexue.fm/category/Life-Feeling)
- [资源共享](https://kexue.fm/category/Resources)

## NEWPOSTS

- [让炼丹更科学一些（三）：SGD的终...](https://kexue.fm/archives/11480)
- [让炼丹更科学一些（二）：将结论推广...](https://kexue.fm/archives/11469)
- [滑动平均视角下的权重衰减和学习率](https://kexue.fm/archives/11459)
- [生成扩散模型漫谈（三十一）：预测数...](https://kexue.fm/archives/11428)
- [Muon优化器指南：快速上手与关键细节](https://kexue.fm/archives/11416)
- [AdamW的Weight RMS的...](https://kexue.fm/archives/11404)
- [n个正态随机数的最大值的渐近估计](https://kexue.fm/archives/11390)
- [流形上的最速下降：5\. 对偶梯度下降](https://kexue.fm/archives/11388)
- [低精度Attention可能存在有...](https://kexue.fm/archives/11371)
- [MuP之上：1. 好模型的三个特征](https://kexue.fm/archives/11340)

## COMMENTS

- [喝一口可乐: 理解了，感谢苏神回复，数学上给出建模分析确实清晰了很多，再次感...](https://kexue.fm/archives/10958/comment-page-3#comment-29030)
- [CuddleSabe1: 感觉普通的 flow matching 可以看成 degrad...](https://kexue.fm/archives/10958/comment-page-1#comment-29029)
- [岁月如书: 受教了，感谢](https://kexue.fm/archives/11126/comment-page-3#comment-29028)
- [苏剑林: 是](https://kexue.fm/archives/11126/comment-page-3#comment-29027)
- [岁月如书: 哦哦，原来是有实验结论，那是我盲目了。多问一句，你说的atte...](https://kexue.fm/archives/11126/comment-page-3#comment-29026)
- [苏剑林: attention sink指的是第一个token的atten...](https://kexue.fm/archives/11126/comment-page-3#comment-29025)
- [苏剑林: 这也许是好事呢？SGD倒是保留了模长，但它就普遍不如不保留模长...](https://kexue.fm/archives/11459/comment-page-1#comment-29024)
- [岁月如书: maxlogit 是attention qk乘积中出现了大值，...](https://kexue.fm/archives/11126/comment-page-3#comment-29023)
- [岁月如书: \[comment=29016\]苏剑林\[/comment\]他通过...](https://kexue.fm/archives/11459/comment-page-1#comment-29022)
- [苏剑林: 我好像也就只有把小的放大然后加噪声的思路](https://kexue.fm/archives/10667/comment-page-1#comment-29021)

## USERLOGIN

- [登录](https://kexue.fm/admin/login.php)

[科学空间\|Scientific Spaces](https://kexue.fm)

- [登录](https://kexue.fm/admin/login.php)
- [打赏](https://kexue.fm/reward.html)
- [公式](https://kexue.fm/latex.html)
- [天象](https://kexue.fm/ac.html)
- [链接](https://kexue.fm/links.html)
- [时光](https://kexue.fm/me.html)
- [博览](https://kexue.fm/science.html)
- [归档](https://kexue.fm/content.html)

渴望成为一个小飞侠

- [欢迎订阅](https://kexue.fm/feed)
- [个性邮箱](https://kexue.fm/archives/119)
- [天象信息](https://kexue.fm/ac.html)
- [观测ISS](https://kexue.fm/archives/41)
- [LaTeX](https://kexue.fm/latex.html)
- [关于博主](https://kexue.fm/me.html)

欢迎访问“科学空间”，这里将与您共同探讨自然科学，回味人生百态；也期待大家的分享～

- [**千奇百怪** Everything](https://kexue.fm/category/Everything)
- [**天文探索** Astronomy](https://kexue.fm/category/Astronomy)
- [**数学研究** Mathematics](https://kexue.fm/category/Mathematics)
- [**物理化学** Phy-chem](https://kexue.fm/category/Phy-chem)
- [**信息时代** Big-Data](https://kexue.fm/category/Big-Data)
- [**生物自然** Biology](https://kexue.fm/category/Biology)
- [**图片摄影** Photograph](https://kexue.fm/category/Photograph)
- [**问题百科** Questions](https://kexue.fm/category/Questions)
- [**生活/情感** Life-Feeling](https://kexue.fm/category/Life-Feeling)
- [**资源共享** Resources](https://kexue.fm/category/Resources)

- [**千奇百怪**](https://kexue.fm/category/Everything)
- [**天文探索**](https://kexue.fm/category/Astronomy)
- [**数学研究**](https://kexue.fm/category/Mathematics)
- [**物理化学**](https://kexue.fm/category/Phy-chem)
- [**信息时代**](https://kexue.fm/category/Big-Data)
- [**生物自然**](https://kexue.fm/category/Biology)
- [**图片摄影**](https://kexue.fm/category/Photograph)
- [**问题百科**](https://kexue.fm/category/Questions)
- [**生活/情感**](https://kexue.fm/category/Life-Feeling)
- [**资源共享**](https://kexue.fm/category/Resources)

[首页](https://kexue.fm) [数学研究](https://kexue.fm/category/Mathematics) ReLU/GeLU/Swish的一个恒等式

16Aug

# [ReLU/GeLU/Swish的一个恒等式](https://kexue.fm/archives/11233)

By 苏剑林 \|
2025-08-16 \|
28425位读者\|

今天水一点轻松的内容，它基于笔者这两天意识到的一个恒等式。这个恒等式实际上很简单，但初看之下会有点意料之外的感觉，所以来记录一下。

## 基本结果 [\#](https://kexue.fm/kexue.fm\#%E5%9F%BA%E6%9C%AC%E7%BB%93%E6%9E%9C)

我们知道$\\newcommand{relu}{\\mathop{\\text{relu}}}\\relu(x) = \\max(x, 0)$，容易证明如下恒等式
\\begin{equation}x = \\relu(x) - \\relu(-x)\\end{equation}
如果$x$是一个向量，那么上式就更直观了，$\\relu(x)$是提取出$x$的正分量，$- \\relu(-x)$是提取出$x$的负分量，两者相加就得到原本的向量。

## 一般结论 [\#](https://kexue.fm/kexue.fm\#%E4%B8%80%E8%88%AC%E7%BB%93%E8%AE%BA)

接下来的问题是 [GeLU](https://kexue.fm/archives/7309)、 [Swish](https://papers.cool/arxiv/1710.05941) 等激活函数成立类似的恒等式吗？初看之下并不成立，然而事实上是成立的！我们甚至还有更一般的结论：

> 设$\\phi(x)$是任意奇函数，$f(x)=\\frac{1}{2}(\\phi(x) + 1)x$，那么恒成立
> \\begin{equation}x = f(x) - f(-x)\\end{equation}

证明该结论也是一件很轻松的事，这里就不展开了。对于Swish来说我们有$\\phi(x) = \\tanh(\\frac{x}{2})$，对于GeLU来说则有$\\phi(x)=\\mathop{\\text{erf}}(\\frac{x}{\\sqrt{2}})$，它们都是奇函数，所以成立同样的恒等式。

## 意义思考 [\#](https://kexue.fm/kexue.fm\#%E6%84%8F%E4%B9%89%E6%80%9D%E8%80%83)

上述恒等式写成矩阵形式是
\\begin{equation}x = f(x) - f(-x) = f(x\[1, -1\])\\begin{bmatrix}1 \\\ -1\\end{bmatrix}\\end{equation}
这表明以ReLU、GeLU、Swish等为激活函数时，两层神经网络有退化为一层的能力，这意味着它们可以自适应地调节模型的实际深度，这与ResNet的工作原理异曲同工，这也许是这些激活函数为什么比传统的Tanh、Sigmoid等更好的原因之一。

_**转载到请包括本文地址：** [https://kexue.fm/archives/11233](https://kexue.fm/archives/11233)_

_**更详细的转载事宜请参考：**_ [《科学空间FAQ》](https://kexue.fm/archives/6508#%E6%96%87%E7%AB%A0%E5%A6%82%E4%BD%95%E8%BD%AC%E8%BD%BD/%E5%BC%95%E7%94%A8)

**如果您还有什么疑惑或建议，欢迎在下方评论区继续讨论。**

**如果您觉得本文还不错，欢迎 [分享](https://kexue.fm/kexue.fm#share)/ [打赏](https://kexue.fm/kexue.fm#pay) 本文。打赏并非要从中获得收益，而是希望知道科学空间获得了多少读者的真心关注。当然，如果你无视它，也不会影响你的阅读。再次表示欢迎和感谢！**

打赏

微信打赏

支付宝打赏

因为网站后台对打赏并无记录，因此欢迎在打赏时候备注留言。你还可以 [**点击这里**](http://mail.qq.com/cgi-bin/qm_share?t=qm_mailme&email=tN7d1drY3drrx8H0xcWa19vZ) 或在下方评论区留言来告知你的建议或需求。

**如果您需要引用本文，请参考：**

苏剑林. (Aug. 16, 2025). 《ReLU/GeLU/Swish的一个恒等式 》\[Blog post\]. Retrieved from [https://kexue.fm/archives/11233](https://kexue.fm/archives/11233)

@online{kexuefm-11233,
        title={ReLU/GeLU/Swish的一个恒等式},
        author={苏剑林},
        year={2025},
        month={Aug},
        url={\\url{https://kexue.fm/archives/11233}},
}

分类： [数学研究](https://kexue.fm/category/Mathematics)    标签： [分析](https://kexue.fm/tag/%E5%88%86%E6%9E%90/), [神经网络](https://kexue.fm/tag/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/), [恒等式](https://kexue.fm/tag/%E6%81%92%E7%AD%89%E5%BC%8F/)[15 评论](https://kexue.fm/archives/11233#comments)

< [流形上的最速下降：3\. Muon + Stiefel](https://kexue.fm/archives/11221) \| [流形上的最速下降：4\. Muon + 谱球面](https://kexue.fm/archives/11241) >

### 你也许还对下面的内容感兴趣

- [低精度Attention可能存在有偏的舍入误差](https://kexue.fm/archives/11371)
- [为什么Adam的Update RMS是0.2？](https://kexue.fm/archives/11267)
- [等值振荡定理：最优多项式逼近的充要条件](https://kexue.fm/archives/10972)
- [SVD的导数](https://kexue.fm/archives/10878)
- [通过梯度近似寻找Normalization的替代品](https://kexue.fm/archives/10831)
- [通向概率分布之路：盘点Softmax及其替代品](https://kexue.fm/archives/10145)
- [用傅里叶级数拟合一维概率密度函数](https://kexue.fm/archives/10007)
- [基于量子化假设推导模型的尺度定律（Scaling Law）](https://kexue.fm/archives/9607)
- [为什么现在的LLM都是Decoder-only的架构？](https://kexue.fm/archives/9529)
- [生成扩散模型漫谈（十八）：得分匹配 = 条件得分匹配](https://kexue.fm/archives/9509)

[发表你的看法](https://kexue.fm/kexue.fm#comment_form)

Xiaoiec

August 16th, 2025

苏神，收到您的启发，制作了一个激活函数：
$$ \\text{ReLAF}(x) = x + \\alpha \\cdot \\tanh(\\beta \\cdot x) $$
您觉得这个激活函数怎么样？
（初始化时把$\\alpha$初始化成0）

[回复评论](https://kexue.fm/archives/11233/comment-page-1?replyTo=28406#respond-post-11233)

Xiaoiec 发表于
August 16th, 2025

（或者很小的数）
需要配合梯度裁剪使用

[回复评论](https://kexue.fm/archives/11233/comment-page-1?replyTo=28407#respond-post-11233)

Xiaoiec 发表于
August 16th, 2025

搞错了，是$$ \\text{ReLAF}(x) = x \\cdot \\sigma(\\beta x + b) $$
初始化beta是0，b是3.5

[回复评论](https://kexue.fm/archives/11233/comment-page-1?replyTo=28408#respond-post-11233)

[苏剑林](https://kexue.fm) 发表于
August 22nd, 2025

好像也没看出跟本文的关联呀。

[回复评论](https://kexue.fm/archives/11233/comment-page-1?replyTo=28426#respond-post-11233)

Hangliang Ding

August 16th, 2025

之前有一个类似的经典问题用两层的神经网络构造abs(x) 就是利用 y=abs(x) = relu(x) + relu(-x)，这个任务其他激活函数不好解。

苏神是这样理解的吧

[回复评论](https://kexue.fm/archives/11233/comment-page-1?replyTo=28409#respond-post-11233)

[苏剑林](https://kexue.fm) 发表于
August 22nd, 2025

呃，是倒是，但好像也跟本文没啥直接联系的样子～

[回复评论](https://kexue.fm/archives/11233/comment-page-1?replyTo=28427#respond-post-11233)

Da1sypetals

August 17th, 2025

如果有人和我一样没看懂，可以听GPT解释一下。两层退化为一层”的更一般形式

取任意矩阵 $U,V$，把第一层做成“$+U$ 和 $-U$”的并列，第二层做成“$+V$ 和 $-V$”的并列：

$$
\\underbrace{\\begin{bmatrix}V & -V\\end{bmatrix}}\_{\\text{线性 2}}
\\;\\; f\\!\\left(
\\underbrace{\\begin{bmatrix}U\\\-U\\end{bmatrix}}\_{\\text{线性 1}}
x\\right)
= V\\,\\big(f(Ux)-f(-Ux)\\big)
= V(Ux)
= (VU)\\,x .
$$

第二个等号由文中上述$f$的性质保证。中间那层非线性被完全“抵消”掉了，所以两层（线性→激活→线性）等价于一层线性 $VU$。这就是文中“有退化为一层的能力”的意思。

[回复评论](https://kexue.fm/archives/11233/comment-page-1?replyTo=28414#respond-post-11233)

[苏剑林](https://kexue.fm) 发表于
August 22nd, 2025

是这个意思。当然也可以部分地“抵消”，所以说这是一个自主控制深度的性质（直观感觉就是1.5层这样）。

[回复评论](https://kexue.fm/archives/11233/comment-page-1?replyTo=28430#respond-post-11233)

liang wu 发表于
September 11th, 2025

还需要加什么技巧用于训练，然后训练后，根据这个技巧裁剪网络深度呢

[回复评论](https://kexue.fm/archives/11233/comment-page-1?replyTo=28542#respond-post-11233)

[苏剑林](https://kexue.fm) 发表于
September 13th, 2025

这只是一种理解，我倾向于很难用于裁剪，除非引入更多的inductive bias。

[回复评论](https://kexue.fm/archives/11233/comment-page-1?replyTo=28554#respond-post-11233)

Nemo Lv 发表于
November 4th, 2025

提问，模型中实际做激活的是$ϕ(\\cdot)$,但是按你的理解是用$f(\\cdot)$来做激活了，为什么可以这样理解呢？

[回复评论](https://kexue.fm/archives/11233/comment-page-1?replyTo=28766#respond-post-11233)

[苏剑林](https://kexue.fm) 发表于
November 5th, 2025

激活函数是$f(x)$，$\\phi(x)$是构造$f(x)$的一部份。

[回复评论](https://kexue.fm/archives/11233/comment-page-1?replyTo=28775#respond-post-11233)

[长琴](https://yam.gift)

August 21st, 2025

真巧妙

[回复评论](https://kexue.fm/archives/11233/comment-page-1?replyTo=28420#respond-post-11233)

Simpleson

September 3rd, 2025

请教大佬两个问题：
1\. 把GLU代入这个性质：特定情况下，GLU是不是刚好能抵消掉激活函数，并退化成一层线性变换？
2\. 以前诞生的激活函数有一部分被弃用，也许只是因为凑巧不符合本文规律？
比如LReLU：
\\\[\\text{LReLU}(x) = \\max(0, x) + \\alpha \\min(0, x)\\\]
也许本来应该是这样的？
\\\[\\text{LReLU}(x) = (1-\\alpha)\\max(0, x) + \\alpha \\min(0, x)\\\]

[回复评论](https://kexue.fm/archives/11233/comment-page-1?replyTo=28498#respond-post-11233)

[苏剑林](https://kexue.fm) 发表于
September 5th, 2025

1、GLU想要退化为一层linear更简单了，只要gate那边退化为一个常数就行；
2、这个就没那么绝对了，只是说满足本文的性质可能会有一点点理论好处，但也不是绝对的优势条件。

[回复评论](https://kexue.fm/archives/11233/comment-page-1?replyTo=28516#respond-post-11233)

[取消回复](https://kexue.fm/archives/11233#respond-post-11233)

你的大名

电子邮箱

个人网站（选填）

1\. 可以使用LaTeX代码，点击“预览效果”可查看效果；2. 可以通过点击评论楼层编号来引用该楼层；3. 网站可能会有点卡，如非确认评论失败，请 **不要重复点击提交**。

### 内容速览

[基本结果](https://kexue.fm/kexue.fm#%E5%9F%BA%E6%9C%AC%E7%BB%93%E6%9E%9C)
[一般结论](https://kexue.fm/kexue.fm#%E4%B8%80%E8%88%AC%E7%BB%93%E8%AE%BA)
[意义思考](https://kexue.fm/kexue.fm#%E6%84%8F%E4%B9%89%E6%80%9D%E8%80%83)

### 智能搜索

支持整句搜索！网站自动使用 [结巴分词](https://github.com/fxsjy/jieba) 进行分词，并结合ngrams排序算法给出合理的搜索结果。

### 热门标签

[生成模型](https://kexue.fm/tag/%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B/) [attention](https://kexue.fm/tag/attention/) [优化](https://kexue.fm/tag/%E4%BC%98%E5%8C%96/) [语言模型](https://kexue.fm/tag/%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/) [模型](https://kexue.fm/tag/%E6%A8%A1%E5%9E%8B/) [网站](https://kexue.fm/tag/%E7%BD%91%E7%AB%99/) [梯度](https://kexue.fm/tag/%E6%A2%AF%E5%BA%A6/) [概率](https://kexue.fm/tag/%E6%A6%82%E7%8E%87/) [矩阵](https://kexue.fm/tag/%E7%9F%A9%E9%98%B5/) [优化器](https://kexue.fm/tag/%E4%BC%98%E5%8C%96%E5%99%A8/) [转载](https://kexue.fm/tag/%E8%BD%AC%E8%BD%BD/) [微分方程](https://kexue.fm/tag/%E5%BE%AE%E5%88%86%E6%96%B9%E7%A8%8B/) [分析](https://kexue.fm/tag/%E5%88%86%E6%9E%90/) [天象](https://kexue.fm/tag/%E5%A4%A9%E8%B1%A1/) [深度学习](https://kexue.fm/tag/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/) [积分](https://kexue.fm/tag/%E7%A7%AF%E5%88%86/) [python](https://kexue.fm/tag/python/) [扩散](https://kexue.fm/tag/%E6%89%A9%E6%95%A3/) [力学](https://kexue.fm/tag/%E5%8A%9B%E5%AD%A6/) [无监督](https://kexue.fm/tag/%E6%97%A0%E7%9B%91%E7%9D%A3/) [几何](https://kexue.fm/tag/%E5%87%A0%E4%BD%95/) [节日](https://kexue.fm/tag/%E8%8A%82%E6%97%A5/) [生活](https://kexue.fm/tag/%E7%94%9F%E6%B4%BB/) [文本生成](https://kexue.fm/tag/%E6%96%87%E6%9C%AC%E7%94%9F%E6%88%90/) [数论](https://kexue.fm/tag/%E6%95%B0%E8%AE%BA/)

### 随机文章

- [基于量子化假设推导模型的尺度定律（Scaling Law）](https://kexue.fm/archives/9607)
- [2010年全国天文奥赛终于可以报名了](https://kexue.fm/archives/332)
- [VQ的旋转技巧：梯度直通估计的一般推广](https://kexue.fm/archives/10489)
- [Cool Papers浏览器扩展升级至v0.2.0](https://kexue.fm/archives/10480)
- [我是一个费曼迷](https://kexue.fm/archives/3204)
- [从Hessian近似看自适应学习率优化器](https://kexue.fm/archives/10588)
- [Transformer升级之路：19、第二类旋转位置编码](https://kexue.fm/archives/10862)
- [MoE环游记：1、从几何意义出发](https://kexue.fm/archives/10699)
- [GlobalPointer下的“KL散度”应该是怎样的？](https://kexue.fm/archives/9039)
- [从语言模型到Seq2Seq：Transformer如戏，全靠Mask](https://kexue.fm/archives/6933)

### 最近评论

- [喝一口可乐](https://kexue.fm/archives/10958/comment-page-3#comment-29030): 理解了，感谢苏神回复，数学上给出建模分析确实清晰了很多，再次感谢苏神回复！
- [CuddleSabe1](https://kexue.fm/archives/10958/comment-page-1#comment-29029): 感觉普通的 flow matching 可以看成 degrade-aware image de...
- [岁月如书](https://kexue.fm/archives/11126/comment-page-3#comment-29028): 受教了，感谢
- [苏剑林](https://kexue.fm/archives/11126/comment-page-3#comment-29027): 是
- [岁月如书](https://kexue.fm/archives/11126/comment-page-3#comment-29026): 哦哦，原来是有实验结论，那是我盲目了。多问一句，你说的attention + output g...
- [苏剑林](https://kexue.fm/archives/11126/comment-page-3#comment-29025): attention sink指的是第一个token的attention普遍不可忽略，不一定是爆...
- [苏剑林](https://kexue.fm/archives/11459/comment-page-1#comment-29024): 这也许是好事呢？SGD倒是保留了模长，但它就普遍不如不保留模长的SignSGD或者Normal...
- [岁月如书](https://kexue.fm/archives/11126/comment-page-3#comment-29023): maxlogit 是attention qk乘积中出现了大值，attention sink等于...
- [岁月如书](https://kexue.fm/archives/11459/comment-page-1#comment-29022): \[comment=29016\]苏剑林\[/comment\]他通过Newton-schulz迭代近...
- [苏剑林](https://kexue.fm/archives/10667/comment-page-1#comment-29021): 我好像也就只有把小的放大然后加噪声的思路

### 友情链接

- [Cool Papers](https://papers.cool)
- [数学研发](https://bbs.emath.ac.cn)
- [Seatop](http://www.seatop.com.cn/)
- [Xiaoxia](https://xiaoxia.org/)
- [积分表-网络版](https://kexue.fm/sci/integral/index.html)
- [丝路博傲](http://blog.dvxj.com/)
- [数学之家](http://www.2math.cn/)
- [有趣天文奇观](http://interesting-sky.china-vo.org/)
- [TwistedW](http://www.twistedwg.com/)
- [godweiyang](https://godweiyang.com/)
- [AI柠檬](https://blog.ailemon.net/)
- [王登科-DK博客](https://greatdk.com)
- [ESON](https://blog.eson.org/)
- [枫之羽](https://fzhiy.net/)
- [coding-zuo](https://coding-zuo.github.io/)
- [博科园](https://www.bokeyuan.net/)
- [孔皮皮的博客](https://www.kppkkp.top/)
- [运鹏的博客](https://yunpengtai.top/)
- [jiming.site](https://jiming.site/)
- [OmegaXYZ](https://www.omegaxyz.com/)
- [EAI猩球](https://www.robotech.ink/)
- [文举的博客](https://liwenju0.com/)
- [申请链接](https://kexue.fm/links.html)

本站采用创作共用版权协议，要求署名、非商业用途和保持一致。转载本站内容必须也遵循“ [署名-非商业用途-保持一致](http://creativecommons.org/licenses/by-nc-nd/2.5/cn/)”的创作共用协议。
© 2009-2025 Scientific Spaces. All rights reserved. Theme by [laogui](http://www.laogui.com). Powered by [Typecho](http://typecho.org). 备案号: [粤ICP备09093259号-1/2](https://beian.miit.gov.cn/)。