![MobileSideBar](https://kexue.fm/usr/themes/geekg/images/slide-button.png)

## SEARCH

## MENU

- [打赏](https://kexue.fm/reward.html)
- [公式](https://kexue.fm/latex.html)
- [天象](https://kexue.fm/ac.html)
- [链接](https://kexue.fm/links.html)
- [时光](https://kexue.fm/me.html)
- [博览](https://kexue.fm/science.html)
- [归档](https://kexue.fm/content.html)

## CATEGORIES

- [千奇百怪](https://kexue.fm/category/Everything)
- [天文探索](https://kexue.fm/category/Astronomy)
- [数学研究](https://kexue.fm/category/Mathematics)
- [物理化学](https://kexue.fm/category/Phy-chem)
- [信息时代](https://kexue.fm/category/Big-Data)
- [生物自然](https://kexue.fm/category/Biology)
- [图片摄影](https://kexue.fm/category/Photograph)
- [问题百科](https://kexue.fm/category/Questions)
- [生活/情感](https://kexue.fm/category/Life-Feeling)
- [资源共享](https://kexue.fm/category/Resources)

## NEWPOSTS

- [SVD的导数](https://kexue.fm/archives/10878)
- [智能家居之手搓一套能接入米家的零冷水装置](https://kexue.fm/archives/10869)
- [Transformer升级之路：1...](https://kexue.fm/archives/10862)
- [矩阵的有效秩（Effective ...](https://kexue.fm/archives/10847)
- [通过梯度近似寻找Normaliza...](https://kexue.fm/archives/10831)
- [MoE环游记：4、难处应当多投入](https://kexue.fm/archives/10815)
- [高阶muP：更简明但更高明的谱条件缩放](https://kexue.fm/archives/10795)
- [初探muP：超参数的跨模型尺度迁移规律](https://kexue.fm/archives/10770)
- [MoE环游记：3、换个思路来分配](https://kexue.fm/archives/10757)
- [Muon续集：为什么我们选择尝试M...](https://kexue.fm/archives/10739)

## COMMENTS

- [苏剑林: 1、明白了，我将$q\_{\\phi}(z\|x)$看成$q\_{\\p...](https://kexue.fm/archives/5239/comment-page-3#comment-27496)
- [Suahi: 谢谢苏老师的回复！1\. 首先回复您为什么ELBO不带KL，并不...](https://kexue.fm/archives/5239/comment-page-3#comment-27493)
- [eular: 是的，当$k$比较大时会出现这种情况。](https://kexue.fm/archives/10373/comment-page-1#comment-27492)
- [苏剑林: 肯定是$\\mathbb{E}\_{x \\sim p\_{data}...](https://kexue.fm/archives/5239/comment-page-3#comment-27491)
- [苏剑林: 你的意思是$\\lambda(\\boldsymbol{x}) <...](https://kexue.fm/archives/10373/comment-page-1#comment-27490)
- [苏剑林: 好问题，下一篇文章可能会讨论这个问题](https://kexue.fm/archives/10735/comment-page-1#comment-27489)
- [苏剑林: 不大熟悉，但都diffusion forcing了，还有必要CM吗](https://kexue.fm/archives/10633/comment-page-1#comment-27488)
- [苏剑林: 简单看了一下，好像没什么新东西呀？还是我看漏了什么？](https://kexue.fm/archives/10711/comment-page-2#comment-27487)
- [苏剑林: 感谢指出，已修正。](https://kexue.fm/archives/8601/comment-page-1#comment-27486)
- [苏剑林: 扩散桥确实时不时刷到，但没理解这一套东西有什么特别价值或者应用...](https://kexue.fm/archives/9209/comment-page-7#comment-27485)

## USERLOGIN

- [登录](https://kexue.fm/admin/login.php)

[科学空间\|Scientific Spaces](https://kexue.fm)

- [登录](https://kexue.fm/admin/login.php)
- [打赏](https://kexue.fm/reward.html)
- [公式](https://kexue.fm/latex.html)
- [天象](https://kexue.fm/ac.html)
- [链接](https://kexue.fm/links.html)
- [时光](https://kexue.fm/me.html)
- [博览](https://kexue.fm/science.html)
- [归档](https://kexue.fm/content.html)

渴望成为一个小飞侠

- [![](https://kexue.fm/usr/themes/geekg/images/rss.png)\
\
欢迎订阅](https://kexue.fm/feed)
- [![](https://kexue.fm/usr/themes/geekg/images/mail.png)\
\
个性邮箱](https://kexue.fm/archives/119)
- [![](https://kexue.fm/usr/themes/geekg/images/Saturn.png)\
\
天象信息](https://kexue.fm/ac.html)
- [![](https://kexue.fm/usr/themes/geekg/images/iss.png)\
\
观测ISS](https://kexue.fm/archives/41)
- [![](https://kexue.fm/usr/themes/geekg/images/pi.png)\
\
LaTeX](https://kexue.fm/latex.html)
- [![](https://kexue.fm/usr/themes/geekg/images/mlogo.png)\
\
关于博主](https://kexue.fm/me.html)

欢迎访问“科学空间”，这里将与您共同探讨自然科学，回味人生百态；也期待大家的分享～

- [**千奇百怪** Everything](https://kexue.fm/category/Everything)
- [**天文探索** Astronomy](https://kexue.fm/category/Astronomy)
- [**数学研究** Mathematics](https://kexue.fm/category/Mathematics)
- [**物理化学** Phy-chem](https://kexue.fm/category/Phy-chem)
- [**信息时代** Big-Data](https://kexue.fm/category/Big-Data)
- [**生物自然** Biology](https://kexue.fm/category/Biology)
- [**图片摄影** Photograph](https://kexue.fm/category/Photograph)
- [**问题百科** Questions](https://kexue.fm/category/Questions)
- [**生活/情感** Life-Feeling](https://kexue.fm/category/Life-Feeling)
- [**资源共享** Resources](https://kexue.fm/category/Resources)

- [**千奇百怪**](https://kexue.fm/category/Everything)
- [**天文探索**](https://kexue.fm/category/Astronomy)
- [**数学研究**](https://kexue.fm/category/Mathematics)
- [**物理化学**](https://kexue.fm/category/Phy-chem)
- [**信息时代**](https://kexue.fm/category/Big-Data)
- [**生物自然**](https://kexue.fm/category/Biology)
- [**图片摄影**](https://kexue.fm/category/Photograph)
- [**问题百科**](https://kexue.fm/category/Questions)
- [**生活/情感**](https://kexue.fm/category/Life-Feeling)
- [**资源共享**](https://kexue.fm/category/Resources)

[首页](https://kexue.fm) [信息时代](https://kexue.fm/category/Big-Data) Google新搜出的优化器Lion：效率与效果兼得的“训练狮”

16Feb

# [Google新搜出的优化器Lion：效率与效果兼得的“训练狮”](https://kexue.fm/archives/9473)

By 苏剑林 \|
2023-02-16 \|
61459位读者\|

昨天在Arixv上发现了Google新发的一篇论文 [《Symbolic Discovery of Optimization Algorithms》](https://papers.cool/arxiv/2302.06675)，主要是讲自动搜索优化器的，咋看上去没啥意思，因为类似的工作也有不少，大多数结果都索然无味。然而，细读之下才发现别有洞天，原来作者们通过数千TPU小时的算力搜索并结合人工干预，得到了一个速度更快、显存更省的优化器Lion（Evo**L**ved S**i**gn M**o**me**n**tum，不得不吐槽这名字起得真勉强），并在图像分类、图文匹配、扩散模型、语言模型预训练和微调等诸多任务上做了充分的实验，多数任务都显示Lion比目前主流的AdamW等优化器有着更好的效果。

更省显存还更好效果，真可谓是鱼与熊掌都兼得了，什么样的优化器能有这么强悍的性能？本文一起来欣赏一下论文的成果。

## 先说结果 [\#](https://kexue.fm/archives/9473\#%E5%85%88%E8%AF%B4%E7%BB%93%E6%9E%9C)

本文主要关心搜索出来的优化器本身，所以关于搜索过程的细节就不讨论了，对此有兴趣读者自行看原论文就好。Lion优化器的更新过程为

\\begin{equation}\\text{Lion}:=\\left\\{\\begin{aligned}

&\\boldsymbol{u}\_t = \\text{sign}\\big(\\beta\_1 \\boldsymbol{m}\_{t-1} + \\left(1 - \\beta\_1\\right) \\boldsymbol{g}\_t\\big) \\\

&\\boldsymbol{\\theta}\_t = \\boldsymbol{\\theta}\_{t-1} - \\eta\_t (\\boldsymbol{u}\_t \\color{skyblue}{ + \\lambda\_t \\boldsymbol{\\theta}\_{t-1}}) \\\

&\\boldsymbol{m}\_t = \\beta\_2 \\boldsymbol{m}\_{t-1} + \\left(1 - \\beta\_2\\right) \\boldsymbol{g}\_t

\\end{aligned}\\right.\\end{equation}

其中$\\boldsymbol{g}\_t = \\nabla\_{\\boldsymbol{\\theta}} L(\\boldsymbol{\\theta}\_{t-1})$是损失函数的梯度，$\\text{sign}$是 [符号函数](https://en.wikipedia.org/wiki/Sign_function)，即正数变为1、负数变为-1。我们可以对比一下目前的主流优化器 [AdamW](https://papers.cool/arxiv/1711.05101) 的更新过程

\\begin{equation}\\text{Adam}\\color{skyblue}{\\text{W}}:=\\left\\{\\begin{aligned}

&\\boldsymbol{m}\_t = \\beta\_1 \\boldsymbol{m}\_{t-1} + \\left(1 - \\beta\_1\\right) \\boldsymbol{g}\_t\\\

&\\boldsymbol{v}\_t = \\beta\_2 \\boldsymbol{v}\_{t-1} + \\left(1 - \\beta\_2\\right) \\boldsymbol{g}\_t^2\\\

&\\hat{\\boldsymbol{m}}\_t = \\boldsymbol{m}\_t\\left/\\left(1 - \\beta\_1^t\\right)\\right.\\\

&\\hat{\\boldsymbol{v}}\_t = \\boldsymbol{v}\_t\\left/\\left(1 - \\beta\_2^t\\right)\\right.\\\

&\\boldsymbol{u}\_t =\\hat{\\boldsymbol{m}}\_t\\left/\\left(\\sqrt{\\hat{\\boldsymbol{v}}\_t} + \\epsilon\\right)\\right.\\\

&\\boldsymbol{\\theta}\_t = \\boldsymbol{\\theta}\_{t-1} - \\eta\_t (\\boldsymbol{u}\_t \\color{skyblue}{ + \\lambda\_t \\boldsymbol{\\theta}\_{t-1}})

\\end{aligned}\\right.\\end{equation}

对比很明显，Lion相比AdamW参数更少（少了个$\\epsilon$），少缓存了一组参数$\\boldsymbol{v}$（所以更省显存），并且去掉了AdamW更新过程中计算量最大的除法和开根号运算（所以更快）。

在此之前，跟Lion最相似的优化器应该是 [SIGNUM](https://papers.cool/arxiv/1802.04434)，其更新过程为

\\begin{equation}\\text{SIGNUM}:=\\left\\{\\begin{aligned}

&\\boldsymbol{m}\_t = \\beta \\boldsymbol{m}\_{t-1} + \\left(1 - \\beta\\right) \\boldsymbol{g}\_t \\\

&\\boldsymbol{u}\_t = \\text{sign}\\big(\\boldsymbol{m}\_t\\big) \\\

&\\boldsymbol{\\theta}\_t = \\boldsymbol{\\theta}\_{t-1} - \\eta\_t \\boldsymbol{u}\_t \\end{aligned}\\right.\\end{equation}

跟Lion一样，SIGNUM也用到了符号函数处理更新量，而且比Lion更加简化（等价于Lion在$\\beta\_1=\\beta\_2$和$\\lambda\_t=0$的特例），但是很遗憾，SIGNUM并没有取得更好的效果，它的设计初衷只是降低分布式计算中的传输成本。Lion的更新规则有所不同，尤其是动量的更新放在了变量的更新之后，并且在充分的实验中显示出了它在效果上的优势。

## 论文实验 [\#](https://kexue.fm/archives/9473\#%E8%AE%BA%E6%96%87%E5%AE%9E%E9%AA%8C)

本文开头就说了，Lion在相当多的任务上都做了实验，实验结果很多，下面罗列一些笔者认为比较关键的结果。

[![Lion在NLU和NLG任务上的结果，大部分都比AdamW、Adafactor优秀](https://kexue.fm/usr/uploads/2023/02/3166342404.png)](https://kexue.fm/usr/uploads/2023/02/3166342404.png)

Lion在NLU和NLG任务上的结果，大部分都比AdamW、Adafactor优秀

[![在视觉Transformer上Lion与众多优化器的对比](https://kexue.fm/usr/uploads/2023/02/3192886331.png)](https://kexue.fm/usr/uploads/2023/02/3192886331.png)

在视觉Transformer上Lion与众多优化器的对比

[![在CV的分类任务上，Lion收敛速度更快](https://kexue.fm/usr/uploads/2023/02/3511074534.png)](https://kexue.fm/usr/uploads/2023/02/3511074534.png)

在CV的分类任务上，Lion收敛速度更快

[![在NLP的自回归生成上，Lion的收敛速度更快](https://kexue.fm/usr/uploads/2023/02/4036852564.png)](https://kexue.fm/usr/uploads/2023/02/4036852564.png)

在NLP的自回归生成上，Lion的收敛速度更快

[![上右图是ImageNet上的训练曲线，显示Lion尽管验证集效果更好，但训练集上的效果未必会优于AdamW](https://kexue.fm/usr/uploads/2023/02/3568656053.png)](https://kexue.fm/usr/uploads/2023/02/3568656053.png)

上右图是ImageNet上的训练曲线，显示Lion尽管验证集效果更好，但训练集上的效果未必会优于AdamW

## 超参设置 [\#](https://kexue.fm/archives/9473\#%E8%B6%85%E5%8F%82%E8%AE%BE%E7%BD%AE)

看到论文效果如此惊人，笔者也跃跃欲试。在跑实验之前，自然需要了解一下各个超参的设置。首先是$\\beta\_1,\\beta\_2$，原论文自动搜索出来的结果是$\\beta\_1=0.9,\\beta=0.99$，并在大部分实验中复用了这个组合，但是在NLP的任务上则使用了$\\beta\_1=0.95,\\beta\_2=0.98$这个组合（论文的详细实验配置在最后一页的Table 12）。

比较关键的学习率$\\eta$和权重衰减率$\\lambda$，由于Lion的更新量$\\boldsymbol{u}$每个分量的绝对值都是1，这通常比AdamW要大，所以学习率要缩小10倍以上，才能获得大致相同的更新幅度；而由于学习率降低了，那么为了使权重衰减的幅度保持不变，权重衰减率应该要放大相应的倍数。原论文的最后一页给出了各个实验的超参数参考值，其中小模型（Base级别）上使用的是$\\eta = 3\\times 10^{-4}$和$\\lambda=0.01$，大模型（参数10亿以上）则适当降低了学习率到$\\eta = 2\\times 10^{-4}$甚至$\\eta = 10^{-4}$。

事实上，之前我们在 [《基于Amos优化器思想推导出来的一些“炼丹策略”》](https://kexue.fm/archives/9344) 就推导过学习率和权重衰减率的一个组合方案，参考这个方案来设置是最方便的。在该方案中，更新量写为（记号跟前面的描述略有不同，但不至于混淆，应该就不强行统一了）

\\begin{equation}\\boldsymbol{\\theta}\_{t+1} = \\boldsymbol{\\theta}\_t - (\\alpha\_t \\boldsymbol{u}\_t + \\rho\_t\\boldsymbol{\\theta}\_t)\\end{equation}

其中

\\begin{equation}\\alpha\_t \\approx \\frac{\\alpha\_0\\Vert\\boldsymbol{\\varepsilon}\_0\\Vert}{\\Vert\\boldsymbol{u}\_t\\Vert} \\frac{1}{\\kappa t + 1},\\quad \\rho\_t \\approx \\frac{\\alpha\_0^2}{2q} \\frac{1}{\\kappa t + 1}\\end{equation}

其中$\\boldsymbol{u}\_t$是原本的更新量；$\\alpha\_0$是（初始阶段）参数变化的相对大小，一般是$10^{-3}$级别，表示每步更新后参数模长的变化幅度大致是千分之一；$q$是一个超参数，没什么特殊情况可以设为1；$\\kappa$是控制学习率衰减速度的超参数，可以根据训练数据大小等设置。

由于$\\boldsymbol{u}\_t$经过了$\\text{sign}$运算，因此$\\Vert\\boldsymbol{u}\_t\\Vert=\\sqrt{k}$，$k$是参数的维度；$\\Vert\\boldsymbol{\\varepsilon}\_0\\Vert\\approx\\sqrt{k}\\sigma$，这我们在 [《基于Amos优化器思想推导出来的一些“炼丹策略”》](https://kexue.fm/archives/9344) 已经推导过了，其中$\\sigma$是参数的变化尺度，对于乘性矩阵，$\\sigma^2$就是它的初始化方差。所以，经过一系列简化之后，有

\\begin{equation}\\alpha\_t \\approx \\frac{\\alpha\_0\\sigma}{\\kappa t + 1},\\quad \\rho\_t \\approx \\frac{\\alpha\_0^2}{2(\\kappa t + 1)}\\end{equation}

这里的$\\alpha\_t$就是前面的$\\eta\_t$，而$\\lambda\_t = \\rho\_t / \\alpha\_t = \\alpha\_0 / 2\\sigma$。按照BERT base的$d=768$来算，初始化方差的量级大致在$1/d$左右，于是$\\sigma = \\sqrt{1/d}\\approx 0.036$，假设$\\alpha\_0$取$1.11 \\times 10^{-3}$（为了给结果凑个整），那么按照上式学习率大约是$4\\times 10^{-5}$、衰减率大约是$0.015$。在笔者自己的MLM预训练实验中，选取这两个组合效果比较好。

> **个人实现： [https://github.com/bojone/bert4keras](https://github.com/bojone/bert4keras/commit/b60e7cfe076c0302473bbc3d63fed7e97f1c377f)**

## 延伸思考 [\#](https://kexue.fm/archives/9473\#%E5%BB%B6%E4%BC%B8%E6%80%9D%E8%80%83)

总体来看，Lion表现可圈可点，不管是原论文还是笔者自己的实验中，跟AdamW相比都有一战之力，再加上Lion更快以及更省显存的特点，或者可以预见未来的主流优化器将有它的一席之地。

自Adam提出以来，由于其快速收敛的特性成为了很多模型的默认优化器。甚至有学者提出，这个现象将反过来导致一个进化效应：所有的模型改进都在往Adam有利的方向发展，换句话说，由于我们选择了Adam作为优化器，那么就有可能将很多实际有效、但是在Adam优化器上无效的改动都抛弃了，剩下的都是对Adam有利的改进，详细的评价可以参考 [《NEURAL NETWORKS (MAYBE) EVOLVED TO MAKE ADAM THE BEST OPTIMIZER》](https://parameterfree.com/2020/12/06/neural-network-maybe-evolved-to-make-adam-the-best-optimizer/)。所以，在此大背景之下，能够发现比Adam更简单且更有效的优化器，是一件很了不起的事情，哪怕它是借助大量算力搜索出来的。

可能读者会有疑问：Lion凭啥可以取得更好的泛化性能呢？原论文的解释是$\\text{sign}$这个操作引入了额外的噪声（相比于准确的浮点值），它使得模型进入了Loss更平坦（但未必更小）的区域，从而泛化性能更好。为了验证这一点，作者比较了AdamW和Lion训练出来的模型权重的抗干扰能力，结果显示Lion的抗干扰能力更好。然而，理论上来说，这只能证明Lion确实进入到了更平坦的区域，但无法证明该结果是$\\text{sign}$操作造成的。不过，Adam发表这么多年了，关于它的机理也还没有彻底研究清楚，而Lion只是刚刚提出，就不必过于吹毛求疵了。

笔者的猜测是，Lion通过$\\text{sign}$操作平等地对待了每一个分量，使得模型充分地发挥了每一个分量的作用，从而有更好的泛化性能。如果是SGD，那么更新的大小正比于它的梯度，然而有些分量梯度小，可能仅仅是因为它没初始化好，而并非它不重要，所以Lion的$\\text{sign}$操作算是为每个参数都提供了“恢复活力”甚至“再创辉煌”的机会。事实上可以证明，Adam早期的更新量也接近于$\\text{sign}$，只是随着训练步数的增加才逐渐偏离。

Lion是不是足够完美呢？显然不是，比如原论文就指出它在小batch\_size（小于64）的时候效果不如AdamW，这也不难理解，本来$\\text{sign}$已经带来了噪声，而小batch\_size则进一步增加了噪声，噪声这个东西，必须适量才好，所以两者叠加之下，很可能有噪声过量导致效果恶化。另外，也正因为$\\text{sign}$加剧了优化过程的噪声，所以参数设置不当时容易出现损失变大等发散情况，这时候可以尝试引入Warmup，或者增加Warmup步数。还有，Lion依旧需要缓存动量参数，所以它的显存占用多于 [AdaFactor](https://kexue.fm/archives/7302)，能不能进一步优化这部分参数量呢？暂时还不得而知。

## 文章小结 [\#](https://kexue.fm/archives/9473\#%E6%96%87%E7%AB%A0%E5%B0%8F%E7%BB%93)

本文介绍了Google新提出的优化器Lion，它通过大量算力搜索并结合人工干预得出，相比主流的AdamW，有着速度更快且更省内存的特点，并且大量实验结果显示，它在多数任务上都有着不逊色于甚至优于AdamW的表现。

_**转载到请包括本文地址：** [https://kexue.fm/archives/9473](https://kexue.fm/archives/9473)_

_**更详细的转载事宜请参考：**_ [《科学空间FAQ》](https://kexue.fm/archives/6508#%E6%96%87%E7%AB%A0%E5%A6%82%E4%BD%95%E8%BD%AC%E8%BD%BD/%E5%BC%95%E7%94%A8)

**如果您还有什么疑惑或建议，欢迎在下方评论区继续讨论。**

**如果您觉得本文还不错，欢迎 [分享](https://kexue.fm/archives/9473#share)/ [打赏](https://kexue.fm/archives/9473#pay) 本文。打赏并非要从中获得收益，而是希望知道科学空间获得了多少读者的真心关注。当然，如果你无视它，也不会影响你的阅读。再次表示欢迎和感谢！**

打赏

![科学空间](https://kexue.fm/usr/themes/geekg/payment/wx.png)

微信打赏

![科学空间](https://kexue.fm/usr/themes/geekg/payment/zfb.png)

支付宝打赏

因为网站后台对打赏并无记录，因此欢迎在打赏时候备注留言。

你还可以 [**点击这里**](http://mail.qq.com/cgi-bin/qm_share?t=qm_mailme&email=tN7d1drY3drrx8H0xcWa19vZ) 或在下方评论区留言来告知你的建议或需求。

**如果您需要引用本文，请参考：**

苏剑林. (Feb. 16, 2023). 《Google新搜出的优化器Lion：效率与效果兼得的“训练狮” 》\[Blog post\]. Retrieved from [https://kexue.fm/archives/9473](https://kexue.fm/archives/9473)

@online{kexuefm-9473,

        title={Google新搜出的优化器Lion：效率与效果兼得的“训练狮”},

        author={苏剑林},

        year={2023},

        month={Feb},

        url={\\url{https://kexue.fm/archives/9473}},

}

分类： [信息时代](https://kexue.fm/category/Big-Data)    标签： [分析](https://kexue.fm/tag/%E5%88%86%E6%9E%90/), [优化](https://kexue.fm/tag/%E4%BC%98%E5%8C%96/), [优化器](https://kexue.fm/tag/%E4%BC%98%E5%8C%96%E5%99%A8/)[19 评论](https://kexue.fm/archives/9473#comments)

< [生成扩散模型漫谈（十六）：W距离 ≤ 得分匹配](https://kexue.fm/archives/9467) \| [生成扩散模型漫谈（十七）：构建ODE的一般步骤（下）](https://kexue.fm/archives/9497) >

### 你也许还对下面的内容感兴趣

- [SVD的导数](https://kexue.fm/archives/10878)
- [通过梯度近似寻找Normalization的替代品](https://kexue.fm/archives/10831)
- [MoE环游记：4、难处应当多投入](https://kexue.fm/archives/10815)
- [高阶muP：更简明但更高明的谱条件缩放](https://kexue.fm/archives/10795)
- [初探muP：超参数的跨模型尺度迁移规律](https://kexue.fm/archives/10770)
- [Muon续集：为什么我们选择尝试Muon？](https://kexue.fm/archives/10739)
- [为什么梯度裁剪的默认模长是1？](https://kexue.fm/archives/10657)
- [从谱范数梯度到新式权重衰减的思考](https://kexue.fm/archives/10648)
- [Muon优化器赏析：从向量到矩阵的本质跨越](https://kexue.fm/archives/10592)
- [从Hessian近似看自适应学习率优化器](https://kexue.fm/archives/10588)

[发表你的看法](https://kexue.fm/archives/9473#comment_form)

lianghy2749

February 20th, 2023

那是不是可以使用梯度累积的方式满足batch\_size > 64的要求呢？

[回复评论](https://kexue.fm/archives/9473/comment-page-1?replyTo=20966#respond-post-9473)

[苏剑林](https://kexue.fm) 发表于
February 20th, 2023

这个自然可以。

[回复评论](https://kexue.fm/archives/9473/comment-page-1?replyTo=20977#respond-post-9473)

runwayrun

February 21st, 2023

请问"超参设置"小节里衰减率≈0.015是怎么算出来的呢？我算的是：(1.11e-3)\*\*2/2=6.160500000000001e-07

[回复评论](https://kexue.fm/archives/9473/comment-page-1?replyTo=20986#respond-post-9473)

[郑之杰](http://0809zheng.github.io/) 发表于
February 22nd, 2023

根据习惯$\\theta\_{t+1} = \\theta\_t -\\alpha\_t(u\_t+\\lambda\_t \\theta\_t)= \\theta\_t -\\alpha\_t u\_t-\\rho\_t\\theta\_t$，衰减率一般指代$\\lambda\_t=\\rho\_t/\\alpha\_t$。

[回复评论](https://kexue.fm/archives/9473/comment-page-1?replyTo=20992#respond-post-9473)

[苏剑林](https://kexue.fm) 发表于
February 24th, 2023

是的，一般称衰减率指的是$\\lambda\_t$。

[回复评论](https://kexue.fm/archives/9473/comment-page-1?replyTo=21005#respond-post-9473)

漠然

February 21st, 2023

感谢苏神！我通过可视化对比了Lion与Adam优化器，可以看到Lion优化器趋向于在大范围进行搜索解空间以后再进行小范围搜索，与adam不同，adam趋向于直接使用小范围局部的梯度直接走向局部最小值，此外SGD也有类似的表现，而SIGNUM则与Lion相近，但是由于其没有衰减系数，无法快速的锁定大致范围。此外Lion优化器必须搭配学习率衰减。

github地址：https://github.com/nengwp/Lion-vs-Adam

[回复评论](https://kexue.fm/archives/9473/comment-page-1?replyTo=20987#respond-post-9473)

zw

February 21st, 2023

lr的schedule有什么讲究吗？warmup和decay的策略

[回复评论](https://kexue.fm/archives/9473/comment-page-1?replyTo=20988#respond-post-9473)

[苏剑林](https://kexue.fm) 发表于
February 24th, 2023

这个网上有很多参考工作了，我自己没有太好的经验可以分享。

[回复评论](https://kexue.fm/archives/9473/comment-page-1?replyTo=21006#respond-post-9473)

AaronK

March 2nd, 2023

Hi, I found that the LION optimizer is related to the paper "Noise Is Not the Main Factor Behind the Gap Between Sgd and Adam on Transformers, But Sign Descent Might Be" (https://openreview.net/pdf?id=a65YK0cqH8g). This paper empirically showed that Sign-SGD behaves more like Adam and benefits from noise.

I appreciate your idea that "Lion通过sign操作平等地对待了每一个分量，使得模型充分地发挥了每一个分量的作用，从而有更好的泛化性能". Also, I am curious about the idea of "事实上可以证明，Adam早期的更新量也接近于sign，只是随着训练步数的增加才逐渐偏离". I wonder what is the intuition behind the idea.

Thank you.

[回复评论](https://kexue.fm/archives/9473/comment-page-1?replyTo=21030#respond-post-9473)

[苏剑林](https://kexue.fm) 发表于
March 2nd, 2023

Thank you for sharing the paper! It is indeed valuable work.

It is easy to prove that $\\hat{\\boldsymbol{m}}\_1 = \\boldsymbol{g}\_1$ and $\\hat{\\boldsymbol{v}}\_1 = \|\\boldsymbol{g}\_1\|$. Therefore we have $\\boldsymbol{u}\_1 = \\boldsymbol{g}\_1 / \|\\boldsymbol{g}\_1\|=\\text{sign}(\\boldsymbol{g}\_1)$ for Adam. At the beginning of training, $\\boldsymbol{g}\_2,\\boldsymbol{g}\_3,\\cdots$ still has a direction similar to $\\boldsymbol{g}\_1$, so the result still holds approximately.

[回复评论](https://kexue.fm/archives/9473/comment-page-1?replyTo=21041#respond-post-9473)

hazdzz

April 4th, 2023

Noise Is Not the Main Factor Behind the Gap Between Sgd and Adam on Transformers, But Sign Descent Might Be 這篇 paper 有從理論角度闡明 sign 對梯度下降的優缺點。

[回复评论](https://kexue.fm/archives/9473/comment-page-1?replyTo=21312#respond-post-9473)

[苏剑林](https://kexue.fm) 发表于
April 4th, 2023

谢谢。之前已经找到过这篇文献。

[回复评论](https://kexue.fm/archives/9473/comment-page-1?replyTo=21337#respond-post-9473)

l

April 9th, 2023

有个没想通的地方

\> Lion的更新规则有所不同，尤其是动量的更新放在了变量的更新之后，并且在充分的实验中显示出了它在效果上的优势。

在公式（1）中，LION在更新动量$m\_t$时， $g\_t$是在新的参数下重新算出来的吗？

\- 如果是的话，那么公式（1）中的第一个公式计算$u\_t$时，$g\_t$是不是应该改为$g\_{t-1}$？

\- 如果不是的话，那么动量更新放在前后好像也没什么差异了？

[回复评论](https://kexue.fm/archives/9473/comment-page-1?replyTo=21368#respond-post-9473)

l 发表于
April 9th, 2023

看原论文中的伪代码，也是没有重新算$g\_t$的，那么动量更新在变量更新之前还是之后，会有什么不同吗？

[回复评论](https://kexue.fm/archives/9473/comment-page-1?replyTo=21369#respond-post-9473)

[苏剑林](https://kexue.fm) 发表于
April 10th, 2023

这里跟具体的实现有关系，就是如果先更新了$\\boldsymbol{m}\_t$，那么$\\boldsymbol{m}\_{t-1}$就被覆盖了，无法直接算更新量了（或者只能间接从$\\boldsymbol{m}\_t$倒推出来）。

[回复评论](https://kexue.fm/archives/9473/comment-page-1?replyTo=21374#respond-post-9473)

Earthson Lu

May 31st, 2023

请教一个问题，Lion这个优化器适用于Embedding的训练吗？我发现使用Lion之后，Embedding的norm2会越来越大，最后就炸了（即便配合weight\_decay也是杯水车薪，很难控制住）。

我测试下来发现Adam类的优化器在embedding更新的时候，embedding的norm2会非常稳定，增长非常缓慢。

目前我摸索出来比较稳定的策略是，先全部用NAdam小学习率初始化一个epoch，然后主干用Lion，embedding还是用NAdam，配合warmup+decay。

[回复评论](https://kexue.fm/archives/9473/comment-page-1?replyTo=21831#respond-post-9473)

[苏剑林](https://kexue.fm) 发表于
May 31st, 2023

我后面都是用Tiger了（可以理解为Lion的一个case）： [https://kexue.fm/archives/9512](https://kexue.fm/archives/9512)

里边的关键之一是用权重的RMS来自适应调节学习率，我这边直接从零预训练的表现都不错（最大尝试过3亿参数量的模型训练）。

[回复评论](https://kexue.fm/archives/9473/comment-page-1?replyTo=21843#respond-post-9473)

kpmokpmo

July 26th, 2023

苏神，前一段同时接触到两个今年出的优化器lion和adan，在我的数据集上后者较优，不知道您对adan怎么看。Adan: Adaptive Nesterov Momentum Algorithm for Faster Optimizing Deep Models

[回复评论](https://kexue.fm/archives/9473/comment-page-1?replyTo=22345#respond-post-9473)

[苏剑林](https://kexue.fm) 发表于
July 27th, 2023

看了看，增加了两组缓存变量（$\\boldsymbol{g}\_{k-1}$和$\\boldsymbol{n}\_{k-1}$），看起来有点脑壳疼...

[回复评论](https://kexue.fm/archives/9473/comment-page-1?replyTo=22357#respond-post-9473)

[取消回复](https://kexue.fm/archives/9473#respond-post-9473)

你的大名

电子邮箱

个人网站（选填）

1\. 可以使用LaTeX代码，点击“预览效果”可查看效果；

2\. 可以通过点击评论楼层编号来引用该楼层；

3\. 网站可能会有点卡，如非确认评论失败，请不要重复点击提交。

### 内容速览

[先说结果](https://kexue.fm/archives/9473#%E5%85%88%E8%AF%B4%E7%BB%93%E6%9E%9C)
[论文实验](https://kexue.fm/archives/9473#%E8%AE%BA%E6%96%87%E5%AE%9E%E9%AA%8C)
[超参设置](https://kexue.fm/archives/9473#%E8%B6%85%E5%8F%82%E8%AE%BE%E7%BD%AE)
[延伸思考](https://kexue.fm/archives/9473#%E5%BB%B6%E4%BC%B8%E6%80%9D%E8%80%83)
[文章小结](https://kexue.fm/archives/9473#%E6%96%87%E7%AB%A0%E5%B0%8F%E7%BB%93)

### 智能搜索

支持整句搜索！网站自动使用 [结巴分词](https://github.com/fxsjy/jieba) 进行分词，并结合ngrams排序算法给出合理的搜索结果。

### 热门标签

[生成模型](https://kexue.fm/tag/%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B/) [attention](https://kexue.fm/tag/attention/) [模型](https://kexue.fm/tag/%E6%A8%A1%E5%9E%8B/) [语言模型](https://kexue.fm/tag/%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/) [优化](https://kexue.fm/tag/%E4%BC%98%E5%8C%96/) [网站](https://kexue.fm/tag/%E7%BD%91%E7%AB%99/) [概率](https://kexue.fm/tag/%E6%A6%82%E7%8E%87/) [梯度](https://kexue.fm/tag/%E6%A2%AF%E5%BA%A6/) [转载](https://kexue.fm/tag/%E8%BD%AC%E8%BD%BD/) [微分方程](https://kexue.fm/tag/%E5%BE%AE%E5%88%86%E6%96%B9%E7%A8%8B/) [天象](https://kexue.fm/tag/%E5%A4%A9%E8%B1%A1/) [矩阵](https://kexue.fm/tag/%E7%9F%A9%E9%98%B5/) [深度学习](https://kexue.fm/tag/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/) [分析](https://kexue.fm/tag/%E5%88%86%E6%9E%90/) [积分](https://kexue.fm/tag/%E7%A7%AF%E5%88%86/) [python](https://kexue.fm/tag/python/) [力学](https://kexue.fm/tag/%E5%8A%9B%E5%AD%A6/) [无监督](https://kexue.fm/tag/%E6%97%A0%E7%9B%91%E7%9D%A3/) [几何](https://kexue.fm/tag/%E5%87%A0%E4%BD%95/) [优化器](https://kexue.fm/tag/%E4%BC%98%E5%8C%96%E5%99%A8/) [扩散](https://kexue.fm/tag/%E6%89%A9%E6%95%A3/) [节日](https://kexue.fm/tag/%E8%8A%82%E6%97%A5/) [生活](https://kexue.fm/tag/%E7%94%9F%E6%B4%BB/) [文本生成](https://kexue.fm/tag/%E6%96%87%E6%9C%AC%E7%94%9F%E6%88%90/) [数论](https://kexue.fm/tag/%E6%95%B0%E8%AE%BA/)

### 随机文章

- [生成扩散模型漫谈（十八）：得分匹配 = 条件得分匹配](https://kexue.fm/archives/9509)
- [数学基本技艺(A Mathematical Trivium)](https://kexue.fm/archives/2071)
- [基于fine tune的图像分类（百度分狗竞赛）](https://kexue.fm/archives/4611)
- [地球引力场的悬链线方程](https://kexue.fm/archives/1361)
- [今天把Blog升级了](https://kexue.fm/archives/21)
- [变分与理论力学略览](https://kexue.fm/archives/1304)
- [又一道川菜！媲美“开水白菜”的瓜燕穗肚](https://kexue.fm/archives/6158)
- [两男一女分享2009年诺贝尔化学奖](https://kexue.fm/archives/171)
- [有趣的求极限题：随心所欲的放缩](https://kexue.fm/archives/3256)
- [哈哈，我的“《圣经》”到了](https://kexue.fm/archives/2008)

### 最近评论

- [苏剑林](https://kexue.fm/archives/5239/comment-page-3#comment-27496): 1、明白了，我将$q\_{\\phi}(z\|x)$看成$q\_{\\phi}(x\|z)$了（ELBO厌...
- [Suahi](https://kexue.fm/archives/5239/comment-page-3#comment-27493): 谢谢苏老师的回复！1\. 首先回复您为什么ELBO不带KL，并不是最终损失函数形式，需要做如下变...
- [eular](https://kexue.fm/archives/10373/comment-page-1#comment-27492): 是的，当$k$比较大时会出现这种情况。
- [苏剑林](https://kexue.fm/archives/5239/comment-page-3#comment-27491): 肯定是$\\mathbb{E}\_{x \\sim p\_{data}(x)}\[\\log(p\_{\\th...
- [苏剑林](https://kexue.fm/archives/10373/comment-page-1#comment-27490): 你的意思是$\\lambda(\\boldsymbol{x}) < x\_{\\min}$，一般情况下...
- [苏剑林](https://kexue.fm/archives/10735/comment-page-1#comment-27489): 好问题，下一篇文章可能会讨论这个问题
- [苏剑林](https://kexue.fm/archives/10633/comment-page-1#comment-27488): 不大熟悉，但都diffusion forcing了，还有必要CM吗
- [苏剑林](https://kexue.fm/archives/10711/comment-page-2#comment-27487): 简单看了一下，好像没什么新东西呀？还是我看漏了什么？
- [苏剑林](https://kexue.fm/archives/8601/comment-page-1#comment-27486): 感谢指出，已修正。
- [苏剑林](https://kexue.fm/archives/9209/comment-page-7#comment-27485): 扩散桥确实时不时刷到，但没理解这一套东西有什么特别价值或者应用，所以也就没去学习。如果您了解它...

### 友情链接

- [Cool Papers](https://papers.cool)
- [数学研发](https://bbs.emath.ac.cn)
- [Seatop](http://www.seatop.com.cn/)
- [Xiaoxia](https://xiaoxia.org/)
- [积分表-网络版](https://kexue.fm/sci/integral/index.html)
- [丝路博傲](http://blog.dvxj.com/)
- [ph4ntasy 饭特稀](http://www.ph4ntasy.com/)
- [数学之家](http://www.2math.cn/)
- [有趣天文奇观](http://interesting-sky.china-vo.org/)
- [TwistedW](http://www.twistedwg.com/)
- [godweiyang](https://godweiyang.com/)
- [AI柠檬](https://blog.ailemon.net/)
- [王登科-DK博客](https://greatdk.com)
- [ESON](https://blog.eson.org/)
- [枫之羽](https://fzhiy.net/)
- [Mathor's blog](https://wmathor.com/)
- [coding-zuo](https://coding-zuo.github.io/)
- [博科园](https://www.bokeyuan.net/)
- [孔皮皮的博客](https://www.kppkkp.top/)
- [运鹏的博客](https://yunpengtai.top/)
- [jiming.site](https://jiming.site/)
- [OmegaXYZ](https://www.omegaxyz.com/)
- [Blog by Eacls](https://www.eacls.top/)
- [EAI猩球](https://www.robotech.ink/)
- [文举的博客](https://liwenju0.com/)
- [申请链接](https://kexue.fm/links.html)

[![署名-非商业用途-保持一致](https://kexue.fm/usr/themes/geekg/images/cc.gif)](http://creativecommons.org/licenses/by-nc-nd/2.5/cn/) 本站采用创作共用版权协议，要求署名、非商业用途和保持一致。转载本站内容必须也遵循“ [署名-非商业用途-保持一致](http://creativecommons.org/licenses/by-nc-nd/2.5/cn/)”的创作共用协议。

© 2009-2025 Scientific Spaces. All rights reserved. Theme by [laogui](http://www.laogui.com). Powered by [Typecho](http://typecho.org). 备案号: [粤ICP备09093259号-1/2](https://beian.miit.gov.cn/)。