![MobileSideBar](https://kexue.fm/usr/themes/geekg/images/slide-button.png)

## SEARCH

## MENU

- [打赏](https://kexue.fm/reward.html)
- [公式](https://kexue.fm/latex.html)
- [天象](https://kexue.fm/ac.html)
- [链接](https://kexue.fm/links.html)
- [时光](https://kexue.fm/me.html)
- [博览](https://kexue.fm/science.html)
- [归档](https://kexue.fm/content.html)

## CATEGORIES

- [千奇百怪](https://kexue.fm/category/Everything)
- [天文探索](https://kexue.fm/category/Astronomy)
- [数学研究](https://kexue.fm/category/Mathematics)
- [物理化学](https://kexue.fm/category/Phy-chem)
- [信息时代](https://kexue.fm/category/Big-Data)
- [生物自然](https://kexue.fm/category/Biology)
- [图片摄影](https://kexue.fm/category/Photograph)
- [问题百科](https://kexue.fm/category/Questions)
- [生活/情感](https://kexue.fm/category/Life-Feeling)
- [资源共享](https://kexue.fm/category/Resources)

## NEWPOSTS

- [SVD的导数](https://kexue.fm/archives/10878)
- [智能家居之手搓一套能接入米家的零冷水装置](https://kexue.fm/archives/10869)
- [Transformer升级之路：1...](https://kexue.fm/archives/10862)
- [矩阵的有效秩（Effective ...](https://kexue.fm/archives/10847)
- [通过梯度近似寻找Normaliza...](https://kexue.fm/archives/10831)
- [MoE环游记：4、难处应当多投入](https://kexue.fm/archives/10815)
- [高阶muP：更简明但更高明的谱条件缩放](https://kexue.fm/archives/10795)
- [初探muP：超参数的跨模型尺度迁移规律](https://kexue.fm/archives/10770)
- [MoE环游记：3、换个思路来分配](https://kexue.fm/archives/10757)
- [Muon续集：为什么我们选择尝试M...](https://kexue.fm/archives/10739)

## COMMENTS

- [长琴: mla出来之后就知道苏神一定会念念不忘。hah。没注意到公众号...](https://kexue.fm/archives/10862/comment-page-1#comment-27503)
- [Suahi: 谢谢苏老师的回复！\
我也是在从MLE出发推导VAE的Loss函...](https://kexue.fm/archives/5239/comment-page-3#comment-27502)
- [lay: 苏神，我在训练一个1.5B模型的过程中在某一步grad\_nor...](https://kexue.fm/archives/10657/comment-page-1#comment-27500)
- [hazdzz: 如果 RoPE 结合 Givens rotation meth...](https://kexue.fm/archives/10862/comment-page-1#comment-27499)
- [autumn23333: 请问一下论文附带的代码是不是写错了fourier\_sin = ...](https://kexue.fm/archives/10862/comment-page-1#comment-27498)
- [SunlightZero: 在《Step-by-Step Diffusion: An El...](https://kexue.fm/archives/9164/comment-page-4#comment-27497)
- [苏剑林: 1、明白了，我将$q\_{\\phi}(z\|x)$看成$q\_{\\p...](https://kexue.fm/archives/5239/comment-page-3#comment-27496)
- [Suahi: 谢谢苏老师的回复！1\. 首先回复您为什么ELBO不带KL，并不...](https://kexue.fm/archives/5239/comment-page-3#comment-27493)
- [eular: 是的，当$k$比较大时会出现这种情况。](https://kexue.fm/archives/10373/comment-page-1#comment-27492)
- [苏剑林: 肯定是$\\mathbb{E}\_{x \\sim p\_{data}...](https://kexue.fm/archives/5239/comment-page-3#comment-27491)

## USERLOGIN

- [登录](https://kexue.fm/admin/login.php)

[科学空间\|Scientific Spaces](https://kexue.fm)

- [登录](https://kexue.fm/admin/login.php)
- [打赏](https://kexue.fm/reward.html)
- [公式](https://kexue.fm/latex.html)
- [天象](https://kexue.fm/ac.html)
- [链接](https://kexue.fm/links.html)
- [时光](https://kexue.fm/me.html)
- [博览](https://kexue.fm/science.html)
- [归档](https://kexue.fm/content.html)

渴望成为一个小飞侠

- [![](https://kexue.fm/usr/themes/geekg/images/rss.png)\
\
欢迎订阅](https://kexue.fm/feed)
- [![](https://kexue.fm/usr/themes/geekg/images/mail.png)\
\
个性邮箱](https://kexue.fm/archives/119)
- [![](https://kexue.fm/usr/themes/geekg/images/Saturn.png)\
\
天象信息](https://kexue.fm/ac.html)
- [![](https://kexue.fm/usr/themes/geekg/images/iss.png)\
\
观测ISS](https://kexue.fm/archives/41)
- [![](https://kexue.fm/usr/themes/geekg/images/pi.png)\
\
LaTeX](https://kexue.fm/latex.html)
- [![](https://kexue.fm/usr/themes/geekg/images/mlogo.png)\
\
关于博主](https://kexue.fm/me.html)

欢迎访问“科学空间”，这里将与您共同探讨自然科学，回味人生百态；也期待大家的分享～

- [**千奇百怪** Everything](https://kexue.fm/category/Everything)
- [**天文探索** Astronomy](https://kexue.fm/category/Astronomy)
- [**数学研究** Mathematics](https://kexue.fm/category/Mathematics)
- [**物理化学** Phy-chem](https://kexue.fm/category/Phy-chem)
- [**信息时代** Big-Data](https://kexue.fm/category/Big-Data)
- [**生物自然** Biology](https://kexue.fm/category/Biology)
- [**图片摄影** Photograph](https://kexue.fm/category/Photograph)
- [**问题百科** Questions](https://kexue.fm/category/Questions)
- [**生活/情感** Life-Feeling](https://kexue.fm/category/Life-Feeling)
- [**资源共享** Resources](https://kexue.fm/category/Resources)

- [**千奇百怪**](https://kexue.fm/category/Everything)
- [**天文探索**](https://kexue.fm/category/Astronomy)
- [**数学研究**](https://kexue.fm/category/Mathematics)
- [**物理化学**](https://kexue.fm/category/Phy-chem)
- [**信息时代**](https://kexue.fm/category/Big-Data)
- [**生物自然**](https://kexue.fm/category/Biology)
- [**图片摄影**](https://kexue.fm/category/Photograph)
- [**问题百科**](https://kexue.fm/category/Questions)
- [**生活/情感**](https://kexue.fm/category/Life-Feeling)
- [**资源共享**](https://kexue.fm/category/Resources)

[首页](https://kexue.fm) [数学研究](https://kexue.fm/category/Mathematics) 漫谈重参数：从正态分布到Gumbel Softmax

10Jun

# [漫谈重参数：从正态分布到Gumbel Softmax](https://kexue.fm/archives/6705)

By 苏剑林 \|
2019-06-10 \|
270892位读者\|

最近在用VAE处理一些文本问题的时候遇到了对离散形式的后验分布求期望的问题，于是沿着“离散分布 + 重参数”这个思路一直搜索下去，最后搜到了Gumbel Softmax，从对Gumbel Softmax的学习过程中，把重参数的相关内容都捋了一遍，还学到一些梯度估计的新知识，遂记录在此。

文章从连续情形出发开始介绍重参数，主要的例子是正态分布的重参数；然后引入离散分布的重参数，这就涉及到了Gumbel Softmax，包括Gumbel Softmax的一些证明和讨论；最后再讲讲重参数背后的一些故事，这主要跟梯度估计有关。

## 基本概念 [\#](https://kexue.fm/archives/6705\#%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5)

**重参数（Reparameterization）** 实际上是处理如下期望形式的目标函数的一种技巧：

\\begin{equation}L\_{\\theta}=\\mathbb{E}\_{z\\sim p\_{\\theta}(z)}\[f(z)\]\\label{eq:base}\\end{equation}

这样的目标在VAE中会出现，在文本GAN也会出现，在强化学习中也会出现（$f(z)$对应于奖励函数），所以深究下去，我们会经常碰到这样的目标函数。取决于$z$的连续性，它对应不同的形式：

\\begin{equation}\\int p\_{\\theta}(z) f(z)dz\\,\\,\\,\\text{(连续情形)}\\qquad\\qquad \\sum\_{z} p\_{\\theta}(z) f(z)\\,\\,\\,\\text{(离散情形)}\\end{equation}

当然，离散情况下我们更喜欢将记号$z$换成$y$或者$c$。

为了最小化$L\_{\\theta}$，我们就需要把$L\_{\\theta}$明确地写出来，这意味着我们要实现从$p\_{\\theta}(z)$中采样，而$p\_{\\theta}(z)$是带有参数$\\theta$的，如果直接采样的话，那么就失去了$\\theta$的信息（梯度），从而无法更新参数$\\theta$。而Reparameterization则是提供了这样的一种变换，使得我们可以直接从$p\_{\\theta}(z)$中采样，并且保留$\\theta$的梯度。（注：如果考虑最一般的形式，那么$f(z)$也应该带上参数$\\theta$，但这没有增加本质难度。）

## 连续情形 [\#](https://kexue.fm/archives/6705\#%E8%BF%9E%E7%BB%AD%E6%83%85%E5%BD%A2)

简单起见，我们先考虑连续情形

\\begin{equation}L\_{\\theta}=\\int p\_{\\theta}(z) f(z)dz\\label{eq:lianxu}\\end{equation}

其中$p\_{\\theta}(z)$是具有显式概率密度表达式的分布，在 [变分自编码器](https://kexue.fm/archives/5253) 中常见的是正态分布$p\_{\\theta}(z)=\\mathcal{N}\\left(z;\\mu\_{\\theta},\\sigma\_{\\theta}^2\\right)$。

### 形式 [\#](https://kexue.fm/archives/6705\#%E5%BD%A2%E5%BC%8F)

从式$\\eqref{eq:lianxu}$中知道，连续情形的$L\_{\\theta}$实际上就对应一个积分，所以，为了明确写出$L\_{\\theta}$，有两种途径：最直接的方式是精确地完成积分$\\eqref{eq:lianxu}$，得到显式表达式，但这通常都是不可能的了；所以，唯一的办法是转化为采样形式$\\eqref{eq:base}$，并试图在采样过程中保留$\\theta$的梯度。

重参数就是这样的一种技巧，它假设从分布$p\_{\\theta}(z)$中采样可以分解为两个步骤：(1) 从无参数分布$q(\\varepsilon)$中采样一个$\\varepsilon$；(2) 通过变换$z=g\_{\\theta}(\\varepsilon)$生成$z$。那么，式$\\eqref{eq:base}$就变成了

\\begin{equation}L\_{\\theta}=\\mathbb{E}\_{\\varepsilon\\sim q(\\varepsilon)}\[f(g\_{\\theta}(\\varepsilon))\]\\label{eq:reparam}\\end{equation}

这时候被采样的分布就没有任何参数了，全部被转移到$f$内部了，因此可以采样若干个点，当成普通的loss那样写下来了。

### 例子 [\#](https://kexue.fm/archives/6705\#%E4%BE%8B%E5%AD%90)

一个最简单的例子就是正态分布：对于正态分布来说，重参数就是“从$\\mathcal{N}\\left(z;\\mu\_{\\theta},\\sigma\_{\\theta}^2\\right)$中采样一个$z$”变成“从$\\mathcal{N}\\left(\\varepsilon;0, 1\\right)$中采样一个$\\varepsilon$，然后计算$\\varepsilon\\times \\sigma\_{\\theta} + \\mu\_{\\theta}$”，所以

\\begin{equation}\\mathbb{E}\_{z\\sim \\mathcal{N}\\left(z;\\mu\_{\\theta},\\sigma\_{\\theta}^2\\right)}\\big\[f(z)\\big\] = \\mathbb{E}\_{\\varepsilon\\sim \\mathcal{N}\\left(\\varepsilon;0, 1\\right)}\\big\[f(\\varepsilon\\times \\sigma\_{\\theta} + \\mu\_{\\theta})\\big\]\\end{equation}

如何理解直接采样没有梯度而重参数之后就有梯度呢？其实很简单，比如我说从$\\mathcal{N}\\left(z;\\mu\_{\\theta},\\sigma\_{\\theta}^2\\right)$中采样一个数来，然后你跟我说采样到5，我完全看不出5跟$\\theta$有什么关系呀（求梯度只能为0）；但是如果先从$\\mathcal{N}\\left(\\varepsilon;0, 1\\right)$中采样一个数比如$0.2$，然后计算$0.2 \\sigma\_{\\theta} + \\mu\_{\\theta}$，这样我就知道采样出来的结果跟$\\theta$的关系了（能求出有效的梯度）。

### 总结 [\#](https://kexue.fm/archives/6705\#%E6%80%BB%E7%BB%93)

让我们把前面的内容重新整理一下。总的来说，连续情形的重参数还是比较简单的：连续情形下，我们要处理的$L\_{\\theta}$实际上是式$\\eqref{eq:lianxu}$，由于精确的积分我们没有办法显式地写出来，所以需要转化为采样，而为了在采样的过程中得到有效的梯度，我们就需要重参数。

从数学本质来看，重参数是一种积分变换，即原来是关于$z$积分，通过$z=g\_{\\theta}(\\varepsilon)$变换之后得到新的积分形式，

## 离散情形 [\#](https://kexue.fm/archives/6705\#%E7%A6%BB%E6%95%A3%E6%83%85%E5%BD%A2)

为了突出“离散”，我们将随机变量$z$换成$y$，即对于离散情形要面对的目标函数是

\\begin{equation}L\_{\\theta}=\\mathbb{E}\_{y\\sim p\_{\\theta}(y)}\[f(y)\]=\\sum\_y p\_{\\theta}(y) f(y)\\label{eq:lisan}\\end{equation}

其中离散意味着一般情况$y$是可枚举的，换句话说$p\_{\\theta}(y)$此时是一个$k$分类模型：

\\begin{equation}p\_{\\theta}(y)=softmax\\big(o\_1,o\_2,\\dots,o\_k\\big)=\\frac{1}{\\sum\\limits\_{i=1}^k e^{o\_i}}\\left(e^{o\_1}, e^{o\_2}, \\dots, e^{o\_k}\\right)\\label{eq:softmax}\\end{equation}

其中各个$o\_i$是$\\theta$的函数。

### 分析 [\#](https://kexue.fm/archives/6705\#%E5%88%86%E6%9E%90)

读者看到$\\eqref{eq:lisan}$中的求和，第一反应可能是“求和？那就求呗，又不是求不了。”。

的确，这也是笔者当时看到它的第一反应。与连续情形的$\\eqref{eq:lianxu}$不一样，式$\\eqref{eq:lianxu}$如果直接硬杠的话需要完成积分（也可以看成无穷多个点的求和），我们没法做到这一点。但是对于离散的$\\eqref{eq:lisan}$，只不过是有限项求和，理论上确实可以直接完成求和再去梯度下降。

但是，如果$k$特别大呢？举个例子，假设$y$是一个100维的向量，每个元素不是0就是1（二元变量），那么所有不同的$y$的总数目就是$2^{100}$，要对这样的$2^{100}$个单项进行求和，计算量是难以接受的；还有一个典型的例子是seq2seq的解码端（如果要做文本GAN就需要面对它），它的类别总数目是$\|V\|^l$，其中$\|V\|$是词表大小而$l$是句子长度。这样的情况下，直接完成精确的求和都是难以实现的。

### 形式 [\#](https://kexue.fm/archives/6705\#%E5%BD%A2%E5%BC%8F)

所以，还是需要回到采样上去，如果能够采样若干个点就能得到$\\eqref{eq:lisan}$的有效估计，并且还不损失梯度信息，那自然是最好了。为此，需要先引入Gumbel Max，它提供了一种从类别分布中采样的方法。

假设每个类别的概率是$p\_1,p\_2,\\dots,p\_k$，那么下述过程提供了一种依概率采样类别的方案，称为Gumbel Max：

\\begin{equation}\\mathop{\\text{argmax}}\_i \\Big(\\log p\_i - \\log(-\\log \\varepsilon\_i)\\Big)\_{i=1}^k,\\quad \\varepsilon\_i\\sim U\[0, 1\]\\end{equation}

也就是说，先算出各个概率的对数$\\log p\_i$，然后从均匀分布$U\[0,1\]$中采样$k$个随机数$\\varepsilon\_1,\\dots,\\varepsilon\_k$，把$-\\log(-\\log \\varepsilon\_i)$加到$\\log p\_i$上去，最后把最大值对应的类别抽取出来就行了。

后面我们会证明，这样的过程精确等价于依概率$p\_1,p\_2,\\dots,p\_k$采样一个类别，换句话说，在Gumbel Max中，输出$i$的概率正好是$p\_i$。由于现在的随机性已经转移到$U\[0,1\]$上去了，并且$U\[0,1\]$不带有未知参数，因此Gumbel Max就是离散分布的一个重参数过程。

但是，我们希望重参数不丢失梯度信息，但是Gumbel Max做不到，因为$\\mathop{\\text{argmax}}$不可导，为此，需要做进一步的近似。首先，留意到在神经网络中，处理离散输入的基本方法是转化为one hot形式，包括Embedding层的本质也是one hot全连接（参考 [《词向量与Embedding究竟是怎么回事？》](https://kexue.fm/archives/4122)），因此$\\mathop{\\text{argmax}}$实际上是$\\text{onehot}(\\mathop{\\text{argmax}}))$，然后，我们寻求$\\text{onehot}(\\mathop{\\text{argmax}}))$的光滑近似，它就是$softmax$（参考 [《函数光滑化杂谈：不可导函数的可导逼近》](https://kexue.fm/archives/6620)）。

由此，我们得到Gumbel Max的光滑近似版本——Gumbel Softmax：

\\begin{equation}softmax \\Big(\\big(\\log p\_i - \\log(-\\log \\varepsilon\_i)\\big)\\big/\\tau\\Big)\_{i=1}^k,\\quad \\varepsilon\_i\\sim U\[0, 1\]\\end{equation}

其中参数$\\tau > 0$称为退火参数，它越小输出结果就越接近one hot形式（但同时梯度消失就越严重）。提示一个小技巧，如果$p\_i$是softmax的输出，即$\\eqref{eq:softmax}$的形式，那么大可不必先算出$p\_i$再取对数，直接将$\\log p\_i$替换为$o\_i$即可：

\\begin{equation}softmax \\Big(\\big(o\_i - \\log(-\\log \\varepsilon\_i)\\big)\\big/\\tau\\Big)\_{i=1}^k,\\quad \\varepsilon\_i\\sim U\[0, 1\]\\end{equation}

> **Gumbel Max的证明：**
>
> Gumbel Max的形式看上去有点复杂，远没有正态分布的重参数简单，但事实上只要鼓起勇气去看它，它连证明都不困难。我们想要证明Gumbel Max最后输出$i$的概率是$p\_i$，不失一般性，这里我们证明输出1的概率是$p\_1$。
>
> 注意，输出1意味着$\\log p\_1 - \\log(-\\log \\varepsilon\_1)$是最大的，这又意味着：
>
> \\begin{equation}\\begin{aligned}
>
> &\\log p\_1 - \\log(-\\log \\varepsilon\_1) > \\log p\_2 - \\log(-\\log \\varepsilon\_2) \\\
>
> &\\log p\_1 - \\log(-\\log \\varepsilon\_1) > \\log p\_3 - \\log(-\\log \\varepsilon\_3) \\\
>
> &\\qquad \\vdots\\\
>
> &\\log p\_1 - \\log(-\\log \\varepsilon\_1) > \\log p\_k - \\log(-\\log \\varepsilon\_k)
>
> \\end{aligned}
>
> \\end{equation}
>
> 注意每个不等式都是独立的，也就是说$\\log p\_1 - \\log(-\\log \\varepsilon\_1)$与$\\log p\_2 - \\log(-\\log \\varepsilon\_2)$的关系如何，也不影响它跟$\\log p\_3 - \\log(-\\log \\varepsilon\_3)$的关系。这样我们只需要单独分析每一个不等式的概率。不失一般性，我们只分析第一个不等式，化简后得到：
>
> \\begin{equation}\\varepsilon\_2 < \\varepsilon\_1^{p\_2 / p\_1}\\leq 1 \\end{equation}
>
> 由于$\\varepsilon\_2\\sim U\[0,1\]$，所以$\\varepsilon\_2 < \\varepsilon\_1^{p\_2 / p\_1}$的概率就是$\\varepsilon\_1^{p\_2 / p\_1}$，这就是固定$\\varepsilon\_1$的情况下，第一个不等式成立的概率。那么，所有不等式同时成立的概率是
>
> \\begin{equation}\\varepsilon\_1^{p\_2 / p\_1}\\varepsilon\_1^{p\_3 / p\_1}\\dots \\varepsilon\_1^{p\_k / p\_1}=\\varepsilon\_1^{(p\_2 + p\_3 + \\dots + p\_k) / p\_1}=\\varepsilon\_1^{(1/p\_1)-1}\\end{equation}
>
> 然后对所有$\\varepsilon\_1$求平均，就是
>
> \\begin{equation}\\int\_0^1 \\varepsilon\_1^{(1/p\_1)-1}d\\varepsilon\_1 = p\_1\\end{equation}
>
> 这就是类别1出现的概率，它就是$p\_1$。至此，我们完成了Gumbel Max采样过程的证明。

### 例子 [\#](https://kexue.fm/archives/6705\#%E4%BE%8B%E5%AD%90)

跟连续情形一样，Gumbel Softmax就是用在需要求$\\mathbb{E}\_{y\\sim p\_{\\theta}(y)}\[f(y)\]$、且无法直接完成对$y$求和的场景，这时候我们算出$p\_{\\theta}(y)$（或者$o\_i$），然后选定一个$\\tau > 0$，用Gumbel Softmax算出一个随机向量来$\\tilde{y}$，代入计算得到$f(\\tilde{y})$，它就是$\\mathbb{E}\_{y\\sim p\_{\\theta}(y)}\[f(y)\]$的一个好的近似，且保留了梯度信息。

注意，Gumbel Softmax不是类别采样的等价形式，Gumbel Max才是。而Gumbel Max可以看成是Gumbel Softmax在$\\tau \\to 0$时的极限。所以在应用Gumbel Softmax时，开始可以选择较大的$\\tau$（比如1），然后慢慢退火到一个接近于0的数（比如0.01），这样才能得到比较好的结果。

下面提供一个自己实现的离散隐变量的VAE例子：

[https://github.com/bojone/vae/blob/master/vae\_keras\_cnn\_gs.py](https://github.com/bojone/vae/blob/master/vae_keras_cnn_gs.py)

效果图：

[![基于Gumbel Softmax重参数的离散隐变量VAE生成](https://kexue.fm/usr/uploads/2019/06/990920715.png)](https://kexue.fm/usr/uploads/2019/06/990920715.png)

基于Gumbel Softmax重参数的离散隐变量VAE生成

### 溯源 [\#](https://kexue.fm/archives/6705\#%E6%BA%AF%E6%BA%90)

Gumbel Max由来已久，但首次提出并应用Gumbel Softmax的是论文 [《Categorical Reparameterization with Gumbel-Softmax》](https://papers.cool/arxiv/1611.01144)，这篇论文主要探讨了部分隐变量是离散型变量的变分推断问题，比如基于VAE的半监督学习（方法上有点类似 [《变分自编码器（四）：一步到位的聚类方案》](https://kexue.fm/archives/5887/)）。其后，在文章 [《GANS for Sequences of Discrete Elements with the Gumbel-softmax Distribution》](https://papers.cool/arxiv/1611.04051) 中，Gumbel Softmax首次被用在离散序列生成，但还不是文本生成，而是比较简单的人造字符序列。

其后， [SeqGAN](https://papers.cool/arxiv/1609.05473) 被提出，自那以后文本GAN模型一直以与强化学习结合的方式出现，基于Gumbel Softmax的纯深度学习和梯度下降的方法相对沉寂，直到 [RelGAN](https://openreview.net/forum?id=rJedV3R5tm) 的出现。 **RelGAN** 是ICLR 2019提出的模型，它提出了新型的生成器和判别器结构，使得直接用Gumbel Softmax训练出的文本GAN大幅度超过了以往的各种文本GAN模型。关于RelGAN，我们后面有机会再谈。

### 总结 [\#](https://kexue.fm/archives/6705\#%E6%80%BB%E7%BB%93)

这部分内容主要介绍的是Gumbel Softmax，它是离散情形下$\\eqref{eq:base}$型损失的一个重参数技巧。

理论上来说，离散情形的$\\eqref{eq:base}$只是有限项求和，不一定需要重参数。但事实上，“有限”也可能是相当大的数字，因此遍历求和可能难以进行，所以还是要转化为采样形式，从而需要重参数技巧，这就是Gumbel Softmax，源于对Gumbel Max的光滑化。

除了上述视角外，还有一个辅助的视角：Gumbel Softmax通过$\\tau\\to 0$的退火来逐渐逼近one hot，相比直接用原始的Softmax进行退火，区别在于原始Softmax退火只能得到最大值位置为1的one hot向量，而Gumbel Softmax有概率得到非最大值位置的one hot向量，增加了随机性，会使得基于采样的训练更充分一些。

## 背后的故事 [\#](https://kexue.fm/archives/6705\#%E8%83%8C%E5%90%8E%E7%9A%84%E6%95%85%E4%BA%8B)

重参数就这样介绍完了吗？远远没有，重参数的背后，实际上是一个称为“梯度估计（gradient estimator）”的大家族，而重参数只不过是这个大家族中的一员。每年的ICLR、ICML等顶会上搜索gradient estimator、REINFORCE等关键词，可以搜索到不少文章，说明这是个大家还在钻研的课题。

要想说清重参数的来龙去脉，也要说些梯度估计的故事。

### SF估计 [\#](https://kexue.fm/archives/6705\#SF%E4%BC%B0%E8%AE%A1)

前面我们分别讲了连续型和离散型的重参数，都是在“loss层面”讲述的，也就是说都是想办法把loss显式地定义好，剩下的交给框架自动求导、自动优化就是了。而事实上，就算不能显式地写出loss函数，也不妨碍我们对它求导，自然也不妨碍我们去用梯度下降了。比如

\\begin{equation}\\begin{aligned}\\frac{\\partial}{\\partial\\theta}\\int p\_{\\theta}(z) f(z)dz=&\\int f(z) \\frac{\\partial}{\\partial\\theta} p\_{\\theta}(z) dz\\\

=&\\int p\_{\\theta}(z)\\times\\frac{f(z)}{p\_{\\theta}(z)}\\frac{\\partial}{\\partial\\theta} p\_{\\theta}(z) dz\\\

=&\\mathbb{E}\_{z\\sim p\_{\\theta}(z)}\\left\[\\frac{f(z)}{p\_{\\theta}(z)}\\frac{\\partial}{\\partial\\theta} p\_{\\theta}(z)\\right\]\\\

=&\\mathbb{E}\_{z\\sim p\_{\\theta}(z)}\\Big\[f(z)\\frac{\\partial}{\\partial\\theta} \\log p\_{\\theta}(z)\\Big\]

\\end{aligned}\\label{eq:sf}\\end{equation}

现在我们得到了梯度的一个估计式，称为“SF估计”，全称是Score Function Estimator，这是对原来损失函数的最朴素的估计，在强化学习中$z$代表着策略，那么上式就是一个最基本的策略梯度，所以有时候也直接称上述估计为叫REINFORCE。要注意，对离散情形的损失函数重新推导一遍，结果也是一样的，也就是说，上述结果是通用的，不区分$z$是连续变量还是离散变量。现在我们可以直接从$p\_{\\theta}(z)$中采样若干个点来估算式$\\eqref{eq:sf}$的值了，不用担心会不会没梯度，因为式$\\eqref{eq:sf}$本身就是梯度了。

### 梯度方差 [\#](https://kexue.fm/archives/6705\#%E6%A2%AF%E5%BA%A6%E6%96%B9%E5%B7%AE)

看上去很美好，得到了一个连续和离散变量都适用的估计式，那为什么还需要重参数呢？

主要的原因是：SF估计的方差太大。式$\\eqref{eq:sf}$是函数$f(z) \\frac{\\partial}{\\partial\\theta} \\log p\_{\\theta}(z)$在分布$p\_{\\theta}(z)$下的期望，我们要采样几个点来算（理想情况下，希望只采样一个点），换句话说，我们想用下面的近似

\\begin{equation}\\mathbb{E}\_{z\\sim p\_{\\theta}(z)}\\Big\[f(z) \\frac{\\partial}{\\partial\\theta} \\log p\_{\\theta}(z)\\Big\]\\approx f(\\tilde{z}) \\frac{\\partial}{\\partial\\theta} \\log p\_{\\theta}(\\tilde{z}),\\quad \\tilde{z}\\sim p\_{\\theta}(z)\\end{equation}

于是问题就来了：这样的梯度估计方差很大。

什么是方差很大？它有什么影响？举个简单的例子，假如$\\alpha = avg(\[4, 5, 6\]) = avg(\[0, 5, 10\])$，也就是说，我们的目标$\\alpha$是三个数的平均值，这三个数要不就是$4,5,6$，要不就是$0,5,10$，在精确估计的情况下，两者是等价的，但是如果每一组只能随机选其中一个数呢？第一组可能选到4，这也没什么，跟准确值5只差一点；但是第二组可能选到0，这跟准确值5差得就有点大了。也就是说，随机选一个的情况下，第二组估计的波动（方差）太大了。类似地，SF估计出来的梯度方差也是如此，这导致了我们用梯度下降优化的时候相当不稳定，非常容易崩。

### 降方差 [\#](https://kexue.fm/archives/6705\#%E9%99%8D%E6%96%B9%E5%B7%AE)

从形式上看，式$\\eqref{eq:sf}$是非常漂亮的，本身形式不复杂，而且对离散变量和连续变量都通用，还对$f$没有特别要求（相反，重参数要求$f$可导，但是在诸如强化学习的场景下，$f(z)$对应着奖励函数，很难做到光滑可导）。所以，很多文章探讨基于式$\\eqref{eq:sf}$的 **降方差** 技巧，论文 [《Categorical Reparameterization with Gumbel-Softmax》](https://papers.cool/arxiv/1611.01144) 就列举了一些，近几年来也有一些新发展，总之，还是那句话，大家搜索gradient estimator、REINFORCE等关键词，就有不少文章了。

重参数是另一种降方差技巧，为此，我们写出重参数后的$\\eqref{eq:reparam}$的梯度表达式：

\\begin{equation}\\begin{aligned}\\frac{\\partial}{\\partial\\theta}\\mathbb{E}\_{\\varepsilon\\sim q(\\varepsilon)}\[f(g\_{\\theta}(\\varepsilon))\]=&\\mathbb{E}\_{\\varepsilon\\sim q(\\varepsilon)}\\left\[\\frac{\\partial}{\\partial\\theta}f(g\_{\\theta}(\\varepsilon))\\right\]\\\

=&\\mathbb{E}\_{\\varepsilon\\sim q(\\varepsilon)}\\left\[\\frac{\\partial f}{\\partial g} \\frac{\\partial g\_{\\theta}(\\varepsilon)}{\\partial\\theta}\\right\]

\\end{aligned}\\end{equation}

对比SF估计的式$\\eqref{eq:sf}$，我们可以直观感知为什么上式方差更小了：

> 1、SF估计中包含了$\\log p\_{\\theta}(z)$，我们知道，作为一个合理的概率分布，一般都在无穷远处（即$\\Vert z\\Vert \\to \\infty$）都会有$p\_{\\theta}(z)\\to 0$，取了$\\log$之后反而会趋于负无穷，换句话说，$\\log p\_{\\theta}(z)$这一项实际上放大了无穷远处的波动，从而一定程度上增加了方差；
>
> 2、SF估计中包含的是$f$而重参数之后变成了$\\frac{\\partial f}{\\partial g}$，$f$一般是神经网络，而通常我们定义的神经网络模型其实都是$\\mathcal{O}(z)$级别的模型，从而我们可以预期它的梯度是$\\mathcal{O}(1)$级别的（不严格成立，只能说在平均意义下基本成立），所以相对情况下更平稳一些，因此$f$的方差也比$\\frac{\\partial f}{\\partial g}$的方差要大。

鉴于这两个理由，我们就可以得出，一般情况下重参数之后梯度估计的方差会比SF估计要小。注意，这里还是要强调“一般情况”，换言之，“重参数降低梯度估计的方差”这个结论不是绝对成立的，上述两个理由都是在一般情况下（我们面对的多数模型）成立，如果非要较劲，我们总能构造出重参数反而增加方差的例子。

## 文章小结 [\#](https://kexue.fm/archives/6705\#%E6%96%87%E7%AB%A0%E5%B0%8F%E7%BB%93)

经过一番长篇大论，我们总算把重参数的故事基本上都捋清楚了。更深入地理解重参数技巧，是更好地理解VAE及文本GAN的必经之路。

从loss层面看，我们需要分连续和离散两种情形：连续情形下，重参数是用采样形式且不损失梯度地写出loss的方法；离散情形下，重参数有着跟连续情形一样的作用，不过更根本的原因是降低计算量（否则直接遍历求和也行）。从梯度估计层面看，重参数是降低梯度估计方差的一种有效手段，而同时还有其他的降低方差手段也被不少学者研究中。

总之，怎么看也不是个让人省心的玩意～

_**转载到请包括本文地址：** [https://kexue.fm/archives/6705](https://kexue.fm/archives/6705)_

_**更详细的转载事宜请参考：**_ [《科学空间FAQ》](https://kexue.fm/archives/6508#%E6%96%87%E7%AB%A0%E5%A6%82%E4%BD%95%E8%BD%AC%E8%BD%BD/%E5%BC%95%E7%94%A8)

**如果您还有什么疑惑或建议，欢迎在下方评论区继续讨论。**

**如果您觉得本文还不错，欢迎 [分享](https://kexue.fm/archives/6705#share)/ [打赏](https://kexue.fm/archives/6705#pay) 本文。打赏并非要从中获得收益，而是希望知道科学空间获得了多少读者的真心关注。当然，如果你无视它，也不会影响你的阅读。再次表示欢迎和感谢！**

打赏

![科学空间](https://kexue.fm/usr/themes/geekg/payment/wx.png)

微信打赏

![科学空间](https://kexue.fm/usr/themes/geekg/payment/zfb.png)

支付宝打赏

因为网站后台对打赏并无记录，因此欢迎在打赏时候备注留言。

你还可以 [**点击这里**](http://mail.qq.com/cgi-bin/qm_share?t=qm_mailme&email=tN7d1drY3drrx8H0xcWa19vZ) 或在下方评论区留言来告知你的建议或需求。

**如果您需要引用本文，请参考：**

苏剑林. (Jun. 10, 2019). 《漫谈重参数：从正态分布到Gumbel Softmax 》\[Blog post\]. Retrieved from [https://kexue.fm/archives/6705](https://kexue.fm/archives/6705)

@online{kexuefm-6705,

        title={漫谈重参数：从正态分布到Gumbel Softmax},

        author={苏剑林},

        year={2019},

        month={Jun},

        url={\\url{https://kexue.fm/archives/6705}},

}

分类： [数学研究](https://kexue.fm/category/Mathematics)    标签： [概率](https://kexue.fm/tag/%E6%A6%82%E7%8E%87/), [算法](https://kexue.fm/tag/%E7%AE%97%E6%B3%95/), [优化](https://kexue.fm/tag/%E4%BC%98%E5%8C%96/), [重参数](https://kexue.fm/tag/%E9%87%8D%E5%8F%82%E6%95%B0/)[95 评论](https://kexue.fm/archives/6705#comments)

< [端午&高考乱弹：怀念的，也许只是怀念本身](https://kexue.fm/archives/6704) \| [当Bert遇上Keras：这可能是Bert最简单的打开姿势](https://kexue.fm/archives/6736) >

### 你也许还对下面的内容感兴趣

- [MoE环游记：4、难处应当多投入](https://kexue.fm/archives/10815)
- [为什么梯度裁剪的默认模长是1？](https://kexue.fm/archives/10657)
- [从谱范数梯度到新式权重衰减的思考](https://kexue.fm/archives/10648)
- [从Hessian近似看自适应学习率优化器](https://kexue.fm/archives/10588)
- [Softmax后传：寻找Top-K的光滑近似](https://kexue.fm/archives/10373)
- [通向最优分布之路：概率空间的最小化](https://kexue.fm/archives/10289)
- [通向概率分布之路：盘点Softmax及其替代品](https://kexue.fm/archives/10145)
- [缓存与效果的极限拉扯：从MHA、MQA、GQA到MLA](https://kexue.fm/archives/10091)
- [用傅里叶级数拟合一维概率密度函数](https://kexue.fm/archives/10007)
- [旁门左道之如何让Python的重试代码更加优雅](https://kexue.fm/archives/9938)

[发表你的看法](https://kexue.fm/archives/6705#comment_form)

1. [«](https://kexue.fm/archives/6705/comment-page-3#comments)
2. [1](https://kexue.fm/archives/6705/comment-page-1#comments)
3. [2](https://kexue.fm/archives/6705/comment-page-2#comments)
4. [3](https://kexue.fm/archives/6705/comment-page-3#comments)
5. [4](https://kexue.fm/archives/6705/comment-page-4#comments)

[重参数 (Reparameterization)\_Johngo学长](https://www.johngo689.com/615227/)

June 15th, 2023

\[...\]苏剑林. (Jun. 10, 2019). 《漫谈重参数：从正态分布到 Gumbel Softmax 》\[Blog post\]. Retrieved from [https://kexue.fm/archives/6705](https://kexue.fm/archives/6705)\[...\]

[回复评论](https://kexue.fm/archives/6705/comment-page-4?replyTo=21995#respond-post-6705)

龙行

July 11th, 2023

苏神 这里如果变成了离散的，那应该是需要对隐变量Z做离散采样了吧?这个时候，在VAE中的隐变量Z是否还是假设为N(0,1)，KL散度又该咋算呀。看了您的代码，重构loss也从MSE变成了二元交叉熵，那这里的p(z\|x),q(z)是不是假设成了均匀分布？此时几何上，是不是由椭圆变成了方形，还是一维的线段？q(x\|z)假设为了伯努利分布，因为是黑白图像？不知道我想的对不对，但是这样好像才能对的上公式。

[回复评论](https://kexue.fm/archives/6705/comment-page-4?replyTo=22226#respond-post-6705)

[苏剑林](https://kexue.fm) 发表于
July 17th, 2023

重构loss可以是mse或者交叉熵，对应$q(x\|z)$是正态分布或者伯努利分布，这个不影响。

离散主要是假设$p(z\|x)$是离散分布，采样结果是one hot，KL散度直接根据定义算就行。

[回复评论](https://kexue.fm/archives/6705/comment-page-4?replyTo=22249#respond-post-6705)

龙行

July 11th, 2023

式15也还不太明白，这里的logpsita也需要再计算一遍导数，如果直接采样psita，也一样不能算出它对于sita的导数吧?这样整体的导数也算不出来吧？

[回复评论](https://kexue.fm/archives/6705/comment-page-4?replyTo=22227#respond-post-6705)

[苏剑林](https://kexue.fm) 发表于
July 17th, 2023

$$\\mathbb{E}\_{z\\sim p\_{\\theta}(z)}\\Big\[f(z)\\frac{\\partial}{\\partial\\theta} \\log p\_{\\theta}(z)\\Big\]$$

这个式子本来就已经是梯度，$z\\sim p\_{\\theta}(z)$只需要知道采样结果$z$，这一步不需要$z$或者$\\theta$的梯度，采样得到$z$后，代入$f(z)\\log p\_{\\theta}(z)$后，再求$p\_{\\theta}(z)$的梯度。

[回复评论](https://kexue.fm/archives/6705/comment-page-4?replyTo=22250#respond-post-6705)

JIAMING 发表于
February 27th, 2024

采样出来的$z$对$\\theta$的导数不可求，但是$p\_{\\theta}(z)$对于$\\theta$能求。我理解这里是在求后者。

[回复评论](https://kexue.fm/archives/6705/comment-page-4?replyTo=23790#respond-post-6705)

Boltzmachine

August 11th, 2023

请教一下，式（10）中用 o\_i 去替换 logp\_i时，式（7）分母中的求和号怎么没了呢，这一项并不等于0呀

[回复评论](https://kexue.fm/archives/6705/comment-page-4?replyTo=22479#respond-post-6705)

[苏剑林](https://kexue.fm) 发表于
August 11th, 2023

全体logits加上同一个常数，softmax的结果不变。

[回复评论](https://kexue.fm/archives/6705/comment-page-4?replyTo=22483#respond-post-6705)

zzzz 发表于
December 22nd, 2023

$p\_i = softmax(o\_i) = \\frac{e^{o\_i}}{\\sum\_{j}^{k}e^{o\_j}},$

$log(p\_i)=o\_i-\\sum\_{j=1}^{k}e^{o\_j}=o\_i-c,$

$softmax\[(log(p\_i)+g\_i)/\\tau\] = \\\

\\frac{e^{(log(p\_i)+g\_i)/\\tau}}{\\sum\_{j}^{k}e^{(log(p\_j)+g\_j)/\\tau}} =

\\frac{e^{(o\_i-c+g\_i)/\\tau}}{\\sum\_{j}^{k}e^{(o\_j-c+g\_j)/\\tau}} = \\frac{e^{(o\_i+g\_i)/\\tau}}{\\sum\_{j}^{k}e^{(o\_j+g\_j)/\\tau}} = softmax\[(o\_i+g\_i)/\\tau\]，\\\g\_i\\sim \\text{Gumbel}(0,1).$

[回复评论](https://kexue.fm/archives/6705/comment-page-4?replyTo=23330#respond-post-6705)

[苏剑林](https://kexue.fm) 发表于
December 26th, 2023

感谢。

[回复评论](https://kexue.fm/archives/6705/comment-page-4?replyTo=23350#respond-post-6705)

ZhenBo Wang

May 22nd, 2024

你好，首先感谢你的文章，但是我有一个问题，就是关于Gumbel-Softmax中的logits输入，用你文章中的公式表示应该就是指logPi，Pi指概率，但事实上，在我们使用Gumbel-Softmax时，logits直接用的就是模型的输出，并没有对其进行softmax获得概率，并接着使用log，这是为什么呢？？

[回复评论](https://kexue.fm/archives/6705/comment-page-4?replyTo=24399#respond-post-6705)

[苏剑林](https://kexue.fm) 发表于
May 24th, 2024

因为$\\log softmax(logits)$与$logits$之间只差一个常数，这个常数既不影响$\\mathop{\\arg\\max}$的结果，也不影响$softmax$的结果，所以不管是Gumbel Max还是Gumbel Softmax都可以直接用logits

[回复评论](https://kexue.fm/archives/6705/comment-page-4?replyTo=24422#respond-post-6705)

t.k.

June 5th, 2024

Gumbel Max的证明 里（12）式不等号是不是反了？

[回复评论](https://kexue.fm/archives/6705/comment-page-4?replyTo=24491#respond-post-6705)

[苏剑林](https://kexue.fm) 发表于
June 5th, 2024

检查了一下，似乎没有

[回复评论](https://kexue.fm/archives/6705/comment-page-4?replyTo=24500#respond-post-6705)

Lyu You 发表于
January 8th, 2025

$\\epsilon \\sim \\mathcal{U}\[0,1\]$， $\\ln \\epsilon$ 小于0

[回复评论](https://kexue.fm/archives/6705/comment-page-4?replyTo=26207#respond-post-6705)

zzq

July 4th, 2024

证明那里，公式(14)是不是少乘了一个$\\varepsilon$。

[回复评论](https://kexue.fm/archives/6705/comment-page-4?replyTo=24692#respond-post-6705)

[苏剑林](https://kexue.fm) 发表于
July 6th, 2024

似乎没有？

[回复评论](https://kexue.fm/archives/6705/comment-page-4?replyTo=24710#respond-post-6705)

zzq 发表于
July 13th, 2024

嗯我搞错了，现在的公式(14)，积分结果才是$p$，之前想的求平均一般不是是变量和概率相乘，然后对整个区间积分么。所以，这里说的求平均，却没有乘以变量，觉得弄错了。

[回复评论](https://kexue.fm/archives/6705/comment-page-4?replyTo=24814#respond-post-6705)

ai

September 8th, 2024

你好，我想问一下为什么不能直接使用下面的式子进行梯度估计呢？

$\\mathbb{E}\_{z\\sim p\_{\\theta}}\[f(z)\] \\approx \\sum\_{i=1}^{N}f(z\_i)p\_{\\theta}(z\_i) \\Rightarrow \\frac{\\partial}{\\partial \\theta}E \\approx \\sum\_{i=1}^{N}f(z\_i)\\frac{\\partial}{\\partial}p\_{\\theta}(z\_i)$

[回复评论](https://kexue.fm/archives/6705/comment-page-4?replyTo=25175#respond-post-6705)

[苏剑林](https://kexue.fm) 发表于
September 9th, 2024

你这样是数值积分的思想吧，问题是$z\_i$取什么呢？如果取固定值，那怎么也做不到在数学期望下等于精确值吧？此外，很显然它跟SF估计一样存在方差大的问题。

[回复评论](https://kexue.fm/archives/6705/comment-page-4?replyTo=25190#respond-post-6705)

1. [«](https://kexue.fm/archives/6705/comment-page-3#comments)
2. [1](https://kexue.fm/archives/6705/comment-page-1#comments)
3. [2](https://kexue.fm/archives/6705/comment-page-2#comments)
4. [3](https://kexue.fm/archives/6705/comment-page-3#comments)
5. [4](https://kexue.fm/archives/6705/comment-page-4#comments)

[取消回复](https://kexue.fm/archives/6705#respond-post-6705)

你的大名

电子邮箱

个人网站（选填）

1\. 可以使用LaTeX代码，点击“预览效果”可查看效果；

2\. 可以通过点击评论楼层编号来引用该楼层；

3\. 网站可能会有点卡，如非确认评论失败，请不要重复点击提交。

### 内容速览

[基本概念](https://kexue.fm/archives/6705#%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5)
[连续情形](https://kexue.fm/archives/6705#%E8%BF%9E%E7%BB%AD%E6%83%85%E5%BD%A2)
[形式](https://kexue.fm/archives/6705#%E5%BD%A2%E5%BC%8F)
[例子](https://kexue.fm/archives/6705#%E4%BE%8B%E5%AD%90)
[总结](https://kexue.fm/archives/6705#%E6%80%BB%E7%BB%93)
[离散情形](https://kexue.fm/archives/6705#%E7%A6%BB%E6%95%A3%E6%83%85%E5%BD%A2)
[分析](https://kexue.fm/archives/6705#%E5%88%86%E6%9E%90)
[形式](https://kexue.fm/archives/6705#%E5%BD%A2%E5%BC%8F)
[例子](https://kexue.fm/archives/6705#%E4%BE%8B%E5%AD%90)
[溯源](https://kexue.fm/archives/6705#%E6%BA%AF%E6%BA%90)
[总结](https://kexue.fm/archives/6705#%E6%80%BB%E7%BB%93)
[背后的故事](https://kexue.fm/archives/6705#%E8%83%8C%E5%90%8E%E7%9A%84%E6%95%85%E4%BA%8B)
[SF估计](https://kexue.fm/archives/6705#SF%E4%BC%B0%E8%AE%A1)
[梯度方差](https://kexue.fm/archives/6705#%E6%A2%AF%E5%BA%A6%E6%96%B9%E5%B7%AE)
[降方差](https://kexue.fm/archives/6705#%E9%99%8D%E6%96%B9%E5%B7%AE)
[文章小结](https://kexue.fm/archives/6705#%E6%96%87%E7%AB%A0%E5%B0%8F%E7%BB%93)

### 智能搜索

支持整句搜索！网站自动使用 [结巴分词](https://github.com/fxsjy/jieba) 进行分词，并结合ngrams排序算法给出合理的搜索结果。

### 热门标签

[生成模型](https://kexue.fm/tag/%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B/) [attention](https://kexue.fm/tag/attention/) [模型](https://kexue.fm/tag/%E6%A8%A1%E5%9E%8B/) [语言模型](https://kexue.fm/tag/%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/) [优化](https://kexue.fm/tag/%E4%BC%98%E5%8C%96/) [网站](https://kexue.fm/tag/%E7%BD%91%E7%AB%99/) [概率](https://kexue.fm/tag/%E6%A6%82%E7%8E%87/) [梯度](https://kexue.fm/tag/%E6%A2%AF%E5%BA%A6/) [转载](https://kexue.fm/tag/%E8%BD%AC%E8%BD%BD/) [微分方程](https://kexue.fm/tag/%E5%BE%AE%E5%88%86%E6%96%B9%E7%A8%8B/) [天象](https://kexue.fm/tag/%E5%A4%A9%E8%B1%A1/) [矩阵](https://kexue.fm/tag/%E7%9F%A9%E9%98%B5/) [深度学习](https://kexue.fm/tag/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/) [分析](https://kexue.fm/tag/%E5%88%86%E6%9E%90/) [积分](https://kexue.fm/tag/%E7%A7%AF%E5%88%86/) [python](https://kexue.fm/tag/python/) [力学](https://kexue.fm/tag/%E5%8A%9B%E5%AD%A6/) [无监督](https://kexue.fm/tag/%E6%97%A0%E7%9B%91%E7%9D%A3/) [几何](https://kexue.fm/tag/%E5%87%A0%E4%BD%95/) [优化器](https://kexue.fm/tag/%E4%BC%98%E5%8C%96%E5%99%A8/) [扩散](https://kexue.fm/tag/%E6%89%A9%E6%95%A3/) [节日](https://kexue.fm/tag/%E8%8A%82%E6%97%A5/) [生活](https://kexue.fm/tag/%E7%94%9F%E6%B4%BB/) [文本生成](https://kexue.fm/tag/%E6%96%87%E6%9C%AC%E7%94%9F%E6%88%90/) [数论](https://kexue.fm/tag/%E6%95%B0%E8%AE%BA/)

### 随机文章

- [今天我们都是舟曲人——举国哀悼舟曲遇难同胞](https://kexue.fm/archives/857)
- [【外微分浅谈】2\. 反对称的威力](https://kexue.fm/archives/4054)
- [浅谈引力助推](https://kexue.fm/archives/1953)
- [能量视角下的GAN模型（一）：GAN＝“挖坑”＋“跳坑”](https://kexue.fm/archives/6316)
- [【NASA每日一图】武汉上空的日食“钻戒”](https://kexue.fm/archives/66)
- [解答不等式的误区...](https://kexue.fm/archives/644)
- [用ALBERT和ELECTRA之前，请确认你真的了解它们](https://kexue.fm/archives/7846)
- [泛化性乱弹：从随机噪声、梯度惩罚到虚拟对抗训练](https://kexue.fm/archives/7466)
- [网友:椭圆定长弦中点轨迹的一种解法](https://kexue.fm/archives/1898)
- [让风筝飞](https://kexue.fm/archives/2891)

### 最近评论

- [长琴](https://kexue.fm/archives/10862/comment-page-1#comment-27503): mla出来之后就知道苏神一定会念念不忘。hah。没注意到公众号不更了，才发现……我说咋好久不见...
- [Suahi](https://kexue.fm/archives/5239/comment-page-3#comment-27502): 谢谢苏老师的回复！
我也是在从MLE出发推导VAE的Loss函数这个过程中产生了这些疑问，所以...
- [lay](https://kexue.fm/archives/10657/comment-page-1#comment-27500): 苏神，我在训练一个1.5B模型的过程中在某一步grad\_norm会有很大的spike(比如好几...
- [hazdzz](https://kexue.fm/archives/10862/comment-page-1#comment-27499): 如果 RoPE 结合 Givens rotation method 的变体，Transform...
- [autumn23333](https://kexue.fm/archives/10862/comment-page-1#comment-27498): 请问一下论文附带的代码是不是写错了fourier\_sin = F. pad ( input =...
- [SunlightZero](https://kexue.fm/archives/9164/comment-page-4#comment-27497): 在《Step-by-Step Diffusion: An Elementary Tutoria...
- [苏剑林](https://kexue.fm/archives/5239/comment-page-3#comment-27496): 1、明白了，我将$q\_{\\phi}(z\|x)$看成$q\_{\\phi}(x\|z)$了（ELBO厌...
- [Suahi](https://kexue.fm/archives/5239/comment-page-3#comment-27493): 谢谢苏老师的回复！1\. 首先回复您为什么ELBO不带KL，并不是最终损失函数形式，需要做如下变...
- [eular](https://kexue.fm/archives/10373/comment-page-1#comment-27492): 是的，当$k$比较大时会出现这种情况。
- [苏剑林](https://kexue.fm/archives/5239/comment-page-3#comment-27491): 肯定是$\\mathbb{E}\_{x \\sim p\_{data}(x)}\[\\log(p\_{\\th...

### 友情链接

- [Cool Papers](https://papers.cool)
- [数学研发](https://bbs.emath.ac.cn)
- [Seatop](http://www.seatop.com.cn/)
- [Xiaoxia](https://xiaoxia.org/)
- [积分表-网络版](https://kexue.fm/sci/integral/index.html)
- [丝路博傲](http://blog.dvxj.com/)
- [ph4ntasy 饭特稀](http://www.ph4ntasy.com/)
- [数学之家](http://www.2math.cn/)
- [有趣天文奇观](http://interesting-sky.china-vo.org/)
- [TwistedW](http://www.twistedwg.com/)
- [godweiyang](https://godweiyang.com/)
- [AI柠檬](https://blog.ailemon.net/)
- [王登科-DK博客](https://greatdk.com)
- [ESON](https://blog.eson.org/)
- [枫之羽](https://fzhiy.net/)
- [Mathor's blog](https://wmathor.com/)
- [coding-zuo](https://coding-zuo.github.io/)
- [博科园](https://www.bokeyuan.net/)
- [孔皮皮的博客](https://www.kppkkp.top/)
- [运鹏的博客](https://yunpengtai.top/)
- [jiming.site](https://jiming.site/)
- [OmegaXYZ](https://www.omegaxyz.com/)
- [Blog by Eacls](https://www.eacls.top/)
- [EAI猩球](https://www.robotech.ink/)
- [文举的博客](https://liwenju0.com/)
- [申请链接](https://kexue.fm/links.html)

[![署名-非商业用途-保持一致](https://kexue.fm/usr/themes/geekg/images/cc.gif)](http://creativecommons.org/licenses/by-nc-nd/2.5/cn/) 本站采用创作共用版权协议，要求署名、非商业用途和保持一致。转载本站内容必须也遵循“ [署名-非商业用途-保持一致](http://creativecommons.org/licenses/by-nc-nd/2.5/cn/)”的创作共用协议。

© 2009-2025 Scientific Spaces. All rights reserved. Theme by [laogui](http://www.laogui.com). Powered by [Typecho](http://typecho.org). 备案号: [粤ICP备09093259号-1/2](https://beian.miit.gov.cn/)。