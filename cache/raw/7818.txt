![MobileSideBar](https://kexue.fm/usr/themes/geekg/images/slide-button.png)

## SEARCH

## MENU

- [打赏](https://kexue.fm/reward.html)
- [公式](https://kexue.fm/latex.html)
- [天象](https://kexue.fm/ac.html)
- [链接](https://kexue.fm/links.html)
- [时光](https://kexue.fm/me.html)
- [博览](https://kexue.fm/science.html)
- [归档](https://kexue.fm/content.html)

## CATEGORIES

- [千奇百怪](https://kexue.fm/category/Everything)
- [天文探索](https://kexue.fm/category/Astronomy)
- [数学研究](https://kexue.fm/category/Mathematics)
- [物理化学](https://kexue.fm/category/Phy-chem)
- [信息时代](https://kexue.fm/category/Big-Data)
- [生物自然](https://kexue.fm/category/Biology)
- [图片摄影](https://kexue.fm/category/Photograph)
- [问题百科](https://kexue.fm/category/Questions)
- [生活/情感](https://kexue.fm/category/Life-Feeling)
- [资源共享](https://kexue.fm/category/Resources)

## NEWPOSTS

- [SVD的导数](https://kexue.fm/archives/10878)
- [智能家居之手搓一套能接入米家的零冷水装置](https://kexue.fm/archives/10869)
- [Transformer升级之路：1...](https://kexue.fm/archives/10862)
- [矩阵的有效秩（Effective ...](https://kexue.fm/archives/10847)
- [通过梯度近似寻找Normaliza...](https://kexue.fm/archives/10831)
- [MoE环游记：4、难处应当多投入](https://kexue.fm/archives/10815)
- [高阶muP：更简明但更高明的谱条件缩放](https://kexue.fm/archives/10795)
- [初探muP：超参数的跨模型尺度迁移规律](https://kexue.fm/archives/10770)
- [MoE环游记：3、换个思路来分配](https://kexue.fm/archives/10757)
- [Muon续集：为什么我们选择尝试M...](https://kexue.fm/archives/10739)

## COMMENTS

- [SunlightZero: 在《Step-by-Step Diffusion: An El...](https://kexue.fm/archives/9164/comment-page-4#comment-27497)
- [苏剑林: 1、明白了，我将$q\_{\\phi}(z\|x)$看成$q\_{\\p...](https://kexue.fm/archives/5239/comment-page-3#comment-27496)
- [Suahi: 谢谢苏老师的回复！1\. 首先回复您为什么ELBO不带KL，并不...](https://kexue.fm/archives/5239/comment-page-3#comment-27493)
- [eular: 是的，当$k$比较大时会出现这种情况。](https://kexue.fm/archives/10373/comment-page-1#comment-27492)
- [苏剑林: 肯定是$\\mathbb{E}\_{x \\sim p\_{data}...](https://kexue.fm/archives/5239/comment-page-3#comment-27491)
- [苏剑林: 你的意思是$\\lambda(\\boldsymbol{x}) <...](https://kexue.fm/archives/10373/comment-page-1#comment-27490)
- [苏剑林: 好问题，下一篇文章可能会讨论这个问题](https://kexue.fm/archives/10735/comment-page-1#comment-27489)
- [苏剑林: 不大熟悉，但都diffusion forcing了，还有必要CM吗](https://kexue.fm/archives/10633/comment-page-1#comment-27488)
- [苏剑林: 简单看了一下，好像没什么新东西呀？还是我看漏了什么？](https://kexue.fm/archives/10711/comment-page-2#comment-27487)
- [苏剑林: 感谢指出，已修正。](https://kexue.fm/archives/8601/comment-page-1#comment-27486)

## USERLOGIN

- [登录](https://kexue.fm/admin/login.php)

[科学空间\|Scientific Spaces](https://kexue.fm)

- [登录](https://kexue.fm/admin/login.php)
- [打赏](https://kexue.fm/reward.html)
- [公式](https://kexue.fm/latex.html)
- [天象](https://kexue.fm/ac.html)
- [链接](https://kexue.fm/links.html)
- [时光](https://kexue.fm/me.html)
- [博览](https://kexue.fm/science.html)
- [归档](https://kexue.fm/content.html)

渴望成为一个小飞侠

- [![](https://kexue.fm/usr/themes/geekg/images/rss.png)\
\
欢迎订阅](https://kexue.fm/feed)
- [![](https://kexue.fm/usr/themes/geekg/images/mail.png)\
\
个性邮箱](https://kexue.fm/archives/119)
- [![](https://kexue.fm/usr/themes/geekg/images/Saturn.png)\
\
天象信息](https://kexue.fm/ac.html)
- [![](https://kexue.fm/usr/themes/geekg/images/iss.png)\
\
观测ISS](https://kexue.fm/archives/41)
- [![](https://kexue.fm/usr/themes/geekg/images/pi.png)\
\
LaTeX](https://kexue.fm/latex.html)
- [![](https://kexue.fm/usr/themes/geekg/images/mlogo.png)\
\
关于博主](https://kexue.fm/me.html)

欢迎访问“科学空间”，这里将与您共同探讨自然科学，回味人生百态；也期待大家的分享～

- [**千奇百怪** Everything](https://kexue.fm/category/Everything)
- [**天文探索** Astronomy](https://kexue.fm/category/Astronomy)
- [**数学研究** Mathematics](https://kexue.fm/category/Mathematics)
- [**物理化学** Phy-chem](https://kexue.fm/category/Phy-chem)
- [**信息时代** Big-Data](https://kexue.fm/category/Big-Data)
- [**生物自然** Biology](https://kexue.fm/category/Biology)
- [**图片摄影** Photograph](https://kexue.fm/category/Photograph)
- [**问题百科** Questions](https://kexue.fm/category/Questions)
- [**生活/情感** Life-Feeling](https://kexue.fm/category/Life-Feeling)
- [**资源共享** Resources](https://kexue.fm/category/Resources)

- [**千奇百怪**](https://kexue.fm/category/Everything)
- [**天文探索**](https://kexue.fm/category/Astronomy)
- [**数学研究**](https://kexue.fm/category/Mathematics)
- [**物理化学**](https://kexue.fm/category/Phy-chem)
- [**信息时代**](https://kexue.fm/category/Big-Data)
- [**生物自然**](https://kexue.fm/category/Biology)
- [**图片摄影**](https://kexue.fm/category/Photograph)
- [**问题百科**](https://kexue.fm/category/Questions)
- [**生活/情感**](https://kexue.fm/category/Life-Feeling)
- [**资源共享**](https://kexue.fm/category/Resources)

[首页](https://kexue.fm) [信息时代](https://kexue.fm/category/Big-Data) TeaForN：让Teacher Forcing更有“远见”一些

27Oct

# [TeaForN：让Teacher Forcing更有“远见”一些](https://kexue.fm/archives/7818)

By 苏剑林 \|
2020-10-27 \|
46600位读者\|

Teacher Forcing是Seq2Seq模型的经典训练方式，而Exposure Bias则是Teacher Forcing的经典缺陷，这对于搞文本生成的同学来说应该是耳熟能详的事实了。笔者之前也曾写过博文 [《Seq2Seq中Exposure Bias现象的浅析与对策》](https://kexue.fm/archives/7259)，初步地分析过Exposure Bias问题。

本文则介绍Google新提出的一种名为“ **TeaForN**”的缓解Exposure Bias现象的方案，来自论文 [《TeaForN: Teacher-Forcing with N-grams》](https://papers.cool/arxiv/2010.03494)，它通过嵌套迭代的方式，让模型能提前预估到后$N$个token（而不仅仅是当前要预测的token），其处理思路上颇有可圈可点之处，值得我们学习。

（注：为了尽量跟本博客旧文章保持一致，本文的记号与原论文的记号有所不同，请大家以理解符号含义为主，不要强记符号形式。）

## Teacher Forcing [\#](https://kexue.fm/archives/7818\#Teacher%20Forcing)

文章 [《Seq2Seq中Exposure Bias现象的浅析与对策》](https://kexue.fm/archives/7259) 已经相对详细地介绍了Teacher Forcing，这里仅做简要回顾。首先，Seq2Seq模型将联合概率分解为多个条件概率的乘积，这就是所谓的“自回归模型”：

\\begin{equation}\\begin{aligned}p(\\boldsymbol{y}\|\\boldsymbol{x})=&\\,p(y\_1,y\_2,\\dots,y\_n\|\\boldsymbol{x})\\\

=&\\,p(y\_1\|\\boldsymbol{x})p(y\_2\|\\boldsymbol{x},y\_1)\\dots p(y\_n\|\\boldsymbol{x},y\_1,\\dots,y\_{n-1})

\\end{aligned}\\end{equation}

然后，当我们训练第$t$步的模型$\\dots p(y\_t\|\\boldsymbol{x},y\_1,\\dots,y\_{t-1})$时，我们假设$\\boldsymbol{x},y\_1,\\dots,y\_{t-1}$都是已知的，然后让模型只预测$y\_t$，这就是Teacher Forcing。但在预测阶段，真实的$y\_1,\\dots,y\_{t-1}$都是未知的，此时它们是递归地预测出来的，可能会存在传递误差等情况。因此Teacher Forcing的问题就是训练和预测存在不一致性，这让我们很难从训练过程掌握预测的效果。

## 没什么远见 [\#](https://kexue.fm/archives/7818\#%E6%B2%A1%E4%BB%80%E4%B9%88%E8%BF%9C%E8%A7%81)

怎么更具体理解这个不一致性所带来的问题呢？我们可以将它理解“没什么远见”。在解码器中，输入$\\boldsymbol{x}$和前$t-1$个输出token共同编码得到向量$h\_t$，在Teacher Forcing中，这个$h\_t$只是用来预测$y\_t$，跟$\\boldsymbol{y}\_{> t}$没有直接联系，换句话说，它的“见识”也就局限在$t$这一步了。

[![Teacher Forcing示意图](https://kexue.fm/usr/uploads/2020/10/3788343598.png)](https://kexue.fm/usr/uploads/2020/10/3788343598.png)

Teacher Forcing示意图

比如上图中的$h\_3$向量，Teacher Forcing只让它用来预测“阴”，事实上“阴”的预测结果也会影响“晴”、“圆”、“缺”的预测，也就是说$h\_3$也应该与“晴”、“圆”、“缺”有所关联，而Teacher Forcing没有显式地建立这种关联。所以模型在解码的时候每一步很可能只输出局部最高概率的token，这就容易出现高频安全回复或者重复解码现象。

## Student Forcing [\#](https://kexue.fm/archives/7818\#Student%20Forcing)

为了提高模型的“前瞻能力”，最彻底的方法当然是训练阶段也按照解码的方式来进行，即$h\_1,h\_2,\\dots,h\_t$也像解码阶段一样递归地预测出来，不依赖于真实标签，我们不妨称这种方式为Student Forcing。但是，Student Forcing的训练方式来带来两个严重的问题：

> 第一， **牺牲并行**。对于Teacher Forcing来说，如果Decoder使用的是CNN或Transformer这样的结构，那么训练阶段是所有token都可以并行训练的（预测阶段还是串行），但如果Student Forcing的话则一直都是串行。
>
> 第二， **极难收敛**。Student Forcing通常需要用Gumbel Softmax或强化学习来回传梯度，它们的训练都面临着严重的不稳定性，一般都要用Teacher Forcing预训练后才能用Student Forcing，但即便如此也不算特别稳定。

形象地理解，Student Forcing相当于老师完全让学生独立探究一个复杂的问题，不做手把手教学，只对学生的结果好坏做个最终评价。这样一旦学生能探索成功，那可能说明学生的能力很强了，但问题就是缺乏老师的“循循善诱”，学生“碰壁”的几率更加大。

## 往前多看几步 [\#](https://kexue.fm/archives/7818\#%E5%BE%80%E5%89%8D%E5%A4%9A%E7%9C%8B%E5%87%A0%E6%AD%A5)

有没有介乎Teacher Forcing与Student Forcing之间的方法呢？有，本文所介绍的TeaForN就算是其中一种，它的思想是常规的Teacher Forcing相当于在训练的时候只往前看1步，而Student Forcing相当于在训练的时候往前看了$L$步（$L$是目标句子长度），如果我们只是往前多看几步（相当于看到了N-gram），那么理论上就能提高“远见”，并且不至于严重牺牲模型的并行性。其示意图如下：

[![TeaForN示意图](https://kexue.fm/usr/uploads/2020/10/1985218481.png)](https://kexue.fm/usr/uploads/2020/10/1985218481.png)

TeaForN示意图

直观来看，就是把输出结果再往前迭代多遍，这样一来前$t-1$个token要预测的就不仅仅是第$t$个token了，还有第$t+1,t+2,\\cdots$个。比如在上图中，最后我们用$h\_6^{(3)}$来预测了“缺”字，而我们可以看到$h\_6^{(3)}$只依赖于“月”、“有”、“阴”三个字，所以我们也可以理解为$h\_4^{(1)}$这个向量同时要预测“晴”、“圆”、“缺”三个字，因此也就提高了“远见”。

## 用数学的话来说 [\#](https://kexue.fm/archives/7818\#%E7%94%A8%E6%95%B0%E5%AD%A6%E7%9A%84%E8%AF%9D%E6%9D%A5%E8%AF%B4)

用数学语言来描述，我们可以将Decoder分为Embedding层$E$和剩余部分$M$两个部分，Embedding层负责将输入句子$s=\[w\_0, w\_1, w\_2, \\cdots, w\_{L-1}\]$映射为向量序列$\[e\_0, e\_1, e\_2, \\cdots, e\_{L-1}\]$（其中$w\_0$是固定的解码起始标记，也就是上图的\[S\]，有些文章记为 ），然后交给模型$M$处理，得到向量序列$\[h\_1, h\_2, h\_3, \\cdots, h\_L\]$，即

\\begin{equation}\[h\_1, h\_2, h\_3, \\cdots, h\_L\] = M(E(\[w\_0, w\_1, w\_2, \\cdots, w\_{L-1}\]))\\end{equation}

接着通过$p\_t = softmax(Wh\_t + b)$得到第$t$步的token概率分布，最后用$-\\log p\_t\[w\_t\]$作为损失函数训练，这便是常规的Teacher Forcing。

可以想象，负责映射到token分布的输出向量序列$\[h\_1, h\_2, h\_3, \\cdots, h\_{L-1}\]$某种程度上跟Embedding序列$\[e\_1, e\_2, e\_3, \\cdots, e\_{L-1}\]$是相似的，如果我们补充一个$e\_0$进去，然后将$\[e\_0, h\_1, h\_2, \\cdots, h\_{L-1}\]$也送入到模型$M$中再处理一次，是否可以呢？也就是

\\begin{equation}\\begin{aligned}\[\]

\\left\[e\_0,e\_1,e\_2,\\cdots,e\_{L-1}\\right\]& = E\\left(\\left\[w\_0, w\_1,w\_2,\\cdots,w\_{L-1}\\right\]\\right)\\\

\\left\[h\_1^{(1)},h\_2^{(1)},h\_3^{(1)},\\cdots,h\_L^{(1)}\\right\]& = M\\left(\\left\[e\_0,e\_1,e\_2,\\cdots,e\_{L-1}\\right\]\\right)\\\

\\left\[h\_1^{(2)},h\_2^{(2)},h\_3^{(2)},\\cdots,h\_L^{(2)}\\right\]& = M\\left(\\left\[e\_0, h\_1^{(1)},h\_2^{(1)},\\cdots,h\_{L-1}^{(1)}\\right\]\\right)\\\

\\left\[h\_1^{(3)},h\_2^{(3)},h\_3^{(3)},\\cdots,h\_L^{(3)}\\right\]& = M\\left(\\left\[e\_0, h\_1^{(2)},h\_2^{(2)},\\cdots,h\_{L-1}^{(2)}\\right\]\\right)\\\

&\\,\\,\\vdots

\\end{aligned}\\end{equation}

然后每一个$h$我们都算概率分布$p\_t^{(i)} = softmax(Wh\_t^{(i)} + b)$，最后算交叉熵并加权叠加

\\begin{equation}\\text{loss} = -\\sum\_{t=1}^L \\sum\_{i=1}^N \\lambda\_i \\log p\_t^{(i)}\[w\_t\]\\end{equation}

训练完成后，我们只用$E$和$M$做常规的解码操作（比如Beam Search），也就是只用$h\_t^{(1)}$而不需要$h\_t^{(2)},h\_t^{(3)},\\cdots$了。这个流程就是本文的主角TeaForN了。

## 效果、思考与讨论 [\#](https://kexue.fm/archives/7818\#%E6%95%88%E6%9E%9C%E3%80%81%E6%80%9D%E8%80%83%E4%B8%8E%E8%AE%A8%E8%AE%BA)

至于实验效果，自然是有提升的，从原论文的实验表格来看，在beam\_size比较大时提升比较明显。其实也不难理解，按道理来说，这样处理后再不济应该也不会下降，因此算是一种“稳赚不赔”的策略了。

[![TeaForN的实验结果之一（文本摘要）](https://kexue.fm/usr/uploads/2020/10/585091705.png)](https://kexue.fm/usr/uploads/2020/10/585091705.png)

TeaForN的实验结果之一（文本摘要）

原论文讨论了几个值得商榷的点，我们这里也来看下。

首先，模型每一步迭代所用的$M$该不该共享权重？直觉来想共享是更好的，如果不共享权重，那么往前看$N$步，那么参数量就差不多是原来的$N$倍了，感觉是不大好。当然最好还是靠实验实验，原论文确实做了这个比较，证实了我们的直觉。

[![TeaForN在机器翻译上的效果，其中包含了是否贡献权重的比较](https://kexue.fm/usr/uploads/2020/10/3931798100.png)](https://kexue.fm/usr/uploads/2020/10/3931798100.png)

TeaForN在机器翻译上的效果，其中包含了是否贡献权重的比较

其次，可能最主要的疑问是：在迭代过程中将$\[h\_1, h\_2, h\_3, \\cdots, h\_{L-1}\]$当作$\[e\_1, e\_2, e\_3, \\cdots, e\_{L-1}\]$用是否真的靠谱？当然，实验结果已经表明了是可行的，这就是最有说服力的论据了。但由于$h\_t$到$p\_t$是通过内积来构建的，所以$h\_t$跟$e\_t$未必相似，如果能让它们更接近些，效果会不会更好？原论文考虑了如下的方式：

\\begin{equation}\\frac{\\sum\\limits\_{w\\in \\text{Top}\_k(p\_t)}p\_t\[w\] e\_w}{\\sum\\limits\_{w\\in \\text{Top}\_k(p\_t)}p\_t\[w\]}\\end{equation}

也就是说，每一步算出$p\_t$后，取概率最大的$k$个token，将它们的Embedding向量加权平均来作为下一步迭代的输入。原论文实验了$k=4$和$k=\|V\|$（词表大小），结果如下图。总的来说Topk的效果不大稳定，好的情况也跟直接用$h\_t$差不多，因此就没必要尝试别的了。

[![用Topk对Embedding加权平均的方式代替h的效果](https://kexue.fm/usr/uploads/2020/10/4010108602.png)](https://kexue.fm/usr/uploads/2020/10/4010108602.png)

用Topk对Embedding加权平均的方式代替h的效果

当然，我觉得要是论文再比较一下通过Gumbel Softmax来模拟采样效果就更加完美了。

## 来自文末的总结 [\#](https://kexue.fm/archives/7818\#%E6%9D%A5%E8%87%AA%E6%96%87%E6%9C%AB%E7%9A%84%E6%80%BB%E7%BB%93)

本文分享了Google新提出来一种称为TeaForN的训练方式，它介乎Teacher Forcing和Student Forcing之间，能缓解模型的Exposure Bias问题，并且不用严重牺牲模型训练的并行性，是一种值得尝试的策略。除此之外，它实际上还提供了一种解决此类问题的新思想（通过迭代保持并行和前瞻），其中颇有值得回味的地方。

_**转载到请包括本文地址：** [https://kexue.fm/archives/7818](https://kexue.fm/archives/7818)_

_**更详细的转载事宜请参考：**_ [《科学空间FAQ》](https://kexue.fm/archives/6508#%E6%96%87%E7%AB%A0%E5%A6%82%E4%BD%95%E8%BD%AC%E8%BD%BD/%E5%BC%95%E7%94%A8)

**如果您还有什么疑惑或建议，欢迎在下方评论区继续讨论。**

**如果您觉得本文还不错，欢迎 [分享](https://kexue.fm/archives/7818#share)/ [打赏](https://kexue.fm/archives/7818#pay) 本文。打赏并非要从中获得收益，而是希望知道科学空间获得了多少读者的真心关注。当然，如果你无视它，也不会影响你的阅读。再次表示欢迎和感谢！**

打赏

![科学空间](https://kexue.fm/usr/themes/geekg/payment/wx.png)

微信打赏

![科学空间](https://kexue.fm/usr/themes/geekg/payment/zfb.png)

支付宝打赏

因为网站后台对打赏并无记录，因此欢迎在打赏时候备注留言。

你还可以 [**点击这里**](http://mail.qq.com/cgi-bin/qm_share?t=qm_mailme&email=tN7d1drY3drrx8H0xcWa19vZ) 或在下方评论区留言来告知你的建议或需求。

**如果您需要引用本文，请参考：**

苏剑林. (Oct. 27, 2020). 《TeaForN：让Teacher Forcing更有“远见”一些 》\[Blog post\]. Retrieved from [https://kexue.fm/archives/7818](https://kexue.fm/archives/7818)

@online{kexuefm-7818,

        title={TeaForN：让Teacher Forcing更有“远见”一些},

        author={苏剑林},

        year={2020},

        month={Oct},

        url={\\url{https://kexue.fm/archives/7818}},

}

分类： [信息时代](https://kexue.fm/category/Big-Data)    标签： [优化](https://kexue.fm/tag/%E4%BC%98%E5%8C%96/), [语言模型](https://kexue.fm/tag/%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/), [文本生成](https://kexue.fm/tag/%E6%96%87%E6%9C%AC%E7%94%9F%E6%88%90/)[9 评论](https://kexue.fm/archives/7818#comments)

< [BERT可以上几年级了？Seq2Seq“硬刚”小学数学应用题](https://kexue.fm/archives/7809) \| [用ALBERT和ELECTRA之前，请确认你真的了解它们](https://kexue.fm/archives/7846) >

### 你也许还对下面的内容感兴趣

- [Transformer升级之路：19、第二类旋转位置编码](https://kexue.fm/archives/10862)
- [MoE环游记：4、难处应当多投入](https://kexue.fm/archives/10815)
- [为什么梯度裁剪的默认模长是1？](https://kexue.fm/archives/10657)
- [从谱范数梯度到新式权重衰减的思考](https://kexue.fm/archives/10648)
- [从Hessian近似看自适应学习率优化器](https://kexue.fm/archives/10588)
- [Decoder-only的LLM为什么需要位置编码？](https://kexue.fm/archives/10347)
- [通向最优分布之路：概率空间的最小化](https://kexue.fm/archives/10289)
- [Monarch矩阵：计算高效的稀疏型矩阵分解](https://kexue.fm/archives/10249)
- [缓存与效果的极限拉扯：从MHA、MQA、GQA到MLA](https://kexue.fm/archives/10091)
- [时空之章：将Attention视为平方复杂度的RNN](https://kexue.fm/archives/10017)

[发表你的看法](https://kexue.fm/archives/7818#comment_form)

肖子洋

October 30th, 2020

苏神，我读了原论文，发现原论文的说法是第s个Decoder预测的标签是y(s+1),...,y(t+s)，给出的Ground Truth是Go,y(1),...,y(t-1)。这样的话Decoder的预测内容会不会超出EOS之后的内容呢？比如Decoder 1负责预测y(2),...,y(t+1)，但y(t+1)已经不存在了呀，这一点会不会是论文的笔误呢？

[回复评论](https://kexue.fm/archives/7818/comment-page-1?replyTo=14672#respond-post-7818)

[苏剑林](https://kexue.fm) 发表于
October 31st, 2020

超过长度自然就不会预测了啊...可能人家觉得这是显然成立的处理方式，不用特别说明吧。

[回复评论](https://kexue.fm/archives/7818/comment-page-1?replyTo=14673#respond-post-7818)

肖子洋 发表于
October 31st, 2020

噢噢，那明白了

[回复评论](https://kexue.fm/archives/7818/comment-page-1?replyTo=14675#respond-post-7818)

smzhang

May 19th, 2022

苏神，TeaFor2的情况是不是本质上就是scheduled sampling？

[回复评论](https://kexue.fm/archives/7818/comment-page-1?replyTo=19150#respond-post-7818)

[苏剑林](https://kexue.fm) 发表于
May 20th, 2022

第一次听说scheduled sampling，看了看是有点像，但不完全是。

[回复评论](https://kexue.fm/archives/7818/comment-page-1?replyTo=19159#respond-post-7818)

smzhang

September 20th, 2022

苏神，再请教您两个问题：

首先您文章里提到的student-forcing的训练方法是不是主要指MIXER、MRT这类序列级优化方法？如果直接在student-forcing的模式下仍然用token级的MLE进行监督呢，直觉上会有什么不同吗（在不考虑Transformer并行性降低的角度下）？

另外模型的有“远见”这一性质，会使其在预测的概率分布上相较于普通MLE训练得到的模型在什么方面有差异呢，比如候选词的排序？

十分感谢！

[回复评论](https://kexue.fm/archives/7818/comment-page-1?replyTo=19852#respond-post-7818)

[苏剑林](https://kexue.fm) 发表于
September 22nd, 2022

我没听说过“MIXER、MRT这类序列级优化方法”，不知道它们是啥。student-forcing就是按照解码方式如beam\_search、greedy\_search或者random\_sample得到结果序列后，再与标签序列进行对比算loss。所以student-forcing就是序列级的，没有token级的做法。

[回复评论](https://kexue.fm/archives/7818/comment-page-1?replyTo=19863#respond-post-7818)

smzhang 发表于
September 22nd, 2022

感谢您的解答，那请问“与标签序列进行对比算loss”具体是怎么实现的呢？或者有相关的参考文献介绍这类方法嘛？

[回复评论](https://kexue.fm/archives/7818/comment-page-1?replyTo=19872#respond-post-7818)

[苏剑林](https://kexue.fm) 发表于
September 23rd, 2022

“再与标签序列进行对比算loss”就是交叉熵，很简单，难在“按照解码方式如beam\_search、greedy\_search或者random\_sample得到结果序列”，这需要以RNN方式实现解码模型，其中还要用到gumbel softmax或者reinforce。

相关工作可以参考 https://arxiv.org/abs/1906.02448 等。

[回复评论](https://kexue.fm/archives/7818/comment-page-1?replyTo=19881#respond-post-7818)

[取消回复](https://kexue.fm/archives/7818#respond-post-7818)

你的大名

电子邮箱

个人网站（选填）

1\. 可以使用LaTeX代码，点击“预览效果”可查看效果；

2\. 可以通过点击评论楼层编号来引用该楼层；

3\. 网站可能会有点卡，如非确认评论失败，请不要重复点击提交。

### 内容速览

[Teacher Forcing](https://kexue.fm/archives/7818#Teacher%20Forcing)
[没什么远见](https://kexue.fm/archives/7818#%E6%B2%A1%E4%BB%80%E4%B9%88%E8%BF%9C%E8%A7%81)
[Student Forcing](https://kexue.fm/archives/7818#Student%20Forcing)
[往前多看几步](https://kexue.fm/archives/7818#%E5%BE%80%E5%89%8D%E5%A4%9A%E7%9C%8B%E5%87%A0%E6%AD%A5)
[用数学的话来说](https://kexue.fm/archives/7818#%E7%94%A8%E6%95%B0%E5%AD%A6%E7%9A%84%E8%AF%9D%E6%9D%A5%E8%AF%B4)
[效果、思考与讨论](https://kexue.fm/archives/7818#%E6%95%88%E6%9E%9C%E3%80%81%E6%80%9D%E8%80%83%E4%B8%8E%E8%AE%A8%E8%AE%BA)
[来自文末的总结](https://kexue.fm/archives/7818#%E6%9D%A5%E8%87%AA%E6%96%87%E6%9C%AB%E7%9A%84%E6%80%BB%E7%BB%93)

### 智能搜索

支持整句搜索！网站自动使用 [结巴分词](https://github.com/fxsjy/jieba) 进行分词，并结合ngrams排序算法给出合理的搜索结果。

### 热门标签

[生成模型](https://kexue.fm/tag/%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B/) [attention](https://kexue.fm/tag/attention/) [模型](https://kexue.fm/tag/%E6%A8%A1%E5%9E%8B/) [语言模型](https://kexue.fm/tag/%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/) [优化](https://kexue.fm/tag/%E4%BC%98%E5%8C%96/) [网站](https://kexue.fm/tag/%E7%BD%91%E7%AB%99/) [概率](https://kexue.fm/tag/%E6%A6%82%E7%8E%87/) [梯度](https://kexue.fm/tag/%E6%A2%AF%E5%BA%A6/) [转载](https://kexue.fm/tag/%E8%BD%AC%E8%BD%BD/) [微分方程](https://kexue.fm/tag/%E5%BE%AE%E5%88%86%E6%96%B9%E7%A8%8B/) [天象](https://kexue.fm/tag/%E5%A4%A9%E8%B1%A1/) [矩阵](https://kexue.fm/tag/%E7%9F%A9%E9%98%B5/) [深度学习](https://kexue.fm/tag/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/) [分析](https://kexue.fm/tag/%E5%88%86%E6%9E%90/) [积分](https://kexue.fm/tag/%E7%A7%AF%E5%88%86/) [python](https://kexue.fm/tag/python/) [力学](https://kexue.fm/tag/%E5%8A%9B%E5%AD%A6/) [无监督](https://kexue.fm/tag/%E6%97%A0%E7%9B%91%E7%9D%A3/) [几何](https://kexue.fm/tag/%E5%87%A0%E4%BD%95/) [优化器](https://kexue.fm/tag/%E4%BC%98%E5%8C%96%E5%99%A8/) [扩散](https://kexue.fm/tag/%E6%89%A9%E6%95%A3/) [节日](https://kexue.fm/tag/%E8%8A%82%E6%97%A5/) [生活](https://kexue.fm/tag/%E7%94%9F%E6%B4%BB/) [文本生成](https://kexue.fm/tag/%E6%96%87%E6%9C%AC%E7%94%9F%E6%88%90/) [数论](https://kexue.fm/tag/%E6%95%B0%E8%AE%BA/)

### 随机文章

- [基于Amos优化器思想推导出来的一些“炼丹策略”](https://kexue.fm/archives/9344)
- [Self-Orthogonality Module：一个即插即用的核正交化模块](https://kexue.fm/archives/7169)
- [为什么Pre Norm的效果不如Post Norm？](https://kexue.fm/archives/9009)
- [Transformer升级之路：19、第二类旋转位置编码](https://kexue.fm/archives/10862)
- [生活中的趣味数学：同一天生日概率有多大](https://kexue.fm/archives/40)
- [2020年全年天象](https://kexue.fm/archives/7144)
- [2010年3D电视入客厅](https://kexue.fm/archives/122)
- [【竖直上抛】炮弹能够射多高(第二宇宙速度)？](https://kexue.fm/archives/342)
- [\[欧拉数学\]素数倒数之和](https://kexue.fm/archives/1510)
- [《方程与宇宙》:拉格朗日点,复数,向量(五)](https://kexue.fm/archives/874)

### 最近评论

- [SunlightZero](https://kexue.fm/archives/9164/comment-page-4#comment-27497): 在《Step-by-Step Diffusion: An Elementary Tutoria...
- [苏剑林](https://kexue.fm/archives/5239/comment-page-3#comment-27496): 1、明白了，我将$q\_{\\phi}(z\|x)$看成$q\_{\\phi}(x\|z)$了（ELBO厌...
- [Suahi](https://kexue.fm/archives/5239/comment-page-3#comment-27493): 谢谢苏老师的回复！1\. 首先回复您为什么ELBO不带KL，并不是最终损失函数形式，需要做如下变...
- [eular](https://kexue.fm/archives/10373/comment-page-1#comment-27492): 是的，当$k$比较大时会出现这种情况。
- [苏剑林](https://kexue.fm/archives/5239/comment-page-3#comment-27491): 肯定是$\\mathbb{E}\_{x \\sim p\_{data}(x)}\[\\log(p\_{\\th...
- [苏剑林](https://kexue.fm/archives/10373/comment-page-1#comment-27490): 你的意思是$\\lambda(\\boldsymbol{x}) < x\_{\\min}$，一般情况下...
- [苏剑林](https://kexue.fm/archives/10735/comment-page-1#comment-27489): 好问题，下一篇文章可能会讨论这个问题
- [苏剑林](https://kexue.fm/archives/10633/comment-page-1#comment-27488): 不大熟悉，但都diffusion forcing了，还有必要CM吗
- [苏剑林](https://kexue.fm/archives/10711/comment-page-2#comment-27487): 简单看了一下，好像没什么新东西呀？还是我看漏了什么？
- [苏剑林](https://kexue.fm/archives/8601/comment-page-1#comment-27486): 感谢指出，已修正。

### 友情链接

- [Cool Papers](https://papers.cool)
- [数学研发](https://bbs.emath.ac.cn)
- [Seatop](http://www.seatop.com.cn/)
- [Xiaoxia](https://xiaoxia.org/)
- [积分表-网络版](https://kexue.fm/sci/integral/index.html)
- [丝路博傲](http://blog.dvxj.com/)
- [ph4ntasy 饭特稀](http://www.ph4ntasy.com/)
- [数学之家](http://www.2math.cn/)
- [有趣天文奇观](http://interesting-sky.china-vo.org/)
- [TwistedW](http://www.twistedwg.com/)
- [godweiyang](https://godweiyang.com/)
- [AI柠檬](https://blog.ailemon.net/)
- [王登科-DK博客](https://greatdk.com)
- [ESON](https://blog.eson.org/)
- [枫之羽](https://fzhiy.net/)
- [Mathor's blog](https://wmathor.com/)
- [coding-zuo](https://coding-zuo.github.io/)
- [博科园](https://www.bokeyuan.net/)
- [孔皮皮的博客](https://www.kppkkp.top/)
- [运鹏的博客](https://yunpengtai.top/)
- [jiming.site](https://jiming.site/)
- [OmegaXYZ](https://www.omegaxyz.com/)
- [Blog by Eacls](https://www.eacls.top/)
- [EAI猩球](https://www.robotech.ink/)
- [文举的博客](https://liwenju0.com/)
- [申请链接](https://kexue.fm/links.html)

[![署名-非商业用途-保持一致](https://kexue.fm/usr/themes/geekg/images/cc.gif)](http://creativecommons.org/licenses/by-nc-nd/2.5/cn/) 本站采用创作共用版权协议，要求署名、非商业用途和保持一致。转载本站内容必须也遵循“ [署名-非商业用途-保持一致](http://creativecommons.org/licenses/by-nc-nd/2.5/cn/)”的创作共用协议。

© 2009-2025 Scientific Spaces. All rights reserved. Theme by [laogui](http://www.laogui.com). Powered by [Typecho](http://typecho.org). 备案号: [粤ICP备09093259号-1/2](https://beian.miit.gov.cn/)。