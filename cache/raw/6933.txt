## SEARCH

## MENU

- [打赏](https://kexue.fm/reward.html)
- [公式](https://kexue.fm/latex.html)
- [天象](https://kexue.fm/ac.html)
- [链接](https://kexue.fm/links.html)
- [时光](https://kexue.fm/me.html)
- [博览](https://kexue.fm/science.html)
- [归档](https://kexue.fm/content.html)

## CATEGORIES

- [千奇百怪](https://kexue.fm/category/Everything)
- [天文探索](https://kexue.fm/category/Astronomy)
- [数学研究](https://kexue.fm/category/Mathematics)
- [物理化学](https://kexue.fm/category/Phy-chem)
- [信息时代](https://kexue.fm/category/Big-Data)
- [生物自然](https://kexue.fm/category/Biology)
- [图片摄影](https://kexue.fm/category/Photograph)
- [问题百科](https://kexue.fm/category/Questions)
- [生活/情感](https://kexue.fm/category/Life-Feeling)
- [资源共享](https://kexue.fm/category/Resources)

## NEWPOSTS

- [通过msign来计算mclip（奇...](https://kexue.fm/archives/11006)
- [msign算子的Newton-Sc...](https://kexue.fm/archives/10996)
- [等值振荡定理：最优多项式逼近的充要条件](https://kexue.fm/archives/10972)
- [生成扩散模型漫谈（三十）：从瞬时速...](https://kexue.fm/archives/10958)
- [MoE环游记：5、均匀分布的反思](https://kexue.fm/archives/10945)
- [msign算子的Newton-Sc...](https://kexue.fm/archives/10922)
- [Transformer升级之路：2...](https://kexue.fm/archives/10907)
- [一道概率不等式：盯着它到显然成立为止！](https://kexue.fm/archives/10902)
- [SVD的导数](https://kexue.fm/archives/10878)
- [智能家居之手搓一套能接入米家的零冷水装置](https://kexue.fm/archives/10869)

## COMMENTS

- [苏剑林: 刚入门那会的文章，不用深究了。](https://kexue.fm/archives/481/comment-page-1#comment-27835)
- [苏剑林: 目前各方面的实测效果看来不会，我觉得本质上就是因为partia...](https://kexue.fm/archives/10122/comment-page-1#comment-27834)
- [苏剑林: 首先，瞬时速度为什么跟$t$无关？其次，现在reflow和me...](https://kexue.fm/archives/10958/comment-page-1#comment-27833)
- [苏剑林: 对于每一步数据都严格对齐来说，0.01的loss差距不小了，因...](https://kexue.fm/archives/10907/comment-page-2#comment-27832)
- [苏剑林: \[comment=27808\]rpsun\[/comment\]\
...](https://kexue.fm/archives/10699/comment-page-1#comment-27831)
- [苏剑林: 自己都没怎么关注天象了，惭愧](https://kexue.fm/archives/1490/comment-page-1#comment-27830)
- [苏剑林: 原来如此。其实只要预测空间是连续空间，并且任务本质是一对多的输...](https://kexue.fm/archives/10958/comment-page-1#comment-27829)
- [苏剑林: 你可以拿一批语料去eval，看各个expert分别激活了多少次呀。](https://kexue.fm/archives/10945/comment-page-1#comment-27828)
- [苏剑林: 首先这个分布肯定是存在的（贝叶斯公式无关分布），然后它的概率密...](https://kexue.fm/archives/9164/comment-page-4#comment-27827)
- [苏剑林: 这两天看了下FoPE，感觉它的分析有点道理，但它实现的代码跟论...](https://kexue.fm/archives/10907/comment-page-2#comment-27826)

## USERLOGIN

- [登录](https://kexue.fm/admin/login.php)

[科学空间\|Scientific Spaces](https://kexue.fm)

- [登录](https://kexue.fm/admin/login.php)
- [打赏](https://kexue.fm/reward.html)
- [公式](https://kexue.fm/latex.html)
- [天象](https://kexue.fm/ac.html)
- [链接](https://kexue.fm/links.html)
- [时光](https://kexue.fm/me.html)
- [博览](https://kexue.fm/science.html)
- [归档](https://kexue.fm/content.html)

渴望成为一个小飞侠

- [欢迎订阅](https://kexue.fm/feed)
- [个性邮箱](https://kexue.fm/archives/119)
- [天象信息](https://kexue.fm/ac.html)
- [观测ISS](https://kexue.fm/archives/41)
- [LaTeX](https://kexue.fm/latex.html)
- [关于博主](https://kexue.fm/me.html)

欢迎访问“科学空间”，这里将与您共同探讨自然科学，回味人生百态；也期待大家的分享～

- [**千奇百怪** Everything](https://kexue.fm/category/Everything)
- [**天文探索** Astronomy](https://kexue.fm/category/Astronomy)
- [**数学研究** Mathematics](https://kexue.fm/category/Mathematics)
- [**物理化学** Phy-chem](https://kexue.fm/category/Phy-chem)
- [**信息时代** Big-Data](https://kexue.fm/category/Big-Data)
- [**生物自然** Biology](https://kexue.fm/category/Biology)
- [**图片摄影** Photograph](https://kexue.fm/category/Photograph)
- [**问题百科** Questions](https://kexue.fm/category/Questions)
- [**生活/情感** Life-Feeling](https://kexue.fm/category/Life-Feeling)
- [**资源共享** Resources](https://kexue.fm/category/Resources)

- [**千奇百怪**](https://kexue.fm/category/Everything)
- [**天文探索**](https://kexue.fm/category/Astronomy)
- [**数学研究**](https://kexue.fm/category/Mathematics)
- [**物理化学**](https://kexue.fm/category/Phy-chem)
- [**信息时代**](https://kexue.fm/category/Big-Data)
- [**生物自然**](https://kexue.fm/category/Biology)
- [**图片摄影**](https://kexue.fm/category/Photograph)
- [**问题百科**](https://kexue.fm/category/Questions)
- [**生活/情感**](https://kexue.fm/category/Life-Feeling)
- [**资源共享**](https://kexue.fm/category/Resources)

[首页](https://kexue.fm) [信息时代](https://kexue.fm/category/Big-Data) 从语言模型到Seq2Seq：Transformer如戏，全靠Mask

18Sep

# [从语言模型到Seq2Seq：Transformer如戏，全靠Mask](https://kexue.fm/archives/6933)

By 苏剑林 \|
2019-09-18 \|
387092位读者\|

相信近一年来（尤其是近半年来），大家都能很频繁地看到各种Transformer相关工作（比如Bert、GPT、XLNet等等）的报导，连同各种基础评测任务的评测指标不断被刷新。同时，也有很多相关的博客、专栏等对这些模型做科普和解读。

俗话说，“外行看热闹，内行看门道”，我们不仅要在“是什么”这个层面去理解这些工作，我们还需要思考“为什么”。这个“为什么”不仅仅是“为什么要这样做”，还包括“为什么可以这样做”。比如，在谈到XLNet的乱序语言模型时，我们或许已经从诸多介绍中明白了乱序语言模型的好处，那不妨更进一步思考一下：

> 为什么Transformer可以实现乱序语言模型？是怎么实现的？RNN可以实现吗？

本文从对Attention矩阵进行Mask的角度，来分析为什么众多Transformer模型可以玩得如此“出彩”的基本原因，正如标题所述“Transformer如戏，全靠Mask”，这是各种花式Transformer模型的重要“门道”之一。

读完本文，你或许可以了解到：

> 1、Attention矩阵的Mask方式与各种预训练方案的关系；
>
> 2、直接利用预训练的Bert模型来做Seq2Seq任务。

## 背景 [\#](https://kexue.fm/archives/6933\#%E8%83%8C%E6%99%AF)

自 [《Attention is All You Need》](https://kexue.fm/archives/4765) 以后，基于纯Attention的Transformer类模型逐渐变得流行起来，而Bert的出现则将这股潮流推向了一个新的高度。而后，各种基于大规模预训练的Transformer模型的工作不断出现，有基于现成的模型做应用的，有试图更好地去解释和可视化这些模型的，还有改进架构、改进预训练方式等以得到更好结果的。总的来说，这些以预训练为基础的工作层出不穷，有种琳琅满目的感觉。甚至一定程度上来说，如果你还没有微调过Bert，那已经算是落后于主流的NLP技术了。

### 花式预训练 [\#](https://kexue.fm/archives/6933\#%E8%8A%B1%E5%BC%8F%E9%A2%84%E8%AE%AD%E7%BB%83)

众所周知，传统的模型预训练手段就是语言模型，比如 [ELMo](https://papers.cool/arxiv/1802.05365) 模型就是以BiLSTM为基础架构、用两个方向的语言模型分别预训练两个方向的LSTM的，后面的OpenAI的GPT、 [GPT-2](https://d4mucfpksywv.cloudfront.net/better-language-models/language-models.pdf) 也是坚定不移地坚持着用祖传的（标准的、单向的）语言模型来预训练。

然而，还有更多花样的预训练玩法。比如 [Bert](https://papers.cool/arxiv/1810.04805) 就用了称之为“掩码语言模型（Masked Language Model）”的方式来预训练，不过这只是普通语言模型的一种变体；还有 [XLNet](https://papers.cool/arxiv/1906.08237) 则提出了更彻底的“Permutation Language Modeling”，我们可以称之为“乱序语言模型”；还有 [UNILM](https://papers.cool/arxiv/1905.03197) 模型，直接用单个Bert的架构做Seq2Seq，你可以将它作为一种预训练手段，又或者干脆就用它来做Seq2Seq任务...

如此花样百出，让我们不禁疑问：为什么刚好在Transformer流行的时代，才出现这种各种大型预训练模型“百花齐放，百家争鸣”的现象？

### Transformer专属 [\#](https://kexue.fm/archives/6933\#Transformer%E4%B8%93%E5%B1%9E)

事实上，除了单向语言模型及其简单变体掩码语言模型之外，UNILM的Seq2Seq预训练、XLNet的乱序语言模型预训练，基本可以说是专为Transformer架构定制的。说白了，如果是RNN架构，根本就不能用乱序语言模型的方式来预训练，至于Seq2Seq的预训练方式，则必须同时引入两个模型（encoder和decoder），而无法像Transformer架构一样，可以一个模型搞定。

这其中的奥妙主要在Attention矩阵之上。Attention实际上相当于将输入两两地算相似度，这构成了一个$n^2$大小的相似度矩阵（即Attention矩阵，$n$是句子长度，本文的Attention均指Self Attention），这意味着它的空间占用量是$\\mathcal{O}(n^2)$量级，相比之下，RNN模型、CNN模型只不过是$\\mathcal{O}(n)$，所以实际上Attention通常更耗显存。然而，有弊也有利，更大的空间占用也意味着拥有了更多的可能性，我们可以通过往这个$\\mathcal{O}(n^2)$级别的Attention矩阵加入各种先验约束，使得它可以做更灵活的任务。说白了，也就只有纯Attention的模型，才有那么大的“容量”去承载那么多的“花样”。

而加入先验约束的方式，就是对Attention矩阵进行不同形式的Mask，这便是本文要关注的焦点。

## 分析 [\#](https://kexue.fm/archives/6933\#%E5%88%86%E6%9E%90)

在 [《〈Attention is All You Need〉浅读（简介+代码）》](https://kexue.fm/archives/4765) 一文中笔者已经对Attention做了基本介绍，这里仅做简单回顾。Attention的数学形式为：
\\begin{equation}Attention(\\boldsymbol{Q},\\boldsymbol{K},\\boldsymbol{V}) = softmax\\left(\\frac{\\boldsymbol{Q}\\boldsymbol{K}^{\\top}}{\\sqrt{d\_k}}\\right)\\boldsymbol{V}\\end{equation}

这里的$\\boldsymbol{Q}\\in \\mathbb{R}^{l\_q\\times d\_q},\\boldsymbol{K}\\in\\mathbb{R}^{l\_k\\times d\_q},\\boldsymbol{V}\\in\\mathbb{R}^{l\_k\\times d\_v}$，分别代表query、key、value的向量序列，其中我们可以认为key和value是一一对应的，而$\\boldsymbol{Q}\\boldsymbol{K}^{\\top}$则是将query、key的向量两两做内积，然后用$softmax$归一化，就得到一个$l\_q\\times l\_k$的Attention矩阵，它描述的就是query和key之间任意两个元素的关联强度，后面我们要讲的故事，都是在这个Attention矩阵上下功夫。最后再与$\\boldsymbol{V}$相乘，相当于按照这个关联强度将$\\boldsymbol{V}$的各个向量加权求和，最终输出一个$l\_q\\times d\_v$的向量序列。

目前最常用的Attention方式当数Self Attention，即$\\boldsymbol{Q},\\boldsymbol{K},\\boldsymbol{V}$都是同一个向量序列经过线性变换而来的，而Transformer则是Self Attention跟Position-Wise全连接层（相当于kernel size为1的一维卷积）的组合。所以，Transformer就是基于Attention的向量序列到向量序列的变换。

在本节中，我们将会比较详细地分析Attention矩阵的Mask方式，这分别对应单向语言模型、乱序语言模型、Seq2Seq的实现原理。

### 单向语言模型 [\#](https://kexue.fm/archives/6933\#%E5%8D%95%E5%90%91%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B)

语言模型可以说是一个无条件的文本生成模型，如果读者还不了解文本生成模型，可以自行查阅相关资料并配合 [《玩转Keras之seq2seq自动生成标题》](https://kexue.fm/archives/5861) 一文来理解。单向语言模型相当于把训练语料通过下述条件概率分布的方式“记住”了：
\\begin{equation}p(x\_1,x\_2,x\_3,\\dots,x\_n)=p(x\_1) p(x\_2\|x\_1) p(x\_3\|x\_1,x\_2) \\dots p(x\_n\|x\_1,\\dots,x\_{n-1})\\end{equation}

我们一般说的“语言模型”，就是指单向的（更狭义的只是指正向的）语言模型。语言模型的关键点是要防止看到“未来信息”。如上式，预测$x\_1$的时候，是没有任何外部输入的；而预测$x\_2$的时候，只能输入$x\_1$，预测$x\_3$的时候，只能输入$x\_1,x\_2$；依此类推。

单向语言模型图示。每预测一个token，只依赖于前面的token。

RNN模型是天然适合做语言模型的，因为它本身就是递归的运算；如果用CNN来做的话，则需要对卷积核进行Mask，即需要将卷积核对应右边的部分置零。如果是Transformer呢？那需要一个下三角矩阵形式的Attention矩阵：

单向（正向）语言模型的Mask方式

如图所示，Attention矩阵的每一行事实上代表着输出，而每一列代表着输入，而Attention矩阵就表示输出和输入的关联。假定白色方格都代表0，那么第1行表示“北”只能跟起始标记 相关了，而第2行就表示“京”只能跟起始标记 和“北”相关了，依此类推。所以，只需要在Transformer的Attention矩阵中引入下三角形形式的Mask，并将输入输出错开一位训练，就可以实现单向语言模型了。（至于Mask的实现方式，可以参考 [《“让Keras更酷一些！”：层中层与mask》的Mask一节](https://kexue.fm/archives/6810#Mask)。）

### 乱序语言模型 [\#](https://kexue.fm/archives/6933\#%E4%B9%B1%E5%BA%8F%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B)

乱序语言模型是XLNet提出来的概念，它主要用于XLNet的预训练上。说到XLNet，我觉得它的乱序语言模型这种预训练方式是很有意思的，但是我并不喜欢它将基本架构换成了Transformer-XL。我觉得谁有资源可以试试“Bert+乱序语言语言模型预训练”的组合，或许会有意外的发现。

乱序语言模型跟语言模型一样，都是做条件概率分解，但是乱序语言模型的分解顺序是随机的：
\\begin{equation}\\begin{aligned}p(x\_1,x\_2,x\_3,\\dots,x\_n)=&p(x\_1) p(x\_2\|x\_1) p(x\_3\|x\_1,x\_2) \\dots p(x\_n\|x\_1,x\_2,\\dots,x\_{n-1})\\\
=&p(x\_3) p(x\_1\|x\_3) p(x\_2\|x\_3,x\_1) \\dots p(x\_n\|x\_3,x\_1,\\dots,x\_{n-1})\\\
=&\\dots\\\
=&p(x\_{n-1})p(x\_1\|x\_{n-1})p(x\_n\|x\_{n-1}, x\_1)\\dots p(x\_2\|x\_{n-1}, x\_1,\\dots,x\_3)\\end{aligned}\\end{equation}

总之，$x\_1,x\_2,\\dots,x\_n$任意一种“出场顺序”都有可能。原则上来说，每一种顺序都对应着一个模型，所以原则上就有$n!$个语言模型。而基于Transformer的模型，则可以将这所有顺序都做到一个模型中去！

那怎么做到这一点呢？还是以“北京欢迎你”的生成为例，假设随机的一种生成顺序为“ → 迎 → 京 → 你 → 欢 → 北 → ”，那么我们只需要用下图中第二个子图的方式去Mask掉Attention矩阵，就可以达到目的了：

正向语言模型的Mask

乱序语言模型的Mask

倒序语言模型的Mask

跟前面的单向语言模型类似，第4行只有一个蓝色格，表示“迎”只能跟起始标记 相关，而第2行有两个蓝色格，表示“京”只能跟起始标记 和“迎”相关，依此类推。直观来看，这就像是把单向语言模型的下三角形式的Mask“打乱”了。

也就是说，实现某种特定顺序的语言模型，就相当于将原来的下三角形式的Mask以某种方式打乱。正因为Attention提供了这样的一个$n\\times n$的Attention矩阵，我们才有足够多的自由度去以不同的方式去Mask这个矩阵，从而实现多样化的效果。

说到这里，读者可能会有一个实现上的疑问：打乱后的Mask似乎没看出什么规律呀，难道每次都要随机生成一个这样的似乎没有什么明显概率的Mask矩阵？事实上有一种更简单的、数学上等效的训练方案。这个训练方案源于纯Attention的模型本质上是一个无序的模型，它里边的词序实际上是通过Position Embedding加上去的。也就是说，我们输入的不仅只有token本身，还包括token所在的位置id；再换言之，你觉得你是输入了序列“\[北, 京, 欢, 迎, 你\]”，实际上你输入的是集合“{(北, 1), (京, 2), (欢, 3), (迎, 4), (你, 5)}”。

重新排序，使得正向语言模型就可以实现乱序语言模型

既然只是一个集合，跟顺序无关，那么我们完全可以换一种顺序输入，比如刚才的“ → 迎 → 京 → 你 → 欢 → 北 → ”，我们可以按“(迎, 4), (京, 2), (你, 5), (欢, 3), (北, 1)”的顺序输入，也就是说将token打乱为“迎,京,你,欢,北”输入到Transformer中，但是第1个token的position就不是1了，而是4；依此类推。这样换过来之后，Mask矩阵可以恢复为下三角矩阵，所以只需要在输入层面打乱即可，这样操作起来就更简单了。

### Seq2Seq [\#](https://kexue.fm/archives/6933\#Seq2Seq)

现在到我们的“重头戏”了：**将Bert等Transformer架构跟Seq2Seq结合起来**。为什么说重头戏呢？因为原则上来说，任何NLP问题都可以转化为Seq2Seq来做，它是一个真正意义上的万能模型。所以如果能够做到Seq2Seq，理论上就可以实现任意任务了。

将Bert与Seq2Seq结合的比较知名的工作有两个： [MASS](https://papers.cool/arxiv/1905.02450) 和 [UNILM](https://papers.cool/arxiv/1905.03197)，两者都是微软的工作，两者还都在同一个月发的～其中MASS还是普通的Seq2Seq架构，分别用Bert类似的Transformer模型来做encoder和decoder，它的主要贡献就是提供了一种Seq2Seq思想的预训练方案；真正有意思的是UNILM，它提供了一种很优雅的方式，能够让我们直接用单个Bert模型就可以做Seq2Seq任务，而不用区分encoder和decoder。而实现这一点几乎不费吹灰之力——只需要一个特别的Mask。

（插曲：事实的顺序是笔者前两周自己独立地想到了用单个Bert模型做Seq2Seq的思路，然后去找资料发现这个思路已经被做了，正是UNILM。）

UNILM直接将Seq2Seq当成句子补全来做。假如输入是“你想吃啥”，目标句子是“白切鸡”，那UNILM将这两个句子拼成一个：\[CLS\] 你 想 吃 啥 \[SEP\] 白 切 鸡 \[SEP\]。经过这样转化之后，最简单的方案就是训练一个语言模型，然后输入“\[CLS\] 你 想 吃 啥 \[SEP\]”来逐字预测“白 切 鸡”，直到出现“\[SEP\]”为止，即如下面的左图：

用单向语言模型的方式做Seq2Seq

设计更适合的Mask做Seq2Seq

不过左图只是最朴素的方案，它把“你想吃啥”也加入了预测范围了（导致它这部分的Attention是单向的，即对应部分的Mask矩阵是下三角），事实上这是不必要的，属于额外的约束。真正要预测的只是“白切鸡”这部分，所以我们可以把“你想吃啥”这部分的Mask去掉，得到上面的右图的Mask。

这样一来，输入部分的Attention是双向的，输出部分的Attention是单向，满足Seq2Seq的要求，而且没有额外约束。这便是UNILM里边提供的用单个Bert模型就可以完成Seq2Seq任务的思路，只要添加上述形状的Mask，而不需要修改模型架构，并且还可以直接沿用Bert的Masked Language Model预训练权重，收敛更快。这符合“一Bert在手，天下我有”的万用模型的初衷，个人认为这是非常优雅的方案。

UNILM做Seq2Seq模型图示。输入部分内部可做双向Attention，输出部分只做单向Attention。

## 实验 [\#](https://kexue.fm/archives/6933\#%E5%AE%9E%E9%AA%8C)

事实上，上述的这些Mask方案，基本上都已经被集成在笔者写的 [bert4keras](https://kexue.fm/archives/6915)，读者可以直接用bert4keras加载bert的预训练权重，并且调用上述Mask方案来做相应的任务。下面，我们给出一个利用UNILM的思路做一个快速收敛的Seq2Seq模型的例子。

### 代码开源 [\#](https://kexue.fm/archives/6933\#%E4%BB%A3%E7%A0%81%E5%BC%80%E6%BA%90)

这次代码的测试任务依然是之前的标题生成，代码调整自 [《玩转Keras之seq2seq自动生成标题》](https://kexue.fm/archives/5861) 里边的代码，并且得益于 [bert4keras](https://github.com/bojone/bert4keras) 的封装，模型部分的代码实现非常简单清爽。这一次直接使用了 [THUCNews](http://thuctc.thunlp.org/#%E4%B8%AD%E6%96%87%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E6%95%B0%E6%8D%AE%E9%9B%86THUCNews) 的原始数据集，读者可以自行下载数据集和源码测试复现。

详细请看： [**task\_seq2seq\_autotitle.py**](https://github.com/bojone/bert4keras/blob/master/examples/task_seq2seq_autotitle.py)

这个效果能有多好呢？经过实验，在标题生成的任务上，从第一个epoch（1000个iteration）开始，就已经能生成基本可读的标题了。相应地，以前用LSTM做的时候，大概需要多几十倍的iteration才有同样的效果。

第一个epoch（1000步）就可以得到基本可读的生成结果

### 简单说明 [\#](https://kexue.fm/archives/6933\#%E7%AE%80%E5%8D%95%E8%AF%B4%E6%98%8E)

下面对代码的关键部分做简要说明。

首先，输入格式还是以 `token_id` 和 `segment_id` 输入，比如：

```
tokens = ['[ClS]', u'你', u'想', u'吃', u'啥', '[SEP]', u'白', u'切', u'鸡', '[SEP]']
token_ids = [token_dict[t] for t in tokens]
segment_ids = [0, 0, 0, 0, 0, 0, 1, 1, 1, 1]
```

`segment_ids` 用来区分输入句子和目标句子，0对应的为输入句子，1对应的为目标句子，只需要自带的 `tokenizer.encode` 就可以生成这种 `token_id` 和 `segment_id` 了。

至于搭建模型，就只有寥寥几行：

```
model = build_transformer_model(
 config_path,
 checkpoint_path,
 application='unilm',
 keep_tokens=keep_tokens
)

model.summary()

y_in = model.input[0][:, 1:] # 目标tokens
y_mask = model.input[1][:, 1:]
y = model.output[:, :-1] # 预测tokens，预测与目标错开一位

# 交叉熵作为loss，并mask掉输入部分的预测
cross_entropy = K.sparse_categorical_crossentropy(y_in, y)
cross_entropy = K.sum(cross_entropy * y_mask) / K.sum(y_mask)
```

注意 `build_transformer_model` 中只要设置 `application='unilm'`，就会自动加载Bert的MLM部分，并且传入对应的Mask，剩下就只需要把loss写好就行了。另外还有一个 `keep_tokens`，这个是用来精简Embedding层用的，对于中文Bert来说，总的tokens大概有2万个，这意味着最后预测生成的token时是一个2万分类问题。但事实上有接近一半的tokens都不会分出来（理论上都不会），因此这2万分类浪费了一些计算量。于是这里提供了一个选项，我们可以自行维护一个token表，然后传入对应的id，只保留这部分token，这样就可以降低计算量了（精简后一般只是原来的一半，甚至更少）。

剩下的就是通过beam search来解码等步骤了，这与一般的Seq2Seq无异，不再赘述，大家看 [《玩转Keras之seq2seq自动生成标题》](https://kexue.fm/archives/5861) 和代码即可。

## 总结 [\#](https://kexue.fm/archives/6933\#%E6%80%BB%E7%BB%93)

**本文相对系统地总结了Transformer中Attention矩阵的Mask技巧，并且给出了用UNILM方案来做Seq2Seq的实现。对于同语言的Seq2Seq的文本生成任务来说，采用UNILM的思路加载Bert的MLM预训练权重，能够有效、快速地实现并提升生成效果，值得一试。**

_**转载到请包括本文地址：** [https://kexue.fm/archives/6933](https://kexue.fm/archives/6933)_

_**更详细的转载事宜请参考：**_ [《科学空间FAQ》](https://kexue.fm/archives/6508#%E6%96%87%E7%AB%A0%E5%A6%82%E4%BD%95%E8%BD%AC%E8%BD%BD/%E5%BC%95%E7%94%A8)

**如果您还有什么疑惑或建议，欢迎在下方评论区继续讨论。**

**如果您觉得本文还不错，欢迎 [分享](https://kexue.fm/archives/6933#share)/ [打赏](https://kexue.fm/archives/6933#pay) 本文。打赏并非要从中获得收益，而是希望知道科学空间获得了多少读者的真心关注。当然，如果你无视它，也不会影响你的阅读。再次表示欢迎和感谢！**

打赏

微信打赏

支付宝打赏

因为网站后台对打赏并无记录，因此欢迎在打赏时候备注留言。你还可以 [**点击这里**](http://mail.qq.com/cgi-bin/qm_share?t=qm_mailme&email=tN7d1drY3drrx8H0xcWa19vZ) 或在下方评论区留言来告知你的建议或需求。

**如果您需要引用本文，请参考：**

苏剑林. (Sep. 18, 2019). 《从语言模型到Seq2Seq：Transformer如戏，全靠Mask 》\[Blog post\]. Retrieved from [https://kexue.fm/archives/6933](https://kexue.fm/archives/6933)

@online{kexuefm-6933,
        title={从语言模型到Seq2Seq：Transformer如戏，全靠Mask},
        author={苏剑林},
        year={2019},
        month={Sep},
        url={\\url{https://kexue.fm/archives/6933}},
}

分类： [信息时代](https://kexue.fm/category/Big-Data)    标签： [语言模型](https://kexue.fm/tag/%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/), [NLP](https://kexue.fm/tag/NLP/), [文本生成](https://kexue.fm/tag/%E6%96%87%E6%9C%AC%E7%94%9F%E6%88%90/), [attention](https://kexue.fm/tag/attention/)[149 评论](https://kexue.fm/archives/6933#comments)

< [重新写了之前的新词发现算法：更快更好的新词发现](https://kexue.fm/archives/6920) \| [“让Keras更酷一些！”：层与模型的重用技巧](https://kexue.fm/archives/6985) >

### 你也许还对下面的内容感兴趣

- [Transformer升级之路：20、MLA究竟好在哪里？](https://kexue.fm/archives/10907)
- [Transformer升级之路：19、第二类旋转位置编码](https://kexue.fm/archives/10862)
- [细水长flow之TARFLOW：流模型满血归来？](https://kexue.fm/archives/10667)
- [“闭门造车”之多模态思路浅谈（三）：位置编码](https://kexue.fm/archives/10352)
- [Decoder-only的LLM为什么需要位置编码？](https://kexue.fm/archives/10347)
- [Monarch矩阵：计算高效的稀疏型矩阵分解](https://kexue.fm/archives/10249)
- [Transformer升级之路：18、RoPE的底数选择原则](https://kexue.fm/archives/10122)
- [缓存与效果的极限拉扯：从MHA、MQA、GQA到MLA](https://kexue.fm/archives/10091)
- [Transformer升级之路：17、多模态位置编码的简单思考](https://kexue.fm/archives/10040)
- [时空之章：将Attention视为平方复杂度的RNN](https://kexue.fm/archives/10017)

[发表你的看法](https://kexue.fm/archives/6933#comment_form)

1. [«](https://kexue.fm/archives/6933/comment-page-7#comments)
2. [1](https://kexue.fm/archives/6933/comment-page-1#comments)
3. ...
4. [5](https://kexue.fm/archives/6933/comment-page-5#comments)
5. [6](https://kexue.fm/archives/6933/comment-page-6#comments)
6. [7](https://kexue.fm/archives/6933/comment-page-7#comments)
7. [8](https://kexue.fm/archives/6933/comment-page-8#comments)

[gps做成 门禁卡模样\_一种可导航用临时门禁卡及其使用方法与流程 \| Coding栈](https://www.itcode1024.com/86732/)

March 10th, 2023

\[...\]苏剑林. (Sep. 18, 2019). 《从语言模型到Seq2Seq：Transformer如戏，全靠Mask 》\[Blog post\]. Retrieved from [https://spaces.ac.cn/archives/6933](https://spaces.ac.cn/archives/6933)\[...\]

[回复评论](https://kexue.fm/archives/6933/comment-page-8?replyTo=21100#respond-post-6933)

[【读论文】一种通用句子编码：Universal Sentence Encoder \| Coding栈](https://www.itcode1024.com/87339/)

March 11th, 2023

\[...\]使用苏神bert4keras框架，模型是unilm，直接用单个Bert的架构做Seq2Seq，详细介绍请转入从语言模型到Seq2Seq。\[...\]

[回复评论](https://kexue.fm/archives/6933/comment-page-8?replyTo=21114#respond-post-6933)

[LLVM 编译器 \| Coding栈](https://www.itcode1024.com/87421/)

March 11th, 2023

\[...\]苏剑林. (Sep. 18, 2019). 《从语言模型到Seq2Seq：Transformer如戏，全靠Mask 》\[Blog post\]. Retrieved from [https://spaces.ac.cn/archives/6933](https://spaces.ac.cn/archives/6933)\[...\]

[回复评论](https://kexue.fm/archives/6933/comment-page-8?replyTo=21115#respond-post-6933)

[MCAN论文进阶——MoVie: Revisting Modulated Convolutions for Visual Counting and Beyond 论文笔记 \| Coding栈](https://www.itcode1024.com/87903/)

March 12th, 2023

\[...\]苏剑林. (Sep. 18, 2019). 《从语言模型到Seq2Seq：Transformer如戏，全靠Mask 》\[Blog post\]. Retrieved from [https://spaces.ac.cn/archives/6933](https://spaces.ac.cn/archives/6933)\[...\]

[回复评论](https://kexue.fm/archives/6933/comment-page-8?replyTo=21125#respond-post-6933)

[基于keras4bert的seq2seq机制的文章标题生成 R11; 白墨代码网](http://code.bmoook.com/%e5%9f%ba%e4%ba%8ekeras4bert%e7%9a%84seq2seq%e6%9c%ba%e5%88%b6%e7%9a%84%e6%96%87%e7%ab%a0%e6%a0%87%e9%a2%98%e7%94%9f%e6%88%90/)

March 13th, 2023

\[...\]本次训练实战参照的是该篇博客文章： [https://kexue.fm/archives/6933](https://kexue.fm/archives/6933)\[...\]

[回复评论](https://kexue.fm/archives/6933/comment-page-8?replyTo=21140#respond-post-6933)

[Bert不完全手册2. Bert不能做NLG？MASS/UNILM/BART\_Johngo学长](https://www.johngo689.com/566174/)

June 4th, 2023

\[...\]苏剑林. (Sep. 18, 2019). 《从语言模型到Seq2Seq：Transformer如戏，全靠Mask 》\[Blog post\]. Retrieved from [https://spaces.ac.cn/archives/6933](https://spaces.ac.cn/archives/6933)\[...\]

[回复评论](https://kexue.fm/archives/6933/comment-page-8?replyTo=21862#respond-post-6933)

[Bert不完全手册2. Bert不能做NLG？MASS/UNILM/BART \| Coding栈](https://www.itcode1024.com/136591/)

June 9th, 2023

\[...\]苏剑林. (Sep. 18, 2019). 《从语言模型到Seq2Seq：Transformer如戏，全靠Mask 》\[Blog post\]. Retrieved from [https://spaces.ac.cn/archives/6933](https://spaces.ac.cn/archives/6933)\[...\]

[回复评论](https://kexue.fm/archives/6933/comment-page-8?replyTo=21915#respond-post-6933)

[Bert不完全手册2. Bert不能做NLG？MASS/UNILM/BART \| Coding栈](https://www.itcode1024.com/137220/)

June 9th, 2023

\[...\]苏剑林. (Sep. 18, 2019). 《从语言模型到Seq2Seq：Transformer如戏，全靠Mask 》\[Blog post\]. Retrieved from [https://spaces.ac.cn/archives/6933](https://spaces.ac.cn/archives/6933)\[...\]

[回复评论](https://kexue.fm/archives/6933/comment-page-8?replyTo=21917#respond-post-6933)

casbupt

June 19th, 2023

"从语言模型到Seq2Seq：Transformer如戏，全靠Mask" 中提到“build\_transformer\_model中只要设置application='unilm'，就会自动加载Bert的MLM部分，并且传入对应的Mask”。
这里的Mask是指的Attention Mask吗，还是也包括了对target sentence的mask操作(按100%比例)?

\[UniLM开源代码中s2s有个mask\_prob的参数, finetune时针对target sentence根据这个参数按比例(比如>70%)来mask. \]

[回复评论](https://kexue.fm/archives/6933/comment-page-8?replyTo=22022#respond-post-6933)

[苏剑林](https://kexue.fm) 发表于
June 25th, 2023

这里的unilm特指混合的attention mask，不是指预训练方案，跟微软的原论文无关

[回复评论](https://kexue.fm/archives/6933/comment-page-8?replyTo=22043#respond-post-6933)

1. [«](https://kexue.fm/archives/6933/comment-page-7#comments)
2. [1](https://kexue.fm/archives/6933/comment-page-1#comments)
3. ...
4. [5](https://kexue.fm/archives/6933/comment-page-5#comments)
5. [6](https://kexue.fm/archives/6933/comment-page-6#comments)
6. [7](https://kexue.fm/archives/6933/comment-page-7#comments)
7. [8](https://kexue.fm/archives/6933/comment-page-8#comments)

[取消回复](https://kexue.fm/archives/6933#respond-post-6933)

你的大名

电子邮箱

个人网站（选填）

1\. 可以使用LaTeX代码，点击“预览效果”可查看效果；2. 可以通过点击评论楼层编号来引用该楼层；3. 网站可能会有点卡，如非确认评论失败，请 **不要重复点击提交**。

### 内容速览

[背景](https://kexue.fm/archives/6933#%E8%83%8C%E6%99%AF)
[花式预训练](https://kexue.fm/archives/6933#%E8%8A%B1%E5%BC%8F%E9%A2%84%E8%AE%AD%E7%BB%83)
[Transformer专属](https://kexue.fm/archives/6933#Transformer%E4%B8%93%E5%B1%9E)
[分析](https://kexue.fm/archives/6933#%E5%88%86%E6%9E%90)
[单向语言模型](https://kexue.fm/archives/6933#%E5%8D%95%E5%90%91%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B)
[乱序语言模型](https://kexue.fm/archives/6933#%E4%B9%B1%E5%BA%8F%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B)
[Seq2Seq](https://kexue.fm/archives/6933#Seq2Seq)
[实验](https://kexue.fm/archives/6933#%E5%AE%9E%E9%AA%8C)
[代码开源](https://kexue.fm/archives/6933#%E4%BB%A3%E7%A0%81%E5%BC%80%E6%BA%90)
[简单说明](https://kexue.fm/archives/6933#%E7%AE%80%E5%8D%95%E8%AF%B4%E6%98%8E)
[总结](https://kexue.fm/archives/6933#%E6%80%BB%E7%BB%93)

### 智能搜索

支持整句搜索！网站自动使用 [结巴分词](https://github.com/fxsjy/jieba) 进行分词，并结合ngrams排序算法给出合理的搜索结果。

### 热门标签

[生成模型](https://kexue.fm/tag/%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B/) [attention](https://kexue.fm/tag/attention/) [优化](https://kexue.fm/tag/%E4%BC%98%E5%8C%96/) [语言模型](https://kexue.fm/tag/%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/) [模型](https://kexue.fm/tag/%E6%A8%A1%E5%9E%8B/) [网站](https://kexue.fm/tag/%E7%BD%91%E7%AB%99/) [概率](https://kexue.fm/tag/%E6%A6%82%E7%8E%87/) [梯度](https://kexue.fm/tag/%E6%A2%AF%E5%BA%A6/) [转载](https://kexue.fm/tag/%E8%BD%AC%E8%BD%BD/) [微分方程](https://kexue.fm/tag/%E5%BE%AE%E5%88%86%E6%96%B9%E7%A8%8B/) [矩阵](https://kexue.fm/tag/%E7%9F%A9%E9%98%B5/) [天象](https://kexue.fm/tag/%E5%A4%A9%E8%B1%A1/) [分析](https://kexue.fm/tag/%E5%88%86%E6%9E%90/) [深度学习](https://kexue.fm/tag/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/) [积分](https://kexue.fm/tag/%E7%A7%AF%E5%88%86/) [python](https://kexue.fm/tag/python/) [优化器](https://kexue.fm/tag/%E4%BC%98%E5%8C%96%E5%99%A8/) [力学](https://kexue.fm/tag/%E5%8A%9B%E5%AD%A6/) [无监督](https://kexue.fm/tag/%E6%97%A0%E7%9B%91%E7%9D%A3/) [扩散](https://kexue.fm/tag/%E6%89%A9%E6%95%A3/) [几何](https://kexue.fm/tag/%E5%87%A0%E4%BD%95/) [节日](https://kexue.fm/tag/%E8%8A%82%E6%97%A5/) [生活](https://kexue.fm/tag/%E7%94%9F%E6%B4%BB/) [文本生成](https://kexue.fm/tag/%E6%96%87%E6%9C%AC%E7%94%9F%E6%88%90/) [数论](https://kexue.fm/tag/%E6%95%B0%E8%AE%BA/)

### 随机文章

- [变分自编码器（五）：VAE + BN = 更好的VAE](https://kexue.fm/archives/7381)
- [不用L约束又不会梯度消失的GAN，了解一下？](https://kexue.fm/archives/6163)
- [【NASA每日一图】经典的猎户座星云](https://kexue.fm/archives/102)
- [【搜出来的文本】⋅（一）从文本生成到搜索采样](https://kexue.fm/archives/8062)
- [算符的艺术：差分、微分与伯努利数](https://kexue.fm/archives/3018)
- [GlobalPointer下的“KL散度”应该是怎样的？](https://kexue.fm/archives/9039)
- [OCR技术浅探：1. 全文简述](https://kexue.fm/archives/3774)
- [更别致的词向量模型(六)：代码、分享与结语](https://kexue.fm/archives/4681)
- [费曼路径积分思想的发展(二)](https://kexue.fm/archives/1846)
- [关于自由落体公式的简单修正](https://kexue.fm/archives/584)

### 最近评论

- [苏剑林](https://kexue.fm/archives/481/comment-page-1#comment-27835): 刚入门那会的文章，不用深究了。
- [苏剑林](https://kexue.fm/archives/10122/comment-page-1#comment-27834): 目前各方面的实测效果看来不会，我觉得本质上就是因为partial rope的实测效果优于rop...
- [苏剑林](https://kexue.fm/archives/10958/comment-page-1#comment-27833): 首先，瞬时速度为什么跟$t$无关？其次，现在reflow和meanflow的第一、第二目标，不...
- [苏剑林](https://kexue.fm/archives/10907/comment-page-2#comment-27832): 对于每一步数据都严格对齐来说，0.01的loss差距不小了，因为它代表了每一个step的los...
- [苏剑林](https://kexue.fm/archives/10699/comment-page-1#comment-27831): \[comment=27808\]rpsun\[/comment\]
有人这样做了：https://a...
- [苏剑林](https://kexue.fm/archives/1490/comment-page-1#comment-27830): 自己都没怎么关注天象了，惭愧
- [苏剑林](https://kexue.fm/archives/10958/comment-page-1#comment-27829): 原来如此。其实只要预测空间是连续空间，并且任务本质是一对多的输出，那么都有可能关联到Diffu...
- [苏剑林](https://kexue.fm/archives/10945/comment-page-1#comment-27828): 你可以拿一批语料去eval，看各个expert分别激活了多少次呀。
- [苏剑林](https://kexue.fm/archives/9164/comment-page-4#comment-27827): 首先这个分布肯定是存在的（贝叶斯公式无关分布），然后它的概率密度对数是二次函数形式，概率密度的...
- [苏剑林](https://kexue.fm/archives/10907/comment-page-2#comment-27826): 这两天看了下FoPE，感觉它的分析有点道理，但它实现的代码跟论文其实是不一样的。看论文的描述，...

### 友情链接

- [Cool Papers](https://papers.cool)
- [数学研发](https://bbs.emath.ac.cn)
- [Seatop](http://www.seatop.com.cn/)
- [Xiaoxia](https://xiaoxia.org/)
- [积分表-网络版](https://kexue.fm/sci/integral/index.html)
- [丝路博傲](http://blog.dvxj.com/)
- [ph4ntasy 饭特稀](http://www.ph4ntasy.com/)
- [数学之家](http://www.2math.cn/)
- [有趣天文奇观](http://interesting-sky.china-vo.org/)
- [TwistedW](http://www.twistedwg.com/)
- [godweiyang](https://godweiyang.com/)
- [AI柠檬](https://blog.ailemon.net/)
- [王登科-DK博客](https://greatdk.com)
- [ESON](https://blog.eson.org/)
- [枫之羽](https://fzhiy.net/)
- [Mathor's blog](https://wmathor.com/)
- [coding-zuo](https://coding-zuo.github.io/)
- [博科园](https://www.bokeyuan.net/)
- [孔皮皮的博客](https://www.kppkkp.top/)
- [运鹏的博客](https://yunpengtai.top/)
- [jiming.site](https://jiming.site/)
- [OmegaXYZ](https://www.omegaxyz.com/)
- [Blog by Eacls](https://www.eacls.top/)
- [EAI猩球](https://www.robotech.ink/)
- [文举的博客](https://liwenju0.com/)
- [用代码打点酱油](https://bruceyuan.com/)
- [申请链接](https://kexue.fm/links.html)

本站采用创作共用版权协议，要求署名、非商业用途和保持一致。转载本站内容必须也遵循“ [署名-非商业用途-保持一致](http://creativecommons.org/licenses/by-nc-nd/2.5/cn/)”的创作共用协议。
© 2009-2025 Scientific Spaces. All rights reserved. Theme by [laogui](http://www.laogui.com). Powered by [Typecho](http://typecho.org). 备案号: [粤ICP备09093259号-1/2](https://beian.miit.gov.cn/)。