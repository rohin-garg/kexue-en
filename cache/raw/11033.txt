## SEARCH

## MENU

- [打赏](https://kexue.fm/reward.html)
- [公式](https://kexue.fm/latex.html)
- [天象](https://kexue.fm/ac.html)
- [链接](https://kexue.fm/links.html)
- [时光](https://kexue.fm/me.html)
- [博览](https://kexue.fm/science.html)
- [归档](https://kexue.fm/content.html)

## CATEGORIES

- [千奇百怪](https://kexue.fm/category/Everything)
- [天文探索](https://kexue.fm/category/Astronomy)
- [数学研究](https://kexue.fm/category/Mathematics)
- [物理化学](https://kexue.fm/category/Phy-chem)
- [信息时代](https://kexue.fm/category/Big-Data)
- [生物自然](https://kexue.fm/category/Biology)
- [图片摄影](https://kexue.fm/category/Photograph)
- [问题百科](https://kexue.fm/category/Questions)
- [生活/情感](https://kexue.fm/category/Life-Feeling)
- [资源共享](https://kexue.fm/category/Resources)

## NEWPOSTS

- [线性注意力简史：从模仿、创新到反哺](https://kexue.fm/archives/11033)
- [msign的导数](https://kexue.fm/archives/11025)
- [通过msign来计算mclip（奇...](https://kexue.fm/archives/11006)
- [msign算子的Newton-Sc...](https://kexue.fm/archives/10996)
- [等值振荡定理：最优多项式逼近的充要条件](https://kexue.fm/archives/10972)
- [生成扩散模型漫谈（三十）：从瞬时速...](https://kexue.fm/archives/10958)
- [MoE环游记：5、均匀分布的反思](https://kexue.fm/archives/10945)
- [msign算子的Newton-Sc...](https://kexue.fm/archives/10922)
- [Transformer升级之路：2...](https://kexue.fm/archives/10907)
- [一道概率不等式：盯着它到显然成立为止！](https://kexue.fm/archives/10902)

## COMMENTS

- [忍者猫: 这优化器的作者真的应该给你打钱](https://kexue.fm/archives/10592/comment-page-2#comment-27952)
- [Chaofa Yuan: 写得太好了](https://kexue.fm/archives/11033/comment-page-1#comment-27951)
- [Skyler Lin: respect苏神！](https://kexue.fm/archives/11033/comment-page-1#comment-27949)
- [宋佳铭: 对，个人感觉mean flow就是continuous tim...](https://kexue.fm/archives/10958/comment-page-1#comment-27947)
- [宋佳铭: 的确，对sg这个事情我感觉如果是用‘归纳’法做是不太能避免的，...](https://kexue.fm/archives/10958/comment-page-1#comment-27946)
- [MoFHeka: 苏老师您好，请问一下这套结论在稀疏参数上应该如何应用？比如大规...](https://kexue.fm/archives/10542/comment-page-1#comment-27945)
- [苏剑林: Temp LoRA倒是有印象，其实思想是一样的，如果我单独开一...](https://kexue.fm/archives/11033/comment-page-1#comment-27944)
- [苏剑林: 你搜搜mamba、rwkv甚至rnn做vision的工作，其实...](https://kexue.fm/archives/11033/comment-page-1#comment-27943)
- [苏剑林: 问题1可以看看 https://kexue.fm/archiv...](https://kexue.fm/archives/9379/comment-page-1#comment-27942)
- [苏剑林: 你的“信息量”怎么定义？直观来说，reflow训练的是切线模型...](https://kexue.fm/archives/10958/comment-page-2#comment-27941)

## USERLOGIN

- [登录](https://kexue.fm/admin/login.php)

[科学空间\|Scientific Spaces](https://kexue.fm)

- [登录](https://kexue.fm/admin/login.php)
- [打赏](https://kexue.fm/reward.html)
- [公式](https://kexue.fm/latex.html)
- [天象](https://kexue.fm/ac.html)
- [链接](https://kexue.fm/links.html)
- [时光](https://kexue.fm/me.html)
- [博览](https://kexue.fm/science.html)
- [归档](https://kexue.fm/content.html)

渴望成为一个小飞侠

- [欢迎订阅](https://kexue.fm/feed)
- [个性邮箱](https://kexue.fm/archives/119)
- [天象信息](https://kexue.fm/ac.html)
- [观测ISS](https://kexue.fm/archives/41)
- [LaTeX](https://kexue.fm/latex.html)
- [关于博主](https://kexue.fm/me.html)

欢迎访问“科学空间”，这里将与您共同探讨自然科学，回味人生百态；也期待大家的分享～

- [**千奇百怪** Everything](https://kexue.fm/category/Everything)
- [**天文探索** Astronomy](https://kexue.fm/category/Astronomy)
- [**数学研究** Mathematics](https://kexue.fm/category/Mathematics)
- [**物理化学** Phy-chem](https://kexue.fm/category/Phy-chem)
- [**信息时代** Big-Data](https://kexue.fm/category/Big-Data)
- [**生物自然** Biology](https://kexue.fm/category/Biology)
- [**图片摄影** Photograph](https://kexue.fm/category/Photograph)
- [**问题百科** Questions](https://kexue.fm/category/Questions)
- [**生活/情感** Life-Feeling](https://kexue.fm/category/Life-Feeling)
- [**资源共享** Resources](https://kexue.fm/category/Resources)

- [**千奇百怪**](https://kexue.fm/category/Everything)
- [**天文探索**](https://kexue.fm/category/Astronomy)
- [**数学研究**](https://kexue.fm/category/Mathematics)
- [**物理化学**](https://kexue.fm/category/Phy-chem)
- [**信息时代**](https://kexue.fm/category/Big-Data)
- [**生物自然**](https://kexue.fm/category/Biology)
- [**图片摄影**](https://kexue.fm/category/Photograph)
- [**问题百科**](https://kexue.fm/category/Questions)
- [**生活/情感**](https://kexue.fm/category/Life-Feeling)
- [**资源共享**](https://kexue.fm/category/Resources)

[首页](https://kexue.fm) [信息时代](https://kexue.fm/category/Big-Data) 线性注意力简史：从模仿、创新到反哺

20Jun

# [线性注意力简史：从模仿、创新到反哺](https://kexue.fm/archives/11033)

By 苏剑林 \|
2025-06-20 \|
4502位读者\|

在中文圈，本站应该算是比较早关注线性Attention的了，在2020年写首篇相关博客 [《线性Attention的探索：Attention必须有个Softmax吗？》](https://kexue.fm/archives/7546) 时，大家主要讨论的还是BERT相关的Softmax Attention。事后来看，在BERT时代考虑线性Attention并不是太明智，因为当时训练长度比较短，且模型主要还是Encoder，用线性Attention来做基本没有优势。对此，笔者也曾撰文 [《线性Transformer应该不是你要等的那个模型》](https://kexue.fm/archives/8610) 表达这一观点。

直到ChatGPT的出世，倒逼大家都去做Decoder-only的生成式模型，这跟线性Attention的RNN形式高度契合。同时，追求更长的训练长度也使得Softmax Attention的二次复杂度瓶颈愈发明显。在这样的新背景下，线性Attention越来越体现出竞争力，甚至出现了“反哺”Softmax Attention的迹象。

## 平方复杂度 [\#](https://kexue.fm/archives/11033\#%E5%B9%B3%E6%96%B9%E5%A4%8D%E6%9D%82%E5%BA%A6)

首先引入一些记号：
\\begin{equation}\\begin{gathered}
\\boldsymbol{q}\_i,\\boldsymbol{k}\_i,\\boldsymbol{v}\_i,\\boldsymbol{o}\_i \\in \\mathbb{R}^{d\\times 1} \\\\[6pt\]
\\boldsymbol{Q}=\[\\boldsymbol{q}\_1,\\boldsymbol{q}\_2,\\cdots,\\boldsymbol{q}\_n\]^{\\top}\\in\\mathbb{R}^{n\\times d} \\\\[6pt\]
\\boldsymbol{K}=\[\\boldsymbol{k}\_1,\\boldsymbol{k}\_2,\\cdots,\\boldsymbol{k}\_n\]^{\\top}\\in\\mathbb{R}^{n\\times d} \\\\[6pt\]
\\boldsymbol{V}=\[\\boldsymbol{v}\_1,\\boldsymbol{v}\_2,\\cdots,\\boldsymbol{v}\_n\]^{\\top}\\in\\mathbb{R}^{n\\times d} \\\\[6pt\]
\\boldsymbol{O}=\[\\boldsymbol{o}\_1,\\boldsymbol{o}\_2,\\cdots,\\boldsymbol{o}\_n\]^{\\top}\\in\\mathbb{R}^{n\\times d} \\\\[6pt\]
\\end{gathered}\\end{equation}
一个Attention模型，本质上是一个$\\boldsymbol{Q},\\boldsymbol{K},\\boldsymbol{V}\\to \\boldsymbol{O}$的映射。本文主要关心Causal场景，这意味着$\\boldsymbol{o}\_t$至多跟$\\boldsymbol{Q}\_{\[:t\]},\\boldsymbol{K}\_{\[:t\]},\\boldsymbol{V}\_{\[:t\]}$相关。原则上，$\\boldsymbol{Q},\\boldsymbol{K}$的$d$与$\\boldsymbol{V},\\boldsymbol{O}$的$d$可以不一致，比如 [GAU](https://kexue.fm/archives/8934) 和 [MLA](https://kexue.fm/archives/10091) 便是如此，但将它们简化成同一个并不改变问题本质。

标准的Softmax Attention，通常是指 [《Attention is All You Need》](https://kexue.fm/archives/4765) 所提的Attention机制：
\\begin{equation}\\boldsymbol{O} = \\mathop{\\text{softmax}}(\\boldsymbol{Q}\\boldsymbol{K}^{\\top} + \\log \\boldsymbol{M})\\boldsymbol{V}\\end{equation}
这里省略了缩放因子$1/\\sqrt{d}$，因为它总可以吸收到$\\boldsymbol{Q},\\boldsymbol{K}$里边，$\\mathop{\\text{softmax}}$是对第二个维度进行指数归一化，而$\\boldsymbol{M}\\in\\mathbb{R}^{n\\times n}$是一个下三角阵，称为掩码矩阵，定义为
\\begin{equation}M\_{i,j} = \\left\\{\\begin{aligned} &1, &i \\geq j \\\ &0, &i < j\\end{aligned}\\right.\\end{equation}
$\\log\\boldsymbol{M}$是指对$\\boldsymbol{M}$的分量逐一取$\\log$，其中$\\log 0 = -\\infty$。Softmax Attention用分量形式写出来则是
\\begin{equation}\\boldsymbol{o}\_t = \\frac{\\sum\_{j=1}^t \\exp(\\boldsymbol{q}\_t^{\\top}\\boldsymbol{k}\_j) \\boldsymbol{v}\_j}{\\sum\_{j=1}^t \\exp(\\boldsymbol{q}\_t^{\\top}\\boldsymbol{k}\_j) }\\end{equation}
其中分母的作用主要是保持数值稳定性，另外就是如果我们给$\\boldsymbol{O}$加上RMSNorm，那么分母也会自动消去，所以Softmax Attention的核心是分子部分，即
\\begin{equation}\\boldsymbol{O} = \\exp(\\boldsymbol{Q}\\boldsymbol{K}^{\\top} + \\log \\boldsymbol{M})\\boldsymbol{V} = (\\exp(\\boldsymbol{Q}\\boldsymbol{K}^{\\top})\\odot \\boldsymbol{M})\\boldsymbol{V}\\end{equation}
其中$\\odot$是Hadamard积，$\\exp$是逐分量取指数。不难看出，分母其实就是将$\\boldsymbol{V}$换成一个$n\\times 1$的全1矩阵，如果有需要，我们再补上即可。Softmax Attention的标准实现需要把$n\\times n$的矩阵$\\exp(\\boldsymbol{Q}\\boldsymbol{K}^{\\top})$算出来，所以空间和时间复杂度都正比于$n^2$。 [Flash Attention](https://papers.cool/arxiv/2205.14135) 的出现降低了空间需求，但平方的时间复杂度依然无法避免。

## 最初的模样 [\#](https://kexue.fm/archives/11033\#%E6%9C%80%E5%88%9D%E7%9A%84%E6%A8%A1%E6%A0%B7)

线性Attention最早的思路主要是模仿和近似Softmax Attention，其中最简单的方案是直接去掉$\\exp$，得到
\\begin{equation}\\boldsymbol{O} = ((\\boldsymbol{Q}\\boldsymbol{K}^{\\top})\\odot \\boldsymbol{M})\\boldsymbol{V}\\label{eq:linear-attn}\\end{equation}
为什么这个形式是“线性”Attention的呢？为了快速理解这一点，我们不妨先考虑去掉$\\odot \\boldsymbol{M}$的非Causal版，此时$\\boldsymbol{O} = (\\boldsymbol{Q}\\boldsymbol{K}^{\\top})\\boldsymbol{V} = \\boldsymbol{Q}(\\boldsymbol{K}^{\\top}\\boldsymbol{V})$，注意计算$\\boldsymbol{K}^{\\top}\\boldsymbol{V}$的复杂度是$\\mathcal{O}(nd^2)$，结果是$d\\times d$矩阵，然后跟$\\boldsymbol{Q}$相乘复杂度也是$\\mathcal{O}(nd^2)$，所以它复杂度是线性依赖于$n$。

至于Causal版$\\eqref{eq:linear-attn}$，我们可以从分量形式理解，写出：
\\begin{equation}\\boldsymbol{o}\_t = \\sum\_{j=1}^t \\boldsymbol{v}\_j (\\boldsymbol{k}\_j^{\\top} \\boldsymbol{q}\_t) = \\sum\_{j=1}^t (\\boldsymbol{v}\_j \\boldsymbol{k}\_j^{\\top}) \\boldsymbol{q}\_t = \\left(\\sum\_{j=1}^t \\boldsymbol{v}\_j \\boldsymbol{k}\_j^{\\top}\\right) \\boldsymbol{q}\_t\\end{equation}
如果我们记括号部分为$\\boldsymbol{S}\_t$，那么有
\\begin{equation}\\boldsymbol{o}\_t = \\boldsymbol{S}\_t \\boldsymbol{q}\_t, \\qquad \\boldsymbol{S}\_t = \\boldsymbol{S}\_{t-1} + \\boldsymbol{v}\_t \\boldsymbol{k}\_t^{\\top}\\label{eq:linear-attn-rnn}\\end{equation}
由此可见，Causal形式的Attention可以写成一个以$\\boldsymbol{S}\_t$为State的线性RNN，因此每一步的复杂度是常数，总的复杂度正比于序列长度$n$。注意这里出现了“线性RNN”，它是更广义的概念，线性Attention属于线性RNN的一种，线性RNN也单独发展过一段时间，比如之前介绍过的 [LRU](https://kexue.fm/archives/9554)、 [SSM](https://kexue.fm/tag/ssm/) 等，但最近比较有竞争力的线性架构都具有线性Attention的形式。

早年的线性Attention还有一些非常明显的模仿Softmax Attention的特点，比如会给式$\\eqref{eq:linear-attn}$加入分母来归一化，而为了归一化，那么$\\boldsymbol{k}\_j^{\\top} \\boldsymbol{q}\_t$就必须非负，于是又给$\\boldsymbol{Q},\\boldsymbol{K}$加上了非负的激活函数，以 [Performer](https://kexue.fm/archives/7921)、 [RFA](https://papers.cool/arxiv/2103.02143) 为代表的一系列工作，更是以近似$\\exp(\\boldsymbol{Q}\\boldsymbol{K}^{\\top})$为出发点来构建模型。

然而，后来的研究如 [《The Devil in Linear Transformer》](https://papers.cool/arxiv/2210.10340) 发现，在序列长度维度归一化并不能完全避免数值不稳定性，倒不如直接事后归一化，如
\\begin{equation}\\boldsymbol{O} = \\mathop{\\text{RMSNorm}}(((\\boldsymbol{Q}\\boldsymbol{K}^{\\top})\\odot \\boldsymbol{M})\\boldsymbol{V})\\end{equation}
而既然不用归一化，那么给$\\boldsymbol{Q},\\boldsymbol{K}$加非负的激活函数来保证$\\boldsymbol{k}\_j^{\\top} \\boldsymbol{q}\_t$非负就非必须了。那给$\\boldsymbol{Q},\\boldsymbol{K}$加（不一定非负的）激活函数还有意义吗？笔者的观点是，加激活函数是大家的自由，不排除加某个激活函数能够调出更好的效果，但加激活函数并不改变线性Attention的形式，所以不影响我们的描述，另外就是现有的结果表明，其实不加已经足够好。

## 花式遗忘门 [\#](https://kexue.fm/archives/11033\#%E8%8A%B1%E5%BC%8F%E9%81%97%E5%BF%98%E9%97%A8)

从式$\\eqref{eq:linear-attn-rnn}$我们可以看出，目前的线性Attention本质上就是个$\\mathop{\\text{cumsum}}$，即将所有历史信息都等权地叠加，不难想象当叠加的token足够多时，每个token的信息占比都会变得极小，于是单靠固定大小的$\\boldsymbol{S}\_t$矩阵甚至无法准确重建任意一个token，直观类比就是每个token的记忆都变得模糊不清。

为了缓解这个问题， [RetNet](https://papers.cool/arxiv/2307.08621) 给线性Attention引入了遗忘效应：
\\begin{equation}\\boldsymbol{o}\_t = \\boldsymbol{S}\_t \\boldsymbol{q}\_t, \\qquad \\boldsymbol{S}\_t = \\gamma\\boldsymbol{S}\_{t-1} + \\boldsymbol{v}\_t \\boldsymbol{k}\_t^{\\top}\\label{eq:linear-attn-retnet}\\end{equation}
其中衰减因子$\\gamma\\in(0,1)$，在RetNet中被设为常数，也有设为可训练参数的，以及将$\\gamma$改为对角矩阵的，等等， [MiniMax-01](https://papers.cool/arxiv/2501.08313) 所用的线性Attention也是这种。注意，衰减因子在RetNet前也有，不过它们多以线性RNN的形式出现，如上一节提到的 [LRU](https://kexue.fm/archives/9554)、 [SSM](https://kexue.fm/tag/ssm/) 等，RetNet应该是首次将它跟线性Attention结合起来。加入衰减因子后，模型会倾向于遗忘掉更为久远的历史信息，从而至少保证最近token的分辨率，说白了就是跟语言模型特性相符的“就近原则（Recency Bias）”的体现，从而往往能工作得更好。

此外，一个值得关注的细节是RetNet还给$\\boldsymbol{Q},\\boldsymbol{K}$加上了 [RoPE](https://kexue.fm/archives/9403)，这相当于将衰减因子推广到复数$\\gamma e^{\\text{i}\\theta}$，从 [LRU](https://kexue.fm/archives/9554) 的角度看则是考虑了复数的特征值。尽管给RNN加位置编码的操作看上去似乎有点违和，但有些实验比如最近的 [TransXSSM](https://papers.cool/arxiv/2506.09507) 表明，给线性Attention加RoPE也有一定的正面作用。当然，这可能取决于具体的模型变体和实验设置。

式$\\eqref{eq:linear-attn-retnet}$的一个简单推广是将$\\gamma$更换为位置$t$的函数$\\gamma\_t$，这在 [SSM](https://kexue.fm/tag/ssm/) 中已经有所体现。后来， [DFW](https://papers.cool/arxiv/2210.04243)、 [Mamba](https://papers.cool/arxiv/2312.00752)、 [Mamba2](https://papers.cool/arxiv/2405.21060) 等工作，将它推广成跟输入相关，形成了“data-dependent decay”相关的一系列工作，这跟以往GRU、LSTM等非线性RNN的“遗忘门（forget gate）”其实已经非常相似了，只不过为了保持模型的线性性，去掉了遗忘门对State（如$\\boldsymbol{S}\_t$）的依赖。

为什么我们偏爱线性RNN呢？因为线性RNN基本都能找到某种方式来并行训练，这使得它相比Softmax Attention更具竞争力——在训练效率和推理效率上都不逊色。其中，并行化的“通解”是转化为 [Prefix Sum](https://en.wikipedia.org/wiki/Prefix_sum) 问题然后Associative Scan，大体思路我们在 [《Google新作试图“复活”RNN：RNN能否再次辉煌？》](https://kexue.fm/archives/9554) 的“并行化”一节也简单介绍过。

然而，“通解”并不是GPU高效的，GPU最高效的是矩阵乘法，所以找到大量使用矩阵乘法的并行算法是最理想的，甚至都不用并行，只要找到充分使用矩阵乘法的Chunk by Chunk递归格式，都能明显提高训练效率。这反过来对模型提出了要求，如只有外积形式的遗忘门才能实现这个目的，典型反例就是Mamba，它是非外积的遗忘门，无法充分发挥GPU的性能，所以才有了后续Mamba2和 [GLA](https://papers.cool/arxiv/2312.06635) 等变化。

## 测试时训练 [\#](https://kexue.fm/archives/11033\#%E6%B5%8B%E8%AF%95%E6%97%B6%E8%AE%AD%E7%BB%83)

至此，线性Attention从最初的简单模仿Softmax Attention，到引入静态衰减因子乃至“data-dependent decay”，已经形成了自身的特色并在不少任务上发挥价值。然而，这些进展多数是靠人工凭经验设计出来的，我们不禁要问： **有没有更上层的原则来指导线性Attention甚至是一般的序列模型（Token-Mixer）的设计？**

对于这个问题， [TTT（Test Time Training）](https://papers.cool/arxiv/2407.04620) 给出了自己的答案，它将序列模型的构建视为一个“在线学习（Online Learning）”问题，并提出用优化器来构建（不一定是线性的）RNN的做法。具体来说，它将$\\boldsymbol{K},\\boldsymbol{V}$视作语料对$(\\boldsymbol{k}\_1, \\boldsymbol{v}\_1),(\\boldsymbol{k}\_2, \\boldsymbol{v}\_2),\\cdots,(\\boldsymbol{k}\_t, \\boldsymbol{v}\_t)$，根据这些语料训练得到一个模型$\\boldsymbol{v} = \\boldsymbol{f}(\\boldsymbol{S}\_t;\\boldsymbol{k})$，最后输出$\\boldsymbol{o}\_t = \\boldsymbol{f}(\\boldsymbol{S}\_t;\\boldsymbol{q}\_t)$，其中$\\boldsymbol{S}\_t$是模型参数，至于模型结构很大程度上是任意的。

这跟RNN有什么关系呢？很简单，优化器如SGD、Adam等，它们本质上就是一个关于模型参数的RNN！其实这个观点并不新鲜，早在2017年Meta Learning盛行那会就已经有研究人员提出并利用了这点，只不过当时的想法是尝试用RNN（LSTM）去模拟一个更好的优化器，详情可以参考 [《Optimization as a Model for Few-Shot Learning》](https://openreview.net/forum?id=rJY0-Kcll)。

正所谓“风水轮流转”，时隔多年TTT反过来提出通过优化器来构建RNN。它的流程是这样的：首先，当前模型参数为$\\boldsymbol{S}\_{t-1}$，优化器（SGD）接收到新数据$(\\boldsymbol{k}\_t, \\boldsymbol{v}\_t)$，根据该数据将模型参数更新为$\\boldsymbol{S}\_t$，最后返回$\\boldsymbol{q}\_t$的预测结果$\\boldsymbol{f}(\\boldsymbol{S}\_{t-1};\\boldsymbol{q}\_t)$，依此类推。所以，TTT所实现的RNN可以统一地写成
\\begin{equation}\\boldsymbol{o}\_t = \\boldsymbol{f}(\\boldsymbol{S}\_t; \\boldsymbol{q}\_t), \\qquad \\boldsymbol{S}\_t = \\boldsymbol{S}\_{t-1} - \\eta\_t\\nabla\_{\\boldsymbol{S}\_{t-1}}\\mathcal{L}(\\boldsymbol{f}(\\boldsymbol{S}\_{t-1};\\boldsymbol{k}\_t), \\boldsymbol{v}\_t)\\label{eq:ttt-rnn}\\end{equation}
其中$\\mathcal{L}(\\boldsymbol{f}(\\boldsymbol{S}\_{t-1};\\boldsymbol{k}\_t), \\boldsymbol{v}\_t)$是当前数据$(\\boldsymbol{k}\_t, \\boldsymbol{v}\_t)$在当前参数$\\boldsymbol{S}\_{t-1}$下的损失函数，$\\eta\_t$则是学习率参数，参考上一节的“data-dependent decay”，它也可以做成data-dependent的。这个形式可以覆盖非常多的RNN模型，比如式$\\eqref{eq:linear-attn-rnn}$和$\\eqref{eq:linear-attn-retnet}$都是它的一个特例：
$$\\begin{array}{c\|cc\|ccc}
\\hline
& \\text{RNN} & \\boldsymbol{o}\_t & \\boldsymbol{f}(\\boldsymbol{S};\\boldsymbol{k}) & \\mathcal{L}(\\boldsymbol{f}(\\boldsymbol{S};\\boldsymbol{k}),\\boldsymbol{v}) & \\eta\_t \\\
\\hline
\\eqref{eq:linear-attn-rnn} & \\boldsymbol{S}\_t = \\boldsymbol{S}\_{t-1} + \\boldsymbol{v}\_t \\boldsymbol{k}\_t^{\\top} & \\boldsymbol{o}\_t = \\boldsymbol{S}\_t \\boldsymbol{q}\_t & \\boldsymbol{S}\\boldsymbol{k} & -\\boldsymbol{v}^{\\top}(\\boldsymbol{S}\\boldsymbol{k}) & 1 \\\
\\eqref{eq:linear-attn-retnet} & \\boldsymbol{S}\_t = \\gamma\\boldsymbol{S}\_{t-1} + \\boldsymbol{v}\_t \\boldsymbol{k}\_t^{\\top} & \\boldsymbol{o}\_t = \\boldsymbol{S}\_t \\boldsymbol{q}\_t & \\boldsymbol{S}\\boldsymbol{k} & -\\boldsymbol{v}^{\\top}(\\boldsymbol{S}\\boldsymbol{k}) + \\frac{1-\\gamma}{2}\\Vert\\boldsymbol{S}\\Vert\_F^2 & 1 \\\
\\hline
\\end{array}$$
TTT原文则致力于探索mini-batch下的非线性RNN，后来的 [Titans](https://papers.cool/arxiv/2501.00663) 则给TTT的SGD加上了动量，再后面 [《Test-Time Training Done Right》](https://papers.cool/arxiv/2505.23884) 则探索了large-batch的TTT用法，还探索了“TTT + Muon”的组合。注意，TTT只是利用优化器来构建RNN，RNN以外的参数如$\\boldsymbol{Q},\\boldsymbol{K},\\boldsymbol{V}$的可训练参数，还是将整个模型构建起来后用整体的优化器训练的。

一个更值得思考的问题是：为什么TTT可以成为构建RNN的“指导原则”呢？RNN的核心目标，是将历史数据有效地压缩到一个固定大小的State中，而模型参数正好是固定大小的，训练模型某种程度上就相当于把训练数据压缩到模型权重中，TTT正是利用了它跟RNN目标的高度契合性。说直白一点，如果将RNN视为一个压缩任务，TTT将模型$\\boldsymbol{f}$视为“解压器”，它的权重则是“压缩包”，而压缩算法则是SGD，压缩率则是损失$\\mathcal{L}$。

这样一来，我们就不用花心思构建递归格式了，转而构建模型$\\boldsymbol{f}$和损失$\\mathcal{L}$，一个RNN强不强、靠不靠谱，我们也只需看对应的$\\boldsymbol{f}$和$\\mathcal{L}$就可以心中有数。

除此之外，TTT用Online Learning构建RNN，意味着所得RNN必然非常契合ICL（In Context Learning）任务，这也是TTT作为“指导原则”的优势。此前 [《Why Can GPT Learn In-Context? Language Models Implicitly Perform Gradient Descent as Meta-Optimizers》](https://papers.cool/arxiv/2212.10559) 甚至反过来，将Softmax Attention去掉Softmax成线性Attention来解释它的ICL能力，用现在的视角看它就是构造了对应的TTT出来。

## 除旧而迎新 [\#](https://kexue.fm/archives/11033\#%E9%99%A4%E6%97%A7%E8%80%8C%E8%BF%8E%E6%96%B0)

例如，最早的线性Attention对应的损失函数是$-\\boldsymbol{v}^{\\top}(\\boldsymbol{S}\\boldsymbol{k})$，这一看就是个不大靠谱的目标，因为它是无下界的，这可能会导致$\\boldsymbol{S}$趋于无穷。相比之下，RetNet往损失函数加入了L2正则项，避免了这种风险，从优化角度看也缓解了过拟合的风险，从而得到一个更好的RNN。

然而，用内积作为损失函数虽然简洁且有一定道理，但它不是直接鼓励$\\boldsymbol{S}\\boldsymbol{k}=\\boldsymbol{v}$，所以并非一个理想的回归损失。更好的目标函数应该是平方损失，即$\\frac{1}{2}\\Vert\\boldsymbol{S}\\boldsymbol{k} - \\boldsymbol{v}\\Vert^2$，将它代入到TTT的公式$\\eqref{eq:ttt-rnn}$得到
\\begin{equation}\\boldsymbol{o}\_t = \\boldsymbol{f}(\\boldsymbol{S}\_t; \\boldsymbol{q}\_t), \\qquad \\boldsymbol{S}\_t = \\boldsymbol{S}\_{t-1} - \\eta\_t \\underbrace{(\\boldsymbol{S}\_{t-1} \\boldsymbol{k}\_t - \\boldsymbol{v}\_t)\\boldsymbol{k}\_t^{\\top}}\_{\\nabla\_{\\boldsymbol{S}\_{t-1}}\\frac{1}{2}\\Vert\\boldsymbol{S}\_{t-1}\\boldsymbol{k}\_t - \\boldsymbol{v}\_t\\Vert^2}\\end{equation}
这便是DeltaNet，这个名字出自 [《Parallelizing Linear Transformers with the Delta Rule over Sequence Length》](https://papers.cool/arxiv/2406.06484)，更早则是由 [《Linear Transformers Are Secretly Fast Weight Programmers》](https://papers.cool/arxiv/2102.11174) 提出。留意到$\\eta\_t (\\boldsymbol{S}\_{t-1} \\boldsymbol{k}\_t - \\boldsymbol{v}\_t)\\boldsymbol{k}\_t^{\\top} = (\\boldsymbol{S}\_{t-1} (\\sqrt{\\eta\_t}\\boldsymbol{k}\_t) - (\\sqrt{\\eta\_t}\\boldsymbol{v}\_t))(\\sqrt{\\eta\_t}\\boldsymbol{k}\_t)^{\\top}$，这意味着$\\eta\_t$总可以吸收到$\\boldsymbol{k}\_t,\\boldsymbol{v}\_t$的定义中去，所以我们接下来的分析都只考虑$\\eta\_t=1$的情况：
\\begin{equation}\\begin{aligned}
\\boldsymbol{S}\_t =&\\, \\boldsymbol{S}\_{t-1} -(\\boldsymbol{S}\_{t-1} \\boldsymbol{k}\_t - \\boldsymbol{v}\_t)\\boldsymbol{k}\_t^{\\top} \\\
=&\\, \\boldsymbol{S}\_{t-1} -(\\boldsymbol{S}\_{t-1} \\boldsymbol{k}\_t)\\boldsymbol{k}\_t^{\\top} + \\boldsymbol{v}\_t\\boldsymbol{k}\_t^{\\top} \\\
=&\\, \\boldsymbol{S}\_{t-1} (\\boldsymbol{I} - \\boldsymbol{k}\_t\\boldsymbol{k}\_t^{\\top}) + \\boldsymbol{v}\_t\\boldsymbol{k}\_t^{\\top}
\\end{aligned}\\label{eq:linear-attn-deltanet}\\end{equation}
如果有需要，我们再把$\\boldsymbol{k}\_t,\\boldsymbol{v}\_t$换成$\\sqrt{\\eta\_t}\\boldsymbol{k}\_t,\\sqrt{\\eta\_t}\\boldsymbol{v}\_t$，就可以将$\\eta\_t$恢复出来。对比线性Attention最早的形式$\\eqref{eq:linear-attn-rnn}$，DeltaNet的区别是在加$\\boldsymbol{v}\_t\\boldsymbol{k}\_t^{\\top}$前多减了个$(\\boldsymbol{S}\_{t-1} \\boldsymbol{k}\_t)\\boldsymbol{k}\_t^{\\top}$，其中$\\boldsymbol{S}\_{t-1} \\boldsymbol{k}\_t$可以理解为新输入$\\boldsymbol{k}\_t$在旧模型$\\boldsymbol{S}\_{t-1}$下的预测结果。

直观来想，“先减后加”就是先移除模型对$\\boldsymbol{k}\_t$的旧认知，然后根据$(\\boldsymbol{k}\_t,\\boldsymbol{v}\_t)$补充新认知，达到“除旧迎新”的效果。这个规则称为“ [Delta Rule](https://en.wikipedia.org/wiki/Delta_rule)”，正是DeltaNet一词中“Delta”的来源。Delta Rule并不新鲜，它又称为 [Least Mean Square](https://en.wikipedia.org/wiki/Least_mean_squares_filter)、Widrow-Hoff Algorithm等，已经是上个世纪60年代的了。事实上，这个领域完全新的东西很少，很多改动都可以追溯到某个“上古时期”的工作，目前的努力主要集中在挖掘其中能Scalable的部分。

另外需要指出的是，按照时间的顺序，是DeltaNet在前，TTT在后，从Online Learning角度理解RNN，其实在TTT之前已经零星地体现在一些工作中，但TTT系统地提出了这个“指导原则”，并且将它用于构建新RNN模型，所以我们把TTT放在前面，使得整个介绍更加流畅自然一些。

有些读者可能疑问：DeltaNet还算线性RNN吗？答案是肯定的。我们所说的线性RNN，是指递归公式对State变量的依赖关系是线性的，但对输入或$\\boldsymbol{q},\\boldsymbol{k},\\boldsymbol{v}$的依赖可以是非线性的（当然不同依赖形式的并行效率会有所不同），从式$\\eqref{eq:linear-attn-deltanet}$可以看出，等号右端始终只是出现了$\\boldsymbol{S}\_{t-1}$的一次方，所以它满足线性的定义。

## 求逆来相助 [\#](https://kexue.fm/archives/11033\#%E6%B1%82%E9%80%86%E6%9D%A5%E7%9B%B8%E5%8A%A9)

前面我们说了，线性RNN最理想的（即GPU高效的）并行算法是充分使用矩阵乘法的形式。为了完成这一目标，我们先将DeltaNet写成
\\begin{equation}\\boldsymbol{S}\_t = \\boldsymbol{S}\_{t-1} + (\\boldsymbol{v}\_t - \\boldsymbol{S}\_{t-1} \\boldsymbol{k}\_t)\\boldsymbol{k}\_t^{\\top}\\end{equation}
记$\\boldsymbol{u}\_t = \\boldsymbol{v}\_t - \\boldsymbol{S}\_{t-1} \\boldsymbol{k}\_t$，那么$\\boldsymbol{S}\_t = \\boldsymbol{S}\_{t-1} + \\boldsymbol{u}\_t\\boldsymbol{k}\_t^{\\top}$，也就是说它只是在最早的线性Attention基础上把$\\boldsymbol{V}$换成了$\\boldsymbol{U}=\[\\boldsymbol{u}\_1,\\boldsymbol{u}\_2,\\cdots,\\boldsymbol{u}\_n\]^{\\top}$，将它迭代$t-1$次，我们有
\\begin{equation}\\boldsymbol{S}\_{t-1} = \\sum\_{j=1}^{t-1} \\boldsymbol{u}\_j\\boldsymbol{k}\_j^{\\top}\\qquad\\Rightarrow\\qquad \\boldsymbol{u}\_t = \\boldsymbol{v}\_t - \\left(\\sum\_{j=1}^{t-1} \\boldsymbol{u}\_j\\boldsymbol{k}\_j^{\\top}\\right)\\boldsymbol{k}\_t = \\boldsymbol{v}\_t - \\sum\_{j=1}^{t-1} \\boldsymbol{u}\_j(\\boldsymbol{k}\_j^{\\top}\\boldsymbol{k}\_t)\\end{equation}
最后的等式写成矩阵形式是$\\boldsymbol{U} = \\boldsymbol{V} - ((\\boldsymbol{K}\\boldsymbol{K}^{\\top})\\odot (\\boldsymbol{M} - \\boldsymbol{I}))\\boldsymbol{U}$，这是一个线性方程组，它的解可以直接表示为
\\begin{equation}\\boldsymbol{U} = (\\boldsymbol{I} + \\underbrace{(\\boldsymbol{K}\\boldsymbol{K}^{\\top})\\odot (\\boldsymbol{M} - \\boldsymbol{I})}\_{\\text{记为}\\boldsymbol{B}})^{-1}\\boldsymbol{V}\\end{equation}
这里出现了$(\\boldsymbol{I}+\\boldsymbol{B})^{-1}$，一个$n\\times n$矩阵的逆，标准复杂度是$\\mathcal{O}(n^3)$，比Softmax Attention还高！不过好在我们不需要显式的逆而是只要$\\boldsymbol{U}$，这可以转化为解方程组$(\\boldsymbol{I}+\\boldsymbol{B})\\boldsymbol{U}=\\boldsymbol{V}$，复杂度降到$\\mathcal{O}(n^2)$。进一步地，利用$\\boldsymbol{I}+\\boldsymbol{B}$是下三角阵以及$\\boldsymbol{B}$的低秩结构，可以将复杂度降到线性，写成分块矩阵乘法后就可以充分利用GPU。这些细节只能请大家阅读原论文了，本文先把主要数学原理介绍清楚。

DeltaNet之后， [Gated DeltaNet（GDN）](https://papers.cool/arxiv/2412.06464) 进一步地将遗忘门引入到DeltaNet之中，这倒是可以预料的变化。Gated DeltaNet的原始引入方式是
\\begin{equation}\\boldsymbol{S}\_t = \\boldsymbol{S}\_{t-1} (\\alpha\_t (\\boldsymbol{I} - \\beta\_t\\boldsymbol{k}\_t\\boldsymbol{k}\_t^{\\top})) + \\beta\_t\\boldsymbol{v}\_t\\boldsymbol{k}\_t^{\\top}\\end{equation}
但个人认为，这个提法其实显式打破了Delta Rule，更好的提法应该是像 [Comba](https://papers.cool/arxiv/2506.02475) 一样，只乘到第一个$\\boldsymbol{S}\_{t-1}$上：
\\begin{equation}\\boldsymbol{S}\_t = \\gamma\_t\\boldsymbol{S}\_{t-1} + \\eta\_t(\\boldsymbol{v}\_t - \\boldsymbol{S}\_{t-1}\\boldsymbol{k}\_t)\\boldsymbol{k}\_t^{\\top}\\end{equation}
它相当于将损失函数取$\\frac{1}{2}\\Vert\\boldsymbol{S}\\boldsymbol{k} - \\boldsymbol{v}\\Vert^2 + \\frac{1-\\gamma}{\\eta}\\Vert\\boldsymbol{S}\\Vert\_F^2$。当然，从数学上来说，这两个提法都是等价的：
\\begin{equation}\\boldsymbol{S}\_{t-1} (\\alpha\_t (\\boldsymbol{I} - \\beta\_t\\boldsymbol{k}\_t\\boldsymbol{k}\_t^{\\top})) + \\beta\_t\\boldsymbol{v}\_t\\boldsymbol{k}\_t^{\\top} = \\alpha\_t \\boldsymbol{S}\_{t-1} + \\alpha\_t \\beta\_t (\\boldsymbol{v}\_t/\\alpha\_t - \\boldsymbol{S}\_{t-1}\\boldsymbol{k}\_t)\\boldsymbol{k}\_t^{\\top}\\end{equation}
即$\\gamma\_t = \\alpha\_t, \\eta\_t = \\alpha\_t \\beta\_t$然后把$1/\\alpha\_t$吸收到$\\boldsymbol{v}\_t$就可以转化为后者了。所以说，这两个形式在数学上并没有区别，由于多数$\\alpha\_t$会接近于1，所以能力上估计也没啥区别（Comba说后者会好一点），只不过后者更直观地保留了Delta Rule的样子。

## 反哺进行时 [\#](https://kexue.fm/archives/11033\#%E5%8F%8D%E5%93%BA%E8%BF%9B%E8%A1%8C%E6%97%B6)

开头提到，如今的线性Attention不仅能跟Softmax Attention一较高低，甚至开始“反哺”Softmax Attention。这看似不可思议，但细思之下并不难理解。某种意义上，这些年Softmax Attention一直在退步，从MHA、GQA到MQA都是为了压缩KV Cache而做减法。而线性Attention没有KV Cache问题，所以一直往更好的方向前进。

为了更好看出这一点，我们不妨将前面提到的Attention机制都以矩阵形式写出来：
\\begin{array}{c\|c}
\\hline
& \\text{公式} \\\\[4pt\]
\\hline
\\text{Softmax Attention} & (\\exp(\\boldsymbol{Q}\\boldsymbol{K}^{\\top})\\odot \\boldsymbol{M})\\boldsymbol{V} \\\\[4pt\]
\\text{最早的线性Attention} & ((\\boldsymbol{Q}\\boldsymbol{K}^{\\top})\\odot \\boldsymbol{M})\\boldsymbol{V} \\\\[4pt\]
\\text{加入遗忘门后} & ((\\boldsymbol{Q}\\boldsymbol{K}^{\\top})\\odot \\boldsymbol{\\Gamma})\\boldsymbol{V} \\\\[4pt\]
\\text{DeltaNet} & ((\\boldsymbol{Q}\\boldsymbol{K}^{\\top})\\odot \\boldsymbol{M})(\\boldsymbol{I} + (\\boldsymbol{K}\\boldsymbol{K}^{\\top})\\odot (\\boldsymbol{M} - \\boldsymbol{I}))^{-1}\\boldsymbol{V} \\\\[4pt\]
\\text{Gated DeltaNet} & ((\\boldsymbol{Q}\\boldsymbol{K}^{\\top})\\odot \\boldsymbol{\\Gamma})(\\boldsymbol{I} + (\\boldsymbol{K}\\boldsymbol{K}^{\\top})\\odot (\\boldsymbol{\\Gamma} - \\boldsymbol{I}))^{-1}\\boldsymbol{V} \\\\[4pt\]
\\hline
\\end{array}
其中
\\begin{equation}\\Gamma\_{i,j} = \\left\\{\\begin{aligned} &\\prod\_{\\tau=j+1}^i \\gamma\_{\\tau}, &i > j \\\\[6pt\] &\\qquad 1, &i = j \\\\[6pt\] &\\qquad 0, &i < j\\end{aligned}\\right.\\end{equation}
这样看来，Softmax Attention的形式还仅停留在最早的线性Attention那会（当然这也证明了它的强大）。那“反哺”怎么实现呢？首先我们需要一种方法把Softmax Attention转化为线性Attention，这个并不难，早在 [《Transformer升级之路：5、作为无限维的线性Attention》](https://kexue.fm/archives/8601) 我们就总结了三种将Softmax Attention转化为无限维线性Attention的方案。

总之，就是存在一个映射$\\phi$，将$\\boldsymbol{Q},\\boldsymbol{K}$从$n\\times d$映射到$n\\times \\infty$，满足$\\exp(\\boldsymbol{Q}\\boldsymbol{K}^{\\top}) = \\phi(\\boldsymbol{Q})\\phi(\\boldsymbol{K})^{\\top}$，这称为“核技巧”。那接下来的事情就简单了，我们只需将上述表格中的线性Attention的$\\boldsymbol{Q},\\boldsymbol{K}$换成$\\phi(\\boldsymbol{Q}),\\phi(\\boldsymbol{K})$，最后再设法恢复$\\exp$并归一化，就得到新的Softmax Attention变体了。例如，代入到遗忘门的公式，我们有
\\begin{equation}((\\phi(\\boldsymbol{Q})\\phi(\\boldsymbol{K})^{\\top})\\odot \\boldsymbol{\\Gamma})\\boldsymbol{V} = \\exp(\\boldsymbol{Q}\\boldsymbol{K}^{\\top} + \\log\\boldsymbol{\\Gamma})\\boldsymbol{V}\\end{equation}
如果$\\gamma\_t$取常数，那么其实就是 [《Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation》](https://papers.cool/arxiv/2108.12409) 所提的ALIBI，而如果$\\gamma\_t$是依赖于输入的，那么就是 [《Forgetting Transformer: Softmax Attention with a Forget Gate》](https://papers.cool/arxiv/2503.02130) 所提的FoX。

一个更有意思的结果是 [《Understanding Transformer from the Perspective of Associative Memory》](https://papers.cool/arxiv/2505.19488) 所提的DeltaFormer，顾名思义它是Softmax Attention的DeltaNet版本。将DeltaNet的$\\boldsymbol{Q},\\boldsymbol{K}$换成$\\phi(\\boldsymbol{Q}),\\phi(\\boldsymbol{K})$，我们有
\\begin{equation}\\begin{aligned}
&\\,((\\phi(\\boldsymbol{Q})\\phi(\\boldsymbol{K})^{\\top})\\odot \\boldsymbol{M})(\\boldsymbol{I} + (\\phi(\\boldsymbol{K})\\phi(\\boldsymbol{K})^{\\top})\\odot (\\boldsymbol{M} - \\boldsymbol{I}))^{-1}\\boldsymbol{V} \\\\[8pt\]
=&\\,\\underbrace{\\exp(\\boldsymbol{Q} \\boldsymbol{K}^{\\top} + \\log\\boldsymbol{M})}\_{\\text{记为}\\boldsymbol{A}}(\\boldsymbol{I} + \\underbrace{\\exp(\\boldsymbol{K} \\boldsymbol{K}^{\\top} + \\log(\\boldsymbol{M} - \\boldsymbol{I}))}\_{\\text{记为}\\boldsymbol{B}})^{-1}\\boldsymbol{V}
\\end{aligned}\\end{equation}
如果要归一化，我们将$\\exp$换成$\\text{softmax}$即可。相比Softmax Attention，DeltaFormer将原本的$\\boldsymbol{A}\\boldsymbol{V}$改成了$\\boldsymbol{A}(\\boldsymbol{I}+\\boldsymbol{B})^{-1}\\boldsymbol{V}$，注意到
\\begin{equation}\\begin{aligned}
\\boldsymbol{A}(\\boldsymbol{I}+\\boldsymbol{B})^{-1}\\boldsymbol{V} =&\\, \\boldsymbol{A}(\\boldsymbol{I}-\\boldsymbol{B}+\\boldsymbol{B}^2- \\boldsymbol{B}^3 + \\cdots)\\boldsymbol{V} \\\
=&\\, \\boldsymbol{A}(\\boldsymbol{V}-\\boldsymbol{B}\\boldsymbol{V}+\\boldsymbol{B}^2\\boldsymbol{V}- \\boldsymbol{B}^3\\boldsymbol{V} + \\cdots)
\\end{aligned}\\end{equation}
所以DeltaFormer相当于先用$\\boldsymbol{K},\\boldsymbol{K},\\boldsymbol{V}$算多次Attention，将结果叠加起来后作为新的$\\boldsymbol{V}$，再跟$\\boldsymbol{Q},\\boldsymbol{K}$算一次Attention，这个特性让它对Multi-Hop的任务有奇效（比如Code）。此外，DeltaFormer的这个特点还意味着它跟MQA特别搭配，因为$(\\boldsymbol{I}+\\boldsymbol{B})^{-1}\\boldsymbol{V}$这部分只有$\\boldsymbol{K},\\boldsymbol{V}$参与，而对于MQA来说$\\boldsymbol{K},\\boldsymbol{V}$只有Single-Head，计算量相比MHA会明显降低。

不过，在笔者看来，这种固定系数的叠加可能是“没有免费午餐”，比如笔者的实验结果显示，DeltaFormer的语言模型损失并无太大变化，这意味着如果某些任务的损失明显降低，必然有另一些任务的损失上升了。

## 硬核编码术 [\#](https://kexue.fm/archives/11033\#%E7%A1%AC%E6%A0%B8%E7%BC%96%E7%A0%81%E6%9C%AF)

还有一个值得关注的反哺工作是PaTH Attention，出自 [《PaTH Attention: Position Encoding via Accumulating Householder Transformations》](https://papers.cool/arxiv/2505.16381)，它从位置编码的角度将DeltaNet反哺到Softmax Attention。

我们在 [《Transformer升级之路：6、旋转位置编码的完备性分析》](https://kexue.fm/archives/9403) 指出，对于任何正交矩阵$\\boldsymbol{\\Omega}$，$\\boldsymbol{R}\_m = \\boldsymbol{\\Omega}^m$都是广义的RoPE。除了旋转矩阵，还有哪些容易构建的正交矩阵呢？PaTH用的是 [Householder矩阵](https://en.wikipedia.org/wiki/Householder_transformation)：设$\\boldsymbol{w}$是任意模长为$\\sqrt{2}$的列向量，那么$\\boldsymbol{I}-\\boldsymbol{w}\\boldsymbol{w}^{\\top}$是一个正交矩阵，这我们在 [《从一个单位向量变换到另一个单位向量的正交矩阵》](https://kexue.fm/archives/8453) 也推导过，几何意义是镜面反射。

容易看出，这跟DeltaNet中$\\boldsymbol{S}\_{t-1}$所乘的$\\boldsymbol{I}-\\boldsymbol{k}\_t\\boldsymbol{k}\_t^{\\top}$是一样的，所以PaTH干脆把这部分照搬过来，即放弃$\\boldsymbol{\\Omega}^m$这个形式，也放弃$\\boldsymbol{w}$模长为$\\sqrt{2}$的约束，直接用一系列$\\boldsymbol{I}-\\boldsymbol{w}\\boldsymbol{w}^{\\top}$连乘来表达位置信息：
\\begin{equation}\\boldsymbol{q}\_i^{\\top}\\boldsymbol{k}\_j \\qquad\\to\\qquad \\boldsymbol{q}\_i^{\\top}\\underbrace{(\\boldsymbol{I}-\\boldsymbol{w}\_i\\boldsymbol{w}\_i^{\\top})(\\boldsymbol{I}-\\boldsymbol{w}\_{i-1}\\boldsymbol{w}\_{i-1}^{\\top})\\cdots(\\boldsymbol{I}-\\boldsymbol{w}\_{j+1}\\boldsymbol{w}\_{j+1}^{\\top})}\_{\\text{记为}\\boldsymbol{R}\_{i,j}}\\boldsymbol{k}\_j \\end{equation}
将$\\boldsymbol{R}\_{i,j}$写成递归形式是$\\boldsymbol{R}\_{i,j} = (\\boldsymbol{I}-\\boldsymbol{w}\_i\\boldsymbol{w}\_i^{\\top})\\boldsymbol{R}\_{i-1,j},\\boldsymbol{R}\_{j,j} = \\boldsymbol{I}$。对比DeltaNet的式$\\eqref{eq:linear-attn-deltanet}$，上式相当于$\\boldsymbol{v}\_t$恒等于零，但初值$\\boldsymbol{S}\_0$不再是零。使用“ [求逆来相助](https://kexue.fm/archives/11033#%E6%B1%82%E9%80%86%E6%9D%A5%E7%9B%B8%E5%8A%A9)”一节同样的过程，我们可以得到
\\begin{equation}\\boldsymbol{R}\_{i,j} = \\boldsymbol{I} - \\boldsymbol{W}\_{\[j:i\]}^{\\top}(\\boldsymbol{I} + (\\boldsymbol{W}\_{\[j:i\]}\\boldsymbol{W}\_{\[j:i\]}^{\\top})\\odot(\\boldsymbol{M} - \\boldsymbol{I}))^{-1}\\boldsymbol{W}\_{\[j:i\]}\\end{equation}
其中$\\boldsymbol{W}=\[\\boldsymbol{w}\_1,\\boldsymbol{w}\_2,\\cdots,\\boldsymbol{w}\_n\]^{\\top}$，切片按Numpy来理解，如$\\boldsymbol{W}\_{\[j:i\]}=\[\\boldsymbol{w}\_{j+1},\\boldsymbol{w}\_{j+2},\\cdots,\\boldsymbol{w}\_i\]^{\\top}$，切片优先级高于转置。注意求逆的是下三角阵，三角阵有一个重要特性，逆矩阵的对角线元素等于原矩阵对角线元素的倒数，如果是分块三角阵则对角块也满足这个特性，于是我们可以写出
\\begin{equation}(\\boldsymbol{I} + (\\boldsymbol{W}\_{\[j:i\]}\\boldsymbol{W}\_{\[j:i\]}^{\\top})\\odot(\\boldsymbol{M} - \\boldsymbol{I}))^{-1} = (\\underbrace{(\\boldsymbol{I} + (\\boldsymbol{W}\\boldsymbol{W}^{\\top})\\odot(\\boldsymbol{M} - \\boldsymbol{I}))^{-1}}\_{\\text{记为}\\boldsymbol{T}})\_{\[j:i,j:i\]}\\end{equation}
接下来的变换，写成分量形式可能好理解一些
\\begin{equation}\\begin{aligned}
A\_{i,j} =&\\, \\boldsymbol{q}\_i^{\\top} \\boldsymbol{R}\_{i,j} \\boldsymbol{k}\_j \\\\[6pt\]
=&\\, \\boldsymbol{q}\_i^{\\top}\\boldsymbol{k}\_j - \\boldsymbol{q}\_i^{\\top}\\boldsymbol{W}\_{\[j:i\]}^{\\top}\\boldsymbol{T}\_{\[j:i,j:i\]}\\boldsymbol{W}\_{\[j:i\]}\\boldsymbol{k}\_j \\\
=&\\, \\boldsymbol{q}\_i^{\\top}\\boldsymbol{k}\_j - \\sum\_{p=1}^d \\sum\_{l=j+1}^i \\sum\_{r=j+1}^i \\sum\_{s=1}^d Q\_{i,p} W\_{l,p} T\_{l,r} W\_{r,s} K\_{j,s} \\\
=&\\, \\boldsymbol{q}\_i^{\\top}\\boldsymbol{k}\_j - \\sum\_{p=1}^d \\sum\_{l=1}^i \\sum\_{r=j+1}^n \\sum\_{s=1}^d Q\_{i,p} W\_{l,p} T\_{l,r} W\_{r,s} K\_{j,s} \\\
=&\\, \\boldsymbol{q}\_i^{\\top}\\boldsymbol{k}\_j - \\sum\_{p=1}^d \\sum\_{l=1}^n \\sum\_{r=1}^n \\sum\_{s=1}^d Q\_{i,p} W\_{l,p} \\chi\_{l \\leq i} T\_{l,r} \\chi\_{r \\geq j+1}W\_{r,s} K\_{j,s} \\\
=&\\, \\boldsymbol{q}\_i^{\\top}\\boldsymbol{k}\_j - \\sum\_{l=1}^n \\sum\_{r=1}^n \\underbrace{\\left(\\chi\_{l \\leq i}\\sum\_{p=1}^d Q\_{i,p} W\_{l,p}\\right)}\_{(\\boldsymbol{Q}\\boldsymbol{W}^{\\top})\\odot\\boldsymbol{M}} T\_{l,r} \\underbrace{\\left(\\chi\_{r \\geq j+1} \\sum\_{s=1}^d W\_{r,s} K\_{j,s}\\right)}\_{(\\boldsymbol{W}\\boldsymbol{K}^{\\top})\\odot(\\boldsymbol{M} - \\boldsymbol{I})} \\\
\\end{aligned}\\end{equation}
这里有几个关键点：比较巧妙的是第4个等号，它利用了$\\boldsymbol{T}$是下三角矩阵这一点，所以$l < r$时$T\_{l,r}$自动为零；第5个等号，$\\chi$为示性函数，满足下标的条件时为1，否则为0；第6个等号，当我们分别处理$p,s$两部分求和时，结果是$\\boldsymbol{Q}\\boldsymbol{W}^{\\top}$和$\\boldsymbol{W}\\boldsymbol{K}^{\\top}$，而乘$\\chi\_{l \\leq i}$刚好表示保留$\\boldsymbol{Q}\\boldsymbol{W}^{\\top}$的下三角部分（连同对角线），而乘$\\chi\_{r \\geq j+1}$则表示保留$\\boldsymbol{W}\\boldsymbol{K}^{\\top}$的下三角部分（不包括对角线）。

至此，我们可以把整个（Softmax之前的）注意力矩阵写出来：
\\begin{equation}\\boldsymbol{A} = (\\boldsymbol{Q}\\boldsymbol{K}^{\\top})\\odot\\boldsymbol{M} - ((\\boldsymbol{Q}\\boldsymbol{W}^{\\top})\\odot\\boldsymbol{M})(\\boldsymbol{I} + (\\boldsymbol{W}\\boldsymbol{W}^{\\top})\\odot(\\boldsymbol{M} - \\boldsymbol{I}))^{-1}((\\boldsymbol{W}\\boldsymbol{K}^{\\top})\\odot(\\boldsymbol{M} - \\boldsymbol{I})) \\label{eq:path-attn}\\end{equation}
有没有被震惊到？这还没完。直接求逆复杂度是$\\mathcal{O}(n^3)$，这肯定无法接受，还要想办法利用$\\boldsymbol{W}\\boldsymbol{W}^{\\top}$的低秩特点将复杂度降低到$\\mathcal{O}(n^2)$，然后还要推反向传播，最后写成类似Flash Attention的高效实现，这些细节大家只能看原论文挖掘了，总之全程都非常硬核。

从位置编码的角度看，PaTH是 [CoPE（Contextual Position Encoding）](https://papers.cool/arxiv/2405.18719) 的一种，它的位置并不是编号$1,2,3,\\cdots$，而是根据上下文内容自动生成的位置信号。类似地，FoX也可以看成是Contextual版的ALIBI。上下文相关的位置信息是当前线性Attention的主要特征，也可能是反哺Softmax Attention的主要方向。

## 化简乐无穷 [\#](https://kexue.fm/archives/11033\#%E5%8C%96%E7%AE%80%E4%B9%90%E6%97%A0%E7%A9%B7)

我们不妨再深入点探讨一下PaTH，这不仅有助于我们了解PaTH，也能帮助我们更熟悉DeltaNet，两者本身就是高度相关的。这一节我们从PaTH的两个特例入手，它可以帮助我们更好地理解PaTH与DeltaNet的关联。

第一个特例是$\\boldsymbol{W}=\\boldsymbol{K}$，代入到$\\eqref{eq:path-attn}$得到
\\begin{equation}\\begin{aligned}
\\boldsymbol{A} =&\\, ((\\boldsymbol{Q}\\boldsymbol{K}^{\\top})\\odot\\boldsymbol{M})(\\boldsymbol{I} - (\\boldsymbol{I} + (\\boldsymbol{K}\\boldsymbol{K}^{\\top})\\odot(\\boldsymbol{M} - \\boldsymbol{I}))^{-1}((\\boldsymbol{K}\\boldsymbol{K}^{\\top})\\odot(\\boldsymbol{M} - \\boldsymbol{I}))) \\\\[6pt\]
=&\\, ((\\boldsymbol{Q}\\boldsymbol{K}^{\\top})\\odot\\boldsymbol{M})(\\boldsymbol{I} + (\\boldsymbol{K}\\boldsymbol{K}^{\\top})\\odot(\\boldsymbol{M} - \\boldsymbol{I}))^{-1} \\qquad (\\text{注}:\\boldsymbol{I} - (\\boldsymbol{I} + \\boldsymbol{A})^{-1} \\boldsymbol{A} = (\\boldsymbol{I}+\\boldsymbol{A})^{-1})
\\end{aligned}\\end{equation}
有没有觉得有点熟悉？这刚好就是DeltaNet的Attention矩阵！从这个特例看来，PaTH和DeltaFormer的区别就在于，DeltaFormer基于核技巧，给DeltaNet的$\\boldsymbol{Q}\\boldsymbol{K}^{\\top}$和$\\boldsymbol{K}\\boldsymbol{K}^{\\top}$分别加上$\\exp$，而PaTH直接给DeltaNet的Attention矩阵加上$\\exp$。

第二个特例是重新引入$\\Vert\\boldsymbol{w}\\Vert=\\sqrt{2}$这个约束，此时$\\boldsymbol{I}-\\boldsymbol{w}\\boldsymbol{w}^{\\top}$是正交矩阵，我们引入
\\begin{equation}\\begin{aligned}
\\boldsymbol{R}\_i \\triangleq&\\, (\\boldsymbol{I}-\\boldsymbol{w}\_i\\boldsymbol{w}\_i^{\\top})(\\boldsymbol{I}-\\boldsymbol{w}\_{i-1}\\boldsymbol{w}\_{i-1}^{\\top})\\cdots(\\boldsymbol{I}-\\boldsymbol{w}\_1\\boldsymbol{w}\_1^{\\top}) \\\
=&\\, \\boldsymbol{I} - \\boldsymbol{W}\_{\[:i\]}^{\\top}(\\boldsymbol{I} + (\\boldsymbol{W}\_{\[:i\]}\\boldsymbol{W}\_{\[:i\]}^{\\top})\\odot(\\boldsymbol{M} - \\boldsymbol{I}))^{-1}\\boldsymbol{W}\_{\[:i\]} \\\
=&\\,\\boldsymbol{R}\_{i,0}
\\end{aligned}\\end{equation}
那么$\\boldsymbol{R}\_{i,j} = \\boldsymbol{R}\_i \\boldsymbol{R}\_j^{\\top}$。这个等式意味着我们可以像RoPE一样，用绝对位置的方式实现相对位置的PaTH，即只需要给每个$\\boldsymbol{q}\_i^{\\top},\\boldsymbol{k}\_i^{\\top}$都乘上$\\boldsymbol{R}\_i$，然后套用Softmax Attention的实现就行。那么乘$\\boldsymbol{R}\_i$是什么运算呢？重复上一节的展开过程，我们有
\\begin{equation}\\begin{aligned}
(\\boldsymbol{q}\_i^{\\top} \\boldsymbol{R}\_{i})\_s =&\\, (\\boldsymbol{q}\_i^{\\top} - \\boldsymbol{q}\_i^{\\top}\\boldsymbol{W}\_{\[:i\]}^{\\top}\\boldsymbol{T}\_{\[:i,:i\]}\\boldsymbol{W}\_{\[:i\]})\_s \\\
=&\\, Q\_{i,s} - \\sum\_{p=1}^d \\sum\_{l=1}^i \\sum\_{r=1}^i Q\_{i,p} W\_{l,p} T\_{l,r} W\_{r,s} \\\
=&\\, Q\_{i,s} - \\sum\_{p=1}^d \\sum\_{l=1}^i \\sum\_{r=1}^n Q\_{i,p} W\_{l,p} T\_{l,r} W\_{r,s} \\\
=&\\, Q\_{i,s} - \\sum\_{p=1}^d \\sum\_{l=1}^n \\sum\_{r=1}^n \\chi\_{l\\leq i} Q\_{i,p} W\_{l,p} T\_{l,r} W\_{r,s} \\\
=&\\, Q\_{i,s} - \\sum\_{l=1}^n \\underbrace{\\chi\_{l\\leq i} \\sum\_{p=1}^d Q\_{i,p} W\_{l,p}}\_{(\\boldsymbol{Q}\\boldsymbol{W}^{\\top})\\odot\\boldsymbol{M}}\\, \\underbrace{\\sum\_{r=1}^n T\_{l,r} W\_{r,s}}\_{\\boldsymbol{T}\\boldsymbol{W}}
\\end{aligned}\\end{equation}
写成矩阵形式就是
\\begin{equation}\\boldsymbol{\\boldsymbol{Q}} - ((\\boldsymbol{Q}\\boldsymbol{W}^{\\top})\\odot\\boldsymbol{M})(\\boldsymbol{I} + (\\boldsymbol{W}\\boldsymbol{W}^{\\top})\\odot(\\boldsymbol{M} - \\boldsymbol{I}))^{-1}\\boldsymbol{W}\\end{equation}
是不是又觉得有点熟悉？其实第二部分就是$\\text{DeltaNet}(\\boldsymbol{Q},\\boldsymbol{W},\\boldsymbol{W})$！所以这种情况下PaTH实现的效果等价于是
\\begin{equation}\\mathop{\\text{SoftmaxAttention}}(\\underbrace{\\boldsymbol{Q}-\\mathop{\\text{DeltaNet}}(\\boldsymbol{Q},\\boldsymbol{W},\\boldsymbol{W})}\_{\\tilde{\\boldsymbol{Q}}},\\underbrace{\\boldsymbol{K}-\\mathop{\\text{DeltaNet}}(\\boldsymbol{K},\\boldsymbol{W},\\boldsymbol{W})}\_{\\tilde{\\boldsymbol{K}}},\\boldsymbol{V})\\end{equation}
也就是用DeltaNet给$\\boldsymbol{Q},\\boldsymbol{K}$加位置编码。这样看PaTH（在$\\Vert\\boldsymbol{w}\\Vert=\\sqrt{2}$这个约束下）就相当于Softmax Attention与DeltaNet的某种层间混合。当然我们也可以考虑放弃前面的推导，即便$\\Vert\\boldsymbol{w}\\Vert\\neq\\sqrt{2}$时也按照上式来实现，这就类似于通过 [Canon Layers](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=5240330) 的方案，用卷积给$\\boldsymbol{Q},\\boldsymbol{K}$加位置信息了，只不过这里的卷积不再是短卷积，而是DeltaNet这种长卷积。

## 剑走偏锋法 [\#](https://kexue.fm/archives/11033\#%E5%89%91%E8%B5%B0%E5%81%8F%E9%94%8B%E6%B3%95)

最后，我们再看最近的一个同样值得关注的线性Attention模型——MesaNet（还有一个大同小异的同期工作 [Atlas](https://papers.cool/arxiv/2505.23735)）。TTT的Online Learning视角告诉我们，DeltaNet其实就是在用SGD优化目标函数$\\frac{1}{2}\\Vert\\boldsymbol{S}\\boldsymbol{k} - \\boldsymbol{v}\\Vert^2$，而我们仔细观察就会发现，$\\boldsymbol{S}\\boldsymbol{k}$只是$\\boldsymbol{k}$的线性函数，所以这实际上只是一个线性回归问题，线性回归是有解析解的！
\\begin{equation}\\boldsymbol{S}\_t = \\boldsymbol{G}\_t \\boldsymbol{H}\_t^{-1},\\quad \\boldsymbol{G}\_t = \\sum\_{j=1}^t \\boldsymbol{v}\_j \\boldsymbol{k}\_j^{\\top},\\quad \\boldsymbol{H}\_t = \\sum\_{j=1}^t \\boldsymbol{k}\_j \\boldsymbol{k}\_j^{\\top}\\end{equation}
MesaNet就是利用这个解析解来构建序列模型的，其想法起源于 [《Uncovering mesa-optimization algorithms in Transformers》](https://papers.cool/arxiv/2309.05858)，高效训练则是由 [《MesaNet: Sequence Modeling by Locally Optimal Test-Time Training》](https://papers.cool/arxiv/2506.05233) 实现。MesaNet在上述公式基础上给$\\boldsymbol{G}\_t,\\boldsymbol{H}\_t$加入遗忘门，然后求时加上对角阵$\\boldsymbol{\\Lambda}\_t$避免不可逆，总的模型是
\\begin{equation}\\boldsymbol{o}\_t = \\boldsymbol{G}\_t (\\boldsymbol{H}\_t + \\boldsymbol{\\Lambda}\_t)^{-1} \\boldsymbol{q}\_t,\\quad \\boldsymbol{G}\_t = \\gamma\_t \\boldsymbol{G}\_{t-1} + \\boldsymbol{v}\_t \\boldsymbol{k}\_t^{\\top},\\quad\\boldsymbol{H}\_t = \\gamma\_t \\boldsymbol{H}\_{t-1} + \\boldsymbol{k}\_t \\boldsymbol{k}\_t^{\\top}\\end{equation}
很明显，$\\boldsymbol{G}\_t,\\boldsymbol{H}\_t$关于序列长度的复杂度是线性的，所以$\\boldsymbol{o}\_t$的计算复杂度也是线性的，因此MesaNet仍然属于线性Attention的范畴，并且由于解析解的缘故，基本上可以保证大多数情况下它优于DeltaNet甚至Gated DeltaNet。从信号处理的角度看，MesaNet与DeltaNet是 [Recursive Least Square](https://en.wikipedia.org/wiki/Recursive_least_squares_filter) 和 [Least Mean Square](https://en.wikipedia.org/wiki/Least_mean_squares_filter) 的区别。

看上去都是优点，为啥笔者会将它归入“剑走偏锋”呢？在笔者看来，MesaNet“成也解析解，败也解析解”，解析解使得它通常优于DeltaNet，但也给人一种“到此为止”的感觉，因为只要稍变一下就几乎没有机会求得解析解了。纵观整个数学史，所有依赖于解析解的分支在今天几乎已经都没落了，因为解析解实在太稀罕、太没有代表性了。

从实现上来看，MesaNet需要求逆的矩阵$\\boldsymbol{H}\_t + \\boldsymbol{\\Lambda}\_t$并不是三角阵，尽管$(\\boldsymbol{H}\_t + \\boldsymbol{\\Lambda}\_t)^{-1} \\boldsymbol{q}\_t$仍然可以转化为解方程而不需要显式逆，但非三角阵仍使得它求解复杂度会增加不少。如何尽可能低成本地并行计算全体$(\\boldsymbol{H}\_t + \\boldsymbol{\\Lambda}\_t)^{-1} \\boldsymbol{q}\_t$将会是MesaNet长期的难点，目前论文用到的是“ [共轭梯度法](https://en.wikipedia.org/wiki/Conjugate_gradient_method)”求近似解，能用但并不完美。

再就是从理论能力上看，MesaNet也并非严格优于DeltaNet。这是因为MesaNet的$\\boldsymbol{G}\_t,\\boldsymbol{H}\_t$更新规则还是简单的滑动平均形式，它的求逆也不涉及到Token之间的交互，所以它的能力极限大概不如拥有Delta Rule的DeltaNet。直观理解就是，MesaNet会尽力记住全体$\\boldsymbol{k},\\boldsymbol{v}$，这在多数情况下是好事，但某些情况下会导致比较模糊的记忆，而DeltaNet的原则是“除旧迎新”，因为“除旧”的缘故，它可以实现长期、精准地记忆某些内容。

总的来说，MesaNet是一个让人赏心悦目的模型，但解析解也增加了它的复杂性和限制了它的灵活性，留下了不少亟待探索的空间。如果读者想要了解更多基于线性回归来构建序列模型的内容，还可以阅读 [TTR](https://papers.cool/arxiv/2501.12352)，它对各种线性回归目标下的序列模型做了详细讨论。

## 方兴未艾路 [\#](https://kexue.fm/archives/11033\#%E6%96%B9%E5%85%B4%E6%9C%AA%E8%89%BE%E8%B7%AF)

本文简要梳理了线性Attention的发展脉络，并介绍了部分模型的数学原理。线性Attention从模仿Softmax Attention起步，逐渐发展出自身特色，如今已成为极具竞争力的序列建模方案，甚至反过来为Softmax Attention的发展提供了新思路，这一过程本身充满了趣味性和启发性。

_**转载到请包括本文地址：** [https://kexue.fm/archives/11033](https://kexue.fm/archives/11033)_

_**更详细的转载事宜请参考：**_ [《科学空间FAQ》](https://kexue.fm/archives/6508#%E6%96%87%E7%AB%A0%E5%A6%82%E4%BD%95%E8%BD%AC%E8%BD%BD/%E5%BC%95%E7%94%A8)

**如果您还有什么疑惑或建议，欢迎在下方评论区继续讨论。**

**如果您觉得本文还不错，欢迎 [分享](https://kexue.fm/archives/11033#share)/ [打赏](https://kexue.fm/archives/11033#pay) 本文。打赏并非要从中获得收益，而是希望知道科学空间获得了多少读者的真心关注。当然，如果你无视它，也不会影响你的阅读。再次表示欢迎和感谢！**

打赏

微信打赏

支付宝打赏

因为网站后台对打赏并无记录，因此欢迎在打赏时候备注留言。你还可以 [**点击这里**](http://mail.qq.com/cgi-bin/qm_share?t=qm_mailme&email=tN7d1drY3drrx8H0xcWa19vZ) 或在下方评论区留言来告知你的建议或需求。

**如果您需要引用本文，请参考：**

苏剑林. (Jun. 20, 2025). 《线性注意力简史：从模仿、创新到反哺 》\[Blog post\]. Retrieved from [https://kexue.fm/archives/11033](https://kexue.fm/archives/11033)

@online{kexuefm-11033,
        title={线性注意力简史：从模仿、创新到反哺},
        author={苏剑林},
        year={2025},
        month={Jun},
        url={\\url{https://kexue.fm/archives/11033}},
}

分类： [信息时代](https://kexue.fm/category/Big-Data)    标签： [线性](https://kexue.fm/tag/%E7%BA%BF%E6%80%A7/), [RNN](https://kexue.fm/tag/RNN/), [生成模型](https://kexue.fm/tag/%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B/), [attention](https://kexue.fm/tag/attention/)[9 评论](https://kexue.fm/archives/11033#comments)

< [msign的导数](https://kexue.fm/archives/11025) \| >

### 你也许还对下面的内容感兴趣

- [生成扩散模型漫谈（三十）：从瞬时速度到平均速度](https://kexue.fm/archives/10958)
- [Transformer升级之路：20、MLA究竟好在哪里？](https://kexue.fm/archives/10907)
- [Transformer升级之路：19、第二类旋转位置编码](https://kexue.fm/archives/10862)
- [生成扩散模型漫谈（二十九）：用DDPM来离散编码](https://kexue.fm/archives/10711)
- [细水长flow之TARFLOW：流模型满血归来？](https://kexue.fm/archives/10667)
- [生成扩散模型漫谈（二十八）：分步理解一致性模型](https://kexue.fm/archives/10633)
- [生成扩散模型漫谈（二十七）：将步长作为条件输入](https://kexue.fm/archives/10617)
- [生成扩散模型漫谈（二十六）：基于恒等式的蒸馏（下）](https://kexue.fm/archives/10567)
- [VQ的又一技巧：给编码表加一个线性变换](https://kexue.fm/archives/10519)
- [VQ的旋转技巧：梯度直通估计的一般推广](https://kexue.fm/archives/10489)

[发表你的看法](https://kexue.fm/archives/11033#comment_form)

spursgo

June 20th, 2025

飞来阁围观ing

[回复评论](https://kexue.fm/archives/11033/comment-page-1?replyTo=27924#respond-post-11033)

[长琴](https://yam.gift)

June 20th, 2025

又是高质量的一篇，我的阅读速度不如苏的写作速度1/10。。

[回复评论](https://kexue.fm/archives/11033/comment-page-1?replyTo=27925#respond-post-11033)

dqs

June 20th, 2025

请问您觉得这一类方法对于diffusion这种非causal的transformer有什么用处吗？我目前做的任务涉及非常多的token，原始attention已经完全训不动了，目前在尝试follow很多年前bert的老路，不知道这些新思路有没有什么指导意义。

[回复评论](https://kexue.fm/archives/11033/comment-page-1?replyTo=27926#respond-post-11033)

dqs 发表于
June 20th, 2025

比如说我现在在考虑用swin划窗口做attention，这个感受野还是有问题，需要全局信息的任务搞不定。有没有可能维护一个前缀状态和后缀状态，用某种方法融合，而不是单纯正着和倒着做两遍rnn？我也没太想清楚

[回复评论](https://kexue.fm/archives/11033/comment-page-1?replyTo=27927#respond-post-11033)

[苏剑林](https://kexue.fm) 发表于
June 20th, 2025

你搜搜mamba、rwkv甚至rnn做vision的工作，其实不少。不过多数确实像你说的，正反做两遍rnn拼接起来。之前在 [https://kexue.fm/archives/7546](https://kexue.fm/archives/7546) 提到的一些线性attention工作，倒是主要关心encoder场景的，但在今天看来能力可能都不大行了。但按照我的理解，其实cv大部份任务都attention的依赖都比较弱？只要你CNN用得够多够好，attention纯粹是辅助作用，这样直接用早期弱一点的线性attention也无妨？

[回复评论](https://kexue.fm/archives/11033/comment-page-1?replyTo=27943#respond-post-11033)

Nova

June 20th, 2025

苏神，我个人总有一种感觉现在的 TTT 太“小气”了，它自己按照某种先验造训练目标没啥道理啊..若真要“将模型f视为“解压器”，权重是“压缩包””，\[Temp Lora\]（https://arxiv.org/abs/2401.11504）的思路反到更简单粗暴...您对这篇怎么看呢

[回复评论](https://kexue.fm/archives/11033/comment-page-1?replyTo=27929#respond-post-11033)

[苏剑林](https://kexue.fm) 发表于
June 20th, 2025

Temp LoRA倒是有印象，其实思想是一样的，如果我单独开一篇文章介绍TTT的话，应该会提到它。

其实我们可以将TTT看成slow-fast weight的组合，TTT的模型权重是里边的fast weight，它根据数据来更新，TTT外边的是slow weight，训完就不动了。必定是有快有慢配合才可续的。Temp LoRA更像是把整个模型都当作fast weight了（虽然它用LoRA，但区别不大），成本难以接受，而且未必科学。

当然，如果你想要“大气”的TTT，看起来 TTT done right（ https://arxiv.org/abs/2505.23884 ）会是你感兴趣的。

[回复评论](https://kexue.fm/archives/11033/comment-page-1?replyTo=27944#respond-post-11033)

Skyler Lin

June 21st, 2025

respect苏神！

[回复评论](https://kexue.fm/archives/11033/comment-page-1?replyTo=27949#respond-post-11033)

[Chaofa Yuan](https://yuanchaofa.com/)

June 21st, 2025

写得太好了

[回复评论](https://kexue.fm/archives/11033/comment-page-1?replyTo=27951#respond-post-11033)

[取消回复](https://kexue.fm/archives/11033#respond-post-11033)

你的大名

电子邮箱

个人网站（选填）

1\. 可以使用LaTeX代码，点击“预览效果”可查看效果；2. 可以通过点击评论楼层编号来引用该楼层；3. 网站可能会有点卡，如非确认评论失败，请 **不要重复点击提交**。

### 内容速览

[平方复杂度](https://kexue.fm/archives/11033#%E5%B9%B3%E6%96%B9%E5%A4%8D%E6%9D%82%E5%BA%A6)
[最初的模样](https://kexue.fm/archives/11033#%E6%9C%80%E5%88%9D%E7%9A%84%E6%A8%A1%E6%A0%B7)
[花式遗忘门](https://kexue.fm/archives/11033#%E8%8A%B1%E5%BC%8F%E9%81%97%E5%BF%98%E9%97%A8)
[测试时训练](https://kexue.fm/archives/11033#%E6%B5%8B%E8%AF%95%E6%97%B6%E8%AE%AD%E7%BB%83)
[除旧而迎新](https://kexue.fm/archives/11033#%E9%99%A4%E6%97%A7%E8%80%8C%E8%BF%8E%E6%96%B0)
[求逆来相助](https://kexue.fm/archives/11033#%E6%B1%82%E9%80%86%E6%9D%A5%E7%9B%B8%E5%8A%A9)
[反哺进行时](https://kexue.fm/archives/11033#%E5%8F%8D%E5%93%BA%E8%BF%9B%E8%A1%8C%E6%97%B6)
[硬核编码术](https://kexue.fm/archives/11033#%E7%A1%AC%E6%A0%B8%E7%BC%96%E7%A0%81%E6%9C%AF)
[化简乐无穷](https://kexue.fm/archives/11033#%E5%8C%96%E7%AE%80%E4%B9%90%E6%97%A0%E7%A9%B7)
[剑走偏锋法](https://kexue.fm/archives/11033#%E5%89%91%E8%B5%B0%E5%81%8F%E9%94%8B%E6%B3%95)
[方兴未艾路](https://kexue.fm/archives/11033#%E6%96%B9%E5%85%B4%E6%9C%AA%E8%89%BE%E8%B7%AF)

### 智能搜索

支持整句搜索！网站自动使用 [结巴分词](https://github.com/fxsjy/jieba) 进行分词，并结合ngrams排序算法给出合理的搜索结果。

### 热门标签

[生成模型](https://kexue.fm/tag/%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B/) [attention](https://kexue.fm/tag/attention/) [优化](https://kexue.fm/tag/%E4%BC%98%E5%8C%96/) [语言模型](https://kexue.fm/tag/%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/) [模型](https://kexue.fm/tag/%E6%A8%A1%E5%9E%8B/) [网站](https://kexue.fm/tag/%E7%BD%91%E7%AB%99/) [概率](https://kexue.fm/tag/%E6%A6%82%E7%8E%87/) [梯度](https://kexue.fm/tag/%E6%A2%AF%E5%BA%A6/) [转载](https://kexue.fm/tag/%E8%BD%AC%E8%BD%BD/) [微分方程](https://kexue.fm/tag/%E5%BE%AE%E5%88%86%E6%96%B9%E7%A8%8B/) [矩阵](https://kexue.fm/tag/%E7%9F%A9%E9%98%B5/) [天象](https://kexue.fm/tag/%E5%A4%A9%E8%B1%A1/) [分析](https://kexue.fm/tag/%E5%88%86%E6%9E%90/) [深度学习](https://kexue.fm/tag/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/) [积分](https://kexue.fm/tag/%E7%A7%AF%E5%88%86/) [python](https://kexue.fm/tag/python/) [优化器](https://kexue.fm/tag/%E4%BC%98%E5%8C%96%E5%99%A8/) [力学](https://kexue.fm/tag/%E5%8A%9B%E5%AD%A6/) [无监督](https://kexue.fm/tag/%E6%97%A0%E7%9B%91%E7%9D%A3/) [扩散](https://kexue.fm/tag/%E6%89%A9%E6%95%A3/) [几何](https://kexue.fm/tag/%E5%87%A0%E4%BD%95/) [节日](https://kexue.fm/tag/%E8%8A%82%E6%97%A5/) [生活](https://kexue.fm/tag/%E7%94%9F%E6%B4%BB/) [文本生成](https://kexue.fm/tag/%E6%96%87%E6%9C%AC%E7%94%9F%E6%88%90/) [数论](https://kexue.fm/tag/%E6%95%B0%E8%AE%BA/)

### 随机文章

- [Monarch矩阵：计算高效的稀疏型矩阵分解](https://kexue.fm/archives/10249)
- [变分自编码器（七）：球面上的VAE（vMF-VAE）](https://kexue.fm/archives/8404)
- [能量视角下的GAN模型（三）：生成模型=能量模型](https://kexue.fm/archives/6612)
- [【中文分词系列】 7\. 深度学习分词？只需一个词典！](https://kexue.fm/archives/4245)
- [行动起来！共同应对全球气候变暖](https://kexue.fm/archives/107)
- [借助变分法变换坐标](https://kexue.fm/archives/3181)
- [\[共享\]不等式文集](https://kexue.fm/archives/1689)
- [一个人的数学建模：碎纸复原](https://kexue.fm/archives/2067)
- [当Matlab遇上牛顿法](https://kexue.fm/archives/1995)
- [让人惊叹的Johnson-Lindenstrauss引理：应用篇](https://kexue.fm/archives/8706)

### 最近评论

- [忍者猫](https://kexue.fm/archives/10592/comment-page-2#comment-27952): 这优化器的作者真的应该给你打钱
- [Chaofa Yuan](https://kexue.fm/archives/11033/comment-page-1#comment-27951): 写得太好了
- [Skyler Lin](https://kexue.fm/archives/11033/comment-page-1#comment-27949): respect苏神！
- [宋佳铭](https://kexue.fm/archives/10958/comment-page-1#comment-27947): 对，个人感觉mean flow就是continuous time CTM
- [宋佳铭](https://kexue.fm/archives/10958/comment-page-1#comment-27946): 的确，对sg这个事情我感觉如果是用‘归纳’法做是不太能避免的，因为毕竟是用步长短的模型去约束步...
- [MoFHeka](https://kexue.fm/archives/10542/comment-page-1#comment-27945): 苏老师您好，请问一下这套结论在稀疏参数上应该如何应用？比如大规模稀疏Embedding，每个B...
- [苏剑林](https://kexue.fm/archives/11033/comment-page-1#comment-27944): Temp LoRA倒是有印象，其实思想是一样的，如果我单独开一篇文章介绍TTT的话，应该会提到...
- [苏剑林](https://kexue.fm/archives/11033/comment-page-1#comment-27943): 你搜搜mamba、rwkv甚至rnn做vision的工作，其实不少。不过多数确实像你说的，正反...
- [苏剑林](https://kexue.fm/archives/9379/comment-page-1#comment-27942): 问题1可以看看 https://kexue.fm/archives/4718 ，简单来说就是点...
- [苏剑林](https://kexue.fm/archives/10958/comment-page-2#comment-27941): 你的“信息量”怎么定义？直观来说，reflow训练的是切线模型，而一步生成需要的是割线模型，m...

### 友情链接

- [Cool Papers](https://papers.cool)
- [数学研发](https://bbs.emath.ac.cn)
- [Seatop](http://www.seatop.com.cn/)
- [Xiaoxia](https://xiaoxia.org/)
- [积分表-网络版](https://kexue.fm/sci/integral/index.html)
- [丝路博傲](http://blog.dvxj.com/)
- [ph4ntasy 饭特稀](http://www.ph4ntasy.com/)
- [数学之家](http://www.2math.cn/)
- [有趣天文奇观](http://interesting-sky.china-vo.org/)
- [TwistedW](http://www.twistedwg.com/)
- [godweiyang](https://godweiyang.com/)
- [AI柠檬](https://blog.ailemon.net/)
- [王登科-DK博客](https://greatdk.com)
- [ESON](https://blog.eson.org/)
- [枫之羽](https://fzhiy.net/)
- [Mathor's blog](https://wmathor.com/)
- [coding-zuo](https://coding-zuo.github.io/)
- [博科园](https://www.bokeyuan.net/)
- [孔皮皮的博客](https://www.kppkkp.top/)
- [运鹏的博客](https://yunpengtai.top/)
- [jiming.site](https://jiming.site/)
- [OmegaXYZ](https://www.omegaxyz.com/)
- [Blog by Eacls](https://www.eacls.top/)
- [EAI猩球](https://www.robotech.ink/)
- [文举的博客](https://liwenju0.com/)
- [用代码打点酱油](https://bruceyuan.com/)
- [申请链接](https://kexue.fm/links.html)

本站采用创作共用版权协议，要求署名、非商业用途和保持一致。转载本站内容必须也遵循“ [署名-非商业用途-保持一致](http://creativecommons.org/licenses/by-nc-nd/2.5/cn/)”的创作共用协议。
© 2009-2025 Scientific Spaces. All rights reserved. Theme by [laogui](http://www.laogui.com). Powered by [Typecho](http://typecho.org). 备案号: [粤ICP备09093259号-1/2](https://beian.miit.gov.cn/)。