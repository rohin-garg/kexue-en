## SEARCH

## MENU

- [打赏](https://kexue.fm/reward.html)
- [公式](https://kexue.fm/latex.html)
- [天象](https://kexue.fm/ac.html)
- [链接](https://kexue.fm/links.html)
- [时光](https://kexue.fm/me.html)
- [博览](https://kexue.fm/science.html)
- [归档](https://kexue.fm/content.html)

## CATEGORIES

- [千奇百怪](https://kexue.fm/category/Everything)
- [天文探索](https://kexue.fm/category/Astronomy)
- [数学研究](https://kexue.fm/category/Mathematics)
- [物理化学](https://kexue.fm/category/Phy-chem)
- [信息时代](https://kexue.fm/category/Big-Data)
- [生物自然](https://kexue.fm/category/Biology)
- [图片摄影](https://kexue.fm/category/Photograph)
- [问题百科](https://kexue.fm/category/Questions)
- [生活/情感](https://kexue.fm/category/Life-Feeling)
- [资源共享](https://kexue.fm/category/Resources)

## NEWPOSTS

- [低精度Attention可能存在有...](https://kexue.fm/archives/11371)
- [MuP之上：1. 好模型的三个特征](https://kexue.fm/archives/11340)
- [随机矩阵的谱范数的快速估计](https://kexue.fm/archives/11335)
- [DiVeQ：一种非常简洁的VQ训练方案](https://kexue.fm/archives/11328)
- [为什么线性注意力要加Short C...](https://kexue.fm/archives/11320)
- [AdamW的Weight RMS的...](https://kexue.fm/archives/11307)
- [重新思考学习率与Batch Siz...](https://kexue.fm/archives/11301)
- [重新思考学习率与Batch Siz...](https://kexue.fm/archives/11285)
- [重新思考学习率与Batch Siz...](https://kexue.fm/archives/11280)
- [为什么Adam的Update RM...](https://kexue.fm/archives/11267)

## COMMENTS

- [苏剑林: 受教了，感谢指点，我沿着修正后的理解继续思考一下。](https://kexue.fm/archives/11371/comment-page-1#comment-28700)
- [PengleZhang: 是的, 我们可以理解为, 低精度 tensor core 理论...](https://kexue.fm/archives/11371/comment-page-1#comment-28699)
- [苏剑林: \[comment=28697\]PengleZhang\[/com...](https://kexue.fm/archives/11371/comment-page-1#comment-28698)
- [PengleZhang: 嗯. 固有精度为 bf16 是有问题的.\
在 \`torch.m...](https://kexue.fm/archives/11371/comment-page-1#comment-28697)
- [苏剑林: 我要表达的是：$\\bar{p}\_i$,$v\_i$都是bf16的...](https://kexue.fm/archives/11371/comment-page-1#comment-28696)
- [PengleZhang: $\\overline p$ 和 $v\_i$ 是 bf16 的,...](https://kexue.fm/archives/11371/comment-page-1#comment-28695)
- [苏剑林: 如果我没理解错，$\\sum\_i \\bar{p}\_i v\_i$的...](https://kexue.fm/archives/11371/comment-page-1#comment-28694)
- [PengleZhang: 感谢回复!\
但是感觉这和我的解释没有冲突.\
p 和 v 都是 ...](https://kexue.fm/archives/11371/comment-page-1#comment-28693)
- [苏剑林: 统一回复 \[comment=28687\]jerrick\[/co...](https://kexue.fm/archives/11371/comment-page-1#comment-28692)
- [苏剑林: 我觉得就是两个不同特点的SDE咯，你这里的“理解”是什么含义呢？](https://kexue.fm/archives/9209/comment-page-7#comment-28691)

## USERLOGIN

- [登录](https://kexue.fm/admin/login.php)

[科学空间\|Scientific Spaces](https://kexue.fm)

- [登录](https://kexue.fm/admin/login.php)
- [打赏](https://kexue.fm/reward.html)
- [公式](https://kexue.fm/latex.html)
- [天象](https://kexue.fm/ac.html)
- [链接](https://kexue.fm/links.html)
- [时光](https://kexue.fm/me.html)
- [博览](https://kexue.fm/science.html)
- [归档](https://kexue.fm/content.html)

渴望成为一个小飞侠

- [欢迎订阅](https://kexue.fm/feed)
- [个性邮箱](https://kexue.fm/archives/119)
- [天象信息](https://kexue.fm/ac.html)
- [观测ISS](https://kexue.fm/archives/41)
- [LaTeX](https://kexue.fm/latex.html)
- [关于博主](https://kexue.fm/me.html)

欢迎访问“科学空间”，这里将与您共同探讨自然科学，回味人生百态；也期待大家的分享～

- [**千奇百怪** Everything](https://kexue.fm/category/Everything)
- [**天文探索** Astronomy](https://kexue.fm/category/Astronomy)
- [**数学研究** Mathematics](https://kexue.fm/category/Mathematics)
- [**物理化学** Phy-chem](https://kexue.fm/category/Phy-chem)
- [**信息时代** Big-Data](https://kexue.fm/category/Big-Data)
- [**生物自然** Biology](https://kexue.fm/category/Biology)
- [**图片摄影** Photograph](https://kexue.fm/category/Photograph)
- [**问题百科** Questions](https://kexue.fm/category/Questions)
- [**生活/情感** Life-Feeling](https://kexue.fm/category/Life-Feeling)
- [**资源共享** Resources](https://kexue.fm/category/Resources)

- [**千奇百怪**](https://kexue.fm/category/Everything)
- [**天文探索**](https://kexue.fm/category/Astronomy)
- [**数学研究**](https://kexue.fm/category/Mathematics)
- [**物理化学**](https://kexue.fm/category/Phy-chem)
- [**信息时代**](https://kexue.fm/category/Big-Data)
- [**生物自然**](https://kexue.fm/category/Biology)
- [**图片摄影**](https://kexue.fm/category/Photograph)
- [**问题百科**](https://kexue.fm/category/Questions)
- [**生活/情感**](https://kexue.fm/category/Life-Feeling)
- [**资源共享**](https://kexue.fm/category/Resources)

[首页](https://kexue.fm) [信息时代](https://kexue.fm/category/Big-Data) 低精度Attention可能存在有偏的舍入误差

27Oct

# [低精度Attention可能存在有偏的舍入误差](https://kexue.fm/archives/11371)

By 苏剑林 \|
2025-10-27 \|
1956位读者\|

前段时间笔者在arXiv上刷到了论文 [《Why Low-Precision Transformer Training Fails: An Analysis on Flash Attention》](https://papers.cool/arxiv/2510.04212)，里面描述的实验现象跟我们在训练 [Kimi K2](https://papers.cool/arxiv/2507.20534) 时出现的一些现象很吻合，比如都是第二层Attention开始出现问题。论文将其归因为低精度Attention固有的有偏误差，这个分析角度是比较出乎笔者意料的，所以饶有兴致地阅读了一番。

然而，论文的表述似乎比较让人费解——当然也有笔者本就不大熟悉低精度运算的原因。总之，经过多次向作者请教后，笔者才勉强看懂论文，遂将自己的理解记录在此，供大家参考。

## 结论简述 [\#](https://kexue.fm/kexue.fm\#%E7%BB%93%E8%AE%BA%E7%AE%80%E8%BF%B0)

要指出的是，论文标题虽然点名了“Flash Attention”，但按照论文的描述，即便block\_size取到训练长度那么大，相同的问题依然会出现，所以Flash Attention的分块计算并不是引起问题的原因，因此我们可以按照朴素的低精度Attention实现来简化分析。

简单起见，我们只分析单头Attention，设$\\boldsymbol{Q},\\boldsymbol{K},\\boldsymbol{V}\\in\\mathbb{R}^{n\\times d}$，记$\\boldsymbol{S} = \\boldsymbol{Q}\\boldsymbol{K}^{\\top}$，加粗的$\\boldsymbol{1}$是指$n\\times 1$的全1矩阵，$\\boldsymbol{S}\_{\\max}$则指$\\boldsymbol{S}$每行取最大值后得到的$n\\times 1$矩阵，那么
\\begin{equation}\\boldsymbol{O} = \\frac{\\exp(\\boldsymbol{S})\\boldsymbol{V}}{\\exp(\\boldsymbol{S})\\boldsymbol{1}} = \\frac{\\exp(\\boldsymbol{S} - \\boldsymbol{S}\_{\\max})\\boldsymbol{V}}{\\exp(\\boldsymbol{S}- \\boldsymbol{S}\_{\\max})\\boldsymbol{1}}\\end{equation}
我们记$\\bar{\\boldsymbol{P}} = \\exp(\\boldsymbol{S} - \\boldsymbol{S}\_{\\max})$，那么Attention的关键计算是矩阵乘法$\\bar{\\boldsymbol{P}}\\boldsymbol{V}$，它一般是在BF16精度下进行。论文给出的结论是： **在低精度计算下，$\\bar{\\boldsymbol{P}}\\boldsymbol{V}$这一步存在有偏的舍入误差。** 也就是说，在长期平均下，低精度计算的$\\bar{\\boldsymbol{P}}\\boldsymbol{V}$跟准确值的差的期望并不是零。

这样一来，不同训练步骤之间的偏差可能就会持续累积，从而引起 [MaxLogit爆炸](https://kexue.fm/archives/11126)、Loss Spike等问题，直至训练崩溃。当然，严格来讲这只能算是MaxLogit爆炸等问题的一种可能的产生机制，不一定是全部，但即便如此，也值得我们学习和思考一番。

## 向偶舍入 [\#](https://kexue.fm/kexue.fm\#%E5%90%91%E5%81%B6%E8%88%8D%E5%85%A5)

为了理解论文结论，我们先来补习一些关于舍入误差的基本常识。之所以会写这一节，原因开头就说了——笔者本身并不熟悉低精度运算——所以这一节完全是写给自己补基础的，对此已有了解的读者完全可以略过。

我们知道，常用的舍入（Round）方式是“四舍五入”：在10进制中，一个正的1位小数要舍去最后一位，0～4就会变成0，产生的误差是$0,-0.1,-0.2,-0.3,-0.4$；5～9就会变成10，产生的误差是$0.5,0.4,0.3,0.2,0.1$。不知道大家发现没，这些误差的平均值并不是0，而是0.05，即“四舍五入”平均而言会放大原来的数字，产生正的偏差。

当然，相对偏差会随着舍去位数的增加而减少，比如一个2位小数要舍去2个小数位，平均误差则是0.005。但不论如何，四舍五入的这个正偏差总是存在的，只不过是大小不同。偏差的根源在中间点，比如0.51和0.49，它们分别往上/往下舍入，误差刚好抵消，但对于0.50不管规定它往上舍入还是往下舍入，都没有另一个数跟它抵消误差。

为了消除偏差， [IEEE 754](https://en.wikipedia.org/wiki/IEEE_754) 提出了“向偶舍入（Round-to-Even）”原则，它规定对于中间情形，应该按照靠近偶数的方向舍入，比如2.5舍去最后一位要变成2，但3.5舍去最后一位则变成4，这样“5”就各有一半的几率产生$\\pm 5$的误差，平均误差变为零，从而消除了偏差。

回到计算机领域。我们知道计算机使用二进制，它只有0和1，那么1就起到了10进制的“5”的角色。二进制中“四舍五入”的偏差更形象，因为末位只能是0或1：如果是0，自然不用改变，而如果是1，则触发“五入”而进1。所以，二进制数按“四舍五入”舍去末位，结果必然大于或等于原数，因此也需要“向偶舍入”来消除偏差。

## BF16加法 [\#](https://kexue.fm/kexue.fm\#BF16%E5%8A%A0%E6%B3%95)

接着我们重温一下BF16格式。BF16用16位二进制表示一个浮点数，其中1位符号、7位尾数、8位指数，8位指数的设计让它表示的范围跟FP32（1位符号、23位尾数、8位指数）一致，这也使它成为如今LLM训练的主要浮点格式。

BF16保留了较多的指数位，代价必然是尾数较少，从而能表示精度较低。为了缓解低精度带来累积误差，BF16运算采取的策略是“BF16相乘、FP32相加”，也就是说BF16数的累加都是先转换成FP32，然后在FP32空间中相加得到FP32的结果，最后再转回BF16的。

现在我们考虑两个 **符号和指数相同** 的BF16数字相加。为什么要选指数相同来分析呢？因为我们要估计误差，指数相同意味着这两个数同数量级，相加后最有可能产生最大的误差。举个例子，如果两个数相加的数相差100倍，那么我哪怕直接返回最大者，误差也不过1%，所以最大误差往往在同数量级的数相加时发生。

两个符号和指数相同的BF16数字相加，必然会出现进位，比如“1.0000001 + 1.0000100 = 10.0000101 = 1.00000101 × 10”，这时候需要指数加1，并且舍去最后一位1，才能转换成BF16格式。如上一节所述，如果按照“四舍五入”舍去末位，那么将会产生正的偏差。不过我们已经知道，科学家早已发现了这个偏差，因此提出了“向偶舍入”来消除偏差。

## 两大一小 [\#](https://kexue.fm/kexue.fm\#%E4%B8%A4%E5%A4%A7%E4%B8%80%E5%B0%8F)

所以，到目前为止，一切结果都在可控和预期的范围内，还没有偏差产生。然而，不出意外的话，意外出现了。

现在让我们考虑三个同符号的数相加，这三个数的特点是：其中两个数指数相同且很大，第三个数很小。比如我们在上一节的例子“1.0000001 + 1.0000100”基础上再加上“0.0000000001”，那么得到“1.0000001 + 1.0000100 + 0.0000000001= 10.0000101001 = 1.00000101001 × 10”。

原本两个数相加，结果是“1.00000101 × 10”，舍去末位时会触发“向偶舍入”，得到“1.0000010 × 10”，可现在多了一个极小数，转换成BF16时要舍去的尾数变成了“1001”，比中间点更大，所以触发向上舍入原则，结果是“1.0000011 × 10”。那么在原本两个数相加的视角看来，第三个极小数的出现，破坏了“向偶舍入”规则，使得正偏差再次出现！

当然，这种情况出现条件看上去还是很苛刻的。首先三个数需要同号，其次需要满足“两大一小”，其中两个大数刚好能触发进位，然后小数小到只能影响FP32的尾数（即第9～23位尾数）。这样一来，小数很小，本身舍去都没多大误差，但它的存在，偏偏刚好能破坏了两个大数的“向偶舍入”规则，从而带来了单侧的偏差。

## 量身定制 [\#](https://kexue.fm/kexue.fm\#%E9%87%8F%E8%BA%AB%E5%AE%9A%E5%88%B6)

这么苛刻的条件，实际中真的能出现吗？一般情况情况下还真不容易，但对于Attention来说，这仿佛就是“量身定制”的Bug！

我们取出$\\bar{\\boldsymbol{P}}\\boldsymbol{V}$的某行某列（也就是某个元素），它可以写成
\\begin{equation}\\sum\_{i=1}^n \\bar{p}\_i v\_i \\label{eq:sum-pi-vi}\\end{equation}
其中$\\bar{p}\_i = \\exp(s\_i - \\max(s\_i))\\leq 1$。我们知道，Softmax Attention的特点是能够“集中注意力”，也就是说注意力可能会集中在有限几个Token上，体现在$\\bar{p}\_i$上就是少数几个Token的$\\bar{p}\_i$接近于1，剩下的则会非常接近于0，但由于$\\exp$的缘故，无法精确等于0（除非下溢出BF16的表示空间）。

然后，随着层数的堆叠和训练的进行，输入$\\boldsymbol{V}$可能会出现“各向异性”，其中一种表现是某些维度的正负号分布不均匀，不失一般性，我们假设$v\_i$大部分都是正数（负数同理），并且数量级大致相等。那么，求和$\\eqref{eq:sum-pi-vi}$可以分为两部分：少数几个能接近于1的$\\bar{p}\_i$跟$v\_i$相乘，成为求和的主项，剩下的余项是大部分接近于0的$\\bar{p}\_i$与$v\_i$相乘。

如此一来，“天时地利”俱备，完美触发了上一节说的Bug：大部分项都是正数，主项求和满足进位条件；剩下余项极小，只能影响FP32最末端的尾数，刚好破坏了“向偶舍入”的规则，从而产生了偏差；最后，由于“集中注意力”的缘故，主项的数目不会多，所以进位也不会太多（舍去位数越多，偏差越小），使得偏差处于显著区间！

这一套组合下来，可不就是为Attention定制的“专属Bug”？

## 干掉余项 [\#](https://kexue.fm/kexue.fm\#%E5%B9%B2%E6%8E%89%E4%BD%99%E9%A1%B9)

了解问题的来龙去脉后，我们再来思考一下怎么解决问题。

表面上看，引发偏差的原因是极小的余项破坏了“向偶舍入”，但更深入思考一下，其实根本原因是“四舍五入”这个规则在中间处存在一个突变点，在突变点附近容易因为扰动而产生偏差，“向偶舍入”虽然能消除偏差，但消除不了突变点。理想的根治办法是 [Stochastic Rounding](https://en.wikipedia.org/wiki/Rounding#Stochastic_rounding)，也就是依概率向上/向下舍入，这样最大程度上避免了小扰动带来的偏差。

然而，Stochastic Rounding不容易有高效的硬件级实现，所以现在多数硬件的矩阵乘法算子都不带Stochastic Rounding。因此，原论文选择了另一条路径，直接面对问题，其思路笔者称为“干掉余项”。具体来说，在检测到某个触发条件时，我们将Attention的计算公式改为
\\begin{equation}\\boldsymbol{O} = \\frac{\\exp(\\boldsymbol{S})\\boldsymbol{V}}{\\exp(\\boldsymbol{S})\\boldsymbol{1}} = \\frac{\\exp(\\boldsymbol{S} - \\beta\\boldsymbol{S}\_{\\max})\\boldsymbol{V}}{\\exp(\\boldsymbol{S}- \\beta\\boldsymbol{S}\_{\\max})\\boldsymbol{1}}\\end{equation}
其中$\\beta > 1$。这样一来，每一项都需要多除以$\\exp((\\beta-1)\\boldsymbol{S}\_{\\max})$，这是一个并不算小的数（论文设置$\\beta \\geq 2$），于是原本就极小的余项，就容易下溢至零而消失，那么“向偶舍入”便重新发挥作用，从而消除偏差。

那么，检测条件是什么呢？原论文考虑得比较简单，就是矩阵$\\boldsymbol{S}$的行出现大于等于两次最大值时，修改就会触发，此时$\\bar{p}\_i$中至少有两个1。但笔者认为这里肯定有很大调整空间的，算是留下了一个改进方向吧。另外要注意的是，Flash Attention是分Block计算的，所以这个检测条件和修改也是按Block进行，细节可以参考原论文附录的代码。

## 延伸思考 [\#](https://kexue.fm/kexue.fm\#%E5%BB%B6%E4%BC%B8%E6%80%9D%E8%80%83)

总的来说，论文提供了理解MaxLogit爆炸等现象的一个比较独特的视角，它能解释一些事情，但无法覆盖全貌，也留下了很多值得思考的地方（吐槽点）。

首先，论文对Attention偏差的分析依赖于$\\boldsymbol{V}$的各向异性，这也许可以解释为什么第2层Attention才出现MaxLogit爆炸等异常：因为第1层Attention的输入是Embedding，它相对来说还没那么容易出现各向异性；而第2层及以后的Attention的输入经过了前面的Attention，可能会固有地存在各向异性（ [参考](https://papers.cool/arxiv/2401.12143)）。

不过，这无法解释为什么MaxLogit爆炸只在个别层出现，比如论文的实验现象是只有第2层出问题，而K2的结果是2～4层出问题。同样地，这显然也无法解释为啥Muon比Adam更容易出现MaxLogit爆炸（出自Moonlight、K2）。所以，这应该是架构、优化器和低精度等多方面因素的综合结果，单看精度问题是不完整的。

此外，还有一个值得深思的问题是因果关系。论文的Attention偏差的另一个产生条件是注意力集中在少数几个Token上，此时对Attention计算进行干预，成功防止了它的后续异常。然而，笔者观察了一个正常训练的小模型，它的注意力没有想象中那么集中，比如平均Top-1的平均概率不到0.2、Top-400的累积概率才能达到0.9（训练长度4096）。

所以，Attention偏差究竟是训练崩溃的“因”还是“果”？换言之，当出现“注意力集中在少数几个Token上”时，有没有可能说明模型已经进入崩溃范围内了？这时候才进行干预，会不会“为时已晚”？比如虽然在指标上是防止了一些异常，但有没有可能模型已经没法Scale下去了？这些暂时都不得而知。

## 文章小结 [\#](https://kexue.fm/kexue.fm\#%E6%96%87%E7%AB%A0%E5%B0%8F%E7%BB%93)

本文分享了一篇关于低精度Attention计算偏差的分析论文，同时借着这个机会，给自己补习了一下低精度计算的基础内容。

_**转载到请包括本文地址：** [https://kexue.fm/archives/11371](https://kexue.fm/archives/11371)_

_**更详细的转载事宜请参考：**_ [《科学空间FAQ》](https://kexue.fm/archives/6508#%E6%96%87%E7%AB%A0%E5%A6%82%E4%BD%95%E8%BD%AC%E8%BD%BD/%E5%BC%95%E7%94%A8)

**如果您还有什么疑惑或建议，欢迎在下方评论区继续讨论。**

**如果您觉得本文还不错，欢迎 [分享](https://kexue.fm/kexue.fm#share)/ [打赏](https://kexue.fm/kexue.fm#pay) 本文。打赏并非要从中获得收益，而是希望知道科学空间获得了多少读者的真心关注。当然，如果你无视它，也不会影响你的阅读。再次表示欢迎和感谢！**

打赏

微信打赏

支付宝打赏

因为网站后台对打赏并无记录，因此欢迎在打赏时候备注留言。你还可以 [**点击这里**](http://mail.qq.com/cgi-bin/qm_share?t=qm_mailme&email=tN7d1drY3drrx8H0xcWa19vZ) 或在下方评论区留言来告知你的建议或需求。

**如果您需要引用本文，请参考：**

苏剑林. (Oct. 27, 2025). 《低精度Attention可能存在有偏的舍入误差 》\[Blog post\]. Retrieved from [https://kexue.fm/archives/11371](https://kexue.fm/archives/11371)

@online{kexuefm-11371,
        title={低精度Attention可能存在有偏的舍入误差},
        author={苏剑林},
        year={2025},
        month={Oct},
        url={\\url{https://kexue.fm/archives/11371}},
}

分类： [信息时代](https://kexue.fm/category/Big-Data)    标签： [近似](https://kexue.fm/tag/%E8%BF%91%E4%BC%BC/), [分析](https://kexue.fm/tag/%E5%88%86%E6%9E%90/), [优化](https://kexue.fm/tag/%E4%BC%98%E5%8C%96/), [attention](https://kexue.fm/tag/attention/)[11 评论](https://kexue.fm/archives/11371#comments)

< [MuP之上：1. 好模型的三个特征](https://kexue.fm/archives/11340) \| >

### 你也许还对下面的内容感兴趣

- [MuP之上：1. 好模型的三个特征](https://kexue.fm/archives/11340)
- [随机矩阵的谱范数的快速估计](https://kexue.fm/archives/11335)
- [为什么线性注意力要加Short Conv？](https://kexue.fm/archives/11320)
- [为什么Adam的Update RMS是0.2？](https://kexue.fm/archives/11267)
- [ReLU/GeLU/Swish的一个恒等式](https://kexue.fm/archives/11233)
- [QK-Clip：让Muon在Scaleup之路上更进一步](https://kexue.fm/archives/11126)
- [Transformer升级之路：21、MLA好在哪里?（下）](https://kexue.fm/archives/11111)
- [“对角+低秩”三角阵的高效求逆方法](https://kexue.fm/archives/11072)
- [通过msign来计算奇异值裁剪mclip（下）](https://kexue.fm/archives/11059)
- [线性注意力简史：从模仿、创新到反哺](https://kexue.fm/archives/11033)

[发表你的看法](https://kexue.fm/kexue.fm#comment_form)

jerrick

October 27th, 2025

苏神好，

两大一小相加产生误差至少需要满足以下条件：
1\. 每一次相加都先提升到fp32相加后下降bf16
2\. flash-attn kernel 使用以上混合精度机制

有一个疑问：如果不是，那单纯的bf16精度相加，过小的数值本身就是0了吧？
我没有仔细去看过flash-attn的cuda实现，但似乎有几个issue提到fa目前不支持fp32或者混合精度。（待考证

[回复评论](https://kexue.fm/archives/11371/comment-page-1?replyTo=28687#respond-post-11371)

PengleZhang 发表于
October 27th, 2025

确实, 之前也看到了这篇文章, 虽然切入点很有意思, 但是在 flash attention 中 $pv$ 结果是 fp32 的, 并且也确实是 fp32 累加. 实际训练中应该并不是 $\\sum pv$ 的精度问题.
对于 flash attention 而言, 不断 rescale O 再加上 pv 可能有更大的精度影响.

[回复评论](https://kexue.fm/archives/11371/comment-page-1?replyTo=28688#respond-post-11371)

[苏剑林](https://kexue.fm) 发表于
October 27th, 2025

统一回复 [@jerrick\|comment-28687](https://kexue.fm/archives/11371/comment-page-1#comment-28687) 和 [@PengleZhang\|comment-28688](https://kexue.fm/archives/11371/comment-page-1#comment-28688)：

本文的解读，以及原论文的分析，都是考虑了bf16在fp32空间中累加后所出现的偏差，并不是说加一个数就cast一次到bf16的情形。它是在同号假设下，少量几个主项的bf16数，加上大量的极小的bf16数，在fp32空间求和，然后将结果cast成bf16所产生的偏差。

由于求和是在fp32下进行的，所以过小的数值一般也到不了零，但刚刚好破坏了向偶舍入，所以出现偏差，这就是产生偏差的大致逻辑。

[回复评论](https://kexue.fm/archives/11371/comment-page-1?replyTo=28692#respond-post-11371)

PengleZhang 发表于
October 27th, 2025

感谢回复!
但是感觉这和我的解释没有冲突.
p 和 v 都是 bf16, 但是 pv 在 tensor core 中是 fp32 累加, 最后得到的结果也是 fp32.(bf16 x bf16 -> fp32).
所以 $\\sum pv$ 的每一项都是 fp32, 并不是 bf16 recast 到 fp32 然后在 fp32 累加.
可能的误差是 p 从 fp32 转换到 bf16, 以及最终 o 从 fp32 转化到 bf16.

[回复评论](https://kexue.fm/archives/11371/comment-page-1?replyTo=28693#respond-post-11371)

[苏剑林](https://kexue.fm) 发表于
October 27th, 2025

如果我没理解错，$\\sum\_i \\bar{p}\_i v\_i$的每一项$\\bar{p}\_i v\_i$都是bf16的（乘法是低精度的，加法才是高精度的），只是它们的累加在fp32完成？即等价运算是bf16精度的$\\bar{p}\_i v\_i$，cast到fp32后求和，然后cast到bf16。

[回复评论](https://kexue.fm/archives/11371/comment-page-1?replyTo=28694#respond-post-11371)

PengleZhang 发表于
October 27th, 2025

$\\overline p$ 和 $v\_i$ 是 bf16 的, 但是 $\\overline p v\_i$ 是 fp32. 低精度乘法得到 fp32, 然后进行 fp32 的累加.
如果我们相信 tensorcore 的精度(相信 nvidia), 则可以认为精度与 $\\overline p, v\_i$ fp32 -> bf16 -> fp32, 然后进行 fp32 的乘法是相同的.
不知道我们是否是一个意思.

[回复评论](https://kexue.fm/archives/11371/comment-page-1?replyTo=28695#respond-post-11371)

[苏剑林](https://kexue.fm) 发表于
October 27th, 2025

我要表达的是：$\\bar{p}\_i$,$v\_i$都是bf16的，$\\bar{p}\_i v\_i$的dtype是fp32的，但它固有精度只有bf16，也就是说它就相当于一个bf16的数cast成fp32的。

当然我是完全的门外行，只是根据只言片语的信息给出的判断：
https://github.com/pytorch/pytorch/issues/146241
https://www.kimi.com/share/d3vj7u6mcu0njtiddlc0

[回复评论](https://kexue.fm/archives/11371/comment-page-1?replyTo=28696#respond-post-11371)

PengleZhang 发表于
October 27th, 2025

嗯. 固有精度为 bf16 是有问题的.
在 \`torch.matmul\` 层面确实是 bf16 in bf16 out.
但是在 kernel 内部, 实际的 mma 指令是 bf16 in fp32 out.
我们在使用 cutlass 或者 cublas 时, 需要显式指定内部计算累加的 dtype.
使用 cutlass(cute) 构建的 flash attention 也不例外.

我们可以在 kernel 内调用 mma.sync 检查. c, d 的最后 16 bit 并不是 0.
也因如此, 在 hopper 中 fp8 gemm 有 bug, c, d 是 fp23 精度, 所以 deepseek 才需要自己写 deep gemm kernel 绕过 tensor core 累加.

[苏剑林](https://kexue.fm)

October 27th, 2025

[@PengleZhang\|comment-28697](https://kexue.fm/archives/11371/comment-page-1#comment-28697)

我新开一楼。设两个矩阵$A,B$，$C=AB$，那么
$$C\_{i,j} = \\sum\_k A\_{i,k} B\_{k,j}$$

==========

1\. “在 \`torch.matmul\` 层面确实是 bf16 in bf16 out.”
2\. “但是在 kernel 内部, 实际的 mma 指令是 bf16 in fp32 out”

你这两行，应该都是指$C\_{i,j}$的格式是bf16/fp32？还是指单个数乘法即$A\_{i,k} B\_{k,j}$的运算方式？

==========

“我们在使用 cutlass 或者 cublas 时, 需要显式指定内部计算累加的 dtype.”

这个是指求和$\\sum\_k$在fp32内完成？

==========

最后你的意思是指单个$A\_{i,k} B\_{k,j}$的格式是fp32，并且第8～23位尾数也非零是吧。这个我确实是不知道的。不过这样一来，bf16的matmul，不管乘加其实都在fp32内进行了，还有明显的加速空间吗？

不过倒是可以去思考一下，在这个说法下，本文的分析应该怎么修改～

[回复评论](https://kexue.fm/archives/11371/comment-page-1?replyTo=28698#respond-post-11371)

PengleZhang 发表于
October 27th, 2025

是的, 我们可以理解为, 低精度 tensor core 理论上的误差, 只有 $A,B$ 转换到 bf16 这一步. 其他部分与 fp32 一致.

我们考虑计算一次乘加 $d=ab+c$. 尽管 $ab,c,d$ 都是 fp32, 但是 $ab$ 这一步骤本身是 bf16\*bf16. 相较于 fp32\*fp32 仍然能够节省 cycle.
tensor core 的主要加速在于大矩阵可以通过脉冲阵列等方法实现高效的硬件并行.
而低精度可以降低乘加中乘法的复杂度, 并且降低数据带宽需求.

文章切入的角度很好. 尽管我们现在都在用 bf16 甚至 fp8 训练, 但是对于单个操作的精度分析远远不足. 之前 meta 有讨论 flash attention的fusion与普通 attention 实现精度误差的文章, 最近也有讨论 RoPE qk 截断到 bf16 精度影响的文章, 但这方面的探索整体还是不足的.

[回复评论](https://kexue.fm/archives/11371/comment-page-1?replyTo=28699#respond-post-11371)

[苏剑林](https://kexue.fm) 发表于
October 27th, 2025

受教了，感谢指点，我沿着修正后的理解继续思考一下。

[回复评论](https://kexue.fm/archives/11371/comment-page-1?replyTo=28700#respond-post-11371)

[取消回复](https://kexue.fm/archives/11371#respond-post-11371)

你的大名

电子邮箱

个人网站（选填）

1\. 可以使用LaTeX代码，点击“预览效果”可查看效果；2. 可以通过点击评论楼层编号来引用该楼层；3. 网站可能会有点卡，如非确认评论失败，请 **不要重复点击提交**。

### 内容速览

[结论简述](https://kexue.fm/kexue.fm#%E7%BB%93%E8%AE%BA%E7%AE%80%E8%BF%B0)
[向偶舍入](https://kexue.fm/kexue.fm#%E5%90%91%E5%81%B6%E8%88%8D%E5%85%A5)
[BF16加法](https://kexue.fm/kexue.fm#BF16%E5%8A%A0%E6%B3%95)
[两大一小](https://kexue.fm/kexue.fm#%E4%B8%A4%E5%A4%A7%E4%B8%80%E5%B0%8F)
[量身定制](https://kexue.fm/kexue.fm#%E9%87%8F%E8%BA%AB%E5%AE%9A%E5%88%B6)
[干掉余项](https://kexue.fm/kexue.fm#%E5%B9%B2%E6%8E%89%E4%BD%99%E9%A1%B9)
[延伸思考](https://kexue.fm/kexue.fm#%E5%BB%B6%E4%BC%B8%E6%80%9D%E8%80%83)
[文章小结](https://kexue.fm/kexue.fm#%E6%96%87%E7%AB%A0%E5%B0%8F%E7%BB%93)

### 智能搜索

支持整句搜索！网站自动使用 [结巴分词](https://github.com/fxsjy/jieba) 进行分词，并结合ngrams排序算法给出合理的搜索结果。

### 热门标签

[生成模型](https://kexue.fm/tag/%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B/) [attention](https://kexue.fm/tag/attention/) [优化](https://kexue.fm/tag/%E4%BC%98%E5%8C%96/) [语言模型](https://kexue.fm/tag/%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/) [模型](https://kexue.fm/tag/%E6%A8%A1%E5%9E%8B/) [网站](https://kexue.fm/tag/%E7%BD%91%E7%AB%99/) [概率](https://kexue.fm/tag/%E6%A6%82%E7%8E%87/) [梯度](https://kexue.fm/tag/%E6%A2%AF%E5%BA%A6/) [矩阵](https://kexue.fm/tag/%E7%9F%A9%E9%98%B5/) [转载](https://kexue.fm/tag/%E8%BD%AC%E8%BD%BD/) [优化器](https://kexue.fm/tag/%E4%BC%98%E5%8C%96%E5%99%A8/) [微分方程](https://kexue.fm/tag/%E5%BE%AE%E5%88%86%E6%96%B9%E7%A8%8B/) [分析](https://kexue.fm/tag/%E5%88%86%E6%9E%90/) [天象](https://kexue.fm/tag/%E5%A4%A9%E8%B1%A1/) [深度学习](https://kexue.fm/tag/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/) [积分](https://kexue.fm/tag/%E7%A7%AF%E5%88%86/) [python](https://kexue.fm/tag/python/) [力学](https://kexue.fm/tag/%E5%8A%9B%E5%AD%A6/) [无监督](https://kexue.fm/tag/%E6%97%A0%E7%9B%91%E7%9D%A3/) [扩散](https://kexue.fm/tag/%E6%89%A9%E6%95%A3/) [几何](https://kexue.fm/tag/%E5%87%A0%E4%BD%95/) [节日](https://kexue.fm/tag/%E8%8A%82%E6%97%A5/) [生活](https://kexue.fm/tag/%E7%94%9F%E6%B4%BB/) [文本生成](https://kexue.fm/tag/%E6%96%87%E6%9C%AC%E7%94%9F%E6%88%90/) [数论](https://kexue.fm/tag/%E6%95%B0%E8%AE%BA/)

### 随机文章

- [线性微分方程组：已知特解求通解](https://kexue.fm/archives/2644)
- [fashion mnist的一个baseline (MobileNet 95%)](https://kexue.fm/archives/4556)
- [不成功的尝试：将多标签交叉熵推广到“n个m分类”上去](https://kexue.fm/archives/9158)
- [更换了一个相册程序](https://kexue.fm/archives/1622)
- [一维弹簧的运动（上）](https://kexue.fm/archives/2430)
- [素数之美1：所有素数之积](https://kexue.fm/archives/2789)
- [警察捉贼,追牛问题,导弹跟踪](https://kexue.fm/archives/1047)
- [从对称角度看代数方程](https://kexue.fm/archives/1336)
- [基于Bert的NL2SQL模型：一个简明的Baseline](https://kexue.fm/archives/6771)
- [【NASA每日一图】火星上的沙尘暴](https://kexue.fm/archives/410)

### 最近评论

- [苏剑林](https://kexue.fm/archives/11371/comment-page-1#comment-28700): 受教了，感谢指点，我沿着修正后的理解继续思考一下。
- [PengleZhang](https://kexue.fm/archives/11371/comment-page-1#comment-28699): 是的, 我们可以理解为, 低精度 tensor core 理论上的误差, 只有 $A,B$ 转...
- [苏剑林](https://kexue.fm/archives/11371/comment-page-1#comment-28698): \[comment=28697\]PengleZhang\[/comment\]我新开一楼。设两个矩阵...
- [PengleZhang](https://kexue.fm/archives/11371/comment-page-1#comment-28697): 嗯. 固有精度为 bf16 是有问题的.
在 \`torch.matmul\` 层面确实是 bf1...
- [苏剑林](https://kexue.fm/archives/11371/comment-page-1#comment-28696): 我要表达的是：$\\bar{p}\_i$,$v\_i$都是bf16的，$\\bar{p}\_i v\_i$...
- [PengleZhang](https://kexue.fm/archives/11371/comment-page-1#comment-28695): $\\overline p$ 和 $v\_i$ 是 bf16 的, 但是 $\\overline p...
- [苏剑林](https://kexue.fm/archives/11371/comment-page-1#comment-28694): 如果我没理解错，$\\sum\_i \\bar{p}\_i v\_i$的每一项$\\bar{p}\_i v\_...
- [PengleZhang](https://kexue.fm/archives/11371/comment-page-1#comment-28693): 感谢回复!
但是感觉这和我的解释没有冲突.
p 和 v 都是 bf16, 但是 pv 在 te...
- [苏剑林](https://kexue.fm/archives/11371/comment-page-1#comment-28692): 统一回复 \[comment=28687\]jerrick\[/comment\]和\[comment=...
- [苏剑林](https://kexue.fm/archives/9209/comment-page-7#comment-28691): 我觉得就是两个不同特点的SDE咯，你这里的“理解”是什么含义呢？

### 友情链接

- [Cool Papers](https://papers.cool)
- [数学研发](https://bbs.emath.ac.cn)
- [Seatop](http://www.seatop.com.cn/)
- [Xiaoxia](https://xiaoxia.org/)
- [积分表-网络版](https://kexue.fm/sci/integral/index.html)
- [丝路博傲](http://blog.dvxj.com/)
- [数学之家](http://www.2math.cn/)
- [有趣天文奇观](http://interesting-sky.china-vo.org/)
- [TwistedW](http://www.twistedwg.com/)
- [godweiyang](https://godweiyang.com/)
- [AI柠檬](https://blog.ailemon.net/)
- [王登科-DK博客](https://greatdk.com)
- [ESON](https://blog.eson.org/)
- [枫之羽](https://fzhiy.net/)
- [Mathor's blog](https://wmathor.com/)
- [coding-zuo](https://coding-zuo.github.io/)
- [博科园](https://www.bokeyuan.net/)
- [孔皮皮的博客](https://www.kppkkp.top/)
- [运鹏的博客](https://yunpengtai.top/)
- [jiming.site](https://jiming.site/)
- [OmegaXYZ](https://www.omegaxyz.com/)
- [EAI猩球](https://www.robotech.ink/)
- [文举的博客](https://liwenju0.com/)
- [用代码打点酱油](https://bruceyuan.com/)
- [Zhang's blog](https://armcvai.cn/)
- [申请链接](https://kexue.fm/links.html)

本站采用创作共用版权协议，要求署名、非商业用途和保持一致。转载本站内容必须也遵循“ [署名-非商业用途-保持一致](http://creativecommons.org/licenses/by-nc-nd/2.5/cn/)”的创作共用协议。
© 2009-2025 Scientific Spaces. All rights reserved. Theme by [laogui](http://www.laogui.com). Powered by [Typecho](http://typecho.org). 备案号: [粤ICP备09093259号-1/2](https://beian.miit.gov.cn/)。