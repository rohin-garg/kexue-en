熵不变性Softmax的一个快速推导 - 科学空间|Scientific Spaces
![MobileSideBar](https://kexue.fm/usr/themes/geekg/images/slide-button.png "MobileSideBar")
## SEARCH
## MENU
* [打赏](https://kexue.fm/reward.html)
* [公式](https://kexue.fm/latex.html)
* [天象](https://kexue.fm/ac.html)
* [链接](https://kexue.fm/links.html)
* [时光](https://kexue.fm/me.html)
* [博览](https://kexue.fm/science.html)
* [归档](https://kexue.fm/content.html)
## CATEGORIES
* [千奇百怪](https://kexue.fm/category/Everything)
* [天文探索](https://kexue.fm/category/Astronomy)
* [数学研究](https://kexue.fm/category/Mathematics)
* [物理化学](https://kexue.fm/category/Phy-chem)
* [信息时代](https://kexue.fm/category/Big-Data)
* [生物自然](https://kexue.fm/category/Biology)
* [图片摄影](https://kexue.fm/category/Photograph)
* [问题百科](https://kexue.fm/category/Questions)
* [生活/情感](https://kexue.fm/category/Life-Feeling)
* [资源共享](https://kexue.fm/category/Resources)
## NEWPOSTS
* [让炼丹更科学一些（五）：基于梯度精...](https://kexue.fm/archives/11530)
* [让炼丹更科学一些（四）：新恒等式，...](https://kexue.fm/archives/11494)
* [为什么DeltaNet要加L2 N...](https://kexue.fm/archives/11486)
* [让炼丹更科学一些（三）：SGD的终...](https://kexue.fm/archives/11480)
* [让炼丹更科学一些（二）：将结论推广...](https://kexue.fm/archives/11469)
* [滑动平均视角下的权重衰减和学习率](https://kexue.fm/archives/11459)
* [生成扩散模型漫谈（三十一）：预测数...](https://kexue.fm/archives/11428)
* [Muon优化器指南：快速上手与关键细节](https://kexue.fm/archives/11416)
* [AdamW的Weight RMS的...](https://kexue.fm/archives/11404)
* [n个正态随机数的最大值的渐近估计](https://kexue.fm/archives/11390)
## COMMENTS
* [Rapture D: 我有一个问题，为什么不考虑亥姆霍兹定理和斯托克斯公式。](https://kexue.fm/archives/11530/comment-page-1#comment-29104)
* [mofheka: 苏神是还在用jax是么？最近在做基于Google Pathwa...](https://kexue.fm/archives/11390/comment-page-1#comment-29103)
* [长琴: 看懂这篇博客也不是一件容易的事情。](https://kexue.fm/archives/11530/comment-page-1#comment-29102)
* [AlexLi: 苏老师，请教一下(7)式中将 $\\mu(x\_t)$ 传给$p...](https://kexue.fm/archives/9257/comment-page-4#comment-29101)
* [tyler\_zxc: "Performer的思想是将标准的Attention线性化，...](https://kexue.fm/archives/7921/comment-page-2#comment-29100)
* [我: 似乎并非mHC提出矩阵的思想？之前hyper connecti...](https://kexue.fm/archives/11494/comment-page-1#comment-29099)
* [winter: 苏神您好，假如对于比较均匀的attention weightP...](https://kexue.fm/archives/10847/comment-page-1#comment-29098)
* [苏剑林: KL散度、JS散度、W距离啥的，都行啊，看你喜欢哪个](https://kexue.fm/archives/8512/comment-page-2#comment-29097)
* [苏剑林: 没有绝对公平的对比方法，主要看你关心什么。比如，如果只关心推理...](https://kexue.fm/archives/9119/comment-page-14#comment-29096)
* [苏剑林: 如果我有时间重新搭建博客，应该会用python自己写了，而不用...](https://kexue.fm/links.html/comment-page-6#comment-29095)
## USERLOGIN
* [登录](https://kexue.fm/admin/login.php)
[科学空间|Scientific Spaces](https://kexue.fm)
* [登录](https://kexue.fm/admin/login.php)
* [打赏](https://kexue.fm/reward.html)
* [公式](https://kexue.fm/latex.html)
* [天象](https://kexue.fm/ac.html)
* [链接](https://kexue.fm/links.html)
* [时光](https://kexue.fm/me.html)
* [博览](https://kexue.fm/science.html)
* [归档](https://kexue.fm/content.html)
渴望成为一个小飞侠* [![](https://kexue.fm/usr/themes/geekg/images/rss.png)
欢迎订阅](https://kexue.fm/feed)
* [![](https://kexue.fm/usr/themes/geekg/images/mail.png)
个性邮箱](https://kexue.fm/archives/119)
* [![](https://kexue.fm/usr/themes/geekg/images/Saturn.png)
天象信息](https://kexue.fm/ac.html)
* [![](https://kexue.fm/usr/themes/geekg/images/iss.png)
观测ISS](https://kexue.fm/archives/41)
* [![](https://kexue.fm/usr/themes/geekg/images/pi.png)
LaTeX](https://kexue.fm/latex.html)
* [![](https://kexue.fm/usr/themes/geekg/images/mlogo.png)
关于博主](https://kexue.fm/me.html)
欢迎访问“科学空间”，这里将与您共同探讨自然科学，回味人生百态；也期待大家的分享～* [**千奇百怪**Everything](https://kexue.fm/category/Everything)
* [**天文探索**Astronomy](https://kexue.fm/category/Astronomy)
* [**数学研究**Mathematics](https://kexue.fm/category/Mathematics)
* [**物理化学**Phy-chem](https://kexue.fm/category/Phy-chem)
* [**信息时代**Big-Data](https://kexue.fm/category/Big-Data)
* [**生物自然**Biology](https://kexue.fm/category/Biology)
* [**图片摄影**Photograph](https://kexue.fm/category/Photograph)
* [**问题百科**Questions](https://kexue.fm/category/Questions)
* [**生活/情感**Life-Feeling](https://kexue.fm/category/Life-Feeling)
* [**资源共享**Resources](https://kexue.fm/category/Resources)
* [**千奇百怪**](https://kexue.fm/category/Everything)
* [**天文探索**](https://kexue.fm/category/Astronomy)
* [**数学研究**](https://kexue.fm/category/Mathematics)
* [**物理化学**](https://kexue.fm/category/Phy-chem)
* [**信息时代**](https://kexue.fm/category/Big-Data)
* [**生物自然**](https://kexue.fm/category/Biology)
* [**图片摄影**](https://kexue.fm/category/Photograph)
* [**问题百科**](https://kexue.fm/category/Questions)
* [**生活/情感**](https://kexue.fm/category/Life-Feeling)
* [**资源共享**](https://kexue.fm/category/Resources)
[首页](https://kexue.fm)[数学研究](https://kexue.fm/category/Mathematics)熵不变性Softmax的一个快速推导
11Apr
# [熵不变性Softmax的一个快速推导](https://kexue.fm/archives/9034)
By苏剑林|2022-04-11|30348位读者|:
在文章[《从熵不变性看Attention的Scale操作》](https://kexue.fm/archives/8823)中，我们推导了一版具有熵不变性质的注意力机制：
\\begin{equation}Attention(Q,K,V) = softmax\\left(\\frac{\\kappa \\log n}{d}QK^{\\top}\\right)V\\label{eq:a}\\end{equation}
可以观察到，它主要是往Softmax里边引入了长度相关的缩放因子$\\log n$来实现的。原来的推导比较繁琐，并且做了较多的假设，不利于直观理解，本文为其补充一个相对简明快速的推导。
## 推导过程[#](#推导过程)
我们可以抛开注意力机制的背景，直接设有$s\_1,s\_2,\\cdots,s\_n\\in\\mathbb{R}$，定义
$$p\_i = \\frac{e^{\\lambda s\_i}}{\\sum\\limits\_{i=1}^n e^{\\lambda s\_i}}$$
显然这就是$s\_1,s\_2,\\cdots,s\_n$同时乘上缩放因子$\\lambda$后做Softmax的结果。现在我们算它的熵
\\begin{equation}\\begin{aligned}H =&\, -\s&\, -\sum\_{i=1}^n p\_i \\log p\_i = \\log\\sum\_{i=1}^n e^{\\lambda s\_i} - \\lambda\\sum\_{i=1}^n p\_i s\_i \\\\
=&\, \lo&\, \log n + \\log\\frac{1}{n}\\sum\_{i=1}^n e^{\\lambda s\_i} - \\lambda\\sum\_{i=1}^n p\_i s\_i
\\end{aligned}\\end{equation}
第一项的$\\log$里边是“先指数后平均”，我们用“先平均后指数”（平均场）来近似它：
\\begin{equation}
\\log\\frac{1}{n}\\sum\_{i=1}^n e^{\\lambda s\_i}\\approx \\log\\exp\\left(\\frac{1}{n}\\sum\_{i=1}^n \\lambda s\_i\\right) = \\lambda \\bar{s}
\\end{equation}
然后我们知道Softmax是会侧重于$\\max$的那个（参考[《函数光滑化杂谈：不可导函数的可导逼近》](https://kexue.fm/archives/6620#softmax)），所以有近似
\\begin{equation}\\lambda\\sum\_{i=1}^n p\_i s\_i \\approx \\lambda s\_{\\max}\\end{equation}
所以\\begin{equation}H\\approx \\log n - \\lambda(s\_{\\max} - \\bar{s})\\end{equation}
所谓熵不变性，就是希望尽可能地消除长度$n$的影响，所以根据上式我们需要有$\\lambda\\propto \\log n$。如果放到注意力机制中，那么$s$的形式为$\\langle \\boldsymbol{q}, \\boldsymbol{k}\\rangle\\propto d$（$d$是向量维度），所以需要有$\\lambda\\propto \\frac{1}{d}$，综合起来就是
\\begin{equation}\\lambda\\propto \\frac{\\log n}{d}\\end{equation}
这就是文章开头式$\\eqref{eq:a}$的结果。
## 文章小结[#](#文章小结)
为之前提出的“熵不变性Softmax”构思了一个简单明快的推导。
***转载到请包括本文地址：** [https://kexue.fm/archives/9034](https://kexue.fm/archives/9034)*
***更详细的转载事宜请参考：*** [《科学空间FAQ》](https://kexue.fm/archives/6508#文章如何转载/引用)
**如果您还有什么疑惑或建议，欢迎在下方评论区继续讨论。**
**如果您觉得本文还不错，欢迎[分享](#share)/[打赏](#pay)本文。打赏并非要从中获得收益，而是希望知道科学空间获得了多少读者的真心关注。当然，如果你无视它，也不会影响你的阅读。再次表示欢迎和感谢！**
打赏![科学空间](https://kexue.fm/usr/themes/geekg/payment/wx.png)
微信打赏![科学空间](https://kexue.fm/usr/themes/geekg/payment/zfb.png)
支付宝打赏因为网站后台对打赏并无记录，因此欢迎在打赏时候备注留言。 你还可以[**点击这里**](http://mail.qq.com/cgi-bin/qm_share?t=qm_mailme&email=tN7d1drY3drrx8H0xcWa19vZ)或在下方评论区留言来告知你的建议或需求。
**如果您需要引用本文，请参考：**
苏剑林. (Apr. 11, 2022). 《熵不变性Softmax的一个快速推导 》[Blog post]. Retrieved from[https://kexue.fm/archives/9034](https://kexue.fm/archives/9034)
@online{kexuefm-9034,
title={熵不变性Softmax的一个快速推导},
author={苏剑林},
year={2022},
month={Apr},
url={\\url{https://kexue.fm/archives/9034}},
}
分类：[数学研究](https://kexue.fm/category/Mathematics) 标签：[近似](https://kexue.fm/tag/近似/),[熵](https://kexue.fm/tag/熵/),[attention](https://kexue.fm/tag/attention/)[6 评论](https://kexue.fm/archives/9034#comments)
&lt;[听说Attention与Softmax更配哦～](https://kexue.fm/archives/9019)|[GlobalPointer下的“KL散度”应该是怎样的？](https://kexue.fm/archives/9039)&gt;
### 你也许还对下面的内容感兴趣* [为什么DeltaNet要加L2 Normalize？](https://kexue.fm/archives/11486)
* [n个正态随机数的最大值的渐近估计](https://kexue.fm/archives/11390)
* [低精度Attention可能存在有偏的舍入误差](https://kexue.fm/archives/11371)
* [随机矩阵的谱范数的快速估计](https://kexue.fm/archives/11335)
* [为什么线性注意力要加Short Conv？](https://kexue.fm/archives/11320)
* [QK-Clip：让Muon在Scaleup之路上更进一步](https://kexue.fm/archives/11126)
* [Transformer升级之路：21、MLA好在哪里?（下）](https://kexue.fm/archives/11111)
* [“对角+低秩”三角阵的高效求逆方法](https://kexue.fm/archives/11072)
* [通过msign来计算奇异值裁剪mclip（下）](https://kexue.fm/archives/11059)
* [线性注意力简史：从模仿、创新到反哺](https://kexue.fm/archives/11033)
[发表你的看法](#comment_form)
alafeizai
February 1st, 2023
精彩，请问式(3)，即log里用“先平均后指数”替换“先指数后平均”来近似，这一步的误差分析有什么地方可以参考么，感觉lambda\*Si可以很大的话，误差上限有可能会不小，极限情况下lambda\*Sk是n，其它是0的话，那么先平均再指数是e，而先指数再平均约是e^n了
[回复评论](https://kexue.fm/archives/9034/comment-page-1?replyTo=20847#respond-post-9034)
[苏剑林](https://kexue.fm)发表于 February 3rd, 2023
这个过程更像是一个量纲分析，很难再在定量上有啥进展，因为最终的结果也只是$\\lambda\\propto \\frac{\\log n}{d}$而非准确的值，若不然深究的话，很多结果都经不起质疑，比如我们用到最后一步用到$\\langle \\boldsymbol{q},\\boldsymbol{k}\\rangle \\propto d$（$d$是向量维度），这也只是一个直觉上的近似。
[回复评论](https://kexue.fm/archives/9034/comment-page-1?replyTo=20873#respond-post-9034)
wjfwzzc
June 29th, 2023
(3)和(4)是不是可以直接合并掉，这样就更“快速”了。“先指数后平均”就是logsumexp，直接光滑近似到max了。
[回复评论](https://kexue.fm/archives/9034/comment-page-1?replyTo=22110#respond-post-9034)
[苏剑林](https://kexue.fm)发表于 July 1st, 2023
没看明白，怎么合并？[回复评论](https://kexue.fm/archives/9034/comment-page-1?replyTo=22126#respond-post-9034)
tmz 发表于August 19th, 2025
我猜测你的意思是将(2),(3)式里面的log-sum-exp直接替换成max，但是其实这里(3)和(4)对应的是(2)中的两项，直接替换只会变成两个近似max相减最后没办法确定$\\lambda$的限制
关于(3)式的推导我觉得是比较特殊的，$softmax(s\_i)s\_i$和log-sum-exp都是对max的光滑近似，它们的区别主要在于近似的程度，$softmax(s\_i)s\_i$取值在$[min(s\_i),max(s\_i)]$之间，而且更接近$max(s\_i)$；log-sum-exp在$max(s\_i)$的基础上还有比较多被忽略掉的其他$exp(s\_i)$的加和，因此它的取值在$[max(s\_i)，max(s\_i)+log(n)]$之间，在$s\_i$值相同时取另一端极值，所以当$max(s\_i)\>\>s\_j$的时候$H$趋近于0，意思是有突出的峰值时不需要考虑熵不变性的影响，我们主要需要讨论的是$s\_i$相等的时候，$H$趋近于$log(n)$，这种趋近于均匀分布的情况下的熵不变性，因此在这种情况下平均场理论的近似会比max光滑近似更加合理
[回复评论](https://kexue.fm/archives/9034/comment-page-1?replyTo=28417#respond-post-9034)
[苏剑林](https://kexue.fm)发表于 August 22nd, 2025
现在的推导是有点取巧的成分，我们可以理解为是一个下界结果吧，这样就比较严谨。[回复评论](https://kexue.fm/archives/9034/comment-page-1?replyTo=28431#respond-post-9034)
[取消回复](https://kexue.fm/archives/9034#respond-post-9034)
你的大名电子邮箱个人网站（选填）1. 可以使用LaTeX代码，点击“预览效果”可查看效果；
2. 可以通过点击评论楼层编号来引用该楼层；3. 网站可能会有点卡，如非确认评论失败，请**不要重复点击提交**。
********************
### 内容速览* [推导过程](#推导过程)
* [文章小结](#文章小结)
********************
### 智能搜索支持整句搜索！网站自动使用[结巴分词](https://github.com/fxsjy/jieba)进行分词，并结合ngrams排序算法给出合理的搜索结果。
********************
### 热门标签[生成模型](https://kexue.fm/tag/生成模型/)[attention](https://kexue.fm/tag/attention/)[优化](https://kexue.fm/tag/优化/)[语言模型](https://kexue.fm/tag/语言模型/)[模型](https://kexue.fm/tag/模型/)[梯度](https://kexue.fm/tag/梯度/)[网站](https://kexue.fm/tag/网站/)[概率](https://kexue.fm/tag/概率/)[矩阵](https://kexue.fm/tag/矩阵/)[优化器](https://kexue.fm/tag/优化器/)[转载](https://kexue.fm/tag/转载/)[微分方程](https://kexue.fm/tag/微分方程/)[分析](https://kexue.fm/tag/分析/)[天象](https://kexue.fm/tag/天象/)[深度学习](https://kexue.fm/tag/深度学习/)[积分](https://kexue.fm/tag/积分/)[python](https://kexue.fm/tag/python/)[扩散](https://kexue.fm/tag/扩散/)[力学](https://kexue.fm/tag/力学/)[无监督](https://kexue.fm/tag/无监督/)[几何](https://kexue.fm/tag/几何/)[节日](https://kexue.fm/tag/节日/)[生活](https://kexue.fm/tag/生活/)[文本生成](https://kexue.fm/tag/文本生成/)[数论](https://kexue.fm/tag/数论/)
********************
********************
### 随机文章* [为什么Adam的Update RMS是0.2？](https://kexue.fm/archives/11267)
* [勒贝格(Lebesgue)控制收敛定理](https://kexue.fm/archives/3194)
* [《为什么现在的LLM都是Decoder-only的架构？》FAQ](https://kexue.fm/archives/9547)
* [高考结束了](https://kexue.fm/archives/1613)
* [【理解黎曼几何】8. 处处皆几何(力学几何化)](https://kexue.fm/archives/4046)
* [2012年快乐！](https://kexue.fm/archives/1523)
* [果壳中的条件随机场(CRF In A Nutshell)](https://kexue.fm/archives/4695)
* [【NASA每日一图】武汉上空的日食“钻戒”](https://kexue.fm/archives/66)
* [【外微分浅谈】5. 几何意义](https://kexue.fm/archives/4062)
* [变分法的一个技巧及其“误用”](https://kexue.fm/archives/2040)
********************
********************
### 最近评论* [Rapture D](https://kexue.fm/archives/11530/comment-page-1#comment-29104): 我有一个问题，为什么不考虑亥姆霍兹定理和斯托克斯公式。* [mofheka](https://kexue.fm/archives/11390/comment-page-1#comment-29103): 苏神是还在用jax是么？最近在做基于Google Pathway的理念做一个动态版的MPMD框...
* [长琴](https://kexue.fm/archives/11530/comment-page-1#comment-29102): 看懂这篇博客也不是一件容易的事情。* [AlexLi](https://kexue.fm/archives/9257/comment-page-4#comment-29101): 苏老师，请教一下(7)式中将 $\\mu(x\_t)$ 传给$p\_o$ 进行推理的操作。$x\_...
* [tyler\_zxc](https://kexue.fm/archives/7921/comment-page-2#comment-29100): "Performer的思想是将标准的Attention线性化，所以为什么不干脆直接训练一个线性...
* [我](https://kexue.fm/archives/11494/comment-page-1#comment-29099): 似乎并非mHC提出矩阵的思想？之前hyper connection就是了
* [winter](https://kexue.fm/archives/10847/comment-page-1#comment-29098): 苏神您好，假如对于比较均匀的attention weightP，往往呈现long tail分布...
* [苏剑林](https://kexue.fm/archives/8512/comment-page-2#comment-29097): KL散度、JS散度、W距离啥的，都行啊，看你喜欢哪个
* [苏剑林](https://kexue.fm/archives/9119/comment-page-14#comment-29096): 没有绝对公平的对比方法，主要看你关心什么。比如，如果只关心推理成本和推理效果，那么有的方法可以...
* [苏剑林](https://kexue.fm/links.html/comment-page-6#comment-29095): 如果我有时间重新搭建博客，应该会用python自己写了，而不用第三方架构，这样可玩性好很多。事...
********************
********************
### 友情链接* [Cool Papers](https://papers.cool)
* [数学研发](https://bbs.emath.ac.cn)
* [Seatop](http://www.seatop.com.cn/)
* [Xiaoxia](https://xiaoxia.org/)
* [积分表-网络版](https://kexue.fm/sci/integral/index.html)
* [丝路博傲](http://blog.dvxj.com/)
* [数学之家](http://www.2math.cn/)
* [有趣天文奇观](http://interesting-sky.china-vo.org/)
* [TwistedW](http://www.twistedwg.com/)
* [godweiyang](https://godweiyang.com/)
* [AI柠檬](https://blog.ailemon.net/)
* [王登科-DK博客](https://greatdk.com)
* [ESON](https://blog.eson.org/)
* [枫之羽](https://fzhiy.net/)
* [coding-zuo](https://coding-zuo.github.io/)
* [博科园](https://www.bokeyuan.net/)
* [孔皮皮的博客](https://www.kppkkp.top/)
* [运鹏的博客](https://yunpengtai.top/)
* [jiming.site](https://jiming.site/)
* [OmegaXYZ](https://www.omegaxyz.com/)
* [EAI猩球](https://www.robotech.ink/)
* [文举的博客](https://liwenju0.com/)
* [申请链接](https://kexue.fm/links.html)
********************
[![署名-非商业用途-保持一致](https://kexue.fm/usr/themes/geekg/images/cc.gif)](http://creativecommons.org/licenses/by-nc-nd/2.5/cn/)本站采用创作共用版权协议，要求署名、非商业用途和保持一致。转载本站内容必须也遵循“[署名-非商业用途-保持一致](http://creativecommons.org/licenses/by-nc-nd/2.5/cn/)”的创作共用协议。
©2009-2026 Scientific Spaces. All rights reserved. Theme by[laogui](http://www.laogui.com). Powered by[Typecho](http://typecho.org). 备案号:[粤ICP备09093259号-1/2](https://beian.miit.gov.cn/)。