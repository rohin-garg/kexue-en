## SEARCH

## MENU

- [打赏](https://kexue.fm/reward.html)
- [公式](https://kexue.fm/latex.html)
- [天象](https://kexue.fm/ac.html)
- [链接](https://kexue.fm/links.html)
- [时光](https://kexue.fm/me.html)
- [博览](https://kexue.fm/science.html)
- [归档](https://kexue.fm/content.html)

## CATEGORIES

- [千奇百怪](https://kexue.fm/category/Everything)
- [天文探索](https://kexue.fm/category/Astronomy)
- [数学研究](https://kexue.fm/category/Mathematics)
- [物理化学](https://kexue.fm/category/Phy-chem)
- [信息时代](https://kexue.fm/category/Big-Data)
- [生物自然](https://kexue.fm/category/Biology)
- [图片摄影](https://kexue.fm/category/Photograph)
- [问题百科](https://kexue.fm/category/Questions)
- [生活/情感](https://kexue.fm/category/Life-Feeling)
- [资源共享](https://kexue.fm/category/Resources)

## NEWPOSTS

- [线性注意力简史：从模仿、创新到反哺](https://kexue.fm/archives/11033)
- [msign的导数](https://kexue.fm/archives/11025)
- [通过msign来计算mclip（奇...](https://kexue.fm/archives/11006)
- [msign算子的Newton-Sc...](https://kexue.fm/archives/10996)
- [等值振荡定理：最优多项式逼近的充要条件](https://kexue.fm/archives/10972)
- [生成扩散模型漫谈（三十）：从瞬时速...](https://kexue.fm/archives/10958)
- [MoE环游记：5、均匀分布的反思](https://kexue.fm/archives/10945)
- [msign算子的Newton-Sc...](https://kexue.fm/archives/10922)
- [Transformer升级之路：2...](https://kexue.fm/archives/10907)
- [一道概率不等式：盯着它到显然成立为止！](https://kexue.fm/archives/10902)

## COMMENTS

- [忍者猫: 这优化器的作者真的应该给你打钱](https://kexue.fm/archives/10592/comment-page-2#comment-27952)
- [Chaofa Yuan: 写得太好了](https://kexue.fm/archives/11033/comment-page-1#comment-27951)
- [Skyler Lin: respect苏神！](https://kexue.fm/archives/11033/comment-page-1#comment-27949)
- [宋佳铭: 对，个人感觉mean flow就是continuous tim...](https://kexue.fm/archives/10958/comment-page-1#comment-27947)
- [宋佳铭: 的确，对sg这个事情我感觉如果是用‘归纳’法做是不太能避免的，...](https://kexue.fm/archives/10958/comment-page-1#comment-27946)
- [MoFHeka: 苏老师您好，请问一下这套结论在稀疏参数上应该如何应用？比如大规...](https://kexue.fm/archives/10542/comment-page-1#comment-27945)
- [苏剑林: Temp LoRA倒是有印象，其实思想是一样的，如果我单独开一...](https://kexue.fm/archives/11033/comment-page-1#comment-27944)
- [苏剑林: 你搜搜mamba、rwkv甚至rnn做vision的工作，其实...](https://kexue.fm/archives/11033/comment-page-1#comment-27943)
- [苏剑林: 问题1可以看看 https://kexue.fm/archiv...](https://kexue.fm/archives/9379/comment-page-1#comment-27942)
- [苏剑林: 你的“信息量”怎么定义？直观来说，reflow训练的是切线模型...](https://kexue.fm/archives/10958/comment-page-2#comment-27941)

## USERLOGIN

- [登录](https://kexue.fm/admin/login.php)

[科学空间\|Scientific Spaces](https://kexue.fm)

- [登录](https://kexue.fm/admin/login.php)
- [打赏](https://kexue.fm/reward.html)
- [公式](https://kexue.fm/latex.html)
- [天象](https://kexue.fm/ac.html)
- [链接](https://kexue.fm/links.html)
- [时光](https://kexue.fm/me.html)
- [博览](https://kexue.fm/science.html)
- [归档](https://kexue.fm/content.html)

渴望成为一个小飞侠

- [欢迎订阅](https://kexue.fm/feed)
- [个性邮箱](https://kexue.fm/archives/119)
- [天象信息](https://kexue.fm/ac.html)
- [观测ISS](https://kexue.fm/archives/41)
- [LaTeX](https://kexue.fm/latex.html)
- [关于博主](https://kexue.fm/me.html)

欢迎访问“科学空间”，这里将与您共同探讨自然科学，回味人生百态；也期待大家的分享～

- [**千奇百怪** Everything](https://kexue.fm/category/Everything)
- [**天文探索** Astronomy](https://kexue.fm/category/Astronomy)
- [**数学研究** Mathematics](https://kexue.fm/category/Mathematics)
- [**物理化学** Phy-chem](https://kexue.fm/category/Phy-chem)
- [**信息时代** Big-Data](https://kexue.fm/category/Big-Data)
- [**生物自然** Biology](https://kexue.fm/category/Biology)
- [**图片摄影** Photograph](https://kexue.fm/category/Photograph)
- [**问题百科** Questions](https://kexue.fm/category/Questions)
- [**生活/情感** Life-Feeling](https://kexue.fm/category/Life-Feeling)
- [**资源共享** Resources](https://kexue.fm/category/Resources)

- [**千奇百怪**](https://kexue.fm/category/Everything)
- [**天文探索**](https://kexue.fm/category/Astronomy)
- [**数学研究**](https://kexue.fm/category/Mathematics)
- [**物理化学**](https://kexue.fm/category/Phy-chem)
- [**信息时代**](https://kexue.fm/category/Big-Data)
- [**生物自然**](https://kexue.fm/category/Biology)
- [**图片摄影**](https://kexue.fm/category/Photograph)
- [**问题百科**](https://kexue.fm/category/Questions)
- [**生活/情感**](https://kexue.fm/category/Life-Feeling)
- [**资源共享**](https://kexue.fm/category/Resources)

[首页](https://kexue.fm) [数学研究](https://kexue.fm/category/Mathematics) [信息时代](https://kexue.fm/category/Big-Data) 基于Amos优化器思想推导出来的一些“炼丹策略”

22Nov

# [基于Amos优化器思想推导出来的一些“炼丹策略”](https://kexue.fm/archives/9344)

By 苏剑林 \|
2022-11-22 \|
40594位读者\|

如果将训练模型比喻为“炼丹”，那么“炼丹炉”显然就是优化器了。据传AdamW优化器是当前训练神经网络最快的方案，这一点笔者也没有一一对比过，具体情况如何不得而知，不过目前做预训练时多数都用AdamW或其变种LAMB倒是真的。然而，正如有了炼丹炉也未必能炼出好丹，即便我们确定了选择AdamW优化器，依然有很多问题还没有确定的答案，比如：

> 1、学习率如何适应不同初始化和参数化？
>
> 2、权重衰减率该怎么调？
>
> 3、学习率应该用什么变化策略？
>
> 4、能不能降低优化器的显存占用？

尽管在实际应用时，我们大多数情况下都可以直接套用前人已经调好的参数和策略，但缺乏比较系统的调参指引，始终会让我们在“炼丹”之时感觉没有底气。在这篇文章中，我们基于Google最近提出的Amos优化器的思路，给出一些参考结果。

## 基础回顾 [\#](https://kexue.fm/archives/9344\#%E5%9F%BA%E7%A1%80%E5%9B%9E%E9%A1%BE)

Amos优化器出自Google最近的论文 [《Amos: An Adam-style Optimizer with Adaptive Weight Decay towards Model-Oriented Scale》](https://papers.cool/arxiv/2210.11693)，它对上述几个问题都推导了比较完整的推导，并通过实验证实了它的有效性。然而，原论文的推导实在是不好读，各种记号和估计都过于随意，给人很“凌乱”感觉。不过好在Amos的思想还不算复杂，我们可以借用一下。

在开始推导之前，我们不妨先回顾一下对于上述几个问题，现有的解决方案是怎样的。

首先，第一个问题，大家可能不大理解“初始化”和“参数化”分别是什么含义，其实这就是模型权重的两种设置方式，常见的就是一个$n\\times n$的矩阵，一般用“均值为0、方差为$1/n$”的方式初始化，详细介绍可以参考笔者之前 [《从几何视角来理解模型参数的初始化策略》](https://kexue.fm/archives/7180)、 [《浅谈Transformer的初始化、参数化与标准化》](https://kexue.fm/archives/8620)。从“方差为$1/n$”我们就可以看到，不同参数有着不同的尺度（或者说数量级），如果我们用同一个学习率更新所有参数，那么就会导致每个参数的更新幅度不一样。这个问题笔者觉得比较优雅的解决方案就是LAMB优化器，它每次更新的模长直接取决于参数本身的模长，学习率只是用来描述相对更新量的大小。

至于权重衰减率问题，至少在预训练领域，笔者观察到的是都是沿用最早的选择0.01，没有发现去调整该参数的工作。而对于学习率变化策略，大家都知道应该要将学习率慢慢降到零，但具体应该选用什么什么下降策略，暂时也没有太多的理论指导，多数结果也只是实验总结出来的。最后，关于节省显存问题，比较经典的工作就是AdaFactor优化器，笔者之前在 [《AdaFactor优化器浅析（附开源实现）》](https://kexue.fm/archives/7302) 也有过介绍。降低优化器显存占用的主要就两个思路，一是去掉动量，二是对二阶矩做低秩分解，Amos本质上也是沿用了这两个思路。

## 问题设置 [\#](https://kexue.fm/archives/9344\#%E9%97%AE%E9%A2%98%E8%AE%BE%E7%BD%AE)

本文主要关心开头的前三个问题，希望能够推导出一些“即插即用”的结果。首先，我们将优化器的更新规则简写成：
\\begin{equation}\\boldsymbol{\\theta}\_{t+1} = \\boldsymbol{\\theta}\_t - \\alpha\_t \\boldsymbol{u}\_t\\end{equation}
其实$\\boldsymbol{\\theta}\_t, \\boldsymbol{\\theta}\_{t+1}$分别代表$t,t+1$时刻的参数值，$\\boldsymbol{u}\_t$代表$t$时刻的更新向量（依赖于任务和数据），而标量$\\alpha\_t > 0$（向量的每个元素都大于0）代表$t$时刻的学习率。

自AdamW起，主流优化器都倾向于把权重衰减（Weight Decay）项从$\\boldsymbol{u}\_t$中独立出来，即
\\begin{equation}\\boldsymbol{\\theta}\_{t+1} = \\boldsymbol{\\theta}\_t - (\\alpha\_t \\boldsymbol{u}\_t + \\rho\_t\\boldsymbol{\\theta}\_t)\\end{equation}
其中$\\rho\_t > 0$是权重衰减率。本文的主要任务，就是希望能解决$\\alpha\_t$和$\\rho\_t$该怎么设置的问题。

## 权重衰减 [\#](https://kexue.fm/archives/9344\#%E6%9D%83%E9%87%8D%E8%A1%B0%E5%87%8F)

我们知道，权重衰减也好，L2正则也好，它本身是跟训练目标无关的，它只是一个辅助项，目的是提高模型的泛化能力。既然是辅助，那么一个基本的要求就是它不应该“喧宾夺主”，为此，我们不妨加入一个限制：
\\begin{equation}\\mathcal{O}(\\alpha\_t^2) = \\mathcal{O}(\\rho\_t)\\end{equation}
也就是说，在整个更新过程中，权重衰减带来的更新量始终要比目标相关的更新量高一阶，由于$\\alpha\_t,\\rho\_t$基本上都是小于1的，所以更高阶意味着更小。

设优化的参数终点是$\\boldsymbol{\\theta}^\*$，我们记$\\boldsymbol{\\varepsilon}\_t = \\boldsymbol{\\theta}\_t - \\boldsymbol{\\theta}^\*$，根据更新规则可以得到
\\begin{equation}\\begin{aligned}
\\Vert\\boldsymbol{\\varepsilon}\_{t+1}\\Vert^2 =&\\, \\Vert\\boldsymbol{\\theta}\_{t+1} - \\boldsymbol{\\theta}^\*\\Vert^2 \\\
=&\\, \\Vert\\boldsymbol{\\theta}\_t - (\\alpha\_t \\boldsymbol{u}\_t + \\rho\_t\\boldsymbol{\\theta}\_t) - \\boldsymbol{\\theta}^\*\\Vert^2 \\\
\\approx&\\, \\Vert\\boldsymbol{\\varepsilon}\_t\\Vert^2 - 2 \\alpha\_t \\boldsymbol{u}\_t \\cdot \\boldsymbol{\\varepsilon}\_t + \\left(\\alpha\_t^2 \\Vert\\boldsymbol{u}\_t\\Vert^2 - 2 \\rho\_t \\boldsymbol{\\theta}\_t \\cdot \\boldsymbol{\\varepsilon}\_t\\right)
\\end{aligned}\\label{eq:base-approx}\\end{equation}
最后的近似只保留了不超过$\\mathcal{O}(\\alpha\_t^2)$的项。

很明显，$\\Vert\\boldsymbol{\\varepsilon}\_t\\Vert$是当前结果与终点的距离，它自然是越小越好，因此我们自然也希望每一步的更新都能缩小这个距离，即$\\Vert\\boldsymbol{\\varepsilon}\_{t+1}\\Vert < \\Vert\\boldsymbol{\\varepsilon}\_t\\Vert$。而我们看式$\\eqref{eq:base-approx}$，$- 2 \\alpha\_t \\boldsymbol{u}\_t \\cdot \\boldsymbol{\\varepsilon}\_t$可正可负，如果它为负就有助于实现$\\Vert\\boldsymbol{\\varepsilon}\_{t+1}\\Vert < \\Vert\\boldsymbol{\\varepsilon}\_t\\Vert$，但是$\\alpha\_t^2 \\Vert\\boldsymbol{u}\_t\\Vert^2$必然是正的，它是不利于实现$\\Vert\\boldsymbol{\\varepsilon}\_{t+1}\\Vert < \\Vert\\boldsymbol{\\varepsilon}\_t\\Vert$，不过在引入权重衰减后，多出了一项$- 2 \\rho\_t \\boldsymbol{\\theta}\_t \\cdot \\boldsymbol{\\varepsilon}\_t$，如果这一项能抵消掉$\\alpha\_t^2 \\Vert\\boldsymbol{u}\_t\\Vert^2$的负面作用，那么权重衰减的引入就不仅能增强泛化能力，还有利于模型收敛了。

## 可行分析 [\#](https://kexue.fm/archives/9344\#%E5%8F%AF%E8%A1%8C%E5%88%86%E6%9E%90)

所以，接下来的事情，我们就是要考察
\\begin{equation}\\alpha\_t^2 \\Vert\\boldsymbol{u}\_t\\Vert^2 = 2 \\rho\_t \\boldsymbol{\\theta}\_t \\cdot \\boldsymbol{\\varepsilon}\_t\\label{eq:base-cond}\\end{equation}
的可行性。所谓可行性，就是$\\boldsymbol{\\theta}\_t \\cdot \\boldsymbol{\\varepsilon}\_t$能否大于0，只有它大于0，左右两端才有可能相等。利用$\\boldsymbol{\\varepsilon}\_t$的定义我们得到$\\boldsymbol{\\theta}\_t = \\boldsymbol{\\varepsilon}\_t + \\boldsymbol{\\theta}^\*$，于是
\\begin{equation}\\boldsymbol{\\theta}\_t \\cdot \\boldsymbol{\\varepsilon}\_t = (\\boldsymbol{\\varepsilon}\_t + \\boldsymbol{\\theta}^\*) \\cdot \\boldsymbol{\\varepsilon}\_t = \\Vert \\boldsymbol{\\varepsilon}\_t\\Vert^2 + \\boldsymbol{\\theta}^\* \\cdot \\boldsymbol{\\varepsilon}\_t\\end{equation}
注意$\\boldsymbol{\\theta}^\*$是我们的目标，是一个固定的点，而$\\boldsymbol{\\varepsilon}\_t$是当前时刻与目标的差异向量，两者一般来说没什么必然的相关性，于是我们可以近似认为它们是高维空间中两个随机向量。根据 [《n维空间下两个随机向量的夹角分布》](https://kexue.fm/archives/7076)，我们知道高维空间中两个随机向量几乎都是垂直的，于是$\\boldsymbol{\\theta}^\* \\cdot \\boldsymbol{\\varepsilon}\_t\\approx 0$，即$\\boldsymbol{\\theta}\_t \\cdot \\boldsymbol{\\varepsilon}\_t \\approx \\Vert \\boldsymbol{\\varepsilon}\_t\\Vert^2$。当然，如果不放心，还可以引入一个参数$q$：
\\begin{equation}\\boldsymbol{\\theta}\_t \\cdot \\boldsymbol{\\varepsilon}\_t \\approx q\\Vert \\boldsymbol{\\varepsilon}\_t\\Vert^2\\end{equation}
此时式$\\eqref{eq:base-cond}$就变成了
\\begin{equation}\\alpha\_t^2 \\Vert\\boldsymbol{u}\_t\\Vert^2 \\approx 2 \\rho\_t q\\Vert \\boldsymbol{\\varepsilon}\_t\\Vert^2\\label{eq:base-cond-approx}\\end{equation}
两端都大于0，因此式$\\eqref{eq:base-cond}$是有可能成立的。

## 渐近估计 [\#](https://kexue.fm/archives/9344\#%E6%B8%90%E8%BF%91%E4%BC%B0%E8%AE%A1)

如果式$\\eqref{eq:base-cond}$成立，那么式$\\eqref{eq:base-approx}$就简化为了\\begin{equation}\\Vert\\boldsymbol{\\varepsilon}\_{t+1}\\Vert^2 \\approx \\Vert\\boldsymbol{\\varepsilon}\_t\\Vert^2 - 2 \\alpha\_t \\boldsymbol{u}\_t \\cdot \\boldsymbol{\\varepsilon}\_t = \\Vert\\boldsymbol{\\varepsilon}\_t\\Vert^2 - 2 \\alpha\_t \\Vert\\boldsymbol{u}\_t\\Vert \\Vert\\boldsymbol{\\varepsilon}\_t\\Vert \\cos(\\boldsymbol{u}\_t, \\boldsymbol{\\varepsilon}\_t)\\end{equation}
我们说了$\\boldsymbol{u}\_t$代表的是任务相关的更新量，平均来说它必然是有利于任务的（否则原来的优化器就是有缺陷的了），所以平均来说应该有$\\cos(\\boldsymbol{u}\_t, \\boldsymbol{\\varepsilon}\_t) > 0$。这里我们进一步假设，存在一个$p > 0$，使得$\\cos(\\boldsymbol{u}\_t, \\boldsymbol{\\varepsilon}\_t)\\sim p$，于是我们有
\\begin{equation}\\Vert\\boldsymbol{\\varepsilon}\_{t+1}\\Vert^2 \\approx \\Vert\\boldsymbol{\\varepsilon}\_t\\Vert^2 - 2 \\alpha\_t p\\Vert\\boldsymbol{u}\_t\\Vert \\Vert\\boldsymbol{\\varepsilon}\_t\\Vert\\end{equation}
根据近似$\\eqref{eq:base-cond-approx}$我们有$\\alpha\_t \\Vert\\boldsymbol{u}\_t \\Vert \\Vert \\boldsymbol{\\varepsilon}\_t\\Vert \\approx \\sqrt{2 \\rho\_t q}\\Vert \\boldsymbol{\\varepsilon}\_t\\Vert^2$，代入上式得到
\\begin{equation}\\Vert\\boldsymbol{\\varepsilon}\_{t+1}\\Vert^2 \\approx \\Vert\\boldsymbol{\\varepsilon}\_t\\Vert^2(1 - 2 p\\sqrt{2 \\rho\_t q})\\approx \\Vert\\boldsymbol{\\varepsilon}\_t\\Vert^2\\exp(- 2 p\\sqrt{2 \\rho\_t q})\\end{equation}
一步一步往前递推，可以得到
\\begin{equation}\\Vert\\boldsymbol{\\varepsilon}\_t\\Vert^2 \\approx\\Vert\\boldsymbol{\\varepsilon}\_0\\Vert^2\\exp\\left(- 2 \\sum\_{i=1}^{t-1}p\\sqrt{2 \\rho\_i q}\\right)\\label{eq:varepsilon-t}\\end{equation}
可以看出右端的指数必然是单调递减的，它是一个衰减函数。现在我们再看近似$\\eqref{eq:base-cond-approx}$，它有两个参数$\\alpha\_t$和$\\rho\_t$要调，但只有一个（近似）等式。为了使$\\alpha\_t$和$\\rho\_t$能够同等程度地衰减，我们设$2\\rho\_t q \\approx \\lambda^2 \\Vert\\boldsymbol{\\varepsilon}\_t\\Vert^2$，于是解得
\\begin{equation}\\begin{aligned}\\alpha\_t \\approx \\frac{\\lambda\\Vert\\boldsymbol{\\varepsilon}\_t\\Vert^2}{\\Vert\\boldsymbol{u}\_t\\Vert} \\approx&\\, \\frac{\\lambda\\Vert\\boldsymbol{\\varepsilon}\_0\\Vert^2}{\\Vert\\boldsymbol{u}\_t\\Vert} \\exp\\left(- 2 \\sum\_{i=1}^{t-1}p\\sqrt{2 \\rho\_i q}\\right) \\\
\\rho\_t \\approx \\frac{\\lambda^2\\Vert\\boldsymbol{\\varepsilon}\_t\\Vert^2}{2q} \\approx&\\, \\frac{\\lambda^2\\Vert\\boldsymbol{\\varepsilon}\_0\\Vert^2}{2q} \\exp\\left(- 2 \\sum\_{i=1}^{t-1}p\\sqrt{2 \\rho\_i q}\\right)
\\end{aligned}\\label{eq:alpha-rho}\\end{equation}
这就是本文推出的$\\alpha\_t,\\rho\_t$的变化规律。当然，变化规律是有了，可是还有四个参数$\\lambda,\\Vert\\boldsymbol{\\varepsilon}\_0\\Vert,p,q$要确定，其中$q$相对来说比较简单，直接设$q=1$问题也不大，但即便这样还有三个参数要确定。

## 尺度预判 [\#](https://kexue.fm/archives/9344\#%E5%B0%BA%E5%BA%A6%E9%A2%84%E5%88%A4)

根据定义，$\\Vert\\boldsymbol{\\varepsilon}\_0\\Vert = \\Vert\\boldsymbol{\\theta}\_0 - \\boldsymbol{\\theta}^\*\\Vert$，也就是初始化参数与目标参数的距离，可以理解为参数的变化尺度，它有几种不同的情况。

第一种，参数是矩阵乘法核，比如全连接层、卷积层的kernel矩阵，它们的初始化一般是“均值为0、方差为$\\sigma^2$”（$\\sigma$取决于shape）的随机初始化，这样如果$\\boldsymbol{\\theta}\\in\\mathbb{R}^k$，那么我们就可以估算出$\\Vert\\boldsymbol{\\theta}\_0\\Vert^2\\approx k\\sigma^2$。另外，这类参数有一个特点，就是在合理的初始化下，训练完成后参数的均值方差也不会有太大变化，至少量级是一致的，因此也可以认为$\\Vert\\boldsymbol{\\theta}^\*\\Vert^2\\approx k\\sigma^2$，而因为初始化是随机的，所以$\\boldsymbol{\\theta}\_0 \\cdot \\boldsymbol{\\theta}^\*\\approx 0$，因此
\\begin{equation}\\Vert\\boldsymbol{\\varepsilon}\_0\\Vert^2 = \\Vert\\boldsymbol{\\theta}\_0 - \\boldsymbol{\\theta}^\*\\Vert^2 = \\Vert\\boldsymbol{\\theta}\_0\\Vert^2 + \\Vert\\boldsymbol{\\theta}^\*\\Vert^2 - 2\\boldsymbol{\\theta}\_0 \\cdot \\boldsymbol{\\theta}^\* \\approx 2k\\sigma^2\\end{equation}

第二种，参数是加性偏置项，比如全连接层、卷积层的bias向量，以及Normalization层的$\\boldsymbol{\\beta}$向量，这些参数一般是“全零初始化”，所以$\\Vert\\boldsymbol{\\varepsilon}\_0\\Vert^2 = \\Vert\\boldsymbol{\\theta}^\*\\Vert^2$，如果我们根据经验预测训练好的模型偏置项都在$\\pm\\sigma$附近，那么也可以估计出$\\Vert\\boldsymbol{\\theta}^\*\\Vert^2\\approx k\\sigma^2$，Amos原论文取了$\\sigma=0.5$。最后还有Normalization层的$\\boldsymbol{\\gamma}$向量，它一般是“全1初始化”，训练完成后也是在1附近，不妨假设误差为$\\pm\\sigma$，那么也可以估算出$\\Vert\\boldsymbol{\\theta}^\*\\Vert^2\\approx k\\sigma^2$。这里的$k$都是指向量维度。

可以看出，$\\Vert\\boldsymbol{\\varepsilon}\_0\\Vert^2$的结果都有一个共性，那就是都可以写成$k\\sigma^2$，其中$\\sigma$是我们对参数变化尺度的一个预判。乘性矩阵的$\\sigma$可以直接取初始化的标准差，加性偏置或者$\\boldsymbol{\\gamma}$向量可以直接简单地取$\\sigma=0.5$，或者有其他特殊参数的再做特殊处理。

## 分离尺度 [\#](https://kexue.fm/archives/9344\#%E5%88%86%E7%A6%BB%E5%B0%BA%E5%BA%A6)

现在我们来看完整的更新量，根据式$\\eqref{eq:alpha-rho}$，有
\\begin{equation}\\alpha\_t \\boldsymbol{u}\_t \\approx \\lambda\\Vert\\boldsymbol{\\varepsilon}\_0\\Vert^2 \\times \\frac{\\boldsymbol{u}\_t}{\\Vert\\boldsymbol{u}\_t\\Vert} \\times \\exp\\left(- 2 \\sum\_{i=1}^{t-1}p\\sqrt{2 \\rho\_i q}\\right)\\end{equation}
其中$\\frac{\\boldsymbol{u}\_t}{\\Vert\\boldsymbol{u}\_t\\Vert}$是一个单位向量，控制更新方向，$\\exp$部分是一个衰减项，我们可以先不管它，所以更新量的模长由$\\lambda\\Vert\\boldsymbol{\\varepsilon}\_0\\Vert^2$控制。

回到文章开头的第一个问题“学习率如何适应不同初始化和参数化？”，很明显，直观想法应该就是变化尺度大的参数每一步的更新量应该更大，或者直接简单地正比于变化尺度，而变化尺度我们刚才估计了，可以用$\\Vert\\boldsymbol{\\varepsilon}\_0\\Vert$来描述，所以我们认为应该有$\\lambda\\Vert\\boldsymbol{\\varepsilon}\_0\\Vert^2=\\alpha\_0 \\Vert\\boldsymbol{\\varepsilon}\_0\\Vert$，其中$\\alpha\_0$是全局的初始学习率。反过来解得$\\lambda=\\alpha\_0/\\Vert\\boldsymbol{\\varepsilon}\_0\\Vert$，代入式$\\eqref{eq:alpha-rho}$得到
\\begin{equation}\\alpha\_t \\approx \\frac{\\alpha\_0\\Vert\\boldsymbol{\\varepsilon}\_0\\Vert}{\\Vert\\boldsymbol{u}\_t\\Vert} \\exp\\left(- 2 \\sum\_{i=1}^{t-1}p\\sqrt{2 \\rho\_i q}\\right),\\quad \\rho\_t \\approx \\frac{\\alpha\_0^2}{2q} \\exp\\left(- 2 \\sum\_{i=1}^{t-1}p\\sqrt{2 \\rho\_i q}\\right)\\label{eq:alpha-rho-2}\\end{equation}
其中$\\alpha\_0$代表了每一步的相对更新幅度（全局学习率），这一步没啥推导空间了，一般取$10^{-3}$左右就行，如果任务简单也可以取到$10^{-2}$；$\\Vert\\boldsymbol{\\varepsilon}\_0\\Vert$在上一节已经做了估计，大概是$\\sqrt{k}\\sigma$，$\\sigma$代表参数平均变化尺度，不同参数不一样，我们正是通过它把参数尺度显式地分离了出来，从而达到了自适应参数尺度的效果（更新量正比$\\sigma$）。特别地，如果将上式的$\\Vert\\boldsymbol{\\varepsilon}\_0\\Vert$换成$\\Vert\\boldsymbol{\\theta}\_t\\Vert$，那么就是LAMB优化器。从这里也可以看出，如果$\\boldsymbol{\\theta}$的初始化均值不是0（像$\\boldsymbol{\\gamma}$向量），用$\\Vert\\boldsymbol{\\theta}\_t\\Vert$替代$\\Vert\\boldsymbol{\\varepsilon}\_0\\Vert$是会有问题的，所以LAMB的做法是直接不对这些参数的更新量进行变换（即保留原来的更新规则）。

## 解析近似 [\#](https://kexue.fm/archives/9344\#%E8%A7%A3%E6%9E%90%E8%BF%91%E4%BC%BC)

其实目前的结果已经适合编程实现了，只是参数$p$不好调罢了。为了进一步看出参数$p$是怎么影响衰减函数的，我们可以进一步求出$\\rho\_t$的解析近似！

在式$\\eqref{eq:alpha-rho-2}$的$\\rho\_t$两边乘以$2q$，然后两边开平方，得到
将指数的求和$\\sum\\limits\_{i=1}^{t-1}p\\sqrt{2 \\rho\_i q}$记为$S\_t$，那么上式就对应差分方程
\\begin{equation}\\frac{S\_t - S\_{t-1}}{p} \\approx \\alpha\_0 \\exp\\left(- S\_{t-1}\\right) \\quad \\Rightarrow \\quad S\_{t+1} - S\_t \\approx \\alpha\_0 p\\exp\\left(- S\_t\\right)\\end{equation}
此时衰减函数就是$\\exp\\left(-2S\_t\\right)$。为了求渐近近似，我们用导数代替差分（参考 [《差分方程的摄动法》](https://kexue.fm/archives/3889)），得到
\\begin{equation}\\frac{dS\_t}{dt} \\approx \\alpha\_0 p \\exp\\left(- S\_t\\right)\\end{equation}
这是个简单的微分方程，可以解得（结合$S\_0=0$）
\\begin{equation}\\exp\\left(-2S\_t\\right) \\approx \\frac{1}{(\\alpha\_0 p t + 1)^2}\\end{equation}
这就是衰减函数的显式解，表明超参数应该按照步数的平方反比衰减，代入式$\\eqref{eq:alpha-rho-2}$后的完整结果是
\\begin{equation}\\alpha\_t \\approx \\frac{\\alpha\_0\\Vert\\boldsymbol{\\varepsilon}\_0\\Vert}{\\Vert\\boldsymbol{u}\_t\\Vert} \\frac{1}{(\\alpha\_0 p t + 1)^2},\\quad \\rho\_t \\approx \\frac{\\alpha\_0^2}{2q} \\frac{1}{(\\alpha\_0 p t + 1)^2}\\label{eq:alpha-rho-3}\\end{equation}
这个显式解不但能让编程实现更方便，还使得$p$的含义更为清晰。比如我们希望学习率在$T$步后就降低为原来的一半，那么就有$(\\alpha\_0 p T + 1)^2=2$，从中解得
\\begin{equation}\\alpha\_0 p = \\frac{\\sqrt{2}-1}{T}\\end{equation}
至于$T$应该是多少，这依赖于任务难度和数据量，也没有太大推导空间了。

## 动态收敛 [\#](https://kexue.fm/archives/9344\#%E5%8A%A8%E6%80%81%E6%94%B6%E6%95%9B)

上述讨论的假设是存在常数$p > 0$，使得$\\cos(\\boldsymbol{u}\_t, \\boldsymbol{\\varepsilon}\_t)\\sim p$，这可以理解为模型按照固定的速度收敛，这在实际中很难成立，更常见的是越接近训练的后期，收敛速度相对来说越慢。为此，我们可以进一步假设$p$是步数$t$的函数$p\_t$，这样一来，前面的推导大体上还是成立，只不过相应的常数$p$要换成带下标的$p\_i$：
\\begin{equation}\\sqrt{2\\rho\_t q} \\approx \\alpha\_0 \\exp\\left(- \\sum\_{i=1}^{t-1}p\_i\\sqrt{2 \\rho\_i q}\\right)\\end{equation}
重复上一节的推导，我们得到
\\begin{equation}\\frac{S\_t - S\_{t-1}}{p\_t} \\approx \\alpha\_0 \\exp\\left(- S\_{t-1}\\right) \\quad \\Rightarrow \\quad S\_{t+1} - S\_t \\approx \\alpha\_0 p\_t\\exp\\left(- S\_t\\right)\\end{equation}
近似的微分方程就是
\\begin{equation}\\frac{dS\_t}{dt} \\approx \\alpha\_0 p\_t \\exp\\left(- S\_t\\right)\\end{equation}
积分的结果是
\\begin{equation}\\exp\\left(-S\_t\\right) \\approx \\frac{1}{\\alpha\_0 \\int\_0^t p\_{\\tau} d\\tau + 1}\\end{equation}
但现在多了一个$p\_t$需要确定。为了降低调参成本，我们不妨假设收敛的下降速度跟$\\Vert\\boldsymbol{\\varepsilon}\_t\\Vert$的下降速度一致，而根据式$\\eqref{eq:varepsilon-t}$，$\\Vert\\boldsymbol{\\varepsilon}\_t\\Vert$的衰减函数就是$\\exp\\left(-S\_t\\right)$，所以我们设$p\_t = p\_0\\exp\\left(-S\_t\\right)$，代入上式得到
\\begin{equation}\\exp\\left(-S\_t\\right) \\approx \\frac{1}{\\alpha\_0 p\_0 \\int\_0^t \\exp\\left(-S\_{\\tau}\\right) d\\tau + 1}\\end{equation}
这本质就是一个简单的微分方程，容易解得
\\begin{equation}\\exp\\left(-2S\_t\\right) \\approx \\frac{1}{2\\alpha\_0 p\_0 t + 1}\\end{equation}
代入式$\\eqref{eq:alpha-rho-2}$之后，得到
\\begin{equation}\\alpha\_t \\approx \\frac{\\alpha\_0\\Vert\\boldsymbol{\\varepsilon}\_0\\Vert}{\\Vert\\boldsymbol{u}\_t\\Vert} \\frac{1}{2\\alpha\_0 p\_0 t + 1},\\quad \\rho\_t \\approx \\frac{\\alpha\_0^2}{2q} \\frac{1}{2\\alpha\_0 p\_0 t + 1}\\label{eq:alpha-rho-4}\\end{equation}
单看衰减策略，这正好是“逆时间衰减（Inverse Time Decay）”，也是学习率的常见衰减策略之一。理论上来说，这个结果在假设上比前面的式$\\eqref{eq:alpha-rho-3}$更为合理。

## 文章小结 [\#](https://kexue.fm/archives/9344\#%E6%96%87%E7%AB%A0%E5%B0%8F%E7%BB%93)

本文借鉴了Amos优化器的思路，推导了一些关于学习率和权重衰减率的结果$\\eqref{eq:alpha-rho-3}$、$\\eqref{eq:alpha-rho-4}$，这些结果可以即插即用地应用到现有优化器中，能一定程度上简化调参难度。

_**转载到请包括本文地址：** [https://kexue.fm/archives/9344](https://kexue.fm/archives/9344)_

_**更详细的转载事宜请参考：**_ [《科学空间FAQ》](https://kexue.fm/archives/6508#%E6%96%87%E7%AB%A0%E5%A6%82%E4%BD%95%E8%BD%AC%E8%BD%BD/%E5%BC%95%E7%94%A8)

**如果您还有什么疑惑或建议，欢迎在下方评论区继续讨论。**

**如果您觉得本文还不错，欢迎 [分享](https://kexue.fm/archives/9344#share)/ [打赏](https://kexue.fm/archives/9344#pay) 本文。打赏并非要从中获得收益，而是希望知道科学空间获得了多少读者的真心关注。当然，如果你无视它，也不会影响你的阅读。再次表示欢迎和感谢！**

打赏

微信打赏

支付宝打赏

因为网站后台对打赏并无记录，因此欢迎在打赏时候备注留言。你还可以 [**点击这里**](http://mail.qq.com/cgi-bin/qm_share?t=qm_mailme&email=tN7d1drY3drrx8H0xcWa19vZ) 或在下方评论区留言来告知你的建议或需求。

**如果您需要引用本文，请参考：**

苏剑林. (Nov. 22, 2022). 《基于Amos优化器思想推导出来的一些“炼丹策略” 》\[Blog post\]. Retrieved from [https://kexue.fm/archives/9344](https://kexue.fm/archives/9344)

@online{kexuefm-9344,
        title={基于Amos优化器思想推导出来的一些“炼丹策略”},
        author={苏剑林},
        year={2022},
        month={Nov},
        url={\\url{https://kexue.fm/archives/9344}},
}

分类： [数学研究](https://kexue.fm/category/Mathematics), [信息时代](https://kexue.fm/category/Big-Data)    标签： [分析](https://kexue.fm/tag/%E5%88%86%E6%9E%90/), [优化](https://kexue.fm/tag/%E4%BC%98%E5%8C%96/), [渐近](https://kexue.fm/tag/%E6%B8%90%E8%BF%91/), [优化器](https://kexue.fm/tag/%E4%BC%98%E5%8C%96%E5%99%A8/)[9 评论](https://kexue.fm/archives/9344#comments)

< [CoSENT（三）：作为交互式相似度的损失函数](https://kexue.fm/archives/9341) \| [用热传导方程来指导自监督学习](https://kexue.fm/archives/9359) >

### 你也许还对下面的内容感兴趣

- [msign算子的Newton-Schulz迭代（下）](https://kexue.fm/archives/10996)
- [等值振荡定理：最优多项式逼近的充要条件](https://kexue.fm/archives/10972)
- [MoE环游记：5、均匀分布的反思](https://kexue.fm/archives/10945)
- [msign算子的Newton-Schulz迭代（上）](https://kexue.fm/archives/10922)
- [Transformer升级之路：20、MLA究竟好在哪里？](https://kexue.fm/archives/10907)
- [SVD的导数](https://kexue.fm/archives/10878)
- [通过梯度近似寻找Normalization的替代品](https://kexue.fm/archives/10831)
- [MoE环游记：4、难处应当多投入](https://kexue.fm/archives/10815)
- [高阶muP：更简明但更高明的谱条件缩放](https://kexue.fm/archives/10795)
- [初探muP：超参数的跨模型尺度迁移规律](https://kexue.fm/archives/10770)

[发表你的看法](https://kexue.fm/archives/9344#comment_form)

十里

November 23rd, 2022

苏神您好，今天面试遇到了个问题，什么样的函数能被用作激活函数，您有什么高见嘛

[回复评论](https://kexue.fm/archives/9344/comment-page-1?replyTo=20445#respond-post-9344)

[苏剑林](https://kexue.fm) 发表于
November 23rd, 2022

没什么高见。理论上非线性的都可以，连浮点误差都可以，参考 [https://kexue.fm/archives/4647](https://kexue.fm/archives/4647)

[回复评论](https://kexue.fm/archives/9344/comment-page-1?replyTo=20453#respond-post-9344)

量子侠

February 20th, 2023

$2\\rho\_t q \\approx \\lambda \\lVert \\epsilon\_t \\rVert ^2$ 这个为什么会这么设呀，感觉是根据后面的内容才强行设的这个值

[回复评论](https://kexue.fm/archives/9344/comment-page-1?replyTo=20971#respond-post-9344)

[苏剑林](https://kexue.fm) 发表于
February 20th, 2023

文中有解释，是“为了使$\\alpha\_t$和$\\rho\_t$能够同等程度地衰减”，也就是衰减速度是一致的。至于你如果要问为什么要“使$\\alpha\_t$和$\\rho\_t$能够同等程度地衰减”，那就只能说这是一种直觉了。

[回复评论](https://kexue.fm/archives/9344/comment-page-1?replyTo=20980#respond-post-9344)

xunliang 发表于
May 8th, 2023

意思是学习率和衰减率都和误差呈二次关系

[回复评论](https://kexue.fm/archives/9344/comment-page-1?replyTo=21545#respond-post-9344)

guangcongzheng

August 19th, 2023

非常感谢您的文章！对我帮助很大，我有以下几个问题。

请问AdamW中的weight\_decay应该怎么设置，pytorch默认是1e-2, 一些论文中也有说1e-4比较好。
1\. 根据这篇论文似乎要设置为初始学习率的平方？我目前在训练的是ViT-H级别的图像生成模型，学习率固定为1e-4，所以weight decay应该设置为1e-8吗
2\. 我在训练ViT时，AdamW lr=1e-4，weight decay=0，出现了梯度爆炸的情况，用grad norm=1.0能在梯度爆炸时裁剪避免参数更新，的确是有作用。我想问如果我使用weight decay=1e-4，是否能让梯度爆炸不出现

[回复评论](https://kexue.fm/archives/9344/comment-page-1?replyTo=22543#respond-post-9344)

[苏剑林](https://kexue.fm) 发表于
August 25th, 2023

1、我看LLM的训练，不是0.01就是0.1吧，比如llama2就是0.1，似乎没看到更小的；
2、这个应该没法保证吧。

[回复评论](https://kexue.fm/archives/9344/comment-page-1?replyTo=22563#respond-post-9344)

Itach

December 4th, 2023

很棒，但是不太理解。
优化过程是非凸的，一开始假设
$\\Vert\\boldsymbol{\\varepsilon}\_{t+1}\\Vert < \\Vert\\boldsymbol{\\varepsilon}\_t\\Vert$
更好？这样做合理吗？

[回复评论](https://kexue.fm/archives/9344/comment-page-1?replyTo=23197#respond-post-9344)

[苏剑林](https://kexue.fm) 发表于
December 4th, 2023

但我们需要一个约等于的关系而不单单是不等关系。

[回复评论](https://kexue.fm/archives/9344/comment-page-1?replyTo=23210#respond-post-9344)

[取消回复](https://kexue.fm/archives/9344#respond-post-9344)

你的大名

电子邮箱

个人网站（选填）

1\. 可以使用LaTeX代码，点击“预览效果”可查看效果；2. 可以通过点击评论楼层编号来引用该楼层；3. 网站可能会有点卡，如非确认评论失败，请 **不要重复点击提交**。

### 内容速览

[基础回顾](https://kexue.fm/archives/9344#%E5%9F%BA%E7%A1%80%E5%9B%9E%E9%A1%BE)
[问题设置](https://kexue.fm/archives/9344#%E9%97%AE%E9%A2%98%E8%AE%BE%E7%BD%AE)
[权重衰减](https://kexue.fm/archives/9344#%E6%9D%83%E9%87%8D%E8%A1%B0%E5%87%8F)
[可行分析](https://kexue.fm/archives/9344#%E5%8F%AF%E8%A1%8C%E5%88%86%E6%9E%90)
[渐近估计](https://kexue.fm/archives/9344#%E6%B8%90%E8%BF%91%E4%BC%B0%E8%AE%A1)
[尺度预判](https://kexue.fm/archives/9344#%E5%B0%BA%E5%BA%A6%E9%A2%84%E5%88%A4)
[分离尺度](https://kexue.fm/archives/9344#%E5%88%86%E7%A6%BB%E5%B0%BA%E5%BA%A6)
[解析近似](https://kexue.fm/archives/9344#%E8%A7%A3%E6%9E%90%E8%BF%91%E4%BC%BC)
[动态收敛](https://kexue.fm/archives/9344#%E5%8A%A8%E6%80%81%E6%94%B6%E6%95%9B)
[文章小结](https://kexue.fm/archives/9344#%E6%96%87%E7%AB%A0%E5%B0%8F%E7%BB%93)

### 智能搜索

支持整句搜索！网站自动使用 [结巴分词](https://github.com/fxsjy/jieba) 进行分词，并结合ngrams排序算法给出合理的搜索结果。

### 热门标签

[生成模型](https://kexue.fm/tag/%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B/) [attention](https://kexue.fm/tag/attention/) [优化](https://kexue.fm/tag/%E4%BC%98%E5%8C%96/) [语言模型](https://kexue.fm/tag/%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/) [模型](https://kexue.fm/tag/%E6%A8%A1%E5%9E%8B/) [网站](https://kexue.fm/tag/%E7%BD%91%E7%AB%99/) [概率](https://kexue.fm/tag/%E6%A6%82%E7%8E%87/) [梯度](https://kexue.fm/tag/%E6%A2%AF%E5%BA%A6/) [转载](https://kexue.fm/tag/%E8%BD%AC%E8%BD%BD/) [微分方程](https://kexue.fm/tag/%E5%BE%AE%E5%88%86%E6%96%B9%E7%A8%8B/) [矩阵](https://kexue.fm/tag/%E7%9F%A9%E9%98%B5/) [天象](https://kexue.fm/tag/%E5%A4%A9%E8%B1%A1/) [分析](https://kexue.fm/tag/%E5%88%86%E6%9E%90/) [深度学习](https://kexue.fm/tag/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/) [积分](https://kexue.fm/tag/%E7%A7%AF%E5%88%86/) [python](https://kexue.fm/tag/python/) [优化器](https://kexue.fm/tag/%E4%BC%98%E5%8C%96%E5%99%A8/) [力学](https://kexue.fm/tag/%E5%8A%9B%E5%AD%A6/) [无监督](https://kexue.fm/tag/%E6%97%A0%E7%9B%91%E7%9D%A3/) [扩散](https://kexue.fm/tag/%E6%89%A9%E6%95%A3/) [几何](https://kexue.fm/tag/%E5%87%A0%E4%BD%95/) [节日](https://kexue.fm/tag/%E8%8A%82%E6%97%A5/) [生活](https://kexue.fm/tag/%E7%94%9F%E6%B4%BB/) [文本生成](https://kexue.fm/tag/%E6%96%87%E6%9C%AC%E7%94%9F%E6%88%90/) [数论](https://kexue.fm/tag/%E6%95%B0%E8%AE%BA/)

### 随机文章

- [最小熵原理（六）：词向量的维度应该怎么选择？](https://kexue.fm/archives/7695)
- [继续观测国际空间站](https://kexue.fm/archives/41)
- [单摆运动级数解：初试同伦分析](https://kexue.fm/archives/1935)
- [Efficient GlobalPointer：少点参数，多点效果](https://kexue.fm/archives/8877)
- [与日食失之交臂...](https://kexue.fm/archives/341)
- [最受尊崇的3位诺贝尔奖得主](https://kexue.fm/archives/154)
- [文本情感分类（四）：更好的损失函数](https://kexue.fm/archives/4293)
- [生成扩散模型漫谈（七）：最优扩散方差估计（上）](https://kexue.fm/archives/9245)
- [高阶muP：更简明但更高明的谱条件缩放](https://kexue.fm/archives/10795)
- [《转山》，动人之旅](https://kexue.fm/archives/2063)

### 最近评论

- [忍者猫](https://kexue.fm/archives/10592/comment-page-2#comment-27952): 这优化器的作者真的应该给你打钱
- [Chaofa Yuan](https://kexue.fm/archives/11033/comment-page-1#comment-27951): 写得太好了
- [Skyler Lin](https://kexue.fm/archives/11033/comment-page-1#comment-27949): respect苏神！
- [宋佳铭](https://kexue.fm/archives/10958/comment-page-1#comment-27947): 对，个人感觉mean flow就是continuous time CTM
- [宋佳铭](https://kexue.fm/archives/10958/comment-page-1#comment-27946): 的确，对sg这个事情我感觉如果是用‘归纳’法做是不太能避免的，因为毕竟是用步长短的模型去约束步...
- [MoFHeka](https://kexue.fm/archives/10542/comment-page-1#comment-27945): 苏老师您好，请问一下这套结论在稀疏参数上应该如何应用？比如大规模稀疏Embedding，每个B...
- [苏剑林](https://kexue.fm/archives/11033/comment-page-1#comment-27944): Temp LoRA倒是有印象，其实思想是一样的，如果我单独开一篇文章介绍TTT的话，应该会提到...
- [苏剑林](https://kexue.fm/archives/11033/comment-page-1#comment-27943): 你搜搜mamba、rwkv甚至rnn做vision的工作，其实不少。不过多数确实像你说的，正反...
- [苏剑林](https://kexue.fm/archives/9379/comment-page-1#comment-27942): 问题1可以看看 https://kexue.fm/archives/4718 ，简单来说就是点...
- [苏剑林](https://kexue.fm/archives/10958/comment-page-2#comment-27941): 你的“信息量”怎么定义？直观来说，reflow训练的是切线模型，而一步生成需要的是割线模型，m...

### 友情链接

- [Cool Papers](https://papers.cool)
- [数学研发](https://bbs.emath.ac.cn)
- [Seatop](http://www.seatop.com.cn/)
- [Xiaoxia](https://xiaoxia.org/)
- [积分表-网络版](https://kexue.fm/sci/integral/index.html)
- [丝路博傲](http://blog.dvxj.com/)
- [ph4ntasy 饭特稀](http://www.ph4ntasy.com/)
- [数学之家](http://www.2math.cn/)
- [有趣天文奇观](http://interesting-sky.china-vo.org/)
- [TwistedW](http://www.twistedwg.com/)
- [godweiyang](https://godweiyang.com/)
- [AI柠檬](https://blog.ailemon.net/)
- [王登科-DK博客](https://greatdk.com)
- [ESON](https://blog.eson.org/)
- [枫之羽](https://fzhiy.net/)
- [Mathor's blog](https://wmathor.com/)
- [coding-zuo](https://coding-zuo.github.io/)
- [博科园](https://www.bokeyuan.net/)
- [孔皮皮的博客](https://www.kppkkp.top/)
- [运鹏的博客](https://yunpengtai.top/)
- [jiming.site](https://jiming.site/)
- [OmegaXYZ](https://www.omegaxyz.com/)
- [Blog by Eacls](https://www.eacls.top/)
- [EAI猩球](https://www.robotech.ink/)
- [文举的博客](https://liwenju0.com/)
- [用代码打点酱油](https://bruceyuan.com/)
- [申请链接](https://kexue.fm/links.html)

本站采用创作共用版权协议，要求署名、非商业用途和保持一致。转载本站内容必须也遵循“ [署名-非商业用途-保持一致](http://creativecommons.org/licenses/by-nc-nd/2.5/cn/)”的创作共用协议。
© 2009-2025 Scientific Spaces. All rights reserved. Theme by [laogui](http://www.laogui.com). Powered by [Typecho](http://typecho.org). 备案号: [粤ICP备09093259号-1/2](https://beian.miit.gov.cn/)。