Loading \[MathJax\]/jax/output/HTML-CSS/fonts/TeX/fontdata.js

![MobileSideBar](https://kexue.fm/usr/themes/geekg/images/slide-button.png)

## SEARCH

## MENU

- [打赏](https://kexue.fm/reward.html)
- [公式](https://kexue.fm/latex.html)
- [天象](https://kexue.fm/ac.html)
- [链接](https://kexue.fm/links.html)
- [时光](https://kexue.fm/me.html)
- [博览](https://kexue.fm/science.html)
- [归档](https://kexue.fm/content.html)

## CATEGORIES

- [千奇百怪](https://kexue.fm/category/Everything)
- [天文探索](https://kexue.fm/category/Astronomy)
- [数学研究](https://kexue.fm/category/Mathematics)
- [物理化学](https://kexue.fm/category/Phy-chem)
- [信息时代](https://kexue.fm/category/Big-Data)
- [生物自然](https://kexue.fm/category/Biology)
- [图片摄影](https://kexue.fm/category/Photograph)
- [问题百科](https://kexue.fm/category/Questions)
- [生活/情感](https://kexue.fm/category/Life-Feeling)
- [资源共享](https://kexue.fm/category/Resources)

## NEWPOSTS

- [让炼丹更科学一些（五）：基于梯度精...](https://kexue.fm/archives/11530)
- [让炼丹更科学一些（四）：新恒等式，...](https://kexue.fm/archives/11494)
- [为什么DeltaNet要加L2 N...](https://kexue.fm/archives/11486)
- [让炼丹更科学一些（三）：SGD的终...](https://kexue.fm/archives/11480)
- [让炼丹更科学一些（二）：将结论推广...](https://kexue.fm/archives/11469)
- [滑动平均视角下的权重衰减和学习率](https://kexue.fm/archives/11459)
- [生成扩散模型漫谈（三十一）：预测数...](https://kexue.fm/archives/11428)
- [Muon优化器指南：快速上手与关键细节](https://kexue.fm/archives/11416)
- [AdamW的Weight RMS的...](https://kexue.fm/archives/11404)
- [n个正态随机数的最大值的渐近估计](https://kexue.fm/archives/11390)

## COMMENTS

- [Bin: 今天偶然从某个论坛看到有人推荐您的博客，定睛一看竟然是华师同院...](https://kexue.fm/archives/1990/comment-page-2#comment-29105)
- [Rapture D: 我有一个问题，为什么不考虑亥姆霍兹定理和斯托克斯公式。](https://kexue.fm/archives/11530/comment-page-1#comment-29104)
- [mofheka: 苏神是还在用jax是么？最近在做基于Google Pathwa...](https://kexue.fm/archives/11390/comment-page-1#comment-29103)
- [长琴: 看懂这篇博客也不是一件容易的事情。](https://kexue.fm/archives/11530/comment-page-1#comment-29102)
- [AlexLi: 苏老师，请教一下(7)式中将 \\mu(x\_t) 传给 $p...](https://kexue.fm/archives/9257/comment-page-4#comment-29101)
- [tyler\_zxc: "Performer的思想是将标准的Attention线性化，...](https://kexue.fm/archives/7921/comment-page-2#comment-29100)
- [我: 似乎并非mHC提出矩阵的思想？之前hyper connecti...](https://kexue.fm/archives/11494/comment-page-1#comment-29099)
- [winter: 苏神您好，假如对于比较均匀的attention weightP...](https://kexue.fm/archives/10847/comment-page-1#comment-29098)
- [苏剑林: KL散度、JS散度、W距离啥的，都行啊，看你喜欢哪个](https://kexue.fm/archives/8512/comment-page-2#comment-29097)
- [苏剑林: 没有绝对公平的对比方法，主要看你关心什么。比如，如果只关心推理...](https://kexue.fm/archives/9119/comment-page-14#comment-29096)

## USERLOGIN

- [登录](https://kexue.fm/admin/login.php)

[科学空间\|Scientific Spaces](https://kexue.fm/)

- [登录](https://kexue.fm/admin/login.php)
- [打赏](https://kexue.fm/reward.html)
- [公式](https://kexue.fm/latex.html)
- [天象](https://kexue.fm/ac.html)
- [链接](https://kexue.fm/links.html)
- [时光](https://kexue.fm/me.html)
- [博览](https://kexue.fm/science.html)
- [归档](https://kexue.fm/content.html)

渴望成为一个小飞侠

- [![](https://kexue.fm/usr/themes/geekg/images/rss.png)\\
\\
欢迎订阅](https://kexue.fm/feed)
- [![](https://kexue.fm/usr/themes/geekg/images/mail.png)\\
\\
个性邮箱](https://kexue.fm/archives/119)
- [![](https://kexue.fm/usr/themes/geekg/images/Saturn.png)\\
\\
天象信息](https://kexue.fm/ac.html)
- [![](https://kexue.fm/usr/themes/geekg/images/iss.png)\\
\\
观测ISS](https://kexue.fm/archives/41)
- [![](https://kexue.fm/usr/themes/geekg/images/pi.png)\\
\\
LaTeX](https://kexue.fm/latex.html)
- [![](https://kexue.fm/usr/themes/geekg/images/mlogo.png)\\
\\
关于博主](https://kexue.fm/me.html)

欢迎访问“科学空间”，这里将与您共同探讨自然科学，回味人生百态；也期待大家的分享～

- [**千奇百怪** Everything](https://kexue.fm/category/Everything)
- [**天文探索** Astronomy](https://kexue.fm/category/Astronomy)
- [**数学研究** Mathematics](https://kexue.fm/category/Mathematics)
- [**物理化学** Phy-chem](https://kexue.fm/category/Phy-chem)
- [**信息时代** Big-Data](https://kexue.fm/category/Big-Data)
- [**生物自然** Biology](https://kexue.fm/category/Biology)
- [**图片摄影** Photograph](https://kexue.fm/category/Photograph)
- [**问题百科** Questions](https://kexue.fm/category/Questions)
- [**生活/情感** Life-Feeling](https://kexue.fm/category/Life-Feeling)
- [**资源共享** Resources](https://kexue.fm/category/Resources)

- [**千奇百怪**](https://kexue.fm/category/Everything)
- [**天文探索**](https://kexue.fm/category/Astronomy)
- [**数学研究**](https://kexue.fm/category/Mathematics)
- [**物理化学**](https://kexue.fm/category/Phy-chem)
- [**信息时代**](https://kexue.fm/category/Big-Data)
- [**生物自然**](https://kexue.fm/category/Biology)
- [**图片摄影**](https://kexue.fm/category/Photograph)
- [**问题百科**](https://kexue.fm/category/Questions)
- [**生活/情感**](https://kexue.fm/category/Life-Feeling)
- [**资源共享**](https://kexue.fm/category/Resources)

[首页](https://kexue.fm/) [信息时代](https://kexue.fm/category/Big-Data) 能量视角下的GAN模型（二）：GAN＝“分析”＋“采样”

15Feb

# [能量视角下的GAN模型（二）：GAN＝“分析”＋“采样”](https://kexue.fm/archives/6331)

By 苏剑林 \|
2019-02-15 \|
180821位读者 \|

在这个系列中，我们尝试从能量的视角理解GAN。我们会发现这个视角如此美妙和直观，甚至让人拍案叫绝。

上一篇文章里，我们给出了一个直白而用力的能量图景，这个图景可以让我们轻松理解GAN的很多内容，换句话说，通俗的解释已经能让我们完成大部分的理解了，并且把最终的结论都已经写了出来。在这篇文章中，我们继续从能量的视角理解GAN，这一次，我们争取把前面简单直白的描述，用相对严密的数学语言推导一遍。

跟第一篇文章一样，对于笔者来说，这个推导过程依然直接受启发于Bengio团队的新作 [《Maximum Entropy Generators for Energy-Based Models》](https://papers.cool/arxiv/1901.08508)。

原作者的开源实现： [https://github.com/ritheshkumar95/energy\_based\_generative\_models](https://github.com/ritheshkumar95/energy_based_generative_models)

本文的大致内容如下：

> 1、推导了能量分布下的正负相对抗的更新公式；
>
> 2、比较了理论分析与实验采样的区别，而将两者结合便得到了GAN框架；
>
> 3、导出了生成器的补充loss，理论上可以防止mode collapse；
>
> 4、简单提及了基于能量函数的MCMC采样。

## 数学视角的能量 [\#](https://kexue.fm/archives/6331\#%E6%95%B0%E5%AD%A6%E8%A7%86%E8%A7%92%E7%9A%84%E8%83%BD%E9%87%8F)

在这部分中，我们先来简单引入能量模型，并且推导了能量模型理论上的更新公式，指出它具有正相、负相对抗的特点。

### 能量分布模型 [\#](https://kexue.fm/archives/6331\#%E8%83%BD%E9%87%8F%E5%88%86%E5%B8%83%E6%A8%A1%E5%9E%8B)

首先，我们有一批数据x\_1,x\_2,\\dots,x\_n\\sim p(x)，我们希望用一个概率模型去拟合它，我们选取的模型为

\\begin{equation}q\_{\\theta}(x) = \\frac{e^{-U\_{\\theta}(x)}}{Z\_{\\theta}}\\end{equation}

其中U\_{\\theta}是带参数\\theta的未定函数，我们称为“能量函数”，而Z\_{\\theta}是归一化因子（配分函数）

\\begin{equation}Z\_{\\theta} = \\int e^{-U\_{\\theta}(x)}dx\\label{eq:z}\\end{equation}

这样的分布可以称为“能量分布”，在物理中也被称为“玻尔兹曼分布”。

至于为什么选择这样的能量分布，解释有很多，既可以说是从物理角度受到启发，也可以说是从最大熵原理中受到启发，甚至你也可以简单地认为只是因为这种分布相对容易处理而已。但不可否认，这种分布很常见、很实用，我们用得非常多的softmax激活，其实也就是假设了这种分布。

现在的困难是如何求出参数\\theta来，而困难的来源则是配分函数\\eqref{eq:z}通常难以显式地计算出来。当然，尽管实际计算存在困难，但不妨碍我们继续把推导进行下去。

### 正负相的对抗 [\#](https://kexue.fm/archives/6331\#%E6%AD%A3%E8%B4%9F%E7%9B%B8%E7%9A%84%E5%AF%B9%E6%8A%97)

为了求出参数\\theta，我们先定义对数似然函数：

\\begin{equation}\\mathbb{E}\_{x\\sim p(x)} \\big\[\\log q\_{\\theta}(x)\\big\]\\end{equation}

我们希望它越大越好，也就是希望

\\begin{equation}L\_{\\theta}=\\mathbb{E}\_{x\\sim p(x)} \\big\[-\\log q\_{\\theta}(x)\\big\]\\end{equation}

越小越好，为此，我们对L\_{\\theta}使用梯度下降。我们有

\\begin{equation}\\begin{aligned}\\nabla\_{\\theta}\\log q\_{\\theta}(x)=&\\nabla\_{\\theta}\\log e^{-U\_{\\theta}(x)}-\\nabla\_{\\theta}\\log Z\_{\\theta}\\\
=&-\\nabla\_{\\theta} U\_{\\theta}(x)-\\frac{1}{Z\_{\\theta}}\\nabla\_{\\theta} Z\_{\\theta}\\\
=&-\\nabla\_{\\theta} U\_{\\theta}(x)-\\frac{1}{Z\_{\\theta}}\\nabla\_{\\theta} \\int e^{-U\_{\\theta}(x)}dx\\\
=&-\\nabla\_{\\theta} U\_{\\theta}(x)+\\frac{1}{Z\_{\\theta}} \\int e^{-U\_{\\theta}(x)}\\nabla\_{\\theta} U\_{\\theta}(x) dx\\\
=&-\\nabla\_{\\theta} U\_{\\theta}(x)+\\int \\frac{e^{-U\_{\\theta}(x)}}{Z\_{\\theta}}\\nabla\_{\\theta} U\_{\\theta}(x) dx\\\
=&-\\nabla\_{\\theta} U\_{\\theta}(x)+\\mathbb{E}\_{x\\sim q\_{\\theta}(x)}\\big\[\\nabla\_{\\theta} U\_{\\theta}(x)\\big\]
\\end{aligned}\\end{equation}

所以

\\begin{equation}\\nabla\_{\\theta} L\_{\\theta} = \\mathbb{E}\_{x\\sim p(x)}\\big\[\\nabla\_{\\theta} U\_{\\theta}(x)\\big\] - \\mathbb{E}\_{x\\sim q\_{\\theta}(x)}\\big\[\\nabla\_{\\theta} U\_{\\theta}(x)\\big\]\\label{eq:q-grad}\\end{equation}

这意味着梯度下降的更新公式是

\\begin{equation}\\theta \\leftarrow \\theta - \\varepsilon \\Big(\\mathbb{E}\_{x\\sim p(x)}\\big\[\\nabla\_{\\theta} U\_{\\theta}(x)\\big\] - \\mathbb{E}\_{x\\sim q\_{\\theta}(x)}\\big\[\\nabla\_{\\theta} U\_{\\theta}(x)\\big\]\\Big)\\label{eq:q-grad-gd}\\end{equation}

注意到式\\eqref{eq:q-grad}的特点，它是\\nabla\_{\\theta} U\_{\\theta}(x)分别在真实分布下和拟合分布下的均值之差，这就是机器学习中著名的“正相”和“负相”的分解，式\\eqref{eq:q-grad}体现了正负相之间的对抗，也有人将其对应为我们做梦的过程。

## 扬长避短 ⇒ GAN [\#](https://kexue.fm/archives/6331\#%E6%89%AC%E9%95%BF%E9%81%BF%E7%9F%AD%20%E2%87%92%20GAN)

在这部分中，我们表明“容易分析”与“容易采样”是很难兼容的，容易理论分析的模型，在实验上难以采样计算，而容易采样计算的模型，难以进行简明的理论推导。而试图将两者的优点结合起来，就得到了GAN模型。

### 理论分析与实验采样 [\#](https://kexue.fm/archives/6331\#%E7%90%86%E8%AE%BA%E5%88%86%E6%9E%90%E4%B8%8E%E5%AE%9E%E9%AA%8C%E9%87%87%E6%A0%B7)

事实上，式\\eqref{eq:q-grad}和式\\eqref{eq:q-grad-gd}表明我们开始假设的能量分布模型的理论分析并不困难，但是落实到实验中，我们发现必须要完成从q\_{\\theta}中采样：\\mathbb{E}\_{x\\sim q\_{\\theta}(x)}。也就是说，给定一个具体的U\_{\\theta}(x)，我们要想办法从q\_{\\theta}(x)=e^{-U\_{\\theta}(x)}/Z\_{\\theta}中采样出一批x出来。

然而，就目前而言，我们对从q\_{\\theta}(x)=e^{-U\_{\\theta}(x)}/Z\_{\\theta}中采样并没有任何经验。对于我们来说，方便采样的是如下的过程

\\begin{equation}z\\sim q(z),\\quad x = G\_{\\varphi}(z)\\end{equation}

这里的q(z)代表着标准正态分布。也就是说，我们可以从标准正态分布中采样出一个z出来，然后通过固定的模型G\_{\\varphi}变换为我们想要的x。这意味着这种分布的理论表达式是：

\\begin{equation}q\_{\\varphi}(x) = \\int \\delta\\big(x - G\_{\\varphi}(z)\\big)q(z)dz\\label{eq:q-varphi}\\end{equation}

问题是，如果用q\_{\\varphi}(x)代替原来的q\_{\\theta}(x)，那么采样是方便了，但是类似的理论推导就困难了，换句话说，我们根本推导不出类似\\eqref{eq:q-grad-gd}的结果来。

### GAN诞生记 [\#](https://kexue.fm/archives/6331\#GAN%E8%AF%9E%E7%94%9F%E8%AE%B0)

那么，一个异想天开的念头是：能不能把两者结合起来，在各自擅长的地方发挥各自的优势？

式\\eqref{eq:q-grad-gd}中的\\mathbb{E}\_{x\\sim q\_{\\theta}(x)}不是难以实现吗，那我只把这部分用\\mathbb{E}\_{x\\sim q\_{\\varphi}(x)}代替好了：

\\begin{equation}\\theta \\leftarrow \\theta - \\varepsilon \\Big(\\mathbb{E}\_{x\\sim p(x)}\\big\[\\nabla\_{\\theta} U\_{\\theta}(x)\\big\] - \\mathbb{E}\_{x\\sim q\_{\\varphi}(x)}\\big\[\\nabla\_{\\theta} U\_{\\theta}(x)\\big\]\\Big)\\end{equation}

也就是

\\begin{equation}\\theta \\leftarrow \\theta - \\varepsilon \\Big(\\mathbb{E}\_{x\\sim p(x)}\\big\[\\nabla\_{\\theta} U\_{\\theta}(x)\\big\] - \\mathbb{E}\_{x=G\_{\\varphi}(z),z\\sim q(z)}\\big\[\\nabla\_{\\theta} U\_{\\theta}(x)\\big\]\\Big)\\label{eq:q-grad-gd-new}\\end{equation}

现在采样是方便了，但前提是q\_{\\varphi}(x)跟q\_{\\theta}(x)足够接近才行呀（因为q\_{\\theta}(x)才是标准的、正确的），所以，我们用KL散度来度量两者的差异：

\\begin{equation}\\begin{aligned}KL\\big(q\_{\\varphi}(x)\\big\\Vert q\_{\\theta}(x)\\big)=&\\int q\_{\\varphi}(x) \\log \\frac{q\_{\\varphi}(x)}{q\_{\\theta}(x)}dx \\\
=& \- H\_{\\varphi}(X) + \\mathbb{E}\_{x\\sim q\_{\\varphi}(x)}\\big\[U\_{\\theta}(x)\\big\]+\\log Z\_{\\theta}\\end{aligned}\\end{equation}

式\\eqref{eq:q-grad-gd-new}有效的前提是q\_{\\varphi}(x)跟q\_{\\theta}(x)足够接近，也就是上式足够小，而对于固定的q\_{\\theta}(x)，Z\_{\\theta}是一个常数，所以\\varphi的优化目标是：

\\begin{equation}\\varphi =\\mathop{\\text{argmin}}\_{\\varphi} - H\_{\\varphi}(X) + \\mathbb{E}\_{x\\sim q\_{\\varphi}(x)}\\big\[U\_{\\theta}(x)\\big\]\\label{eq:varphi-gd}\\end{equation}

这里H\_{\\varphi}(X) = - \\int q\_{\\varphi}(x) \\log q\_{\\varphi}(x) dx代表q\_{\\varphi}(x)的熵。\- H\_{\\varphi}(X)希望熵越大越好，这意味着多样性；\\mathbb{E}\_{x\\sim q\_{\\varphi}(x)}\[U\_{\\theta}(x)\]希望图片势能越小越好，这意味着真实性。

另外一方面，注意到式\\eqref{eq:q-grad-gd-new}实际上是目标

\\begin{equation}\\theta =\\mathop{\\text{argmin}}\_{\\theta} \\mathbb{E}\_{x\\sim p(x)}\\big\[U\_{\\theta}(x)\\big\] - \\mathbb{E}\_{x=G\_{\\varphi}(z),z\\sim q(z)}\\big\[U\_{\\theta}(x)\\big\]\\label{eq:theta-gd}\\end{equation}

的梯度下降公式。所以我们发现，整个过程实际上就是\\eqref{eq:theta-gd}和\\eqref{eq:varphi-gd}的交替梯度下降。而正如第一篇所说的，\\theta的这个目标可能带来数值不稳定性，基于第一篇所说的理由，真样本应该在极小值点附近，所以我们可以把梯度惩罚项补充进\\eqref{eq:theta-gd}，得到最终的流程是：

\\begin{equation}\\begin{aligned}\\theta =&\\,\\mathop{\\text{argmin}}\_{\\theta} \\mathbb{E}\_{x\\sim p(x)}\\big\[U\_{\\theta}(x)\\big\] - \\mathbb{E}\_{x=G\_{\\varphi}(z),z\\sim q(z)}\\big\[U\_{\\theta}(x)\\big\] + \\lambda \\mathbb{E}\_{x\\sim p(x)}\\big\[\\Vert \\nabla\_x U\_{\\theta}(x)\\Vert^2\\big\]\\\
\\varphi =&\\,\\mathop{\\text{argmin}}\_{\\varphi} - H\_{\\varphi}(X) + \\mathbb{E}\_{x=G\_{\\varphi}(z),z\\sim q(z)}\\big\[U\_{\\theta}(x)\\big\]
\\end{aligned}\\label{eq:gan-energy}\\end{equation}

这便是基于梯度惩罚的GAN模型，我们在 [《能量视角下的GAN模型（一）》](https://kexue.fm/archives/6316) 中已经把它“头脑风暴”出来了，而现在我们从能量模型的数学分析中把它推导出来了。

所以说，GAN实际上就是能量模型和采样模型各自扬长避短的结果。

## 直击H(X)！ [\#](https://kexue.fm/archives/6331\#%E7%9B%B4%E5%87%BB$H(X)$%EF%BC%81)

现在，距离完整地实现整个模型，就差H\_{\\varphi}(X)了。我们已经说过

\\begin{equation}H\_{\\varphi}(X) = - \\int q\_{\\varphi}(x) \\log q\_{\\varphi}(x) dx\\end{equation}

代表q\_{\\varphi}(x)的熵，而q\_{\\varphi}(x)的理论表达式是\\eqref{eq:q-varphi}，积分难以计算，所以H\_{\\varphi}(X)也难以计算。

打破这一困境的思路是将熵转化为互信息，然后转化为互信息的估计，其估计方式有两种：通过f散度的方式（理论上精确）估计，或者通过信息下界的方式估计。

### 最大熵与互信息 [\#](https://kexue.fm/archives/6331\#%E6%9C%80%E5%A4%A7%E7%86%B5%E4%B8%8E%E4%BA%92%E4%BF%A1%E6%81%AF)

首先，我们可以利用x=G\_{\\varphi}(z)这一点：x=G\_{\\varphi}(z)意味着条件概率q\_{\\varphi}(x\|z) = \\delta\\big(x - G(z)\\big)，即一个确定性的模型，也可以理解为均值为G(z)、方差为0的高斯分布\\mathcal{N}(x;G\_{\\varphi}(z),0)。

然后我们去考虑互信息I(X,Z)：

\\begin{equation}\\begin{aligned}I\_{\\varphi}(X,Z)=&\\iint q\_{\\varphi}(x\|z)q(z)\\log \\frac{q\_{\\varphi}(x\|z)}{q\_{\\varphi}(x)}dxdz\\\
=&\\iint q\_{\\varphi}(x\|z)q(z)\\log q\_{\\varphi}(x\|z) dxdz - \\iint q\_{\\varphi}(x\|z)q(z) \\log q\_{\\varphi}(x)dxdz\\\
=&\\int q(z)\\left(\\int q\_{\\varphi}(x\|z)\\log q\_{\\varphi}(x\|z) dx\\right)dz + H(X)
\\end{aligned}\\end{equation}

现在我们找出了I\_{\\varphi}(X,Z)和H\_{\\varphi}(X)的关系，它们的差是

\\begin{equation}\\int q(z)\\left(\\int q\_{\\varphi}(x\|z)\\log q\_{\\varphi}(x\|z) dx\\right)dz\\triangleq -H\_{\\varphi}(X\|Z)\\end{equation}

事实上H\_{\\varphi}(X\|Z)称为“条件熵”。

如果我们处理的是离散型分布，那么因为x=G\_{\\varphi}(z)是确定性的，所以q\_{\\varphi}(x\|z)\\equiv 1，那么H\_{\\varphi}(X\|Z)为0，即I\_{\\varphi}(X,Z)=H\_{\\varphi}(X)；如果是连续型分布，前面说了可以理解为方差为0的高斯分布\\mathcal{N}(x;G\_{\\varphi}(z),0)，我们可以先考虑常数方差的情况\\mathcal{N}(x;G(z),\\sigma^2)，计算发现H\_{\\varphi}(X\|Z)\\sim \\log \\sigma^2 是一个常数，然后\\sigma \\to 0，不过发现结果是无穷大。无穷大原则上是不能计算的，但事实上方差也不需要等于0，只要足够小，肉眼难以分辨即可。

所以，总的来说我们可以确定互信息I\_{\\varphi}(X,Z)与熵H\_{\\varphi}(X)只相差一个无关紧要的常数，所以在式\\eqref{eq:gan-energy}中，可以将H\_{\\varphi}(X)替换为I\_{\\varphi}(X,Z)：

\\begin{equation}\\begin{aligned}\\theta =&\\,\\mathop{\\text{argmin}}\_{\\theta} \\mathbb{E}\_{x\\sim p(x)}\\big\[U\_{\\theta}(x)\\big\] - \\mathbb{E}\_{x=G\_{\\varphi}(z),z\\sim q(z)}\\big\[U\_{\\theta}(x)\\big\] + \\lambda \\mathbb{E}\_{x\\sim p(x)}\\big\[\\Vert \\nabla\_x U\_{\\theta}(x)\\Vert^2\\big\]\\\
\\varphi =&\\,\\mathop{\\text{argmin}}\_{\\varphi} - I\_{\\varphi}(X,Z) + \\mathbb{E}\_{x=G\_{\\varphi}(z),z\\sim q(z)}\\big\[U\_{\\theta}(x)\\big\]
\\end{aligned}\\label{eq:gan-energy-2}\\end{equation}

现在我们要最小化\- I\_{\\varphi}(X,Z)，也就是最大化互信息I\_{\\varphi}(X,Z)。直观上这也不难理解，因为这一项是用来防止mode callopse的，而如果一旦mode callopse，那么几乎任意的z都生成同一个x，X,Z的互信息一定不会大。

但是将目标从H\_{\\varphi}(X)改为I\_{\\varphi}(X,Z)，看起来只是形式上的转换，似乎依然还没有解决问题。但很幸运的是，我们已经做过最大化互信息的研究了，方法在 [《深度学习的互信息：无监督提取特征》的“互信息本质”一节](https://kexue.fm/archives/6024#%E4%BA%92%E4%BF%A1%E6%81%AF%E6%9C%AC%E8%B4%A8)，也就是说，直接估算互信息已经有解决方案了，读者直接看那篇文章即可，不再重复论述。

### 互信息与信息下界 [\#](https://kexue.fm/archives/6331\#%E4%BA%92%E4%BF%A1%E6%81%AF%E4%B8%8E%E4%BF%A1%E6%81%AF%E4%B8%8B%E7%95%8C)

如果不需要精确估计互信息，那么可以使用InfoGAN中的思路，得到互信息的一个下界，然后去优化这个下界。

从互信息定义出发：

\\begin{equation}I\_{\\varphi}(X,Z)=\\iint q\_{\\varphi}(x\|z)q(z)\\log \\frac{q\_{\\varphi}(x\|z)q(z)}{q\_{\\varphi}(x)q(z)}dxdz\\end{equation}

记q\_{\\varphi}(z\|x) = q\_{\\varphi}(x\|z)q(z)/q\_{\\varphi}(x)，这代表精确的后验分布；然后对于任意近似的后验分布p(z\|x)，我们有

\\begin{equation}\\begin{aligned}I\_{\\varphi}(X,Z)=&\\iint q\_{\\varphi}(x\|z)q(z)\\log \\frac{q\_{\\varphi}(z\|x)}{q(z)}dxdz\\\
=&\\iint q\_{\\varphi}(x\|z)q(z)\\log \\frac{p(z\|x)}{q(z)}dxdz + \\iint q\_{\\varphi}(x\|z)q(z)\\log \\frac{q\_{\\varphi}(z\|x)}{p(z\|x)}dxdz\\\
=&\\iint q\_{\\varphi}(x\|z)q(z)\\log \\frac{p(z\|x)}{q(z)}dxdz + \\int q\_{\\varphi}(x)KL\\Big(q\_{\\varphi}(z\|x) \\Big\\Vert p(z\|x)\\Big)dz\\\
\\geq &\\iint q\_{\\varphi}(x\|z)q(z)\\log \\frac{p(z\|x)}{q(z)}dxdz\\\
=& \\iint q\_{\\varphi}(x\|z)q(z)\\log p(z\|x) - \\underbrace{\\iint q\_{\\varphi}(x\|z)q(z)\\log q(z) dxdz}\_{=\\int q(z)\\log q(z)dz\\,\\,\\text{是一个常数}}
\\end{aligned}\\end{equation}

也就是说，互信息大于等于\\iint q\_{\\varphi}(x\|z)q(z)\\log p(z\|x)加上一个常数。如果最大化互信息，可以考虑最大化这个下界。由于p(z\|x)是任意的，可以简单假设p(z\|x)=\\mathcal{N}\\left(z;E(x),\\sigma^2\\right)，其中E(x)是一个带参数的编码器，代入计算并省去多余的常数，可以发现相当于在生成器加入一项loss：

\\begin{equation}\\mathbb{E}\_{z\\sim q(z)} \\big\[\\Vert z - E(G(z))\\Vert^2\\big\]\\end{equation}

所以，基于InfoGAN的信息下界思路，式\\eqref{eq:gan-energy}变为：

\\begin{equation}\\begin{aligned}\\theta =&\\,\\mathop{\\text{argmin}}\_{\\theta} \\mathbb{E}\_{x\\sim p(x)}\\big\[U\_{\\theta}(x)\\big\] - \\mathbb{E}\_{z\\sim q(z)}\\big\[U\_{\\theta}(G\_{\\varphi}(z))\\big\] + \\lambda\_1 \\mathbb{E}\_{x\\sim p(x)}\\big\[\\Vert \\nabla\_x U\_{\\theta}(x)\\Vert^2\\big\]\\\
\\varphi,E =&\\,\\mathop{\\text{argmin}}\_{\\varphi,E} \\mathbb{E}\_{z\\sim q(z)}\\big\[U\_{\\theta}(G\_{\\varphi}(z)) + \\lambda\_2 \\Vert z - E(G\_{\\varphi}(z))\\Vert^2\\big\]
\\end{aligned}\\label{eq:gan-energy-3}\\end{equation}

到这里，我们已经从两个角度完成了H\_{\\varphi}(X)的处理，从而完成了整个GAN和能量模型的推导。

## MCMC提升效果 [\#](https://kexue.fm/archives/6331\#MCMC%E6%8F%90%E5%8D%87%E6%95%88%E6%9E%9C)

回顾开头，我们是从能量分布出发推导出了GAN模型，而能量函数U(x)也就是GAN模型中的判别器。既然U(x)具有能量函数的含义，那么训练完成后，我们可以利用能量函数的特性做更多有价值的事情，例如引入MCMC来提升效果。

### MCMC的简介 [\#](https://kexue.fm/archives/6331\#MCMC%E7%9A%84%E7%AE%80%E4%BB%8B)

其实对于MCMC，我只是略懂它的含义，并不懂它的方法和精髓，所谓“简介”，仅仅是对其概念做一些基本的介绍。MCMC是“马尔科夫链蒙特卡洛方法（Markov Chain Monte Carlo）”，在我的理解里，它大概是这么个东西：我们难以直接从某个给定的分布q(x)中采样出样本来，但是我们可以构造如下的随机过程：

\\begin{equation}x\_{n+1} = f(x\_n, \\alpha)\\label{eq:suijidigui}\\end{equation}

其中\\alpha是一个便于实现的随机过程，比如从二元分布、正态分布采样等。这样一来，从某个x\_0出发，得到的序列\\{x\_1,x\_2,\\dots,x\_n,\\dots\\}是随机的。

如果进一步能证明式\\eqref{eq:suijidigui}的静态分布正好是q(x)，那么就意味着序列\\{x\_1,x\_2,\\dots,x\_n,\\dots\\}正是从q(x)中采样出来的一批样本，这样就实现了从q(x)中采样了，只不过采样的结果经过了一定的顺序排列。

### Langevin方程 [\#](https://kexue.fm/archives/6331\#Langevin%E6%96%B9%E7%A8%8B)

式\\eqref{eq:suijidigui}的一个特例是Langevin方程：

\\begin{equation}x\_{t+1} = x\_t - \\frac{1}{2}\\varepsilon \\nabla\_x U(x\_t) + \\sqrt{\\varepsilon}\\alpha,\\quad \\alpha \\sim \\mathcal{N}(\\alpha;0,1)\\label{eq:sde}\\end{equation}

它也称为随机微分方程，当\\varepsilon\\to 0时，它的静态分布正好是能量分布

\\begin{equation}p(x) = \\frac{e^{-U(x)}}{Z}\\end{equation}

也就是说，给定能量函数U(x)后，我们可以通过式\\eqref{eq:sde}实现从能量分布中采样，这就是能量分布的MCMC采样的原始思想。

当然，直接从能量函数和式\\eqref{eq:sde}中采样x可能不大现实，因为x维度（常见的情景下，x代表图片）过大，可控性难以保证。另一方面，式\\eqref{eq:sde}最后一项是高斯噪声，所以只要\\varepsilon\\neq 0，那么结果必然是有噪声的，图片真实性也难以保证。

一个有趣的转化是：我们可以不直接考虑x的MCMC采样，而考虑z的采样。因为在前面的模型中，我们最后既得到了能量函数U\_{\\theta}(x)，也得到了生成模型G\_{\\varphi}(z)，这意味着z的能量函数为

\\begin{equation}U\_{\\theta}(G\_{\\varphi}(z))\\end{equation}

> **注：** 这个结果并不是严格成立的，只能算是一个经验公式，严格来讲只有当G的雅可比行列式为1时才成立。我也曾在github上跟作者讨论过，他也指出这没有什么严格的理论推导，只是凭直觉来的，详情可以参考： [https://github.com/ritheshkumar95/energy\_based\_generative\_models/issues/4](https://github.com/ritheshkumar95/energy_based_generative_models/issues/4)

有了z的能量函数，我们可以通过式\\eqref{eq:sde}实现z的MCMC采样：

\\begin{equation}z\_{t+1} = z\_t - \\frac{1}{2}\\varepsilon \\nabla\_z U\_{\\theta}(G\_{\\varphi}(z\_t)) + \\sqrt{\\varepsilon}\\alpha,\\quad \\alpha \\sim \\mathcal{N}(\\alpha;0,1)\\label{eq:sde-2}\\end{equation}

这样刚才说的问题全部都没有了，因为z的维度一般比x小得多，而且也不用担心\\varepsilon\\neq 0带来噪声，因为z本来就是噪声。

### 更好的截断技巧 [\#](https://kexue.fm/archives/6331\#%E6%9B%B4%E5%A5%BD%E7%9A%84%E6%88%AA%E6%96%AD%E6%8A%80%E5%B7%A7)

到这里，如果头脑还没有混乱的读者也许会回过神来：z的分布不就是标准的正态分布吗？采样起来不是很容易吗？为啥还要折腾一套MCMC采样？

理想情况下，z的能量函数U\_{\\theta}(G\_{\\varphi}(z))所对应的能量分布

\\begin{equation}q\_{\\theta,\\varphi}(z)=\\frac{e^{-U\_{\\theta}(G\_{\\varphi}(z))}}{Z}\\end{equation}

确实应该就是我们原始传递给它的标准正态分布q(z)。但事实上，理想和现实总有些差距的，当我们用标准正态分布去训练好一个生成模型后，最后能产生真实的样本的噪声往往会更窄一些，这就需要一些截断技巧，或者说筛选技巧。

比如，基于flow的生成模型在训练完成后，往往使用“退火”技巧，也就是在生成时将噪声的方差设置小一些，这样能生成一些更稳妥的样本，可以参考 [《细水长flow之NICE：流模型的基本概念与实现》](https://kexue.fm/archives/5776#%E9%80%80%E7%81%AB%E5%8F%82%E6%95%B0)。而去年发布的BigGAN，也讨论了GAN中对噪声的截断技巧。

如果我们相信我们的模型，相信能量函数U\_{\\theta}(x)和生成模型G\_{\\varphi}(z)都是有价值的，那么我们有理由相信e^{-U\_{\\theta}(G\_{\\varphi}(z))}/Z会是一个比标准正态分布更好的z的分布（能生成更真实的x的z的分布，因为它将G\_{\\varphi}(z)也纳入了分布的定义中），所以从e^{-U\_{\\theta}(G\_{\\varphi}(z))}/Z采样会优于从q(z)采样，也就是说MCMC采样\\eqref{eq:sde-2}能够提升采样后的生成质量，原论文已经验证了这一点。我们可以将它理解为一种更好的截断技巧。

### 更高效的MALA [\#](https://kexue.fm/archives/6331\#%E6%9B%B4%E9%AB%98%E6%95%88%E7%9A%84MALA)

采样过程\\eqref{eq:sde-2}其实依然会比较低效，原论文事实上用的是改进版本，称为MALA（Metropolis-adjusted Langevin algorithm），它在\\eqref{eq:sde-2}的基础上进一步引入了一个筛选过程：

\\begin{equation}\\begin{aligned}\\tilde{z}\_{t+1} =& z\_t - \\frac{1}{2}\\varepsilon \\nabla\_z U\_{\\theta}(G\_{\\varphi}(z\_t)) + \\sqrt{\\varepsilon}\\alpha,\\quad \\alpha \\sim \\mathcal{N}(\\alpha;0,1)\\\
\\\
z\_{t+1} =& \\left\\{\\begin{aligned}&\\tilde{z}\_{t+1}, \\quad \\text{如果}\\beta < \\gamma\\\ &z\_t, \\quad \\text{其他情况}\\end{aligned}\\right.,\\quad \\beta \\sim U\[0,1\]\\\
\\\
\\gamma =& \\min\\left\\{1, \\frac{q(\\tilde{z}\_{t+1})q(z\_t\|\\tilde{z}\_{t+1})}{q(\\tilde{z}\_{t})q(\\tilde{z}\_{t+1}\|z\_t)}\\right\\}
\\end{aligned}\\end{equation}

这里

\\begin{equation}\\begin{aligned}q(z)\\propto&\\, \\exp\\Big(-U\_{\\theta}(G\_{\\varphi}(z))\\Big)\\\
q(z'\|z)\\propto&\\, \\exp\\left(-\\frac{1}{2\\varepsilon}\\Vert z' - z + \\varepsilon \\nabla\_z U\_{\\theta}(G\_{\\varphi}(z))\\Vert^2\\right)
\\end{aligned}\\end{equation}

也就是说以概率\\gamma接受z\_{t+1}=\\tilde{z}\_{t+1}，以1-\\gamma的概率保持不变。按照 [维基百科上的说法](https://en.wikipedia.org/wiki/Metropolis-adjusted_Langevin_algorithm)，这样的改进能够让采样过程更有机会采样到高概率的样本，这也就意味着能生成更多的真实样本。（笔者并不是很懂这一套理论，所以，只能照搬了～）

## 有力的能量视角 [\#](https://kexue.fm/archives/6331\#%E6%9C%89%E5%8A%9B%E7%9A%84%E8%83%BD%E9%87%8F%E8%A7%86%E8%A7%92)

又是一篇公式长文，总算把能量分布下的GAN的数学推导捋清楚了，GAN是调和“理论分析”与“实验采样”矛盾的产物。总的来说，笔者觉得整个推导过程还是颇具启发性的，也能让我们明白GAN的关键之处和问题所在。

能量视角是一个偏向数学物理的视角，一旦能将机器学习和数学物理联系起来，还将可以很直接地从数学物理处获得启发，甚至使得对应的机器学习不再“黑箱”，这样的视角往往让人陶醉，给人一种有力的感觉。

_**转载到请包括本文地址：** [https://kexue.fm/archives/6331](https://kexue.fm/archives/6331 "能量视角下的GAN模型（二）：GAN＝“分析”＋“采样” ")_

_**更详细的转载事宜请参考：**_ [《科学空间FAQ》](https://kexue.fm/archives/6508#%E6%96%87%E7%AB%A0%E5%A6%82%E4%BD%95%E8%BD%AC%E8%BD%BD/%E5%BC%95%E7%94%A8 "《科学空间FAQ》")

**如果您还有什么疑惑或建议，欢迎在下方评论区继续讨论。**

**如果您觉得本文还不错，欢迎 [分享](https://kexue.fm/archives/6331#share)/ [打赏](https://kexue.fm/archives/6331#pay) 本文。打赏并非要从中获得收益，而是希望知道科学空间获得了多少读者的真心关注。当然，如果你无视它，也不会影响你的阅读。再次表示欢迎和感谢！**

打赏

![科学空间](https://kexue.fm/usr/themes/geekg/payment/wx.png)

微信打赏

![科学空间](https://kexue.fm/usr/themes/geekg/payment/zfb.png)

支付宝打赏

因为网站后台对打赏并无记录，因此欢迎在打赏时候备注留言。

你还可以 [**点击这里**](http://mail.qq.com/cgi-bin/qm_share?t=qm_mailme&email=tN7d1drY3drrx8H0xcWa19vZ) 或在下方评论区留言来告知你的建议或需求。

**如果您需要引用本文，请参考：**

苏剑林. (Feb. 15, 2019). 《能量视角下的GAN模型（二）：GAN＝“分析”＋“采样” 》\[Blog post\]. Retrieved from [https://kexue.fm/archives/6331](https://kexue.fm/archives/6331)

@online{kexuefm-6331,

         title={能量视角下的GAN模型（二）：GAN＝“分析”＋“采样” },

         author={苏剑林},

         year={2019},

         month={Feb},

         url={\\url{https://kexue.fm/archives/6331}},

}


分类： [信息时代](https://kexue.fm/category/Big-Data)    标签： [概率](https://kexue.fm/tag/%E6%A6%82%E7%8E%87/), [能量](https://kexue.fm/tag/%E8%83%BD%E9%87%8F/), [GAN](https://kexue.fm/tag/GAN/), [生成模型](https://kexue.fm/tag/%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B/)[48 评论](https://kexue.fm/archives/6331#comments)

< [能量视角下的GAN模型（一）：GAN＝“挖坑”＋“跳坑”](https://kexue.fm/archives/6316 "能量视角下的GAN模型（一）：GAN＝“挖坑”＋“跳坑”") \| [恒等式 det(exp(A)) = exp(Tr(A)) 赏析](https://kexue.fm/archives/6377 "恒等式 det(exp(A)) = exp(Tr(A)) 赏析") >

### 你也许还对下面的内容感兴趣

- [生成扩散模型漫谈（三十一）：预测数据而非噪声](https://kexue.fm/archives/11428 "生成扩散模型漫谈（三十一）：预测数据而非噪声")
- [n个正态随机数的最大值的渐近估计](https://kexue.fm/archives/11390 "n个正态随机数的最大值的渐近估计")
- [DiVeQ：一种非常简洁的VQ训练方案](https://kexue.fm/archives/11328 "DiVeQ：一种非常简洁的VQ训练方案")
- [为什么线性注意力要加Short Conv？](https://kexue.fm/archives/11320 "为什么线性注意力要加Short Conv？")
- [Transformer升级之路：21、MLA好在哪里?（下）](https://kexue.fm/archives/11111 "Transformer升级之路：21、MLA好在哪里?（下）")
- [线性注意力简史：从模仿、创新到反哺](https://kexue.fm/archives/11033 "线性注意力简史：从模仿、创新到反哺")
- [生成扩散模型漫谈（三十）：从瞬时速度到平均速度](https://kexue.fm/archives/10958 "生成扩散模型漫谈（三十）：从瞬时速度到平均速度")
- [Transformer升级之路：20、MLA好在哪里?（上）](https://kexue.fm/archives/10907 "Transformer升级之路：20、MLA好在哪里?（上）")
- [一道概率不等式：盯着它到显然成立为止！](https://kexue.fm/archives/10902 "一道概率不等式：盯着它到显然成立为止！")
- [生成扩散模型漫谈（二十九）：用DDPM来离散编码](https://kexue.fm/archives/10711 "生成扩散模型漫谈（二十九）：用DDPM来离散编码")

[发表你的看法](https://kexue.fm/archives/6331#comment_form)

1. [«](https://kexue.fm/archives/6331/comment-page-1#comments)
2. [1](https://kexue.fm/archives/6331/comment-page-1#comments)
3. [2](https://kexue.fm/archives/6331/comment-page-2#comments)

GC

August 25th, 2021

请问博主，截断技巧，例如温度（先验方差），是否有解释的参考文献推荐呢？我的理解是，像Flows和GANs可能是SGD训练优化不足导致的。

[回复评论](https://kexue.fm/archives/6331/comment-page-2?replyTo=17199#respond-post-6331)

[苏剑林](https://kexue.fm/) 发表于
August 29th, 2021

抱歉，这方面的工作我并没有继续跟进。

[回复评论](https://kexue.fm/archives/6331/comment-page-2?replyTo=17214#respond-post-6331)

GC

December 29th, 2021

如果D\_{\\theta}和G\_{\\theta}共享（或部分）参数，且想计算梯度并优化\\mathbb{E}\_{G\_{\\theta}}\[\\log D\_{\\theta}\]时，先MC sampling一些G\_{\\theta}且不对G\_{\\theta}进行detach()操作，那么这是在优化什么？至少可以肯定的是，答案不是熵。

[回复评论](https://kexue.fm/archives/6331/comment-page-2?replyTo=18122#respond-post-6331)

GC 发表于
December 30th, 2021

补充下，还是交替优化，先\\mathbb{E}\_{G\_{\\theta}}\[\\log D\_{{\\theta}^{\\prime}}\]，然后再\\mathbb{E}\_{G\_{{\\theta}^{\\prime}}}\[\\log D\_{\\theta}\]。

[回复评论](https://kexue.fm/archives/6331/comment-page-2?replyTo=18125#respond-post-6331)

[苏剑林](https://kexue.fm/) 发表于
January 4th, 2022

不大明白你想表达什么。

[回复评论](https://kexue.fm/archives/6331/comment-page-2?replyTo=18137#respond-post-6331)

[shouldsee](http://www.catsmile.info/)

June 30th, 2022

Benjio团队 笔误了哈

[回复评论](https://kexue.fm/archives/6331/comment-page-2?replyTo=19378#respond-post-6331)

[苏剑林](https://kexue.fm/) 发表于
June 30th, 2022

谢谢，已经修正。

[回复评论](https://kexue.fm/archives/6331/comment-page-2?replyTo=19384#respond-post-6331)

Rhea

August 13th, 2022

您好，请问您文中(25)推到(26)步骤中所提到的「静态分布」指的是什么？我没有查到相关的资料，请问能否给一下相关资料的搜索关键词 或者 能否讲讲这步是如何推导的？提前感谢

[回复评论](https://kexue.fm/archives/6331/comment-page-2?replyTo=19624#respond-post-6331)

[苏剑林](https://kexue.fm/) 发表于
August 16th, 2022

t\\to\\infty时，x\_t的分布。

这个结果根据 [https://kexue.fm/archives/8084](https://kexue.fm/archives/8084) 来自己推也不难。

[回复评论](https://kexue.fm/archives/6331/comment-page-2?replyTo=19629#respond-post-6331)

GC

September 5th, 2022

博主你好，请问为何不用采样来估计熵H(X)，这样不是简单又便捷嘛？

[回复评论](https://kexue.fm/archives/6331/comment-page-2?replyTo=19734#respond-post-6331)

[苏剑林](https://kexue.fm/) 发表于
September 6th, 2022

如何采样估计？\\log q\_{\\varphi}(x)的理论表达式未知，而且采样的时候还要保持梯度，需要用到reinforce方法，方差也大。

[回复评论](https://kexue.fm/archives/6331/comment-page-2?replyTo=19747#respond-post-6331)

GC 发表于
September 6th, 2022

那如果\\log q\_{\\phi}(x)表达式已知，请问是不是就可以简便的使用采样估计了？

[回复评论](https://kexue.fm/archives/6331/comment-page-2?replyTo=19753#respond-post-6331)

[苏剑林](https://kexue.fm/) 发表于
September 8th, 2022

但问题是不存在这个如果

[回复评论](https://kexue.fm/archives/6331/comment-page-2?replyTo=19767#respond-post-6331)

王凯

July 2nd, 2024

请教老师，GAN genertator 基于输入的随机噪声生成sample图片，完全是随机生成的么? 然后持续迭代参数，直到判别器通过？

比较困惑，这个过程不是相当于从零开始猜一张图么，训练过程会很长吧。

[回复评论](https://kexue.fm/archives/6331/comment-page-2?replyTo=24647#respond-post-6331)

[苏剑林](https://kexue.fm/) 发表于
July 2nd, 2024

从你这段评论中，没看出对GAN的理解的明显错误。

[回复评论](https://kexue.fm/archives/6331/comment-page-2?replyTo=24664#respond-post-6331)

王凯 发表于
July 2nd, 2024

感谢苏老师回复。所以是不是能这样理解，GAN 本质就是生成器不断试出符合风格的图片。

[回复评论](https://kexue.fm/archives/6331/comment-page-2?replyTo=24682#respond-post-6331)

[苏剑林](https://kexue.fm/) 发表于
July 6th, 2024

对，但这个尝试的过程得到了判别器的指引。

[回复评论](https://kexue.fm/archives/6331/comment-page-2?replyTo=24703#respond-post-6331)

expii

October 9th, 2024

2024 Nobel prize in physics, for Hinton.

[回复评论](https://kexue.fm/archives/6331/comment-page-2?replyTo=25402#respond-post-6331)

却东

May 28th, 2025

写的很好！但是关于GAN的来源应该是和苏神写的是相反的。在苏神的推导中先有了一个判别器来判断（生成）样本的能量，然后推导出应该有一个NN参数化的生成器来帮助优化能量函数。GAN的来源应该是相反的，我们首先希望使用NN参数化的生成器来做样本生成，但是不知道使用什么evaluator来判断生成的质量，干脆使用NN参数化的判别器。

[回复评论](https://kexue.fm/archives/6331/comment-page-2?replyTo=27720#respond-post-6331)

[苏剑林](https://kexue.fm/) 发表于
June 3rd, 2025

这篇是“能量视角下”的GAN，不是GAN的起源。

[回复评论](https://kexue.fm/archives/6331/comment-page-2?replyTo=27742#respond-post-6331)

1. [«](https://kexue.fm/archives/6331/comment-page-1#comments)
2. [1](https://kexue.fm/archives/6331/comment-page-1#comments)
3. [2](https://kexue.fm/archives/6331/comment-page-2#comments)

[取消回复](https://kexue.fm/archives/6331#respond-post-6331)

你的大名

电子邮箱

个人网站（选填）

1\. 可以使用LaTeX代码，点击“预览效果”可查看效果；

2\. 可以通过点击评论楼层编号来引用该楼层；

3\. 网站可能会有点卡，如非确认评论失败，请 **不要重复点击提交**。

### 内容速览

[数学视角的能量](https://kexue.fm/archives/6331#%E6%95%B0%E5%AD%A6%E8%A7%86%E8%A7%92%E7%9A%84%E8%83%BD%E9%87%8F)
[能量分布模型](https://kexue.fm/archives/6331#%E8%83%BD%E9%87%8F%E5%88%86%E5%B8%83%E6%A8%A1%E5%9E%8B)
[正负相的对抗](https://kexue.fm/archives/6331#%E6%AD%A3%E8%B4%9F%E7%9B%B8%E7%9A%84%E5%AF%B9%E6%8A%97)
[扬长避短 ⇒ GAN](https://kexue.fm/archives/6331#%E6%89%AC%E9%95%BF%E9%81%BF%E7%9F%AD%20%E2%87%92%20GAN)
[理论分析与实验采样](https://kexue.fm/archives/6331#%E7%90%86%E8%AE%BA%E5%88%86%E6%9E%90%E4%B8%8E%E5%AE%9E%E9%AA%8C%E9%87%87%E6%A0%B7)
[GAN诞生记](https://kexue.fm/archives/6331#GAN%E8%AF%9E%E7%94%9F%E8%AE%B0)
[直击H(X)！](https://kexue.fm/archives/6331#%E7%9B%B4%E5%87%BB$H(X)$%EF%BC%81)
[最大熵与互信息](https://kexue.fm/archives/6331#%E6%9C%80%E5%A4%A7%E7%86%B5%E4%B8%8E%E4%BA%92%E4%BF%A1%E6%81%AF)
[互信息与信息下界](https://kexue.fm/archives/6331#%E4%BA%92%E4%BF%A1%E6%81%AF%E4%B8%8E%E4%BF%A1%E6%81%AF%E4%B8%8B%E7%95%8C)
[MCMC提升效果](https://kexue.fm/archives/6331#MCMC%E6%8F%90%E5%8D%87%E6%95%88%E6%9E%9C)
[MCMC的简介](https://kexue.fm/archives/6331#MCMC%E7%9A%84%E7%AE%80%E4%BB%8B)
[Langevin方程](https://kexue.fm/archives/6331#Langevin%E6%96%B9%E7%A8%8B)
[更好的截断技巧](https://kexue.fm/archives/6331#%E6%9B%B4%E5%A5%BD%E7%9A%84%E6%88%AA%E6%96%AD%E6%8A%80%E5%B7%A7)
[更高效的MALA](https://kexue.fm/archives/6331#%E6%9B%B4%E9%AB%98%E6%95%88%E7%9A%84MALA)
[有力的能量视角](https://kexue.fm/archives/6331#%E6%9C%89%E5%8A%9B%E7%9A%84%E8%83%BD%E9%87%8F%E8%A7%86%E8%A7%92)

### 智能搜索

支持整句搜索！网站自动使用 [结巴分词](https://github.com/fxsjy/jieba) 进行分词，并结合ngrams排序算法给出合理的搜索结果。

### 热门标签

[生成模型](https://kexue.fm/tag/%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B/) [attention](https://kexue.fm/tag/attention/) [优化](https://kexue.fm/tag/%E4%BC%98%E5%8C%96/) [语言模型](https://kexue.fm/tag/%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/) [模型](https://kexue.fm/tag/%E6%A8%A1%E5%9E%8B/) [梯度](https://kexue.fm/tag/%E6%A2%AF%E5%BA%A6/) [网站](https://kexue.fm/tag/%E7%BD%91%E7%AB%99/) [概率](https://kexue.fm/tag/%E6%A6%82%E7%8E%87/) [矩阵](https://kexue.fm/tag/%E7%9F%A9%E9%98%B5/) [优化器](https://kexue.fm/tag/%E4%BC%98%E5%8C%96%E5%99%A8/) [转载](https://kexue.fm/tag/%E8%BD%AC%E8%BD%BD/) [微分方程](https://kexue.fm/tag/%E5%BE%AE%E5%88%86%E6%96%B9%E7%A8%8B/) [分析](https://kexue.fm/tag/%E5%88%86%E6%9E%90/) [天象](https://kexue.fm/tag/%E5%A4%A9%E8%B1%A1/) [深度学习](https://kexue.fm/tag/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/) [积分](https://kexue.fm/tag/%E7%A7%AF%E5%88%86/) [python](https://kexue.fm/tag/python/) [扩散](https://kexue.fm/tag/%E6%89%A9%E6%95%A3/) [力学](https://kexue.fm/tag/%E5%8A%9B%E5%AD%A6/) [无监督](https://kexue.fm/tag/%E6%97%A0%E7%9B%91%E7%9D%A3/) [几何](https://kexue.fm/tag/%E5%87%A0%E4%BD%95/) [节日](https://kexue.fm/tag/%E8%8A%82%E6%97%A5/) [生活](https://kexue.fm/tag/%E7%94%9F%E6%B4%BB/) [文本生成](https://kexue.fm/tag/%E6%96%87%E6%9C%AC%E7%94%9F%E6%88%90/) [数论](https://kexue.fm/tag/%E6%95%B0%E8%AE%BA/)

### 随机文章

- [植物拯救了地球，阻止寒冷灭绝之灾！](https://kexue.fm/archives/15)
- [我的大学酒](https://kexue.fm/archives/1690)
- [集合的划分与贝尔数](https://kexue.fm/archives/2985)
- [广东珠海之旅（图片）](https://kexue.fm/archives/67)
- [生活中的趣味数学：同一天生日概率有多大](https://kexue.fm/archives/40)
- [生成扩散模型漫谈（二十三）：信噪比与大图生成（下）](https://kexue.fm/archives/10055)
- [2010年全国天文奥赛终于可以报名了](https://kexue.fm/archives/332)
- [浅谈Transformer的初始化、参数化与标准化](https://kexue.fm/archives/8620)
- [如何训练你的准确率？](https://kexue.fm/archives/9098)
- [洋葱也能用来发电！](https://kexue.fm/archives/37)

### 最近评论

- [Bin](https://kexue.fm/archives/1990/comment-page-2#comment-29105): 今天偶然从某个论坛看到有人推荐您的博客，定睛一看竟然是华师同院的往届师兄！看到这篇2013年的...
- [Rapture D](https://kexue.fm/archives/11530/comment-page-1#comment-29104): 我有一个问题，为什么不考虑亥姆霍兹定理和斯托克斯公式。
- [mofheka](https://kexue.fm/archives/11390/comment-page-1#comment-29103): 苏神是还在用jax是么？最近在做基于Google Pathway的理念做一个动态版的MPMD框...
- [长琴](https://kexue.fm/archives/11530/comment-page-1#comment-29102): 看懂这篇博客也不是一件容易的事情。
- [AlexLi](https://kexue.fm/archives/9257/comment-page-4#comment-29101): 苏老师，请教一下(7)式中将 \\mu(x\_t) 传给 p\_o 进行推理的操作。 $x\_...
- [tyler\_zxc](https://kexue.fm/archives/7921/comment-page-2#comment-29100): "Performer的思想是将标准的Attention线性化，所以为什么不干脆直接训练一个线性...
- [我](https://kexue.fm/archives/11494/comment-page-1#comment-29099): 似乎并非mHC提出矩阵的思想？之前hyper connection就是了
- [winter](https://kexue.fm/archives/10847/comment-page-1#comment-29098): 苏神您好，假如对于比较均匀的attention weightP，往往呈现long tail分布...
- [苏剑林](https://kexue.fm/archives/8512/comment-page-2#comment-29097): KL散度、JS散度、W距离啥的，都行啊，看你喜欢哪个
- [苏剑林](https://kexue.fm/archives/9119/comment-page-14#comment-29096): 没有绝对公平的对比方法，主要看你关心什么。比如，如果只关心推理成本和推理效果，那么有的方法可以...

### 友情链接

- [Cool Papers](https://papers.cool/)
- [数学研发](https://bbs.emath.ac.cn/)
- [Seatop](http://www.seatop.com.cn/)
- [Xiaoxia](https://xiaoxia.org/)
- [积分表-网络版](https://kexue.fm/sci/integral/index.html)
- [丝路博傲](http://blog.dvxj.com/)
- [数学之家](http://www.2math.cn/)
- [有趣天文奇观](http://interesting-sky.china-vo.org/)
- [TwistedW](http://www.twistedwg.com/)
- [godweiyang](https://godweiyang.com/)
- [AI柠檬](https://blog.ailemon.net/)
- [王登科-DK博客](https://greatdk.com/)
- [ESON](https://blog.eson.org/)
- [枫之羽](https://fzhiy.net/)
- [coding-zuo](https://coding-zuo.github.io/)
- [博科园](https://www.bokeyuan.net/)
- [孔皮皮的博客](https://www.kppkkp.top/)
- [运鹏的博客](https://yunpengtai.top/)
- [jiming.site](https://jiming.site/)
- [OmegaXYZ](https://www.omegaxyz.com/)
- [EAI猩球](https://www.robotech.ink/)
- [文举的博客](https://liwenju0.com/)
- [申请链接](https://kexue.fm/links.html)

[![署名-非商业用途-保持一致](https://kexue.fm/usr/themes/geekg/images/cc.gif)](http://creativecommons.org/licenses/by-nc-nd/2.5/cn/) 本站采用创作共用版权协议，要求署名、非商业用途和保持一致。转载本站内容必须也遵循“ [署名-非商业用途-保持一致](http://creativecommons.org/licenses/by-nc-nd/2.5/cn/)”的创作共用协议。



© 2009-2026 Scientific Spaces. All rights reserved. Theme by [laogui](http://www.laogui.com/). Powered by [Typecho](http://typecho.org/). 备案号: [粤ICP备09093259号-1/2](https://beian.miit.gov.cn/ "粤ICP备09093259号")。