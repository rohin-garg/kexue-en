## SEARCH

## MENU

- [打赏](https://kexue.fm/reward.html)
- [公式](https://kexue.fm/latex.html)
- [天象](https://kexue.fm/ac.html)
- [链接](https://kexue.fm/links.html)
- [时光](https://kexue.fm/me.html)
- [博览](https://kexue.fm/science.html)
- [归档](https://kexue.fm/content.html)

## CATEGORIES

- [千奇百怪](https://kexue.fm/category/Everything)
- [天文探索](https://kexue.fm/category/Astronomy)
- [数学研究](https://kexue.fm/category/Mathematics)
- [物理化学](https://kexue.fm/category/Phy-chem)
- [信息时代](https://kexue.fm/category/Big-Data)
- [生物自然](https://kexue.fm/category/Biology)
- [图片摄影](https://kexue.fm/category/Photograph)
- [问题百科](https://kexue.fm/category/Questions)
- [生活/情感](https://kexue.fm/category/Life-Feeling)
- [资源共享](https://kexue.fm/category/Resources)

## NEWPOSTS

- [为什么线性注意力要加Short C...](https://kexue.fm/archives/11320)
- [AdamW的Weight RMS的...](https://kexue.fm/archives/11307)
- [重新思考学习率与Batch Siz...](https://kexue.fm/archives/11301)
- [重新思考学习率与Batch Siz...](https://kexue.fm/archives/11285)
- [重新思考学习率与Batch Siz...](https://kexue.fm/archives/11280)
- [为什么Adam的Update RM...](https://kexue.fm/archives/11267)
- [重新思考学习率与Batch Siz...](https://kexue.fm/archives/11260)
- [Cool Papers更新：简单适...](https://kexue.fm/archives/11250)
- [流形上的最速下降：4\. Muon ...](https://kexue.fm/archives/11241)
- [ReLU/GeLU/Swish的一...](https://kexue.fm/archives/11233)

## COMMENTS

- [mxz: 哇谢谢回复！听起来好像可以类比为是这个向量的地址，哈哈哈苏神竟...](https://kexue.fm/archives/6760/comment-page-7#comment-28632)
- [pb: 谢谢，我也自己画了画图，的确如您所说，多个频率叠加可以得到比较...](https://kexue.fm/archives/8265/comment-page-8#comment-28631)
- [YNuclear: 感谢苏神的回复。我的理解是pθ(x)难以直...](https://kexue.fm/archives/5253/comment-page-18#comment-28630)
- [苏剑林: 好的，那按照我的理解，我们的观点应该是已经对齐了。](https://kexue.fm/archives/10958/comment-page-3#comment-28629)
- [苏剑林: 对，事后来看，这个上界的衰减性意义不大。不过这个上界是有机会达...](https://kexue.fm/archives/8265/comment-page-8#comment-28628)
- [苏剑林: zq是一个向量，但如果它是事先给定的向量表的某一个，那么...](https://kexue.fm/archives/6760/comment-page-7#comment-28627)
- [HanbinZheng: 感谢回复！我想表达的意思只是“根据原论文的推导，这个 $\\te...](https://kexue.fm/archives/10958/comment-page-3#comment-28625)
- [pb: 感谢回复。个人认为给两个完全没有限制的向量分析上界的意义不大，...](https://kexue.fm/archives/8265/comment-page-8#comment-28624)
- [mxz: “这样一来，因为zq是编码表E中的向量之一，所以它实际上...](https://kexue.fm/archives/6760/comment-page-7#comment-28617)
- [Seasons: 感谢苏神回复，我后来也发现这个假设有点问题，因为prefix ...](https://kexue.fm/archives/8711/comment-page-1#comment-28616)

## USERLOGIN

- [登录](https://kexue.fm/admin/login.php)

[科学空间\|Scientific Spaces](https://kexue.fm)

- [登录](https://kexue.fm/admin/login.php)
- [打赏](https://kexue.fm/reward.html)
- [公式](https://kexue.fm/latex.html)
- [天象](https://kexue.fm/ac.html)
- [链接](https://kexue.fm/links.html)
- [时光](https://kexue.fm/me.html)
- [博览](https://kexue.fm/science.html)
- [归档](https://kexue.fm/content.html)

渴望成为一个小飞侠

- [欢迎订阅](https://kexue.fm/feed)
- [个性邮箱](https://kexue.fm/archives/119)
- [天象信息](https://kexue.fm/ac.html)
- [观测ISS](https://kexue.fm/archives/41)
- [LaTeX](https://kexue.fm/latex.html)
- [关于博主](https://kexue.fm/me.html)

欢迎访问“科学空间”，这里将与您共同探讨自然科学，回味人生百态；也期待大家的分享～

- [**千奇百怪** Everything](https://kexue.fm/category/Everything)
- [**天文探索** Astronomy](https://kexue.fm/category/Astronomy)
- [**数学研究** Mathematics](https://kexue.fm/category/Mathematics)
- [**物理化学** Phy-chem](https://kexue.fm/category/Phy-chem)
- [**信息时代** Big-Data](https://kexue.fm/category/Big-Data)
- [**生物自然** Biology](https://kexue.fm/category/Biology)
- [**图片摄影** Photograph](https://kexue.fm/category/Photograph)
- [**问题百科** Questions](https://kexue.fm/category/Questions)
- [**生活/情感** Life-Feeling](https://kexue.fm/category/Life-Feeling)
- [**资源共享** Resources](https://kexue.fm/category/Resources)

- [**千奇百怪**](https://kexue.fm/category/Everything)
- [**天文探索**](https://kexue.fm/category/Astronomy)
- [**数学研究**](https://kexue.fm/category/Mathematics)
- [**物理化学**](https://kexue.fm/category/Phy-chem)
- [**信息时代**](https://kexue.fm/category/Big-Data)
- [**生物自然**](https://kexue.fm/category/Biology)
- [**图片摄影**](https://kexue.fm/category/Photograph)
- [**问题百科**](https://kexue.fm/category/Questions)
- [**生活/情感**](https://kexue.fm/category/Life-Feeling)
- [**资源共享**](https://kexue.fm/category/Resources)

[首页](https://kexue.fm) [信息时代](https://kexue.fm/category/Big-Data) 玩转Keras之seq2seq自动生成标题

1Sep

# [玩转Keras之seq2seq自动生成标题](https://kexue.fm/archives/5861)

By 苏剑林 \|
2018-09-01 \|
431600位读者\|

话说自称搞了这么久的NLP，我都还没有真正跑过NLP与深度学习结合的经典之作——seq2seq。这两天兴致来了，决定学习并实践一番seq2seq，当然最后少不了Keras实现了。

seq2seq可以做的事情非常多，我这挑选的是比较简单的根据文章内容生成标题（中文），也可以理解为自动摘要的一种。选择这个任务主要是因为“文章-标题”这样的语料对比较好找，能快速实验一下。

## seq2seq简介 [\#](https://kexue.fm/archives/5861\#seq2seq%E7%AE%80%E4%BB%8B)

所谓seq2seq，就是指一般的序列到序列的转换任务，比如机器翻译、自动文摘等等，这种任务的特点是输入序列和输出序列是不对齐的，如果对齐的话，那么我们称之为序列标注，这就比seq2seq简单很多了。所以尽管序列标注任务也可以理解为序列到序列的转换，但我们在谈到seq2seq时，一般不包含序列标注。

要自己实现seq2seq，关键是搞懂seq2seq的原理和架构，一旦弄清楚了，其实不管哪个框架实现起来都不复杂。早期有一个 [第三方实现的Keras的seq2seq库](https://github.com/farizrahman4u/seq2seq)，现在作者也已经放弃更新了，也许就是觉得这么简单的事情没必要再建一个库了吧。可以参考的资料还有去年Keras官方博客中写的 [《A ten-minute introduction to sequence-to-sequence learning in Keras》](https://blog.keras.io/a-ten-minute-introduction-to-sequence-to-sequence-learning-in-keras.html)。

### 基本结构 [\#](https://kexue.fm/archives/5861\#%E5%9F%BA%E6%9C%AC%E7%BB%93%E6%9E%84)

假如原句子为X=(a,b,c,d,e,f)，目标输出为Y=(P,Q,R,S,T)，那么一个基本的seq2seq就如下图所示。

基本的seq2seq架构

尽管整个图的线条比较多，可能有点眼花，但其实结构很简单。左边是对输入的encoder，它负责把输入（可能是变长的）编码为一个固定大小的向量，这个可选择的模型就很多了，用GRU、LSTM等RNN结构或者CNN+Pooling、Google的纯Attention等都可以，这个固定大小的向量，理论上就包含了输入句子的全部信息。

而decoder负责将刚才我们编码出来的向量解码为我们期望的输出。与encoder不同，我们在图上强调decoder是“单向递归”的，因为解码过程是递归进行的，具体流程为：

> 1、所有输出端，都以一个通用的 标记开头，以 标记结尾，这两个标记也视为一个词/字；
>
> 2、将 输入decoder，然后得到隐藏层向量，将这个向量与encoder的输出混合，然后送入一个分类器，分类器的结果应当输出P；
>
> 3、将P输入decoder，得到新的隐藏层向量，再次与encoder的输出混合，送入分类器，分类器应输出Q；
>
> 4、依此递归，直到分类器的结果输出 。

这就是一个基本的seq2seq模型的解码过程，在解码的过程中，将每步的解码结果送入到下一步中去，直到输出 位置。

### 训练过程 [\#](https://kexue.fm/archives/5861\#%E8%AE%AD%E7%BB%83%E8%BF%87%E7%A8%8B)

事实上，上图也表明了一般的seq2seq的训练过程。由于训练的时候我们有标注数据对，因此我们能提前预知decoder每一步的输入和输出，因此整个结果实际上是“输入X和Y\[:-1\]，预测Y\[1:\]，即将目标Y错开一位来训练。这种训练方式，称之为Teacher-Forcing。

而decoder同样可以用GRU、LSTM或CNN等结构，但注意再次强调这种“预知未来”的特性仅仅在训练中才有可能，在预测阶段是不存在的，因此decoder在执行每一步时，不能提前使用后面步的输入。所以，如果用RNN结构，一般都只使用单向RNN；如果使用CNN或者纯Attention，那么需要把后面的部分给mask掉（对于卷积来说，就是在卷积核上乘上一个0/1矩阵，使得卷积只能读取当前位置及其“左边”的输入，对于Attention来说也类似，不过是对query的序列进行mask处理）。

> 敏感的读者可能会察觉到，这种训练方案是“局部”的，事实上不够端到端。比如当我们预测R时是假设Q已知的，即Q在前一步被成功预测，但这是不能直接得到保证的。一般前面某一步的预测出错，那么可能导致连锁反应，后面各步的训练和预测都没有意义了。
>
> 有学者考虑过这个问题，比如文章 [《Sequence-to-Sequence Learning as Beam-Search Optimization》](https://papers.cool/arxiv/1606.02960) 把整个解码搜索过程也加入到训练过程，而且还是纯粹梯度下降的（不用强化学习），是非常值得借鉴的一种做法。不过局部训练的计算成本比较低，一般情况下我们都只是使用局部训练来训练seq2seq。

### beam search [\#](https://kexue.fm/archives/5861\#beam%20search)

前面已经多次提到了解码过程，但还不完整。事实上，对于seq2seq来说，我们是在建模
p(Y\|X)=p(Y1\|X)p(Y2\|X,Y1)p(Y3\|X,Y1,Y2)p(Y4\|X,Y1,Y2,Y3)p(Y5\|X,Y1,Y2,Y3,Y4)

显然在解码时，我们希望能找到最大概率的Y，那要怎么做呢？

如果在第一步p(Y1\|X)时，直接选择最大概率的那个（我们期望是目标P），然后代入第二步p(Y2\|X,Y1)，再次选择最大概率的Y2，依此类推，每一步都选择当前最大概率的输出，那么就称为贪心搜索，是一种最低成本的解码方案。但是要注意，这种方案得到的结果未必是最优的，假如第一步我们选择了概率不是最大的Y1，代入第二步时也许会得到非常大的条件概率p(Y2\|X,Y1)，从而两者的乘积会超过逐位取最大的算法。

然而，如果真的要枚举所有路径取最优，那计算量是大到难以接受的（这不是一个马尔可夫过程，动态规划也用不了）。因此，seq2seq使用了一种折中的方法：beam search。

这种算法类似动态规划，但即使在能用动态规划的问题下，它还比动态规划要简单，它的思想是：在每步计算时，只保留当前最优的topk个候选结果。比如取topk=3，那么第一步时，我们只保留使得p(Y1\|X)最大的前3个Y1，然后分别代入p(Y2\|X,Y1)，然后各取前三个Y2，这样一来我们就有32=9个组合了，这时我们计算每一种组合的总概率，然后还是只保留前三个，依次递归，直到出现了第一个 。显然，它本质上还属于贪心搜索的范畴，只不过贪心的过程中保留了更多的可能性，普通的贪心搜索相当于topk=1。

## seq2seq提升 [\#](https://kexue.fm/archives/5861\#seq2seq%E6%8F%90%E5%8D%87)

前面所示的seq2seq模型是标准的，但它把整个输入编码为一个固定大小的向量，然后用这个向量解码，这意味着这个向量理论上能包含原来输入的所有信息，会对encoder和decoder有更高的要求，尤其在机器翻译等信息不变的任务上。因为这种模型相当于让我们“看了一遍中文后就直接写出对应的英文翻译”那样，要求有强大的记忆能力和解码能力，事实上普通人完全不必这样，我们还会反复翻看对比原文，这就导致了下面的两个技巧。

### Attention [\#](https://kexue.fm/archives/5861\#Attention)

Attention目前基本上已经是seq2seq模型的“标配”模块了，它的思想就是：每一步解码时，不仅仅要结合encoder编码出来的固定大小的向量（通读全文），还要往回查阅原来的每一个字词（精读局部），两者配合来决定当前步的输出。

带Attention的seq2seq

至于Attention的具体做法，笔者之前已经撰文介绍过了，请参考 [《Attention is All You Need》浅读（简介+代码）](https://kexue.fm/archives/4765)。Attention一般分为乘性和加性两种，笔者介绍的是Google系统介绍的乘性的Attention，加性的Attention读者可以自行查阅，只要抓住query、key、value三个要素，Attention就都不难理解了。

### 先验知识 [\#](https://kexue.fm/archives/5861\#%E5%85%88%E9%AA%8C%E7%9F%A5%E8%AF%86)

回到用seq2seq生成文章标题这个任务上，模型可以做些简化，并且可以引入一些先验知识。比如，由于输入语言和输出语言都是中文，因此encoder和decoder的Embedding层可以共享参数（也就是用同一套词向量）。这使得模型的参数量大幅度减少了。

此外，还有一个很有用的先验知识：标题中的大部分字词都在文章中出现过（注：仅仅是出现过，并不一定是连续出现，更不能说标题包含在文章中，不然就成为一个普通的序列标注问题了）。这样一来，我们可以用文章中的词集作为一个先验分布，加到解码过程的分类模型中，使得模型在解码输出时更倾向选用文章中已有的字词。

具体来说，在每一步预测时，我们得到总向量x（如前面所述，它应该是decoder当前的隐层向量、encoder的编码向量、当前decoder与encoder的Attention编码三者的拼接），然后接入到全连接层，最终得到一个大小为\|V\|的向量y=(y1,y2,…,y\|V\|)，其中\|V\|是词表的词数。y经过softmax后，得到原本的概率
pi=eyi∑ieyi

这就是原始的分类方案。引入先验分布的方案是，对于每篇文章，我们得到一个大小为\|V\|的0/1向量χ=(χ1,χ2,…,χ\|V\|)，其中χi=1意味着该词在文章中出现过，否则χi=0。将这样的一个0/1向量经过一个缩放平移层得到：
ˆy=s⊗χ+t=(s1χ1+t1,s2χ2+t2,…,s\|V\|χ\|V\|+t\|V\|)
其中s,t为训练参数，然后将这个向量与原来的y取平均后才做softmax
y←y+ˆy2,pi=eyi∑ieyi
经实验，这个先验分布的引入，有助于加快收敛，生成更稳定的、质量更优的标题。

## Keras参考 [\#](https://kexue.fm/archives/5861\#Keras%E5%8F%82%E8%80%83)

又到了快乐的开源时光～

### 基本实现 [\#](https://kexue.fm/archives/5861\#%E5%9F%BA%E6%9C%AC%E5%AE%9E%E7%8E%B0)

基于上面的描述，我收集了80多万篇新闻的语料，来试图训练一个自动标题的模型。简单起见，我选择了以字为基本单位，并且引入了4个额外标记，分别代表mask、unk、start、end。而encoder我使用了双层双向LSTM，decoder使用了双层单向LSTM。具体细节可以参考源码（Python 2.7 + Keras 2.2.4 + Tensorflow 1.8）：

> [https://github.com/bojone/seq2seq/blob/master/seq2seq.py](https://github.com/bojone/seq2seq/blob/master/seq2seq.py)

我以6.4万文章为一个epoch，训练了50个epoch（一个多小时）之后，基本就生成了看上去还行的标题：

> 文章内容：8月28日，网络爆料称，华住集团旗下连锁酒店用户数据疑似发生泄露。从卖家发布的内容看，数据包含华住旗下汉庭、禧玥、桔子、宜必思等10余个品牌酒店的住客信息。泄露的信息包括华住官网注册资料、酒店入住登记的身份 信息及酒店开房记录，住客姓名、手机号、邮箱、身份证号、登录账号密码等。卖家对这个约5亿条数据打包出售。第三方安全平台威胁猎人对信息出售者提供的三万条数据进行验证，认为数据真实性非常高。当天下午，华住集团发 声明称，已在内部迅速开展核查，并第一时间报警。当晚，上海警方消息称，接到华住集团报案，警方已经介入调查。
> **生成标题：《酒店用户数据疑似发生泄露》**
>
> 文章内容：新浪体育讯 北京时间10月16日,NBA中国赛广州站如约开打,火箭再次胜出,以95-85击败篮网。姚明渐入佳境,打了18分39秒,8投5中,拿下10分5个篮板,他还盖帽1次。火箭以两战皆胜的战绩圆满结束中国行。
> **生成标题：《直击:火箭两战皆胜火箭再胜 广州站姚明10分5板》**

当然这只是两个比较好的例子，还有很多不好的例子，直接用到工程上肯定是不够的，还需要很多“黑科技”优化才行。

### mask [\#](https://kexue.fm/archives/5861\#mask)

在seq2seq中，做好mask是非常重要的，所谓mask，就是要遮掩掉不应该读取到的信息、或者是无用的信息，一般是用0/1向量来乘掉它。keras自带的mask机制十分不友好，有些层不支持mask，而普通的LSTM开启了mask后速度几乎下降了一半。所以现在我都是直接以0作为mask的标记，然后自己写个Lambda层进行转化的，这样速度基本无损，而且支持嵌入到任意层，具体可以参考上面的代码。

要注意我们以往一般是不区分mask和unk（未登录词）的，但如果采用我这种方案，还是把未登录词区分一下比较好，因为未登录词尽管我们不清楚具体含义，它还是一个真正的词，至少有占位作用，而mask是我们希望完全抹掉的信息。

### 解码端 [\#](https://kexue.fm/archives/5861\#%E8%A7%A3%E7%A0%81%E7%AB%AF)

代码中已经实现了beam search解码，读者可以自行测试不同的topk对解码结果的影响。

这里要说的是，参考代码中对解码的实现是比较偷懒的，会使得解码速度大降。理论上来说，我们每次得到当前时刻的输出后，我们只需要传入到LSTM的下一步迭代中去，就可以得到下一时刻的输出，但这需要重写解码端的LSTM（也就是要区分训练阶段和测试阶段，两者共享权重），相对复杂，而且对初学者并不友好。所以我使用了一个非常粗暴的方案：每一步预测都重跑一次整个模型，这样一来代码量最少，但是越到后面越慢，原来是O(n)的计算量变成了O(n2)。

## 最后的话 [\#](https://kexue.fm/archives/5861\#%E6%9C%80%E5%90%8E%E7%9A%84%E8%AF%9D)

又用Keras跑通了一个例子，不错不错，坚定不移高举Keras旗帜～

自动标题任务的语料比较好找，而且在seq2seq任务中属于难度比较低的一个，适合大家练手，想要入坑的朋友赶紧上吧哈。

_**转载到请包括本文地址：** [https://kexue.fm/archives/5861](https://kexue.fm/archives/5861)_

_**更详细的转载事宜请参考：**_ [《科学空间FAQ》](https://kexue.fm/archives/6508#%E6%96%87%E7%AB%A0%E5%A6%82%E4%BD%95%E8%BD%AC%E8%BD%BD/%E5%BC%95%E7%94%A8)

**如果您还有什么疑惑或建议，欢迎在下方评论区继续讨论。**

**如果您觉得本文还不错，欢迎 [分享](https://kexue.fm/archives/5861#share)/ [打赏](https://kexue.fm/archives/5861#pay) 本文。打赏并非要从中获得收益，而是希望知道科学空间获得了多少读者的真心关注。当然，如果你无视它，也不会影响你的阅读。再次表示欢迎和感谢！**

打赏

微信打赏

支付宝打赏

因为网站后台对打赏并无记录，因此欢迎在打赏时候备注留言。你还可以 [**点击这里**](http://mail.qq.com/cgi-bin/qm_share?t=qm_mailme&email=tN7d1drY3drrx8H0xcWa19vZ) 或在下方评论区留言来告知你的建议或需求。

**如果您需要引用本文，请参考：**

苏剑林. (Sep. 01, 2018). 《玩转Keras之seq2seq自动生成标题 》\[Blog post\]. Retrieved from [https://kexue.fm/archives/5861](https://kexue.fm/archives/5861)

@online{kexuefm-5861,
        title={玩转Keras之seq2seq自动生成标题},
        author={苏剑林},
        year={2018},
        month={Sep},
        url={\\url{https://kexue.fm/archives/5861}},
}

分类： [信息时代](https://kexue.fm/category/Big-Data)    标签： [模型](https://kexue.fm/tag/%E6%A8%A1%E5%9E%8B/), [NLP](https://kexue.fm/tag/NLP/), [keras](https://kexue.fm/tag/keras/), [文本生成](https://kexue.fm/tag/%E6%96%87%E6%9C%AC%E7%94%9F%E6%88%90/)[129 评论](https://kexue.fm/archives/5861#comments)

< [细水长flow之RealNVP与Glow：流模型的传承与升华](https://kexue.fm/archives/5807) \| [“让Keras更酷一些！”：小众的自定义优化器](https://kexue.fm/archives/5879) >

### 你也许还对下面的内容感兴趣

- [MoE环游记：1、从几何意义出发](https://kexue.fm/archives/10699)
- [基于量子化假设推导模型的尺度定律（Scaling Law）](https://kexue.fm/archives/9607)
- [《为什么现在的LLM都是Decoder-only的架构？》FAQ](https://kexue.fm/archives/9547)
- [为什么现在的LLM都是Decoder-only的架构？](https://kexue.fm/archives/9529)
- [Tiger：一个“抠”到极致的优化器](https://kexue.fm/archives/9512)
- [在bert4keras中使用混合精度和XLA加速训练](https://kexue.fm/archives/9059)
- [为什么需要残差？一个来自DeepNet的视角](https://kexue.fm/archives/8994)
- [门控注意力单元（GAU）还需要Warmup吗？](https://kexue.fm/archives/8990)
- [GPLinker：基于GlobalPointer的事件联合抽取](https://kexue.fm/archives/8926)
- [GPLinker：基于GlobalPointer的实体关系联合抽取](https://kexue.fm/archives/8888)

[发表你的看法](https://kexue.fm/archives/5861#comment_form)

1. [«](https://kexue.fm/archives/5861/comment-page-5#comments)
2. [1](https://kexue.fm/archives/5861/comment-page-1#comments)
3. ...
4. [3](https://kexue.fm/archives/5861/comment-page-3#comments)
5. [4](https://kexue.fm/archives/5861/comment-page-4#comments)
6. [5](https://kexue.fm/archives/5861/comment-page-5#comments)
7. [6](https://kexue.fm/archives/5861/comment-page-6#comments)

77

May 8th, 2024

请问这个标题生成是根据标签集中的内容生成的吗？

[回复评论](https://kexue.fm/archives/5861/comment-page-6?replyTo=24277#respond-post-5861)

[苏剑林](https://kexue.fm) 发表于
May 13th, 2024

对

[回复评论](https://kexue.fm/archives/5861/comment-page-6?replyTo=24298#respond-post-5861)

1. [«](https://kexue.fm/archives/5861/comment-page-5#comments)
2. [1](https://kexue.fm/archives/5861/comment-page-1#comments)
3. ...
4. [3](https://kexue.fm/archives/5861/comment-page-3#comments)
5. [4](https://kexue.fm/archives/5861/comment-page-4#comments)
6. [5](https://kexue.fm/archives/5861/comment-page-5#comments)
7. [6](https://kexue.fm/archives/5861/comment-page-6#comments)

[取消回复](https://kexue.fm/archives/5861#respond-post-5861)

你的大名

电子邮箱

个人网站（选填）

1\. 可以使用LaTeX代码，点击“预览效果”可查看效果；2. 可以通过点击评论楼层编号来引用该楼层；3. 网站可能会有点卡，如非确认评论失败，请 **不要重复点击提交**。

### 内容速览

[seq2seq简介](https://kexue.fm/archives/5861#seq2seq%E7%AE%80%E4%BB%8B)
[基本结构](https://kexue.fm/archives/5861#%E5%9F%BA%E6%9C%AC%E7%BB%93%E6%9E%84)
[训练过程](https://kexue.fm/archives/5861#%E8%AE%AD%E7%BB%83%E8%BF%87%E7%A8%8B)
[beam search](https://kexue.fm/archives/5861#beam%20search)
[seq2seq提升](https://kexue.fm/archives/5861#seq2seq%E6%8F%90%E5%8D%87)
[Attention](https://kexue.fm/archives/5861#Attention)
[先验知识](https://kexue.fm/archives/5861#%E5%85%88%E9%AA%8C%E7%9F%A5%E8%AF%86)
[Keras参考](https://kexue.fm/archives/5861#Keras%E5%8F%82%E8%80%83)
[基本实现](https://kexue.fm/archives/5861#%E5%9F%BA%E6%9C%AC%E5%AE%9E%E7%8E%B0)
[mask](https://kexue.fm/archives/5861#mask)
[解码端](https://kexue.fm/archives/5861#%E8%A7%A3%E7%A0%81%E7%AB%AF)
[最后的话](https://kexue.fm/archives/5861#%E6%9C%80%E5%90%8E%E7%9A%84%E8%AF%9D)

### 智能搜索

支持整句搜索！网站自动使用 [结巴分词](https://github.com/fxsjy/jieba) 进行分词，并结合ngrams排序算法给出合理的搜索结果。

### 热门标签

[生成模型](https://kexue.fm/tag/%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B/) [attention](https://kexue.fm/tag/attention/) [优化](https://kexue.fm/tag/%E4%BC%98%E5%8C%96/) [语言模型](https://kexue.fm/tag/%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/) [模型](https://kexue.fm/tag/%E6%A8%A1%E5%9E%8B/) [网站](https://kexue.fm/tag/%E7%BD%91%E7%AB%99/) [概率](https://kexue.fm/tag/%E6%A6%82%E7%8E%87/) [梯度](https://kexue.fm/tag/%E6%A2%AF%E5%BA%A6/) [转载](https://kexue.fm/tag/%E8%BD%AC%E8%BD%BD/) [矩阵](https://kexue.fm/tag/%E7%9F%A9%E9%98%B5/) [优化器](https://kexue.fm/tag/%E4%BC%98%E5%8C%96%E5%99%A8/) [微分方程](https://kexue.fm/tag/%E5%BE%AE%E5%88%86%E6%96%B9%E7%A8%8B/) [分析](https://kexue.fm/tag/%E5%88%86%E6%9E%90/) [天象](https://kexue.fm/tag/%E5%A4%A9%E8%B1%A1/) [深度学习](https://kexue.fm/tag/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/) [积分](https://kexue.fm/tag/%E7%A7%AF%E5%88%86/) [python](https://kexue.fm/tag/python/) [力学](https://kexue.fm/tag/%E5%8A%9B%E5%AD%A6/) [无监督](https://kexue.fm/tag/%E6%97%A0%E7%9B%91%E7%9D%A3/) [扩散](https://kexue.fm/tag/%E6%89%A9%E6%95%A3/) [几何](https://kexue.fm/tag/%E5%87%A0%E4%BD%95/) [节日](https://kexue.fm/tag/%E8%8A%82%E6%97%A5/) [生活](https://kexue.fm/tag/%E7%94%9F%E6%B4%BB/) [文本生成](https://kexue.fm/tag/%E6%96%87%E6%9C%AC%E7%94%9F%E6%88%90/) [数论](https://kexue.fm/tag/%E6%95%B0%E8%AE%BA/)

### 随机文章

- [祝大家马年快乐！](https://kexue.fm/archives/2339)
- [为节约而生：从标准Attention到稀疏Attention](https://kexue.fm/archives/6853)
- [NBCE：使用朴素贝叶斯扩展LLM的Context处理长度](https://kexue.fm/archives/9617)
- [采样定理：有限个点构建出整个函数](https://kexue.fm/archives/3266)
- [算子与线性常微分方程(上)](https://kexue.fm/archives/1791)
- [上学了，更新放缓......](https://kexue.fm/archives/125)
- [ON-LSTM：用有序神经元表达层次结构](https://kexue.fm/archives/6621)
- [科学空间两岁了](https://kexue.fm/archives/1279)
- [双固定引力中心问题](https://kexue.fm/archives/1350)
- [SquarePlus：可能是运算最简单的ReLU光滑近似](https://kexue.fm/archives/8833)

### 最近评论

- [mxz](https://kexue.fm/archives/6760/comment-page-7#comment-28632): 哇谢谢回复！听起来好像可以类比为是这个向量的地址，哈哈哈苏神竟然还这么耐心地看评论~
- [pb](https://kexue.fm/archives/8265/comment-page-8#comment-28631): 谢谢，我也自己画了画图，的确如您所说，多个频率叠加可以得到比较有趣的衰减效应。
- [YNuclear](https://kexue.fm/archives/5253/comment-page-18#comment-28630): 感谢苏神的回复。我的理解是pθ(x)难以直接求解，因此VAE引入了后验分布...
- [苏剑林](https://kexue.fm/archives/10958/comment-page-3#comment-28629): 好的，那按照我的理解，我们的观点应该是已经对齐了。
- [苏剑林](https://kexue.fm/archives/8265/comment-page-8#comment-28628): 对，事后来看，这个上界的衰减性意义不大。不过这个上界是有机会达到的（虽然概率很小），所以算是给...
- [苏剑林](https://kexue.fm/archives/6760/comment-page-7#comment-28627): zq是一个向量，但如果它是事先给定的向量表的某一个，那么我们就可以用向量表中的编号代替它...
- [HanbinZheng](https://kexue.fm/archives/10958/comment-page-3#comment-28625): 感谢回复！我想表达的意思只是“根据原论文的推导，这个 sg\[⋅\] 是...
- [pb](https://kexue.fm/archives/8265/comment-page-8#comment-28624): 感谢回复。个人认为给两个完全没有限制的向量分析上界的意义不大，因为对于任意的 m,n 都存...
- [mxz](https://kexue.fm/archives/6760/comment-page-7#comment-28617): “这样一来，因为zq是编码表E中的向量之一，所以它实际上就等价于1,2,…,K这K个整数...
- [Seasons](https://kexue.fm/archives/8711/comment-page-1#comment-28616): 感谢苏神回复，我后来也发现这个假设有点问题，因为prefix tuning得到的这个向量除了在...

### 友情链接

- [Cool Papers](https://papers.cool)
- [数学研发](https://bbs.emath.ac.cn)
- [Seatop](http://www.seatop.com.cn/)
- [Xiaoxia](https://xiaoxia.org/)
- [积分表-网络版](https://kexue.fm/sci/integral/index.html)
- [丝路博傲](http://blog.dvxj.com/)
- [数学之家](http://www.2math.cn/)
- [有趣天文奇观](http://interesting-sky.china-vo.org/)
- [TwistedW](http://www.twistedwg.com/)
- [godweiyang](https://godweiyang.com/)
- [AI柠檬](https://blog.ailemon.net/)
- [王登科-DK博客](https://greatdk.com)
- [ESON](https://blog.eson.org/)
- [枫之羽](https://fzhiy.net/)
- [Mathor's blog](https://wmathor.com/)
- [coding-zuo](https://coding-zuo.github.io/)
- [博科园](https://www.bokeyuan.net/)
- [孔皮皮的博客](https://www.kppkkp.top/)
- [运鹏的博客](https://yunpengtai.top/)
- [jiming.site](https://jiming.site/)
- [OmegaXYZ](https://www.omegaxyz.com/)
- [EAI猩球](https://www.robotech.ink/)
- [文举的博客](https://liwenju0.com/)
- [用代码打点酱油](https://bruceyuan.com/)
- [Zhang's blog](https://armcvai.cn/)
- [申请链接](https://kexue.fm/links.html)

本站采用创作共用版权协议，要求署名、非商业用途和保持一致。转载本站内容必须也遵循“ [署名-非商业用途-保持一致](http://creativecommons.org/licenses/by-nc-nd/2.5/cn/)”的创作共用协议。
© 2009-2025 Scientific Spaces. All rights reserved. Theme by [laogui](http://www.laogui.com). Powered by [Typecho](http://typecho.org). 备案号: [粤ICP备09093259号-1/2](https://beian.miit.gov.cn/)。