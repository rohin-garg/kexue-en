## SEARCH

## MENU

- [打赏](https://kexue.fm/reward.html)
- [公式](https://kexue.fm/latex.html)
- [天象](https://kexue.fm/ac.html)
- [链接](https://kexue.fm/links.html)
- [时光](https://kexue.fm/me.html)
- [博览](https://kexue.fm/science.html)
- [归档](https://kexue.fm/content.html)

## CATEGORIES

- [千奇百怪](https://kexue.fm/category/Everything)
- [天文探索](https://kexue.fm/category/Astronomy)
- [数学研究](https://kexue.fm/category/Mathematics)
- [物理化学](https://kexue.fm/category/Phy-chem)
- [信息时代](https://kexue.fm/category/Big-Data)
- [生物自然](https://kexue.fm/category/Biology)
- [图片摄影](https://kexue.fm/category/Photograph)
- [问题百科](https://kexue.fm/category/Questions)
- [生活/情感](https://kexue.fm/category/Life-Feeling)
- [资源共享](https://kexue.fm/category/Resources)

## NEWPOSTS

- [“对角+低秩”三角阵的高效求逆方法](https://kexue.fm/archives/11072)
- [通过msign来计算奇异值裁剪mc...](https://kexue.fm/archives/11059)
- [矩阵符号函数mcsgn能计算什么？](https://kexue.fm/archives/11056)
- [线性注意力简史：从模仿、创新到反哺](https://kexue.fm/archives/11033)
- [msign的导数](https://kexue.fm/archives/11025)
- [通过msign来计算奇异值裁剪mc...](https://kexue.fm/archives/11006)
- [msign算子的Newton-Sc...](https://kexue.fm/archives/10996)
- [等值振荡定理：最优多项式逼近的充要条件](https://kexue.fm/archives/10972)
- [生成扩散模型漫谈（三十）：从瞬时速...](https://kexue.fm/archives/10958)
- [MoE环游记：5、均匀分布的反思](https://kexue.fm/archives/10945)

## COMMENTS

- [WB: 非常清楚的blog。我有一个小问题想问一下，推导的时候用的是不...](https://kexue.fm/archives/10795/comment-page-1#comment-28056)
- [liangzhh: 谢谢大佬的分享，感觉中间有两个手误敲错，式(9)最后应该是加号...](https://kexue.fm/archives/11072/comment-page-1#comment-28055)
- [lidhrandom: Equation 3的等号右侧第二项的第一个${\\Lambda...](https://kexue.fm/archives/11072/comment-page-1#comment-28054)
- [Kuo: 在 $PaTH$ 论文章节 \`UT Transform for...](https://kexue.fm/archives/11033/comment-page-1#comment-28053)
- [Fanhao: 假定Hessian阵正定，那不是意味着$L(\\theta)$是...](https://kexue.fm/archives/10542/comment-page-1#comment-28052)
- [曲笑一: 对于第一个疑问，我看到分布式的版本已经开源。我在想如果将每个梯...](https://kexue.fm/archives/10739/comment-page-2#comment-28051)
- [曲笑一: 苏老师您好，阅读了您关于Muon系列的博客，受益匪浅。在此有两...](https://kexue.fm/archives/10739/comment-page-2#comment-28050)
- [tll1945tll1937: 老师，您好，向您请教一个问题：会不会因为LoRA中用到的梯度的...](https://kexue.fm/archives/10266/comment-page-1#comment-28049)
- [香蕉大王: 还是刚刚flow matching的例子$\\frac{d x\_...](https://kexue.fm/archives/9280/comment-page-2#comment-28048)
- [香蕉大王: 谢谢老师回复。明白老师您说的了。我再想请问一个小小的问题：既然...](https://kexue.fm/archives/9280/comment-page-2#comment-28047)

## USERLOGIN

- [登录](https://kexue.fm/admin/login.php)

[科学空间\|Scientific Spaces](https://kexue.fm)

- [登录](https://kexue.fm/admin/login.php)
- [打赏](https://kexue.fm/reward.html)
- [公式](https://kexue.fm/latex.html)
- [天象](https://kexue.fm/ac.html)
- [链接](https://kexue.fm/links.html)
- [时光](https://kexue.fm/me.html)
- [博览](https://kexue.fm/science.html)
- [归档](https://kexue.fm/content.html)

渴望成为一个小飞侠

- [欢迎订阅](https://kexue.fm/feed)
- [个性邮箱](https://kexue.fm/archives/119)
- [天象信息](https://kexue.fm/ac.html)
- [观测ISS](https://kexue.fm/archives/41)
- [LaTeX](https://kexue.fm/latex.html)
- [关于博主](https://kexue.fm/me.html)

欢迎访问“科学空间”，这里将与您共同探讨自然科学，回味人生百态；也期待大家的分享～

- [**千奇百怪** Everything](https://kexue.fm/category/Everything)
- [**天文探索** Astronomy](https://kexue.fm/category/Astronomy)
- [**数学研究** Mathematics](https://kexue.fm/category/Mathematics)
- [**物理化学** Phy-chem](https://kexue.fm/category/Phy-chem)
- [**信息时代** Big-Data](https://kexue.fm/category/Big-Data)
- [**生物自然** Biology](https://kexue.fm/category/Biology)
- [**图片摄影** Photograph](https://kexue.fm/category/Photograph)
- [**问题百科** Questions](https://kexue.fm/category/Questions)
- [**生活/情感** Life-Feeling](https://kexue.fm/category/Life-Feeling)
- [**资源共享** Resources](https://kexue.fm/category/Resources)

- [**千奇百怪**](https://kexue.fm/category/Everything)
- [**天文探索**](https://kexue.fm/category/Astronomy)
- [**数学研究**](https://kexue.fm/category/Mathematics)
- [**物理化学**](https://kexue.fm/category/Phy-chem)
- [**信息时代**](https://kexue.fm/category/Big-Data)
- [**生物自然**](https://kexue.fm/category/Biology)
- [**图片摄影**](https://kexue.fm/category/Photograph)
- [**问题百科**](https://kexue.fm/category/Questions)
- [**生活/情感**](https://kexue.fm/category/Life-Feeling)
- [**资源共享**](https://kexue.fm/category/Resources)

[首页](https://kexue.fm) [信息时代](https://kexue.fm/category/Big-Data) NBCE：使用朴素贝叶斯扩展LLM的Context处理长度

23May

# [NBCE：使用朴素贝叶斯扩展LLM的Context处理长度](https://kexue.fm/archives/9617)

By 苏剑林 \|
2023-05-23 \|
107513位读者\|

> 在LLM时代还玩朴素贝叶斯（Naive Bayes）？

这可能是许多读者在看到标题后的首个想法。确实如此，当古老的朴素贝叶斯与前沿的LLM相遇时，产生了令人惊讶的效果——我们可以直接扩展现有LLM模型的Context处理长度，无需对模型进行微调，也不依赖于模型架构，具有线性效率，而且效果看起来还不错——这就是本文所提出的NBCE（ **N** aive **B** ayes-based **C** ontext **E** xtension）方法。

## 摸石过河 [\#](https://kexue.fm/archives/9617\#%E6%91%B8%E7%9F%B3%E8%BF%87%E6%B2%B3)

假设$T$为要生成的token序列，$S\_1,S\_2,\\cdots,S\_n$是给定的若干个相对独立的Context集合（比如$n$个不同的段落，至少不是一个句子被分割为两个片段那种），假设它们的总长度已经超过了训练长度，而单个$S\_k$加$T$还在训练长度内。我们需要根据$S\_1,S\_2,\\cdots,S\_n$生成$T$，即估计$p(T\|S\_1, S\_2,\\cdots,S\_n)$。

简单来说，朴素贝叶斯就是“贝叶斯公式+独立假设”。根据贝叶斯公式：
\\begin{equation}p(T\|S\_1, S\_2,\\cdots,S\_n) \\propto p(S\_1, S\_2,\\cdots,S\_n\|T)p(T)\\end{equation}
这里的$\\propto$，是省去了与$T$无关的常数因子。根据（条件）独立假设：
\\begin{equation}p(S\_1, S\_2,\\cdots,S\_n\|T) = \\prod\_{k=1}^n p(S\_k\|T)\\end{equation}
所以有
\\begin{equation}p(T\|S\_1, S\_2,\\cdots,S\_n) \\propto p(T)\\prod\_{k=1}^n p(S\_k\|T)\\end{equation}
再次根据贝叶斯公式$p(S\_k\|T) \\propto \\frac{p(T\|S\_k)}{p(T)}$，得到
\\begin{equation}p(T\|S\_1, S\_2,\\cdots,S\_n) \\propto \\frac{1}{p^{n-1}(T)}\\prod\_{k=1}^n p(T\|S\_k)\\end{equation}
或者
\\begin{equation}\\log p(T\|S\_1, S\_2,\\cdots,S\_n) = \\color{red}{\\sum\_{k=1}^n \\log p(T\|S\_k)} - \\color{green}{(n-1)\\log p(T)} + \\color{skyblue}{\\text{常数}}\\label{eq:nbce-1}\\end{equation}

这里的$\\color{red}{p(T\|S\_k)}$和$\\color{green}{p(T)}$都可以直接用现有的LLM进行计算，而且只要是语言模型都行，跟架构无关，也不需要用长文本微调。其中，$\\color{red}{p(T\|S\_k)}$是单个Context所预测的概率，$\\color{green}{p(T)}$则无Context（或者Context为空）的概率，并且多个Context可以放在同一个batch中并行计算，计算量随着Context数的增加是线性增长的。

## 抽丝剥茧 [\#](https://kexue.fm/archives/9617\#%E6%8A%BD%E4%B8%9D%E5%89%A5%E8%8C%A7)

当然，朴素贝叶斯依赖于独立假设，这会限制它的实际效果。为了“青出于蓝而胜于蓝”，我们不妨将式$\\eqref{eq:nbce-1}$进一步“抽丝剥茧”、“去芜存菁”，以达到更好的效果。

首先我们记$\\log p(T\|S) = \[\\log p(T\|S\_1),\\cdots,\\log p(T\|S\_n)\]$，以及
\\begin{equation}\\overline{\\log p(T\|S)} = \\frac{1}{n}\\sum\_{k=1}^n \\log p(T\|S\_k)\\end{equation}
并设$\\beta = n - 1$，那么式$\\eqref{eq:nbce-1}$可以重写为
\\begin{equation}\\log p(T\|S\_1, S\_2,\\cdots,S\_n) = \\color{red}{(\\beta + 1)\\overline{\\log p(T\|S)}} - \\color{green}{\\beta\\log p(T)} + \\color{skyblue}{\\text{常数}}\\label{eq:nbce-2}\\end{equation}

重写为上述形式后，自然而言地引出了两个问题：

> 1、如果将$\\beta$作为超参数来调，是否可能取得更好的效果？
>
> 2、$\\overline{\\log p(T\|S)}$就是$\\log p(T\|S)$的Average Pooling，那么换成其他Pooling方法（简记为$\\mathcal{P}$）是否有更好的效果？即
> \\begin{equation}\\log p(T\|S\_1, S\_2,\\cdots,S\_n) = \\color{red}{(\\beta + 1)\\mathcal{P}\[\\log p(T\|S)\]} - \\color{green}{\\beta\\log p(T)} + \\color{skyblue}{\\text{常数}}\\label{eq:nbce-3}\\end{equation}

于是笔者在7B模型上围绕这两个问题进行调试，得到的初步结论是：在阅读理解场景中Max Pooling配合$\\beta=0.25$，用Greedy Search总体表现比较好，然而Random Sample出来的结果基本不可读。

## 最终方案 [\#](https://kexue.fm/archives/9617\#%E6%9C%80%E7%BB%88%E6%96%B9%E6%A1%88)

为什么会出现Greedy Search好而Random Sample差的情况呢？我们知道，Random Sample是“按照分布采样”，它的效果差说明Max Pooling的结果不是一个合理的分布；而Greedy Search只关心最大概率者，而不关心分布的合理性，它的效果好告诉我们概率最大的token正确性较高。

概率越大说明不确定性越低，所以为了改善Random Sample的效果，我们将Pooling方式改为直接输出不确定性最低的那个分布：
\\begin{equation}\\begin{aligned}
&\\mathcal{P}\[\\log p(T\|S)\] = \\log p(T\|S\_{\\color{red}{k}}) \\\\[5pt\]
&\\color{red}{k} = \\mathop{\\text{argmin}} \\big\\{H\_1,H\_2,\\cdots,H\_n\\big\\} \\\\[5pt\]
&H\_i = -\\sum\_T p(T\|S\_i)\\log p(T\|S\_i)
\\end{aligned}\\end{equation}
代入到式$\\eqref{eq:nbce-3}$，就是最终的NBCE（ **N** aive **B** ayes-based **C** ontext **E** xtension）。

值得指出的是，虽然我们的出发点是朴素贝叶斯，但一般化后的式$\\eqref{eq:nbce-3}$已经超出了常规的朴素贝叶斯的范畴，同时保留了朴素贝叶斯的可解释性。不难看出，式$\\eqref{eq:nbce-3}$的形式很是直观：

> 1、不同Context的预测结果通过方法$\\mathcal{P}$聚合（或者说投票）在一起（权重为$\\beta+1$），并减去无Context的预测结果（权重为$\\beta$）；
>
> 2、之所以要减去无Context预测结果，是为了让模型更加倾向于结合Context而不是纯粹根据自身知识储备来回答（注：3天后出现在Arxiv的论文 [《Trusting Your Evidence: Hallucinate Less with Context-aware Decoding》](https://papers.cool/arxiv/2305.14739) 也提出了相同的技巧用来减少幻觉）；
>
> 3、不同场景可以选择不同的$\\beta$，比如需要结合Context做阅读理解的，可以考虑较大的$\\beta$，如果偏向于自由创作，则选择较小的$\\beta$，笔者认为$\\beta\\geq -1$都是合理的。

## 参考实现 [\#](https://kexue.fm/archives/9617\#%E5%8F%82%E8%80%83%E5%AE%9E%E7%8E%B0)

下面给出NBCE的参考实现：

> **Github: [https://github.com/bojone/NBCE](https://github.com/bojone/NBCE)**

从演示代码可以看出，NBCE的实现很简单，只需要修改一下解码函数中的logits构建方式，跟解码算法的选择并不冲突。

Naive Bayes-based Context Extension（NBCE）示意图

所给的Demo包含12段不同的Context，总长度为9000多字，连同8个问题一次性输入到模型中（模型训练长度为2048，参数量为7B，可以在 [OpenBuddy](https://openbuddy.ai/) 下载），模型能够逐一根据所给Context正确回答这8个问题。值得指出的是，所有的Context、问题和答案加起来，超过了1万字！另外，有朋友简单尝试了简历匹配和作文打分应用，效果也尚可，非常建议大家亲自调试一下。

## 相关工作 [\#](https://kexue.fm/archives/9617\#%E7%9B%B8%E5%85%B3%E5%B7%A5%E4%BD%9C)

扩展LLM的Context长度其实已有不少，但多数是通过结合检索或者摘要的方式来缩短样本的长Context，如 [Unlimiformer](https://papers.cool/arxiv/2305.01625)。由于不是直接处理长Context，因此通常无法做精细的阅读理解，而且这些方案往往需要在训练阶段就考虑进去，而不是事后即插即用到已有的LLM模型中。

在NBCE之前，能够不微调地扩展Context长度的方案是Parallel Context Window（下面简称PCW），出自论文 [《Parallel Context Windows for Large Language Models》](https://papers.cool/arxiv/2212.10947) 和 [《Structured Prompting: Scaling In-Context Learning to 1,000 Examples》](https://papers.cool/arxiv/2212.06713)，两篇论文是同一时期不同作者的工作，但所提的方法只有细微的差别，因此这里都将它们叫做PCW。

PCW适用于Self Attention模型，主要修改包括Position Encoding和Attention Mask，如下图所示：

Parallel Context Window

首先确定Context的最大长度$L$（图中为6），然后每个Context的最后一个位置编码为$L-1$，倒数第二个位置编码为$L-2$，...，依此类推，这种编码方式我们称为“右对齐”（或者“左缩进”）；另一边，对于Task Tokens部分（Prompt+生成内容），我们的位置编码是$L,L+1,L+2,\\cdots$。每个Context单独编码，所以对应的Attention Mask是分块对角矩阵，而因为是LM，所以是分块对角下三角阵；至于Task Tokens部分需要结合所有的Context，所以它需要Attention到所有Context（以及它自身）。这样一来，如果将每个Context单独拿出来，和Task Tokens拼在一起，其Attention模式就跟原本的LM一致了。

或许有读者看出，其实NBCE跟PCW有着很相似的特性，比如对于Context都是无序的、平权的。事实上，如果将NBCE应用到单层单头注意力模型中，那么结果大致上就是PCW。为了显示这一点，我们写出单层单头注意力的语言模型为
\\begin{equation}p(x\_t\|x\_{< t}) = softmax\\left(\\sum\_{i=1}^t a\_{t,i}v\_i W\\right)\\end{equation}
所以大致上有$\\log p(x\_t\|x\_{< t}) \\sim \\sum\\limits\_{i=1}^t a\_{t,i}v\_i W$，接着代入到式$\\eqref{eq:nbce-2}$并取$\\beta=0$，得到
\\begin{equation}\\log p(T\|S\_1, S\_2,\\cdots,S\_n) \\sim \\frac{1}{n}\\sum\_{k=1}^n\\left(\\sum\_{i\\in S\_k} a\_{T,i}v\_i\\right) W = \\left(\\sum\_{i\\in S\_1\\oplus\\cdots\\oplus S\_n} \\frac{a\_{T,i}}{n}v\_i\\right) W \\end{equation}
这里假设的是$T$是单个token，但其实已经不失一般性了，$\\oplus$是拼接的意思。在上式中，$S\_k\\oplus T$是作为一个连续片段来推理的（NBCE的设定），所以它们的位置编码相邻，而$a\_{T,i}/n$构成了$T$与所有$S\_i$的一个整体Attention（求和同样是1），这些特性跟PCW其实是一致的，PCW只不过是以Attention Mask的方式更优雅地整合到每一层中。

因此，PCW大致上就是Average Pooling版的NBCE，我们实测也发现它跟Average Pooling版的NBCE有着相似的缺点——当Context数据增加时，输出的结果开始不够准确，具体表现为主题相关，但是作为问题的答案来说是错误的。

## 延伸思考 [\#](https://kexue.fm/archives/9617\#%E5%BB%B6%E4%BC%B8%E6%80%9D%E8%80%83)

NBCE的一大缺点是无序性，即无法识别Context的输入顺序，这在续写故事等场景可能表现欠佳。为了缓解这一点，可以考虑在每一个Context前面加个能指示序信息的prefix，就好比小说中的“第一章”、“第二章”那样。

总的来说，目前笔者关于NBCE的测试都限于“阅读理解”场景，即“理解”长文本，能否用此方法来“生成”长文本，还是个未知数，期待大家的测试结果。

此外，还有一个有意思的问题是：

> 既然朴素贝叶斯都能在LLM领域能派上用场，那么其他传统概率模型（比如HMM）是否也能在LLM领域有它们的一席之地呢？

## 文章小结 [\#](https://kexue.fm/archives/9617\#%E6%96%87%E7%AB%A0%E5%B0%8F%E7%BB%93)

本文提出了NBCE（Naive Bayes-based Context Extension），它基于朴素贝叶斯思想来扩展LLM的Context处理长度，有着即插即用、模型无关、无须微调、线性效率、实现简单等优点，并且看上去效果还不错，欢迎大家测试。

_**转载到请包括本文地址：** [https://kexue.fm/archives/9617](https://kexue.fm/archives/9617)_

_**更详细的转载事宜请参考：**_ [《科学空间FAQ》](https://kexue.fm/archives/6508#%E6%96%87%E7%AB%A0%E5%A6%82%E4%BD%95%E8%BD%AC%E8%BD%BD/%E5%BC%95%E7%94%A8)

**如果您还有什么疑惑或建议，欢迎在下方评论区继续讨论。**

**如果您觉得本文还不错，欢迎 [分享](https://kexue.fm/archives/9617#share)/ [打赏](https://kexue.fm/archives/9617#pay) 本文。打赏并非要从中获得收益，而是希望知道科学空间获得了多少读者的真心关注。当然，如果你无视它，也不会影响你的阅读。再次表示欢迎和感谢！**

打赏

微信打赏

支付宝打赏

因为网站后台对打赏并无记录，因此欢迎在打赏时候备注留言。你还可以 [**点击这里**](http://mail.qq.com/cgi-bin/qm_share?t=qm_mailme&email=tN7d1drY3drrx8H0xcWa19vZ) 或在下方评论区留言来告知你的建议或需求。

**如果您需要引用本文，请参考：**

苏剑林. (May. 23, 2023). 《NBCE：使用朴素贝叶斯扩展LLM的Context处理长度 》\[Blog post\]. Retrieved from [https://kexue.fm/archives/9617](https://kexue.fm/archives/9617)

@online{kexuefm-9617,
        title={NBCE：使用朴素贝叶斯扩展LLM的Context处理长度},
        author={苏剑林},
        year={2023},
        month={May},
        url={\\url{https://kexue.fm/archives/9617}},
}

分类： [信息时代](https://kexue.fm/category/Big-Data)    标签： [语言模型](https://kexue.fm/tag/%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/), [外推](https://kexue.fm/tag/%E5%A4%96%E6%8E%A8/), [LLM](https://kexue.fm/tag/LLM/), [贝叶斯](https://kexue.fm/tag/%E8%B4%9D%E5%8F%B6%E6%96%AF/)[62 评论](https://kexue.fm/archives/9617#comments)

< [基于量子化假设推导模型的尺度定律（Scaling Law）](https://kexue.fm/archives/9607) \| [关于NBCE方法的一些补充说明和分析](https://kexue.fm/archives/9632) >

### 你也许还对下面的内容感兴趣

- [Transformer升级之路：20、MLA好在哪里?（上）](https://kexue.fm/archives/10907)
- [Transformer升级之路：19、第二类旋转位置编码](https://kexue.fm/archives/10862)
- [Decoder-only的LLM为什么需要位置编码？](https://kexue.fm/archives/10347)
- [Monarch矩阵：计算高效的稀疏型矩阵分解](https://kexue.fm/archives/10249)
- [缓存与效果的极限拉扯：从MHA、MQA、GQA到MLA](https://kexue.fm/archives/10091)
- [时空之章：将Attention视为平方复杂度的RNN](https://kexue.fm/archives/10017)
- [Transformer升级之路：16、“复盘”长度外推技术](https://kexue.fm/archives/9948)
- [我在Performer中发现了Transformer-VQ的踪迹](https://kexue.fm/archives/9862)
- [Transformer升级之路：15、Key归一化助力长度外推](https://kexue.fm/archives/9859)
- [预训练一下，Transformer的长序列成绩还能涨不少！](https://kexue.fm/archives/9787)

[发表你的看法](https://kexue.fm/archives/9617#comment_form)

1. [«](https://kexue.fm/archives/9617/comment-page-2#comments)
2. [1](https://kexue.fm/archives/9617/comment-page-1#comments)
3. [2](https://kexue.fm/archives/9617/comment-page-2#comments)
4. [3](https://kexue.fm/archives/9617/comment-page-3#comments)

junmo

June 8th, 2023

以及苏神我们发现在INT8量化模型上NBCE的效果似乎会略好一些。感觉是不是和TopP的思路相同，实际是量化rounding的引入，降低了解码概率的噪声影响，是不是可以考虑下INT8量化推理，10K的文本不到30G的显存就可以

[回复评论](https://kexue.fm/archives/9617/comment-page-3?replyTo=21891#respond-post-9617)

[苏剑林](https://kexue.fm) 发表于
June 8th, 2023

关于量化，我没具体实践过，所以不确定rounding能否起到类似截断的作用。

不过可以尝试从另一个角度解释。如果你用的模型是chatglm，我认为它是不够好的（至少没有openbuddy），可能是过拟合导致的。而对于过拟合的模型，量化可能有助于减缓过拟合，从而提升效果。

[回复评论](https://kexue.fm/archives/9617/comment-page-3?replyTo=21902#respond-post-9617)

junmo 发表于
June 9th, 2023

苏神还想请教下，这里说chatglm的过拟合，具体是指哪一方面呢？是指它在指令微调上存在一定的过拟合么？这个过拟合是如何发现的呢？
Thanks~

[回复评论](https://kexue.fm/archives/9617/comment-page-3?replyTo=21919#respond-post-9617)

[苏剑林](https://kexue.fm) 发表于
June 12th, 2023

比如用英文提问，答案可能会出现中英混合，这某种程度上是对中文的过拟合了。其他过拟合我没有特别测，但感觉大家都在6b、7b这个量级下，chatglm表现并不是很优异，猜测是过拟合造成的。

[回复评论](https://kexue.fm/archives/9617/comment-page-3?replyTo=21942#respond-post-9617)

junmo 发表于
June 21st, 2023

哦哦是这个意思，确实中英平行感觉是差点意思。感谢苏神的回答！

[回复评论](https://kexue.fm/archives/9617/comment-page-3?replyTo=22028#respond-post-9617)

[Naive Bayes is all you need ? R11; AI 資訊](https://news.aitime.space/2023/06/3197/)

June 8th, 2023

\[...\]很抱歉，起了这么个具有标题党特征的题目。在写完《NBCE：使用朴素贝叶斯扩展LLM的Context处理长度》之后，笔者就觉得朴素贝叶斯（Naive Bayes）跟Attention机制有很多相同的特征，后来再推导了一下发现，Attention机制其实可以看成是一种广义的、参数化的朴素贝叶斯。既然如此，那么“Attention is All You Need”不也就意味着“Naive Bayes i\[...\]

[回复评论](https://kexue.fm/archives/9617/comment-page-3?replyTo=21909#respond-post-9617)

[解密Prompt系列8. 无需训练让LLM支持超长输入:知识库 & unlimiformer & PCW & NBCE \| 呱唧呱唧网](http://www.itfaba.com/jishufenxian/107941.html)

June 13th, 2023

\[...\]苏剑林. (May. 23, 2023). 《NBCE：使用朴素贝叶斯扩展LLM的Context处理长度 》\[Blog post\]. Retrieved from [https://spaces.ac.cn/archives/9617](https://spaces.ac.cn/archives/9617)\[...\]

[回复评论](https://kexue.fm/archives/9617/comment-page-3?replyTo=21959#respond-post-9617)

[解密Prompt系列8. 无需训练让LLM支持超长输入:知识库 & unlimiformer & PCW & NBCE R11; AI 資訊](https://news.aitime.space/2023/06/4386/)

June 14th, 2023

\[...\]苏剑林. (May. 23, 2023). 《NBCE：使用朴素贝叶斯扩展LLM的Context处理长度 》\[Blog post\]. Retrieved from [https://spaces.ac.cn/archives/9617](https://spaces.ac.cn/archives/9617)\[...\]

[回复评论](https://kexue.fm/archives/9617/comment-page-3?replyTo=21967#respond-post-9617)

william

March 4th, 2024

苏神，求教一下：目前看NBCE的实现上是分别用不同的Context+prompt，送给LLM进行批量推理，假设有12段Context，问题有8个。这里有几个问题我不大明白：
1\. 因为prompt都是要回答8个问题，但是Context每次只有一个，LLM理论上每个Context都会回答8个问题，那12个Context就会回答12\*8=96次，是怎么做到最终8个问题对应8个答案的呢？
2\. 鉴于NBCE这种Context+prompt的方案，对于使用LLM的用户来讲，可能只有一个256k长度的大文本，如果使用NBCE，需要事先对这个长文本进行切块处理，每块不高于原始Context长度？如果需要切块，如何处理上下文的联系？

烦请苏神指导一下，谢谢

[回复评论](https://kexue.fm/archives/9617/comment-page-3?replyTo=23856#respond-post-9617)

[苏剑林](https://kexue.fm) 发表于
March 7th, 2024

1、在模型不够强的情况下，对于与问题无关的context，回答问题时的预测概率就会比较低，相对来说问题相关的context就会概率高，因此基于熵的选择能够比较准确地、自适应地定位到正确的context，不过对于模型本身足够强（比如70B的模型）时，NBCE通常效果不好，因此对于问题无关的context，模型会以非常高的概率直接输出“无法回答”之类的回复，从而导致所有问题都是“无法回答”。

2、如果只有一个足够长的context，可以考虑用sliding window的方式进行切分，跟以往做长文本阅读理解一样的思路。

[回复评论](https://kexue.fm/archives/9617/comment-page-3?replyTo=23881#respond-post-9617)

1. [«](https://kexue.fm/archives/9617/comment-page-2#comments)
2. [1](https://kexue.fm/archives/9617/comment-page-1#comments)
3. [2](https://kexue.fm/archives/9617/comment-page-2#comments)
4. [3](https://kexue.fm/archives/9617/comment-page-3#comments)

[取消回复](https://kexue.fm/archives/9617#respond-post-9617)

你的大名

电子邮箱

个人网站（选填）

1\. 可以使用LaTeX代码，点击“预览效果”可查看效果；2. 可以通过点击评论楼层编号来引用该楼层；3. 网站可能会有点卡，如非确认评论失败，请 **不要重复点击提交**。

### 内容速览

[摸石过河](https://kexue.fm/archives/9617#%E6%91%B8%E7%9F%B3%E8%BF%87%E6%B2%B3)
[抽丝剥茧](https://kexue.fm/archives/9617#%E6%8A%BD%E4%B8%9D%E5%89%A5%E8%8C%A7)
[最终方案](https://kexue.fm/archives/9617#%E6%9C%80%E7%BB%88%E6%96%B9%E6%A1%88)
[参考实现](https://kexue.fm/archives/9617#%E5%8F%82%E8%80%83%E5%AE%9E%E7%8E%B0)
[相关工作](https://kexue.fm/archives/9617#%E7%9B%B8%E5%85%B3%E5%B7%A5%E4%BD%9C)
[延伸思考](https://kexue.fm/archives/9617#%E5%BB%B6%E4%BC%B8%E6%80%9D%E8%80%83)
[文章小结](https://kexue.fm/archives/9617#%E6%96%87%E7%AB%A0%E5%B0%8F%E7%BB%93)

### 智能搜索

支持整句搜索！网站自动使用 [结巴分词](https://github.com/fxsjy/jieba) 进行分词，并结合ngrams排序算法给出合理的搜索结果。

### 热门标签

[生成模型](https://kexue.fm/tag/%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B/) [attention](https://kexue.fm/tag/attention/) [优化](https://kexue.fm/tag/%E4%BC%98%E5%8C%96/) [语言模型](https://kexue.fm/tag/%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/) [模型](https://kexue.fm/tag/%E6%A8%A1%E5%9E%8B/) [网站](https://kexue.fm/tag/%E7%BD%91%E7%AB%99/) [概率](https://kexue.fm/tag/%E6%A6%82%E7%8E%87/) [梯度](https://kexue.fm/tag/%E6%A2%AF%E5%BA%A6/) [转载](https://kexue.fm/tag/%E8%BD%AC%E8%BD%BD/) [矩阵](https://kexue.fm/tag/%E7%9F%A9%E9%98%B5/) [微分方程](https://kexue.fm/tag/%E5%BE%AE%E5%88%86%E6%96%B9%E7%A8%8B/) [天象](https://kexue.fm/tag/%E5%A4%A9%E8%B1%A1/) [分析](https://kexue.fm/tag/%E5%88%86%E6%9E%90/) [深度学习](https://kexue.fm/tag/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/) [积分](https://kexue.fm/tag/%E7%A7%AF%E5%88%86/) [python](https://kexue.fm/tag/python/) [优化器](https://kexue.fm/tag/%E4%BC%98%E5%8C%96%E5%99%A8/) [力学](https://kexue.fm/tag/%E5%8A%9B%E5%AD%A6/) [无监督](https://kexue.fm/tag/%E6%97%A0%E7%9B%91%E7%9D%A3/) [扩散](https://kexue.fm/tag/%E6%89%A9%E6%95%A3/) [几何](https://kexue.fm/tag/%E5%87%A0%E4%BD%95/) [节日](https://kexue.fm/tag/%E8%8A%82%E6%97%A5/) [生活](https://kexue.fm/tag/%E7%94%9F%E6%B4%BB/) [文本生成](https://kexue.fm/tag/%E6%96%87%E6%9C%AC%E7%94%9F%E6%88%90/) [数论](https://kexue.fm/tag/%E6%95%B0%E8%AE%BA/)

### 随机文章

- [科学空间终于恢复访问了！](https://kexue.fm/archives/1658)
- [【翻译】庆祝希格斯玻色子的最终发现！](https://kexue.fm/archives/1661)
- [400年前的今天，望远镜诞生了](https://kexue.fm/archives/100)
- [【语料】百度的中文问答数据集WebQA](https://kexue.fm/archives/4338)
- [【NASA每日一图】水星上的双环陨石坑](https://kexue.fm/archives/168)
- [【备忘】在自己的电脑上搭建服务器](https://kexue.fm/archives/1665)
- [路径积分系列：4.随机微分方程](https://kexue.fm/archives/3762)
- [基于Conv1D的光谱分类模型（一维序列分类）](https://kexue.fm/archives/5505)
- [2010年全国天文奥赛终于可以报名了](https://kexue.fm/archives/332)
- [几何-算术均值不等式的一般证明](https://kexue.fm/archives/96)

### 最近评论

- [WB](https://kexue.fm/archives/10795/comment-page-1#comment-28056): 非常清楚的blog。我有一个小问题想问一下，推导的时候用的是不等式（10），这里左边O(1)，...
- [liangzhh](https://kexue.fm/archives/11072/comment-page-1#comment-28055): 谢谢大佬的分享，感觉中间有两个手误敲错，式(9)最后应该是加号，另外是chunk而不是chuck吧？
- [lidhrandom](https://kexue.fm/archives/11072/comment-page-1#comment-28054): Equation 3的等号右侧第二项的第一个${\\Lambda^{-1}}$疑似不应取逆
- [Kuo](https://kexue.fm/archives/11033/comment-page-1#comment-28053): 在 $PaTH$ 论文章节 \`UT Transform for Products of Hou...
- [Fanhao](https://kexue.fm/archives/10542/comment-page-1#comment-28052): 假定Hessian阵正定，那不是意味着$L(\\theta)$是$\\theta$的凸函数吗？这一...
- [曲笑一](https://kexue.fm/archives/10739/comment-page-2#comment-28051): 对于第一个疑问，我看到分布式的版本已经开源。我在想如果将每个梯度矩阵G拆分为N\*N,再利用mu...
- [曲笑一](https://kexue.fm/archives/10739/comment-page-2#comment-28050): 苏老师您好，阅读了您关于Muon系列的博客，受益匪浅。在此有两个疑问想请教您：第一个问题是，M...
- [tll1945tll1937](https://kexue.fm/archives/10266/comment-page-1#comment-28049): 老师，您好，向您请教一个问题：会不会因为LoRA中用到的梯度的维度仅仅是全参数微调中梯度的维度...
- [香蕉大王](https://kexue.fm/archives/9280/comment-page-2#comment-28048): 还是刚刚flow matching的例子$\\frac{d x\_t}{dt} = v\_\\thet...
- [香蕉大王](https://kexue.fm/archives/9280/comment-page-2#comment-28047): 谢谢老师回复。明白老师您说的了。我再想请问一个小小的问题：既然ODE未必是唯一的，有没有人尝试...

### 友情链接

- [Cool Papers](https://papers.cool)
- [数学研发](https://bbs.emath.ac.cn)
- [Seatop](http://www.seatop.com.cn/)
- [Xiaoxia](https://xiaoxia.org/)
- [积分表-网络版](https://kexue.fm/sci/integral/index.html)
- [丝路博傲](http://blog.dvxj.com/)
- [ph4ntasy 饭特稀](http://www.ph4ntasy.com/)
- [数学之家](http://www.2math.cn/)
- [有趣天文奇观](http://interesting-sky.china-vo.org/)
- [TwistedW](http://www.twistedwg.com/)
- [godweiyang](https://godweiyang.com/)
- [AI柠檬](https://blog.ailemon.net/)
- [王登科-DK博客](https://greatdk.com)
- [ESON](https://blog.eson.org/)
- [枫之羽](https://fzhiy.net/)
- [Mathor's blog](https://wmathor.com/)
- [coding-zuo](https://coding-zuo.github.io/)
- [博科园](https://www.bokeyuan.net/)
- [孔皮皮的博客](https://www.kppkkp.top/)
- [运鹏的博客](https://yunpengtai.top/)
- [jiming.site](https://jiming.site/)
- [OmegaXYZ](https://www.omegaxyz.com/)
- [Blog by Eacls](https://www.eacls.top/)
- [EAI猩球](https://www.robotech.ink/)
- [文举的博客](https://liwenju0.com/)
- [用代码打点酱油](https://bruceyuan.com/)
- [申请链接](https://kexue.fm/links.html)

本站采用创作共用版权协议，要求署名、非商业用途和保持一致。转载本站内容必须也遵循“ [署名-非商业用途-保持一致](http://creativecommons.org/licenses/by-nc-nd/2.5/cn/)”的创作共用协议。
© 2009-2025 Scientific Spaces. All rights reserved. Theme by [laogui](http://www.laogui.com). Powered by [Typecho](http://typecho.org). 备案号: [粤ICP备09093259号-1/2](https://beian.miit.gov.cn/)。