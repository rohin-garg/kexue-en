![MobileSideBar](https://kexue.fm/usr/themes/geekg/images/slide-button.png)

## SEARCH

## MENU

- [打赏](https://kexue.fm/reward.html)
- [公式](https://kexue.fm/latex.html)
- [天象](https://kexue.fm/ac.html)
- [链接](https://kexue.fm/links.html)
- [时光](https://kexue.fm/me.html)
- [博览](https://kexue.fm/science.html)
- [归档](https://kexue.fm/content.html)

## CATEGORIES

- [千奇百怪](https://kexue.fm/category/Everything)
- [天文探索](https://kexue.fm/category/Astronomy)
- [数学研究](https://kexue.fm/category/Mathematics)
- [物理化学](https://kexue.fm/category/Phy-chem)
- [信息时代](https://kexue.fm/category/Big-Data)
- [生物自然](https://kexue.fm/category/Biology)
- [图片摄影](https://kexue.fm/category/Photograph)
- [问题百科](https://kexue.fm/category/Questions)
- [生活/情感](https://kexue.fm/category/Life-Feeling)
- [资源共享](https://kexue.fm/category/Resources)

## NEWPOSTS

- [让炼丹更科学一些（五）：基于梯度精...](https://kexue.fm/archives/11530)
- [让炼丹更科学一些（四）：新恒等式，...](https://kexue.fm/archives/11494)
- [为什么DeltaNet要加L2 N...](https://kexue.fm/archives/11486)
- [让炼丹更科学一些（三）：SGD的终...](https://kexue.fm/archives/11480)
- [让炼丹更科学一些（二）：将结论推广...](https://kexue.fm/archives/11469)
- [滑动平均视角下的权重衰减和学习率](https://kexue.fm/archives/11459)
- [生成扩散模型漫谈（三十一）：预测数...](https://kexue.fm/archives/11428)
- [Muon优化器指南：快速上手与关键细节](https://kexue.fm/archives/11416)
- [AdamW的Weight RMS的...](https://kexue.fm/archives/11404)
- [n个正态随机数的最大值的渐近估计](https://kexue.fm/archives/11390)

## COMMENTS

- [Bin: 今天偶然从某个论坛看到有人推荐您的博客，定睛一看竟然是华师同院...](https://kexue.fm/archives/1990/comment-page-2#comment-29105)
- [Rapture D: 我有一个问题，为什么不考虑亥姆霍兹定理和斯托克斯公式。](https://kexue.fm/archives/11530/comment-page-1#comment-29104)
- [mofheka: 苏神是还在用jax是么？最近在做基于Google Pathwa...](https://kexue.fm/archives/11390/comment-page-1#comment-29103)
- [长琴: 看懂这篇博客也不是一件容易的事情。](https://kexue.fm/archives/11530/comment-page-1#comment-29102)
- [AlexLi: 苏老师，请教一下(7)式中将 μ(xt)μ(xt) 传给 $p...](https://kexue.fm/archives/9257/comment-page-4#comment-29101)
- [tyler\_zxc: "Performer的思想是将标准的Attention线性化，...](https://kexue.fm/archives/7921/comment-page-2#comment-29100)
- [我: 似乎并非mHC提出矩阵的思想？之前hyper connecti...](https://kexue.fm/archives/11494/comment-page-1#comment-29099)
- [winter: 苏神您好，假如对于比较均匀的attention weightP...](https://kexue.fm/archives/10847/comment-page-1#comment-29098)
- [苏剑林: KL散度、JS散度、W距离啥的，都行啊，看你喜欢哪个](https://kexue.fm/archives/8512/comment-page-2#comment-29097)
- [苏剑林: 没有绝对公平的对比方法，主要看你关心什么。比如，如果只关心推理...](https://kexue.fm/archives/9119/comment-page-14#comment-29096)

## USERLOGIN

- [登录](https://kexue.fm/admin/login.php)

[科学空间\|Scientific Spaces](https://kexue.fm/)

- [登录](https://kexue.fm/admin/login.php)
- [打赏](https://kexue.fm/reward.html)
- [公式](https://kexue.fm/latex.html)
- [天象](https://kexue.fm/ac.html)
- [链接](https://kexue.fm/links.html)
- [时光](https://kexue.fm/me.html)
- [博览](https://kexue.fm/science.html)
- [归档](https://kexue.fm/content.html)

渴望成为一个小飞侠

- [![](https://kexue.fm/usr/themes/geekg/images/rss.png)\\
\\
欢迎订阅](https://kexue.fm/feed)
- [![](https://kexue.fm/usr/themes/geekg/images/mail.png)\\
\\
个性邮箱](https://kexue.fm/archives/119)
- [![](https://kexue.fm/usr/themes/geekg/images/Saturn.png)\\
\\
天象信息](https://kexue.fm/ac.html)
- [![](https://kexue.fm/usr/themes/geekg/images/iss.png)\\
\\
观测ISS](https://kexue.fm/archives/41)
- [![](https://kexue.fm/usr/themes/geekg/images/pi.png)\\
\\
LaTeX](https://kexue.fm/latex.html)
- [![](https://kexue.fm/usr/themes/geekg/images/mlogo.png)\\
\\
关于博主](https://kexue.fm/me.html)

欢迎访问“科学空间”，这里将与您共同探讨自然科学，回味人生百态；也期待大家的分享～

- [**千奇百怪** Everything](https://kexue.fm/category/Everything)
- [**天文探索** Astronomy](https://kexue.fm/category/Astronomy)
- [**数学研究** Mathematics](https://kexue.fm/category/Mathematics)
- [**物理化学** Phy-chem](https://kexue.fm/category/Phy-chem)
- [**信息时代** Big-Data](https://kexue.fm/category/Big-Data)
- [**生物自然** Biology](https://kexue.fm/category/Biology)
- [**图片摄影** Photograph](https://kexue.fm/category/Photograph)
- [**问题百科** Questions](https://kexue.fm/category/Questions)
- [**生活/情感** Life-Feeling](https://kexue.fm/category/Life-Feeling)
- [**资源共享** Resources](https://kexue.fm/category/Resources)

- [**千奇百怪**](https://kexue.fm/category/Everything)
- [**天文探索**](https://kexue.fm/category/Astronomy)
- [**数学研究**](https://kexue.fm/category/Mathematics)
- [**物理化学**](https://kexue.fm/category/Phy-chem)
- [**信息时代**](https://kexue.fm/category/Big-Data)
- [**生物自然**](https://kexue.fm/category/Biology)
- [**图片摄影**](https://kexue.fm/category/Photograph)
- [**问题百科**](https://kexue.fm/category/Questions)
- [**生活/情感**](https://kexue.fm/category/Life-Feeling)
- [**资源共享**](https://kexue.fm/category/Resources)

[首页](https://kexue.fm/) [信息时代](https://kexue.fm/category/Big-Data) 更别致的词向量模型(二)：对语言进行建模

19Nov

# [更别致的词向量模型(二)：对语言进行建模](https://kexue.fm/archives/4669)

By 苏剑林 \|
2017-11-19 \|
68758位读者 \|

## 从条件概率到互信息 [\#](https://kexue.fm/archives/4669\#%E4%BB%8E%E6%9D%A1%E4%BB%B6%E6%A6%82%E7%8E%87%E5%88%B0%E4%BA%92%E4%BF%A1%E6%81%AF)

目前，词向量模型的原理基本都是词的上下文的分布可以揭示这个词的语义，就好比“看看你跟什么样的人交往，就知道你是什么样的人”，所以词向量模型的核心就是对上下文的关系进行建模。除了glove之外，几乎所有词向量模型都是在对条件概率P(w\|context)P(w\|context)进行建模，比如Word2Vec的skip gram模型就是对条件概率P(w2\|w1)P(w2\|w1)进行建模。但这个量其实是有些缺点的，首先它是不对称的，即P(w2\|w1)P(w2\|w1)不一定等于P(w1\|w2)P(w1\|w2)，这样我们在建模的时候，就要把上下文向量和目标向量区分开，它们不能在同一向量空间中；其次，它是有界的、归一化的量，这就意味着我们必须使用softmax等方法将它压缩归一，这造成了优化上的困难。

事实上，在NLP的世界里，有一个更加对称的量比单纯的P(w2\|w1)P(w2\|w1)更为重要，那就是

P(w1,w2)P(w1)P(w2)=P(w2\|w1)P(w2)(1)(1)P(w1,w2)P(w1)P(w2)=P(w2\|w1)P(w2)

这个量的大概意思是“两个词真实碰面的概率是它们随机相遇的概率的多少倍”，如果它远远大于1，那么表明它们倾向于共同出现而不是随机组合的，当然如果它远远小于1，那就意味着它们俩是刻意回避对方的。这个量在NLP界是举足轻重的，我们暂且称它为“相关度“，当然，它的对数值更加出名，大名为点互信息（Pointwise Mutual Information，PMI）：

PMI(w1,w2)=logP(w1,w2)P(w1)P(w2)(2)(2)PMI(w1,w2)=log⁡P(w1,w2)P(w1)P(w2)

有了上面的理论基础，我们认为，如果能直接对相关度进行建模，会比直接对条件概率P(w2\|w1)P(w2\|w1)建模更加合理，所以本文就围绕这个角度进行展开。在此之前，我们先进一步展示一下互信息本身的美妙性质。

## 互信息的可加性 [\#](https://kexue.fm/archives/4669\#%E4%BA%92%E4%BF%A1%E6%81%AF%E7%9A%84%E5%8F%AF%E5%8A%A0%E6%80%A7)

相关度（等价地，互信息）在朴素假设下，有着非常漂亮的分解性质。所谓朴素假设，就是指特征之间是相互独立的，这样我们就有P(a,b)=P(a)P(b)P(a,b)=P(a)P(b)，也就是将联合概率进行分解，从而简化模型。

比如，考虑两个量Q,AQ,A之间的互信息，Q,AQ,A不是单个特征，而是多个特征的组合：Q=(q1,…,qk),A=(a1,…,al)Q=(q1,…,qk),A=(a1,…,al)，现在考虑它们的相关度，即

P(Q,A)P(Q)P(A)==P(q1,…,qk;a1,…,al)P(q1,…,qk)P(a1,…,al)P(q1,…,qk\|a1,…,al)P(q1,…,qk)(3)(3)P(Q,A)P(Q)P(A)=P(q1,…,qk;a1,…,al)P(q1,…,qk)P(a1,…,al)=P(q1,…,qk\|a1,…,al)P(q1,…,qk)

用朴素假设就得到

P(q1,…,qk\|a1,…,al)P(q1,…,qk)=∏ki=1P(qi\|a1,…,al)∏ki=1P(qi)(4)(4)P(q1,…,qk\|a1,…,al)P(q1,…,qk)=∏i=1kP(qi\|a1,…,al)∏i=1kP(qi)

用贝叶斯公式，得到

∏ki=1P(qi\|a1,…,al)∏ki=1P(qi)==∏ki=1P(a1,…,al\|qi)P(qi)/P(a1,…,al)∏ki=1P(qi)∏i=1kP(a1,…,al\|qi)P(a1,…,al)(5)(5)∏i=1kP(qi\|a1,…,al)∏i=1kP(qi)=∏i=1kP(a1,…,al\|qi)P(qi)/P(a1,…,al)∏i=1kP(qi)=∏i=1kP(a1,…,al\|qi)P(a1,…,al)

再用一次朴素假设，得到

∏i=1kP(a1,…,al\|qi)P(a1,…,al)==∏i=1k∏lj=1P(aj\|qi)∏lj=1P(aj)∏i=1k∏j=1lP(qi,aj)P(qi)P(aj)(6)(6)∏i=1kP(a1,…,al\|qi)P(a1,…,al)=∏i=1k∏j=1lP(aj\|qi)∏j=1lP(aj)=∏i=1k∏j=1lP(qi,aj)P(qi)P(aj)

这表明，在朴素假设下，两个多元变量的相关度，等于它们两两单变量的相关度的乘积。如果两边取对数，那么结果就更加好看了，即

PMI(Q,A)=∑i=1k∑j=1lPMI(qi,aj)(7)(7)PMI(Q,A)=∑i=1k∑j=1lPMI(qi,aj)

也就是说，两个多元变量之间的互信息，等于两两单变量之间的互信息之和，换句话说，互信息是可加的！

## 插播：番外篇 [\#](https://kexue.fm/archives/4669\#%E6%8F%92%E6%92%AD%EF%BC%9A%E7%95%AA%E5%A4%96%E7%AF%87)

为了让大家更直观地理解词向量建模的原理，现在让我们想象自己是语言界的“月老”，我们的目的是测定任意两个词之间的“缘分”，为每个词寻找最佳的另一半铺路～

所谓“有缘千里来相会，无缘见面不相识”，对于每个词来说，最佳的另一半肯定都是它的“有缘词”。怎样的两个词才算是“有缘”呢？那自然是“你的眼里有我，我的眼里也有你”了。前面已经说了，skip gram模型关心的是条件概率P(w2\|w1)P(w2\|w1)，导致的结果是“w1w1的眼里有w2w2，w2w2的眼里却未必有w1w1”，也就是说，w2w2更多的是词语界的“花花公子”，如“的”、“了”这些停用词，它们跟谁都能混在一起，但未必对谁都真心。因此，为了“你中有我，我中有你”，就必须同时考虑P(w2\|w1)P(w2\|w1)和P(w1\|w2)P(w1\|w2)，或者考虑一个更加对称的量——也就是前面说的“相关度”了。所以“月老”决定用相关度来定量描述两个词之间的“缘分”值。

接下来，“月老”就开始工作了，开始逐一算词与词之间的“缘分”了。算着算着，他就发现严重的问题了。

首先，数目太多了，算不完。要知道词语界可是有数万甚至数十万、将来还可能是数百万的词语，如果两两的缘分都算一次并记录下来，那将要一个数十亿乃至数万亿的表格，而且这工作量也不少，也许月老下岗了也还不能把它们都算完，但从负责任的角度，我们不能忽略任意两个词在一起的可能性呀！

其次，词与词之间的N次邂逅，相对于漫漫历史长河，也不过是沧海一粟。两个词没有碰过面，真的就表明它们毫无缘分了吗？现在没有，可不代表将来没有。作为谨慎的月老，显然是不能这么武断下结论的。词与词之间的关系错综复杂，因此哪怕两个词没有碰过面，也不能一刀切，也得估算一下它们的缘分值。

_**转载到请包括本文地址：** [https://kexue.fm/archives/4669](https://kexue.fm/archives/4669 "更别致的词向量模型(二)：对语言进行建模")_

_**更详细的转载事宜请参考：**_ [《科学空间FAQ》](https://kexue.fm/archives/6508#%E6%96%87%E7%AB%A0%E5%A6%82%E4%BD%95%E8%BD%AC%E8%BD%BD/%E5%BC%95%E7%94%A8 "《科学空间FAQ》")

**如果您还有什么疑惑或建议，欢迎在下方评论区继续讨论。**

**如果您觉得本文还不错，欢迎 [分享](https://kexue.fm/archives/4669#share)/ [打赏](https://kexue.fm/archives/4669#pay) 本文。打赏并非要从中获得收益，而是希望知道科学空间获得了多少读者的真心关注。当然，如果你无视它，也不会影响你的阅读。再次表示欢迎和感谢！**

打赏

![科学空间](https://kexue.fm/usr/themes/geekg/payment/wx.png)

微信打赏

![科学空间](https://kexue.fm/usr/themes/geekg/payment/zfb.png)

支付宝打赏

因为网站后台对打赏并无记录，因此欢迎在打赏时候备注留言。

你还可以 [**点击这里**](http://mail.qq.com/cgi-bin/qm_share?t=qm_mailme&email=tN7d1drY3drrx8H0xcWa19vZ) 或在下方评论区留言来告知你的建议或需求。

**如果您需要引用本文，请参考：**

苏剑林. (Nov. 19, 2017). 《更别致的词向量模型(二)：对语言进行建模 》\[Blog post\]. Retrieved from [https://kexue.fm/archives/4669](https://kexue.fm/archives/4669)

@online{kexuefm-4669,

         title={更别致的词向量模型(二)：对语言进行建模},

         author={苏剑林},

         year={2017},

         month={Nov},

         url={\\url{https://kexue.fm/archives/4669}},

}


分类： [信息时代](https://kexue.fm/category/Big-Data)    标签： [词向量](https://kexue.fm/tag/%E8%AF%8D%E5%90%91%E9%87%8F/), [互信息](https://kexue.fm/tag/%E4%BA%92%E4%BF%A1%E6%81%AF/), [glove](https://kexue.fm/tag/glove/)[9 评论](https://kexue.fm/archives/4669#comments)

< [更别致的词向量模型(一)：simpler glove](https://kexue.fm/archives/4667 "更别致的词向量模型(一)：simpler glove") \| [更别致的词向量模型(三)：描述相关的模型](https://kexue.fm/archives/4671 "更别致的词向量模型(三)：描述相关的模型") >

### 你也许还对下面的内容感兴趣

- [关于维度公式“n > 8.33 log N”的可用性分析](https://kexue.fm/archives/8711 "关于维度公式“n > 8.33 log N”的可用性分析")
- [一个二值化词向量模型，是怎么跟果蝇搭上关系的？](https://kexue.fm/archives/8159 "一个二值化词向量模型，是怎么跟果蝇搭上关系的？")
- [最小熵原理（六）：词向量的维度应该怎么选择？](https://kexue.fm/archives/7695 " 最小熵原理（六）：词向量的维度应该怎么选择？")
- [通过互信息思想来缓解类别不平衡问题](https://kexue.fm/archives/7615 "通过互信息思想来缓解类别不平衡问题")
- [JoSE：球面上的词向量和句向量](https://kexue.fm/archives/7063 "JoSE：球面上的词向量和句向量")
- [HSIC简介：一个有意思的判断相关性的思路](https://kexue.fm/archives/6910 "HSIC简介：一个有意思的判断相关性的思路")
- [最小熵原理（四）：“物以类聚”之从图书馆到词向量](https://kexue.fm/archives/6191 "最小熵原理（四）：“物以类聚”之从图书馆到词向量")
- [从变分编码、信息瓶颈到正态分布：论遗忘的重要性](https://kexue.fm/archives/6181 "从变分编码、信息瓶颈到正态分布：论遗忘的重要性")
- [变分自编码器 = 最小化先验分布 \+ 最大化互信息](https://kexue.fm/archives/6088 "变分自编码器 = 最小化先验分布 + 最大化互信息")
- [深度学习的互信息：无监督提取特征](https://kexue.fm/archives/6024 "深度学习的互信息：无监督提取特征")

[发表你的看法](https://kexue.fm/archives/4669#comment_form)

freeopen

January 3rd, 2018

请问为什么有界的、归一化的量会造成优化上的困难，特别是归一化，在机器学习中不是最常用的做法么？

[回复评论](https://kexue.fm/archives/4669/comment-page-1?replyTo=8512#respond-post-4669)

[苏剑林](http://kexue.fm/) 发表于
January 4th, 2018

如果从激活函数的角度来看，有界意味着存在饱和区，因此会有可能梯度消失的问题。当然，这是一般的情况，在softmax中一般也不会关心它的饱和。

softmax主要的问题是，因为它的归一化的，所以我们必须要算归一化因子Z，而这个Z必须遍历所有词表来算的，所以理论上每步计算量都很大，因此只能通过各种近似方法（huffman softmax、负采样等）来算。

[回复评论](https://kexue.fm/archives/4669/comment-page-1?replyTo=8516#respond-post-4669)

VInnyHu

October 9th, 2019

想请问下苏神，为什么利用相关度进行建模会比条件概率建模更加合理呢？还是没有看懂

[回复评论](https://kexue.fm/archives/4669/comment-page-1?replyTo=12152#respond-post-4669)

[苏剑林](https://kexue.fm/) 发表于
October 9th, 2019

因为相关度能挖掘出真正有关联意义的组合，而当模型拟合能力有限时，应当让模型优先学习那些有本质相关性的组合。

[回复评论](https://kexue.fm/archives/4669/comment-page-1?replyTo=12158#respond-post-4669)

xxw

February 13th, 2021

先祝苏神新年快乐!

在阅读本文时有一处不解 : "其次，它是有界的、归一化的量，这就意味着我们必须使用softmax等方法将它压缩归一，这造成了优化上的困难" , 请问苏神 1.这里的优化具体是指的什么优化? 2.为什么有界的、归一化的量,使用softmax的方式归一化后会造成优化上的困难? 有界且可以归一化在我的"刻板印象"中一直是比较好的性质呀? 请苏神指点

[回复评论](https://kexue.fm/archives/4669/comment-page-1?replyTo=15552#respond-post-4669)

xxw 发表于
February 13th, 2021

尴尬了 发完才看到一楼已经有了类似的问题 苏神忽略这个问题~ 新年快乐!

[回复评论](https://kexue.fm/archives/4669/comment-page-1?replyTo=15553#respond-post-4669)

[苏剑林](https://kexue.fm/) 发表于
February 14th, 2021

好的

[回复评论](https://kexue.fm/archives/4669/comment-page-1?replyTo=15554#respond-post-4669)

[wa007](https://www.cnblogs.com/wa007)

August 21st, 2021

苏神，请问下，为什么skip gram 用到的条件概率 P(w1\|w2)P(w1\|w2) 是不对称的呢？ w2w2 可以作为 w1w1 的上下文，同样 w1w1 也可以作为 w2w2 的上下文，这不应该是对称的嘛？

[回复评论](https://kexue.fm/archives/4669/comment-page-1?replyTo=17167#respond-post-4669)

[苏剑林](https://kexue.fm/) 发表于
August 21st, 2021

本来就没有对称的理由，比如电影院是用来看电影的，所以P(电影\|电影院)P(电影\|电影院)会比较大，但是看电影不一定要去电影院，所以P(电影院\|电影)P(电影院\|电影)没那么大。

[回复评论](https://kexue.fm/archives/4669/comment-page-1?replyTo=17176#respond-post-4669)

[取消回复](https://kexue.fm/archives/4669#respond-post-4669)

你的大名

电子邮箱

个人网站（选填）

1\. 可以使用LaTeX代码，点击“预览效果”可查看效果；

2\. 可以通过点击评论楼层编号来引用该楼层；

3\. 网站可能会有点卡，如非确认评论失败，请 **不要重复点击提交**。

### 内容速览

[从条件概率到互信息](https://kexue.fm/archives/4669#%E4%BB%8E%E6%9D%A1%E4%BB%B6%E6%A6%82%E7%8E%87%E5%88%B0%E4%BA%92%E4%BF%A1%E6%81%AF)
[互信息的可加性](https://kexue.fm/archives/4669#%E4%BA%92%E4%BF%A1%E6%81%AF%E7%9A%84%E5%8F%AF%E5%8A%A0%E6%80%A7)
[插播：番外篇](https://kexue.fm/archives/4669#%E6%8F%92%E6%92%AD%EF%BC%9A%E7%95%AA%E5%A4%96%E7%AF%87)

### 智能搜索

支持整句搜索！网站自动使用 [结巴分词](https://github.com/fxsjy/jieba) 进行分词，并结合ngrams排序算法给出合理的搜索结果。

### 热门标签

[生成模型](https://kexue.fm/tag/%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B/) [attention](https://kexue.fm/tag/attention/) [优化](https://kexue.fm/tag/%E4%BC%98%E5%8C%96/) [语言模型](https://kexue.fm/tag/%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/) [模型](https://kexue.fm/tag/%E6%A8%A1%E5%9E%8B/) [梯度](https://kexue.fm/tag/%E6%A2%AF%E5%BA%A6/) [网站](https://kexue.fm/tag/%E7%BD%91%E7%AB%99/) [概率](https://kexue.fm/tag/%E6%A6%82%E7%8E%87/) [矩阵](https://kexue.fm/tag/%E7%9F%A9%E9%98%B5/) [优化器](https://kexue.fm/tag/%E4%BC%98%E5%8C%96%E5%99%A8/) [转载](https://kexue.fm/tag/%E8%BD%AC%E8%BD%BD/) [微分方程](https://kexue.fm/tag/%E5%BE%AE%E5%88%86%E6%96%B9%E7%A8%8B/) [分析](https://kexue.fm/tag/%E5%88%86%E6%9E%90/) [天象](https://kexue.fm/tag/%E5%A4%A9%E8%B1%A1/) [深度学习](https://kexue.fm/tag/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/) [积分](https://kexue.fm/tag/%E7%A7%AF%E5%88%86/) [python](https://kexue.fm/tag/python/) [扩散](https://kexue.fm/tag/%E6%89%A9%E6%95%A3/) [力学](https://kexue.fm/tag/%E5%8A%9B%E5%AD%A6/) [无监督](https://kexue.fm/tag/%E6%97%A0%E7%9B%91%E7%9D%A3/) [几何](https://kexue.fm/tag/%E5%87%A0%E4%BD%95/) [节日](https://kexue.fm/tag/%E8%8A%82%E6%97%A5/) [生活](https://kexue.fm/tag/%E7%94%9F%E6%B4%BB/) [文本生成](https://kexue.fm/tag/%E6%96%87%E6%9C%AC%E7%94%9F%E6%88%90/) [数论](https://kexue.fm/tag/%E6%95%B0%E8%AE%BA/)

### 随机文章

- [算符的艺术：差分、微分与伯努利数](https://kexue.fm/archives/3018)
- [均值不等式的两个巧妙证明](https://kexue.fm/archives/1716)
- [圆周率节快乐！\|\| 原来已经写了十年博客～](https://kexue.fm/archives/6469)
- [重提“旋转弹簧伸长”问题（变分解法）](https://kexue.fm/archives/1311)
- [角的疑惑——为什么使用弧度？](https://kexue.fm/archives/1868)
- [上学了，更新放缓......](https://kexue.fm/archives/125)
- [高三的第一周](https://kexue.fm/archives/1459)
- [2012北约自主招生数学](https://kexue.fm/archives/1551)
- [指数梯度下降 \+ 元学习 = 自适应学习率](https://kexue.fm/archives/8968)
- [月球上的多角度反射镜](https://kexue.fm/archives/904)

### 最近评论

- [Bin](https://kexue.fm/archives/1990/comment-page-2#comment-29105): 今天偶然从某个论坛看到有人推荐您的博客，定睛一看竟然是华师同院的往届师兄！看到这篇2013年的...
- [Rapture D](https://kexue.fm/archives/11530/comment-page-1#comment-29104): 我有一个问题，为什么不考虑亥姆霍兹定理和斯托克斯公式。
- [mofheka](https://kexue.fm/archives/11390/comment-page-1#comment-29103): 苏神是还在用jax是么？最近在做基于Google Pathway的理念做一个动态版的MPMD框...
- [长琴](https://kexue.fm/archives/11530/comment-page-1#comment-29102): 看懂这篇博客也不是一件容易的事情。
- [AlexLi](https://kexue.fm/archives/9257/comment-page-4#comment-29101): 苏老师，请教一下(7)式中将 μ(xt)μ(xt) 传给 popo 进行推理的操作。 $x\_...
- [tyler\_zxc](https://kexue.fm/archives/7921/comment-page-2#comment-29100): "Performer的思想是将标准的Attention线性化，所以为什么不干脆直接训练一个线性...
- [我](https://kexue.fm/archives/11494/comment-page-1#comment-29099): 似乎并非mHC提出矩阵的思想？之前hyper connection就是了
- [winter](https://kexue.fm/archives/10847/comment-page-1#comment-29098): 苏神您好，假如对于比较均匀的attention weightP，往往呈现long tail分布...
- [苏剑林](https://kexue.fm/archives/8512/comment-page-2#comment-29097): KL散度、JS散度、W距离啥的，都行啊，看你喜欢哪个
- [苏剑林](https://kexue.fm/archives/9119/comment-page-14#comment-29096): 没有绝对公平的对比方法，主要看你关心什么。比如，如果只关心推理成本和推理效果，那么有的方法可以...

### 友情链接

- [Cool Papers](https://papers.cool/)
- [数学研发](https://bbs.emath.ac.cn/)
- [Seatop](http://www.seatop.com.cn/)
- [Xiaoxia](https://xiaoxia.org/)
- [积分表-网络版](https://kexue.fm/sci/integral/index.html)
- [丝路博傲](http://blog.dvxj.com/)
- [数学之家](http://www.2math.cn/)
- [有趣天文奇观](http://interesting-sky.china-vo.org/)
- [TwistedW](http://www.twistedwg.com/)
- [godweiyang](https://godweiyang.com/)
- [AI柠檬](https://blog.ailemon.net/)
- [王登科-DK博客](https://greatdk.com/)
- [ESON](https://blog.eson.org/)
- [枫之羽](https://fzhiy.net/)
- [coding-zuo](https://coding-zuo.github.io/)
- [博科园](https://www.bokeyuan.net/)
- [孔皮皮的博客](https://www.kppkkp.top/)
- [运鹏的博客](https://yunpengtai.top/)
- [jiming.site](https://jiming.site/)
- [OmegaXYZ](https://www.omegaxyz.com/)
- [EAI猩球](https://www.robotech.ink/)
- [文举的博客](https://liwenju0.com/)
- [申请链接](https://kexue.fm/links.html)

[![署名-非商业用途-保持一致](https://kexue.fm/usr/themes/geekg/images/cc.gif)](http://creativecommons.org/licenses/by-nc-nd/2.5/cn/) 本站采用创作共用版权协议，要求署名、非商业用途和保持一致。转载本站内容必须也遵循“ [署名-非商业用途-保持一致](http://creativecommons.org/licenses/by-nc-nd/2.5/cn/)”的创作共用协议。



© 2009-2026 Scientific Spaces. All rights reserved. Theme by [laogui](http://www.laogui.com/). Powered by [Typecho](http://typecho.org/). 备案号: [粤ICP备09093259号-1/2](https://beian.miit.gov.cn/ "粤ICP备09093259号")。