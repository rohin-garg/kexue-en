## SEARCH

## MENU

- [打赏](https://kexue.fm/reward.html)
- [公式](https://kexue.fm/latex.html)
- [天象](https://kexue.fm/ac.html)
- [链接](https://kexue.fm/links.html)
- [时光](https://kexue.fm/me.html)
- [博览](https://kexue.fm/science.html)
- [归档](https://kexue.fm/content.html)

## CATEGORIES

- [千奇百怪](https://kexue.fm/category/Everything)
- [天文探索](https://kexue.fm/category/Astronomy)
- [数学研究](https://kexue.fm/category/Mathematics)
- [物理化学](https://kexue.fm/category/Phy-chem)
- [信息时代](https://kexue.fm/category/Big-Data)
- [生物自然](https://kexue.fm/category/Biology)
- [图片摄影](https://kexue.fm/category/Photograph)
- [问题百科](https://kexue.fm/category/Questions)
- [生活/情感](https://kexue.fm/category/Life-Feeling)
- [资源共享](https://kexue.fm/category/Resources)

## NEWPOSTS

- [线性注意力简史：从模仿、创新到反哺](https://kexue.fm/archives/11033)
- [msign的导数](https://kexue.fm/archives/11025)
- [通过msign来计算mclip（奇...](https://kexue.fm/archives/11006)
- [msign算子的Newton-Sc...](https://kexue.fm/archives/10996)
- [等值振荡定理：最优多项式逼近的充要条件](https://kexue.fm/archives/10972)
- [生成扩散模型漫谈（三十）：从瞬时速...](https://kexue.fm/archives/10958)
- [MoE环游记：5、均匀分布的反思](https://kexue.fm/archives/10945)
- [msign算子的Newton-Sc...](https://kexue.fm/archives/10922)
- [Transformer升级之路：2...](https://kexue.fm/archives/10907)
- [一道概率不等式：盯着它到显然成立为止！](https://kexue.fm/archives/10902)

## COMMENTS

- [石子131: 也许可以尝试把热水管的回水管的开关阀做成用户的手动阀，在热水管...](https://kexue.fm/archives/9405/comment-page-2#comment-27955)
- [Kuo: 我的理解，这是一个迭代过程，注意下标K是指condition还...](https://kexue.fm/archives/10795/comment-page-1#comment-27954)
- [musicfish1973: 好的设计都是相似的,haha](https://kexue.fm/archives/8009/comment-page-1#comment-27953)
- [忍者猫: 这优化器的作者真的应该给你打钱](https://kexue.fm/archives/10592/comment-page-2#comment-27952)
- [Chaofa Yuan: 写得太好了](https://kexue.fm/archives/11033/comment-page-1#comment-27951)
- [Skyler Lin: respect苏神！](https://kexue.fm/archives/11033/comment-page-1#comment-27949)
- [宋佳铭: 对，个人感觉mean flow就是continuous tim...](https://kexue.fm/archives/10958/comment-page-1#comment-27947)
- [宋佳铭: 的确，对sg这个事情我感觉如果是用‘归纳’法做是不太能避免的，...](https://kexue.fm/archives/10958/comment-page-1#comment-27946)
- [MoFHeka: 苏老师您好，请问一下这套结论在稀疏参数上应该如何应用？比如大规...](https://kexue.fm/archives/10542/comment-page-1#comment-27945)
- [苏剑林: Temp LoRA倒是有印象，其实思想是一样的，如果我单独开一...](https://kexue.fm/archives/11033/comment-page-1#comment-27944)

## USERLOGIN

- [登录](https://kexue.fm/admin/login.php)

[科学空间\|Scientific Spaces](https://kexue.fm)

- [登录](https://kexue.fm/admin/login.php)
- [打赏](https://kexue.fm/reward.html)
- [公式](https://kexue.fm/latex.html)
- [天象](https://kexue.fm/ac.html)
- [链接](https://kexue.fm/links.html)
- [时光](https://kexue.fm/me.html)
- [博览](https://kexue.fm/science.html)
- [归档](https://kexue.fm/content.html)

渴望成为一个小飞侠

- [欢迎订阅](https://kexue.fm/feed)
- [个性邮箱](https://kexue.fm/archives/119)
- [天象信息](https://kexue.fm/ac.html)
- [观测ISS](https://kexue.fm/archives/41)
- [LaTeX](https://kexue.fm/latex.html)
- [关于博主](https://kexue.fm/me.html)

欢迎访问“科学空间”，这里将与您共同探讨自然科学，回味人生百态；也期待大家的分享～

- [**千奇百怪** Everything](https://kexue.fm/category/Everything)
- [**天文探索** Astronomy](https://kexue.fm/category/Astronomy)
- [**数学研究** Mathematics](https://kexue.fm/category/Mathematics)
- [**物理化学** Phy-chem](https://kexue.fm/category/Phy-chem)
- [**信息时代** Big-Data](https://kexue.fm/category/Big-Data)
- [**生物自然** Biology](https://kexue.fm/category/Biology)
- [**图片摄影** Photograph](https://kexue.fm/category/Photograph)
- [**问题百科** Questions](https://kexue.fm/category/Questions)
- [**生活/情感** Life-Feeling](https://kexue.fm/category/Life-Feeling)
- [**资源共享** Resources](https://kexue.fm/category/Resources)

- [**千奇百怪**](https://kexue.fm/category/Everything)
- [**天文探索**](https://kexue.fm/category/Astronomy)
- [**数学研究**](https://kexue.fm/category/Mathematics)
- [**物理化学**](https://kexue.fm/category/Phy-chem)
- [**信息时代**](https://kexue.fm/category/Big-Data)
- [**生物自然**](https://kexue.fm/category/Biology)
- [**图片摄影**](https://kexue.fm/category/Photograph)
- [**问题百科**](https://kexue.fm/category/Questions)
- [**生活/情感**](https://kexue.fm/category/Life-Feeling)
- [**资源共享**](https://kexue.fm/category/Resources)

[首页](https://kexue.fm) [信息时代](https://kexue.fm/category/Big-Data) Ladder Side-Tuning：预训练模型的“过墙梯”

20Jun

# [Ladder Side-Tuning：预训练模型的“过墙梯”](https://kexue.fm/archives/9138)

By 苏剑林 \|
2022-06-20 \|
87385位读者\|

如果说大型的预训练模型是自然语言处理的“张良计”，那么对应的“过墙梯”是什么呢？笔者认为是高效地微调这些大模型到特定任务上的各种技巧。除了直接微调全部参数外，还有像 [Adapter](https://papers.cool/arxiv/1902.00751)、 [P-Tuning](https://kexue.fm/archives/8295) 等很多参数高效的微调技巧，它们能够通过只微调很少的参数来达到接近全量参数微调的效果。然而，这些技巧通常只是“参数高效”而并非“训练高效”，因为它们依旧需要在整个模型中反向传播来获得少部分可训练参数的梯度，说白了，就是可训练的参数确实是少了很多，但是训练速度并没有明显提升。

最近的一篇论文 [《LST: Ladder Side-Tuning for Parameter and Memory Efficient Transfer Learning》](https://papers.cool/arxiv/2206.06522) 则提出了一个新的名为“Ladder Side-Tuning（LST）”的训练技巧，它号称同时达到了参数高效和训练高效。是否真有这么理想的“过墙梯”？本来就让我们一起来学习一下。

## 方法大意 [\#](https://kexue.fm/archives/9138\#%E6%96%B9%E6%B3%95%E5%A4%A7%E6%84%8F)

其实LST这把“过墙梯”的结构，用原论文的Figure 2就可以清晰地说清楚了：

LST与Adaper、P-tuning的对比图

反向传播，也就是求模型梯度，是从输出层向输入层逐步计算的，因此反向传播的深度/计算量，取决于最靠近输入层的参数深度，跟可训练的参数量没有太必然的联系。对于Adapter来说，它在每一层后面都插入了一个小规模的层，虽然其余参数都固定了，只有新插入的层可训练，但每一层都新层，所以反向传播要传到输入层；对于P-tuning来说，本质上它是只有在Embedding层中有少量可训练参数，但Embedding层是输入层，因此它的反向传播也要贯穿整个模型。因此，这两种方案能提升的训练效率并不多。

至于LST，它是在原有大模型的基础上搭建了一个“旁支”（梯子），将大模型的部分层输出作为旁枝模型的输入，所有的训练参数尽在旁枝模型中，由于大模型仅提供输入，因此反向传播的复杂度取决于旁枝模型的规模，并不需要直接在原始大模型上执行反向传播，因此是可以明显提升训练效率的。

## 实验效果 [\#](https://kexue.fm/archives/9138\#%E5%AE%9E%E9%AA%8C%E6%95%88%E6%9E%9C)

原论文做了不少LST的实验，包括NLP、CV的，下面是LST在GLUE数据集上的效果：

LST在GLUE上的实验结果

可以看到，LST确实具备了参数高效和训练高效的特点，能够在较小的训练参数和训练成本的情况下，达到一个不错的微调效果。特别是最后两行的实验结果，体现出了LST在有限训练资源下微调大模型的可能性。

笔者在中文的CLUE任务上也做了简单尝试，参考代码为：

> **Github： [https://github.com/bojone/LST-CLUE](https://github.com/bojone/LST-CLUE)**

注意，原论文的“梯子”是用跟Adapter中的MLP层来搭建的，而笔者上述实现直接用了Transformer一样的“Attention + FFN”组合，可训练的参数量控制在100万左右，约为base版的1.2%，或者large版的0.4%，梯子的初始化直接用随机初始化，最终在验证集的效果如下：
$$\\small{\\begin{array}{c\|ccccccccccc}
\\hline
& \\text{iflytek} & \\text{tnews} & \\text{afqmc} & \\text{cmnli} & \\text{ocnli} & \\text{wsc} & \\text{csl} & \\text{cmrc2018} & \\text{c3} & \\text{chid} & \\text{cluener}\\\
\\hline
\\text{BERT base} & 60.06 & 56.80 & 72.41 & 79.56 & 73.93 & 78.62 & 83.93 & 56.17 & 60.54 & 85.69 & 79.45 \\\
\\text{RoBERTa base} & 60.64 & 58.06 & 74.05 & 81.24 & 76.00 & 87.50 & 84.50 & 56.54 & 67.66 & 86.71 & 79.47\\\
\\hline
\\text{RoBERTa base + LST} & 59.29 & 56.82 & 70.37 & 76.27 & 71.02 & 68.09 & 82.63 & 42.50 & 56.97 & 69.35 & 78.30\\\
\\text{RoBERTa large + LST} & 60.41 & 57.12 & 72.36 & 75.80 & 72.07 & 75.00 & 84.23 & 39.98 & 60.19 & 72.55 & 77.80\\\
\\hline
\\end{array}}$$

可以看到，实验结果没有原论文的英文实验那么乐观（当然不排除是笔者自己的实现不够好），但训练效率确实有明显提升（平均来说提升一倍左右）。整个实验下来，笔者的感觉是对于比较常规、一般难度的分类任务，LST能取得相近的效果，但对于比较困难的任务，比如阅读理解等，LST会有非常明显的下降。

当然，其实应该不只是LST有这个问题，大部分号称参数高效的微调方法估计都有这个问题，因为这些方法的实验任务多数都只是GLUE，GLUE其实全都是相对简单的分类任务...

## 延伸思考 [\#](https://kexue.fm/archives/9138\#%E5%BB%B6%E4%BC%B8%E6%80%9D%E8%80%83)

从“事后诸葛亮”来看，其实LST也算不上多高明的做法，本质上就是把预训练模型固定住，然后把其输出层和部分中间层的结果作为补充输入来训练一个新的小模型，理解到这一点之后，想必很多读者已经在脑海中酝酿着自己的相似方案了。不过，LST真正的意义在于告诉我们可以这样做，并且给出了一个可行的参考方案，以及实验证明了它确实是大模型的一个有效利用方案。

有类似研究经验的读者会发现，LST新增的“梯子”分支的初始化是个问题，如果完全随机初始化的话，可能会有训练上的困难，效果效果会不理想。这一点原论文也提到了，它提供了一个截取大模型矩阵权重来作为小模型矩阵初始化的方案，从而提升了LST的最终效果，其细节可以在论文中找到，至于笔记自己的实现，就纯粹是简单验证LST的有效性，所以就偷懒没实现这一步。

进一步想，既然LST新增的“梯子”分支存在初始化难题，而LST确实是微调大模型的有效方案，那么未来我们在训练新的大模型时，是不是就可以事先把这个“梯子”也预留好呢？也就是说，我们直接把这个“梯子”作为预训练模型的一部分做大规模的预训练，后面微调的时候，就只微调“梯子”，这样就可以实现高效地微调大模型，又不用担心初始化问题？

从形式上来看，笔者觉得LST跟 [《BERT-of-Theseus：基于模块替换的模型压缩方法》](https://kexue.fm/archives/7575) 中介绍的BERT-of-Theseus挺相似的，只不过一个目的是蒸馏小模型，还是需要用到大模型来反向传播；而LST目的则是提升训练效率，不需要大模型来反向传播，但推理时需要用到大模型来前向传播。可以说两者有点互补了。

## 文章小结 [\#](https://kexue.fm/archives/9138\#%E6%96%87%E7%AB%A0%E5%B0%8F%E7%BB%93)

本文主要介绍了同时具备参数高效和训练高效特点的一种大模型微调方法——Ladder Side-Tuning。

_**转载到请包括本文地址：** [https://kexue.fm/archives/9138](https://kexue.fm/archives/9138)_

_**更详细的转载事宜请参考：**_ [《科学空间FAQ》](https://kexue.fm/archives/6508#%E6%96%87%E7%AB%A0%E5%A6%82%E4%BD%95%E8%BD%AC%E8%BD%BD/%E5%BC%95%E7%94%A8)

**如果您还有什么疑惑或建议，欢迎在下方评论区继续讨论。**

**如果您觉得本文还不错，欢迎 [分享](https://kexue.fm/archives/9138#share)/ [打赏](https://kexue.fm/archives/9138#pay) 本文。打赏并非要从中获得收益，而是希望知道科学空间获得了多少读者的真心关注。当然，如果你无视它，也不会影响你的阅读。再次表示欢迎和感谢！**

打赏

微信打赏

支付宝打赏

因为网站后台对打赏并无记录，因此欢迎在打赏时候备注留言。你还可以 [**点击这里**](http://mail.qq.com/cgi-bin/qm_share?t=qm_mailme&email=tN7d1drY3drrx8H0xcWa19vZ) 或在下方评论区留言来告知你的建议或需求。

**如果您需要引用本文，请参考：**

苏剑林. (Jun. 20, 2022). 《Ladder Side-Tuning：预训练模型的“过墙梯” 》\[Blog post\]. Retrieved from [https://kexue.fm/archives/9138](https://kexue.fm/archives/9138)

@online{kexuefm-9138,
        title={Ladder Side-Tuning：预训练模型的“过墙梯”},
        author={苏剑林},
        year={2022},
        month={Jun},
        url={\\url{https://kexue.fm/archives/9138}},
}

分类： [信息时代](https://kexue.fm/category/Big-Data)    标签： [语言模型](https://kexue.fm/tag/%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/), [预训练](https://kexue.fm/tag/%E9%A2%84%E8%AE%AD%E7%BB%83/)[34 评论](https://kexue.fm/archives/9138#comments)

< [生成扩散模型漫谈（一）：DDPM = 拆楼 + 建楼](https://kexue.fm/archives/9119) \| [“维度灾难”之Hubness现象浅析](https://kexue.fm/archives/9147) >

### 你也许还对下面的内容感兴趣

- [Transformer升级之路：20、MLA究竟好在哪里？](https://kexue.fm/archives/10907)
- [Transformer升级之路：19、第二类旋转位置编码](https://kexue.fm/archives/10862)
- [Decoder-only的LLM为什么需要位置编码？](https://kexue.fm/archives/10347)
- [Monarch矩阵：计算高效的稀疏型矩阵分解](https://kexue.fm/archives/10249)
- [缓存与效果的极限拉扯：从MHA、MQA、GQA到MLA](https://kexue.fm/archives/10091)
- [时空之章：将Attention视为平方复杂度的RNN](https://kexue.fm/archives/10017)
- [我在Performer中发现了Transformer-VQ的踪迹](https://kexue.fm/archives/9862)
- [预训练一下，Transformer的长序列成绩还能涨不少！](https://kexue.fm/archives/9787)
- [脑洞大开：非线性RNN居然也可以并行计算？](https://kexue.fm/archives/9783)
- [大词表语言模型在续写任务上的一个问题及对策](https://kexue.fm/archives/9762)

[发表你的看法](https://kexue.fm/archives/9138#comment_form)

1. [«](https://kexue.fm/archives/9138/comment-page-1#comments)
2. [1](https://kexue.fm/archives/9138/comment-page-1#comments)
3. [2](https://kexue.fm/archives/9138/comment-page-2#comments)

[解密Prompt系列6. lora指令微调扣细节-请冷静,1个小时真不够~\_Python技术站](https://pythonjishu.com/yiuysxotjz/)

April 30th, 2023

\[...\]苏剑林. (Jun. 20, 2022). 《Ladder Side-Tuning：预训练模型的“过墙梯” 》\[Blog post\]. Retrieved from [https://kexue.fm/archives/9138](https://kexue.fm/archives/9138)\[...\]

[回复评论](https://kexue.fm/archives/9138/comment-page-2?replyTo=21502#respond-post-9138)

[解密Prompt系列6. lora指令微调扣细节-请冷静,1个小时真不够~ - CodeUUU](https://codeuuu.com/p/77374.html)

April 30th, 2023

\[...\]苏剑林. (Jun. 20, 2022). 《Ladder Side-Tuning：预训练模型的“过墙梯” 》\[Blog post\]. Retrieved from [https://kexue.fm/archives/9138](https://kexue.fm/archives/9138)\[...\]

[回复评论](https://kexue.fm/archives/9138/comment-page-2?replyTo=21504#respond-post-9138)

[解密Prompt系列6. lora指令微调扣细节-请冷静,1个小时真不够~ \| 呱唧呱唧网](http://www.itfaba.com/jishufenxian/99375.html)

May 1st, 2023

\[...\]苏剑林. (Jun. 20, 2022). 《Ladder Side-Tuning：预训练模型的“过墙梯” 》\[Blog post\]. Retrieved from [https://kexue.fm/archives/9138](https://kexue.fm/archives/9138)\[...\]

[回复评论](https://kexue.fm/archives/9138/comment-page-2?replyTo=21508#respond-post-9138)

安小东小尼

May 25th, 2023

好像被吞评论了
"""
def apply\_main\_layers(self, inputs, index):
x = super(LST\_BERT, self).apply\_main\_layers(inputs, index)
n = (index + 1) \* self.skip - 1
n = 'Expert-Transformer-%d-FeedForward-Norm' % n
y = self.expert\_model.get\_layer(n).output
y = keras.layers.Dense(self.hidden\_size, use\_bias=False)(y)
return keras.layers.Add()(\[x, y\])
"""
苏老师，请问一下您这里的实现好像和原文不一样，您是最后相加的，而原文是先相加再进入adapter里的，是我理解有误吗，请赐教！

[回复评论](https://kexue.fm/archives/9138/comment-page-2?replyTo=21759#respond-post-9138)

[苏剑林](https://kexue.fm) 发表于
May 25th, 2023

如果先相加，维度不同的话怎么加？

[回复评论](https://kexue.fm/archives/9138/comment-page-2?replyTo=21765#respond-post-9138)

csq

June 28th, 2023

苏神，请问下代码到底是如何实现反向传播不经过大模型的呀。 看了好久没弄明白。

[回复评论](https://kexue.fm/archives/9138/comment-page-2?replyTo=22083#respond-post-9138)

[苏剑林](https://kexue.fm) 发表于
June 28th, 2023

直接按原理实现前向模型，优化器求梯度时就不会经过大模型。

[回复评论](https://kexue.fm/archives/9138/comment-page-2?replyTo=22098#respond-post-9138)

ccccscqwdqw

July 6th, 2023

感谢苏神！！！我懂啦，模型就是正常流程实现，只是把要反向传播的参数传给优化器求梯度就行了

[回复评论](https://kexue.fm/archives/9138/comment-page-2?replyTo=22165#respond-post-9138)

Wison

September 14th, 2023

为什么prompt tuning的显存占用要高于fully finetuning？

[回复评论](https://kexue.fm/archives/9138/comment-page-2?replyTo=22726#respond-post-9138)

[苏剑林](https://kexue.fm) 发表于
September 19th, 2023

貌似没有？

[回复评论](https://kexue.fm/archives/9138/comment-page-2?replyTo=22745#respond-post-9138)

qer

October 4th, 2023

同问，根据他实验table1好像 prompt tuning 在戌年的时候显存占用很高，苏神您能分析下这是为什么吗

[回复评论](https://kexue.fm/archives/9138/comment-page-2?replyTo=22841#respond-post-9138)

[苏剑林](https://kexue.fm) 发表于
October 6th, 2023

full fine-tuning不是17.6GB吗？prompt tuning不是11.6GB吗？哪里更高？

[回复评论](https://kexue.fm/archives/9138/comment-page-2?replyTo=22852#respond-post-9138)

111

December 9th, 2023

他那个代码也太乱了，都不知道哪个项目是哪个项目，一堆sh文件

[回复评论](https://kexue.fm/archives/9138/comment-page-2?replyTo=23250#respond-post-9138)

1. [«](https://kexue.fm/archives/9138/comment-page-1#comments)
2. [1](https://kexue.fm/archives/9138/comment-page-1#comments)
3. [2](https://kexue.fm/archives/9138/comment-page-2#comments)

[取消回复](https://kexue.fm/archives/9138#respond-post-9138)

你的大名

电子邮箱

个人网站（选填）

1\. 可以使用LaTeX代码，点击“预览效果”可查看效果；2. 可以通过点击评论楼层编号来引用该楼层；3. 网站可能会有点卡，如非确认评论失败，请 **不要重复点击提交**。

### 内容速览

[方法大意](https://kexue.fm/archives/9138#%E6%96%B9%E6%B3%95%E5%A4%A7%E6%84%8F)
[实验效果](https://kexue.fm/archives/9138#%E5%AE%9E%E9%AA%8C%E6%95%88%E6%9E%9C)
[延伸思考](https://kexue.fm/archives/9138#%E5%BB%B6%E4%BC%B8%E6%80%9D%E8%80%83)
[文章小结](https://kexue.fm/archives/9138#%E6%96%87%E7%AB%A0%E5%B0%8F%E7%BB%93)

### 智能搜索

支持整句搜索！网站自动使用 [结巴分词](https://github.com/fxsjy/jieba) 进行分词，并结合ngrams排序算法给出合理的搜索结果。

### 热门标签

[生成模型](https://kexue.fm/tag/%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B/) [attention](https://kexue.fm/tag/attention/) [优化](https://kexue.fm/tag/%E4%BC%98%E5%8C%96/) [语言模型](https://kexue.fm/tag/%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/) [模型](https://kexue.fm/tag/%E6%A8%A1%E5%9E%8B/) [网站](https://kexue.fm/tag/%E7%BD%91%E7%AB%99/) [概率](https://kexue.fm/tag/%E6%A6%82%E7%8E%87/) [梯度](https://kexue.fm/tag/%E6%A2%AF%E5%BA%A6/) [转载](https://kexue.fm/tag/%E8%BD%AC%E8%BD%BD/) [微分方程](https://kexue.fm/tag/%E5%BE%AE%E5%88%86%E6%96%B9%E7%A8%8B/) [矩阵](https://kexue.fm/tag/%E7%9F%A9%E9%98%B5/) [天象](https://kexue.fm/tag/%E5%A4%A9%E8%B1%A1/) [分析](https://kexue.fm/tag/%E5%88%86%E6%9E%90/) [深度学习](https://kexue.fm/tag/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/) [积分](https://kexue.fm/tag/%E7%A7%AF%E5%88%86/) [python](https://kexue.fm/tag/python/) [优化器](https://kexue.fm/tag/%E4%BC%98%E5%8C%96%E5%99%A8/) [力学](https://kexue.fm/tag/%E5%8A%9B%E5%AD%A6/) [无监督](https://kexue.fm/tag/%E6%97%A0%E7%9B%91%E7%9D%A3/) [扩散](https://kexue.fm/tag/%E6%89%A9%E6%95%A3/) [几何](https://kexue.fm/tag/%E5%87%A0%E4%BD%95/) [节日](https://kexue.fm/tag/%E8%8A%82%E6%97%A5/) [生活](https://kexue.fm/tag/%E7%94%9F%E6%B4%BB/) [文本生成](https://kexue.fm/tag/%E6%96%87%E6%9C%AC%E7%94%9F%E6%88%90/) [数论](https://kexue.fm/tag/%E6%95%B0%E8%AE%BA/)

### 随机文章

- [正弦级数和余弦级数](https://kexue.fm/archives/3101)
- [高一新生活](https://kexue.fm/archives/110)
- [祝大家端午节快乐！](https://kexue.fm/archives/681)
- [Transformer升级之路：15、Key归一化助力长度外推](https://kexue.fm/archives/9859)
- [线性Transformer应该不是你要等的那个模型](https://kexue.fm/archives/8610)
- [【中文分词系列】 1\. 基于AC自动机的快速分词](https://kexue.fm/archives/3908)
- [用PyPy提高Python脚本执行效率](https://kexue.fm/archives/2621)
- [新浪sina.cn邮箱体验(免费邀请您来体验)](https://kexue.fm/archives/126)
- [费曼积分法——积分符号内取微分(4)](https://kexue.fm/archives/1637)
- [变分自编码器（七）：球面上的VAE（vMF-VAE）](https://kexue.fm/archives/8404)

### 最近评论

- [石子131](https://kexue.fm/archives/9405/comment-page-2#comment-27955): 也许可以尝试把热水管的回水管的开关阀做成用户的手动阀，在热水管临近回水管、手动阀靠近热水管侧加...
- [Kuo](https://kexue.fm/archives/10795/comment-page-1#comment-27954): 我的理解，这是一个迭代过程，注意下标K是指condition还是desideratum
- [musicfish1973](https://kexue.fm/archives/8009/comment-page-1#comment-27953): 好的设计都是相似的,haha
- [忍者猫](https://kexue.fm/archives/10592/comment-page-2#comment-27952): 这优化器的作者真的应该给你打钱
- [Chaofa Yuan](https://kexue.fm/archives/11033/comment-page-1#comment-27951): 写得太好了
- [Skyler Lin](https://kexue.fm/archives/11033/comment-page-1#comment-27949): respect苏神！
- [宋佳铭](https://kexue.fm/archives/10958/comment-page-1#comment-27947): 对，个人感觉mean flow就是continuous time CTM
- [宋佳铭](https://kexue.fm/archives/10958/comment-page-1#comment-27946): 的确，对sg这个事情我感觉如果是用‘归纳’法做是不太能避免的，因为毕竟是用步长短的模型去约束步...
- [MoFHeka](https://kexue.fm/archives/10542/comment-page-1#comment-27945): 苏老师您好，请问一下这套结论在稀疏参数上应该如何应用？比如大规模稀疏Embedding，每个B...
- [苏剑林](https://kexue.fm/archives/11033/comment-page-1#comment-27944): Temp LoRA倒是有印象，其实思想是一样的，如果我单独开一篇文章介绍TTT的话，应该会提到...

### 友情链接

- [Cool Papers](https://papers.cool)
- [数学研发](https://bbs.emath.ac.cn)
- [Seatop](http://www.seatop.com.cn/)
- [Xiaoxia](https://xiaoxia.org/)
- [积分表-网络版](https://kexue.fm/sci/integral/index.html)
- [丝路博傲](http://blog.dvxj.com/)
- [ph4ntasy 饭特稀](http://www.ph4ntasy.com/)
- [数学之家](http://www.2math.cn/)
- [有趣天文奇观](http://interesting-sky.china-vo.org/)
- [TwistedW](http://www.twistedwg.com/)
- [godweiyang](https://godweiyang.com/)
- [AI柠檬](https://blog.ailemon.net/)
- [王登科-DK博客](https://greatdk.com)
- [ESON](https://blog.eson.org/)
- [枫之羽](https://fzhiy.net/)
- [Mathor's blog](https://wmathor.com/)
- [coding-zuo](https://coding-zuo.github.io/)
- [博科园](https://www.bokeyuan.net/)
- [孔皮皮的博客](https://www.kppkkp.top/)
- [运鹏的博客](https://yunpengtai.top/)
- [jiming.site](https://jiming.site/)
- [OmegaXYZ](https://www.omegaxyz.com/)
- [Blog by Eacls](https://www.eacls.top/)
- [EAI猩球](https://www.robotech.ink/)
- [文举的博客](https://liwenju0.com/)
- [用代码打点酱油](https://bruceyuan.com/)
- [申请链接](https://kexue.fm/links.html)

本站采用创作共用版权协议，要求署名、非商业用途和保持一致。转载本站内容必须也遵循“ [署名-非商业用途-保持一致](http://creativecommons.org/licenses/by-nc-nd/2.5/cn/)”的创作共用协议。
© 2009-2025 Scientific Spaces. All rights reserved. Theme by [laogui](http://www.laogui.com). Powered by [Typecho](http://typecho.org). 备案号: [粤ICP备09093259号-1/2](https://beian.miit.gov.cn/)。