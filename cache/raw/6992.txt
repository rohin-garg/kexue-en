## SEARCH

## MENU

- [打赏](https://kexue.fm/reward.html)
- [公式](https://kexue.fm/latex.html)
- [天象](https://kexue.fm/ac.html)
- [链接](https://kexue.fm/links.html)
- [时光](https://kexue.fm/me.html)
- [博览](https://kexue.fm/science.html)
- [归档](https://kexue.fm/content.html)

## CATEGORIES

- [千奇百怪](https://kexue.fm/category/Everything)
- [天文探索](https://kexue.fm/category/Astronomy)
- [数学研究](https://kexue.fm/category/Mathematics)
- [物理化学](https://kexue.fm/category/Phy-chem)
- [信息时代](https://kexue.fm/category/Big-Data)
- [生物自然](https://kexue.fm/category/Biology)
- [图片摄影](https://kexue.fm/category/Photograph)
- [问题百科](https://kexue.fm/category/Questions)
- [生活/情感](https://kexue.fm/category/Life-Feeling)
- [资源共享](https://kexue.fm/category/Resources)

## NEWPOSTS

- [msign的导数](https://kexue.fm/archives/11025)
- [通过msign来计算mclip（奇...](https://kexue.fm/archives/11006)
- [msign算子的Newton-Sc...](https://kexue.fm/archives/10996)
- [等值振荡定理：最优多项式逼近的充要条件](https://kexue.fm/archives/10972)
- [生成扩散模型漫谈（三十）：从瞬时速...](https://kexue.fm/archives/10958)
- [MoE环游记：5、均匀分布的反思](https://kexue.fm/archives/10945)
- [msign算子的Newton-Sc...](https://kexue.fm/archives/10922)
- [Transformer升级之路：2...](https://kexue.fm/archives/10907)
- [一道概率不等式：盯着它到显然成立为止！](https://kexue.fm/archives/10902)
- [SVD的导数](https://kexue.fm/archives/10878)

## COMMENTS

- [rpsun: 老师您好，最近在自己的任务上尝试了muon，甚至只修改了学习率...](https://kexue.fm/archives/10592/comment-page-2#comment-27912)
- [盏一: 我之前做的笔记:Q: 公式 (14) 的理解.A: 首先基于 ...](https://kexue.fm/archives/8265/comment-page-8#comment-27911)
- [盏一: 哦哦哦 你是说 $\\exp nB$ 是正交矩阵! 并不是说 B.](https://kexue.fm/archives/8397/comment-page-3#comment-27910)
- [盏一: 呃, 是我脑子乱了... 忘了 $\\exp(0) = I$. ...](https://kexue.fm/archives/8397/comment-page-3#comment-27908)
- [盏一: 苏神, 请教一下\> 并且还可以证明它一定是正交矩阵是怎么证明的...](https://kexue.fm/archives/8397/comment-page-3#comment-27907)
- [sk: 请问公式14是怎么得出来的？](https://kexue.fm/archives/8265/comment-page-8#comment-27906)
- [tll1945tll1937: 真心实意的向大家请教问题：看了文章“对齐全量微调！这是我看过最...](https://kexue.fm/archives/10266/comment-page-1#comment-27901)
- [oYo\_logan: \[comment=27017\]苏剑林\[/comment\]苏神，...](https://kexue.fm/archives/10757/comment-page-1#comment-27897)
- [z123: 在参数矩阵较多的CNN小模型上，Muon会明显慢于Adam，这...](https://kexue.fm/archives/10592/comment-page-1#comment-27896)
- [dry: 苏神好，一直有个疑问，ReFlow构建的ODE是$dx\_t/d...](https://kexue.fm/archives/10958/comment-page-2#comment-27895)

## USERLOGIN

- [登录](https://kexue.fm/admin/login.php)

[科学空间\|Scientific Spaces](https://kexue.fm)

- [登录](https://kexue.fm/admin/login.php)
- [打赏](https://kexue.fm/reward.html)
- [公式](https://kexue.fm/latex.html)
- [天象](https://kexue.fm/ac.html)
- [链接](https://kexue.fm/links.html)
- [时光](https://kexue.fm/me.html)
- [博览](https://kexue.fm/science.html)
- [归档](https://kexue.fm/content.html)

渴望成为一个小飞侠

- [欢迎订阅](https://kexue.fm/feed)
- [个性邮箱](https://kexue.fm/archives/119)
- [天象信息](https://kexue.fm/ac.html)
- [观测ISS](https://kexue.fm/archives/41)
- [LaTeX](https://kexue.fm/latex.html)
- [关于博主](https://kexue.fm/me.html)

欢迎访问“科学空间”，这里将与您共同探讨自然科学，回味人生百态；也期待大家的分享～

- [**千奇百怪** Everything](https://kexue.fm/category/Everything)
- [**天文探索** Astronomy](https://kexue.fm/category/Astronomy)
- [**数学研究** Mathematics](https://kexue.fm/category/Mathematics)
- [**物理化学** Phy-chem](https://kexue.fm/category/Phy-chem)
- [**信息时代** Big-Data](https://kexue.fm/category/Big-Data)
- [**生物自然** Biology](https://kexue.fm/category/Biology)
- [**图片摄影** Photograph](https://kexue.fm/category/Photograph)
- [**问题百科** Questions](https://kexue.fm/category/Questions)
- [**生活/情感** Life-Feeling](https://kexue.fm/category/Life-Feeling)
- [**资源共享** Resources](https://kexue.fm/category/Resources)

- [**千奇百怪**](https://kexue.fm/category/Everything)
- [**天文探索**](https://kexue.fm/category/Astronomy)
- [**数学研究**](https://kexue.fm/category/Mathematics)
- [**物理化学**](https://kexue.fm/category/Phy-chem)
- [**信息时代**](https://kexue.fm/category/Big-Data)
- [**生物自然**](https://kexue.fm/category/Biology)
- [**图片摄影**](https://kexue.fm/category/Photograph)
- [**问题百科**](https://kexue.fm/category/Questions)
- [**生活/情感**](https://kexue.fm/category/Life-Feeling)
- [**资源共享**](https://kexue.fm/category/Resources)

[首页](https://kexue.fm) [信息时代](https://kexue.fm/category/Big-Data) BN究竟起了什么作用？一个闭门造车的分析

11Oct

# [BN究竟起了什么作用？一个闭门造车的分析](https://kexue.fm/archives/6992)

By 苏剑林 \|
2019-10-11 \|
144721位读者\|

BN，也就是 [Batch Normalization](https://papers.cool/arxiv/1502.03167)，是当前深度学习模型（尤其是视觉相关模型）的一个相当重要的技巧，它能加速训练，甚至有一定的抗过拟合作用，还允许我们用更大的学习率，总的来说颇多好处（前提是你跑得起较大的batch size）。

那BN究竟是怎么起作用呢？早期的解释主要是基于概率分布的，大概意思是将每一层的输入分布都归一化到$\\mathcal{N}(0,1)$上，减少了所谓的Internal Covariate Shift，从而稳定乃至加速了训练。这种解释看上去没什么毛病，但细思之下其实有问题的：不管哪一层的输入都不可能严格满足正态分布，从而单纯地将均值方差标准化无法实现标准分布$\\mathcal{N}(0,1)$；其次，就算能做到$\\mathcal{N}(0,1)$，这种诠释也无法进一步解释其他归一化手段（如Instance Normalization、Layer Normalization）起作用的原因。

在去年的论文 [《How Does Batch Normalization Help Optimization?》](https://papers.cool/arxiv/1805.11604) 里边，作者明确地提出了上述质疑，否定了原来的一些观点，并提出了自己关于BN的新理解： **他们认为BN主要作用是使得整个损失函数的landscape更为平滑，从而使得我们可以更平稳地进行训练。**

本博文主要也是分享这篇论文的结论，但论述方法是笔者“闭门造车”地构思的。窃认为原论文的论述过于晦涩了，尤其是数学部分太不好理解，所以本文试图尽可能直观地表达同样观点。

**（注：阅读本文之前，请确保你已经清楚知道BN是什么，本文不再重复介绍BN的概念和流程。）**

## 一些基础结论 [\#](https://kexue.fm/archives/6992\#%E4%B8%80%E4%BA%9B%E5%9F%BA%E7%A1%80%E7%BB%93%E8%AE%BA)

在这部分内容中我们先给出一个核心的不等式，继而推导梯度下降，并得到一些关于模型训练的基本结论，为后面BN的分析铺垫。

### 核心不等式 [\#](https://kexue.fm/archives/6992\#%E6%A0%B8%E5%BF%83%E4%B8%8D%E7%AD%89%E5%BC%8F)

假设函数$f(\\theta)$的梯度满足Lipschitz约束（$L$约束），即存在常数$L$使得下述恒成立
\\begin{equation}\\Vert \\nabla\_{\\theta} f(\\theta + \\Delta \\theta) - \\nabla\_{\\theta} f(\\theta)\\Vert\_2\\leq L\\Vert \\Delta\\theta\\Vert\_2\\end{equation}
那么我们有如下不等式
\\begin{equation}f(\\theta+\\Delta\\theta) \\leq f(\\theta) + \\left\\langle \\nabla\_{\\theta}f(\\theta), \\Delta\\theta\\right\\rangle + \\frac{1}{2}L \\Vert \\Delta\\theta\\Vert\_2^2\\label{eq:core-eq}\\end{equation}

> 证明并不难，定义辅助函数$f(\\theta + t\\Delta\\theta),t\\in\[0, 1\]$，然后直接得到\\begin{equation}\\begin{aligned}f(\\theta + \\Delta\\theta) - f(\\theta)=&\\int\_0^1\\frac{\\partial f(\\theta + t\\Delta\\theta)}{\\partial t} dt\\\
> =&\\int\_0^1\\left\\langle\\nabla\_{\\theta} f(\\theta + t\\Delta\\theta), \\Delta\\theta\\right\\rangle dt\\\
> =&\\left\\langle\\nabla\_{\\theta} f(\\theta), \\Delta\\theta\\right\\rangle + \\int\_0^1\\left\\langle\\nabla\_{\\theta} f(\\theta + t\\Delta\\theta) - \\nabla\_{\\theta} f(\\theta), \\Delta\\theta\\right\\rangle dt\\\
> \\leq&\\left\\langle\\nabla\_{\\theta} f(\\theta), \\Delta\\theta\\right\\rangle + \\int\_0^1\\Vert\\nabla\_{\\theta} f(\\theta + t\\Delta\\theta) - \\nabla\_{\\theta} f(\\theta)\\Vert\_2 \\cdot \\Vert \\Delta\\theta\\Vert\_2 dt\\\
> \\leq&\\left\\langle\\nabla\_{\\theta} f(\\theta), \\Delta\\theta\\right\\rangle + \\int\_0^1 L \\Vert \\Delta\\theta\\Vert\_2^2 t dt\\\
> = &\\left\\langle\\nabla\_{\\theta} f(\\theta), \\Delta\\theta\\right\\rangle + \\frac{1}{2} L \\Vert \\Delta\\theta\\Vert\_2^2
> \\end{aligned}\\end{equation}

### 梯度下降 [\#](https://kexue.fm/archives/6992\#%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D)

假设$f(\\theta)$是损失函数，而我们的目标是最小化$f(\\theta)$，那么这个不等式告诉我们很多信息。首先，既然是最小化，自然是希望每一步都在下降，即$f(\\theta+\\Delta\\theta) < f(\\theta) $，而$\\frac{1}{2}L \\Vert \\Delta\\theta\\Vert\_2^2$必然是非负的，所以要想下降的唯一选择就是$\\left\\langle \\nabla\_{\\theta}f(\\theta), \\Delta\\theta\\right\\rangle < 0$，这样一个自然的选择就是
\\begin{equation}\\Delta\\theta = -\\eta \\nabla\_{\\theta}f(\\theta)\\label{eq:gd}\\end{equation}
这里$\\eta > 0$是一个标量，即学习率。

可以发现，式$\\eqref{eq:gd}$就是梯度下降的更新公式，所以这也就是关于梯度下降的一种推导了，而且这个推导过程所包含的信息量更为丰富，因为它是一个严格的不等式，所以它还可以告诉我们关于训练的一些结论。

### Lipschitz约束 [\#](https://kexue.fm/archives/6992\#Lipschitz%E7%BA%A6%E6%9D%9F)

将梯度下降公式代入到不等式$\\eqref{eq:core-eq}$，我们得到
\\begin{equation}f(\\theta+\\Delta\\theta) \\leq f(\\theta) + \\left(\\frac{1}{2}L\\eta^2 - \\eta\\right) \\Vert \\nabla\_{\\theta}f(\\theta)\\Vert\_2^2\\end{equation}
注意到，保证损失函数下降的一个充分条件是$\\frac{1}{2}L\\eta^2 - \\eta < 0$，为了做到这一点，要不就要$\\eta$足够小，要不就要$L$足够小。但是$\\eta$足够小意味着学习速度会相当慢，所以更理想的情况是$L$能足够小，降低了$L$就可以用更大的学习率了，能加快学习速度，这也是它的好处之一。

但$L$是$f(\\theta)$的内在属性，因此只能通过调整$f$本身来降低$L$。

## BN是怎样炼成的 [\#](https://kexue.fm/archives/6992\#BN%E6%98%AF%E6%80%8E%E6%A0%B7%E7%82%BC%E6%88%90%E7%9A%84)

本节将会表明：以降低神经网络的梯度的$L$常数为目的，可以很自然地导出BN。也就是说，BN降低了神经网络的梯度的$L$常数，从而使得神经网络的学习更加容易，比如可以使用更大的学习率。而降低梯度的$L$常数，直观来看就是让损失函数没那么“跌宕起伏”，也就是使得landscape更光滑的意思了。

> **注：** 我们之前就讨论过$L$约束，之前我们讨论的是神经网络关于“输入”满足$L$约束，这导致了权重的谱正则和谱归一化（请参考 [《深度学习中的Lipschitz约束：泛化与生成模型》](https://kexue.fm/archives/6051)），本文则是要讨论神经网络（的梯度）关于“参数”满足$L$约束，这导致了对输入的各种归一化手段，而BN是其中最自然的一种。

### 梯度分析 [\#](https://kexue.fm/archives/6992\#%E6%A2%AF%E5%BA%A6%E5%88%86%E6%9E%90)

以监督学习为例，假设神经网络表示为$\\hat{y}=h(x;\\theta)$，损失函数取$l(y,\\hat{y})$，那么我们要做的事情是
\\begin{equation}\\theta = \\mathop{\\text{argmin}}\_{\\theta}\\, \\mathbb{E}\_{(x,y)\\sim p(x,y)}\\left\[l(y, h(x;\\theta))\\right\]\\end{equation}
也就是$f(\\theta)=\\mathbb{E}\_{(x,y)\\sim p(x,y)}\\left\[l(y, h(x;\\theta))\\right\]$，所以
\\begin{equation}\\begin{aligned}\\nabla\_{\\theta}f(\\theta)=&\\mathbb{E}\_{(x,y)\\sim p(x,y)}\\left\[\\nabla\_{\\theta}l(y, h(x;\\theta))\\right\]\\\
=&\\mathbb{E}\_{(x,y)\\sim p(x,y)}\\left\[\\nabla\_{h}l(y, h(x;\\theta))\\nabla\_{\\theta}h(x;\\theta)\\right\]\\end{aligned}\\end{equation}

顺便说明一下，本文的每个记号均没有加粗，但是根据实际情况不同它既有可能表示标量，也有可能表示向量。

### 非线性假设 [\#](https://kexue.fm/archives/6992\#%E9%9D%9E%E7%BA%BF%E6%80%A7%E5%81%87%E8%AE%BE)

显然，$f(\\theta)$是一个非线性函数，它的非线性来源有两个：

> 1、损失函数$l(y,\\hat{y})$一般是非线性的；
>
> 2、神经网络$h(x;\\theta)$中的激活函数是非线性的。

关于激活函数，当前主流的激活函数基本上都满足一个特性：导数的绝对值不超过某个常数。我们现在来考虑这个特性能否推广到损失函数中去，即（在整个训练过程中）损失函数的梯度$\\nabla\_{h}l(y, h(x;\\theta))$是否会被局限在某个范围内？

看上去，这个假设通常都是不成立的，比如交叉熵是$-\\log p$，而它的导数是$-1/p$，显然不可能被约束在某个有限范围。但是，损失函数联通最后一层的激活函数一起考虑时，则通常是满足这个约束的。比如二分类是最后一层通常用sigmoid激活，这时候配合交叉熵就是
\\begin{equation}-\\log \\text{sigmoid}(h(x;\\theta)) = \\log \\left(1 + e^{-h(x;\\theta)}\\right)\\end{equation}
这时候它关于$h$的的梯度在-1到1之间。当然，确实存在一些情况是不成立的，比如回归问题通常用mse做损失函数，并且最后一层通常不加激活函数，这时候它的梯度是一个线性函数，不会局限在一个有限范围内。这种情况下，我们只能寄望于模型有良好的初始化以及良好的优化器，使得在整个训练过程中$\\nabla\_{h}l(y, h(x;\\theta))$都比较稳定了。这个“寄望”看似比较强，但其实能训练成功的神经网络基本上都满足这个“寄望”。

### 柯西不等式 [\#](https://kexue.fm/archives/6992\#%E6%9F%AF%E8%A5%BF%E4%B8%8D%E7%AD%89%E5%BC%8F)

我们的目的是探讨$\\nabla\_{\\theta}f(\\theta)$满足$L$约束的程度，并且探讨降低这个$L$的方法。为此，我们先考虑最简单的单层神经网络（输入向量，输出标量）$h(x;w,b)=g\\left(\\left\\langle x, w\\right\\rangle + b\\right)$，这里的$g$是激活函数。这时候
\\begin{equation}\\begin{aligned}\\mathbb{E}\_{(x,y)\\sim p(x,y)}\\left\[\\nabla\_{b}f(w,b)\\right\]&=\\mathbb{E}\_{(x,y)\\sim p(x,y)}\\left\[\\frac{\\partial l\_{w,b}}{\\partial g}\\dot{g}\\left(\\left\\langle x, w\\right\\rangle + b\\right)\\right\]\\\
\\mathbb{E}\_{(x,y)\\sim p(x,y)}\\left\[\\nabla\_{w}f(w,b)\\right\]&=\\mathbb{E}\_{(x,y)\\sim p(x,y)}\\left\[\\frac{\\partial l\_{w,b}}{\\partial g}\\dot{g}\\left(\\left\\langle x, w\\right\\rangle + b\\right)x\\right\]
\\end{aligned}\\label{eq:grads}\\end{equation}

基于我们的假设，$\\frac{\\partial l\_{w,b}}{\\partial g}$和$\\dot{g}\\left(\\left\\langle x, w\\right\\rangle + b\\right)$都被限制在某个范围之内，所以可以看到偏置项$b$的梯度是很平稳的，它的更新也应当会是很平稳的。但是$w$的梯度不一样，它跟输入$x$直接相关。

关于$w$的梯度差，我们有
\\begin{equation}\\begin{aligned}
&\\big\\Vert\\mathbb{E}\_{(x,y)\\sim p(x,y)}\\left\[\\nabla\_{w}f(w+\\Delta w,b)\\right\] - \\mathbb{E}\_{(x,y)\\sim p(x,y)}\\left\[\\nabla\_{w}f(w,b)\\right\]\\big\\Vert\_2\\\
=&\\Bigg\\Vert\\mathbb{E}\_{(x,y)\\sim p(x,y)}\\left\[\\left(\\frac{\\partial l\_{w+\\Delta w,b}}{\\partial g}\\dot{g}\\left(\\left\\langle x, w+\\Delta w\\right\\rangle + b\\right) - \\frac{\\partial l\_{w,b}}{\\partial g}\\dot{g}\\left(\\left\\langle x, w\\right\\rangle + b\\right)\\right)x\\right\]\\Bigg\\Vert\_2
\\end{aligned}\\end{equation}

将圆括号部分记为$\\lambda(x, y; w,b,\\Delta w)$，根据前面的讨论，它被约束在某个范围之内，这部分依然是平稳项，既然如此，我们不妨假设它天然满足$L$约束，即
\\begin{equation}\\Vert\\lambda(x, y; w,b,\\Delta w)\\Vert\_2=\\mathcal{O}\\left(\\Vert\\Delta w\\Vert\_2\\right)\\end{equation}
这时候我们只需要关心好额外的$x$。根据柯西不等式，我们有
\\begin{equation}\\begin{aligned}&\\Big\\Vert\\mathbb{E}\_{(x,y)\\sim p(x,y)}\\left\[\\lambda(x, y; w,b,\\Delta w) x\\right\]\\Big\\Vert\_2\\\
\\leq & \\sqrt{\\mathbb{E}\_{(x,y)\\sim p(x,y)}\\left\[\\lambda(x, y; w,b,\\Delta w)^2\\right\]}\\times \\sqrt{\\big\|\\mathbb{E}\_{x\\sim p(x)}\\left\[x\\otimes x\\right\]\\big\|\_1}
\\end{aligned}\\label{eq:kexi}\\end{equation}

这样一来，我们得到了与（当前层）参数无关的$\\big\|\\mathbb{E}\_{x\\sim p(x)}\\left\[x\\otimes x\\right\]\\big\|\_1$，如果我们希望降低$L$常数，最直接的方法是降低这一项。

### 减均值除标准差 [\#](https://kexue.fm/archives/6992\#%E5%87%8F%E5%9D%87%E5%80%BC%E9%99%A4%E6%A0%87%E5%87%86%E5%B7%AE)

要注意，虽然我们很希望降低梯度的$L$常数，但这是有前提的——必须在不会明显降低原来神经网络拟合能力的前提下，否则只需要简单乘个0就可以让$L$降低到0了，但这并没有意义。

式$\\eqref{eq:kexi}$的结果告诉我们，想办法降低$\\big\|\\mathbb{E}\_{x\\sim p(x)}\\left\[x\\otimes x\\right\]\\big\|\_1$是个直接的做法，这意味着我们要对输入$x$进行变换。然后根据刚才的“不降低拟合能力”的前提，最简单并且可能有效的方法就是平移变换了，即我们考虑$x \\to x - \\mu$，换言之，考虑适当的$\\mu$使得
\\begin{equation}\\big\|\\mathbb{E}\_{x\\sim p(x)}\\left\[(x-\\mu)\\otimes (x-\\mu)\\right\]\\big\|\_1\\end{equation}
最小化。这只不过是一个二次函数的最小值问题，不难解得最优的$\\mu$是
\\begin{equation}\\mu = \\mathbb{E}\_{x\\sim p(x)}\\left\[x\\right\]\\label{eq:mu}\\end{equation}
这正好是所有样本的均值。于是，我们得到：

> **结论1：** 将输入减去所有样本的均值，能降低梯度的$L$常数，是一个有利于优化又不降低神经网络拟合能力的操作。

接着，我们考虑缩放变换，即$x - \\mu \\to \\frac{x - \\mu}{\\sigma}$，这里的$\\sigma$是一个跟$x$大小一样的向量，而除法则是逐位相除。这导致
\\begin{equation}\\big\|\\mathbb{E}\_{x\\sim p(x)}\\left\[(x-\\mu)\\otimes (x-\\mu)\\right\]\\big\|\_1 \\to \\left\|\\frac{\\mathbb{E}\_{x\\sim p(x)}\\left\[(x-\\mu)\\otimes (x-\\mu)\\right\]}{\\sigma\\otimes \\sigma}\\right\|\_1\\end{equation}

$\\sigma$是对$L$的一个最直接的缩放因子，但问题是缩放到哪里比较好？如果一味追求更小的$L$，那直接$\\sigma\\to \\infty$就好了，但这样的神经网络已经完全没有拟合能力了；但如果$\\sigma$太小导致$L$过大，那又不利于优化。所以我们需要一个标准。

以什么为标准好呢？再次回去看梯度的表达式$\\eqref{eq:grads}$，前面已经说了，偏置项的梯度不会被$x$明显地影响，所以它似乎会是一个靠谱的标准。如果是这样的话，那相当于将输入$x$的这一项权重直接缩放为1，那也就是说，$\\frac{\\mathbb{E}\_{x\\sim p(x)}\\left\[(x-\\mu)\\otimes (x-\\mu)\\right\]}{\\sigma\\otimes \\sigma}$变成了一个全1向量，再换言之：
\\begin{equation}\\sigma = \\sqrt{\\mathbb{E}\_{x\\sim p(x)}\\left\[(x-\\mu)\\otimes (x-\\mu)\\right\]}\\label{eq:sigma}\\end{equation}

这样一来，一个相对自然的选择是将$\\sigma$取为输入的标准差。这时候，我们能感觉到除以标准差这一项，更像是一个自适应的学习率校正项，它一定程度上消除了不同层级的输入对参数优化的差异性，使得整个网络的优化更为“同步”，或者说使得神经网络的每一层更为“平权”，从而更充分地利用好了整个神经网络，减少了在某一层过拟合的可能性。当然，如果输入的量级过大时，除以标准差这一项也有助于降低梯度的$L$常数。

于是有结论：

> **结论2：** 将输入（减去所有样本的均值后）除以所有样本的标准差，有类似自适应学习率的作用，使得每一层的更新更为同步，减少了在某一层过拟合的可能性，是一个提升神经网络性能的操作。

### 推导穷，BN现 [\#](https://kexue.fm/archives/6992\#%E6%8E%A8%E5%AF%BC%E7%A9%B7%EF%BC%8CBN%E7%8E%B0)

前面的推导，虽然表明上仅以单层神经网络（输入向量，输出标量）为例子，但是结论已经有足够的代表性了，因为多层神经网络本质上也就是单层神经网络的复合而已（关于这个论点，可以参考笔者旧作 [《从Boosting学习到神经网络：看山是山？》](https://kexue.fm/archives/3873)）。

所以有了前面的两个结论，那么BN基本就可以落实了：训练的时候，每一层的**输入**都减去均值除以标准差即可，不过由于每个batch的只是整体的近似，而期望$\\eqref{eq:mu},\\eqref{eq:sigma}$是全体样本的均值和标准差，所以BN避免不了的是batch size大点效果才好，这对算力提出了要求。此外，这个分析过程的一个结论是： **BN应当放在全连接/卷积前面。**

此外，我们还要维护一组变量，把训练过程中的均值方差存起来，供预测时使用，这就是BN中通过滑动平均来统计的均值方差变量了。至于BN的标准设计中，减均值除标准差后还补充上的$\\beta,\\gamma$项，我认为仅是锦上添花作用，不是最必要的，所以也没法多做解释了。

## 简单的总结 [\#](https://kexue.fm/archives/6992\#%E7%AE%80%E5%8D%95%E7%9A%84%E6%80%BB%E7%BB%93)

本文从优化角度分析了BN其作用的原理，所持的观点跟 [《How Does Batch Normalization Help Optimization?》](https://papers.cool/arxiv/1805.11604) 基本一致，但是所用的数学论证和描述方式个人认为会更简单易懂写。最终的结论是减去均值那一项，有助于降低神经网络梯度的$L$常数，而除以标准差的那一项，更多的是起到类似自适应学习率的作用，使得每个参数的更新更加同步，而不至于对某一层、某个参数过拟合。

当然，上述诠释只是一些粗糙的引导，完整地解释BN是一件很难的事情，BN的作用更像是多种因素的复合结果，比如对于我们主流的激活函数来说，$\[-1, 1\]$基本上都是非线性较强的区间，所以将输入弄成均值为0、方差为1，也能更充分地发挥激活函数的非线性能力，不至于过于浪费神经网络的拟合能力。

总之，神经网络的理论分析都是很艰难的事情，远不是笔者能胜任的，也就只能在这里写写博客，讲讲可有可无的故事来贻笑大方罢了～

_**转载到请包括本文地址：** [https://kexue.fm/archives/6992](https://kexue.fm/archives/6992)_

_**更详细的转载事宜请参考：**_ [《科学空间FAQ》](https://kexue.fm/archives/6508#%E6%96%87%E7%AB%A0%E5%A6%82%E4%BD%95%E8%BD%AC%E8%BD%BD/%E5%BC%95%E7%94%A8)

**如果您还有什么疑惑或建议，欢迎在下方评论区继续讨论。**

**如果您觉得本文还不错，欢迎 [分享](https://kexue.fm/archives/6992#share)/ [打赏](https://kexue.fm/archives/6992#pay) 本文。打赏并非要从中获得收益，而是希望知道科学空间获得了多少读者的真心关注。当然，如果你无视它，也不会影响你的阅读。再次表示欢迎和感谢！**

打赏

微信打赏

支付宝打赏

因为网站后台对打赏并无记录，因此欢迎在打赏时候备注留言。你还可以 [**点击这里**](http://mail.qq.com/cgi-bin/qm_share?t=qm_mailme&email=tN7d1drY3drrx8H0xcWa19vZ) 或在下方评论区留言来告知你的建议或需求。

**如果您需要引用本文，请参考：**

苏剑林. (Oct. 11, 2019). 《BN究竟起了什么作用？一个闭门造车的分析 》\[Blog post\]. Retrieved from [https://kexue.fm/archives/6992](https://kexue.fm/archives/6992)

@online{kexuefm-6992,
        title={BN究竟起了什么作用？一个闭门造车的分析},
        author={苏剑林},
        year={2019},
        month={Oct},
        url={\\url{https://kexue.fm/archives/6992}},
}

分类： [信息时代](https://kexue.fm/category/Big-Data)    标签： [模型](https://kexue.fm/tag/%E6%A8%A1%E5%9E%8B/), [优化](https://kexue.fm/tag/%E4%BC%98%E5%8C%96/), [神经网络](https://kexue.fm/tag/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/)[40 评论](https://kexue.fm/archives/6992#comments)

< [“让Keras更酷一些！”：层与模型的重用技巧](https://kexue.fm/archives/6985) \| [最小熵原理（五）：“层层递进”之社区发现与聚类](https://kexue.fm/archives/7006) >

### 你也许还对下面的内容感兴趣

- [MoE环游记：5、均匀分布的反思](https://kexue.fm/archives/10945)
- [Transformer升级之路：20、MLA究竟好在哪里？](https://kexue.fm/archives/10907)
- [MoE环游记：4、难处应当多投入](https://kexue.fm/archives/10815)
- [MoE环游记：1、从几何意义出发](https://kexue.fm/archives/10699)
- [为什么梯度裁剪的默认模长是1？](https://kexue.fm/archives/10657)
- [从谱范数梯度到新式权重衰减的思考](https://kexue.fm/archives/10648)
- [从Hessian近似看自适应学习率优化器](https://kexue.fm/archives/10588)
- [通向最优分布之路：概率空间的最小化](https://kexue.fm/archives/10289)
- [缓存与效果的极限拉扯：从MHA、MQA、GQA到MLA](https://kexue.fm/archives/10091)
- [旁门左道之如何让Python的重试代码更加优雅](https://kexue.fm/archives/9938)

[发表你的看法](https://kexue.fm/archives/6992#comment_form)

1. [«](https://kexue.fm/archives/6992/comment-page-1#comments)
2. [1](https://kexue.fm/archives/6992/comment-page-1#comments)
3. [2](https://kexue.fm/archives/6992/comment-page-2#comments)

cy

April 3rd, 2022

请问苏神$\\big\|\\mathbb{E}\_{x\\sim p(x)}\\left\[x\\otimes x\\right\]\\big\|\_1$中的$\\otimes$表示的是向量外积还是张量积呢？

[回复评论](https://kexue.fm/archives/6992/comment-page-2?replyTo=18856#respond-post-6992)

[K.L.](https://k-l-lambda.github.io/)

July 20th, 2022

stylegan第一版生成的人脸图中总是有个圆斑，后来第二版作者去掉了BN就好了．可见BN也有局限性．感觉有点像photoshop里的图像自动对比度处理，让图像的细节能看得更清晰，但也破坏了数据的先验分布信息．

[回复评论](https://kexue.fm/archives/6992/comment-page-2?replyTo=19507#respond-post-6992)

[Klayand](http://klayand.github.io)

March 26th, 2024

请问苏神，为什么要先考虑平移，再考虑缩放？如果单从结果的角度分析，直接考虑缩放，缩放因子为 均值，不也能将其变成等式(9)吗？

[回复评论](https://kexue.fm/archives/6992/comment-page-2?replyTo=24012#respond-post-6992)

[苏剑林](https://kexue.fm) 发表于
March 27th, 2024

均值有可能为0，缩放因子怎么可以用均值？当然RMS是有可能的，对应(Batch) RMS Norm了

之所以先考虑平移，是因为平移对应加减，它对模型表达能力的影响最小～

[回复评论](https://kexue.fm/archives/6992/comment-page-2?replyTo=24024#respond-post-6992)

zyr

July 11th, 2024

请问苏神，这里的推论“BN应当放在全连接/卷积前面”好像和大家普遍采用的不太一样？我理解BN必须放在激活函数之前，但是想不明白BN在全连接/卷积的前后有什么影响QAQ

参考: https://stackoverflow.com/questions/39691902/ordering-of-batch-normalization-and-dropout

[回复评论](https://kexue.fm/archives/6992/comment-page-2?replyTo=24776#respond-post-6992)

[苏剑林](https://kexue.fm) 发表于
July 12th, 2024

不同的假设有不同的结论吧，当前Transformer中主流的Pre Norm就是把Norm放到Attention/MLP前面的。

事实上本文的结论，更准确应该是将BN加在输入之后，至于输入之后是什么，取决于我们怎么看待一个Block。本文的视角是$Activation(Dense(x))$这样一个运算视为一个Block，所以输入之后就是全连接之前；早期的ResNet-v2，是将$Dense(Activation(x))$视为一个Block，所以输入之后就是激活函数之前，同样可以借用本文的分析得出相近的结论。

[回复评论](https://kexue.fm/archives/6992/comment-page-2?replyTo=24794#respond-post-6992)

笑雨

July 29th, 2024

1， [https://kexue.fm/archives/6051](https://kexue.fm/archives/6051) 对于“输入”满足L约束，需要对W进行谱约束
2，此文描述对于“梯度”满足L约束，需要对X进行BN归一化。请问这里是否也可以写成对X进行谱归一化？会不会更有效果？

[回复评论](https://kexue.fm/archives/6992/comment-page-2?replyTo=24917#respond-post-6992)

笑雨 发表于
July 29th, 2024

在中间，是否也可以对X进行截断操作？

[回复评论](https://kexue.fm/archives/6992/comment-page-2?replyTo=24918#respond-post-6992)

[苏剑林](https://kexue.fm) 发表于
July 29th, 2024

怎么截断法？

[回复评论](https://kexue.fm/archives/6992/comment-page-2?replyTo=24932#respond-post-6992)

笑雨 发表于
July 30th, 2024

clipping：裁剪。不叫截断，我翻译错了
weight clipping
由 Wasserstein GAN 提出。在利用 gradient descent 进行参数更新后，再对所有参数进行裁剪操作：

if w < -threshold:
w = -threshold
elif w > threshold:
w = threshold
else:
w = identity(w)

或许在输入和输入变换后进行裁剪，或者在中间，或者后半段。如果输入不可以裁剪，就网络中间层或者靠近尾部层进行裁剪。因为尾部十分靠近loss func给的梯度，他们往往相对前面层较大（梯度消失往往说的是网络的前面层），而它们太大也会影响结果的输出。或许L2正则只用正则后面层的参数，前面层某些值较大根本没法有太大的影响到结果。我想到droupout也是只把后面的丢掉，它前面不丢掉；如果这样可以行，这样就把droupout为什么只丢掉后面层的原因，就给串起来了，说不定可以得到一些有用的结果。

[回复评论](https://kexue.fm/archives/6992/comment-page-2?replyTo=24934#respond-post-6992)

[苏剑林](https://kexue.fm) 发表于
August 7th, 2024

clip输入的话会丢信息啊，你不能为normalize而normalize。

[回复评论](https://kexue.fm/archives/6992/comment-page-2?replyTo=24962#respond-post-6992)

[苏剑林](https://kexue.fm) 发表于
July 29th, 2024

理论上可以，问题是更复杂了。

[回复评论](https://kexue.fm/archives/6992/comment-page-2?replyTo=24931#respond-post-6992)

1. [«](https://kexue.fm/archives/6992/comment-page-1#comments)
2. [1](https://kexue.fm/archives/6992/comment-page-1#comments)
3. [2](https://kexue.fm/archives/6992/comment-page-2#comments)

[取消回复](https://kexue.fm/archives/6992#respond-post-6992)

你的大名

电子邮箱

个人网站（选填）

1\. 可以使用LaTeX代码，点击“预览效果”可查看效果；2. 可以通过点击评论楼层编号来引用该楼层；3. 网站可能会有点卡，如非确认评论失败，请 **不要重复点击提交**。

### 内容速览

[一些基础结论](https://kexue.fm/archives/6992#%E4%B8%80%E4%BA%9B%E5%9F%BA%E7%A1%80%E7%BB%93%E8%AE%BA)
[核心不等式](https://kexue.fm/archives/6992#%E6%A0%B8%E5%BF%83%E4%B8%8D%E7%AD%89%E5%BC%8F)
[梯度下降](https://kexue.fm/archives/6992#%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D)
[Lipschitz约束](https://kexue.fm/archives/6992#Lipschitz%E7%BA%A6%E6%9D%9F)
[BN是怎样炼成的](https://kexue.fm/archives/6992#BN%E6%98%AF%E6%80%8E%E6%A0%B7%E7%82%BC%E6%88%90%E7%9A%84)
[梯度分析](https://kexue.fm/archives/6992#%E6%A2%AF%E5%BA%A6%E5%88%86%E6%9E%90)
[非线性假设](https://kexue.fm/archives/6992#%E9%9D%9E%E7%BA%BF%E6%80%A7%E5%81%87%E8%AE%BE)
[柯西不等式](https://kexue.fm/archives/6992#%E6%9F%AF%E8%A5%BF%E4%B8%8D%E7%AD%89%E5%BC%8F)
[减均值除标准差](https://kexue.fm/archives/6992#%E5%87%8F%E5%9D%87%E5%80%BC%E9%99%A4%E6%A0%87%E5%87%86%E5%B7%AE)
[推导穷，BN现](https://kexue.fm/archives/6992#%E6%8E%A8%E5%AF%BC%E7%A9%B7%EF%BC%8CBN%E7%8E%B0)
[简单的总结](https://kexue.fm/archives/6992#%E7%AE%80%E5%8D%95%E7%9A%84%E6%80%BB%E7%BB%93)

### 智能搜索

支持整句搜索！网站自动使用 [结巴分词](https://github.com/fxsjy/jieba) 进行分词，并结合ngrams排序算法给出合理的搜索结果。

### 热门标签

[生成模型](https://kexue.fm/tag/%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B/) [attention](https://kexue.fm/tag/attention/) [优化](https://kexue.fm/tag/%E4%BC%98%E5%8C%96/) [语言模型](https://kexue.fm/tag/%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/) [模型](https://kexue.fm/tag/%E6%A8%A1%E5%9E%8B/) [网站](https://kexue.fm/tag/%E7%BD%91%E7%AB%99/) [概率](https://kexue.fm/tag/%E6%A6%82%E7%8E%87/) [梯度](https://kexue.fm/tag/%E6%A2%AF%E5%BA%A6/) [转载](https://kexue.fm/tag/%E8%BD%AC%E8%BD%BD/) [微分方程](https://kexue.fm/tag/%E5%BE%AE%E5%88%86%E6%96%B9%E7%A8%8B/) [矩阵](https://kexue.fm/tag/%E7%9F%A9%E9%98%B5/) [天象](https://kexue.fm/tag/%E5%A4%A9%E8%B1%A1/) [分析](https://kexue.fm/tag/%E5%88%86%E6%9E%90/) [深度学习](https://kexue.fm/tag/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/) [积分](https://kexue.fm/tag/%E7%A7%AF%E5%88%86/) [python](https://kexue.fm/tag/python/) [优化器](https://kexue.fm/tag/%E4%BC%98%E5%8C%96%E5%99%A8/) [力学](https://kexue.fm/tag/%E5%8A%9B%E5%AD%A6/) [无监督](https://kexue.fm/tag/%E6%97%A0%E7%9B%91%E7%9D%A3/) [扩散](https://kexue.fm/tag/%E6%89%A9%E6%95%A3/) [几何](https://kexue.fm/tag/%E5%87%A0%E4%BD%95/) [节日](https://kexue.fm/tag/%E8%8A%82%E6%97%A5/) [生活](https://kexue.fm/tag/%E7%94%9F%E6%B4%BB/) [文本生成](https://kexue.fm/tag/%E6%96%87%E6%9C%AC%E7%94%9F%E6%88%90/) [数论](https://kexue.fm/tag/%E6%95%B0%E8%AE%BA/)

### 随机文章

- [日全食多路联合直播频道](https://kexue.fm/archives/25)
- [混沌的世界——“星之轨迹”的研究](https://kexue.fm/archives/1525)
- [《向量》系列——5.平面向量微分方程与复数](https://kexue.fm/archives/963)
- [封闭曲线所围成的面积：一个新技巧](https://kexue.fm/archives/3441)
- [五种零食揭示宇宙的形状](https://kexue.fm/archives/58)
- [基于Bert的NL2SQL模型：一个简明的Baseline](https://kexue.fm/archives/6771)
- [OCR技术浅探：3. 特征提取(2)](https://kexue.fm/archives/3802)
- [\[问题解答\]运煤车的最大路程（更正）](https://kexue.fm/archives/2587)
- [洋葱也能用来发电！](https://kexue.fm/archives/37)
- [能量视角下的GAN模型（二）：GAN＝“分析”＋“采样”](https://kexue.fm/archives/6331)

### 最近评论

- [rpsun](https://kexue.fm/archives/10592/comment-page-2#comment-27912): 老师您好，最近在自己的任务上尝试了muon，甚至只修改了学习率，同时不加区分地对所有二维以上的...
- [盏一](https://kexue.fm/archives/8265/comment-page-8#comment-27911): 我之前做的笔记:Q: 公式 (14) 的理解.A: 首先基于 \[定理 5\](https://b...
- [盏一](https://kexue.fm/archives/8397/comment-page-3#comment-27910): 哦哦哦 你是说 $\\exp nB$ 是正交矩阵! 并不是说 B.
- [盏一](https://kexue.fm/archives/8397/comment-page-3#comment-27908): 呃, 是我脑子乱了... 忘了 $\\exp(0) = I$. 所以只要 $\\Vert B^T+...
- [盏一](https://kexue.fm/archives/8397/comment-page-3#comment-27907): 苏神, 请教一下\> 并且还可以证明它一定是正交矩阵是怎么证明的. 我本来以为隐式利用了 $\\V...
- [sk](https://kexue.fm/archives/8265/comment-page-8#comment-27906): 请问公式14是怎么得出来的？
- [tll1945tll1937](https://kexue.fm/archives/10266/comment-page-1#comment-27901): 真心实意的向大家请教问题：看了文章“对齐全量微调！这是我看过最精彩的LoRA改进（二）”，我实...
- [oYo\_logan](https://kexue.fm/archives/10757/comment-page-1#comment-27897): \[comment=27017\]苏剑林\[/comment\]苏神，想请教一下，我理解在一个batc...
- [z123](https://kexue.fm/archives/10592/comment-page-1#comment-27896): 在参数矩阵较多的CNN小模型上，Muon会明显慢于Adam，这方面有什么优化提速的方案吗？
- [dry](https://kexue.fm/archives/10958/comment-page-2#comment-27895): 苏神好，一直有个疑问，ReFlow构建的ODE是$dx\_t/dt=x\_1-x\_0$，为什么这并...

### 友情链接

- [Cool Papers](https://papers.cool)
- [数学研发](https://bbs.emath.ac.cn)
- [Seatop](http://www.seatop.com.cn/)
- [Xiaoxia](https://xiaoxia.org/)
- [积分表-网络版](https://kexue.fm/sci/integral/index.html)
- [丝路博傲](http://blog.dvxj.com/)
- [ph4ntasy 饭特稀](http://www.ph4ntasy.com/)
- [数学之家](http://www.2math.cn/)
- [有趣天文奇观](http://interesting-sky.china-vo.org/)
- [TwistedW](http://www.twistedwg.com/)
- [godweiyang](https://godweiyang.com/)
- [AI柠檬](https://blog.ailemon.net/)
- [王登科-DK博客](https://greatdk.com)
- [ESON](https://blog.eson.org/)
- [枫之羽](https://fzhiy.net/)
- [Mathor's blog](https://wmathor.com/)
- [coding-zuo](https://coding-zuo.github.io/)
- [博科园](https://www.bokeyuan.net/)
- [孔皮皮的博客](https://www.kppkkp.top/)
- [运鹏的博客](https://yunpengtai.top/)
- [jiming.site](https://jiming.site/)
- [OmegaXYZ](https://www.omegaxyz.com/)
- [Blog by Eacls](https://www.eacls.top/)
- [EAI猩球](https://www.robotech.ink/)
- [文举的博客](https://liwenju0.com/)
- [用代码打点酱油](https://bruceyuan.com/)
- [申请链接](https://kexue.fm/links.html)

本站采用创作共用版权协议，要求署名、非商业用途和保持一致。转载本站内容必须也遵循“ [署名-非商业用途-保持一致](http://creativecommons.org/licenses/by-nc-nd/2.5/cn/)”的创作共用协议。
© 2009-2025 Scientific Spaces. All rights reserved. Theme by [laogui](http://www.laogui.com). Powered by [Typecho](http://typecho.org). 备案号: [粤ICP备09093259号-1/2](https://beian.miit.gov.cn/)。