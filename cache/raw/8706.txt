Loading \[MathJax\]/jax/output/HTML-CSS/fonts/TeX/fontdata.js

![MobileSideBar](https://kexue.fm/usr/themes/geekg/images/slide-button.png)

## SEARCH

## MENU

- [打赏](https://kexue.fm/reward.html)
- [公式](https://kexue.fm/latex.html)
- [天象](https://kexue.fm/ac.html)
- [链接](https://kexue.fm/links.html)
- [时光](https://kexue.fm/me.html)
- [博览](https://kexue.fm/science.html)
- [归档](https://kexue.fm/content.html)

## CATEGORIES

- [千奇百怪](https://kexue.fm/category/Everything)
- [天文探索](https://kexue.fm/category/Astronomy)
- [数学研究](https://kexue.fm/category/Mathematics)
- [物理化学](https://kexue.fm/category/Phy-chem)
- [信息时代](https://kexue.fm/category/Big-Data)
- [生物自然](https://kexue.fm/category/Biology)
- [图片摄影](https://kexue.fm/category/Photograph)
- [问题百科](https://kexue.fm/category/Questions)
- [生活/情感](https://kexue.fm/category/Life-Feeling)
- [资源共享](https://kexue.fm/category/Resources)

## NEWPOSTS

- [让炼丹更科学一些（五）：基于梯度精...](https://kexue.fm/archives/11530)
- [让炼丹更科学一些（四）：新恒等式，...](https://kexue.fm/archives/11494)
- [为什么DeltaNet要加L2 N...](https://kexue.fm/archives/11486)
- [让炼丹更科学一些（三）：SGD的终...](https://kexue.fm/archives/11480)
- [让炼丹更科学一些（二）：将结论推广...](https://kexue.fm/archives/11469)
- [滑动平均视角下的权重衰减和学习率](https://kexue.fm/archives/11459)
- [生成扩散模型漫谈（三十一）：预测数...](https://kexue.fm/archives/11428)
- [Muon优化器指南：快速上手与关键细节](https://kexue.fm/archives/11416)
- [AdamW的Weight RMS的...](https://kexue.fm/archives/11404)
- [n个正态随机数的最大值的渐近估计](https://kexue.fm/archives/11390)

## COMMENTS

- [Bin: 今天偶然从某个论坛看到有人推荐您的博客，定睛一看竟然是华师同院...](https://kexue.fm/archives/1990/comment-page-2#comment-29105)
- [Rapture D: 我有一个问题，为什么不考虑亥姆霍兹定理和斯托克斯公式。](https://kexue.fm/archives/11530/comment-page-1#comment-29104)
- [mofheka: 苏神是还在用jax是么？最近在做基于Google Pathwa...](https://kexue.fm/archives/11390/comment-page-1#comment-29103)
- [长琴: 看懂这篇博客也不是一件容易的事情。](https://kexue.fm/archives/11530/comment-page-1#comment-29102)
- [AlexLi: 苏老师，请教一下(7)式中将 \\mu(x\_t) 传给 $p...](https://kexue.fm/archives/9257/comment-page-4#comment-29101)
- [tyler\_zxc: "Performer的思想是将标准的Attention线性化，...](https://kexue.fm/archives/7921/comment-page-2#comment-29100)
- [我: 似乎并非mHC提出矩阵的思想？之前hyper connecti...](https://kexue.fm/archives/11494/comment-page-1#comment-29099)
- [winter: 苏神您好，假如对于比较均匀的attention weightP...](https://kexue.fm/archives/10847/comment-page-1#comment-29098)
- [苏剑林: KL散度、JS散度、W距离啥的，都行啊，看你喜欢哪个](https://kexue.fm/archives/8512/comment-page-2#comment-29097)
- [苏剑林: 没有绝对公平的对比方法，主要看你关心什么。比如，如果只关心推理...](https://kexue.fm/archives/9119/comment-page-14#comment-29096)

## USERLOGIN

- [登录](https://kexue.fm/admin/login.php)

[科学空间\|Scientific Spaces](https://kexue.fm/)

- [登录](https://kexue.fm/admin/login.php)
- [打赏](https://kexue.fm/reward.html)
- [公式](https://kexue.fm/latex.html)
- [天象](https://kexue.fm/ac.html)
- [链接](https://kexue.fm/links.html)
- [时光](https://kexue.fm/me.html)
- [博览](https://kexue.fm/science.html)
- [归档](https://kexue.fm/content.html)

渴望成为一个小飞侠

- [![](https://kexue.fm/usr/themes/geekg/images/rss.png)\\
\\
欢迎订阅](https://kexue.fm/feed)
- [![](https://kexue.fm/usr/themes/geekg/images/mail.png)\\
\\
个性邮箱](https://kexue.fm/archives/119)
- [![](https://kexue.fm/usr/themes/geekg/images/Saturn.png)\\
\\
天象信息](https://kexue.fm/ac.html)
- [![](https://kexue.fm/usr/themes/geekg/images/iss.png)\\
\\
观测ISS](https://kexue.fm/archives/41)
- [![](https://kexue.fm/usr/themes/geekg/images/pi.png)\\
\\
LaTeX](https://kexue.fm/latex.html)
- [![](https://kexue.fm/usr/themes/geekg/images/mlogo.png)\\
\\
关于博主](https://kexue.fm/me.html)

欢迎访问“科学空间”，这里将与您共同探讨自然科学，回味人生百态；也期待大家的分享～

- [**千奇百怪** Everything](https://kexue.fm/category/Everything)
- [**天文探索** Astronomy](https://kexue.fm/category/Astronomy)
- [**数学研究** Mathematics](https://kexue.fm/category/Mathematics)
- [**物理化学** Phy-chem](https://kexue.fm/category/Phy-chem)
- [**信息时代** Big-Data](https://kexue.fm/category/Big-Data)
- [**生物自然** Biology](https://kexue.fm/category/Biology)
- [**图片摄影** Photograph](https://kexue.fm/category/Photograph)
- [**问题百科** Questions](https://kexue.fm/category/Questions)
- [**生活/情感** Life-Feeling](https://kexue.fm/category/Life-Feeling)
- [**资源共享** Resources](https://kexue.fm/category/Resources)

- [**千奇百怪**](https://kexue.fm/category/Everything)
- [**天文探索**](https://kexue.fm/category/Astronomy)
- [**数学研究**](https://kexue.fm/category/Mathematics)
- [**物理化学**](https://kexue.fm/category/Phy-chem)
- [**信息时代**](https://kexue.fm/category/Big-Data)
- [**生物自然**](https://kexue.fm/category/Biology)
- [**图片摄影**](https://kexue.fm/category/Photograph)
- [**问题百科**](https://kexue.fm/category/Questions)
- [**生活/情感**](https://kexue.fm/category/Life-Feeling)
- [**资源共享**](https://kexue.fm/category/Resources)

[首页](https://kexue.fm/) [数学研究](https://kexue.fm/category/Mathematics) [信息时代](https://kexue.fm/category/Big-Data) 让人惊叹的Johnson-Lindenstrauss引理：应用篇

24Sep

# [让人惊叹的Johnson-Lindenstrauss引理：应用篇](https://kexue.fm/archives/8706)

By 苏剑林 \|
2021-09-24 \|
50811位读者 \|

上一篇文章 [《让人惊叹的Johnson-Lindenstrauss引理：理论篇》](https://kexue.fm/archives/8679) 中，我们比较详细地介绍了Johnson-Lindenstrauss引理（JL引理）的理论推导，这一篇我们来关注它的应用。

作为一个内容上本身就跟降维相关的结论，JL引理最基本的自然就是作为一个降维方法来用。但除了这个直接应用外，很多看似不相关的算法，比如局部敏感哈希（LSH）、随机SVD等，本质上也依赖于JL引理。此外，对于机器学习模型来说，JL引理通常还能为我们的维度选择提供一些理论解释。

## 降维的工具 [\#](https://kexue.fm/archives/8706\#%E9%99%8D%E7%BB%B4%E7%9A%84%E5%B7%A5%E5%85%B7)

JL引理提供了一个非常简单直接的“随机投影”降维思路：

> 给定N个向量v\_1,v\_2,\\cdots,v\_N\\in\\mathbb{R}^m，如果想要将它降到n维，那么只需要从\\mathcal{N}(0,1/n)中采样一个n\\times m矩阵A，然后Av\_1,Av\_2,\\cdots,Av\_N就是降维后的结果。

这个思路简单快速是毋庸置疑的，读者随之而来的疑问就是：它跟PCA、t-SNE等降维方法相比效果如何？

其实，正如“存在就是合理的”，更复杂的PCA、t-SNE等方法既然还没有被淘汰，那就说明它肯定有比随机投影更好的地方。事实上，JL引理的随机投影只是提供了一种非常基本的降维方法，显示出哪怕在这么简单的方法之后，降维后的维度也只需要\\mathcal{O}(\\log N)，它更多的是一个理论证明。

所以，真要追求降维精度的话，多数情况下PCA、t-SNE等这些专门的降维方法，效果肯定是要比随机投影要好的。而且上一篇文章中我们也提过，JL引理是一个非常充分的条件，它得到的n > \\frac{24\\log N}{\\varepsilon^2}甚至n > \\frac{16\\log N}{\\varepsilon^2}都只是非常充分的界，比如取\\varepsilon=0.1的话，就有n > 1600\\log N了，基本没有实用价值。而换用PCA、t-SNE等更精准的降维方法，可以放宽这个要求，即在更小的维度下达到更好的效果。

## 局部的哈希 [\#](https://kexue.fm/archives/8706\#%E5%B1%80%E9%83%A8%E7%9A%84%E5%93%88%E5%B8%8C)

局部敏感哈希（Locality-Sensitive Hashing，LSH），是近似查找某种度量下的最邻近元素的一种方案。通常来说，我们很少将LSH与JL引理联系起来，但笔者认为，LSH的哈希函数选择上，其实跟JL引理也是紧密相关的。简单来说，LSH就是一个将向量二值化的算法，并且二值化之后的向量能近似保持度量不变。常见的一种方案是通过随机投影来（近似）保持cos值的不变性。

具体来说，根据JL引理，我们从\\mathcal{N}(0,1/n)中采样一个n\\times m矩阵A，那么对于任意v\_i,v\_j\\in\\mathbb{R}^m，都有\\cos(v\_i,v\_j)\\approx \\cos(Av\_i, Av\_j)。当然，随机投影还不是LSH的全部，我们留意到，经过A的投影后，Av\_i,Av\_j的正负分布情况是比较均匀的，所以我们进一步做近似

\\begin{equation}\\cos(v\_i,v\_j)\\approx \\cos(Av\_i, Av\_j)\\approx \\cos(\\text{sign}(Av\_i), \\text{sign}(Av\_j))\\end{equation}

即每个元素我们根据正负号二值化为\\pm 1，这就实现了向量的二值化，并且保持了余弦值近似不变。有了二值化向量后，我们可以建索引、分通等，以加快检索速度，这些就不细说了。

总之，在LSH过程中，关键的一步也是随机投影，这一步本身与JL引理也是紧密相关的。当然，二值化通常会比较明显地牺牲精度，所以根据实际场景的不同，我们并不总是“降维”，即n并不会总是小于m，有时候我们可能还会选择n > m。相关的讨论读者可以参考笔者之前写的 [《一个二值化词向量模型，是怎么跟果蝇搭上关系的？》](https://kexue.fm/archives/8159)。

## 随机的分解 [\#](https://kexue.fm/archives/8706\#%E9%9A%8F%E6%9C%BA%E7%9A%84%E5%88%86%E8%A7%A3)

矩阵分解是解决许多机器学习问题的强大工具，而奇异值分解（SVD）则是其中的典型方法之一。然而，当矩阵比较大的时候，计算精确的SVD分解成本相当大，而实际场景中，待分解矩阵虽然大，但往往也是低秩的，计算精确的SVD分解也没有必要。这时候，“随机SVD分解”便派上用场了。

设待分解矩阵为M\\in\\mathbb{R}^{m\\times n}，m,n都比较大。根据JL引理，我们可以选择比较小的k < \\min(m,n)，使得从\\mathcal{N}(0,1/k)中采样出来n\\times k矩阵Q依然能比较高精度地满足QQ^{\\top}\\approx I（近似正交矩阵），从而M\\approx MQQ^{\\top}。这样，我们可以只对m\\times k矩阵B=MQ做SVD分解，得到MQ=B=U\_B\\Sigma\_B V\_B^{\\top}，那么

\\begin{equation}M\\approx MQQ^{\\top} = U\_B\\Sigma\_B V\_B^{\\top}Q^{\\top} = U\_B \\Sigma\_B (QV\_B)^{\\top}\\end{equation}

就得到了原始矩阵M的一个近似SVD分解。注意，上述Q还只是近似正交矩阵，我们可以通过QR分解（或施密特正交化）使得它变成严格正交，这是一个小细节。在整个过程中，JL引理所告诉我们的是k可以选得比较小，以至于对B=MQ做SVD是比较低成本的，但总体精度也不会太差。

## 词向量维度 [\#](https://kexue.fm/archives/8706\#%E8%AF%8D%E5%90%91%E9%87%8F%E7%BB%B4%E5%BA%A6)

我们说JL引理的通俗理解是“塞下N个向量只需要\\mathcal{O}(\\log N)维空间”，那么回到词向量维度选择问题上，也就是说如果词表大小为N，那么词向量维度是\\mathcal{O}(\\log N)就够了。

非常让人惊震的是，在笔者之前的文章 [《最小熵原理（六）：词向量的维度应该怎么选择？》](https://kexue.fm/archives/7695) 中，曾计算出了一个Skip Gram词向量模型的维度选择公式：

\\begin{equation}n > 8.33\\log N\\end{equation}

其结果与JL引理所给出的\\mathcal{O}(\\log N)如出一辙！上述公式是基于熵的思想进行估计的，与JL引理的出发点几乎没有交集之处，但竟然殊途同归地得到了\\log N。

而且，不仅仅是主体\\log N，我们还看到，基于熵的估计，我们还把\\log N前面的系数8.33也计算出来了，并且以往的实验经验还显示，8.33\\log N这个结果还是挺符合经验的，虽然未必是最优，但至少范围上差不远。这是不是可以反过来说，我们可以通过熵来比较精确地估计具体问题下\\log N前面的系数？

## 多头注意力 [\#](https://kexue.fm/archives/8706\#%E5%A4%9A%E5%A4%B4%E6%B3%A8%E6%84%8F%E5%8A%9B)

关于Attention机制，常见的面试题就是“为什么要多头？”、“head\_size为768的单头注意力，跟head\_size为64的12头注意力有什么区别？”等，也就是说，像BERT这样的Attention模型，为什么要先把head\_size降低到64再做内积？64真的够了吗？

这个问题本质上来说Attention机制是否足以拟合任何概率模式的问题。具体来说，Attention的计算公式为：

\\begin{equation}a\_{i,j} = \\frac{e^{\\langle q\_i, k\_j\\rangle}}{\\sum\\limits\_{j=1}^L e^{\\langle q\_i, k\_j\\rangle}}\\end{equation}

其中q\_i,k\_j\\in\\mathbb{R}^{d}，所谓“够不够”，就是指对于任意给定的概率矩阵p\_{i,j}，上述定义的a\_{i,j}是否都能很好地逼近它？

看到a\_{i,j}的定义，不知道有没有读者觉得熟悉的？如果我们抛开Attention的背景，将q\_i,k\_j分别视为两个“词向量”，那么a\_{i,j}的定义跟Skip Gram模型一模一样！也就是说，单纯看Attention矩阵的计算公式，它跟Skip Gram模型本质上是一样的，所以Attention的head\_size选择，本质上也就是词向量的维度选择。

让我们再来捋一捋过程。我们要回答的是“head\_size多少才够”的问题，这变成了“a\_{i,j}能否逼近任意概率矩阵p\_{i,j}”的问题，也就是说，对于给定p\_{i,j}，我们是否能找到一组q\_1,\\cdots,q\_L,k\_1,\\cdots,k\_L\\in\\mathbb{R}^d，使得a\_{i,j}与p\_{i,j}足够近似，这个问题跟Skip Gram词向量模型的维度选择是数学等价的。

因此，词向量维度选择的结果，也就可以用于Attention的head\_size选择，只不过词表大小变成了序列长度，即d > 8.33\\log L，常见的预训练长度是L=512，代入计算约等于52，同样非常让人震惊，跟常见的head\_size=64确实相差无几！所以，64真的够了，再大也不会有明显提升，倒不如将多出来的计算量用来增加head的数目～

（注：相关讨论还可以参考文献 [《On the Expressive Power of Self-Attention Matrices》](https://papers.cool/arxiv/2106.03764)。）

## 又到了小结 [\#](https://kexue.fm/archives/8706\#%E5%8F%88%E5%88%B0%E4%BA%86%E5%B0%8F%E7%BB%93)

本文主要介绍了Johnson-Lindenstrauss引理（JL引理）的几个直接或间接的应用，可以看到，从降维、哈希的方法，到词向量维度、Attention的头大小等，多多少少都与JL引理有所关联，这进一步显示了JL引理的适用范围之广。

_**转载到请包括本文地址：** [https://kexue.fm/archives/8706](https://kexue.fm/archives/8706 "让人惊叹的Johnson-Lindenstrauss引理：应用篇")_

_**更详细的转载事宜请参考：**_ [《科学空间FAQ》](https://kexue.fm/archives/6508#%E6%96%87%E7%AB%A0%E5%A6%82%E4%BD%95%E8%BD%AC%E8%BD%BD/%E5%BC%95%E7%94%A8 "《科学空间FAQ》")

**如果您还有什么疑惑或建议，欢迎在下方评论区继续讨论。**

**如果您觉得本文还不错，欢迎 [分享](https://kexue.fm/archives/8706#share)/ [打赏](https://kexue.fm/archives/8706#pay) 本文。打赏并非要从中获得收益，而是希望知道科学空间获得了多少读者的真心关注。当然，如果你无视它，也不会影响你的阅读。再次表示欢迎和感谢！**

打赏

![科学空间](https://kexue.fm/usr/themes/geekg/payment/wx.png)

微信打赏

![科学空间](https://kexue.fm/usr/themes/geekg/payment/zfb.png)

支付宝打赏

因为网站后台对打赏并无记录，因此欢迎在打赏时候备注留言。

你还可以 [**点击这里**](http://mail.qq.com/cgi-bin/qm_share?t=qm_mailme&email=tN7d1drY3drrx8H0xcWa19vZ) 或在下方评论区留言来告知你的建议或需求。

**如果您需要引用本文，请参考：**

苏剑林. (Sep. 24, 2021). 《让人惊叹的Johnson-Lindenstrauss引理：应用篇 》\[Blog post\]. Retrieved from [https://kexue.fm/archives/8706](https://kexue.fm/archives/8706)

@online{kexuefm-8706,

         title={让人惊叹的Johnson-Lindenstrauss引理：应用篇},

         author={苏剑林},

         year={2021},

         month={Sep},

         url={\\url{https://kexue.fm/archives/8706}},

}


分类： [数学研究](https://kexue.fm/category/Mathematics), [信息时代](https://kexue.fm/category/Big-Data)    标签： [模型](https://kexue.fm/tag/%E6%A8%A1%E5%9E%8B/), [分析](https://kexue.fm/tag/%E5%88%86%E6%9E%90/), [维度](https://kexue.fm/tag/%E7%BB%B4%E5%BA%A6/), [机器学习](https://kexue.fm/tag/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/)[4 评论](https://kexue.fm/archives/8706#comments)

< [让人惊叹的Johnson-Lindenstrauss引理：理论篇](https://kexue.fm/archives/8679 "让人惊叹的Johnson-Lindenstrauss引理：理论篇") \| [关于维度公式“n > 8.33 log N”的可用性分析](https://kexue.fm/archives/8711 "关于维度公式“n > 8.33 log N”的可用性分析") >

### 你也许还对下面的内容感兴趣

- [低精度Attention可能存在有偏的舍入误差](https://kexue.fm/archives/11371 "低精度Attention可能存在有偏的舍入误差")
- [为什么Adam的Update RMS是0.2？](https://kexue.fm/archives/11267 "为什么Adam的Update RMS是0.2？")
- [ReLU/GeLU/Swish的一个恒等式](https://kexue.fm/archives/11233 "ReLU/GeLU/Swish的一个恒等式")
- [等值振荡定理：最优多项式逼近的充要条件](https://kexue.fm/archives/10972 "等值振荡定理：最优多项式逼近的充要条件")
- [SVD的导数](https://kexue.fm/archives/10878 "SVD的导数")
- [通过梯度近似寻找Normalization的替代品](https://kexue.fm/archives/10831 "通过梯度近似寻找Normalization的替代品")
- [MoE环游记：1、从几何意义出发](https://kexue.fm/archives/10699 "MoE环游记：1、从几何意义出发")
- [通向概率分布之路：盘点Softmax及其替代品](https://kexue.fm/archives/10145 "通向概率分布之路：盘点Softmax及其替代品")
- [用傅里叶级数拟合一维概率密度函数](https://kexue.fm/archives/10007 "用傅里叶级数拟合一维概率密度函数")
- [基于量子化假设推导模型的尺度定律（Scaling Law）](https://kexue.fm/archives/9607 "基于量子化假设推导模型的尺度定律（Scaling Law）")

[发表你的看法](https://kexue.fm/archives/8706#comment_form)

宇航员

October 9th, 2021

head\_size 为什么定义为64，终于看到了令我信服的解释！

[回复评论](https://kexue.fm/archives/8706/comment-page-1?replyTo=17512#respond-post-8706)

lifelike

July 14th, 2024

苏神太强了，我从本科遇到不懂的问题搜到最好的答案是苏神的，现在读博了，搜到的最好的答案还是苏神的

[回复评论](https://kexue.fm/archives/8706/comment-page-1?replyTo=24824#respond-post-8706)

fox

March 14th, 2025

苏神，请问局部敏感哈希那里为什么经过降维后的向量正负分布是均匀的，以及最后的cos(Avi,Avj)≈cos(sign(Avi),sign(Avj))为什么成立？拜谢！

[回复评论](https://kexue.fm/archives/8706/comment-page-1?replyTo=27123#respond-post-8706)

[苏剑林](https://kexue.fm/) 发表于
March 16th, 2025

首先，根据 [https://kexue.fm/archives/8679](https://kexue.fm/archives/8679) 的介绍，这样采样出来的A矩阵是接近正交矩阵的，所以保角，因此有\\cos(v\_i,v\_j)\\approx \\cos(Av\_i, Av\_j)。

其次，由于A的采样分布是0均值并且各向同性的，这意味着A的各向量比较均匀地分布在整个空间中，于是可以想象有一半向量跟v夹角大于90度，有一半向量夹角小于90度，因此大致上来说Av的正负是均匀的。

当然这是可以从理论上严格化的，但这里就不做了。

[回复评论](https://kexue.fm/archives/8706/comment-page-1?replyTo=27146#respond-post-8706)

[取消回复](https://kexue.fm/archives/8706#respond-post-8706)

你的大名

电子邮箱

个人网站（选填）

1\. 可以使用LaTeX代码，点击“预览效果”可查看效果；

2\. 可以通过点击评论楼层编号来引用该楼层；

3\. 网站可能会有点卡，如非确认评论失败，请 **不要重复点击提交**。

### 内容速览

[降维的工具](https://kexue.fm/archives/8706#%E9%99%8D%E7%BB%B4%E7%9A%84%E5%B7%A5%E5%85%B7)
[局部的哈希](https://kexue.fm/archives/8706#%E5%B1%80%E9%83%A8%E7%9A%84%E5%93%88%E5%B8%8C)
[随机的分解](https://kexue.fm/archives/8706#%E9%9A%8F%E6%9C%BA%E7%9A%84%E5%88%86%E8%A7%A3)
[词向量维度](https://kexue.fm/archives/8706#%E8%AF%8D%E5%90%91%E9%87%8F%E7%BB%B4%E5%BA%A6)
[多头注意力](https://kexue.fm/archives/8706#%E5%A4%9A%E5%A4%B4%E6%B3%A8%E6%84%8F%E5%8A%9B)
[又到了小结](https://kexue.fm/archives/8706#%E5%8F%88%E5%88%B0%E4%BA%86%E5%B0%8F%E7%BB%93)

### 智能搜索

支持整句搜索！网站自动使用 [结巴分词](https://github.com/fxsjy/jieba) 进行分词，并结合ngrams排序算法给出合理的搜索结果。

### 热门标签

[生成模型](https://kexue.fm/tag/%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B/) [attention](https://kexue.fm/tag/attention/) [优化](https://kexue.fm/tag/%E4%BC%98%E5%8C%96/) [语言模型](https://kexue.fm/tag/%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/) [模型](https://kexue.fm/tag/%E6%A8%A1%E5%9E%8B/) [梯度](https://kexue.fm/tag/%E6%A2%AF%E5%BA%A6/) [网站](https://kexue.fm/tag/%E7%BD%91%E7%AB%99/) [概率](https://kexue.fm/tag/%E6%A6%82%E7%8E%87/) [矩阵](https://kexue.fm/tag/%E7%9F%A9%E9%98%B5/) [优化器](https://kexue.fm/tag/%E4%BC%98%E5%8C%96%E5%99%A8/) [转载](https://kexue.fm/tag/%E8%BD%AC%E8%BD%BD/) [微分方程](https://kexue.fm/tag/%E5%BE%AE%E5%88%86%E6%96%B9%E7%A8%8B/) [分析](https://kexue.fm/tag/%E5%88%86%E6%9E%90/) [天象](https://kexue.fm/tag/%E5%A4%A9%E8%B1%A1/) [深度学习](https://kexue.fm/tag/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/) [积分](https://kexue.fm/tag/%E7%A7%AF%E5%88%86/) [python](https://kexue.fm/tag/python/) [扩散](https://kexue.fm/tag/%E6%89%A9%E6%95%A3/) [力学](https://kexue.fm/tag/%E5%8A%9B%E5%AD%A6/) [无监督](https://kexue.fm/tag/%E6%97%A0%E7%9B%91%E7%9D%A3/) [几何](https://kexue.fm/tag/%E5%87%A0%E4%BD%95/) [节日](https://kexue.fm/tag/%E8%8A%82%E6%97%A5/) [生活](https://kexue.fm/tag/%E7%94%9F%E6%B4%BB/) [文本生成](https://kexue.fm/tag/%E6%96%87%E6%9C%AC%E7%94%9F%E6%88%90/) [数论](https://kexue.fm/tag/%E6%95%B0%E8%AE%BA/)

### 随机文章

- [2^29363731-1不是素数！](https://kexue.fm/archives/1951)
- [用Numpy实现高效的Apriori算法](https://kexue.fm/archives/5525)
- [重温SSM（三）：HiPPO的高效计算（S4）](https://kexue.fm/archives/10162)
- [一维弹簧的运动（上）](https://kexue.fm/archives/2430)
- [从费马大定理谈起（十一）：有理点与切割线法](https://kexue.fm/archives/2996)
- [百科翻译：臭氧的性质](https://kexue.fm/archives/8)
- [路径积分系列：5.例子和综述](https://kexue.fm/archives/3766)
- [今天我们都是舟曲人——举国哀悼舟曲遇难同胞](https://kexue.fm/archives/857)
- [有限内存下全局打乱几百G文件（Python）](https://kexue.fm/archives/8662)
- [中山大学力学网络教程](https://kexue.fm/archives/726)

### 最近评论

- [Bin](https://kexue.fm/archives/1990/comment-page-2#comment-29105): 今天偶然从某个论坛看到有人推荐您的博客，定睛一看竟然是华师同院的往届师兄！看到这篇2013年的...
- [Rapture D](https://kexue.fm/archives/11530/comment-page-1#comment-29104): 我有一个问题，为什么不考虑亥姆霍兹定理和斯托克斯公式。
- [mofheka](https://kexue.fm/archives/11390/comment-page-1#comment-29103): 苏神是还在用jax是么？最近在做基于Google Pathway的理念做一个动态版的MPMD框...
- [长琴](https://kexue.fm/archives/11530/comment-page-1#comment-29102): 看懂这篇博客也不是一件容易的事情。
- [AlexLi](https://kexue.fm/archives/9257/comment-page-4#comment-29101): 苏老师，请教一下(7)式中将 \\mu(x\_t) 传给 p\_o 进行推理的操作。 $x\_...
- [tyler\_zxc](https://kexue.fm/archives/7921/comment-page-2#comment-29100): "Performer的思想是将标准的Attention线性化，所以为什么不干脆直接训练一个线性...
- [我](https://kexue.fm/archives/11494/comment-page-1#comment-29099): 似乎并非mHC提出矩阵的思想？之前hyper connection就是了
- [winter](https://kexue.fm/archives/10847/comment-page-1#comment-29098): 苏神您好，假如对于比较均匀的attention weightP，往往呈现long tail分布...
- [苏剑林](https://kexue.fm/archives/8512/comment-page-2#comment-29097): KL散度、JS散度、W距离啥的，都行啊，看你喜欢哪个
- [苏剑林](https://kexue.fm/archives/9119/comment-page-14#comment-29096): 没有绝对公平的对比方法，主要看你关心什么。比如，如果只关心推理成本和推理效果，那么有的方法可以...

### 友情链接

- [Cool Papers](https://papers.cool/)
- [数学研发](https://bbs.emath.ac.cn/)
- [Seatop](http://www.seatop.com.cn/)
- [Xiaoxia](https://xiaoxia.org/)
- [积分表-网络版](https://kexue.fm/sci/integral/index.html)
- [丝路博傲](http://blog.dvxj.com/)
- [数学之家](http://www.2math.cn/)
- [有趣天文奇观](http://interesting-sky.china-vo.org/)
- [TwistedW](http://www.twistedwg.com/)
- [godweiyang](https://godweiyang.com/)
- [AI柠檬](https://blog.ailemon.net/)
- [王登科-DK博客](https://greatdk.com/)
- [ESON](https://blog.eson.org/)
- [枫之羽](https://fzhiy.net/)
- [coding-zuo](https://coding-zuo.github.io/)
- [博科园](https://www.bokeyuan.net/)
- [孔皮皮的博客](https://www.kppkkp.top/)
- [运鹏的博客](https://yunpengtai.top/)
- [jiming.site](https://jiming.site/)
- [OmegaXYZ](https://www.omegaxyz.com/)
- [EAI猩球](https://www.robotech.ink/)
- [文举的博客](https://liwenju0.com/)
- [申请链接](https://kexue.fm/links.html)

[![署名-非商业用途-保持一致](https://kexue.fm/usr/themes/geekg/images/cc.gif)](http://creativecommons.org/licenses/by-nc-nd/2.5/cn/) 本站采用创作共用版权协议，要求署名、非商业用途和保持一致。转载本站内容必须也遵循“ [署名-非商业用途-保持一致](http://creativecommons.org/licenses/by-nc-nd/2.5/cn/)”的创作共用协议。



© 2009-2026 Scientific Spaces. All rights reserved. Theme by [laogui](http://www.laogui.com/). Powered by [Typecho](http://typecho.org/). 备案号: [粤ICP备09093259号-1/2](https://beian.miit.gov.cn/ "粤ICP备09093259号")。