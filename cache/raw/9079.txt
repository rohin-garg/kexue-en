当BERT-whitening引入超参数：总有一款适合你 - 科学空间|Scientific Spaces
![MobileSideBar](https://kexue.fm/usr/themes/geekg/images/slide-button.png "MobileSideBar")
## SEARCH
## MENU
* [打赏](https://kexue.fm/reward.html)
* [公式](https://kexue.fm/latex.html)
* [天象](https://kexue.fm/ac.html)
* [链接](https://kexue.fm/links.html)
* [时光](https://kexue.fm/me.html)
* [博览](https://kexue.fm/science.html)
* [归档](https://kexue.fm/content.html)
## CATEGORIES
* [千奇百怪](https://kexue.fm/category/Everything)
* [天文探索](https://kexue.fm/category/Astronomy)
* [数学研究](https://kexue.fm/category/Mathematics)
* [物理化学](https://kexue.fm/category/Phy-chem)
* [信息时代](https://kexue.fm/category/Big-Data)
* [生物自然](https://kexue.fm/category/Biology)
* [图片摄影](https://kexue.fm/category/Photograph)
* [问题百科](https://kexue.fm/category/Questions)
* [生活/情感](https://kexue.fm/category/Life-Feeling)
* [资源共享](https://kexue.fm/category/Resources)
## NEWPOSTS
* [让炼丹更科学一些（五）：基于梯度精...](https://kexue.fm/archives/11530)
* [让炼丹更科学一些（四）：新恒等式，...](https://kexue.fm/archives/11494)
* [为什么DeltaNet要加L2 N...](https://kexue.fm/archives/11486)
* [让炼丹更科学一些（三）：SGD的终...](https://kexue.fm/archives/11480)
* [让炼丹更科学一些（二）：将结论推广...](https://kexue.fm/archives/11469)
* [滑动平均视角下的权重衰减和学习率](https://kexue.fm/archives/11459)
* [生成扩散模型漫谈（三十一）：预测数...](https://kexue.fm/archives/11428)
* [Muon优化器指南：快速上手与关键细节](https://kexue.fm/archives/11416)
* [AdamW的Weight RMS的...](https://kexue.fm/archives/11404)
* [n个正态随机数的最大值的渐近估计](https://kexue.fm/archives/11390)
## COMMENTS
* [Rapture D: 我有一个问题，为什么不考虑亥姆霍兹定理和斯托克斯公式。](https://kexue.fm/archives/11530/comment-page-1#comment-29104)
* [mofheka: 苏神是还在用jax是么？最近在做基于Google Pathwa...](https://kexue.fm/archives/11390/comment-page-1#comment-29103)
* [长琴: 看懂这篇博客也不是一件容易的事情。](https://kexue.fm/archives/11530/comment-page-1#comment-29102)
* [AlexLi: 苏老师，请教一下(7)式中将 $\\mu(x\_t)$ 传给$p...](https://kexue.fm/archives/9257/comment-page-4#comment-29101)
* [tyler\_zxc: "Performer的思想是将标准的Attention线性化，...](https://kexue.fm/archives/7921/comment-page-2#comment-29100)
* [我: 似乎并非mHC提出矩阵的思想？之前hyper connecti...](https://kexue.fm/archives/11494/comment-page-1#comment-29099)
* [winter: 苏神您好，假如对于比较均匀的attention weightP...](https://kexue.fm/archives/10847/comment-page-1#comment-29098)
* [苏剑林: KL散度、JS散度、W距离啥的，都行啊，看你喜欢哪个](https://kexue.fm/archives/8512/comment-page-2#comment-29097)
* [苏剑林: 没有绝对公平的对比方法，主要看你关心什么。比如，如果只关心推理...](https://kexue.fm/archives/9119/comment-page-14#comment-29096)
* [苏剑林: 如果我有时间重新搭建博客，应该会用python自己写了，而不用...](https://kexue.fm/links.html/comment-page-6#comment-29095)
## USERLOGIN
* [登录](https://kexue.fm/admin/login.php)
[科学空间|Scientific Spaces](https://kexue.fm)
* [登录](https://kexue.fm/admin/login.php)
* [打赏](https://kexue.fm/reward.html)
* [公式](https://kexue.fm/latex.html)
* [天象](https://kexue.fm/ac.html)
* [链接](https://kexue.fm/links.html)
* [时光](https://kexue.fm/me.html)
* [博览](https://kexue.fm/science.html)
* [归档](https://kexue.fm/content.html)
渴望成为一个小飞侠* [![](https://kexue.fm/usr/themes/geekg/images/rss.png)
欢迎订阅](https://kexue.fm/feed)
* [![](https://kexue.fm/usr/themes/geekg/images/mail.png)
个性邮箱](https://kexue.fm/archives/119)
* [![](https://kexue.fm/usr/themes/geekg/images/Saturn.png)
天象信息](https://kexue.fm/ac.html)
* [![](https://kexue.fm/usr/themes/geekg/images/iss.png)
观测ISS](https://kexue.fm/archives/41)
* [![](https://kexue.fm/usr/themes/geekg/images/pi.png)
LaTeX](https://kexue.fm/latex.html)
* [![](https://kexue.fm/usr/themes/geekg/images/mlogo.png)
关于博主](https://kexue.fm/me.html)
欢迎访问“科学空间”，这里将与您共同探讨自然科学，回味人生百态；也期待大家的分享～* [**千奇百怪**Everything](https://kexue.fm/category/Everything)
* [**天文探索**Astronomy](https://kexue.fm/category/Astronomy)
* [**数学研究**Mathematics](https://kexue.fm/category/Mathematics)
* [**物理化学**Phy-chem](https://kexue.fm/category/Phy-chem)
* [**信息时代**Big-Data](https://kexue.fm/category/Big-Data)
* [**生物自然**Biology](https://kexue.fm/category/Biology)
* [**图片摄影**Photograph](https://kexue.fm/category/Photograph)
* [**问题百科**Questions](https://kexue.fm/category/Questions)
* [**生活/情感**Life-Feeling](https://kexue.fm/category/Life-Feeling)
* [**资源共享**Resources](https://kexue.fm/category/Resources)
* [**千奇百怪**](https://kexue.fm/category/Everything)
* [**天文探索**](https://kexue.fm/category/Astronomy)
* [**数学研究**](https://kexue.fm/category/Mathematics)
* [**物理化学**](https://kexue.fm/category/Phy-chem)
* [**信息时代**](https://kexue.fm/category/Big-Data)
* [**生物自然**](https://kexue.fm/category/Biology)
* [**图片摄影**](https://kexue.fm/category/Photograph)
* [**问题百科**](https://kexue.fm/category/Questions)
* [**生活/情感**](https://kexue.fm/category/Life-Feeling)
* [**资源共享**](https://kexue.fm/category/Resources)
[首页](https://kexue.fm)[信息时代](https://kexue.fm/category/Big-Data)当BERT-whitening引入超参数：总有一款适合你
18May
# [当BERT-whitening引入超参数：总有一款适合你](https://kexue.fm/archives/9079)
By苏剑林|2022-05-18|61741位读者|:
在[《你可能不需要BERT-flow：一个线性变换媲美BERT-flow》](https://kexue.fm/archives/8069)中，笔者提出了BERT-whitening，验证了一个线性变换就能媲美当时的SOTA方法BERT-flow。此外，BERT-whitening还可以对句向量进行降维，带来更低的内存占用和更快的检索速度。然而，在[《无监督语义相似度哪家强？我们做了个比较全面的评测》](https://kexue.fm/archives/8321)中我们也发现，whitening操作并非总能带来提升，有些模型本身就很贴合任务（如经过有监督训练的SimBERT），那么额外的whitening操作往往会降低效果。
为了弥补这个不足，本文提出往BERT-whitening中引入了两个超参数，通过调节这两个超参数，我们几乎可以总是获得“降维不掉点”的结果。换句话说，即便是原来加上whitening后效果会下降的任务，如今也有机会在降维的同时获得相近甚至更好的效果了。
## 方法概要[#](#方法概要)
目前BERT-whitening的流程是：
\\begin{equation}\\begin{aligned}
\\tilde{\\boldsymbol{x}}\_i =&\, (\b&\, (\boldsymbol{x}\_i - \\boldsymbol{\\mu})\\boldsymbol{U}\\boldsymbol{\\Lambda}^{-1/2} \\\\
\\boldsymbol{\\mu} =&\, \fr&\, \frac{1}{N}\\sum\\limits\_{i=1}^N \\boldsymbol{x}\_i \\\\
\\boldsymbol{\\Sigma} =&\, \fr&\, \frac{1}{N}\\sum\\limits\_{i=1}^N (\\boldsymbol{x}\_i - \\boldsymbol{\\mu})^{\\top}(\\boldsymbol{x}\_i - \\boldsymbol{\\mu}) = \\boldsymbol{U}\\boldsymbol{\\Lambda}\\boldsymbol{U}^{\\top} \\,\\,(\\text{SVD分解})
\\end{aligned}\\end{equation}
其中$\\boldsymbol{x}\_i$是给定的句向量（如无说明，向量默认为行向量），$\\tilde{\\boldsymbol{x}}\_i$是变换后的向量，SVD分解的结果中，$\\boldsymbol{U}$是正交矩阵，$\\boldsymbol{\\Lambda}$是对角矩阵，并且对角线的元素非负且从大到小排列。可以看到，目前的流程是完全固定的，即没有任何可调的超参数。
为了增加一定的调节空间，我们可以往里边引入两个超参数$\\beta,\\gamma$（标量），使其变为
\\begin{equation}\\begin{aligned}
\\tilde{\\boldsymbol{x}}\_i =&\, (\b&\, (\boldsymbol{x}\_i - {\\color{red}\\beta}\\boldsymbol{\\mu})\\boldsymbol{U}\\boldsymbol{\\Lambda}^{-{\\color{red}\\gamma}/2} \\\\
\\boldsymbol{\\mu} =&\, \fr&\, \frac{1}{N}\\sum\\limits\_{i=1}^N \\boldsymbol{x}\_i \\\\
\\boldsymbol{\\Sigma} =&\, \fr&\, \frac{1}{N}\\sum\\limits\_{i=1}^N (\\boldsymbol{x}\_i - {\\color{red}\\beta}\\boldsymbol{\\mu})^{\\top}(\\boldsymbol{x}\_i - {\\color{red}\\beta}\\boldsymbol{\\mu}) = \\boldsymbol{U}\\boldsymbol{\\Lambda}\\boldsymbol{U}^{\\top} \\,\\,(\\text{SVD分解})
\\end{aligned}\\end{equation}
## 思路分析[#](#思路分析)
可以看到，当$\\beta=\\gamma=1$时，就是原来的BERT-whitening；而当$\\beta=\\gamma=0$时，净变换就是
\\begin{equation}\\tilde{\\boldsymbol{x}}\_i =\\boldsymbol{x}\_i \\boldsymbol{U}\\end{equation}
由于$\\boldsymbol{U}$是正交矩阵，所以不改变内积结果，即$\\tilde{\\boldsymbol{x}}\_i\\tilde{\\boldsymbol{x}}\_i^{\\top} = \\boldsymbol{x}\_i \\boldsymbol{U} (\\boldsymbol{x}\_i \\boldsymbol{U})^{\\top} = \\boldsymbol{x}\_i\\boldsymbol{x}\_i^{\\top}$，所以当我们用余弦相似度作为相似度量时，它不会改变原有结果。换句话说，引入这组超参数后，它提供了“不逊色于变换前的效果”的可能性，那么当我们精调这组参数时，就有可能取得比变换前更好的效果。这也是这两个超参数的设计思路。
此外，在这样的改动之下，原来的降维能力还是得以保留的。我们可以将变换拆开为两部分看：\\begin{equation}\\tilde{\\boldsymbol{x}}\_i = \\color{red}{\\underbrace{(\\boldsymbol{x}\_i - \\beta\\boldsymbol{\\mu})\\boldsymbol{U}}\_{\\text{part 1}}}\\color{skyblue}{\\underbrace{\\boldsymbol{\\Lambda}^{-\\gamma/2}}\_{\\text{part 2}}}\\end{equation}
第一部分主要是正交变换$\\boldsymbol{U}$，$\\boldsymbol{U}$是$\\boldsymbol{\\Sigma}$矩阵SVD分解之后的结果，它能将向量$\\boldsymbol{x}\_i - \\beta\\boldsymbol{\\mu}$变换成每个分量尽量独立的新向量，并且新向量的每个分量与0的平均波动正好是由$\\boldsymbol{\\Lambda}^{1/2}$的对角线元素来衡量，如果对应的波动很接近于0，那么我们就可以认为它实际就是0，舍去这个分量也不会影响余弦值的计算结果，这就是降维的原理。而由于SVD分解的结果已经提前将$\\boldsymbol{\\Lambda}$从大到小排好了顺序，因此我们可以直接通过保留前$k$维的操作$\\tilde{\\boldsymbol{x}}\_i\\text{[:}k\\text{]}$就可以实现降到$k$维了。
至于第二部分$\\boldsymbol{\\Lambda}^{-\\gamma/2}$，我们可以理解为当前任务对各向同性的依赖程度，如果$\\gamma=1$，那么相当于每个分量都是各平权的，这可以作为一个无监督的先验结果，但未必对所有任务都是最优的，所以我们可以通过调节$\\gamma$来更好地适应当前任务。
## 实验结果[#](#实验结果)
文章[《无监督语义相似度哪家强？我们做了个比较全面的评测》](https://kexue.fm/archives/8321)已经显示，在ATEC、BQ、LCQMC三个任务上，SimBERT加上默认的whitening操作（即$\\beta=\\gamma=1$）都会导致效果下降，而如果我们取$\\beta=\\gamma=0$，那么结果就不一样了（随便演示了两个组合，其他组合结果相似）：
$$\\small{\\begin{array}{c}
\\text{BERT-P4效果表} \\\\
{\\begin{array}{l|ccccc}
\\hline
& \text& \text{ATEC} & \text& \text{BQ} & \text& \text{LCQMC} & \text& \text{PAWSX} & \text& \text{STS-B} \\\\
\\hline
\\beta=\\gamma=1 & 24.51& 24.51 / \\color{green}{27.00} / \\color{green}{27.91} & 38.81& 38.81 / \\color{red}{32.29} / \\color{red}{37.67} & 64.75& 64.75 / \\color{green}{64.75} / \\color{green}{65.65} & 15.12& 15.12 / \\color{green}{17.80} / \\color{green}{15.34} & 61.66& 61.66 / \\color{green}{69.45} / \\color{green}{69.37}
\\\\
\\beta=\\gamma=0 & 24.51& 24.51 / 24.51 / \\color{green}{24.59} & 38.81& 38.81 / 38.81 / \\color{green}{38.99} & 64.75& 64.75 / 64.75 / \\color{red}{63.45} & 15.12& 15.12 / 15.12 / \\color{red}{14.59} & 61.66& 61.66 / 61.66 / \\color{green}{62.30} \\\\
\\hline
\\end{array}} \\\\
\\\\
\\text{SimBERT-P1效果表} \\\\
{\\begin{array}{l|ccccc}
\\hline
& \text& \text{ATEC} & \text& \text{BQ} & \text& \text{LCQMC} & \text& \text{PAWSX} & \text& \text{STS-B} \\\\
\\hline
\\beta=\\gamma=1 & 38.50& 38.50 / \\color{red}{23.64} / \\color{red}{30.79} & 48.54& 48.54 / \\color{red}{31.78} / \\color{red}{40.01} & 76.23& 76.23 / \\color{red}{75.05} / \\color{red}{74.50} & 15.10& 15.10 / \\color{green}{18.49} / \\color{green}{15.64} & 74.14& 74.14 / \\color{red}{73.37} / \\color{green}{75.29} \\\\
\\beta=\\gamma=0 & 38.50& 38.50 / 38.50 / \\color{green}{38.81} & 48.54& 48.54 / 48.54 / \\color{green}{48.66} & 76.23& 76.23 / 76.23 / \\color{red}{76.22} & 15.10& 15.10 / 15.10 / \\color{red}{14.88} & 74.14& 74.14 / 74.14 / \\color{green}{74.46} \\\\
\\hline
\\end{array}}
\\end{array}}$$
跟之前的文章一样，表格中的每个元素是$a / b / c$的形式，代表该任务在该模型下“不加whitening”的得分为$a$、“加whitening”的得分为$b$、“加whitening并降到256维”的得分为$c$；如果$b \> a$，那么$b$显示为绿色，小于则显示为红色；如果$c \> a$，那么$c$显示为绿色，小于则显示为红色。前面说了，如果不降维的话，$\\beta=\\gamma=0$的净变换就是$\\boldsymbol{U}$，不改变余弦值结果，因此$\\beta=\\gamma=0$时的$a,b$都是相等的。
在这个表格中，我们主要看$a/b/c$中的第三个结果$c$，它是将向量从768维降低到256维的结果，可以看到当$\\beta=\\gamma=0$时，不管是无监督的BERT还是有监督的SimBERT，该结果基本都很接近原始向量的结果（即$a$），部分结果甚至还有提升。这就意味着，$\\beta=\\gamma=0,k=256$这个组合几乎可以算是“免费的午餐”，几乎无损效果，并且实现了降维。
笔者也试过精调$\\beta,\\gamma$，在一些任务上确实能取得比上述两个组合更好的效果，但精调需要标签数据，争议性可能会比较大，这里就不演示了。如果原来的句向量模型本就是有监督训练得到的，用BERT-whitening仅仅是奔着降维去的，那么就可以用验证集来精调一下$\\beta,\\gamma$和$k$了，这种场景下就是无争议的了。
## 文章小结[#](#文章小结)
本文通过引入两个超参数的方式来赋予BERT-whitening一定的调参空间，使其具备“不逊色于变换前的效果”的可能性，并且保留了降维的能力。换言之，即便是之前已经训练好的句向量模型，我们也可以用新的BERT-whitening将它降维，并且保持效果基本不变，有时候甚至还更优～
***转载到请包括本文地址：** [https://kexue.fm/archives/9079](https://kexue.fm/archives/9079)*
***更详细的转载事宜请参考：*** [《科学空间FAQ》](https://kexue.fm/archives/6508#文章如何转载/引用)
**如果您还有什么疑惑或建议，欢迎在下方评论区继续讨论。**
**如果您觉得本文还不错，欢迎[分享](#share)/[打赏](#pay)本文。打赏并非要从中获得收益，而是希望知道科学空间获得了多少读者的真心关注。当然，如果你无视它，也不会影响你的阅读。再次表示欢迎和感谢！**
打赏![科学空间](https://kexue.fm/usr/themes/geekg/payment/wx.png)
微信打赏![科学空间](https://kexue.fm/usr/themes/geekg/payment/zfb.png)
支付宝打赏因为网站后台对打赏并无记录，因此欢迎在打赏时候备注留言。 你还可以[**点击这里**](http://mail.qq.com/cgi-bin/qm_share?t=qm_mailme&email=tN7d1drY3drrx8H0xcWa19vZ)或在下方评论区留言来告知你的建议或需求。
**如果您需要引用本文，请参考：**
苏剑林. (May. 18, 2022). 《当BERT-whitening引入超参数：总有一款适合你 》[Blog post]. Retrieved from[https://kexue.fm/archives/9079](https://kexue.fm/archives/9079)
@online{kexuefm-9079,
title={当BERT-whitening引入超参数：总有一款适合你},
author={苏剑林},
year={2022},
month={May},
url={\\url{https://kexue.fm/archives/9079}},
}
分类：[信息时代](https://kexue.fm/category/Big-Data) 标签：[语言模型](https://kexue.fm/tag/语言模型/),[语义](https://kexue.fm/tag/语义/),[语义相似度](https://kexue.fm/tag/语义相似度/)[14 评论](https://kexue.fm/archives/9079#comments)
&lt;[logsumexp运算的几个不等式](https://kexue.fm/archives/9070)|[从重参数的角度看离散概率分布的构建](https://kexue.fm/archives/9085)&gt;
### 你也许还对下面的内容感兴趣* [Transformer升级之路：21、MLA好在哪里?（下）](https://kexue.fm/archives/11111)
* [Transformer升级之路：20、MLA好在哪里?（上）](https://kexue.fm/archives/10907)
* [Transformer升级之路：19、第二类旋转位置编码](https://kexue.fm/archives/10862)
* [Decoder-only的LLM为什么需要位置编码？](https://kexue.fm/archives/10347)
* [Monarch矩阵：计算高效的稀疏型矩阵分解](https://kexue.fm/archives/10249)
* [缓存与效果的极限拉扯：从MHA、MQA、GQA到MLA](https://kexue.fm/archives/10091)
* [时空之章：将Attention视为平方复杂度的RNN](https://kexue.fm/archives/10017)
* [我在Performer中发现了Transformer-VQ的踪迹](https://kexue.fm/archives/9862)
* [预训练一下，Transformer的长序列成绩还能涨不少！](https://kexue.fm/archives/9787)
* [脑洞大开：非线性RNN居然也可以并行计算？](https://kexue.fm/archives/9783)
[发表你的看法](#comment_form)
孜杰May 18th, 2022
想请教一下苏神，β=1，γ=0时，这个变换和PCA变换是不是非常接近了？
[回复评论](https://kexue.fm/archives/9079/comment-page-1?replyTo=19147#respond-post-9079)
[苏剑林](https://kexue.fm)发表于 May 18th, 2022
是的。[回复评论](https://kexue.fm/archives/9079/comment-page-1?replyTo=19148#respond-post-9079)
保持耐心May 22nd, 2022
苏神，请教您两个问题：1、如果是用蛋白质氨基酸序列数据预训练的出来的bert模型，输出每个字符的特征长度是1280，可以用这个bert whitening降维吗，还是说这个bert whitening只能用在nlp领域内的普通bert？
2、有github地址吗，想看一下这两个超参数为0的实现方式
非常感谢了[回复评论](https://kexue.fm/archives/9079/comment-page-1?replyTo=19166#respond-post-9079)
[苏剑林](https://kexue.fm)发表于 May 24th, 2022
1、whitening适用于基于欧氏距离或者cos检索的向量降维；
2、BERT-whitening已经开源，直接在它上面改就行。
[回复评论](https://kexue.fm/archives/9079/comment-page-1?replyTo=19171#respond-post-9079)
保持耐心发表于May 24th, 2022
好的，谢谢苏神！[回复评论](https://kexue.fm/archives/9079/comment-page-1?replyTo=19175#respond-post-9079)
zhuwenq
May 23rd, 2022
苏神你好，请问BERT-Whitening 操作是针对语义相似度问题或者说是向量之间的余弦相似度问题提出的吗？还是说BERT-Whitening 操作特别是利用SVD 实现的降维操作实际上可以推广到其他任务呢？您的关于BERT-Whitening 的文章对我很有帮助，非常感谢。[回复评论](https://kexue.fm/archives/9079/comment-page-1?replyTo=19168#respond-post-9079)
[苏剑林](https://kexue.fm)发表于 May 24th, 2022
降维是必然会丢失某些信息的，但有可能保留我们关心的那个信息。对于BERT-whitening来说，我们关心的信息就是内积、余弦或者欧氏距离的检索相似度，在BERT-whitening之下可以达到降维的效果，并且保持我们关心的相似度不变甚至有所提升。
[回复评论](https://kexue.fm/archives/9079/comment-page-1?replyTo=19174#respond-post-9079)
Jarence
June 2nd, 2022
苏神你好，请问一下利用标签精调$\\beta,\\gamma$的思路是什么？非常感谢。
[回复评论](https://kexue.fm/archives/9079/comment-page-1?replyTo=19226#respond-post-9079)
[苏剑林](https://kexue.fm)发表于 August 2nd, 2022
思路就是调到效果最优～[回复评论](https://kexue.fm/archives/9079/comment-page-1?replyTo=19571#respond-post-9079)
chrislqpaul
July 31st, 2022
苏神你好，我想问一下bert-whitening适合在一些字符级分类任务中给基于BERT的词向量做降维处理吗
[回复评论](https://kexue.fm/archives/9079/comment-page-1?replyTo=19564#respond-post-9079)
[苏剑林](https://kexue.fm)发表于 August 2nd, 2022
适合与否，这得看实际实验效果呀。[回复评论](https://kexue.fm/archives/9079/comment-page-1?replyTo=19570#respond-post-9079)
mghstar
August 8th, 2022
苏神你好，我基于你这篇bert-whitening，很好地优化了我司的语义召回模型：经过压缩后的向量在粗召回时几乎没有折损，耗时却大大减少，前来感谢
[回复评论](https://kexue.fm/archives/9079/comment-page-1?replyTo=19597#respond-post-9079)
[苏剑林](https://kexue.fm)发表于 August 10th, 2022
恭喜[回复评论](https://kexue.fm/archives/9079/comment-page-1?replyTo=19613#respond-post-9079)
[句向量表示\_Johngo学长](https://www.johngo689.com/556354/)
June 1st, 2023
[...]whitening操作并非总能带来提升，有些模型本身就很贴合任务（如经过有监督训练的SimBERT），那么额外的whitening操作往往会降低效果。BERT-whitening中引入了两个超参数，通过调节这两个超参数，我们几乎可以总是获得”降维不掉点”的结果第一部分主要是正交变换U，U是Σ矩阵SVD分解之后的结果，它能将向量xi−βμ变换成每个分量尽量独立的新向量，并且新向量的每个分量与0的平均[...]
[回复评论](https://kexue.fm/archives/9079/comment-page-1?replyTo=21853#respond-post-9079)
[取消回复](https://kexue.fm/archives/9079#respond-post-9079)
你的大名电子邮箱个人网站（选填）1. 可以使用LaTeX代码，点击“预览效果”可查看效果；
2. 可以通过点击评论楼层编号来引用该楼层；3. 网站可能会有点卡，如非确认评论失败，请**不要重复点击提交**。
********************
### 内容速览* [方法概要](#方法概要)
* [思路分析](#思路分析)
* [实验结果](#实验结果)
* [文章小结](#文章小结)
********************
### 智能搜索支持整句搜索！网站自动使用[结巴分词](https://github.com/fxsjy/jieba)进行分词，并结合ngrams排序算法给出合理的搜索结果。
********************
### 热门标签[生成模型](https://kexue.fm/tag/生成模型/)[attention](https://kexue.fm/tag/attention/)[优化](https://kexue.fm/tag/优化/)[语言模型](https://kexue.fm/tag/语言模型/)[模型](https://kexue.fm/tag/模型/)[梯度](https://kexue.fm/tag/梯度/)[网站](https://kexue.fm/tag/网站/)[概率](https://kexue.fm/tag/概率/)[矩阵](https://kexue.fm/tag/矩阵/)[优化器](https://kexue.fm/tag/优化器/)[转载](https://kexue.fm/tag/转载/)[微分方程](https://kexue.fm/tag/微分方程/)[分析](https://kexue.fm/tag/分析/)[天象](https://kexue.fm/tag/天象/)[深度学习](https://kexue.fm/tag/深度学习/)[积分](https://kexue.fm/tag/积分/)[python](https://kexue.fm/tag/python/)[扩散](https://kexue.fm/tag/扩散/)[力学](https://kexue.fm/tag/力学/)[无监督](https://kexue.fm/tag/无监督/)[几何](https://kexue.fm/tag/几何/)[节日](https://kexue.fm/tag/节日/)[生活](https://kexue.fm/tag/生活/)[文本生成](https://kexue.fm/tag/文本生成/)[数论](https://kexue.fm/tag/数论/)
********************
********************
### 随机文章* [logsumexp运算的几个不等式](https://kexue.fm/archives/9070)
* [缓存与效果的极限拉扯：从MHA、MQA、GQA到MLA](https://kexue.fm/archives/10091)
* [话说金属活动性顺序](https://kexue.fm/archives/89)
* [最新调查解“毒”珠江：工业水污染触目惊心！](https://kexue.fm/archives/222)
* [OCR技术浅探：3. 特征提取(1)](https://kexue.fm/archives/3785)
* [封闭曲线所围成的面积：一个新技巧](https://kexue.fm/archives/3441)
* [2018年全年天象](https://kexue.fm/archives/4760)
* [级数求和——近似的无穷级数](https://kexue.fm/archives/922)
* [费曼积分法——积分符号内取微分(3)](https://kexue.fm/archives/1629)
* [高斯型积分的微扰展开（一）](https://kexue.fm/archives/3217)
********************
********************
### 最近评论* [Rapture D](https://kexue.fm/archives/11530/comment-page-1#comment-29104): 我有一个问题，为什么不考虑亥姆霍兹定理和斯托克斯公式。* [mofheka](https://kexue.fm/archives/11390/comment-page-1#comment-29103): 苏神是还在用jax是么？最近在做基于Google Pathway的理念做一个动态版的MPMD框...
* [长琴](https://kexue.fm/archives/11530/comment-page-1#comment-29102): 看懂这篇博客也不是一件容易的事情。* [AlexLi](https://kexue.fm/archives/9257/comment-page-4#comment-29101): 苏老师，请教一下(7)式中将 $\\mu(x\_t)$ 传给$p\_o$ 进行推理的操作。$x\_...
* [tyler\_zxc](https://kexue.fm/archives/7921/comment-page-2#comment-29100): "Performer的思想是将标准的Attention线性化，所以为什么不干脆直接训练一个线性...
* [我](https://kexue.fm/archives/11494/comment-page-1#comment-29099): 似乎并非mHC提出矩阵的思想？之前hyper connection就是了
* [winter](https://kexue.fm/archives/10847/comment-page-1#comment-29098): 苏神您好，假如对于比较均匀的attention weightP，往往呈现long tail分布...
* [苏剑林](https://kexue.fm/archives/8512/comment-page-2#comment-29097): KL散度、JS散度、W距离啥的，都行啊，看你喜欢哪个
* [苏剑林](https://kexue.fm/archives/9119/comment-page-14#comment-29096): 没有绝对公平的对比方法，主要看你关心什么。比如，如果只关心推理成本和推理效果，那么有的方法可以...
* [苏剑林](https://kexue.fm/links.html/comment-page-6#comment-29095): 如果我有时间重新搭建博客，应该会用python自己写了，而不用第三方架构，这样可玩性好很多。事...
********************
********************
### 友情链接* [Cool Papers](https://papers.cool)
* [数学研发](https://bbs.emath.ac.cn)
* [Seatop](http://www.seatop.com.cn/)
* [Xiaoxia](https://xiaoxia.org/)
* [积分表-网络版](https://kexue.fm/sci/integral/index.html)
* [丝路博傲](http://blog.dvxj.com/)
* [数学之家](http://www.2math.cn/)
* [有趣天文奇观](http://interesting-sky.china-vo.org/)
* [TwistedW](http://www.twistedwg.com/)
* [godweiyang](https://godweiyang.com/)
* [AI柠檬](https://blog.ailemon.net/)
* [王登科-DK博客](https://greatdk.com)
* [ESON](https://blog.eson.org/)
* [枫之羽](https://fzhiy.net/)
* [coding-zuo](https://coding-zuo.github.io/)
* [博科园](https://www.bokeyuan.net/)
* [孔皮皮的博客](https://www.kppkkp.top/)
* [运鹏的博客](https://yunpengtai.top/)
* [jiming.site](https://jiming.site/)
* [OmegaXYZ](https://www.omegaxyz.com/)
* [EAI猩球](https://www.robotech.ink/)
* [文举的博客](https://liwenju0.com/)
* [申请链接](https://kexue.fm/links.html)
********************
[![署名-非商业用途-保持一致](https://kexue.fm/usr/themes/geekg/images/cc.gif)](http://creativecommons.org/licenses/by-nc-nd/2.5/cn/)本站采用创作共用版权协议，要求署名、非商业用途和保持一致。转载本站内容必须也遵循“[署名-非商业用途-保持一致](http://creativecommons.org/licenses/by-nc-nd/2.5/cn/)”的创作共用协议。
©2009-2026 Scientific Spaces. All rights reserved. Theme by[laogui](http://www.laogui.com). Powered by[Typecho](http://typecho.org). 备案号:[粤ICP备09093259号-1/2](https://beian.miit.gov.cn/)。