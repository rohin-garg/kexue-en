## SEARCH

## MENU

- [打赏](https://kexue.fm/reward.html)
- [公式](https://kexue.fm/latex.html)
- [天象](https://kexue.fm/ac.html)
- [链接](https://kexue.fm/links.html)
- [时光](https://kexue.fm/me.html)
- [博览](https://kexue.fm/science.html)
- [归档](https://kexue.fm/content.html)

## CATEGORIES

- [千奇百怪](https://kexue.fm/category/Everything)
- [天文探索](https://kexue.fm/category/Astronomy)
- [数学研究](https://kexue.fm/category/Mathematics)
- [物理化学](https://kexue.fm/category/Phy-chem)
- [信息时代](https://kexue.fm/category/Big-Data)
- [生物自然](https://kexue.fm/category/Biology)
- [图片摄影](https://kexue.fm/category/Photograph)
- [问题百科](https://kexue.fm/category/Questions)
- [生活/情感](https://kexue.fm/category/Life-Feeling)
- [资源共享](https://kexue.fm/category/Resources)

## NEWPOSTS

- [为什么DeltaNet要加L2 N...](https://kexue.fm/archives/11486)
- [让炼丹更科学一些（三）：SGD的终...](https://kexue.fm/archives/11480)
- [让炼丹更科学一些（二）：将结论推广...](https://kexue.fm/archives/11469)
- [滑动平均视角下的权重衰减和学习率](https://kexue.fm/archives/11459)
- [生成扩散模型漫谈（三十一）：预测数...](https://kexue.fm/archives/11428)
- [Muon优化器指南：快速上手与关键细节](https://kexue.fm/archives/11416)
- [AdamW的Weight RMS的...](https://kexue.fm/archives/11404)
- [n个正态随机数的最大值的渐近估计](https://kexue.fm/archives/11390)
- [流形上的最速下降：5\. 对偶梯度下降](https://kexue.fm/archives/11388)
- [低精度Attention可能存在有...](https://kexue.fm/archives/11371)

## COMMENTS

- [kaiyuan: 看了“Linear Transformers Are Secr...](https://kexue.fm/archives/11486/comment-page-1#comment-29036)
- [sog: 好的，符号相同，搞混了呃](https://kexue.fm/archives/11469/comment-page-1#comment-29035)
- [kerry: 还没有通读完后面的系列，提出一些拙见。\
降低方差这一节把原本的...](https://kexue.fm/archives/9119/comment-page-14#comment-29034)
- [Kevin Yin: I wrote https://research.novela...](https://kexue.fm/archives/11158/comment-page-1#comment-29033)
- [罗: 公式(6)显示出来是不是有点小问题？](https://kexue.fm/archives/11480/comment-page-1#comment-29032)
- [cmlin: 本人对这方面不太熟悉，想了解这三个条件的意义及动机，且希望这系...](https://kexue.fm/archives/11340/comment-page-1#comment-29031)
- [喝一口可乐: 理解了，感谢苏神回复，数学上给出建模分析确实清晰了很多，再次感...](https://kexue.fm/archives/10958/comment-page-3#comment-29030)
- [CuddleSabe1: 感觉普通的 flow matching 可以看成 degrad...](https://kexue.fm/archives/10958/comment-page-1#comment-29029)
- [岁月如书: 受教了，感谢](https://kexue.fm/archives/11126/comment-page-3#comment-29028)
- [苏剑林: 是](https://kexue.fm/archives/11126/comment-page-3#comment-29027)

## USERLOGIN

- [登录](https://kexue.fm/admin/login.php)

[科学空间\|Scientific Spaces](https://kexue.fm)

- [登录](https://kexue.fm/admin/login.php)
- [打赏](https://kexue.fm/reward.html)
- [公式](https://kexue.fm/latex.html)
- [天象](https://kexue.fm/ac.html)
- [链接](https://kexue.fm/links.html)
- [时光](https://kexue.fm/me.html)
- [博览](https://kexue.fm/science.html)
- [归档](https://kexue.fm/content.html)

渴望成为一个小飞侠

- [欢迎订阅](https://kexue.fm/feed)
- [个性邮箱](https://kexue.fm/archives/119)
- [天象信息](https://kexue.fm/ac.html)
- [观测ISS](https://kexue.fm/archives/41)
- [LaTeX](https://kexue.fm/latex.html)
- [关于博主](https://kexue.fm/me.html)

欢迎访问“科学空间”，这里将与您共同探讨自然科学，回味人生百态；也期待大家的分享～

- [**千奇百怪** Everything](https://kexue.fm/category/Everything)
- [**天文探索** Astronomy](https://kexue.fm/category/Astronomy)
- [**数学研究** Mathematics](https://kexue.fm/category/Mathematics)
- [**物理化学** Phy-chem](https://kexue.fm/category/Phy-chem)
- [**信息时代** Big-Data](https://kexue.fm/category/Big-Data)
- [**生物自然** Biology](https://kexue.fm/category/Biology)
- [**图片摄影** Photograph](https://kexue.fm/category/Photograph)
- [**问题百科** Questions](https://kexue.fm/category/Questions)
- [**生活/情感** Life-Feeling](https://kexue.fm/category/Life-Feeling)
- [**资源共享** Resources](https://kexue.fm/category/Resources)

- [**千奇百怪**](https://kexue.fm/category/Everything)
- [**天文探索**](https://kexue.fm/category/Astronomy)
- [**数学研究**](https://kexue.fm/category/Mathematics)
- [**物理化学**](https://kexue.fm/category/Phy-chem)
- [**信息时代**](https://kexue.fm/category/Big-Data)
- [**生物自然**](https://kexue.fm/category/Biology)
- [**图片摄影**](https://kexue.fm/category/Photograph)
- [**问题百科**](https://kexue.fm/category/Questions)
- [**生活/情感**](https://kexue.fm/category/Life-Feeling)
- [**资源共享**](https://kexue.fm/category/Resources)

[首页](https://kexue.fm) [信息时代](https://kexue.fm/category/Big-Data) “让Keras更酷一些！”：层与模型的重用技巧

29Sep

# [“让Keras更酷一些！”：层与模型的重用技巧](https://kexue.fm/archives/6985)

By 苏剑林 \|
2019-09-29 \|
153200位读者\|

今天我们继续来深挖Keras，再次体验Keras那无与伦比的优雅设计。这一次我们的焦点是“重用”，主要是层与模型的重复使用。

所谓重用，一般就是奔着两个目标去：一是为了共享权重，也就是说要两个层不仅作用一样，还要共享权重，同步更新；二是避免重写代码，比如我们已经搭建好了一个模型，然后我们想拆解这个模型，构建一些子模型等。

## 基础 [\#](https://kexue.fm/kexue.fm\#%E5%9F%BA%E7%A1%80)

事实上，Keras已经为我们考虑好了很多，所以很多情况下，掌握好基本用法，就已经能满足我们很多需求了。

### 层的重用 [\#](https://kexue.fm/kexue.fm\#%E5%B1%82%E7%9A%84%E9%87%8D%E7%94%A8)

层的重用是最简单的，将层初始化好，存起来，然后反复调用即可：

```
x_in = Input(shape=(784,))
x = x_in

layer = Dense(784, activation='relu') # 初始化一个层，并存起来

x = layer(x) # 第一次调用
x = layer(x) # 再次调用
x = layer(x) # 再次调用
```

要注意的是，必须先初始化好一个层，存为一个变量好再调用，才能保证重复调用的层是共享权重的。反之，如果是下述形式的代码，则是非共享权重的：

```
x = Dense(784, activation='relu')(x)
x = Dense(784, activation='relu')(x) # 跟前面的不共享权重
x = Dense(784, activation='relu')(x) # 跟前面的不共享权重
```

### 模型重用 [\#](https://kexue.fm/kexue.fm\#%E6%A8%A1%E5%9E%8B%E9%87%8D%E7%94%A8)

Keras的模型有着类似层的表现，在调用时可以用跟层一样的方式，比如：

```
x_in = Input(shape=(784,))
x = x_in

x = Dense(10, activation='softmax')(x)

model = Model(x_in, x) # 建立模型

x_in = Input(shape=(100,))
x = x_in

x = Dense(784, activation='relu')(x)
x = model(x) # 将模型当层一样用

model2 = Model(x_in, x)
```

读过Keras源码的朋友就会明白，之所以可以将模型当层那样用，是因为 `Model` 本身就是继承 `Layer` 类来写的，所以模型自然也包含了层的一些相同特性。

### 模型克隆 [\#](https://kexue.fm/kexue.fm\#%E6%A8%A1%E5%9E%8B%E5%85%8B%E9%9A%86)

模型克隆跟模型重用类似，只不过得到的新模型跟原模型不共享权重了，也就是说，仅仅保留完全一样的模型结构，两个模型之间的更新是独立的。Keras提供了模型可用专用的函数，直接调用即可：

```
from keras.models import clone_model

model2 = clone_model(model1)
```

注意， `clone_model` 完全复制了原模型模型的结构，并重新构建了一个模型，但没有复制原模型的权重的值。也就是说，对于同样的输入， `model1.predict` 和 `model2.predict` 的结果是不一样的。

如果要把权重也搬过来，需要手动 `set_weights` 一下：

```
model2.set_weights(K.batch_get_value(model1.weights))
```

## 进阶 [\#](https://kexue.fm/kexue.fm\#%E8%BF%9B%E9%98%B6)

上述谈到的是原封不等的调用原来的层或模型，所以比较简单，Keras都准备好了。下面介绍一些复杂一些的例子。

### 交叉引用 [\#](https://kexue.fm/kexue.fm\#%E4%BA%A4%E5%8F%89%E5%BC%95%E7%94%A8)

这里的交叉引用是指在定义一个新层的时候，沿用已有的某个层的权重，注意这个自定义层可能跟旧层的功能完全不一样，它们之间纯粹是共享了某个权重而已。比如，Bert在训练MLM的时候，最后预测字词概率的全连接层，权重就是跟Embedding层共享的。

参考写法如下：

```
class EmbeddingDense(Layer):
 """运算跟Dense一致，只不过kernel用Embedding层的embedding矩阵
 """
 def __init__(self, embedding_layer, activation='softmax', **kwargs):
 super(EmbeddingDense, self).__init__(**kwargs)
 self.kernel = K.transpose(embedding_layer.embeddings)
 self.activation = activation
 self.units = K.int_shape(self.kernel)[1]

 def build(self, input_shape):
 super(EmbeddingDense, self).build(input_shape)
 self.bias = self.add_weight(name='bias',
 shape=(self.units,),
 initializer='zeros')

 def call(self, inputs):
 outputs = K.dot(inputs, self.kernel)
 outputs = K.bias_add(outputs, self.bias)
 outputs = Activation(self.activation).call(outputs)
 return outputs

 def compute_output_shape(self, input_shape):
 return input_shape[:-1] + (self.units,)

# 用法
embedding_layer = Embedding(10000, 128)
x = embedding_layer(x) # 调用Embedding层
x = EmbeddingDense(embedding_layer)(x) # 调用EmbeddingDense层
```

### 提取中间层 [\#](https://kexue.fm/kexue.fm\#%E6%8F%90%E5%8F%96%E4%B8%AD%E9%97%B4%E5%B1%82)

有时候我们需要从搭建好的模型中提取中间层的特征，并且构建一个新模型，在Keras中这同样是很简单的操作：

```
from keras.applications.resnet50 import ResNet50
model = ResNet50(weights='imagenet')

Model(
 inputs=model.input,
 outputs=[
 model.get_layer('res5a_branch1').output,
 model.get_layer('activation_47').output,
 ]
)
```

### 从中间拆开 [\#](https://kexue.fm/kexue.fm\#%E4%BB%8E%E4%B8%AD%E9%97%B4%E6%8B%86%E5%BC%80)

最后，来到本文最有难度的地方了，我们要将模型从中间拆开，搞懂之后也可以实现往已有模型插入或替换新层的操作。这个需求看上去比较奇葩，但是还别说， [stackoverflow上面还有人提问过](https://stackoverflow.com/questions/49492255/how-to-replace-or-insert-intermediate-layer-in-keras-model)，说明这确实是有价值的。

假设我们有一个现成的模型，它可以分解为
$$\\text{inputs}\\to h\_1 \\to h\_2 \\to h\_3 \\to h\_4 \\to \\text{outputs}$$
那可能我们需要将$h\_2$替换成一个新的输入，然后接上后面的层，来构建一个新模型，即新模型的功能是：
$$\\text{inputs} \\to h\_3 \\to h\_4 \\to \\text{outputs}$$
如果是 `Sequential` 类模型，那比较简单，直接把 `model.layers` 都遍历一边，就可以构建新模型了：

```
x_in = Input(shape=(100,))
x = x_in

for layer in model.layers[2:]:
 x = layer(x)

model2 = Model(x_in, x)

```

但是，如果模型是比较复杂的结构，比如残差结构这种不是一条路走到底的，就没有这么简单了。事实上，这个需求本来没什么难度，该写的Keras本身已经写好了，只不过没有提供现成的接口罢了。为什么这么说，因为我们通过 `model(x)` 这样的代码调用已有模型的时候，实际上Keras就相当于把这个已有的这个 `model` 从头到尾重新搭建了一遍，既然可以重建整个模型，那搭建“半个”模型原则上也是没有任技术难度的，只不过没有现成的接口。具体可以参考 [Keras源码](https://github.com/keras-team/keras/blob/master/keras/engine/network.py) 的 `keras/engine/network.py` 的 `run_internal_graph` 函数。

完整重建一个模型的逻辑在 `run_internal_graph` 函数里边，并且可以看到它还不算简单，所以如无必要我们最好不要重写这个代码。但如果不重写这个代码，又想调用这个代码，实现从中间层拆解模型的功能，唯一的办法是“移花接木”了：通过修改已有模型的一些属性，欺骗一下 `run_internal_graph` 函数，使得它以为模型的输入层是中间层，而不是原始的输入层。有了这个思想，再认真读读 `run_internal_graph` 函数的代码，就不难得到下述参考代码：

```
def get_outputs_of(model, start_tensors, input_layers=None):
 """start_tensors为开始拆开的位置
 """
 # 为此操作建立新模型
 model = Model(inputs=model.input,
 outputs=model.output,
 name='outputs_of_' + model.name)
 # 适配工作，方便使用
 if not isinstance(start_tensors, list):
 start_tensors = [start_tensors]
 if input_layers is None:
 input_layers = [
 Input(shape=K.int_shape(x)[1:], dtype=K.dtype(x))
 for x in start_tensors
 ]
 elif not isinstance(input_layers, list):
 input_layers = [input_layers]
 # 核心：覆盖模型的输入
 model.inputs = start_tensors
 model._input_layers = [x._keras_history[0] for x in input_layers]
 # 适配工作，方便使用
 if len(input_layers) == 1:
 input_layers = input_layers[0]
 # 整理层，参考自 Model 的 run_internal_graph 函数
 layers, tensor_map = [], set()
 for x in model.inputs:
 tensor_map.add(str(id(x)))
 depth_keys = list(model._nodes_by_depth.keys())
 depth_keys.sort(reverse=True)
 for depth in depth_keys:
 nodes = model._nodes_by_depth[depth]
 for node in nodes:
 n = 0
 for x in node.input_tensors:
 if str(id(x)) in tensor_map:
 n += 1
 if n == len(node.input_tensors):
 if node.outbound_layer not in layers:
 layers.append(node.outbound_layer)
 for x in node.output_tensors:
 tensor_map.add(str(id(x)))
 model._layers = layers # 只保留用到的层
 # 计算输出
 outputs = model(input_layers)
 return input_layers, outputs
```

用法：

```
from keras.applications.resnet50 import ResNet50
model = ResNet50(weights='imagenet')

x, y = get_outputs_of(
 model,
 model.get_layer('add_15').output
)

model2 = Model(x, y)
```

代码有点长，但其实逻辑很简单，真正核心的代码只有三行：

```
model.inputs = start_tensors
model._input_layers = [x._keras_history[0] for x in input_layers]
outputs = model(input_layers)
```

也就是覆盖模型的 `model.inputs` 和 `model._input_layers` 就可以实现欺骗模型从中间层开始构建的效果了，其余的多数是适配工作，不是技术上的，而 `model._layers = layers` 这一句是只保留了从中间层开始所用到的层，只是为了统计模型参数量的准确性，如果去掉这一部分，模型的参数量依然是原来整个model那么多。

## 小结 [\#](https://kexue.fm/kexue.fm\#%E5%B0%8F%E7%BB%93)

Keras是最让人赏心悦目的深度学习框架，至少到目前为止，就模型代码的可读性而言，没有之一。可能读者会提到PyTorch，诚然PyTorch也有不少可取之处，但就可读性而言，我认为是比不上Keras的。

在深究Keras的过程中，我不仅惊叹于Keras作者们的深厚而优雅的编程功底，甚至感觉自己的编程技能也提高了不少。不错，我的很多Python编程技巧，都是从读Keras源码中学习到的。

_**转载到请包括本文地址：** [https://kexue.fm/archives/6985](https://kexue.fm/archives/6985)_

_**更详细的转载事宜请参考：**_ [《科学空间FAQ》](https://kexue.fm/archives/6508#%E6%96%87%E7%AB%A0%E5%A6%82%E4%BD%95%E8%BD%AC%E8%BD%BD/%E5%BC%95%E7%94%A8)

**如果您还有什么疑惑或建议，欢迎在下方评论区继续讨论。**

**如果您觉得本文还不错，欢迎 [分享](https://kexue.fm/kexue.fm#share)/ [打赏](https://kexue.fm/kexue.fm#pay) 本文。打赏并非要从中获得收益，而是希望知道科学空间获得了多少读者的真心关注。当然，如果你无视它，也不会影响你的阅读。再次表示欢迎和感谢！**

打赏

微信打赏

支付宝打赏

因为网站后台对打赏并无记录，因此欢迎在打赏时候备注留言。你还可以 [**点击这里**](http://mail.qq.com/cgi-bin/qm_share?t=qm_mailme&email=tN7d1drY3drrx8H0xcWa19vZ) 或在下方评论区留言来告知你的建议或需求。

**如果您需要引用本文，请参考：**

苏剑林. (Sep. 29, 2019). 《“让Keras更酷一些！”：层与模型的重用技巧 》\[Blog post\]. Retrieved from [https://kexue.fm/archives/6985](https://kexue.fm/archives/6985)

@online{kexuefm-6985,
        title={“让Keras更酷一些！”：层与模型的重用技巧},
        author={苏剑林},
        year={2019},
        month={Sep},
        url={\\url{https://kexue.fm/archives/6985}},
}

分类： [信息时代](https://kexue.fm/category/Big-Data)    标签： [模型](https://kexue.fm/tag/%E6%A8%A1%E5%9E%8B/), [keras](https://kexue.fm/tag/keras/)[21 评论](https://kexue.fm/archives/6985#comments)

< [从语言模型到Seq2Seq：Transformer如戏，全靠Mask](https://kexue.fm/archives/6933) \| [BN究竟起了什么作用？一个闭门造车的分析](https://kexue.fm/archives/6992) >

### 你也许还对下面的内容感兴趣

- [MoE环游记：1、从几何意义出发](https://kexue.fm/archives/10699)
- [基于量子化假设推导模型的尺度定律（Scaling Law）](https://kexue.fm/archives/9607)
- [Tiger：一个“抠”到极致的优化器](https://kexue.fm/archives/9512)
- [在bert4keras中使用混合精度和XLA加速训练](https://kexue.fm/archives/9059)
- [为什么需要残差？一个来自DeepNet的视角](https://kexue.fm/archives/8994)
- [门控注意力单元（GAU）还需要Warmup吗？](https://kexue.fm/archives/8990)
- [Efficient GlobalPointer：少点参数，多点效果](https://kexue.fm/archives/8877)
- [Seq2Seq+前缀树：检索任务新范式（以KgCLUE为例）](https://kexue.fm/archives/8802)
- [开局一段扯，数据全靠编？真被一篇“神论文”气到了](https://kexue.fm/archives/8783)
- [Dropout视角下的MLM和MAE：一些新的启发](https://kexue.fm/archives/8770)

[发表你的看法](https://kexue.fm/kexue.fm#comment_form)

NLP\_LEARNER

September 29th, 2019

get\_outputs\_of函数中，第16行的判断是不是多余了？

[回复评论](https://kexue.fm/archives/6985/comment-page-1?replyTo=12088#respond-post-6985)

[苏剑林](https://kexue.fm) 发表于
September 30th, 2019

必要的，但有个笔误，已经修正。

[回复评论](https://kexue.fm/archives/6985/comment-page-1?replyTo=12091#respond-post-6985)

NLP\_LEARNER 发表于
October 2nd, 2019

这样一改，就理解了。

[回复评论](https://kexue.fm/archives/6985/comment-page-1?replyTo=12096#respond-post-6985)

chandlervan

September 30th, 2019

苏神你好，请教一下~
在交叉引用那一节中，如果我把层内的操作放到外面去做，直接做个转置然后点乘，
和您实例里面的把操作包装做一个层，除了写的不一样之外还会有什么区别吗？

[回复评论](https://kexue.fm/archives/6985/comment-page-1?replyTo=12092#respond-post-6985)

[苏剑林](https://kexue.fm) 发表于
October 7th, 2019

没什么区别，但最终还是要用Lambda层封装一下。

[回复评论](https://kexue.fm/archives/6985/comment-page-1?replyTo=12119#respond-post-6985)

[repostone](https://repostone.home.blog/)

October 3rd, 2019

非技术的路过。

[回复评论](https://kexue.fm/archives/6985/comment-page-1?replyTo=12098#respond-post-6985)

San

October 15th, 2019

苏神你好，我想请教一个问题
在keras用fit\_generator进行训练的时候，每个epoch的steps\_per\_epoch可能都是不一样的，它的具体值是在训练集generator生成器中产生或者定义的，这时候fit\_generator里的steps\_per\_epoch该怎么定义呢

[回复评论](https://kexue.fm/archives/6985/comment-page-1?replyTo=12183#respond-post-6985)

[苏剑林](https://kexue.fm) 发表于
October 16th, 2019

steps\_per\_epoch并不影响训练效果，为什么一定要弄成动态的呢？steps\_per\_epoch大概就是“每steps\_per\_epoch步，keras的进度条就换一个新行”的感觉而已。

[回复评论](https://kexue.fm/archives/6985/comment-page-1?replyTo=12192#respond-post-6985)

hellojet

December 4th, 2019

苏神你好，我在往bert的Embedding-Norm层加扰动的时候遇到了问题想请教您。
我想从bert的Embedding-Norm层取出输出，加入扰动
\\begin{lstlisting}
norm\_embedding = ernie.get\_layer('Embedding-Norm').output
noise = tf.random\_normal(shape=tf.shape(norm\_embedding))
perturb = \_scale\_l2(\_mask\_by\_length(noise, l), 5.0)
\_token = Add()(\[norm\_embedding, perturb\])
\_\_output = model\_output(model\_transformer(\_token))
\_loss = K.mean(K.binary\_crossentropy(y, \_\_output))
\\end{lstlisting}
model\_transformer是我从bert中提取的中间层到最后的（None,None,768）输出。
\\begin{lstlisting}
\_x, \_y = get\_outputs\_of(ernie, ernie.get\_layer('Embedding-Norm').output)
model\_transformer = Model(input=\_x, output=\_y)
\\end{lstlisting}
但是在运行的时候报以下错误：
\\begin{lstlisting}
tensorflow.python.framework.errors\_impl.InvalidArgumentError: 2 root error(s) found.
(0) Invalid argument: You must feed a value for placeholder tensor 'Input-Token' with dtype float and shape \[?,513\]
\[\[{{node Input-Token}}\]\]
\[\[Mean\_2/\_3965\]\]
(1) Invalid argument: You must feed a value for placeholder tensor 'Input-Token' with dtype float and shape \[?,513\]
\[\[{{node Input-Token}}\]\]
\\end{lstlisting}

[回复评论](https://kexue.fm/archives/6985/comment-page-1?replyTo=12534#respond-post-6985)

[苏剑林](https://kexue.fm) 发表于
December 6th, 2019

你这个需求还是直接改bert.py的源码吧，你这种改法是不生效的。

[回复评论](https://kexue.fm/archives/6985/comment-page-1?replyTo=12545#respond-post-6985)

林枫

September 22nd, 2020

苏神您好，我运行了您的get\_outputs\_of()方法，但是现在
在for x in node.input\_tensors行
报错：TypeError: Cannot iterate over a tensor with unknown first dimension.

请教一下苏神到底是为什么？
x, y = get\_outputs\_of(
model,
model.get\_layer('concatenate').output
)

[回复评论](https://kexue.fm/archives/6985/comment-page-1?replyTo=14387#respond-post-6985)

[苏剑林](https://kexue.fm) 发表于
September 22nd, 2020

我也很久不用这代码了，根据这文章的编写时间，估计当时的keras版本是2.2.4

[回复评论](https://kexue.fm/archives/6985/comment-page-1?replyTo=14388#respond-post-6985)

林枫 发表于
September 22nd, 2020

的确有可能是版本的问题，我现在的版本是tf2.0.0 和keras2.3.1 在这个环境下，resnet50示例代码也无法跑通

[回复评论](https://kexue.fm/archives/6985/comment-page-1?replyTo=14389#respond-post-6985)

脉望

November 13th, 2020

写得太好了，神级文章，必须打赏！！！

[回复评论](https://kexue.fm/archives/6985/comment-page-1?replyTo=14781#respond-post-6985)

captainst

January 30th, 2021

您好。请问用了get\_outputs\_of之后，得到的新的模型拥有原模型的权重吗？

[回复评论](https://kexue.fm/archives/6985/comment-page-1?replyTo=15426#respond-post-6985)

[苏剑林](https://kexue.fm) 发表于
February 1st, 2021

是的

[回复评论](https://kexue.fm/archives/6985/comment-page-1?replyTo=15432#respond-post-6985)

captainst 发表于
February 1st, 2021

谢谢！如果是这样的话，是不是可以在get\_outputs\_of函数的最后直接把model返回出来，像这样：
return input\_layers, outputs, model
这样的话，返回的这个model就是改造过的，是不是就和model2 = Model(x, y)这样等价

[回复评论](https://kexue.fm/archives/6985/comment-page-1?replyTo=15435#respond-post-6985)

[苏剑林](https://kexue.fm) 发表于
February 1st, 2021

你自己创建不也一样么...

[回复评论](https://kexue.fm/archives/6985/comment-page-1?replyTo=15436#respond-post-6985)

captainst

February 1st, 2021

了解了，再次感谢！
之前大概看看别人写的代码，并没有真正研究过。

[回复评论](https://kexue.fm/archives/6985/comment-page-1?replyTo=15437#respond-post-6985)

zclll

May 11th, 2021

非常感谢，解决大问题了！
想问一下model.get\_layer()得到的是那个层对象吗，如果我在这里直接调用得到的layer进行计算，是不是会影响原本那个model的计算图?

[回复评论](https://kexue.fm/archives/6985/comment-page-1?replyTo=16362#respond-post-6985)

[苏剑林](https://kexue.fm) 发表于
May 11th, 2021

默认情况下所有模型都是同一个图

[回复评论](https://kexue.fm/archives/6985/comment-page-1?replyTo=16368#respond-post-6985)

[取消回复](https://kexue.fm/archives/6985#respond-post-6985)

你的大名

电子邮箱

个人网站（选填）

1\. 可以使用LaTeX代码，点击“预览效果”可查看效果；2. 可以通过点击评论楼层编号来引用该楼层；3. 网站可能会有点卡，如非确认评论失败，请 **不要重复点击提交**。

### 内容速览

[基础](https://kexue.fm/kexue.fm#%E5%9F%BA%E7%A1%80)
[层的重用](https://kexue.fm/kexue.fm#%E5%B1%82%E7%9A%84%E9%87%8D%E7%94%A8)
[模型重用](https://kexue.fm/kexue.fm#%E6%A8%A1%E5%9E%8B%E9%87%8D%E7%94%A8)
[模型克隆](https://kexue.fm/kexue.fm#%E6%A8%A1%E5%9E%8B%E5%85%8B%E9%9A%86)
[进阶](https://kexue.fm/kexue.fm#%E8%BF%9B%E9%98%B6)
[交叉引用](https://kexue.fm/kexue.fm#%E4%BA%A4%E5%8F%89%E5%BC%95%E7%94%A8)
[提取中间层](https://kexue.fm/kexue.fm#%E6%8F%90%E5%8F%96%E4%B8%AD%E9%97%B4%E5%B1%82)
[从中间拆开](https://kexue.fm/kexue.fm#%E4%BB%8E%E4%B8%AD%E9%97%B4%E6%8B%86%E5%BC%80)
[小结](https://kexue.fm/kexue.fm#%E5%B0%8F%E7%BB%93)

### 智能搜索

支持整句搜索！网站自动使用 [结巴分词](https://github.com/fxsjy/jieba) 进行分词，并结合ngrams排序算法给出合理的搜索结果。

### 热门标签

[生成模型](https://kexue.fm/tag/%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B/) [attention](https://kexue.fm/tag/attention/) [优化](https://kexue.fm/tag/%E4%BC%98%E5%8C%96/) [语言模型](https://kexue.fm/tag/%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/) [模型](https://kexue.fm/tag/%E6%A8%A1%E5%9E%8B/) [网站](https://kexue.fm/tag/%E7%BD%91%E7%AB%99/) [梯度](https://kexue.fm/tag/%E6%A2%AF%E5%BA%A6/) [概率](https://kexue.fm/tag/%E6%A6%82%E7%8E%87/) [矩阵](https://kexue.fm/tag/%E7%9F%A9%E9%98%B5/) [优化器](https://kexue.fm/tag/%E4%BC%98%E5%8C%96%E5%99%A8/) [转载](https://kexue.fm/tag/%E8%BD%AC%E8%BD%BD/) [微分方程](https://kexue.fm/tag/%E5%BE%AE%E5%88%86%E6%96%B9%E7%A8%8B/) [分析](https://kexue.fm/tag/%E5%88%86%E6%9E%90/) [天象](https://kexue.fm/tag/%E5%A4%A9%E8%B1%A1/) [深度学习](https://kexue.fm/tag/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/) [积分](https://kexue.fm/tag/%E7%A7%AF%E5%88%86/) [python](https://kexue.fm/tag/python/) [扩散](https://kexue.fm/tag/%E6%89%A9%E6%95%A3/) [力学](https://kexue.fm/tag/%E5%8A%9B%E5%AD%A6/) [无监督](https://kexue.fm/tag/%E6%97%A0%E7%9B%91%E7%9D%A3/) [几何](https://kexue.fm/tag/%E5%87%A0%E4%BD%95/) [节日](https://kexue.fm/tag/%E8%8A%82%E6%97%A5/) [生活](https://kexue.fm/tag/%E7%94%9F%E6%B4%BB/) [文本生成](https://kexue.fm/tag/%E6%96%87%E6%9C%AC%E7%94%9F%E6%88%90/) [数论](https://kexue.fm/tag/%E6%95%B0%E8%AE%BA/)

### 随机文章

- [\[问题解答\]木杆平衡](https://kexue.fm/archives/1536)
- [矩阵化简二次型（无穷小近似处理抛物型）](https://kexue.fm/archives/1841)
- [最受尊崇的3位诺贝尔奖得主](https://kexue.fm/archives/154)
- [【NASA每日一图】IC 1396 星云](https://kexue.fm/archives/86)
- [更别致的词向量模型(六)：代码、分享与结语](https://kexue.fm/archives/4681)
- [logsumexp运算的几个不等式](https://kexue.fm/archives/9070)
- [让炼丹更科学一些（二）：将结论推广到无界域](https://kexue.fm/archives/11469)
- [【NASA每日一图】Messier 88](https://kexue.fm/archives/384)
- [自然数集中 N = ab + c 时 a + b + c 的最小值](https://kexue.fm/archives/9775)
- [【NASA每日一图】明亮的超新星爆发](https://kexue.fm/archives/48)

### 最近评论

- [kaiyuan](https://kexue.fm/archives/11486/comment-page-1#comment-29036): 看了“Linear Transformers Are Secretly Fast Weight...
- [sog](https://kexue.fm/archives/11469/comment-page-1#comment-29035): 好的，符号相同，搞混了呃
- [kerry](https://kexue.fm/archives/9119/comment-page-14#comment-29034): 还没有通读完后面的系列，提出一些拙见。
降低方差这一节把原本的目标“预测单步的noise”变成...
- [Kevin Yin](https://kexue.fm/archives/11158/comment-page-1#comment-29033): I wrote https://research.novelai.net/muonscale/...
- [罗](https://kexue.fm/archives/11480/comment-page-1#comment-29032): 公式(6)显示出来是不是有点小问题？
- [cmlin](https://kexue.fm/archives/11340/comment-page-1#comment-29031): 本人对这方面不太熟悉，想了解这三个条件的意义及动机，且希望这系列可以继续写下去。以下想发表一些...
- [喝一口可乐](https://kexue.fm/archives/10958/comment-page-3#comment-29030): 理解了，感谢苏神回复，数学上给出建模分析确实清晰了很多，再次感谢苏神回复！
- [CuddleSabe1](https://kexue.fm/archives/10958/comment-page-1#comment-29029): 感觉普通的 flow matching 可以看成 degrade-aware image de...
- [岁月如书](https://kexue.fm/archives/11126/comment-page-3#comment-29028): 受教了，感谢
- [苏剑林](https://kexue.fm/archives/11126/comment-page-3#comment-29027): 是

### 友情链接

- [Cool Papers](https://papers.cool)
- [数学研发](https://bbs.emath.ac.cn)
- [Seatop](http://www.seatop.com.cn/)
- [Xiaoxia](https://xiaoxia.org/)
- [积分表-网络版](https://kexue.fm/sci/integral/index.html)
- [丝路博傲](http://blog.dvxj.com/)
- [数学之家](http://www.2math.cn/)
- [有趣天文奇观](http://interesting-sky.china-vo.org/)
- [TwistedW](http://www.twistedwg.com/)
- [godweiyang](https://godweiyang.com/)
- [AI柠檬](https://blog.ailemon.net/)
- [王登科-DK博客](https://greatdk.com)
- [ESON](https://blog.eson.org/)
- [枫之羽](https://fzhiy.net/)
- [coding-zuo](https://coding-zuo.github.io/)
- [博科园](https://www.bokeyuan.net/)
- [孔皮皮的博客](https://www.kppkkp.top/)
- [运鹏的博客](https://yunpengtai.top/)
- [jiming.site](https://jiming.site/)
- [OmegaXYZ](https://www.omegaxyz.com/)
- [EAI猩球](https://www.robotech.ink/)
- [文举的博客](https://liwenju0.com/)
- [申请链接](https://kexue.fm/links.html)

本站采用创作共用版权协议，要求署名、非商业用途和保持一致。转载本站内容必须也遵循“ [署名-非商业用途-保持一致](http://creativecommons.org/licenses/by-nc-nd/2.5/cn/)”的创作共用协议。
© 2009-2025 Scientific Spaces. All rights reserved. Theme by [laogui](http://www.laogui.com). Powered by [Typecho](http://typecho.org). 备案号: [粤ICP备09093259号-1/2](https://beian.miit.gov.cn/)。