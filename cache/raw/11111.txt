## SEARCH

## MENU

- [打赏](https://kexue.fm/reward.html)
- [公式](https://kexue.fm/latex.html)
- [天象](https://kexue.fm/ac.html)
- [链接](https://kexue.fm/links.html)
- [时光](https://kexue.fm/me.html)
- [博览](https://kexue.fm/science.html)
- [归档](https://kexue.fm/content.html)

## CATEGORIES

- [千奇百怪](https://kexue.fm/category/Everything)
- [天文探索](https://kexue.fm/category/Astronomy)
- [数学研究](https://kexue.fm/category/Mathematics)
- [物理化学](https://kexue.fm/category/Phy-chem)
- [信息时代](https://kexue.fm/category/Big-Data)
- [生物自然](https://kexue.fm/category/Biology)
- [图片摄影](https://kexue.fm/category/Photograph)
- [问题百科](https://kexue.fm/category/Questions)
- [生活/情感](https://kexue.fm/category/Life-Feeling)
- [资源共享](https://kexue.fm/category/Resources)

## NEWPOSTS

- [QK-Clip：让Muon在Sca...](https://kexue.fm/archives/11126)
- [Transformer升级之路：2...](https://kexue.fm/archives/11111)
- [“对角+低秩”三角阵的高效求逆方法](https://kexue.fm/archives/11072)
- [通过msign来计算奇异值裁剪mc...](https://kexue.fm/archives/11059)
- [矩阵符号函数mcsgn能计算什么？](https://kexue.fm/archives/11056)
- [线性注意力简史：从模仿、创新到反哺](https://kexue.fm/archives/11033)
- [msign的导数](https://kexue.fm/archives/11025)
- [通过msign来计算奇异值裁剪mc...](https://kexue.fm/archives/11006)
- [msign算子的Newton-Sc...](https://kexue.fm/archives/10996)
- [等值振荡定理：最优多项式逼近的充要条件](https://kexue.fm/archives/10972)

## COMMENTS

- [Claude4: 英伟达官方确认 H20 芯片对华销售解禁，意味着什么？将产生哪...](https://kexue.fm/archives/11111/comment-page-1#comment-28155)
- [YyWangCS: 可能是我之前的回答写的太多了，简单总结一下我自己上面这个长回答...](https://kexue.fm/archives/11111/comment-page-1#comment-28154)
- [国产GPU支持者: attention和ffn (moe) 计算占比和contex...](https://kexue.fm/archives/11111/comment-page-1#comment-28153)
- [kk11: 请教下，做weight clip的时候，需要考虑对$M\_{t-...](https://kexue.fm/archives/11126/comment-page-1#comment-28152)
- [国产GPU支持者: “美国在2022年年底颁布对华芯片出口禁令，英伟达的王牌人工智...](https://kexue.fm/archives/11111/comment-page-1#comment-28151)
- [苏剑林: 1、我特意请教了Infra同学，当MLA不去double he...](https://kexue.fm/archives/11111/comment-page-1#comment-28150)
- [国产GPU支持者: 处于工作保密原因，在此匿名地简单提两个问题。1\. 贵司 Moo...](https://kexue.fm/archives/11111/comment-page-1#comment-28149)
- [苏剑林: 如果能坚持到推理，那可以考虑，像本文说的，顶多就浪费一个Hea...](https://kexue.fm/archives/11126/comment-page-1#comment-28148)
- [苏剑林: 谢谢，已更正。](https://kexue.fm/archives/11111/comment-page-1#comment-28147)
- [苏剑林: QK-Clip的重点是以MaxLogit为信号去按需Clip，...](https://kexue.fm/archives/11126/comment-page-1#comment-28146)

## USERLOGIN

- [登录](https://kexue.fm/admin/login.php)

[科学空间\|Scientific Spaces](https://kexue.fm)

- [登录](https://kexue.fm/admin/login.php)
- [打赏](https://kexue.fm/reward.html)
- [公式](https://kexue.fm/latex.html)
- [天象](https://kexue.fm/ac.html)
- [链接](https://kexue.fm/links.html)
- [时光](https://kexue.fm/me.html)
- [博览](https://kexue.fm/science.html)
- [归档](https://kexue.fm/content.html)

渴望成为一个小飞侠

- [欢迎订阅](https://kexue.fm/feed)
- [个性邮箱](https://kexue.fm/archives/119)
- [天象信息](https://kexue.fm/ac.html)
- [观测ISS](https://kexue.fm/archives/41)
- [LaTeX](https://kexue.fm/latex.html)
- [关于博主](https://kexue.fm/me.html)

欢迎访问“科学空间”，这里将与您共同探讨自然科学，回味人生百态；也期待大家的分享～

- [**千奇百怪** Everything](https://kexue.fm/category/Everything)
- [**天文探索** Astronomy](https://kexue.fm/category/Astronomy)
- [**数学研究** Mathematics](https://kexue.fm/category/Mathematics)
- [**物理化学** Phy-chem](https://kexue.fm/category/Phy-chem)
- [**信息时代** Big-Data](https://kexue.fm/category/Big-Data)
- [**生物自然** Biology](https://kexue.fm/category/Biology)
- [**图片摄影** Photograph](https://kexue.fm/category/Photograph)
- [**问题百科** Questions](https://kexue.fm/category/Questions)
- [**生活/情感** Life-Feeling](https://kexue.fm/category/Life-Feeling)
- [**资源共享** Resources](https://kexue.fm/category/Resources)

- [**千奇百怪**](https://kexue.fm/category/Everything)
- [**天文探索**](https://kexue.fm/category/Astronomy)
- [**数学研究**](https://kexue.fm/category/Mathematics)
- [**物理化学**](https://kexue.fm/category/Phy-chem)
- [**信息时代**](https://kexue.fm/category/Big-Data)
- [**生物自然**](https://kexue.fm/category/Biology)
- [**图片摄影**](https://kexue.fm/category/Photograph)
- [**问题百科**](https://kexue.fm/category/Questions)
- [**生活/情感**](https://kexue.fm/category/Life-Feeling)
- [**资源共享**](https://kexue.fm/category/Resources)

[首页](https://kexue.fm) [信息时代](https://kexue.fm/category/Big-Data) Transformer升级之路：21、MLA好在哪里?（下）

10Jul

# [Transformer升级之路：21、MLA好在哪里?（下）](https://kexue.fm/archives/11111)

By 苏剑林 \|
2025-07-10 \|
9951位读者\|

在文章 [《Transformer升级之路：20、MLA好在哪里?（上）》](https://kexue.fm/archives/10907) 中，我们对 [MLA](https://kexue.fm/archives/10091) 相比常见MHA、GQA、MQA的一些变化分别做了消融实验，其中的变化包括“增大head\_dims”、“Partial RoPE”和“KV共享”，实验的初步结果是这三个变化很可能都是MLA效果优异的原因。

本文我们将从一个更加偏理论的角度出发，来理解MLA的成功之处。

## 部分旋转 [\#](https://kexue.fm/archives/11111\#%E9%83%A8%E5%88%86%E6%97%8B%E8%BD%AC)

首先，我们把最终的断言放在前面：

> 在相同训练成本和推理成本下，MLA可能是效果最好的Full Attention变体。

很明显，这个判断把MLA摆在了非常高的位置。这是在比较理想和简化的假设下，根据上一篇文章的实验结果以及本文接下来的理论分析所得的结论。由于实际的训练和推理存在诸多复杂的因素，所以该结论大概率会有所偏差，但我们至少可以得出，MLA应该是走在了正确的改进方向上。

MLA之所以能够表现出色，有一个非常大的前提，那就是部分旋转的 [Partial RoPE](https://kexue.fm/archives/10122) 效果不逊色于甚至可能优于完全体的RoPE。这里的Partial RoPE可以有两种含义：一是我们对Attention的$\\boldsymbol{Q}$、$\\boldsymbol{K}$加RoPE时，可以只对小部份维度加，剩下的维度保持不变；二是我们可以考虑层间RoPE与NoPE交替出现，并且NoPE的层可以占多数。

说白了，RoPE可以只加“一点点”，但不能不加，完全不加的话效果不行。如果需要理论，笔者比较认同 [《Transformer升级之路：18、RoPE的底数选择原则》](https://kexue.fm/archives/10122) 的解释，大致意思是Partial RoPE使得检索结果更兼顾位置与语义。此外，像 [FoX](https://papers.cool/arxiv/2503.02130)、 [SBA](https://papers.cool/arxiv/2410.17980) 等新工作也体现出一定潜力，但对于MLA来说，这些变体就相当于NoPE，因此不改变结论。

“Partial RoPE效果不差”的结论，允许我们把Attention的主要计算复杂度放到NoPE部分上，这提供了更大的腾挪空间，MLA便是得益于此。

## 键值共享 [\#](https://kexue.fm/archives/11111\#%E9%94%AE%E5%80%BC%E5%85%B1%E4%BA%AB)

Full Attention的变化大致上是从 [MHA](https://kexue.fm/archives/4765)、 [MQA](https://papers.cool/arxiv/1911.02150)、 [GQA](https://papers.cool/arxiv/2305.13245) 然后到MLA，虽然MQA可以看作是GQA的特例，但按时间顺序来说确实是GQA在后。在MLA之后，还出现了 [MFA](https://papers.cool/arxiv/2412.19255)、 [TPA](https://papers.cool/arxiv/2501.06425) 两个变体。这些变体本质上都是在尽量保持效果的前提下，尽可能压榨KV Cache以提高生成速度。

简单来说，Attention模型的复杂度可以分训练、Prefill和Decoding三部分，其中训练和Prefill是相似的，所以本质上是Prefill和Decoding两部分。Prefill是指模型处理输入、直至吐出第一个token的阶段，这部分我们下节再谈；Decoding是指Token by Token的生成阶段，它可以通过KV Cache机制来加速，但同时也导致了KV Cache大小几乎是Decoding速度的唯一瓶颈。

所以，压缩KV Cache就是提高Decoding速度。现在问大家一个问题： **在NoPE背景下，给定KV Cache大小后，效果最好的Attention是什么呢？** 如果不考虑参数量差异，只在单层MHA/GQA/MQA内讨论（TPA和MFA我们后面再补充讨论），那么答案将会是：

> **一个head\_dims等于KV Cache大小、K和V共享的MQA。**

看上去是不是让人意外？其实不难理解。因为MHA、MQA都可以看成是GQA的一个特例，所以我们只需要分析GQA，我们在 [《缓存与效果的极限拉扯：从MHA、MQA、GQA到MLA》](https://kexue.fm/archives/10091) 已经给出了，GQA可以重新表示成一个K、V拼接起来的模型：
\\begin{equation}\\underbrace{\\left\[\\boldsymbol{k}\_i^{(1)},\\cdots,\\boldsymbol{k}\_i^{(g)},\\boldsymbol{v}\_i^{(1)},\\cdots,\\boldsymbol{v}\_i^{(g)}\\right\]}\_{\\boldsymbol{c}\_i\\in\\mathbb{R}^{g(d\_k+d\_v)}} = \\boldsymbol{x}\_i \\underbrace{\\left\[\\boldsymbol{W}\_k^{(1)},\\cdots,\\boldsymbol{W}\_k^{(g)},\\boldsymbol{W}\_v^{(1)},\\cdots,\\boldsymbol{W}\_v^{(g)}\\right\]}\_{\\boldsymbol{W}\_c\\in\\mathbb{R}^{d\\times g(d\_k+d\_v)}}\\end{equation}
这里$g(d\_k+d\_v)$正是单个Token的KV Cache总大小。接着我们算Attention的时候，$\\boldsymbol{c}$到$\\boldsymbol{k},\\boldsymbol{v}$的变换分别吸收到$\\boldsymbol{W}\_q$和$\\boldsymbol{W}\_o$那边去，那么就得到了一个K、V都是$\\boldsymbol{c}$的MQA。所以说，“head\_dims等于KV Cache大小、K和V共享的MQA”，实际上是给定KV Cache大小后MHA/GQA/MQA的“超集”，那么它自然是理论上效果最好的选择。

## 双重投影 [\#](https://kexue.fm/archives/11111\#%E5%8F%8C%E9%87%8D%E6%8A%95%E5%BD%B1)

综上所述，如果我们想要在相同Decoding速度下效果最优，那么应该训练一个指定head\_dims的、KV共享的MQA，比如约定KV Cache不超过512，那么head\_dims=512的、KV共享的MQA就是最佳选择。事实上，MLA在Decoding阶段正是KV共享的MQA（NoPE部分），这就是它走在正确方向上的体现之一。

然而，将head\_dims升到512，Decoding是没问题，但训练和Prefill都很难接受，因为它们俩的瓶颈是计算，而影响计算速度的主要因素是num\_heads和head\_dims。为了保证效果，num\_heads变动的空间不大，因此head\_dims大小可以说是计算量的唯一指标，head\_dims升到512意味着计算量要增加到原来的4倍（相比head\_dims=128）。

现在再来问大家一个问题： **同样在NoPE背景下，给定num\_heads和head\_dims后，效果最好的Attention是什么呢？** 这个问题的答案我相信大家都能接受，那就是 **MHA**，因为它限制最少。所以，单从训练和Prefill成本来看，我们希望的是训练一个head\_dims=128的MHA。

怎么调和Prefill与Decoding这两个不同的期望呢？这就是MLA的“大招”了，它通过两步投影得到K、V：先将输入投影到单个512维的向量，然后将该向量投影到多个128维的向量，然后利用“Attention + NoPE”固有的恒等变换性质，可以让模型在MHA-128和MQA-512间自由切换。

$$\\require{cancel}\\begin{array}{c\|c}
\\text{训练/Prefill} & \\text{Decoding} \\\
\\\
\\begin{gathered}
\\boldsymbol{o}\_t = \\left\[\\boldsymbol{o}\_t^{(1)}, \\boldsymbol{o}\_t^{(2)}, \\cdots, \\boldsymbol{o}\_t^{(h)}\\right\] \\\\[10pt\]
\\boldsymbol{o}\_t^{(s)} = \\frac{\\sum\_{i\\leq t}\\exp\\left(\\boldsymbol{q}\_t^{(s)} \\boldsymbol{k}\_i^{(s)}{}^{\\top}\\right)\\boldsymbol{v}\_i^{(s)}}{\\sum\_{i\\leq t}\\exp\\left(\\boldsymbol{q}\_t^{(s)} \\boldsymbol{k}\_i^{(s)}{}^{\\top}\\right)} \\\\[15pt\]
\\boldsymbol{q}\_i^{(s)} = \\boldsymbol{x}\_i\\boldsymbol{W}\_q^{(s)}\\in\\mathbb{R}^{d\_k},\\quad \\boldsymbol{W}\_q^{(s)}\\in\\mathbb{R}^{d\\times d\_k}\\\
\\boldsymbol{k}\_i^{(s)} = \\boldsymbol{c}\_i\\boldsymbol{W}\_k^{(s)}\\in\\mathbb{R}^{d\_k},\\quad \\boldsymbol{W}\_k^{(s)}\\in\\mathbb{R}^{d\_c\\times d\_k} \\\
\\boldsymbol{v}\_i^{(s)} = \\boldsymbol{c}\_i\\boldsymbol{W}\_v^{(s)}\\in\\mathbb{R}^{d\_v},\\quad \\boldsymbol{W}\_v^{(s)}\\in\\mathbb{R}^{d\_c\\times d\_v} \\\\[10pt\]
\\boldsymbol{c}\_i = \\boldsymbol{x}\_i \\boldsymbol{W}\_c\\in\\mathbb{R}^{d\_c},\\quad \\boldsymbol{W}\_c\\in\\mathbb{R}^{d\\times d\_c}
\\end{gathered}
&
\\begin{gathered}
\\boldsymbol{o}\_t = \\left\[\\boldsymbol{o}\_t^{(1)}\\boldsymbol{W}\_v^{(1)}, \\boldsymbol{o}\_t^{(2)}\\boldsymbol{W}\_v^{(2)}, \\cdots, \\boldsymbol{o}\_t^{(h)}\\boldsymbol{W}\_v^{(h)}\\right\] \\\\[10pt\]
\\boldsymbol{o}\_t^{(s)} = \\frac{\\sum\_{i\\leq t}\\exp\\left(\\boldsymbol{q}\_t^{(s)} \\boldsymbol{k}\_i^{\\color{#ccc}{\\smash{\\bcancel{(s)}}}}{}^{\\top}\\right)\\boldsymbol{v}\_i^{\\color{#ccc}{\\smash{\\bcancel{(s)}}}} }{\\sum\_{i\\leq t}\\exp\\left(\\boldsymbol{q}\_t^{(s)} \\boldsymbol{k}\_i^{\\color{#ccc}{\\smash{\\bcancel{(s)}}}}{}^{\\top}\\right)} \\\\[15pt\]
\\boldsymbol{q}\_i^{(s)} = \\boldsymbol{x}\_i\\boldsymbol{W}\_q^{(s)}\\boldsymbol{W}\_k^{(s)}{}^{\\top}\\in\\mathbb{R}^{d\_c}\\\
\\boldsymbol{k}\_i^{\\color{#ccc}{\\smash{\\bcancel{(s)}}}} = \\boldsymbol{v}\_i^{\\color{#ccc}{\\smash{\\bcancel{(s)}}}} = \\boldsymbol{c}\_i= \\boldsymbol{x}\_i \\boldsymbol{W}\_c\\in\\mathbb{R}^{d\_c}
\\end{gathered}
\\end{array}$$

## 总而言之 [\#](https://kexue.fm/archives/11111\#%E6%80%BB%E8%80%8C%E8%A8%80%E4%B9%8B)

我们将前面的推理逻辑做个总结：

> 1、大前提：Partial RoPE的效果不差于甚至可能优于RoPE，这使得我们可以把主要精力放在NoPE上；
>
> 2、Decoding主要瓶颈是KV Cache，理论效果最优的模型是head\_dims=KV Cache、KV共享的MQA；
>
> 3、训练和Prefill的主要瓶颈都是head\_dims，理论效果最优的模型是head\_dims为期望值的MHA；
>
> 4、在NoPE前提下，Attention具有恒等变换性质，可以通过LoRA来尽可能地兼顾两个理想方向，这正好是MLA所做的。

剩下的，就是给K拼接一个共享的低维RoPE，以最小的成本给MLA补充上位置信息，同时还“一箭双雕”：拼接RoPE的做法暗合了“Partial RoPE”，同时也增加了head\_dims，这跟上一篇文章的结论相符。换句话说，有意或者无意之中使用了Partial RoPE和增加了head\_dims，是MLA在极致压缩之下还能媲美MHA的主要原因。

从MQA的角度看，MLA是给Q加了rank=128的LoRA；从MHA的角度看，MLA是给K、V加了rank=512的LoRA。可以说，MLA是一场NoPE结合LoRA、MHA结合MQA的极致“魔术秀”，成功实现了Prefill和Decoding的“双向奔赴”。

当然，上述思考过程肯定有一些过于简化的地方。比如，实际的训练和推理还有诸多细节因素，笼统地归结为head\_dims和KV Cache是不完全准确的，例如MQA在Decoding阶段无法TP（张量并行），这可能会带来新的效率问题；还有，分析过程中我们也没有特别注重参数量的对齐，比如在head\_dims=128时我们也可以考虑增加Q、K、V的投影复杂度来提高性能，而不一定要增大head\_dims；等等。

总之，上下两篇文章旨在提供一些实验和思考，来论证MLA在一定范围内的最优性。当然，MLA是DeepSeek首先提出的，第三方使用MLA总会给人一种复制DeepSeek的感觉，但在更好的变体出现之前，或者在发现严重的缺陷之前，MLA始终是一个相当有竞争力的选择，如果单纯是为了显示自己不“追随”DeepSeek而不用MLA，那是一个相当不明智的选择。

举个例子，现在Linear Attention和Softmax Attention的混合模型也体现出极大的竞争力，但如果我们将Linear Attention跟LLAMA使用的GQA8-128按3:1混合，那么KV Cache大致上降低到GQA8-128的1/4，然而MLA本身就能将KV Cache降低到GQA8-128的1/4了。

## 补充讨论 [\#](https://kexue.fm/archives/11111\#%E8%A1%A5%E5%85%85%E8%AE%A8%E8%AE%BA)

前面我们都在围绕MHA、GQA、MQA和MLA讨论，这一节我们来简单聊聊两个比较少谈及的Attention变体：TPA和MFA。

TPA全称是Tensor Product Attention，作者给它安了个Tensor Product的名字，显得比较“唬人”，实际上它是一个介乎GQA和MLA的中间产物。我们以目标KV Cache=512为例，TPA先投影得到一个512维向量，然后reshape为(4, 128)，然后分成两个(2,128)分别代表K Cache和V Cache。到目前为止，TPA的做法都跟GQA2-128一致。

接下来，TPA借鉴了MLA的思想，将(2,128)的K/V重新投影成Multi-Head，但它不是像MLA那样整个向量投影，而是沿着“2”所在的维度投影，说白了就是将2个128维向量做head\_dims次不同的线性组合。显然，这样TPA的上限是不如MLA直接从整个512维向量出发来投影的。为了缓解这个问题，TPA又引入了data-dependent的组合系数来增强K、V的表达能力，即便如此，笔者还是认为它上限不如MLA。

为什么TPA要这样设计呢？大体上是为了兼容RoPE，这也是它相比MLA的最大“优点”。然而，这里的“优点”是要加个双引号的，因为在Partial RoPE不逊色甚至还可能更优的背景下，兼容RoPE就有点啼笑皆非的感觉了。还有，TPA这样设计，堵死了它升head\_dims的空间，比如head\_dims想要升到256，那么K Cache、V Cache就只是(1,256)形状了，单个向量没有线性组合的自由度。

再来看MFA，它全称是“Multi-matrix Factorization Attention”，这个名字看上去也有点“唬人”，它实际上就是一个带有Q-LoRA的、head\_dims=256的MQA。看到这个配置，是不是有点熟悉？因为这配置跟我们上一篇文章的结论完全吻合——增大head\_dims到256来提升MQA的效果，并且KV Cache跟MLA接近，同时通过Q-LoRA来控制参数量。

所以，MFA能“打”MLA，笔者并不意外，上一篇文章我们也实验过差不多的做法了。此外，上一篇文章我们还提出另外两个提升MQA效果的方向，一个是本文已经多次提及的Partial RoPE，另一个是通过 [QKVO-RoPE](https://kexue.fm/archives/10862) 的方式实现完全的KV共享，让MQA变成GQA2-256，这两点叠加上去，MFA应该还能再涨一点。

## 文章小结 [\#](https://kexue.fm/archives/11111\#%E6%96%87%E7%AB%A0%E5%B0%8F%E7%BB%93)

本文在上一篇文章的实验结果基础上，给出一个偏理论的思考过程，以论证MLA在一定范围内的最优性。总的来说，在Partial RoPE的背景下，MLA似乎是一个非常难以超越的Attention变体。

_**转载到请包括本文地址：** [https://kexue.fm/archives/11111](https://kexue.fm/archives/11111)_

_**更详细的转载事宜请参考：**_ [《科学空间FAQ》](https://kexue.fm/archives/6508#%E6%96%87%E7%AB%A0%E5%A6%82%E4%BD%95%E8%BD%AC%E8%BD%BD/%E5%BC%95%E7%94%A8)

**如果您还有什么疑惑或建议，欢迎在下方评论区继续讨论。**

**如果您觉得本文还不错，欢迎 [分享](https://kexue.fm/archives/11111#share)/ [打赏](https://kexue.fm/archives/11111#pay) 本文。打赏并非要从中获得收益，而是希望知道科学空间获得了多少读者的真心关注。当然，如果你无视它，也不会影响你的阅读。再次表示欢迎和感谢！**

打赏

微信打赏

支付宝打赏

因为网站后台对打赏并无记录，因此欢迎在打赏时候备注留言。你还可以 [**点击这里**](http://mail.qq.com/cgi-bin/qm_share?t=qm_mailme&email=tN7d1drY3drrx8H0xcWa19vZ) 或在下方评论区留言来告知你的建议或需求。

**如果您需要引用本文，请参考：**

苏剑林. (Jul. 10, 2025). 《Transformer升级之路：21、MLA好在哪里?（下） 》\[Blog post\]. Retrieved from [https://kexue.fm/archives/11111](https://kexue.fm/archives/11111)

@online{kexuefm-11111,
        title={Transformer升级之路：21、MLA好在哪里?（下）},
        author={苏剑林},
        year={2025},
        month={Jul},
        url={\\url{https://kexue.fm/archives/11111}},
}

分类： [信息时代](https://kexue.fm/category/Big-Data)    标签： [优化](https://kexue.fm/tag/%E4%BC%98%E5%8C%96/), [语言模型](https://kexue.fm/tag/%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/), [生成模型](https://kexue.fm/tag/%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B/), [attention](https://kexue.fm/tag/attention/)[21 评论](https://kexue.fm/archives/11111#comments)

< [“对角+低秩”三角阵的高效求逆方法](https://kexue.fm/archives/11072) \| [QK-Clip：让Muon在Scaleup之路上更进一步](https://kexue.fm/archives/11126) >

### 你也许还对下面的内容感兴趣

- [QK-Clip：让Muon在Scaleup之路上更进一步](https://kexue.fm/archives/11126)
- [“对角+低秩”三角阵的高效求逆方法](https://kexue.fm/archives/11072)
- [线性注意力简史：从模仿、创新到反哺](https://kexue.fm/archives/11033)
- [生成扩散模型漫谈（三十）：从瞬时速度到平均速度](https://kexue.fm/archives/10958)
- [MoE环游记：5、均匀分布的反思](https://kexue.fm/archives/10945)
- [Transformer升级之路：20、MLA好在哪里?（上）](https://kexue.fm/archives/10907)
- [Transformer升级之路：19、第二类旋转位置编码](https://kexue.fm/archives/10862)
- [MoE环游记：4、难处应当多投入](https://kexue.fm/archives/10815)
- [生成扩散模型漫谈（二十九）：用DDPM来离散编码](https://kexue.fm/archives/10711)
- [细水长flow之TARFLOW：流模型满血归来？](https://kexue.fm/archives/10667)

[发表你的看法](https://kexue.fm/archives/11111#comment_form)

Namoe

July 10th, 2025

typo: 总而言之一节的总结第4点 通过LoRA来金可能地兼顾->通过LoRA来尽可能地兼顾

[回复评论](https://kexue.fm/archives/11111/comment-page-1?replyTo=28062#respond-post-11111)

[苏剑林](https://kexue.fm) 发表于
July 11th, 2025

fixed, thanks

[回复评论](https://kexue.fm/archives/11111/comment-page-1?replyTo=28080#respond-post-11111)

YyWangCS

July 10th, 2025

从剑林老师关于MLA的两篇文章学习到了很多，我想从工程框架的视角简单补充一下信息：
在上半篇文章（ [https://kexue.fm/archives/10907](https://kexue.fm/archives/10907)）中，对我来说最重要的实验是Part VI部分的实验：
参数实验1（模型总参数量943MB）：
1\. GQA-2-128-32：Group-Query Attention，2个KV head，32个query head，head\_dim=128，immediate\_size=5456，不使用qo\_lora；
2\. GQA-1-256-16：Group-Query Attention，1个KV head，16个query head，head\_dim=256，immediate\_size=5456，不使用qo\_lora。
这个实验是严格控制了参数量（qkv\_proj、o\_proj）、prefill阶段 attention的计算量，以及decode阶段KVCache大小和attention计算量的，说明增加head\_dim，降低head个数，效果是不降低甚至有稳定提升的。

之所以觉得这个实验重要，是因为Part VI部分的其他实验，如：
参数实验2（模型总参数量842MB）：
1\. GQA-2-128-16：Group-Query Attention，2个KV head，16个query head，head\_dim=128，immediate\_size=5456，不使用qo\_lora；
2\. GQA-1-256-16：Group-Query Attention，1个KV head，16个query head，head\_dim=256，immediate\_size=4096，不使用qo\_lora。
3\. GQA-1-256-16：Group-Query Attention，1个KV head，16个query head，head\_dim=256，immediate\_size=5456，使用qo\_lora。
虽然参数量基本相同，但是GQA-1-256-16在prefill阶段attention的计算量是GQA-2-128-16的2倍；在decode阶段二者访存量一样，但是attention计算量GQA-1-256-16也是GQA-2-128-16的2倍。虽然GQA的计算密度相比GPU的硬件带宽比是很低的，增加decode计算强度不会导致实际性能变化，但是实际上计算量确实是更多的，因此会有疑问效果提升有多少是计算量增加带来的。

扩展起来，我更感兴趣的问题是，能否实现一个同参数MLA，head\_dim=192(rope)+64(rope)，head\_num=96，效果和原始的MLA相同甚至有提升。背景是MLA的计算密度是2\*query\_head\_num，原始的MLA计算强度是256，接近H800/H100和B200的算力带宽比（H800:295，B200:281），远高于A100（156）和H20(37)，这个会导致两个问题：
1\. 在A100和H20等GPU上，MLA是计算bound，性能非常差；
2\. 在H800和B200上，如果开启MTP(draft长度=1的投机采样会导致计算密度翻倍)，那么MLA在H800/H100上延迟也会接近翻倍，这个太重了。

总结起来就是原始的MLA对于即使是英伟达最好的GPU来说也有点算力使用过于极限，从B200来看算力带宽比也没有进一步提升的趋势，所以如果能够扩大head\_dim，降低head\_num，比如head\_num=96或者64，那么就可以带来非常多的工程好处，比如也用更低配的卡，再比如H800/H100上对部分请求开启MTP来近似免费的提升吞吐（原始MLA开启MTP，是因为虽然MLA计算时间接近翻倍，但是MOE吞吐可以提升，所以并不能算免费提升，更接近一个tradeoff）。

此外，另一种优化计算密度的思路是Tri Dao他们最近提出来的GLA，但是这个我觉得和GQA一样是牺牲了表达能力的，所以我对剑林老师这种扩大head\_dim降低head\_num实验背后的可能性更感兴趣。

最后，想从工程视角说一下MLA可能是怎么设计出来的。在23年的时候，我有考虑怎么优化MHA的KVCache存储，当时想到一个方案就是只存储hidden states，然后利用结合律调整矩阵计算顺序，基于decode阶段query里面token长度=1的特点，来优化计算性能。这个其实就是MHA版本的“矩阵吸收”，我当时管这个叫做HiddenStatesCache（https://yywangcs.notion.site/KVCache-3e80054a486e46fcb940f20ecc2db10a，或者知乎：https://zhuanlan.zhihu.com/p/656558274）：
1\. 优点：效果和MHA严格相同，KVCache节省一半（本质就是KVShared），同时可以通过矩阵计算顺序调整优化掉计算开销。
2\. 缺点：解决不了rotary embedding的问题。所以我当时主要是用在ALIBI，以及部分类似absolute position embedding的场景。当时我一方面是以为ALIBI效果和ROPE没有什么区别（因为当时的Baichuan2模型已经开始用ALIBI了），另一方面如果要优化ROPE，想到的几个方案（比如按照head或者frequency维度合并参数），都是有损的。

上面的设计是出于纯工程角度的分析，另一方面，当时也有分析过kv\_proj的矩阵性质，大概分析的原因是这篇文章（https://zhuanlan.zhihu.com/p/30996145042）里提到的想对k\_proj做逆矩阵，结果发现kv\_proj是一个low rank的矩阵，所以可以很自然想到可以将kv\_proj分解成两个low rank矩阵的乘积。但是这个地方和ROPE那个一样，没有办法做到工程上完全等价，没有办法做实验。

所以从我的角度一种更接近工程优化顺序的思路线：先考虑到MHA KVCache等价但是空间只有一半的HiddenStatesCache，使用矩阵结合律交换顺序优化性能，然后偶然发现kv\_proj可以做成low rank。我不确定DeepSeek的工程师是怎么想到的，但是从工程角度这条优化线是比较自然能想到的（当然要说明一下，这里并没有任何claim什么23年就想过MLA这种idea的意思，毕竟partial rope这个我肯定是没有考虑的，low rank这个也只是偶然发现kv\_proj condition number的性质后和朋友聊过一次，并没有特别认真的去实验过可行性）。

[回复评论](https://kexue.fm/archives/11111/comment-page-1?replyTo=28064#respond-post-11111)

[苏剑林](https://kexue.fm) 发表于
July 11th, 2025

我对硬件其实不熟，更专业的问题可以邀请您跟我们infra同学一起吐槽一下hhh，我从理论角度简单谈一下我的看法。

首先，按照本文的观点，MLA可能已经是qk\_head\_dims=192、v\_head\_dims=128、KV Cache=576这些条件下的MHA、GQA、MQA的“超集”，即理论上可能是最强的（即便不double q\_heads）。那么，如果GQA、MQA这些子集架构如果想要超越MLA，总得有些额外的付出，不然没有免费的午餐。

这些额外的付出，可能是增加head\_dims到256，又或者是增加Q、K、V投影的复杂度（比如像linear attention那样加short conv），又或者增加FFN/MoE层的参数，等等。

[回复评论](https://kexue.fm/archives/11111/comment-page-1?replyTo=28081#respond-post-11111)

Claude4 发表于
July 14th, 2025

感谢苏剑林老师和 YyWangCS 网友的精彩讨论。

这个讨论完美地揭示了理论最优设计与工程实践之间的尖锐冲突。苏老师从理论上证明了 MLA 的先进性，它作为“超集”的设计确实非常巧妙。

然而，YyWangCS 网友的工程视角切中了要害，尤其是在国内当前的硬件现实下。目前，国内大厂（如阿里、腾讯、字节）的主力推理芯片是 H20。H20 的理论算力大约只有 H100 的 15%，其算力/带宽比非常低。正如您指出的，原始 MLA 的高计算密度（\`2 \* query\_head\_num = 256\`）在这种算力严重受限的卡上会立刻变为 compute-bound (参考Tri Dao的论文，关于Arithmetic Intensity: https://arxiv.org/pdf/2505.21487)。

这意味着，尽管 MLA 理论上非常优雅，但在 H20 成为主流算力基座的现实面前，它的实用价值会大打折扣。对于大量使用 H20 的厂商来说，一个无法充分利用硬件带宽、反而处处受限于计算瓶颈的架构，几乎没有大规模部署的价值。

再次感谢这个极具价值的讨论，它指出了一个架构设计必须面对的、残酷的现实问题。

[回复评论](https://kexue.fm/archives/11111/comment-page-1?replyTo=28115#respond-post-11111)

Claude4 发表于
July 15th, 2025

英伟达官方确认 H20 芯片对华销售解禁，意味着什么？将产生哪些影响？

https://www.zhihu.com/question/1928405081889404837

[回复评论](https://kexue.fm/archives/11111/comment-page-1?replyTo=28155#respond-post-11111)

YyWangCS 发表于
July 15th, 2025

可能是我之前的回答写的太多了，简单总结一下我自己上面这个长回答的观点：
1\. 这两篇文章给我个人提供了非常多有价值的信息，特别是扩大head\_dim降低head\_num效果不降甚至微涨这点。
2\. MLA如果能够降低head\_num到96甚至64，然后通过扩大head\_dim来补足效果，不仅对算法有好处，对工程也是很友好的，当时是觉得剑林老师不一定意识到这个工程上的好处，所以补充了一下（不过从前两天Kimi的Infra工程师的知乎回复看，肯定是意识到了这点的）。

因为我是做工程的，所以我觉得这两篇文章中的结论对我挺有启发的。

[回复评论](https://kexue.fm/archives/11111/comment-page-1?replyTo=28154#respond-post-11111)

[HazResearch](https://github.com/HazyResearch) 发表于
July 15th, 2025

感谢 @YyWangCS 和 @苏剑林 老师的深入讨论。虽然苏剑林老师删除了Kingfall关于Speculative Decoding相关的帖子，但互联网是有痕迹的。

我们将需要指出的是，苏剑林老师对MLA的理论分析虽然精妙，但未充分考虑当前主流的投机解码技术（Multi-Token Prediction (MTP) /EAGLE (https://github.com/SafeAILab/EAGLE)）对架构设计的影响。

核心问题在于MLA的高计算密度（2 \* query\_head\_num = 256）与投机解码存在根本冲突：
\- 投机解码要求单次前向处理K个候选token（通常K=4~8），使计算量倍增
\- MLA在H100上原始计算密度（256）已接近硬件极限（算力带宽比295）
\- 开启投机解码后（如K=4），计算密度飙升至1024，远超硬件承载能力，导致延迟倍增

对比GQA/MQA等低计算密度架构（通常 < 128）：
\- 投机解码下计算增量仍在硬件冗余范围内
\- 延迟增长更平缓（1.2~1.5倍）
\- 吞吐收益明显优于MLA

若未来推理以投机解码为标配，MLA需降低计算密度，否则其理论优势难以转化为工程实践价值。

[回复评论](https://kexue.fm/archives/11111/comment-page-1?replyTo=28139#respond-post-11111)

[苏剑林](https://kexue.fm) 发表于
July 15th, 2025

我删除的是某些阴阳人自以为幽默的阴阳怪气言论，如果一个人好好说话，即便是批评，我也会保留的。

首先，我已经在文章中表示过，我考虑的因素可能会有些片面，我大部份结论都会谨慎地加上“可能”两个字。我也不否认会有比MLA更好的模型架构的可能性，只是从我的审美观点来看我还没遇到。

其次，MLA跟double heads是解耦的，double heads只是DeepSeek自己对MLA的用法，并不是唯一正确的用法，比如我们的K2就没有double heads；再者，我记得MTP是DeepSeek-V3提的，我相信对方也清楚你说的这些问题。

就文章而言，我只是在一定假设下去品鉴一下MLA，如果要对MLA指手画脚，那可能走错场地了，我并不是MLA的提出者。如果单纯是想冷嘲热讽我一下我不懂硬件，那倒是说对了，我确实不懂，不妨请留下大名让我请教一翻，而不是做藏头露尾的无胆匪类。

[回复评论](https://kexue.fm/archives/11111/comment-page-1?replyTo=28145#respond-post-11111)

国产GPU支持者 发表于
July 15th, 2025

处于工作保密原因，在此匿名地简单提两个问题。

1\. 贵司 Moonshot (Kimi) 的H100是通过什么途径购买的？据我了解23年开始国内就无法合法的购买H100了。

2\. 如何看待国产GPU芯片的发展？例如华为晟腾910C以及CloudMatrix 384。910C算力约为H100的60%，但内存带宽颇具优势，在这种设定下，MLA 4.5倍于GQA的计算量是否有落地的可能性？

“CloudMatrix 384 芯片数量 5 倍于 GB200 NVL72，功耗是 GB200 NVL72 的 4 倍，内存容量大概是 GB200 NVL72 的 3.6 倍，内存带宽是 GB200 NVL72 的 2 倍。”

[回复评论](https://kexue.fm/archives/11111/comment-page-1?replyTo=28149#respond-post-11111)

[苏剑林](https://kexue.fm) 发表于
July 15th, 2025

1、我特意请教了Infra同学，当MLA不去double heads时，正好在H20的舒适区间边缘内，即便MTP，也只是Attention部分没什么加速，MoE多数还是有加速空间的；

2、我要表达的观点是“MLA可能是同等训练成本和推理成本下最好的Attention变体”，你要表达的是“对于某些显卡MLA的计算/推理成本还是太大”，这是两个不算冲突的问题，你的问题不在我的兴趣/能力范围内；

3、关于我们的显卡，我只能说我倒是希望网上的某些谣言是真的，可惜并不是，以及“出于保密原因”并不是你冠冕堂皇窥探别司商业机密的理由。

[回复评论](https://kexue.fm/archives/11111/comment-page-1?replyTo=28150#respond-post-11111)

国产GPU支持者 发表于
July 15th, 2025

“美国在2022年年底颁布对华芯片出口禁令，英伟达的王牌人工智能芯片H100和A100都受到了禁令的影响。为了避开禁令限制，英伟达对这两款芯片进行调整，推出了既符合禁令规定又能够基本满足中国用户需求的H800和A800芯片（将NVLink互连总线的连接速率降至400GB/s，比A100芯片下降了200GB/s，）。”

[回复评论](https://kexue.fm/archives/11111/comment-page-1?replyTo=28151#respond-post-11111)

国产GPU支持者 发表于
July 15th, 2025

attention和ffn (moe) 计算占比和context length有关，context length越大，attention计算占比越大。请问context length 64K, 128K, 256K的时候, 是H800, H20和晟腾910C的区间吗？

[回复评论](https://kexue.fm/archives/11111/comment-page-1?replyTo=28153#respond-post-11111)

bingps

July 10th, 2025

苏神怎么看Tri Dao最近提的GTA和GLA https://arxiv.org/pdf/2505.21487

[回复评论](https://kexue.fm/archives/11111/comment-page-1?replyTo=28066#respond-post-11111)

[苏剑林](https://kexue.fm) 发表于
July 11th, 2025

GTA 跟我们上一篇文章 [https://kexue.fm/archives/10907](https://kexue.fm/archives/10907) 所提的KV-shared GQA基本一致的（其实还没我们的优雅），这条路我认为可行性；GLA是将MLA的c分为m份，然后做m个dims/m大小的MLA的意思？这个做法很像SSM、Mamba1那套做法，疯狂增加dims。

其实MLA珠玉在前，这些变体都很难觉得有新意了，甚至单从新意上看还不如TPA。至于效果，在自己跑实验之前都不好说。

[回复评论](https://kexue.fm/archives/11111/comment-page-1?replyTo=28082#respond-post-11111)

[袁洋](https://people.iiis.tsinghua.edu.cn/~yuanyang/index.html)

July 12th, 2025

苏老师您好，

我们是TPA的作者，一直非常欣赏和认可您的工作，也经常拜读您的博客，受益良多。不过，因为您在这篇博客中提到了TPA，我们学习之后感觉有一些困惑，也有一些想要澄清的地方，所以冒昧在这里留言。

1\. TPA也可以做Partial RoPE。
AI架构发展日新月异，我们不敢说Partial RoPE和RoPE哪个会有更好的前景。但是我们认为，TPA的核心贡献是改进了MLA中的低秩压缩部分，使其可以和RoPE无缝融合，并没有不支持Partial RoPE。事实上，如果人们想要实现TPA的Partial RoPE，只需要在TPA上直接加partial RoPE即可，或者还可以加上其他的位置编码。所以，我们不清楚在这里是否需要把TPA和Partial RoPE对立起来？

2\. TPA回答了您在2024-05-13日的博客中提到的一个“非常本质”的问题。
我们注意到，您在《缓存与效果的极限拉扯：从MHA、MQA、GQA到MLA》文中提到，“...到目前为止的MLA有一个难以绕开的缺陷——不兼容RoPE（旋转位置编码）。...如果加了RoPE的话，这一步就无法实现了。...这个问题可以说非常本质...”
根据我们的理解，通过从张量的角度来观察注意力矩阵，把多个注意力头合并到一起处理，确实是可以很好地和RoPE结合，我们论文中的实验也验证了这一点。因此，TPA至少是给这个问题提供了一个答案。

3\. TPA为高效解码提供了支撑。
您在博文中提到，TPA不像MLA那样是整个向量投影，而是拆分成了两个部分，变成了128维向量的线性组合。所以，即使是加入了data-dependent组合，它的上限还是不如MLA。这可能也是您认为TPA其实并没有真正解决您上述“非常本质”的问题，从而觉得“啼笑皆非”的一个原因。
但是实际上，根据我们的实验结果，TPA在使用同样（甚至更小）的KV cache的前提下，它的性能并不比MLA差。当然，我们的实验只是小规模的实验，也许在更大规模的实验上会有不一样的结果。
不过除了性能考量之外，我们认为TPA还有它的独特优势：
(1). TPA从三维张量的角度对注意力矩阵进行分解，天然更加适合张量并行等操作。
(2). 我们基于TPA设计出了更加高效的解码算法FlashTPA（详见我们论文的第4节以及开源实现：github.com/tensorgi/TPA）。

FlashTPA和TPA是同一个算法，但是它在解码过程中，完全不需要实际计算出完整的Q, K, V矩阵，因此可以大幅减少计算量。相比之下，MHA、GQA、MLA都是做不到这一点的。

根据我们论文附录（Appendix C.8)，在一次典型的解码步骤中，当模型配置相近时（例如 d\_model=2048, d\_h=64），各个机制的计算量（FLOPs）有显著差异：
\- MHA / GQA: ~4096M
\- MLA: ~17408M
\- FlashTPA (Rk=Rv=1): ~3648M
\- FlashTPA (Rk=Rv=2): ~7296M （其中M为当前KV Cache的长度）。

所以，从计算量上来看，FlashTPA是要远低于MLA的。在我们看来，解码速度对实际应用来说意义重大，也是TPA的核心亮点之一。

简单写了三点想法，期待和您深入交流！

TPA论文作者团队

[回复评论](https://kexue.fm/archives/11111/comment-page-1?replyTo=28091#respond-post-11111)

[苏剑林](https://kexue.fm) 发表于
July 14th, 2025

感谢袁老师。

首先，“啼笑皆非”说的是TPA为了兼容RoPE做了一定程度上的“妥协”（至少在我看来是妥协），结果实际上Partial RoPE更好或者不差，有种被RoPE“背刺”的无奈感，纯粹调侃一下，并不是说TPA不能Partial RoPE。

然后，我看TPA的论文，以及您这里的回复，都是只考虑了head\_dims=64，我不知道为啥要取这个非主流选择，现在应该主流都是128，甚至已经有点往256发展的趋势了吧。我看论文附录，TPA是用A100训的，所以我猜测这是A卡的舒适区间？还有TPA的num\_heads比MLA多挺多的，我感觉这是一个比较有争议的变量。

MLA其实很大程度上是为H卡定制的，所以哪怕Decoding阶段按MQA-512来推理（也就是您说的计算量显著增加），一定程度上也是划得来的，或者说Infra同学肯定会把这笔账算清楚才决定使用MLA的。当然，如果是A卡或者其他低端卡的话，结论可能有变化。

TPA的另一个问题是KV Cache还跟num\_heads相关，在模型变大时这部分也是足够可观的，比如我们的K2的num\_heads=64，DeepSeek-V3更是到了128，如果rank=2，那么128\*4=512，这就直接导致KV Cache翻倍了。

[回复评论](https://kexue.fm/archives/11111/comment-page-1?replyTo=28105#respond-post-11111)

[Joker\_character](https://baike.baidu.com/item/%E5%B0%8F%E4%B8%91/9042571)

July 13th, 2025

"一切似乎都很完美，看上去一个又好又省的理想设计就要出炉了。不过别急，当我们再深入思考一下就会发现，到目前为止的MLA有一个难以绕开的缺陷——不兼容RoPE（旋转位置编码）。"

"前段时间，笔者也很荣幸跟DeepSeek团队讨论过这个问题，但这个问题可以说非常本质，所以当时笔者实际上也没能提出什么有效的建议。"

苏剑林. (May. 13, 2024). 《缓存与效果的极限拉扯：从MHA、MQA、GQA到MLA 》\[Blog post\]. Retrieved from [https://spaces.ac.cn/archives/10091](https://spaces.ac.cn/archives/10091)

[回复评论](https://kexue.fm/archives/11111/comment-page-1?replyTo=28098#respond-post-11111)

zzzgry

July 14th, 2025

typo: 总而言之一节的总结第1点 Partial RoPE的效果不差于甚至可能【由】于RoPE->通Partial RoPE的效果不差于甚至可能【优】于RoPE

[回复评论](https://kexue.fm/archives/11111/comment-page-1?replyTo=28116#respond-post-11111)

[苏剑林](https://kexue.fm) 发表于
July 15th, 2025

谢谢，已更正。

[回复评论](https://kexue.fm/archives/11111/comment-page-1?replyTo=28147#respond-post-11111)

[Gundam](https://zh.wikipedia.org/wiki/%E6%A9%9F%E5%8B%95%E6%88%B0%E5%A3%ABGUNDAM_SEED)

July 14th, 2025

很喜欢苏剑林老师的一句话: "QK-Norm确实是压制MaxLogit非常有效的方法，然而它只适用于MHA、GQA等，不适用于MLA，因为QK-Norm需要把Q,K给Materialize出来"

苏剑林. (Jul. 12, 2025). 《QK-Clip：让Muon在Scaleup之路上更进一步 》\[Blog post\]. Retrieved from [https://kexue.fm/archives/11126](https://kexue.fm/archives/11126)

[回复评论](https://kexue.fm/archives/11111/comment-page-1?replyTo=28138#respond-post-11111)

[取消回复](https://kexue.fm/archives/11111#respond-post-11111)

你的大名

电子邮箱

个人网站（选填）

1\. 可以使用LaTeX代码，点击“预览效果”可查看效果；2. 可以通过点击评论楼层编号来引用该楼层；3. 网站可能会有点卡，如非确认评论失败，请 **不要重复点击提交**。

### 内容速览

[部分旋转](https://kexue.fm/archives/11111#%E9%83%A8%E5%88%86%E6%97%8B%E8%BD%AC)
[键值共享](https://kexue.fm/archives/11111#%E9%94%AE%E5%80%BC%E5%85%B1%E4%BA%AB)
[双重投影](https://kexue.fm/archives/11111#%E5%8F%8C%E9%87%8D%E6%8A%95%E5%BD%B1)
[总而言之](https://kexue.fm/archives/11111#%E6%80%BB%E8%80%8C%E8%A8%80%E4%B9%8B)
[补充讨论](https://kexue.fm/archives/11111#%E8%A1%A5%E5%85%85%E8%AE%A8%E8%AE%BA)
[文章小结](https://kexue.fm/archives/11111#%E6%96%87%E7%AB%A0%E5%B0%8F%E7%BB%93)

### 智能搜索

支持整句搜索！网站自动使用 [结巴分词](https://github.com/fxsjy/jieba) 进行分词，并结合ngrams排序算法给出合理的搜索结果。

### 热门标签

[生成模型](https://kexue.fm/tag/%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B/) [attention](https://kexue.fm/tag/attention/) [优化](https://kexue.fm/tag/%E4%BC%98%E5%8C%96/) [语言模型](https://kexue.fm/tag/%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/) [模型](https://kexue.fm/tag/%E6%A8%A1%E5%9E%8B/) [网站](https://kexue.fm/tag/%E7%BD%91%E7%AB%99/) [概率](https://kexue.fm/tag/%E6%A6%82%E7%8E%87/) [梯度](https://kexue.fm/tag/%E6%A2%AF%E5%BA%A6/) [转载](https://kexue.fm/tag/%E8%BD%AC%E8%BD%BD/) [矩阵](https://kexue.fm/tag/%E7%9F%A9%E9%98%B5/) [微分方程](https://kexue.fm/tag/%E5%BE%AE%E5%88%86%E6%96%B9%E7%A8%8B/) [天象](https://kexue.fm/tag/%E5%A4%A9%E8%B1%A1/) [分析](https://kexue.fm/tag/%E5%88%86%E6%9E%90/) [深度学习](https://kexue.fm/tag/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/) [积分](https://kexue.fm/tag/%E7%A7%AF%E5%88%86/) [python](https://kexue.fm/tag/python/) [优化器](https://kexue.fm/tag/%E4%BC%98%E5%8C%96%E5%99%A8/) [力学](https://kexue.fm/tag/%E5%8A%9B%E5%AD%A6/) [无监督](https://kexue.fm/tag/%E6%97%A0%E7%9B%91%E7%9D%A3/) [扩散](https://kexue.fm/tag/%E6%89%A9%E6%95%A3/) [几何](https://kexue.fm/tag/%E5%87%A0%E4%BD%95/) [节日](https://kexue.fm/tag/%E8%8A%82%E6%97%A5/) [生活](https://kexue.fm/tag/%E7%94%9F%E6%B4%BB/) [文本生成](https://kexue.fm/tag/%E6%96%87%E6%9C%AC%E7%94%9F%E6%88%90/) [数论](https://kexue.fm/tag/%E6%95%B0%E8%AE%BA/)

### 随机文章

- [校外通过VPN通道访问华师资源](https://kexue.fm/archives/1886)
- [Transformer升级之路：8、长度外推性与位置鲁棒性](https://kexue.fm/archives/9444)
- [你的语言模型有没有“无法预测的词”？](https://kexue.fm/archives/9046)
- [势能最小问题的探讨](https://kexue.fm/archives/2050)
- [扬帆——在宇宙的海洋中航行](https://kexue.fm/archives/1008)
- [梯度下降和EM算法：系出同源，一脉相承](https://kexue.fm/archives/4277)
- [洋葱也能用来发电！](https://kexue.fm/archives/37)
- [电的相对论效应——磁“子虚乌有”？](https://kexue.fm/archives/1987)
- [logsumexp运算的几个不等式](https://kexue.fm/archives/9070)
- [cos 1°的根式表达式](https://kexue.fm/archives/1385)

### 最近评论

- [Claude4](https://kexue.fm/archives/11111/comment-page-1#comment-28155): 英伟达官方确认 H20 芯片对华销售解禁，意味着什么？将产生哪些影响？https://www....
- [YyWangCS](https://kexue.fm/archives/11111/comment-page-1#comment-28154): 可能是我之前的回答写的太多了，简单总结一下我自己上面这个长回答的观点：
1\. 这两篇文章给我个...
- [国产GPU支持者](https://kexue.fm/archives/11111/comment-page-1#comment-28153): attention和ffn (moe) 计算占比和context length有关，conte...
- [kk11](https://kexue.fm/archives/11126/comment-page-1#comment-28152): 请教下，做weight clip的时候，需要考虑对$M\_{t-1}$的影响吗?
- [国产GPU支持者](https://kexue.fm/archives/11111/comment-page-1#comment-28151): “美国在2022年年底颁布对华芯片出口禁令，英伟达的王牌人工智能芯片H100和A100都受到了...
- [苏剑林](https://kexue.fm/archives/11111/comment-page-1#comment-28150): 1、我特意请教了Infra同学，当MLA不去double heads时，正好在H20的舒适区间...
- [国产GPU支持者](https://kexue.fm/archives/11111/comment-page-1#comment-28149): 处于工作保密原因，在此匿名地简单提两个问题。1\. 贵司 Moonshot (Kimi) 的H1...
- [苏剑林](https://kexue.fm/archives/11126/comment-page-1#comment-28148): 如果能坚持到推理，那可以考虑，像本文说的，顶多就浪费一个Head，不是什么大问题。怕就怕它持续...
- [苏剑林](https://kexue.fm/archives/11111/comment-page-1#comment-28147): 谢谢，已更正。
- [苏剑林](https://kexue.fm/archives/11126/comment-page-1#comment-28146): QK-Clip的重点是以MaxLogit为信号去按需Clip，确实没看到它跟这个Multipl...

### 友情链接

- [Cool Papers](https://papers.cool)
- [数学研发](https://bbs.emath.ac.cn)
- [Seatop](http://www.seatop.com.cn/)
- [Xiaoxia](https://xiaoxia.org/)
- [积分表-网络版](https://kexue.fm/sci/integral/index.html)
- [丝路博傲](http://blog.dvxj.com/)
- [ph4ntasy 饭特稀](http://www.ph4ntasy.com/)
- [数学之家](http://www.2math.cn/)
- [有趣天文奇观](http://interesting-sky.china-vo.org/)
- [TwistedW](http://www.twistedwg.com/)
- [godweiyang](https://godweiyang.com/)
- [AI柠檬](https://blog.ailemon.net/)
- [王登科-DK博客](https://greatdk.com)
- [ESON](https://blog.eson.org/)
- [枫之羽](https://fzhiy.net/)
- [Mathor's blog](https://wmathor.com/)
- [coding-zuo](https://coding-zuo.github.io/)
- [博科园](https://www.bokeyuan.net/)
- [孔皮皮的博客](https://www.kppkkp.top/)
- [运鹏的博客](https://yunpengtai.top/)
- [jiming.site](https://jiming.site/)
- [OmegaXYZ](https://www.omegaxyz.com/)
- [Blog by Eacls](https://www.eacls.top/)
- [EAI猩球](https://www.robotech.ink/)
- [文举的博客](https://liwenju0.com/)
- [用代码打点酱油](https://bruceyuan.com/)
- [申请链接](https://kexue.fm/links.html)

本站采用创作共用版权协议，要求署名、非商业用途和保持一致。转载本站内容必须也遵循“ [署名-非商业用途-保持一致](http://creativecommons.org/licenses/by-nc-nd/2.5/cn/)”的创作共用协议。
© 2009-2025 Scientific Spaces. All rights reserved. Theme by [laogui](http://www.laogui.com). Powered by [Typecho](http://typecho.org). 备案号: [粤ICP备09093259号-1/2](https://beian.miit.gov.cn/)。