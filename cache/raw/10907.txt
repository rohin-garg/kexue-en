## SEARCH

## MENU

- [打赏](https://kexue.fm/reward.html)
- [公式](https://kexue.fm/latex.html)
- [天象](https://kexue.fm/ac.html)
- [链接](https://kexue.fm/links.html)
- [时光](https://kexue.fm/me.html)
- [博览](https://kexue.fm/science.html)
- [归档](https://kexue.fm/content.html)

## CATEGORIES

- [千奇百怪](https://kexue.fm/category/Everything)
- [天文探索](https://kexue.fm/category/Astronomy)
- [数学研究](https://kexue.fm/category/Mathematics)
- [物理化学](https://kexue.fm/category/Phy-chem)
- [信息时代](https://kexue.fm/category/Big-Data)
- [生物自然](https://kexue.fm/category/Biology)
- [图片摄影](https://kexue.fm/category/Photograph)
- [问题百科](https://kexue.fm/category/Questions)
- [生活/情感](https://kexue.fm/category/Life-Feeling)
- [资源共享](https://kexue.fm/category/Resources)

## NEWPOSTS

- [让炼丹更科学一些（四）：新恒等式，...](https://kexue.fm/archives/11494)
- [为什么DeltaNet要加L2 N...](https://kexue.fm/archives/11486)
- [让炼丹更科学一些（三）：SGD的终...](https://kexue.fm/archives/11480)
- [让炼丹更科学一些（二）：将结论推广...](https://kexue.fm/archives/11469)
- [滑动平均视角下的权重衰减和学习率](https://kexue.fm/archives/11459)
- [生成扩散模型漫谈（三十一）：预测数...](https://kexue.fm/archives/11428)
- [Muon优化器指南：快速上手与关键细节](https://kexue.fm/archives/11416)
- [AdamW的Weight RMS的...](https://kexue.fm/archives/11404)
- [n个正态随机数的最大值的渐近估计](https://kexue.fm/archives/11390)
- [流形上的最速下降：5\. 对偶梯度下降](https://kexue.fm/archives/11388)

## COMMENTS

- [JamesSand: 谢谢苏神我发现问题在 muon 的 adjust lr 上边如...](https://kexue.fm/archives/11416/comment-page-1#comment-29064)
- [JamesSand: 苏神您好，想问下用 muon 在 adamw pretrian...](https://kexue.fm/archives/11416/comment-page-1#comment-29063)
- [printemps: 苏神怎么看log-linear attention这种变长的h...](https://kexue.fm/archives/11320/comment-page-1#comment-29062)
- [chris stars: 我看这篇论文之前觉得会很厉害，但是看完之后有一种还好的感觉，好...](https://kexue.fm/archives/11428/comment-page-1#comment-29060)
- [sk: 太棒了](https://kexue.fm/content.html/comment-page-1#comment-29059)
- [Jason小白: 苏神您好，从知乎了解到这个space，下面我也贴了我使用Muo...](https://kexue.fm/archives/11416/comment-page-1#comment-29058)
- [且寻: same。本来想转成二重积分的，但想半天整不出来，搞出来这个半...](https://kexue.fm/archives/11480/comment-page-1#comment-29057)
- [苏剑林: 是梯度均值为零的假设。这个问题不是在“数值模拟”一节讨论过了吗...](https://kexue.fm/archives/11267/comment-page-1#comment-29056)
- [苏剑林: X与单位阵的平均平方误差(mse)，作为它跟单位阵的差距，有什...](https://kexue.fm/archives/7180/comment-page-2#comment-29055)
- [苏剑林: 从公式$(12)$到公式$(15)$，都在推导和解释你说的这个...](https://kexue.fm/archives/9209/comment-page-8#comment-29054)

## USERLOGIN

- [登录](https://kexue.fm/admin/login.php)

[科学空间\|Scientific Spaces](https://kexue.fm)

- [登录](https://kexue.fm/admin/login.php)
- [打赏](https://kexue.fm/reward.html)
- [公式](https://kexue.fm/latex.html)
- [天象](https://kexue.fm/ac.html)
- [链接](https://kexue.fm/links.html)
- [时光](https://kexue.fm/me.html)
- [博览](https://kexue.fm/science.html)
- [归档](https://kexue.fm/content.html)

渴望成为一个小飞侠

- [欢迎订阅](https://kexue.fm/feed)
- [个性邮箱](https://kexue.fm/archives/119)
- [天象信息](https://kexue.fm/ac.html)
- [观测ISS](https://kexue.fm/archives/41)
- [LaTeX](https://kexue.fm/latex.html)
- [关于博主](https://kexue.fm/me.html)

欢迎访问“科学空间”，这里将与您共同探讨自然科学，回味人生百态；也期待大家的分享～

- [**千奇百怪** Everything](https://kexue.fm/category/Everything)
- [**天文探索** Astronomy](https://kexue.fm/category/Astronomy)
- [**数学研究** Mathematics](https://kexue.fm/category/Mathematics)
- [**物理化学** Phy-chem](https://kexue.fm/category/Phy-chem)
- [**信息时代** Big-Data](https://kexue.fm/category/Big-Data)
- [**生物自然** Biology](https://kexue.fm/category/Biology)
- [**图片摄影** Photograph](https://kexue.fm/category/Photograph)
- [**问题百科** Questions](https://kexue.fm/category/Questions)
- [**生活/情感** Life-Feeling](https://kexue.fm/category/Life-Feeling)
- [**资源共享** Resources](https://kexue.fm/category/Resources)

- [**千奇百怪**](https://kexue.fm/category/Everything)
- [**天文探索**](https://kexue.fm/category/Astronomy)
- [**数学研究**](https://kexue.fm/category/Mathematics)
- [**物理化学**](https://kexue.fm/category/Phy-chem)
- [**信息时代**](https://kexue.fm/category/Big-Data)
- [**生物自然**](https://kexue.fm/category/Biology)
- [**图片摄影**](https://kexue.fm/category/Photograph)
- [**问题百科**](https://kexue.fm/category/Questions)
- [**生活/情感**](https://kexue.fm/category/Life-Feeling)
- [**资源共享**](https://kexue.fm/category/Resources)

[首页](https://kexue.fm) [信息时代](https://kexue.fm/category/Big-Data) Transformer升级之路：20、MLA好在哪里?（上）

4May

# [Transformer升级之路：20、MLA好在哪里?（上）](https://kexue.fm/archives/10907)

By 苏剑林 \|
2025-05-04 \|
92018位读者\|

自从DeepSeek爆火后，它所提的Attention变体MLA（ **M** ulti-head **L** atent **A** ttention）也愈发受到关注。MLA通过巧妙的设计实现了MHA与MQA的自由切换，使得模型可以根据训练和推理的不同特性（Compute-Bound or Memory-Bound）选择最佳的形式，尽可能地达到效率最大化。

诚然，MLA很有效，但也有观点认为它不够优雅，所以寻找MLA替代品的努力一直存在，包括我们也有在尝试。然而，经过一段时间的实验，我们发现很多KV Cache相同甚至更大的Attention变体，最终效果都不如MLA。这不得不让我们开始反思：MLA的出色表现背后的关键原因究竟是什么？

接下来，本文将详细介绍笔者围绕这一问题的思考过程以及相关实验结果。

## 观察 [\#](https://kexue.fm/kexue.fm\#%E8%A7%82%E5%AF%9F)

MLA提出自 [DeepSeek-V2](https://papers.cool/arxiv/2405.04434)，本文假设读者已经熟悉MLA，至少了解之前的博客 [《缓存与效果的极限拉扯：从MHA、MQA、GQA到MLA》](https://kexue.fm/archives/10091) 所介绍的内容，因此MLA自身的细节将不会过多展开。

MLA的主要特点如下：

> 1、MLA在训练阶段是一个qk\_head\_dims=(128+64)、v\_head\_dims=128的MHA；
>
> 2、MLA在解码阶段是一个qk\_head\_dims=(512+64)、v\_head\_dims=512、KV-Shared的MQA；
>
> 3、MLA的\[qc, qr\]、\[kc, kr\]拼接，可以理解为一种 [Partial RoPE](https://kexue.fm/archives/10122#%E9%83%A8%E5%88%86%E6%97%8B%E8%BD%AC)。

## 猜测 [\#](https://kexue.fm/kexue.fm\#%E7%8C%9C%E6%B5%8B)

MHA、GQA常用的head\_dims是128，而对于MLA来说，不管是从训练看的128+64，还是从推理看的512+64，都要大于128，再结合 [《突破瓶颈，打造更强大的Transformer》](https://kexue.fm/archives/7325) 的经验，我们有：

> **猜测1**： 增大head\_dims是MLA好的关键之一。

另外，KV-Shared这个特性，可以在同等KV Cache大小下，增大GQA的head\_dims或者num\_groups，所以有：

> **猜测2**： KV-Shared是MLA好的关键之一。

最后，此前有一些理论和实验显示Partial RoPE可能会对效果有正面帮助（参考 [《Transformer升级之路：18、RoPE的底数选择原则》](https://kexue.fm/archives/10122#%E9%83%A8%E5%88%86%E6%97%8B%E8%BD%AC)），所以有

> **猜测3**： Partial RoPE是MLA好的关键之一。

## 实验 [\#](https://kexue.fm/kexue.fm\#%E5%AE%9E%E9%AA%8C)

现在我们通过实验逐一检验以上猜测。

### 设置 [\#](https://kexue.fm/kexue.fm\#%E8%AE%BE%E7%BD%AE)

所有实验公共部分的超参数如下：

> 1、类似LLAMA3的Dense模型；
>
> 2、hidden\_size=2048，num\_layers=12，num\_heads=16；
>
> 3、优化器是 [Muon](https://kexue.fm/archives/10592)，Attention部分per head更新；
>
> 4、训练长度为4096，总tokens数为16B，总训练步数为16k；
>
> 5、所有实验都是只改变Attention，所以参数量不会严格对齐。

### Part I [\#](https://kexue.fm/kexue.fm\#Part%20I)

MLA的KV Cache大小是512+64，约等于GQA2-128（第一个数字是num\_groups，第二个数字是head\_dims），所以对比的baseline为GQA2-128和GQA1-256。为了验证Partial RoPE，我们增加了GQA1-256-PR，具体做法是将Q、K的256 dims分成192+64两部分，在64上加RoPE，192不加。

结果如下：
$$\\begin{array}{c\|ccc}
\\hline
& \\text{Params} & \\text{Loss} & \\text{Cache} \\\
\\hline
\\text{MLA} & 894M & 2.721 & 576 \\\
\\text{GQA2-128} & 842M & 2.75 & 512 \\\
\\text{GQA1-256} & 943M & 2.72 & 512 \\\
\\text{GQA1-256-PR} & 943M & 2.711 & 512 \\\
\\hline
\\end{array}$$

即
$$\\text{GQA2-128} < \\text{MLA} \\lesssim \\text{GQA1-256} < \\text{GQA1-256-PR}$$
初步验证了增大head\_dims和Partial RoPE的作用。这样看来，MLA的设计中，RoPE和NoPE拼接这部分看似无奈的设计，极有可能是它效果优异的关键原因！原论文声称MLA甚至优于MHA，大概率也是因为所对比的MHA的head\_dims只有128。

### Part II [\#](https://kexue.fm/kexue.fm\#Part%20II)

为了进一步验证增大head\_dims的作用，我们另外跑了MHA、GQA2-192、MLA-256三个实验，MHA是head\_dims=128的常规MHA，GQA2-192是直接增大GQA2的head\_dims到192，MLA-256是将MLA的128+64提升到192+64，对照如下

$$\\begin{array}{c\|ccc}
\\hline
& \\text{Params} & \\text{Loss} & \\text{Cache} \\\
\\hline
\\text{MHA} & 931M & 2.721 & 4096 \\\
\\text{MLA} & 894M & 2.721 & 576 \\\
\\text{MLA-256} & 989M & 2.705 & 576 \\\
\\text{GQA2-128} & 842M & 2.75 & 512 \\\
\\text{GQA2-192} & 899M & 2.729 & 768 \\\
\\text{GQA1-256} & 943M & 2.72 & 512 \\\
\\text{GQA1-256-PR} & 943M & 2.711 & 512 \\\
\\hline
\\end{array}$$

可以看到，MHA总参数量更多，KV Cache更是7倍于MLA，但Loss才堪堪追平MLA，这跟DeepSeek-V2里边的结论接近。此外，GQA2-192优于GQA2-128，但不如GQA1-256；MLA的head\_dims升到(192+64)后，相比(128+64)也还能进一步提升效果。这些现象都表明，增加head\_dims远比增加num\_groups更有效。

### Part III [\#](https://kexue.fm/kexue.fm\#Part%20III)

接下来我们验证KV-Shared，即K、V共享全部或大部分dims。这里我们主要考虑的替代品是head\_dims不超过256的GQA，并且控制KV Cache的总大小跟MLA接近，所以当KV-Shared时，我们可以至多可以考虑GQA2-256。

由于KV-Shared跟RoPE不完全兼容，参考MLA的做法，我们将256分成192+64两部分，其中

> 1、192部分不加RoPE，在K、V间共享；
>
> 2、64部分加RoPE，只用于K；
>
> 3、V另外再投影64 dims，concat到共享的192 dims上去。

这样一来，K、V的head\_dims都是256，KV Cache总大小是(192+64+64)\*2=640，略大于MLA的512+64=576，这个版本我们简记为“GQA2-(192+64)-S1”，其实“S1”是“Shared-1”的缩写。

### Part IV [\#](https://kexue.fm/kexue.fm\#Part%20IV)

另外一种KV-Shared的方案是：

> 1、192部分不加RoPE，在K、V间共享；
>
> 2、64部分加RoPE，同样在K、V间共享；
>
> 3、做Attention，由于V带RoPE，此时是绝对位置编码效果；
>
> 4、为了保证相对位置编码，将输出分成192+64两部分，64部分再加一次逆向RoPE。

这种做法是K、V完全共享，KV Cache大小是(192+64)\*2=512，略小于MLA。这个版本我们称为“GQA2-(192+64)-S2”，“S2”是“Shared-2”的缩写，背后的原理是笔者新提出的VO-RoPE，参考 [《Transformer升级之路：19、第二类旋转位置编码》](https://kexue.fm/archives/10862)。

### Part V [\#](https://kexue.fm/kexue.fm\#Part%20V)

另外，根据同样思路补了几个GQA4和GQA1的实验。所有实验结果汇总如下：
$$\\begin{array}{c\|ccc\|c}
\\hline
& \\text{Params} & \\text{Loss} & \\text{Cache} & \\text{备注} \\\
\\hline
\\text{MLA} & 894M & 2.721 & 576 & \\\
\\text{MLA-256} & 989M & 2.705 & 576 & \\\
\\text{GQA2-(192+64)-S1} & 946M & 2.714 & 640 & \\\
\\text{GQA2-(192+64)-S2} & 943M & 2.708 & 512 & \\text{引入VO-RoPE} \\\
\\text{GQA4-(64+64)-S2} & 842M & 2.738 & 512 & \\\
\\text{GQA4-(128+64)-S2} & 899M & 2.713 & 768 & \\text{KV Cache最大} \\\
\\text{GQA1-(512+64)-S3} & 1171M & 2.677 & 576 & \\text{head\_dims最大} \\\
\\hline
\\end{array}$$

这里“GQA1-(512+64)-S3”是按照MLA的推理形式实现的MQA，形式介乎S1与S2之间，它的主要特点是head\_dims大。

结果解读：

> 1、KV-Shared的GQA自带Partial RoPE；
>
> 2、KV-Shared的GQA2-256，也能超过MLA；
>
> 3、VO-RoPE的引入，似乎有利于效果（S1 ≲ S2）；
>
> 4、同等KV Cache下，head\_dims越大越好；
>
> 5、GQA2-(192+64)-S2 略微超过 GQA1-256-PR；
>
> 6、GQA4-(128+64)-S2 的KV Cache最大，但效果不是最优，再次表明head\_dims更关键。

关于KV-Shared，还有两点观察：

> 1、训练过程中，GQA1-256-PR前期是明显领先GQA2-(192+64)-S2，但后期被追平甚至略微反先，猜测GQA1-256-PR可能有后劲不足的嫌疑；
>
> 2、如果没有KV-Shared，GQA顶多是GQA1-256，也就是说head\_dims顶天了256，但有KV-Shared的话，GQA可以做到GQA1-512-S，单纯从head\_dims看，KV-Shared天花板更高。

### Part VI [\#](https://kexue.fm/kexue.fm\#Part%20VI)

由于没有严格对齐参数量，可能读者会有“到底是增加参数量还是增加head\_dims更本质”的疑虑，所以这里补充几个对齐参数量的实验。

这里考虑的对齐参数量的方式有三种：

> 1、 **double-heads**：以“GQA2-128 vs GQA1-256”为例，将GQA2-128的num\_heads翻倍，可以让GQA2-128的参数量跟GQA1-256相同；
>
> 2、 **缩减MLP**：缩小MLP（SwiGLU）的intermediate\_size，也可以使得GQA1-256的参数量跟GQA2-128大致相同；
>
> 3、 **Q&O LoRA**：GQA的主要参数量来自Query和Output的投影矩阵，对这两个矩阵改用LoRA，也可以降低GQA1-256的参数量。

实验结果如下：
$$\\begin{array}{c\|ccc\|ccc}
\\hline
& \\text{Params} & \\text{Loss} & \\text{Cache} & \\text{num\_heads} & \\text{intermediate\_size} & \\text{qo\_lora} \\\
\\hline
\\text{MLA} & 894M & 2.721 & 576 & 16 & 5456 & \\text{No}\\\
\\hline
\\text{GQA2-128} & 842M & 2.75 & 512 & 16 & 5456 & \\text{No}\\\
\\text{GQA1-256} & 943M & 2.72 & 512 & 16 & 5456 & \\text{No}\\\
\\hline
\\text{GQA2-128} & 943M & 2.723 & 512 & \\color{red}{32} & 5456 & \\text{No} \\\
\\text{GQA1-256} & 843M & 2.747 & 512 & 16 & \\color{red}{4096} & \\text{No} \\\
\\text{GQA1-256} & 842M & 2.726 & 512 & 16 & 5456 & \\color{red}{\\text{Yes}} \\\
\\hline
\\text{GQA4-(64+64)-S2} & 842M & 2.738 & 512 & 16 & 5456 & \\text{No} \\\
\\text{GQA2-(192+64)-S2} & 943M & 2.708 & 512 & 16 & 5456 & \\text{No} \\\
\\hline
\\text{GQA4-(64+64)-S2} & 943M & 2.711 & 512 & \\color{red}{32} & 5456 & \\text{No} \\\
\\text{GQA2-(192+64)-S2} & 843M & 2.733 & 512 & 16 & \\color{red}{4096} & \\text{No} \\\
\\text{GQA2-(192+64)-S2} & 842M & 2.708 & 512 & 16 & 5456 & \\color{red}{\\text{Yes}} \\\
\\hline
\\end{array}$$

结果主要分三块：

> 1、heads翻倍相比head\_dims翻倍，loss稳定差0.003左右；
>
> 2、缩小MLP比head\_dims减半，loss稳定优0.004左右；
>
> 3、Q&O LoRA性能损失最小，可以实现head\_dims翻倍但参数量不增，且loss明显降。

结论：如果从增加参数量角度看，增大head\_dims可能是效果增益较大的方向，配合Q&O LoRA可以实现参数量几乎不增，但收益仍相当。

## 小结 [\#](https://kexue.fm/kexue.fm\#%E5%B0%8F%E7%BB%93)

初步结论是：

> 1、增大head\_dims收益最大；
>
> 2、Partial RoPE对Loss也有一定帮助；
>
> 3、KV-Shared应该也有一定作用。

这样看来，此前我们一直在head\_dims=128下找MLA的替代品，感觉是起点就先天不足了，难怪一直比不上MLA。要想追平MLA，head\_dims应该要192起步了，并辅以Partial RoPE。至于KV-Shared，也可能有用，但应该还需要更大规模的验证。

## 意义 [\#](https://kexue.fm/kexue.fm\#%E6%84%8F%E4%B9%89)

其实这里边的意义，就看我们换掉MLA的决心有多强。

假设 GQA2-(192+64)-S2 可以替代MLA，但MLA也可以升到256，目前看来 GQA2-(192+64)-S2 比不上 MLA-256 。那么换掉MLA的唯二好处是：

> 1、结构更简单，可以方便加QK-Norm；
>
> 2、解码阶段的head\_dims由512+64变成了256，同时num\_groups变为2，可以TP。

_**转载到请包括本文地址：** [https://kexue.fm/archives/10907](https://kexue.fm/archives/10907)_

_**更详细的转载事宜请参考：**_ [《科学空间FAQ》](https://kexue.fm/archives/6508#%E6%96%87%E7%AB%A0%E5%A6%82%E4%BD%95%E8%BD%AC%E8%BD%BD/%E5%BC%95%E7%94%A8)

**如果您还有什么疑惑或建议，欢迎在下方评论区继续讨论。**

**如果您觉得本文还不错，欢迎 [分享](https://kexue.fm/kexue.fm#share)/ [打赏](https://kexue.fm/kexue.fm#pay) 本文。打赏并非要从中获得收益，而是希望知道科学空间获得了多少读者的真心关注。当然，如果你无视它，也不会影响你的阅读。再次表示欢迎和感谢！**

打赏

微信打赏

支付宝打赏

因为网站后台对打赏并无记录，因此欢迎在打赏时候备注留言。你还可以 [**点击这里**](http://mail.qq.com/cgi-bin/qm_share?t=qm_mailme&email=tN7d1drY3drrx8H0xcWa19vZ) 或在下方评论区留言来告知你的建议或需求。

**如果您需要引用本文，请参考：**

苏剑林. (May. 04, 2025). 《Transformer升级之路：20、MLA好在哪里?（上） 》\[Blog post\]. Retrieved from [https://kexue.fm/archives/10907](https://kexue.fm/archives/10907)

@online{kexuefm-10907,
        title={Transformer升级之路：20、MLA好在哪里?（上）},
        author={苏剑林},
        year={2025},
        month={May},
        url={\\url{https://kexue.fm/archives/10907}},
}

分类： [信息时代](https://kexue.fm/category/Big-Data)    标签： [优化](https://kexue.fm/tag/%E4%BC%98%E5%8C%96/), [语言模型](https://kexue.fm/tag/%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/), [生成模型](https://kexue.fm/tag/%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B/), [attention](https://kexue.fm/tag/attention/)[51 评论](https://kexue.fm/archives/10907#comments)

< [一道概率不等式：盯着它到显然成立为止！](https://kexue.fm/archives/10902) \| [msign算子的Newton-Schulz迭代（上）](https://kexue.fm/archives/10922) >

### 你也许还对下面的内容感兴趣

- [为什么DeltaNet要加L2 Normalize？](https://kexue.fm/archives/11486)
- [生成扩散模型漫谈（三十一）：预测数据而非噪声](https://kexue.fm/archives/11428)
- [Muon优化器指南：快速上手与关键细节](https://kexue.fm/archives/11416)
- [低精度Attention可能存在有偏的舍入误差](https://kexue.fm/archives/11371)
- [MuP之上：1. 好模型的三个特征](https://kexue.fm/archives/11340)
- [DiVeQ：一种非常简洁的VQ训练方案](https://kexue.fm/archives/11328)
- [为什么线性注意力要加Short Conv？](https://kexue.fm/archives/11320)
- [QK-Clip：让Muon在Scaleup之路上更进一步](https://kexue.fm/archives/11126)
- [Transformer升级之路：21、MLA好在哪里?（下）](https://kexue.fm/archives/11111)
- [“对角+低秩”三角阵的高效求逆方法](https://kexue.fm/archives/11072)

[发表你的看法](https://kexue.fm/kexue.fm#comment_form)

1. [«](https://kexue.fm/archives/10907/comment-page-1#comments)
2. [1](https://kexue.fm/archives/10907/comment-page-1#comments)
3. [2](https://kexue.fm/archives/10907/comment-page-2#comments)

jorjiang

June 3rd, 2025

剑林你好，我这里有个简单的MLA的解读，https://zhuanlan.zhihu.com/p/1911795330434986569。
不考虑positional encoding的情况下：
从MHA到MLA就两步：
1.放弃缓存较大的KV，而缓存更小的输入Embedding X，代价是每次decoding step都重复地从X计算KV
2.简单粗暴地给X降维，从而使得缓存的X和重复计算KV的计算量都大幅减小。

其实和正常的MHA相比，MLA压缩了的东西就是输入X，而且这个压缩比较激进，直接从7168压缩到了512 （对q来说是压到1536），这么大的压缩还有效，不知道如何解读。

如果embedding只用512就够了的话。从512纬的空间里，取出128个不一样的128纬投影，这128个头感觉应该有很多重叠，是不是其实根本用不上这么多头？或者是不是可以在此基础上对heads做稀疏话 类似mixture of attention heads之类的

[回复评论](https://kexue.fm/archives/10907/comment-page-2?replyTo=27757#respond-post-10907)

[苏剑林](https://kexue.fm) 发表于
June 3rd, 2025

很遗憾，你这两点“解读”跟MLA真正的精妙之处完全没关系，甚至基本上是错的。

MLA是利用了NoPE的MHA和MQA可以相互变换的特性，在训练和prefill阶段表现为head\_dims=192的MHA，在decoding阶段表现为head\_dims=576的MQA，利用了两个阶段分别是compute-bound和memory-bound的特性，使得两个阶段的效率都实现最大化，并不存在什么“牺牲时间换空间”或者“牺牲空间换时间”的问题。

特别是按照本文的观点，head\_dims很重要，所以MLA的精妙之处在于，以MHA-192的训练成本和prefill成本，实现了近乎MQA-576的效果和decoding成本。

总的来说，MLA的重点是NoPE下MHA和MQA的恒等变换性质，以及在不同阶段针对性地使用不同的形式，不存在谁牺牲谁。在理解MLA前，可以先多了解一下主流的GQA和MQA，了解一下Attention机制的瓶颈在哪。

[回复评论](https://kexue.fm/archives/10907/comment-page-2?replyTo=27761#respond-post-10907)

jorjiang 发表于
June 4th, 2025

看问题的不同角度吧，可能我的这个视角有点绕 不是很直观，你把它看成MQA，而我把它还是看作MHA。
看这个简单公式的话
$\\mathrm {softmax}(\\underbrace{x^{\\top}W\_q}\_{q\\in\\mathbb R^{1\\times d\_h}}\\underbrace{W\_k^{\\top}X}\_{k^{\\top}\\in\\mathbb R^{ d\_h \\times N}}) \\underbrace{X^{\\top}W\_v}\_{v \\in \\mathbb R^{N\\times d\_h}}W\_o = \\mathrm {softmax}(\\underbrace{x^{\\top}W\_qW\_k^{\\top}}\_{q'\\in\\mathbb R^{1\\times d}}\\underbrace{X}\_{k'^{\\top}\\in\\mathbb R^{ d \\times N}}) \\underbrace{X^{\\top}}\_{v'\\in \\mathbb R^{N\\times d}}W\_vW\_o$
应该很明显可以看出，MLA干的就是“不缓存计算完成的KV，而只保存（从7168降维为576）的x（这个x就是论文中的latent，也就是你说的MQA里的那一个头，但我偏向于把它依然看做没算出qkv的x），然后每次重复计算kv”是一回事。只是我说的这个重复计算kv的计算是转嫁到吸收掉的wk和wv的wq和wo里了，因为很明显你可以看到MLA在decoding时新的q和o的计算成本是比普通128纬的MQA大的，而且大出来的就是每次重新计算kv的部分。

你把他看成MQA的话和MHA区别太大，很多东西没有可比性。我这个角度几乎把MLA和原始MHA等价的部分保留下来了。最后很容易看出从计算上MLA和128个头，每个头128纬的MHA唯一的区别就是计算qkv前把x降维了

[回复评论](https://kexue.fm/archives/10907/comment-page-2?replyTo=27765#respond-post-10907)

[苏剑林](https://kexue.fm) 发表于
June 4th, 2025

不是你我将它看成什么，而是它本身就是MQA，你换个名字它也是MQA，而且它是因为能变成MQA才变得更有意义起来，实际decoding也确确实实是按照MQA来decoding。latent压缩很多人都能想到，这不是什么创新，MLA的精华是在NoPE下，这种压缩能够转化成一个MQA，从而可以只缓存latent同时还不影响decoding速度。

如果你只强调latent，没有指出NoPE下对MQA的可转换性，那就不是MLA，而是单纯的hidden\_state压缩。压缩hidden\_state，只缓存压缩后的结果，然后推理时慢慢实时去恢复KV，就是你说的“时间换空间”，这太容易想了，很多事后压缩的方案都是这样的，但MLA的精妙在于人家根本不需要拿时间来换，因为decoding的瓶颈并不是计算。

总之，你在训练阶段和prefill阶段，将它理解为MHA，这没有问题，它本来就是；但是你在解释为什么可以只缓存那个512的latent（实际上还要加上64的RoPE）时，将它理解为“时间换空间”的MHA，那就是根本错误，不是“不同角度”可以接受的。

[回复评论](https://kexue.fm/archives/10907/comment-page-2?replyTo=27767#respond-post-10907)

any 发表于
June 4th, 2025

我个人来看这个解读角度只是缺失遗漏，不算完全错吧。如果单单只看MLA的结构图的话，它的表现确实完全是“将存储kv cache改为存储降维后的Embedding X”。但问题恰恰在于结构图没有展示出MLA最核心的部分：苏神提到的“矩阵吸收”，即“训练和prefill这个compute-bound阶段不做矩阵吸收，decoding这个memory-bound阶段做矩阵吸收”，从而达到效率最大化（具体为何可以省可以看看这里：https://developnotes.readthedocs.io/zh-cn/latest/deepseek.html#absorb）。正是因为“矩阵吸收”只能在nope下做，所以MLA才有了nope和rope部分，如果单单“Cache Compressed Embedding X”的话，是不需要nope/rope操作的。所以MLA是使用这两个相对独立的方式来分别完成“省kv cache”和“优化decoding阶段的访存效率”问题。你在你专栏补充后一点即可

[回复评论](https://kexue.fm/archives/10907/comment-page-2?replyTo=27791#respond-post-10907)

[苏剑林](https://kexue.fm) 发表于
June 4th, 2025

“将存储kv cache改为存储降维后的Embedding X”这句话没错，但是按照我的理解，这句话还不算MLA，后面的不牺牲效率地、按需地进行MQA/MHA转化，才算是MLA，而作者则专门突出它牺牲XXX换XXX，所以我觉得并不正确。

[回复评论](https://kexue.fm/archives/10907/comment-page-2?replyTo=27795#respond-post-10907)

jorjiang 发表于
June 5th, 2025

训练和prefill这个compute-bound阶段不做矩阵吸收，这个用我这个解释更好理解了。正是因为decoding需要kv cache，而且是memory bound，计算不是瓶颈，所以可以引入更大的计算量换取内存空间是划算的，因为矩阵吸收干的就是“牺牲一些计算量换取内存空间”这件事。但是prefill和training不存在kv cache，且是计算bound，所以根本没必要做矩阵吸收，老老实实做原版的MHA就好了反而更快

[回复评论](https://kexue.fm/archives/10907/comment-page-2?replyTo=27800#respond-post-10907)

jorjiang

June 4th, 2025

时间换空间的表述，算是我有缺陷。精确的说是“计算量“换空间。因为你不可否认MLA就是每个decoding step比直接缓存算好的kv计算量变大了，因为q和o多了一个矩阵，这个矩阵就是MHA里的Wk和Wq的，也是128个头的。只不过在英伟达卡上做decoding，这个增加的算力体现出来的“时间”代价不明显。我们在某些国产卡上测试，这些卡小算力大带宽，这个时间换空间就很明显，在上下文长度短，cache不是瓶颈的情况下，还不如直接转成MHA做decoding快一点，所以我才会有这些思考。

我之所以想按照MHA而不是MQA理解，是想找出MLA到底和传统的MHA在数学上归根到底有什么区别，按MQA理解是不好比较的，因为这个MQA不是一个正常的MQA，比如他的K、V是同一个东西，q和o的计算复杂度也远大于K、V，感觉和原始的MHA就是两个东西，你要试图理解他在简单的MHA上到底有什么区别的话，无法直接比较，变量太多。比如说““实现了近乎MQA-576的效果和decoding成本”很好，因为“head\_dims很重要”，这个我就不敢苟同，因为拿MQA和MHA比单头大小没什么意义。但是 我把它看作MHA就会发现他做的实际就是head\_dim=128, num\_heads=128的MHA，所以在head\_dim上并没有你说的那个优势。而且可以明显看出MLA和传统的MHA只有一个差距，就是x被降维了。这个就是MLA实际干的事，数学上和MQA的理解是一摸一样的，好想也好不好想也好，这就是MLA实际干的事。

[回复评论](https://kexue.fm/archives/10907/comment-page-2?replyTo=27771#respond-post-10907)

[苏剑林](https://kexue.fm) 发表于
June 4th, 2025

我前面就说了，它在训练阶段和prefill阶段都是MHA，你理解为MHA，在数学上一点问题的都没，但是你在decoding阶段将其定性为XXX“换”空间，那就不妥，因为它没有牺牲什么东西来换。你说它计算量增加，可以，但计算量不是瓶颈，所以没有增加推理时间，反而因为减少了kv cache而减少了推理时间，这点指出来就不会有人诟病。

“在某些国产卡上测试，这些卡小算力大带宽，这个时间换空间就很明显”，这个有可能，但是现在算法的编程趋势本质上在过拟合某类甚至某个硬件，MLA本身就是为A卡甚至H卡定制的产品，用到其他卡效率差是使用者的问题，并不是MLA设计理念上要通过XXX来“换”XXX。

我的观点很简单，你脑子里是MHA还是MQA，甚至你代码里是MHA还是MQA，没人管，也没什么错误，但做评价或者断言时，应该尊重设计者本身的设计事实。

[回复评论](https://kexue.fm/archives/10907/comment-page-2?replyTo=27779#respond-post-10907)

[苏剑林](https://kexue.fm) 发表于
June 4th, 2025

我建议的一种比较符合事实的理解顺序是（下面说的都是NoPE）：

1、我们“需要”将kv cache压缩到512，是高效、便宜地decoding的需要，而不是因为512就够用或者够好了；

2、限定kv cache=512的前提下，什么模型效果最好呢？答案是kv共享的MQA-512，由于恒等变换的存在，你会发现不管是普通的MQA、GQA还是MHA，都可以转换为一个head\_dims=kv cache的、kv共享的MQA，所以一个head\_dims=kv cache的、kv共享的MQA效果是最好的；

3、问题是，MQA-512在decoding时能接受，训练和prefill都不能接受，所以对QKV引入LoRA，利用恒等变换，在训练和prefill阶段转换为MHA-128（算上RoPE是MHA-192），decoding时又能恢复MQA-512；

4、所以你专栏里“为什么可以给X降维”这个问题的提法，其实不大适合，只是因为decoding效率需要我们给它降维而不是“可以”降维，更适合的问题是“为什么降了维之后效果还能媲美完全体的MHA-128”，本文提出的观点是，因为MLA无形之中将Q、K的head\_dims升到了192，这对效果有本质作用，导致它实现的可能是MQA-256甚至是MQA-512的效果。

[回复评论](https://kexue.fm/archives/10907/comment-page-2?replyTo=27780#respond-post-10907)

jorjiang 发表于
June 4th, 2025

是我有点妄自猜测作者意图了。
“更适合的问题是‘为什么降了维之后效果还能媲美完全体的MHA-128’” ，这确实是我最后的疑问。有空也像你一样做个测试看看

[回复评论](https://kexue.fm/archives/10907/comment-page-2?replyTo=27788#respond-post-10907)

amy

June 5th, 2025

苏老师，您有关注傅里叶旋转位置编码这篇工作吗，想知道您对这篇工作的看法是什么，这篇工作可以work的核心是在于增加了更多的频率，还是像文章中分析的那样，由于attention模块内外对于频谱的损伤，所以必须采取多种频率线性叠加的方式来适应这种损伤。

[回复评论](https://kexue.fm/archives/10907/comment-page-2?replyTo=27799#respond-post-10907)

[苏剑林](https://kexue.fm) 发表于
June 8th, 2025

这两天看了下FoPE，感觉它的分析有点道理，但它实现的代码跟论文其实是不一样的。

看论文的描述，FoPE想要实现的是
$$ q\_n k\_m^\* \[e^{i\\omega\_a(n-m)} + e^{i\\omega\_a(n-m)}\]$$
这种混合频率的相对位置编码，但实际上，它的代码是
$$ \\tilde{q}\_n=q\_n \[e^{i\\omega\_a n} + e^{i\\omega\_a n}\],\\quad \\tilde{k}\_m=k\_m \[e^{i\\omega\_a m} + e^{i\\omega\_a m}\]$$
注意
$$ \\tilde{q}\_n \\tilde{k}\_m^\* \\neq q\_n k\_m^\* \[e^{i\\omega\_a(n-m)} + e^{i\\omega\_a(n-m)}\]$$
甚至$\\tilde{q}\_n \\tilde{k}\_m^\*$都并不是只依赖于$n-m$！换言之FoPE根本不是相对位置编码，而是更接近真正的绝对位置编码。

我不大确定作者是对旋转变换的理解有误还是刻意为之，总之它代码实现的并不是论文所写的。当然，如果它代码跑出来确实不错，那么表明不是相对位置编码也没啥问题，但这个问题应当澄清楚。

如果要严格实现$ q\_n k\_m^\* \[e^{i\\omega\_a(n-m)} + e^{i\\omega\_a(n-m)}\] $ 这种attention，不改flash attention代码实际上是很难做到的。但如果改flash attention就为了实现这个，又好像有点小题大作。

另外我觉得即便实现了这种形式，它的上限还是不如最近出来的PaTH Attention（ https://arxiv.org/abs/2505.16381 ）的，所以如果要关心这个方向的变化，还不如直接看PaTH Attention。

[回复评论](https://kexue.fm/archives/10907/comment-page-2?replyTo=27826#respond-post-10907)

陈少龙

June 8th, 2025

苏神，您好！看了您的分享受益匪浅。但是我有个担心和疑问，您这里的每个实验表里不同对比组的loss其实绝大多数差别都不大，他们之间的差别真的是统计上显著的吗？如果每组实验都是多次实验之后取平均而得来的，那每组实验的标准差是多少。就比如说您Part VI中的那个表，11个loss的均值是2.726，标准差是0.014，那如果每组实验的标准差也是0.014这个量级，那这些不同组的实验是否真的统计上显著不同呢？

[回复评论](https://kexue.fm/archives/10907/comment-page-2?replyTo=27813#respond-post-10907)

[苏剑林](https://kexue.fm) 发表于
June 8th, 2025

对于每一步数据都严格对齐来说，0.01的loss差距不小了，因为它代表了每一个step的loss都好0.01以上～而且这不是发paper，对于一个改动是否有效，我们会有更多更微妙的感知细节，当然也会做更多的另外验证。

[回复评论](https://kexue.fm/archives/10907/comment-page-2?replyTo=27832#respond-post-10907)

1. [«](https://kexue.fm/archives/10907/comment-page-1#comments)
2. [1](https://kexue.fm/archives/10907/comment-page-1#comments)
3. [2](https://kexue.fm/archives/10907/comment-page-2#comments)

[取消回复](https://kexue.fm/archives/10907#respond-post-10907)

你的大名

电子邮箱

个人网站（选填）

1\. 可以使用LaTeX代码，点击“预览效果”可查看效果；2. 可以通过点击评论楼层编号来引用该楼层；3. 网站可能会有点卡，如非确认评论失败，请 **不要重复点击提交**。

### 内容速览

[观察](https://kexue.fm/kexue.fm#%E8%A7%82%E5%AF%9F)
[猜测](https://kexue.fm/kexue.fm#%E7%8C%9C%E6%B5%8B)
[实验](https://kexue.fm/kexue.fm#%E5%AE%9E%E9%AA%8C)
[设置](https://kexue.fm/kexue.fm#%E8%AE%BE%E7%BD%AE)
[Part I](https://kexue.fm/kexue.fm#Part%20I)
[Part II](https://kexue.fm/kexue.fm#Part%20II)
[Part III](https://kexue.fm/kexue.fm#Part%20III)
[Part IV](https://kexue.fm/kexue.fm#Part%20IV)
[Part V](https://kexue.fm/kexue.fm#Part%20V)
[Part VI](https://kexue.fm/kexue.fm#Part%20VI)
[小结](https://kexue.fm/kexue.fm#%E5%B0%8F%E7%BB%93)
[意义](https://kexue.fm/kexue.fm#%E6%84%8F%E4%B9%89)

### 智能搜索

支持整句搜索！网站自动使用 [结巴分词](https://github.com/fxsjy/jieba) 进行分词，并结合ngrams排序算法给出合理的搜索结果。

### 热门标签

[生成模型](https://kexue.fm/tag/%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B/) [attention](https://kexue.fm/tag/attention/) [优化](https://kexue.fm/tag/%E4%BC%98%E5%8C%96/) [语言模型](https://kexue.fm/tag/%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/) [模型](https://kexue.fm/tag/%E6%A8%A1%E5%9E%8B/) [网站](https://kexue.fm/tag/%E7%BD%91%E7%AB%99/) [梯度](https://kexue.fm/tag/%E6%A2%AF%E5%BA%A6/) [概率](https://kexue.fm/tag/%E6%A6%82%E7%8E%87/) [矩阵](https://kexue.fm/tag/%E7%9F%A9%E9%98%B5/) [优化器](https://kexue.fm/tag/%E4%BC%98%E5%8C%96%E5%99%A8/) [转载](https://kexue.fm/tag/%E8%BD%AC%E8%BD%BD/) [微分方程](https://kexue.fm/tag/%E5%BE%AE%E5%88%86%E6%96%B9%E7%A8%8B/) [分析](https://kexue.fm/tag/%E5%88%86%E6%9E%90/) [天象](https://kexue.fm/tag/%E5%A4%A9%E8%B1%A1/) [深度学习](https://kexue.fm/tag/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/) [积分](https://kexue.fm/tag/%E7%A7%AF%E5%88%86/) [python](https://kexue.fm/tag/python/) [扩散](https://kexue.fm/tag/%E6%89%A9%E6%95%A3/) [力学](https://kexue.fm/tag/%E5%8A%9B%E5%AD%A6/) [无监督](https://kexue.fm/tag/%E6%97%A0%E7%9B%91%E7%9D%A3/) [几何](https://kexue.fm/tag/%E5%87%A0%E4%BD%95/) [节日](https://kexue.fm/tag/%E8%8A%82%E6%97%A5/) [生活](https://kexue.fm/tag/%E7%94%9F%E6%B4%BB/) [文本生成](https://kexue.fm/tag/%E6%96%87%E6%9C%AC%E7%94%9F%E6%88%90/) [数论](https://kexue.fm/tag/%E6%95%B0%E8%AE%BA/)

### 随机文章

- [2012春节快乐！](https://kexue.fm/archives/1542)
- [特殊的通项公式：二次非线性递推](https://kexue.fm/archives/3064)
- [开局一段扯，数据全靠编？真被一篇“神论文”气到了](https://kexue.fm/archives/8783)
- [《量子力学与路径积分》习题解答V0.5](https://kexue.fm/archives/3692)
- [正弦级数和余弦级数](https://kexue.fm/archives/3101)
- [【理解黎曼几何】2\. 从勾股定理到黎曼度量](https://kexue.fm/archives/3969)
- [矩阵符号函数mcsgn能计算什么？](https://kexue.fm/archives/11056)
- [生成扩散模型漫谈（十一）：统一扩散模型（应用篇）](https://kexue.fm/archives/9271)
- [最小熵原理（四）：“物以类聚”之从图书馆到词向量](https://kexue.fm/archives/6191)
- [中国第一个诺贝尔奖得主](https://kexue.fm/archives/1740)

### 最近评论

- [JamesSand](https://kexue.fm/archives/11416/comment-page-1#comment-29064): 谢谢苏神我发现问题在 muon 的 adjust lr 上边如果我用 spectral\_nor...
- [JamesSand](https://kexue.fm/archives/11416/comment-page-1#comment-29063): 苏神您好，想问下用 muon 在 adamw pretrian 的 checkpoints (...
- [printemps](https://kexue.fm/archives/11320/comment-page-1#comment-29062): 苏神怎么看log-linear attention这种变长的hidden state?
- [chris stars](https://kexue.fm/archives/11428/comment-page-1#comment-29060): 我看这篇论文之前觉得会很厉害，但是看完之后有一种还好的感觉，好像之前看过的生成模型论文里面提到...
- [sk](https://kexue.fm/content.html/comment-page-1#comment-29059): 太棒了
- [Jason小白](https://kexue.fm/archives/11416/comment-page-1#comment-29058): 苏神您好，从知乎了解到这个space，下面我也贴了我使用Muon的体验，有些困惑，如您方便可以...
- [且寻](https://kexue.fm/archives/11480/comment-page-1#comment-29057): same。本来想转成二重积分的，但想半天整不出来，搞出来这个半成品...
- [苏剑林](https://kexue.fm/archives/11267/comment-page-1#comment-29056): 是梯度均值为零的假设。这个问题不是在“数值模拟”一节讨论过了吗？而且最终结果跟实际观测是接近的...
- [苏剑林](https://kexue.fm/archives/7180/comment-page-2#comment-29055): X与单位阵的平均平方误差(mse)，作为它跟单位阵的差距，有什么问题？当然取误差最大值也是一个...
- [苏剑林](https://kexue.fm/archives/9209/comment-page-8#comment-29054): 从公式$(12)$到公式$(15)$，都在推导和解释你说的这个事啊，以$\\mathbb{E}\_...

### 友情链接

- [Cool Papers](https://papers.cool)
- [数学研发](https://bbs.emath.ac.cn)
- [Seatop](http://www.seatop.com.cn/)
- [Xiaoxia](https://xiaoxia.org/)
- [积分表-网络版](https://kexue.fm/sci/integral/index.html)
- [丝路博傲](http://blog.dvxj.com/)
- [数学之家](http://www.2math.cn/)
- [有趣天文奇观](http://interesting-sky.china-vo.org/)
- [TwistedW](http://www.twistedwg.com/)
- [godweiyang](https://godweiyang.com/)
- [AI柠檬](https://blog.ailemon.net/)
- [王登科-DK博客](https://greatdk.com)
- [ESON](https://blog.eson.org/)
- [枫之羽](https://fzhiy.net/)
- [coding-zuo](https://coding-zuo.github.io/)
- [博科园](https://www.bokeyuan.net/)
- [孔皮皮的博客](https://www.kppkkp.top/)
- [运鹏的博客](https://yunpengtai.top/)
- [jiming.site](https://jiming.site/)
- [OmegaXYZ](https://www.omegaxyz.com/)
- [EAI猩球](https://www.robotech.ink/)
- [文举的博客](https://liwenju0.com/)
- [申请链接](https://kexue.fm/links.html)

本站采用创作共用版权协议，要求署名、非商业用途和保持一致。转载本站内容必须也遵循“ [署名-非商业用途-保持一致](http://creativecommons.org/licenses/by-nc-nd/2.5/cn/)”的创作共用协议。
© 2009-2026 Scientific Spaces. All rights reserved. Theme by [laogui](http://www.laogui.com). Powered by [Typecho](http://typecho.org). 备案号: [粤ICP备09093259号-1/2](https://beian.miit.gov.cn/)。