## SEARCH

## MENU

- [打赏](https://kexue.fm/reward.html)
- [公式](https://kexue.fm/latex.html)
- [天象](https://kexue.fm/ac.html)
- [链接](https://kexue.fm/links.html)
- [时光](https://kexue.fm/me.html)
- [博览](https://kexue.fm/science.html)
- [归档](https://kexue.fm/content.html)

## CATEGORIES

- [千奇百怪](https://kexue.fm/category/Everything)
- [天文探索](https://kexue.fm/category/Astronomy)
- [数学研究](https://kexue.fm/category/Mathematics)
- [物理化学](https://kexue.fm/category/Phy-chem)
- [信息时代](https://kexue.fm/category/Big-Data)
- [生物自然](https://kexue.fm/category/Biology)
- [图片摄影](https://kexue.fm/category/Photograph)
- [问题百科](https://kexue.fm/category/Questions)
- [生活/情感](https://kexue.fm/category/Life-Feeling)
- [资源共享](https://kexue.fm/category/Resources)

## NEWPOSTS

- [生成扩散模型漫谈（三十一）：预测数...](https://kexue.fm/archives/11428)
- [Muon优化器指南：快速上手与关键细节](https://kexue.fm/archives/11416)
- [AdamW的Weight RMS的...](https://kexue.fm/archives/11404)
- [n个正态随机数的最大值的渐近估计](https://kexue.fm/archives/11390)
- [流形上的最速下降：5\. 对偶梯度下降](https://kexue.fm/archives/11388)
- [低精度Attention可能存在有...](https://kexue.fm/archives/11371)
- [MuP之上：1. 好模型的三个特征](https://kexue.fm/archives/11340)
- [随机矩阵的谱范数的快速估计](https://kexue.fm/archives/11335)
- [DiVeQ：一种非常简洁的VQ训练方案](https://kexue.fm/archives/11328)
- [为什么线性注意力要加Short C...](https://kexue.fm/archives/11320)

## COMMENTS

- [qsh: 用muon的时候weights initialization有...](https://kexue.fm/archives/11416/comment-page-1#comment-28933)
- [夺宇: 苏老师的解释和推导好自然啊，比原论文更容易看懂](https://kexue.fm/archives/9152/comment-page-3#comment-28932)
- [夺宇: 苏老师，(17)式中的噪声项可以直接在(8)式中直接添加吗？(...](https://kexue.fm/archives/9119/comment-page-13#comment-28931)
- [Xiaozhi Zhu: 我觉得这个work摆脱了two stages，真正做到E2E，...](https://kexue.fm/archives/11428/comment-page-1#comment-28930)
- [wednesday: 想问问苏老师的数据挖掘学习思路或者学习路径是怎样的](https://kexue.fm/archives/3319/comment-page-1#comment-28929)
- [wednesday: 因为我们只对p(Y\|X)建模，因此$p\_{\\theta}(X)...](https://kexue.fm/archives/5239/comment-page-3#comment-28928)
- [ykwen: 不动点迭代的时候 有没有可能迭代到0附近呢？](https://kexue.fm/archives/10592/comment-page-3#comment-28927)
- [wednesday: 这是针对评论区一位同学问题的提问，现在已经懂了](https://kexue.fm/archives/5253/comment-page-18#comment-28926)
- [wednesday: 针对一个样本和针对一个batch有什么特别的区别吗](https://kexue.fm/archives/5253/comment-page-18#comment-28925)
- [Zhancun: 也可以从natural image space的角度去理解。对...](https://kexue.fm/archives/11428/comment-page-1#comment-28924)

## USERLOGIN

- [登录](https://kexue.fm/admin/login.php)

[科学空间\|Scientific Spaces](https://kexue.fm)

- [登录](https://kexue.fm/admin/login.php)
- [打赏](https://kexue.fm/reward.html)
- [公式](https://kexue.fm/latex.html)
- [天象](https://kexue.fm/ac.html)
- [链接](https://kexue.fm/links.html)
- [时光](https://kexue.fm/me.html)
- [博览](https://kexue.fm/science.html)
- [归档](https://kexue.fm/content.html)

渴望成为一个小飞侠

- [欢迎订阅](https://kexue.fm/feed)
- [个性邮箱](https://kexue.fm/archives/119)
- [天象信息](https://kexue.fm/ac.html)
- [观测ISS](https://kexue.fm/archives/41)
- [LaTeX](https://kexue.fm/latex.html)
- [关于博主](https://kexue.fm/me.html)

欢迎访问“科学空间”，这里将与您共同探讨自然科学，回味人生百态；也期待大家的分享～

- [**千奇百怪** Everything](https://kexue.fm/category/Everything)
- [**天文探索** Astronomy](https://kexue.fm/category/Astronomy)
- [**数学研究** Mathematics](https://kexue.fm/category/Mathematics)
- [**物理化学** Phy-chem](https://kexue.fm/category/Phy-chem)
- [**信息时代** Big-Data](https://kexue.fm/category/Big-Data)
- [**生物自然** Biology](https://kexue.fm/category/Biology)
- [**图片摄影** Photograph](https://kexue.fm/category/Photograph)
- [**问题百科** Questions](https://kexue.fm/category/Questions)
- [**生活/情感** Life-Feeling](https://kexue.fm/category/Life-Feeling)
- [**资源共享** Resources](https://kexue.fm/category/Resources)

- [**千奇百怪**](https://kexue.fm/category/Everything)
- [**天文探索**](https://kexue.fm/category/Astronomy)
- [**数学研究**](https://kexue.fm/category/Mathematics)
- [**物理化学**](https://kexue.fm/category/Phy-chem)
- [**信息时代**](https://kexue.fm/category/Big-Data)
- [**生物自然**](https://kexue.fm/category/Biology)
- [**图片摄影**](https://kexue.fm/category/Photograph)
- [**问题百科**](https://kexue.fm/category/Questions)
- [**生活/情感**](https://kexue.fm/category/Life-Feeling)
- [**资源共享**](https://kexue.fm/category/Resources)

[首页](https://kexue.fm) [数学研究](https://kexue.fm/category/Mathematics) 重温SSM（二）：HiPPO的一些遗留问题

5Jun

# [重温SSM（二）：HiPPO的一些遗留问题](https://kexue.fm/archives/10137)

By 苏剑林 \|
2024-06-05 \|
35356位读者\|

书接上文，在上一篇文章 [《重温SSM（一）：线性系统和HiPPO矩阵》](https://kexue.fm/archives/10114) 中，我们详细讨论了HiPPO逼近框架其HiPPO矩阵的推导，其原理是通过正交函数基来动态地逼近一个实时更新的函数，其投影系数的动力学正好是一个线性系统，而如果以正交多项式为基，那么线性系统的核心矩阵我们可以解析地求解出来，该矩阵就称为HiPPO矩阵。

当然，上一篇文章侧重于HiPPO矩阵的推导，并没有对它的性质做进一步分析，此外诸如“如何离散化以应用于实际数据”、“除了多项式基外其他基是否也可以解析求解”等问题也没有详细讨论到。接下来我们将补充探讨相关问题。

## 离散格式 [\#](https://kexue.fm/kexue.fm\#%E7%A6%BB%E6%95%A3%E6%A0%BC%E5%BC%8F)

假设读者已经阅读并理解上一篇文章的内容，那么这里我们就不再进行过多的铺垫。在上一篇文章中，我们推导出了两类线性ODE系统，分别是：
\\begin{align}
&\\text{HiPPO-LegT:}\\quad x'(t) = Ax(t) + Bu(t) \\label{eq:legt-ode}\\\\[5pt\]
&\\text{HiPPO-LegS:}\\quad x'(t) = \\frac{A}{t}x(t) + \\frac{B}{t}u(t) \\label{eq:legs-ode}\\end{align}
其中$A,B$是与时间$t$无关的常数矩阵，HiPPO矩阵主要指矩阵$A$。在这一节中，我们讨论这两个ODE的离散化。

### 输入转换 [\#](https://kexue.fm/kexue.fm\#%E8%BE%93%E5%85%A5%E8%BD%AC%E6%8D%A2)

在实际场景中，输入的数据点是离散的序列$u\_0,u\_1,u\_2,\\cdots,u\_k,\\cdots$，比如流式输入的音频信号、文本向量等，我们希望用如上的ODE系统来实时记忆这些离散点。为此，我们先定义
\\begin{equation}u(t) = u\_k,\\quad \\text{如果} t\\in\[k\\epsilon, (k + 1)\\epsilon)\\end{equation}
其中$\\epsilon$就是离散化的步长。该定义也就是说在区间$\[k\\epsilon, (k + 1)\\epsilon)$内，$u(t)$是一个常数函数，其值等于$u\_k$。很明显这样定义出来的$u(t)$无损原本$u\_k$序列的信息，因此记忆$u(t)$就相当于记忆$u\_k$序列。

从$u\_k$变换到$u(t)$，可以使得输入信号重新变回连续区间上的函数，方便后面进行积分等运算，此外在离散化的区间内保持为常数，也能够简化离散化后的格式。

### LegT版本 [\#](https://kexue.fm/kexue.fm\#LegT%E7%89%88%E6%9C%AC)

我们先以LegT型ODE$\\eqref{eq:legt-ode}$为例，将它两端积分
\\begin{equation}x(t+\\epsilon) - x(t) = A\\int\_t^{t+\\epsilon} x(s)ds + B\\int\_t^{t+\\epsilon}u(s)ds\\end{equation}
其中$t=k\\epsilon$。根据$u(t)$的定义，它在$\[t, t + \\epsilon)$区间内恒为$u\_k$，于是$u(s)$的积分可以直接算出来：
\\begin{equation}x(t+\\epsilon) - x(t) = A\\int\_t^{t+\\epsilon} x(s)ds + \\epsilon B u\_k\\end{equation}
接下来的结果，就取决于我们如何近似$x(s)$的积分了。假如我们认为在$\[t, t + \\epsilon)$区间内$x(s)$近似恒等于$x(t)$，那么就得到前向欧拉格式
\\begin{equation}x(t+\\epsilon) - x(t) = \\epsilon A x(t) + \\epsilon B u\_k \\quad\\Rightarrow\\quad x(t+\\epsilon) = (I + \\epsilon A)x(t) + \\epsilon B u\_k\\end{equation}
我们认为在$\[t, t + \\epsilon)$区间内$x(s)$近似恒等于$x(t+\\epsilon)$，那么就得到后向欧拉格式
\\begin{equation}x(t+\\epsilon) - x(t) = \\epsilon A x(t+\\epsilon) + \\epsilon B u\_k \\quad\\Rightarrow\\quad x(t+\\epsilon) = (I - \\epsilon A)^{-1}(x(t) + \\epsilon B u\_k)\\end{equation}
前后向欧拉都具有相同的理论精度，但后向通常会有更好的数值稳定性。如果要更准确一些，那么认为在$\[t, t + \\epsilon)$区间内$x(s)$近似恒等于$\\frac{1}{2}\[x(t) + x(t+\\epsilon)\]$，那么得到双线性形式：
\\begin{equation}\\begin{gathered}
x(t+\\epsilon) - x(t) = \\frac{1}{2}\\epsilon A \[x(t) + x(t+\\epsilon)\] + \\epsilon B u\_k \\\
\\Downarrow \\\
x(t+\\epsilon) = (I - \\epsilon A/2)^{-1}\[(I + \\epsilon A/2) x(t) + \\epsilon B u\_k\]
\\end{gathered}\\end{equation}
这也等价于先用前向欧拉走半步，再用后向欧拉走半步。更一般地，我们还可以认为在$\[t, t + \\epsilon)$区间内$x(s)$近似恒等于$\\alpha x(t) + (1 - \\alpha) x(t+\\epsilon)$，其中$\\alpha\\in\[0,1\]$，这就不进一步展开了。事实上，我们也可以完全不做近似，因为结合式$\\eqref{eq:legt-ode}$以及在区间$\[t,t+\\epsilon)$中$u(s)$是常数$u\_k$，我们完全可以用“ [常数变易法](https://en.wikipedia.org/wiki/Variation_ou_parameters)”来精确求解出来，结果是
\\begin{equation}x(t+\\epsilon) = e^{\\epsilon A} x(t) + A^{-1} (e^{\\epsilon A} - I) B u\_k\\label{eq:legt-ode-sol}\\end{equation}
这里的矩阵指数按照级数来定义，可以参考 [《恒等式 det(exp(A)) = exp(Tr(A)) 赏析》](https://kexue.fm/archives/6377#%E7%9F%A9%E9%98%B5%E6%8C%87%E6%95%B0)。

### LegS版本 [\#](https://kexue.fm/kexue.fm\#LegS%E7%89%88%E6%9C%AC)

现在轮到LegS型ODE了，它的思路跟LegT型基本一致，结果也大同小异。首先将式$\\eqref{eq:legs-ode}$两端积分得到
\\begin{equation}x(t+\\epsilon) - x(t) = A\\int\_t^{t+\\epsilon} \\frac{x(s)}{s}ds + B\\int\_t^{t+\\epsilon}\\frac{u(s)}{s}ds\\end{equation}
根据$u(t)$定义，第二项积分的$u(s)$在$\[t,t+\\epsilon)$恒为$u\_k$，所以它相当于$1/s$的积分，可以直接积分出来得$\\ln\\frac{t+\\epsilon}{t}$，当然直接换为一阶近似$\\frac{\\epsilon}{t}$也无妨，因为本身$u\_k$到$u(t)$的变换有很大自由度，这点误差无所谓。至于第一项积分，我们直接采用精度更高的中点近似，得到
\\begin{equation}\\begin{gathered}
x(t+\\epsilon) - x(t) = \\frac{1}{2}\\epsilon A\\left(\\frac{x(t)}{t}+\\frac{x(t+\\epsilon)}{t+\\epsilon}\\right) + \\frac{\\epsilon}{t} B u\_k \\\\[5pt\]
\\Downarrow \\\\[5pt\]
x(t+\\epsilon) = \\left(I - \\frac{\\epsilon A}{2(t+\\epsilon)}\\right)^{-1}\\left\[\\left(I + \\frac{\\epsilon A}{2t}\\right)x(t) + \\frac{\\epsilon}{t} B u\_k\\right\]
\\end{gathered}\\label{eq:legs-ode-bilinear}\\end{equation}
事实上，式$\\eqref{eq:legs-ode}$也可以精确求解，只需要留意到它等价于
\\begin{equation}Ax(t) + Bu(t) = t x'(t) = \\frac{d}{d\\ln t} x(t)\\end{equation}
这意味着只需要做变量代换$\\tau = \\ln t$，那么LegS型ODE就可以转化为LegT型ODE：
\\begin{equation}\\frac{d}{d\\tau} x(e^{\\tau}) = Ax(e^{\\tau}) + Bu(e^{\\tau})\\end{equation}
利用式$\\eqref{eq:legt-ode-sol}$得到（由于变量代换，时间间隔由$\\epsilon$变成$\\ln(t+\\epsilon) - \\ln t$）
\\begin{equation}x(t+\\epsilon) = e^{(\\ln(t+\\epsilon) - \\ln t) A} x(t) + A^{-1} \\big(e^{(\\ln(t+\\epsilon) - \\ln t) A} - I\\big) B u\_k\\label{eq:legs-ode-sol}\\end{equation}
然而，上式虽然是精确解，但不如同为精确解的式$\\eqref{eq:legt-ode-sol}$好用，因为式$\\eqref{eq:legt-ode-sol}$的指数矩阵部分是$e^{\\epsilon A}$，跟时间$t$无关，所以一次性计算完就可以了。但上式中$t$在矩阵指数里边，意味着在迭代过程中需要反复计算矩阵指数，对计算并不友好，所以LegS型ODE我们一般只会用式$\\eqref{eq:legs-ode-bilinear}$来离散化。

## 优良性质 [\#](https://kexue.fm/kexue.fm\#%E4%BC%98%E8%89%AF%E6%80%A7%E8%B4%A8)

接下来，LegS是我们的重点关注对象。重点关注LegS的原因并不难猜，因为从推导的假设来看，它是目前求解出来的唯一一个能够记忆整个历史的ODE系统，这对于很多场景如多轮对话来说至关重要。此外，它还有其他的一些比较良好且实用的性质。

### 尺度等变 [\#](https://kexue.fm/kexue.fm\#%E5%B0%BA%E5%BA%A6%E7%AD%89%E5%8F%98)

比如，LegS的离散化格式$\\eqref{eq:legs-ode-bilinear}$是步长无关的，我们只需要将$t=k\\epsilon$代入里边，并记$x(k\\epsilon)=x\_k$，就可以发现
\\begin{equation}
x\_{k+1} = \\left(I - \\frac{A}{2(k + 1)}\\right)^{-1}\\left\[\\left(I + \\frac{A}{2k}\\right)x\_k + \\frac{1}{k} B u\_k\\right\]\\end{equation}
步长$\\epsilon$被自动地消去了，从而自然地减少了一个需要调的超参数，这对于炼丹人士显然是一个好消息。注意步长无关是LegS型ODE的一个固有性质，它跟具体的离散化方式并无直接关系，比如精确解$\\eqref{eq:legs-ode-sol}$同样是步长无关的：
\\begin{equation}x\_{k+1} = e^{(\\ln(k+1) - \\ln k) A} x\_k + A^{-1} \\big(e^{(\\ln(k+1) - \\ln k) A} - I\\big) B u\_k\\label{eq:legs-ode-sol-2}\\end{equation}
其背后的原因，在于LegS型ODE满足“ **时间尺度等变性（Timescale equivariance）**”——如果我们设$t=\\lambda\\tau$代入LegS型ODE，将得到
\\begin{equation}Ax(\\alpha\\tau) + Bu(\\alpha\\tau) = (\\alpha\\tau)\\times \\frac{d}{d(\\alpha\\tau)} x(\\alpha\\tau) = \\tau \\frac{d}{d\\tau}x(\\alpha\\tau)\\end{equation}
这意味着，当我们将$u(t)$换成$u(\\alpha t)$时，LegS的ODE形式并没有变化，而对应的解则是$x(t)$换成了$x(\\alpha t)$。这个性质的直接后果就是：当我们选择更大的步长时，递归格式不需要发生变化，因为结果$x\_k$的步长也会自动放大，这就是LegS型ODE离散化与步长无关的本质原因。

### 长尾衰减 [\#](https://kexue.fm/kexue.fm\#%E9%95%BF%E5%B0%BE%E8%A1%B0%E5%87%8F)

LegS型ODE的另一个优良性质是，它关于历史信号的记忆是 **多项式衰减（Polynomial decay）** 的，这比常规RNN的指数衰减更缓慢，从而理论上能记忆更长的历史，更不容易梯度消失。为了理解这一点，我们可以从精确解$\\eqref{eq:legs-ode-sol-2}$出发，从式$\\eqref{eq:legs-ode-sol-2}$可以看到，每递归一步，历史信息的衰减效应可以用矩阵指数$e^{(\\ln(k+1) - \\ln k) A}$来描述，那么从第$m$步递归到第$n$步，总的衰减效应是
\\begin{equation}\\prod\_{k=m}^{n-1} e^{(\\ln(k+1) - \\ln k) A} = e^{(\\ln n - \\ln m) A}\\end{equation}
回顾HiPPO-LegS中$A$的形式：
\\begin{equation}A\_{n,k} = -\\left\\{\\begin{array}{l}\\sqrt{(2n+1)(2k+1)}, &k < n \\\ n+1, &k = n \\\
0, &k > n\\end{array}\\right.\\end{equation}
从定义可以看出，$A$是一个下三角阵，其对角线元素为$-1,-2,-3,\\cdots$。我们知道，三角阵的对角线元素正好是它的特征值（参考 [Triangular matrix](https://en.wikipedia.org/wiki/Triangular_matrix)），由此可以看到一个$d\\times d$大小的$A$矩阵，有$d$个不同的特征值$-1,-2,\\cdots,-d$，这说明$A$矩阵是可对角化的，即存在可逆矩阵$P$，使得$A = P^{-1}\\Lambda P$，其中$\\Lambda = \\text{diag}(-1,-2,\\cdots,-d)$，于是我们有
\\begin{equation}\\begin{aligned}
e^{(\\ln n - \\ln m) A} =&\\, e^{(\\ln n - \\ln m) P^{-1}\\Lambda P} \\\
=&\\, P^{-1} e^{(\\ln n - \\ln m) \\Lambda}P \\\
=&\\, P^{-1}\\,\\text{diag}(e^{-(\\ln n - \\ln m)},e^{-2(\\ln n - \\ln m)},\\cdots,e^{-d(\\ln n - \\ln m)})\\,P \\\
=&\\, P^{-1}\\,\\text{diag}\\Big(\\frac{m}{n},\\frac{m^2}{n^2},\\cdots,\\frac{m^d}{n^d}\\Big)\\,P \\\
\\end{aligned}\\end{equation}
可见，最终的衰减函数是$1/n$的$1,2,\\cdots,d$次函数的线性组合，所以LegS型ODE关于历史记忆至多是多项式衰减的，比指数衰减更加长尾，因此理论上有更好的记忆力。

### 计算高效 [\#](https://kexue.fm/kexue.fm\#%E8%AE%A1%E7%AE%97%E9%AB%98%E6%95%88)

最后，我们指出HiPPO-LegS的$A$矩阵是 **计算高效（Computational efficiency）** 的。具体来说，直接按照矩阵乘法的朴素实现的话，一个$d\\times d$的矩阵乘以$d\\times 1$的列向量，需要做$d^2$次乘法，但LegS的$A$矩阵与向量相乘则可以降低到$\\mathcal{O}(d)$次，更进一步地，我们还可以证明离散化后的$\\eqref{eq:legs-ode-bilinear}$也可以在$\\mathcal{O}(d)$完成。

为了理解这一点，我们首先将HiPPO-LegS的$A$矩阵等价地改写成
\\begin{equation}A\_{n,k} = \\left\\{\\begin{array}{l}n\\delta\_{n,k} - \\sqrt{2n+1}\\sqrt{2k+1}, &k \\leq n \\\ 0, &k > n\\end{array}\\right.\\end{equation}
对于向量$v = \[v\_0,v\_1,\\cdots,v\_{d-1}\]$，我们有
\\begin{equation}\\begin{aligned}
(Av)\_n = \\sum\_{k=0}^n A\_{n,k}v\_k =&\\, \\sum\_{k=0}^n \\left(n\\delta\_{n,k} - \\sqrt{2n+1}\\sqrt{2k+1}\\right)v\_k \\\
=&\\, n v\_n -\\sqrt{2n+1}\\sum\_{k=0}^n \\sqrt{2k+1}v\_k
\\end{aligned}\\end{equation}
这包含三种运算，第一项的$n v\_n$是向量$\[0,1,2,\\cdots,d-1\]$与$v$做逐位相乘运算，第二项的$\\sqrt{2k+1}v\_k$则是向量$\[1,\\sqrt{3},\\sqrt{5},\\cdots,\\sqrt{2d-1}\]$与$v$做逐位相乘，然后$\\sum\\limits\_{k=0}^n$就是$\\text{cumsum}$运算，最后乘以$\\sqrt{2n+1}$就是再逐位相乘向量$\[1,\\sqrt{3},\\sqrt{5},\\cdots,\\sqrt{2d-1}\]$，每一步都可以在$\\mathcal{O}(d)$内完成，因此总的复杂度是$\\mathcal{O}(d)$的。

我们再来看$\\eqref{eq:legs-ode-bilinear}$，它包含两步“矩阵-向量”乘法运算，一是$(I+\\lambda A)v$，$\\lambda$是任意实数，刚才我们已经证明了$Av$是计算高效的，自然$(I+\\lambda A)v$也是；二是$(I-\\lambda A)^{-1}v$，接下来我们将证明它也是计算高效的。这只需要留意到求$z=(I-\\lambda A)^{-1}v$等价于解方程$v = (I-\\lambda A)z$，利用上面给出的$Av$表达式，我们可以得到
\\begin{equation}v\_n = z\_n - \\lambda \\left(n z\_n - \\sqrt{2n+1}\\sum\_{k=0}^n \\sqrt{2k+1}z\_k\\right)\\end{equation}
记$S\_n = \\sum\\limits\_{k=0}^n \\sqrt{2k+1}z\_k$，那么$z\_n = \\frac{S\_n - S\_{n-1}}{\\sqrt{2n+1}}$，代入上式得
\\begin{equation}v\_n = \\frac{S\_n - S\_{n-1}}{\\sqrt{2n+1}} - \\lambda \\left(n \\frac{S\_n - S\_{n-1}}{\\sqrt{2n+1}} - \\sqrt{2n+1}S\_n\\right)\\end{equation}
整理得
\\begin{equation}S\_n = \\frac{1 - \\lambda n}{1+\\lambda n + \\lambda}S\_{n-1} + \\frac{\\sqrt{2n+1}}{1+\\lambda n + \\lambda}v\_n\\end{equation}
这是一个标量的递归式，可以完全串行地计算，也可以利用Prefix Sum的相关算法并行计算（参考 [这里](https://kexue.fm/archives/9554#%E5%B9%B6%E8%A1%8C%E5%8C%96)），计算复杂度为$\\mathcal{O}(d)$或者$\\mathcal{O}(d\\log d)$，总之相比$\\mathcal{O}(d^2)$都会更加高效。

## 傅立叶基 [\#](https://kexue.fm/kexue.fm\#%E5%82%85%E7%AB%8B%E5%8F%B6%E5%9F%BA)

最后，我们以傅立叶基的一个推导收尾。在上一篇文章中，我们以傅立叶级数来引出了线性系统，但只推导了邻近窗口形式的结果，而后面的勒让德多项式基我们则推导了邻近窗口和完整区间两个版本（即LegT和LegS）。那么傅立叶基究竟能不能推导一个跟LegS相当的版本呢？其中会面临什么困难呢？下面我们对此进行探讨。

同样地，相关铺垫我们不再重复，按照上一节的记号，傅立叶基的系数为
\\begin{equation}c\_n(T) = \\int\_0^1 u(t\_{\\leq T}(s)) e^{-2i\\pi n s}ds\\end{equation}
跟LegS一样，为了记忆整个$\[0,T\]$区间的信号，我们需要一个$\[0,1\]\\mapsto \[0,T\]$的映射，为此选取最简单的$t\_{\\leq T}(s)=sT$，代入后两边求导得到
\\begin{equation}\\frac{d}{dT}c\_n(T) = \\int\_0^1 u'(sT) s e^{-2i\\pi n s}ds\\end{equation}
分部积分得到
\\begin{equation}\\begin{aligned}
\\frac{d}{dT}c\_n(T) =&\\, \\frac{1}{T}\\int\_0^1 s e^{-2i\\pi n s}d u(sT) \\\
=&\\, \\frac{1}{T} u(sT) s e^{-2i\\pi n s}\\big\|\_{s=0}^{s=1} - \\frac{1}{T}\\int\_0^1 u(sT) d(s e^{-2i\\pi n s})\\\
=&\\, \\frac{1}{T} u(T) - \\frac{1}{T}\\int\_0^1 u(sT) e^{-2i\\pi n s} ds + \\frac{2i\\pi n}{T}\\int\_0^1 u(sT) s e^{-2i\\pi n s} ds\\\
=&\\, \\frac{1}{T} u(T) - \\frac{1}{T}c\_n(T) + \\frac{2i\\pi n}{T}\\int\_0^1 u(sT) s e^{-2i\\pi n s} ds\\\
\\end{aligned}\\end{equation}
上一篇文章我们提到，HiPPO选取勒让德多项式为基的重要原因之一是$(s+1)p\_n'(t)$可以分解为$p\_0(t),p\_1(t),\\cdots,p\_n(t)$的线性组合，而傅里叶基的$s e^{-2i\\pi n s}$则不能做到这一点。但事实上，如果允许误差的话，这个断论是不成立的，因为我们同样可以将$s$分解为傅里叶级数：
\\begin{equation}s = \\frac{1}{2} + \\frac{i}{2\\pi}\\sum\_{k\\neq 0} \\frac{1}{k} e^{2i\\pi k s}\\end{equation}
这里的求和有无限项，如果要截断为有限项的话，就会产生误差，但我们可以先不纠结这一点，直接往上代入得到
\\begin{equation}\\begin{aligned}
&\\,\\frac{2i\\pi n}{T}\\int\_0^1 u(sT) s e^{-2i\\pi n s} ds \\\
=&\\, \\frac{2i\\pi n}{T}\\int\_0^1 u(sT) \\left(\\frac{1}{2} + \\frac{i}{2\\pi}\\sum\_{k\\neq 0} \\frac{1}{k} e^{2i\\pi k s}\\right) e^{-2i\\pi n s} ds \\\
=&\\, \\frac{i\\pi n}{T}\\int\_0^1 u(sT) e^{-2i\\pi n s} ds - \\frac{1}{T}\\sum\_{k\\neq 0} \\frac{n}{k}\\int\_0^1 u(sT) e^{-2i\\pi (n - k) s} ds \\\
=&\\, \\frac{i\\pi n}{T}c\_n(T) - \\frac{1}{T}\\sum\_{k\\neq 0} \\frac{n}{k}c\_{n-k}(T) \\\
=&\\, \\frac{i\\pi n}{T}c\_n(T) - \\frac{1}{T}\\sum\_{k\\neq n} \\frac{n}{n - k}c\_k(T) \\\
\\end{aligned}\\end{equation}
这样一来
\\begin{equation}
\\frac{d}{dT}c\_n(T) = \\frac{1}{T} u(T) + \\frac{i\\pi n - 1}{T}c\_n(T) - \\frac{1}{T}\\sum\_{k\\neq n} \\frac{n}{n - k}c\_k(T)\\end{equation}
所以可以写出
\\begin{equation}\\begin{aligned}
x'(t) =&\\, \\frac{A}{t}x(t) + \\frac{B}{t}u(t)\\\\[8pt\]
\\quad A\_{n,k} =&\\, \\left\\{\\begin{array}{l}-\\frac{n}{n-k}, &k \\neq n \\\ i\\pi n - 1, &k = n\\end{array}\\right.\\\\[8pt\]
B\_n =&\\, 1
\\end{aligned}\\end{equation}
实际使用的时候，我们只需要截断$\|n\|,\|k\|\\leq N$，就可以得到一个$(2N+1)\\times (2N+1)$的矩阵。截断带来的误差其实是无所谓的，因为我们在推导HiPPO-LegT的时候同样引入了有限级数近似，那会我们同样也没考虑误差，或者反过来讲，对于特定的任务，我们会选择适当的规模（即$N$的大小），而这个“适当”的含义之一，就是截断带来误差对于该任务是可以忽略的。

对大多数人来说，傅立叶基的这个推导可能还更容易理解一些，因为勒让德多项式对很多读者来说都比较陌生，尤其是LegT、LegS推导过程中用到的几个恒等式，而对于傅立叶级数大多数读者应该或多或少都有所了解。不过，从结果上来看，傅立叶基的这个结果可能不如LegS实用，一来它引入了复数，这增加了实现的复杂度，二来它推导出的$A$矩阵不像LegS那样是个相对较淡的下三角阵，因此理论分析起来也更为复杂。所以，大家权当它是一道深化对HiPPO的理解的练习题就好。

## 文章小结 [\#](https://kexue.fm/kexue.fm\#%E6%96%87%E7%AB%A0%E5%B0%8F%E7%BB%93)

在这篇文章中，我们补充探讨了上一篇文章介绍的HiPPO的一些遗留问题，其中包括如何对ODE进行离散化、LegS型ODE的一些优良性质，以及利用傅立叶基记忆整个历史区间的结果推导（即LegS的傅立叶版本），以求获得对HiPPO的更全面理解。

_**转载到请包括本文地址：** [https://kexue.fm/archives/10137](https://kexue.fm/archives/10137)_

_**更详细的转载事宜请参考：**_ [《科学空间FAQ》](https://kexue.fm/archives/6508#%E6%96%87%E7%AB%A0%E5%A6%82%E4%BD%95%E8%BD%AC%E8%BD%BD/%E5%BC%95%E7%94%A8)

**如果您还有什么疑惑或建议，欢迎在下方评论区继续讨论。**

**如果您觉得本文还不错，欢迎 [分享](https://kexue.fm/kexue.fm#share)/ [打赏](https://kexue.fm/kexue.fm#pay) 本文。打赏并非要从中获得收益，而是希望知道科学空间获得了多少读者的真心关注。当然，如果你无视它，也不会影响你的阅读。再次表示欢迎和感谢！**

打赏

微信打赏

支付宝打赏

因为网站后台对打赏并无记录，因此欢迎在打赏时候备注留言。你还可以 [**点击这里**](http://mail.qq.com/cgi-bin/qm_share?t=qm_mailme&email=tN7d1drY3drrx8H0xcWa19vZ) 或在下方评论区留言来告知你的建议或需求。

**如果您需要引用本文，请参考：**

苏剑林. (Jun. 05, 2024). 《重温SSM（二）：HiPPO的一些遗留问题 》\[Blog post\]. Retrieved from [https://kexue.fm/archives/10137](https://kexue.fm/archives/10137)

@online{kexuefm-10137,
        title={重温SSM（二）：HiPPO的一些遗留问题},
        author={苏剑林},
        year={2024},
        month={Jun},
        url={\\url{https://kexue.fm/archives/10137}},
}

分类： [数学研究](https://kexue.fm/category/Mathematics)    标签： [线性](https://kexue.fm/tag/%E7%BA%BF%E6%80%A7/), [差分](https://kexue.fm/tag/%E5%B7%AE%E5%88%86/), [RNN](https://kexue.fm/tag/RNN/), [梯度](https://kexue.fm/tag/%E6%A2%AF%E5%BA%A6/), [ssm](https://kexue.fm/tag/ssm/)[8 评论](https://kexue.fm/archives/10137#comments)

< [Transformer升级之路：18、RoPE的底数选择原则](https://kexue.fm/archives/10122) \| [通向概率分布之路：盘点Softmax及其替代品](https://kexue.fm/archives/10145) >

### 你也许还对下面的内容感兴趣

- [AdamW的Weight RMS的渐近估计（下）](https://kexue.fm/archives/11404)
- [DiVeQ：一种非常简洁的VQ训练方案](https://kexue.fm/archives/11328)
- [为什么线性注意力要加Short Conv？](https://kexue.fm/archives/11320)
- [AdamW的Weight RMS的渐近估计（上）](https://kexue.fm/archives/11307)
- [为什么Adam的Update RMS是0.2？](https://kexue.fm/archives/11267)
- [重新思考学习率与Batch Size（一）：现状](https://kexue.fm/archives/11260)
- [矩阵r次方根和逆r次方根的高效计算](https://kexue.fm/archives/11175)
- [矩阵平方根和逆平方根的高效计算](https://kexue.fm/archives/11158)
- [“对角+低秩”三角阵的高效求逆方法](https://kexue.fm/archives/11072)
- [矩阵符号函数mcsgn能计算什么？](https://kexue.fm/archives/11056)

[发表你的看法](https://kexue.fm/kexue.fm#comment_form)

loongfish

June 6th, 2024

作者君有个小笔误：方程(8)第二个等式的右边x(t)的系数矩阵是加法：
\\begin{equation}\\begin{gathered}
x(t+\\epsilon) - x(t) = \\frac{1}{2}\\epsilon A \[x(t) + x(t+\\epsilon)\] + \\epsilon B f\_k \\\
\\Downarrow \\\
x(t+\\epsilon) = (I - \\epsilon A/2)^{-1}\[(I + \\epsilon A/2) x(t) + \\epsilon B f\_k\]
\\end{gathered}\\end{equation}

[回复评论](https://kexue.fm/archives/10137/comment-page-1?replyTo=24507#respond-post-10137)

[苏剑林](https://kexue.fm) 发表于
June 6th, 2024

感谢指出，已经更正～

[回复评论](https://kexue.fm/archives/10137/comment-page-1?replyTo=24508#respond-post-10137)

ningmang208

July 24th, 2024

感谢苏佬分享! 请教一下，根据式 (20)，如何理解 "最终的衰减函数是1/n
的1,2,⋯,d次函数的线性组合" 呢？ 我猜测是 x(t) 的第一个元素以 m/n 衰减，第二个元素以 (m/n)^2 衰减，越靠后的元素衰减越快，可以这样理解吗?

[回复评论](https://kexue.fm/archives/10137/comment-page-1?replyTo=24901#respond-post-10137)

[苏剑林](https://kexue.fm) 发表于
July 29th, 2024

不一定啊，就是$P^{-1}\\,\\text{diag}\\Big(\\frac{m}{n},\\frac{m^2}{n^2},\\cdots,\\frac{m^d}{n^d}\\Big)\\,P$的每一个元素，实际上是$m/n$的不超过$d$次的多项式，$x(t)$的每个元素，其衰减方式也是$m/n$的不超过$d$次的多项式，具体多少次算了才知道。

[回复评论](https://kexue.fm/archives/10137/comment-page-1?replyTo=24919#respond-post-10137)

EthanZou 发表于
August 29th, 2024

我也不太理解这个部分，可以麻烦苏老师更加详细的解释一下嘛

[回复评论](https://kexue.fm/archives/10137/comment-page-1?replyTo=25133#respond-post-10137)

[苏剑林](https://kexue.fm) 发表于
September 1st, 2024

将$P$看成常数矩阵，那么$P^{-1}\\,\\text{diag}\\Big(\\frac{m}{n},\\frac{m^2}{n^2},\\cdots,\\frac{m^d}{n^d}\\Big)\\,P$就是$\\frac{m}{n},\\frac{m^2}{n^2},\\cdots,\\frac{m^d}{n^d}$的线性组合啊（矩阵乘法就是线性运算），这还能怎么详细？

[回复评论](https://kexue.fm/archives/10137/comment-page-1?replyTo=25141#respond-post-10137)

akagiwang

September 11th, 2024

苏神，Timescale equivariance这个词应该翻译成不变性还是等变性？我们的模型在中文语境下其实分不清这两个词，是普遍中文翻译的问题吗。
我理解一个是$ f(x) = f(T(x))$（不变性），一个是$ T(f(x)) = f(T(x))$（等变性）；
这里$u(t)$的步长选择不同$t = \\alpha t$改变了$x(t)$的步长选择应该是不变性还是等变性？

[回复评论](https://kexue.fm/archives/10137/comment-page-1?replyTo=25211#respond-post-10137)

[苏剑林](https://kexue.fm) 发表于
September 12th, 2024

谢谢，参考你的建议改过来了。不过就我的观点看来，其实“等变”还是“不变”都无所谓，因为你单说“等变”还是“不变”，大家都不理解是什么意思，只有后面附以相应的数学说明，才能把它的真正含义解析清楚。

[回复评论](https://kexue.fm/archives/10137/comment-page-1?replyTo=25224#respond-post-10137)

[取消回复](https://kexue.fm/archives/10137#respond-post-10137)

你的大名

电子邮箱

个人网站（选填）

1\. 可以使用LaTeX代码，点击“预览效果”可查看效果；2. 可以通过点击评论楼层编号来引用该楼层；3. 网站可能会有点卡，如非确认评论失败，请 **不要重复点击提交**。

### 内容速览

[离散格式](https://kexue.fm/kexue.fm#%E7%A6%BB%E6%95%A3%E6%A0%BC%E5%BC%8F)
[输入转换](https://kexue.fm/kexue.fm#%E8%BE%93%E5%85%A5%E8%BD%AC%E6%8D%A2)
[LegT版本](https://kexue.fm/kexue.fm#LegT%E7%89%88%E6%9C%AC)
[LegS版本](https://kexue.fm/kexue.fm#LegS%E7%89%88%E6%9C%AC)
[优良性质](https://kexue.fm/kexue.fm#%E4%BC%98%E8%89%AF%E6%80%A7%E8%B4%A8)
[尺度等变](https://kexue.fm/kexue.fm#%E5%B0%BA%E5%BA%A6%E7%AD%89%E5%8F%98)
[长尾衰减](https://kexue.fm/kexue.fm#%E9%95%BF%E5%B0%BE%E8%A1%B0%E5%87%8F)
[计算高效](https://kexue.fm/kexue.fm#%E8%AE%A1%E7%AE%97%E9%AB%98%E6%95%88)
[傅立叶基](https://kexue.fm/kexue.fm#%E5%82%85%E7%AB%8B%E5%8F%B6%E5%9F%BA)
[文章小结](https://kexue.fm/kexue.fm#%E6%96%87%E7%AB%A0%E5%B0%8F%E7%BB%93)

### 智能搜索

支持整句搜索！网站自动使用 [结巴分词](https://github.com/fxsjy/jieba) 进行分词，并结合ngrams排序算法给出合理的搜索结果。

### 热门标签

[生成模型](https://kexue.fm/tag/%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B/) [attention](https://kexue.fm/tag/attention/) [优化](https://kexue.fm/tag/%E4%BC%98%E5%8C%96/) [语言模型](https://kexue.fm/tag/%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/) [模型](https://kexue.fm/tag/%E6%A8%A1%E5%9E%8B/) [网站](https://kexue.fm/tag/%E7%BD%91%E7%AB%99/) [概率](https://kexue.fm/tag/%E6%A6%82%E7%8E%87/) [梯度](https://kexue.fm/tag/%E6%A2%AF%E5%BA%A6/) [矩阵](https://kexue.fm/tag/%E7%9F%A9%E9%98%B5/) [转载](https://kexue.fm/tag/%E8%BD%AC%E8%BD%BD/) [优化器](https://kexue.fm/tag/%E4%BC%98%E5%8C%96%E5%99%A8/) [微分方程](https://kexue.fm/tag/%E5%BE%AE%E5%88%86%E6%96%B9%E7%A8%8B/) [分析](https://kexue.fm/tag/%E5%88%86%E6%9E%90/) [天象](https://kexue.fm/tag/%E5%A4%A9%E8%B1%A1/) [深度学习](https://kexue.fm/tag/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/) [积分](https://kexue.fm/tag/%E7%A7%AF%E5%88%86/) [python](https://kexue.fm/tag/python/) [扩散](https://kexue.fm/tag/%E6%89%A9%E6%95%A3/) [力学](https://kexue.fm/tag/%E5%8A%9B%E5%AD%A6/) [无监督](https://kexue.fm/tag/%E6%97%A0%E7%9B%91%E7%9D%A3/) [几何](https://kexue.fm/tag/%E5%87%A0%E4%BD%95/) [节日](https://kexue.fm/tag/%E8%8A%82%E6%97%A5/) [生活](https://kexue.fm/tag/%E7%94%9F%E6%B4%BB/) [文本生成](https://kexue.fm/tag/%E6%96%87%E6%9C%AC%E7%94%9F%E6%88%90/) [数论](https://kexue.fm/tag/%E6%95%B0%E8%AE%BA/)

### 随机文章

- [【不可思议的Word2Vec】 3.提取关键词](https://kexue.fm/archives/4316)
- [新春快乐！2011年2月重要天象](https://kexue.fm/archives/1190)
- [电的相对论效应——磁“子虚乌有”？](https://kexue.fm/archives/1987)
- [跟风玩玩目前最大的中文GPT2模型（bert4keras）](https://kexue.fm/archives/7912)
- [生成扩散模型漫谈（三）：DDPM = 贝叶斯 + 去噪](https://kexue.fm/archives/9164)
- [折腾windows 8和ubuntu 12](https://kexue.fm/archives/1706)
- [矩阵符号函数mcsgn能计算什么？](https://kexue.fm/archives/11056)
- [貌离神合的RNN与ODE：花式RNN简介](https://kexue.fm/archives/5643)
- [【NASA每日一图】经典的猎户座星云](https://kexue.fm/archives/102)
- [GPLinker：基于GlobalPointer的事件联合抽取](https://kexue.fm/archives/8926)

### 最近评论

- [qsh](https://kexue.fm/archives/11416/comment-page-1#comment-28933): 用muon的时候weights initialization有什么讲究吗？还是直接pytorc...
- [夺宇](https://kexue.fm/archives/9152/comment-page-3#comment-28932): 苏老师的解释和推导好自然啊，比原论文更容易看懂
- [夺宇](https://kexue.fm/archives/9119/comment-page-13#comment-28931): 苏老师，(17)式中的噪声项可以直接在(8)式中直接添加吗？(8)式中添加一个噪声项似乎对后续...
- [Xiaozhi Zhu](https://kexue.fm/archives/11428/comment-page-1#comment-28930): 我觉得这个work摆脱了two stages，真正做到E2E，让feature extract...
- [wednesday](https://kexue.fm/archives/3319/comment-page-1#comment-28929): 想问问苏老师的数据挖掘学习思路或者学习路径是怎样的
- [wednesday](https://kexue.fm/archives/5239/comment-page-3#comment-28928): 因为我们只对p(Y\|X)建模，因此$p\_{\\theta}(X)$我们认为就是$\\tilde{p...
- [ykwen](https://kexue.fm/archives/10592/comment-page-3#comment-28927): 不动点迭代的时候 有没有可能迭代到0附近呢？
- [wednesday](https://kexue.fm/archives/5253/comment-page-18#comment-28926): 这是针对评论区一位同学问题的提问，现在已经懂了
- [wednesday](https://kexue.fm/archives/5253/comment-page-18#comment-28925): 针对一个样本和针对一个batch有什么特别的区别吗
- [Zhancun](https://kexue.fm/archives/11428/comment-page-1#comment-28924): 也可以从natural image space的角度去理解。对于一张256x256x3的图片，...

### 友情链接

- [Cool Papers](https://papers.cool)
- [数学研发](https://bbs.emath.ac.cn)
- [Seatop](http://www.seatop.com.cn/)
- [Xiaoxia](https://xiaoxia.org/)
- [积分表-网络版](https://kexue.fm/sci/integral/index.html)
- [丝路博傲](http://blog.dvxj.com/)
- [数学之家](http://www.2math.cn/)
- [有趣天文奇观](http://interesting-sky.china-vo.org/)
- [TwistedW](http://www.twistedwg.com/)
- [godweiyang](https://godweiyang.com/)
- [AI柠檬](https://blog.ailemon.net/)
- [王登科-DK博客](https://greatdk.com)
- [ESON](https://blog.eson.org/)
- [枫之羽](https://fzhiy.net/)
- [Mathor's blog](https://wmathor.com/)
- [coding-zuo](https://coding-zuo.github.io/)
- [博科园](https://www.bokeyuan.net/)
- [孔皮皮的博客](https://www.kppkkp.top/)
- [运鹏的博客](https://yunpengtai.top/)
- [jiming.site](https://jiming.site/)
- [OmegaXYZ](https://www.omegaxyz.com/)
- [EAI猩球](https://www.robotech.ink/)
- [文举的博客](https://liwenju0.com/)
- [申请链接](https://kexue.fm/links.html)

本站采用创作共用版权协议，要求署名、非商业用途和保持一致。转载本站内容必须也遵循“ [署名-非商业用途-保持一致](http://creativecommons.org/licenses/by-nc-nd/2.5/cn/)”的创作共用协议。
© 2009-2025 Scientific Spaces. All rights reserved. Theme by [laogui](http://www.laogui.com). Powered by [Typecho](http://typecho.org). 备案号: [粤ICP备09093259号-1/2](https://beian.miit.gov.cn/)。