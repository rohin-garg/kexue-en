![MobileSideBar](https://kexue.fm/usr/themes/geekg/images/slide-button.png)

## SEARCH

## MENU

- [打赏](https://kexue.fm/reward.html)
- [公式](https://kexue.fm/latex.html)
- [天象](https://kexue.fm/ac.html)
- [链接](https://kexue.fm/links.html)
- [时光](https://kexue.fm/me.html)
- [博览](https://kexue.fm/science.html)
- [归档](https://kexue.fm/content.html)

## CATEGORIES

- [千奇百怪](https://kexue.fm/category/Everything)
- [天文探索](https://kexue.fm/category/Astronomy)
- [数学研究](https://kexue.fm/category/Mathematics)
- [物理化学](https://kexue.fm/category/Phy-chem)
- [信息时代](https://kexue.fm/category/Big-Data)
- [生物自然](https://kexue.fm/category/Biology)
- [图片摄影](https://kexue.fm/category/Photograph)
- [问题百科](https://kexue.fm/category/Questions)
- [生活/情感](https://kexue.fm/category/Life-Feeling)
- [资源共享](https://kexue.fm/category/Resources)

## NEWPOSTS

- [让炼丹更科学一些（五）：基于梯度精...](https://kexue.fm/archives/11530)
- [让炼丹更科学一些（四）：新恒等式，...](https://kexue.fm/archives/11494)
- [为什么DeltaNet要加L2 N...](https://kexue.fm/archives/11486)
- [让炼丹更科学一些（三）：SGD的终...](https://kexue.fm/archives/11480)
- [让炼丹更科学一些（二）：将结论推广...](https://kexue.fm/archives/11469)
- [滑动平均视角下的权重衰减和学习率](https://kexue.fm/archives/11459)
- [生成扩散模型漫谈（三十一）：预测数...](https://kexue.fm/archives/11428)
- [Muon优化器指南：快速上手与关键细节](https://kexue.fm/archives/11416)
- [AdamW的Weight RMS的...](https://kexue.fm/archives/11404)
- [n个正态随机数的最大值的渐近估计](https://kexue.fm/archives/11390)

## COMMENTS

- [Bin: 今天偶然从某个论坛看到有人推荐您的博客，定睛一看竟然是华师同院...](https://kexue.fm/archives/1990/comment-page-2#comment-29105)
- [Rapture D: 我有一个问题，为什么不考虑亥姆霍兹定理和斯托克斯公式。](https://kexue.fm/archives/11530/comment-page-1#comment-29104)
- [mofheka: 苏神是还在用jax是么？最近在做基于Google Pathwa...](https://kexue.fm/archives/11390/comment-page-1#comment-29103)
- [长琴: 看懂这篇博客也不是一件容易的事情。](https://kexue.fm/archives/11530/comment-page-1#comment-29102)
- [AlexLi: 苏老师，请教一下(7)式中将 $\\mu(x\_t)$ 传给 $p...](https://kexue.fm/archives/9257/comment-page-4#comment-29101)
- [tyler\_zxc: "Performer的思想是将标准的Attention线性化，...](https://kexue.fm/archives/7921/comment-page-2#comment-29100)
- [我: 似乎并非mHC提出矩阵的思想？之前hyper connecti...](https://kexue.fm/archives/11494/comment-page-1#comment-29099)
- [winter: 苏神您好，假如对于比较均匀的attention weightP...](https://kexue.fm/archives/10847/comment-page-1#comment-29098)
- [苏剑林: KL散度、JS散度、W距离啥的，都行啊，看你喜欢哪个](https://kexue.fm/archives/8512/comment-page-2#comment-29097)
- [苏剑林: 没有绝对公平的对比方法，主要看你关心什么。比如，如果只关心推理...](https://kexue.fm/archives/9119/comment-page-14#comment-29096)

## USERLOGIN

- [登录](https://kexue.fm/admin/login.php)

[科学空间\|Scientific Spaces](https://kexue.fm/)

- [登录](https://kexue.fm/admin/login.php)
- [打赏](https://kexue.fm/reward.html)
- [公式](https://kexue.fm/latex.html)
- [天象](https://kexue.fm/ac.html)
- [链接](https://kexue.fm/links.html)
- [时光](https://kexue.fm/me.html)
- [博览](https://kexue.fm/science.html)
- [归档](https://kexue.fm/content.html)

渴望成为一个小飞侠

- [![](https://kexue.fm/usr/themes/geekg/images/rss.png)\\
\\
欢迎订阅](https://kexue.fm/feed)
- [![](https://kexue.fm/usr/themes/geekg/images/mail.png)\\
\\
个性邮箱](https://kexue.fm/archives/119)
- [![](https://kexue.fm/usr/themes/geekg/images/Saturn.png)\\
\\
天象信息](https://kexue.fm/ac.html)
- [![](https://kexue.fm/usr/themes/geekg/images/iss.png)\\
\\
观测ISS](https://kexue.fm/archives/41)
- [![](https://kexue.fm/usr/themes/geekg/images/pi.png)\\
\\
LaTeX](https://kexue.fm/latex.html)
- [![](https://kexue.fm/usr/themes/geekg/images/mlogo.png)\\
\\
关于博主](https://kexue.fm/me.html)

欢迎访问“科学空间”，这里将与您共同探讨自然科学，回味人生百态；也期待大家的分享～

- [**千奇百怪** Everything](https://kexue.fm/category/Everything)
- [**天文探索** Astronomy](https://kexue.fm/category/Astronomy)
- [**数学研究** Mathematics](https://kexue.fm/category/Mathematics)
- [**物理化学** Phy-chem](https://kexue.fm/category/Phy-chem)
- [**信息时代** Big-Data](https://kexue.fm/category/Big-Data)
- [**生物自然** Biology](https://kexue.fm/category/Biology)
- [**图片摄影** Photograph](https://kexue.fm/category/Photograph)
- [**问题百科** Questions](https://kexue.fm/category/Questions)
- [**生活/情感** Life-Feeling](https://kexue.fm/category/Life-Feeling)
- [**资源共享** Resources](https://kexue.fm/category/Resources)

- [**千奇百怪**](https://kexue.fm/category/Everything)
- [**天文探索**](https://kexue.fm/category/Astronomy)
- [**数学研究**](https://kexue.fm/category/Mathematics)
- [**物理化学**](https://kexue.fm/category/Phy-chem)
- [**信息时代**](https://kexue.fm/category/Big-Data)
- [**生物自然**](https://kexue.fm/category/Biology)
- [**图片摄影**](https://kexue.fm/category/Photograph)
- [**问题百科**](https://kexue.fm/category/Questions)
- [**生活/情感**](https://kexue.fm/category/Life-Feeling)
- [**资源共享**](https://kexue.fm/category/Resources)

[首页](https://kexue.fm/) [信息时代](https://kexue.fm/category/Big-Data) “让Keras更酷一些！”：层中层与mask

16Jul

# [“让Keras更酷一些！”：层中层与mask](https://kexue.fm/archives/6810)

By 苏剑林 \|
2019-07-16 \|
192953位读者 \|

这一篇 [“让Keras更酷一些！”](https://kexue.fm/search/%E8%AE%A9Keras%E6%9B%B4%E9%85%B7%E4%B8%80%E4%BA%9B/) 将和读者分享两部分内容：第一部分是“层中层”，顾名思义，是在Keras中自定义层的时候，重用已有的层，这将大大减少自定义层的代码量；另外一部分就是应读者所求，介绍一下序列模型中的mask原理和方法。

## 层中层 [\#](https://kexue.fm/archives/6810\#%E5%B1%82%E4%B8%AD%E5%B1%82)

在 [《“让Keras更酷一些！”：精巧的层与花式的回调》](https://kexue.fm/archives/5765) 一文中我们已经介绍过Keras自定义层的基本方法，其核心步骤是定义`build`和`call`两个函数，其中`build`负责创建可训练的权重，而`call`则定义具体的运算。

### 拒绝重复劳动 [\#](https://kexue.fm/archives/6810\#%E6%8B%92%E7%BB%9D%E9%87%8D%E5%A4%8D%E5%8A%B3%E5%8A%A8)

经常用到自定义层的读者可能会感觉到，在自定义层的时候我们经常在重复劳动，比如我们想要增加一个线性变换，那就要在`build`中增加一个`kernel`和`bias`变量（还要自定义变量的初始化、正则化等），然后在`call`里边用`K.dot`来执行，有时候还需要考虑维度对齐的问题，步骤比较繁琐。但事实上，一个线性变换其实就是一个不加激活函数的`Dense`层罢了，如果在自定义层时能重用已有的层，那显然就可以大大节省代码量了。

事实上，只要你对Python面向对象编程比较熟悉，然后仔细研究Keras的`Layer`的源代码，就不难发现重用已有层的方法了。下面将它整理成比较规范的流程，供读者参考调用。

**（注意：Keras 2.3.0开始就已经内置了层中层功能，不需要下面的自定义`OurLayer`了，直接就用`Layer`即可。）**

### OurLayer [\#](https://kexue.fm/archives/6810\#OurLayer)

首先，我们定义一个新的`OurLayer`类：

```python

```

这个`OurLayer`类继承了原来的`Layer`类，为它增加了`reuse`方法，就是通过它我们可以重用已有的层。

下面是一个简单的例子，定义一个层，运算如下：

$$y = g(f(xW\_1 + b\_1)W\_2 + b\_2)$$

这里$f,g$是激活函数，其实就是两个`Dense`层的复合，如果按照标准的写法，我们需要在`build`那里定义好几个权重，定义权重的时候还需要根据输入来定义shape，还要定义初始化等，步骤很多，但事实上这些在`Dense`层不都写好了吗，直接调用就可以了，参考调用代码如下：

```python

```

是不是特别清爽？

## Mask [\#](https://kexue.fm/archives/6810\#Mask)

这一节我们来讨论一下处理变长序列时的padding和mask问题。

### 证明你思考过 [\#](https://kexue.fm/archives/6810\#%E8%AF%81%E6%98%8E%E4%BD%A0%E6%80%9D%E8%80%83%E8%BF%87)

近来笔者开源的几个模型中大量地用到了mask，不少读者似乎以前从未遇到过这个东西，各种疑问纷至沓来。本来，对一样新东西有所疑问是无可厚非的事情，但问题是不经思考的提问就显得很不负责任了。我一直认为，在向别人提问的时候，需要同时去“证明”自己是思考过的，比如如果你要去解释关于mask的问题，我会先请你回答：

> mask之前的序列大概是怎样的？mask之后序列的哪些位置发生了变化？变成了怎么样？

这三个问题跟mask的原理没有关系，只是要你看懂mask做了什么运算，在此基础上，我们才能去讨论为什么要这样运算。如果你连运算本身都看不懂，那只有两条路可选了，一是放弃这个问题的理解，二是好好学几个月Keras咱们再来讨论。

下面假设读者已经看懂了mask的运算，然后我们来简单讨论一下mask的基本原理。

### 排除padding [\#](https://kexue.fm/archives/6810\#%E6%8E%92%E9%99%A4padding)

mask是伴随这padding出现的，因为神经网络的输入需要一个规整的张量，而文本通常都是不定长的，这样一来就需要裁剪或者填充的方式来使得它们变成定长，按照常规习惯，我们会使用0作为padding符号。

这里用简单的向量来描述padding的原理。假设有一个长度为5的向量：

$$x = \[1, 0, 3, 4, 5\]$$

经过padding变成长度为8：

$$x = \[1, 0, 3, 4, 5, 0, 0, 0\]$$

当你将这个长度为8的向量输入到模型中时，模型并不知道你这个向量究竟是“长度为8的向量”还是“长度为5的向量，填充了3个无意义的0”。为了表示出哪些是有意义的，哪些是padding的，我们还需要一个mask向量（矩阵）：

$$m = \[1, 1, 1, 1, 1, 0, 0, 0\]$$

这是一个0/1向量（矩阵），用1表示有意义的部分，用0表示无意义的padding部分。

所谓mask，就是$x$和$m$的运算，来排除padding带来的效应。比如我们要求$x$的均值，本来期望的结果是：

$$\\text{avg}(x) = \\frac{1 + 0 + 3 + 4 + 5}{5} = 2.6$$

但是由于向量已经经过padding，直接算的话就得到：

$$\\frac{1 + 0 + 3 + 4 + 5 + 0 + 0 + 0}{8} = 1.625$$

会带来偏差。更严重的是，对于同一个输入，每次padding的零的数目可能是不固定的，因此同一个样本每次可能得到不同的均值，这是很不合理的。有了mask向量$m$之后，我们可以重写求均值的运算：

$$\\text{avg}(x) = \\frac{\\text{sum}(x\\otimes m)}{\\text{sum}(m)}$$

这里的$\\otimes$是逐位对应相乘的意思。这样一来，分子只对非padding部分求和，分母则是对非padding部分计数，不管你padding多少个零，最终算出来的结果都是一样的。

如果要求$x$的最大值呢？我们有$\\max(\[1, 0, 3, 4, 5\]) = \\max(\[1, 0, 3, 4, 5, 0, 0, 0\]) = 5$，似乎不用排除padding效应了？在这个例子中是这样，但还有可能是：

$$x = \[-1, -2, -3, -4, -5\]$$

经过padding后变成了

$$x = \[-1, -2, -3, -4, -5, 0, 0, 0\]$$

如果直接对padding后的$x$求$\\max$，那么得到的是0，而0不在原来的范围内。这时候解决的方法是：让padding部分足够小，以至于$\\max$（几乎）不能取到padding部分，比如

$$\\max(x) = \\max\\left(x - (1 - m) \\times 10^{10}\\right)$$

正常来说，神经网络的输入输出的数量级不会很大，所以经过$x - (1 - m) \\times 10^{10}$后，padding部分在$-10^{10}$这个数量级中上，可以保证取$\\max$的话不会取到padding部分了。

处理softmax的padding也是如此。在Attention或者指针网络时，我们就有可能遇到对变长的向量做softmax，如果直接对padding后的向量做softmax，那么padding部分也会平摊一部分概率，导致实际有意义的部分概率之和都不等于1了。解决办法跟$\\max$时一样，让padding部分足够小足够小，使得$e^x$足够接近于0，以至于可以忽略：

$$\\text{sofmax}(x) = \\text{softmax}\\left(x - (1 - m) \\times 10^{10}\\right)$$

上面几个算子的mask处理算是比较特殊的，其余运算的mask处理（除了双向RNN），基本上只需要输出

$$x\\otimes m$$

就行了，也就是让padding部分保持为0。

### Keras实现要点 [\#](https://kexue.fm/archives/6810\#Keras%E5%AE%9E%E7%8E%B0%E8%A6%81%E7%82%B9)

Keras自带了mask功能，但是不建议用，因为自带的mask不够清晰灵活，而且也不支持所有的层，强烈建议读者自己实现mask。

近来开源的好几个模型都已经给出了足够多的mask案例，我相信读者只要认真去阅读源码，一定很容易理解mask的实现方式的，这里简单提一下几个要点。一般来说NLP模型的输入是词ID矩阵，形状为$\\text{\[batch\_size, seq\_len\]}$，其中我会用0作为padding的ID，而1作为UNK的ID，剩下的就随意了，然后我就用一个`Lambda`层生成mask矩阵：

```python

```

这样生成的mask矩阵大小是$\\text{\[batch\_size, seq\_len, 1\]}$，然后词ID矩阵经过`Embedding`层后的大小为$\\text{\[batch\_size, seq\_len, word\_size\]}$，这样一来就可以用mask矩阵对输出结果就行处理了。这种写法只是我的习惯，并非就是唯一的标准。

## 结合：双向RNN [\#](https://kexue.fm/archives/6810\#%E7%BB%93%E5%90%88%EF%BC%9A%E5%8F%8C%E5%90%91RNN)

刚才我们的讨论排除了双向RNN，这是因为RNN是递归模型，没办法简单地mask（主要是逆向RNN这部分）。所谓双向RNN，就是正反各做一次RNN然后拼接或者相加之类的。假如我们要对$\[1, 0, 3, 4, 5, 0, 0, 0\]$做逆向RNN运算时，最后输出的结果都会包含padding部分的0（因为padding部分在一开始就参与了运算）。因此事后是没法排除的，只有在事前排除。

排除的方案是：要做逆向RNN，先将$\[1, 0, 3, 4, 5, 0, 0, 0\]$反转为$\[5, 4, 3, 0, 1, 0, 0, 0\]$，然后做一个正向RNN，然后再把结果反转回去，要注意反转的时候只反转非padding部分（这样才能保证递归运算时padding部分始终不参与，并且保证跟正向RNN的结果对齐），这个tensorflow提供了现成的函数`tf.reverse_sequence()`。

遗憾的是，Keras自带的`Bidirectional`并没有这个功能，所以我重写了它，供读者参考：

```python

```

使用方法跟自带的`Bidirectional`基本一样的，只不过要多传入mask矩阵，比如：

```python

```

## 小结 [\#](https://kexue.fm/archives/6810\#%E5%B0%8F%E7%BB%93)

Keras是一个极其友好、极其灵活的高层深度学习API封装，千万不要听信网上流传的“Keras对新手很友好，但是欠缺灵活性”的谣言～Keras对新手很友好，对老手更友好，对需要频繁自定义模块的用户更更友好。

_**转载到请包括本文地址：** [https://kexue.fm/archives/6810](https://kexue.fm/archives/6810 "“让Keras更酷一些！”：层中层与mask")_

_**更详细的转载事宜请参考：**_ [《科学空间FAQ》](https://kexue.fm/archives/6508#%E6%96%87%E7%AB%A0%E5%A6%82%E4%BD%95%E8%BD%AC%E8%BD%BD/%E5%BC%95%E7%94%A8 "《科学空间FAQ》")

**如果您还有什么疑惑或建议，欢迎在下方评论区继续讨论。**

**如果您觉得本文还不错，欢迎 [分享](https://kexue.fm/archives/6810#share)/ [打赏](https://kexue.fm/archives/6810#pay) 本文。打赏并非要从中获得收益，而是希望知道科学空间获得了多少读者的真心关注。当然，如果你无视它，也不会影响你的阅读。再次表示欢迎和感谢！**

打赏

![科学空间](https://kexue.fm/usr/themes/geekg/payment/wx.png)

微信打赏

![科学空间](https://kexue.fm/usr/themes/geekg/payment/zfb.png)

支付宝打赏

因为网站后台对打赏并无记录，因此欢迎在打赏时候备注留言。

你还可以 [**点击这里**](http://mail.qq.com/cgi-bin/qm_share?t=qm_mailme&email=tN7d1drY3drrx8H0xcWa19vZ) 或在下方评论区留言来告知你的建议或需求。

**如果您需要引用本文，请参考：**

苏剑林. (Jul. 16, 2019). 《“让Keras更酷一些！”：层中层与mask 》\[Blog post\]. Retrieved from [https://kexue.fm/archives/6810](https://kexue.fm/archives/6810)

@online{kexuefm-6810,

         title={“让Keras更酷一些！”：层中层与mask},

         author={苏剑林},

         year={2019},

         month={Jul},

         url={\\url{https://kexue.fm/archives/6810}},

}


分类： [信息时代](https://kexue.fm/category/Big-Data)    标签： [keras](https://kexue.fm/tag/keras/)[49 评论](https://kexue.fm/archives/6810#comments)

< [用时间换取效果：Keras梯度累积优化器](https://kexue.fm/archives/6794 "用时间换取效果：Keras梯度累积优化器") \| [思考：两个椭圆片能粘合成一个立体吗？](https://kexue.fm/archives/6818 "思考：两个椭圆片能粘合成一个立体吗？") >

### 你也许还对下面的内容感兴趣

- [Seq2Seq+前缀树：检索任务新范式（以KgCLUE为例）](https://kexue.fm/archives/8802 "Seq2Seq+前缀树：检索任务新范式（以KgCLUE为例）")
- [bert4keras在手，baseline我有：CLUE基准代码](https://kexue.fm/archives/8739 "bert4keras在手，baseline我有：CLUE基准代码")
- [节省显存的重计算技巧也有了Keras版了](https://kexue.fm/archives/7367 "节省显存的重计算技巧也有了Keras版了")
- [bert4keras在手，baseline我有：百度LIC2020](https://kexue.fm/archives/7321 "bert4keras在手，baseline我有：百度LIC2020")
- [AdaFactor优化器浅析（附开源实现）](https://kexue.fm/archives/7302 "AdaFactor优化器浅析（附开源实现）")
- [对抗训练浅谈：意义、方法和思考（附Keras实现）](https://kexue.fm/archives/7234 "对抗训练浅谈：意义、方法和思考（附Keras实现）")
- [6个派生优化器的简单介绍及其实现](https://kexue.fm/archives/7094 "6个派生优化器的简单介绍及其实现")
- [Keras：Tensorflow的黄金标准](https://kexue.fm/archives/7055 "Keras：Tensorflow的黄金标准")
- [“让Keras更酷一些！”：层与模型的重用技巧](https://kexue.fm/archives/6985 "“让Keras更酷一些！”：层与模型的重用技巧")
- [自己实现了一个bert4keras](https://kexue.fm/archives/6915 "自己实现了一个bert4keras")

[发表你的看法](https://kexue.fm/archives/6810#comment_form)

1. [«](https://kexue.fm/archives/6810/comment-page-1#comments)
2. [1](https://kexue.fm/archives/6810/comment-page-1#comments)
3. [2](https://kexue.fm/archives/6810/comment-page-2#comments)

rickxin

November 18th, 2019

我想请教下，如果双向rnn 实现用concat 的方式 不按楼主的实现方式是不是影响不是很大，楼主有尝尝过嘛？

[回复评论](https://kexue.fm/archives/6810/comment-page-2?replyTo=12397#respond-post-6810)

[苏剑林](https://kexue.fm/) 发表于
November 18th, 2019

按道理是有影响的，至于大不大还真没法评估，但是keras自带的写法已经被广泛使用过了，并没出现大问题，估计影响不大。不过之前也有群友反馈说用了我这个对齐版本会有轻微提升。

[回复评论](https://kexue.fm/archives/6810/comment-page-2?replyTo=12399#respond-post-6810)

rickxin 发表于
November 18th, 2019

对，我感觉要是做了mask 然后concat的时候不对应,应该是会有不少影响的，但是我看了下keras Bidirectional的实现 然后尝试了下 ，他们的实现是对的

[回复评论](https://kexue.fm/archives/6810/comment-page-2?replyTo=12400#respond-post-6810)

[苏剑林](https://kexue.fm/) 发表于
November 18th, 2019

“对的”是什么意思？自带的实现是没有对齐的。

[回复评论](https://kexue.fm/archives/6810/comment-page-2?replyTo=12401#respond-post-6810)

rickxin 发表于
November 18th, 2019

我看是对齐了的，我看是先做了 go\_backwards, rnn 里面设置了go\_backwards 后就会将mask 分开reverse ，然后再拼一起reverse 就对齐了，https://github.com/keras-team/keras/blob/master/keras/layers/wrappers.py#L335 可能咱来看的版本不一样

[回复评论](https://kexue.fm/archives/6810/comment-page-2?replyTo=12404#respond-post-6810)

[苏剑林](https://kexue.fm/) 发表于
November 19th, 2019

不知道你在哪里可以看到对齐，还是你根本不明白对齐是什么意思？

下面链接中的rnn函数，纯粹是用input\_t.reverse()来翻转序列。假如原始是\[1,2,3,0,0\]，反转之后变成了\[0,0,3,2,1\]，那么lstm就是从0开始而不是从3开始了。

https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/keras/backend.py

[回复评论](https://kexue.fm/archives/6810/comment-page-2?replyTo=12413#respond-post-6810)

yyb 发表于
December 12th, 2019

话说根据https://github.com/keras-team/keras/blob/master/keras/layers/wrappers.py的544行的

if self.return\_sequences:

y\_rev = K.reverse(y\_rev, 1)

if self.merge\_mode == 'concat':

output = K.concatenate(\[y, y\_rev\])

最后输出时把反向的LSTM的输出又reverse了一遍，所以最终二者应该是对齐的

[苏剑林](https://kexue.fm/)

December 12th, 2019

[@yyb\|comment-12603](https://kexue.fm/archives/6810/comment-page-2#comment-12603)

看来这位读者也是没有明白对齐的含义了。没有对齐并不是说它没有reverse再reverse，没有对齐指的是它把padding部分也加入了reverse范围。

请认真思考前面给出的例子：假如原始是\[1,2,3,0,0\]，反转之后变成了\[0,0,3,2,1\]，那么rnn就是从0开始而不是从3开始了。也就是说，padding的0是没有含义的，我需要一个1-->2-->3的正向rnn以及一个3-->2-->1的反向rnn，但现在它给了我一个0-->0-->3-->2-->1的反向rnn给我，这不是我要的，我要的是反向之后变成\[3, 2, 1, 0, 0\]，而不是\[0, 0, 3, 2, 1\]。

[回复评论](https://kexue.fm/archives/6810/comment-page-2?replyTo=12611#respond-post-6810)

yyb 发表于
December 13th, 2019

谢谢大神！明白了。rnn开始传入的信息是随机的，我觉得可能只要最后mask部分没有进行学习，00321和32100影响应该是不大的吧。。

[回复评论](https://kexue.fm/archives/6810/comment-page-2?replyTo=12615#respond-post-6810)

[苏剑林](https://kexue.fm/) 发表于
December 13th, 2019

大不大谁知道呢？而且我可能不止pad了两个0，我可能pad了100个0呢？总要把干扰切切实实地排除掉，不能寄望于“应该不大”。

[回复评论](https://kexue.fm/archives/6810/comment-page-2?replyTo=12616#respond-post-6810)

yyb 发表于
December 13th, 2019

是的，另外更大的意义是把cudnnLSTM加入mask支持了。我想借个楼问一下，我用keras训练的时候，经常是在前一个eporch结束用validation数据测试后，下一个eporch开始准确率就迅速提高了，如果没有过拟合还没有问题，但是有时会出现过拟合，不知道为什么保存的模型的准确率是迅速下降后的，cross验证的时候就很麻烦，不知道怎么处理

[回复评论](https://kexue.fm/archives/6810/comment-page-2?replyTo=12617#respond-post-6810)

[s](http://sa/)

April 30th, 2020

（注意：Keras 2.3.0开始就已经内置了层中层功能，不需要下面的自定义OurLayer了，直接就用Layer即可。）

调用self.reuse对应的方法是？a

[回复评论](https://kexue.fm/archives/6810/comment-page-2?replyTo=13242#respond-post-6810)

[苏剑林](https://kexue.fm/) 发表于
April 30th, 2020

keras 2.3.x不需要reuse，直接使用层中层即可。

[回复评论](https://kexue.fm/archives/6810/comment-page-2?replyTo=13243#respond-post-6810)

[s](http://sa/)

April 30th, 2020

苏神2.3层中层可以这样吗？

class OurDense(tf.keras.layers.Layer):

"""原来是继承Layer类，现在继承OurLayer类

"""

def \_\_init\_\_(self, hidden\_dim, output\_dim,

hidden\_activation='linear',

output\_activation='linear', \*\*kwargs):

super(OurDense, self).\_\_init\_\_(\*\*kwargs)

self.hidden\_dim = hidden\_dim

self.output\_dim = output\_dim

self.hidden\_activation = hidden\_activation

self.output\_activation = output\_activation

def build(self, input\_shape):

"""在build方法里边添加需要重用的层，

当然也可以像标准写法一样条件可训练的权重。

"""

super(OurDense, self).build(input\_shape)

self.h\_dense = tf.keras.layers.Dense(self.hidden\_dim,

activation=self.hidden\_activation)

self.o\_dense = tf.keras.layers.Dense(self.output\_dim,

activation=self.output\_activation)

\# def call(self, inputs, \*\*kwargs):

\# """直接reuse一下层，等价于o\_dense(h\_dense(inputs))

\# :param \*\*kwargs:

\# """

\# h = self.reuse(self.h\_dense, inputs)

\# o = self.reuse(self.o\_dense, h)

\# return o

def compute\_output\_shape(self, input\_shape):

return input\_shape\[:-1\] + (self.output\_dim,)

[回复评论](https://kexue.fm/archives/6810/comment-page-2?replyTo=13244#respond-post-6810)

[苏剑林](https://kexue.fm/) 发表于
April 30th, 2020

build了还要call呀

def call(self, inputs):

return self.h\_dense(inputs)

类似这样。

[回复评论](https://kexue.fm/archives/6810/comment-page-2?replyTo=13246#respond-post-6810)

[s](http://sa/) 发表于
April 30th, 2020

THANKS

[回复评论](https://kexue.fm/archives/6810/comment-page-2?replyTo=13247#respond-post-6810)

平常心

July 6th, 2020

就RNN例子来说,keras里面输入的时候不指定input\_length,Embedding层使用mask\_zero=True,也能达到动态变长输入的效果.

[回复评论](https://kexue.fm/archives/6810/comment-page-2?replyTo=13749#respond-post-6810)

人生艰难

March 15th, 2022

苏神您好，假如我做token级的序列标注任务，模型结构为：bert+linear，那请问\[cls\]和\[spe\]需要mask么？

我的理解是同样需要mask

[回复评论](https://kexue.fm/archives/6810/comment-page-2?replyTo=18704#respond-post-6810)

[苏剑林](https://kexue.fm/) 发表于
March 18th, 2022

loss那里mask掉就行

[回复评论](https://kexue.fm/archives/6810/comment-page-2?replyTo=18718#respond-post-6810)

1. [«](https://kexue.fm/archives/6810/comment-page-1#comments)
2. [1](https://kexue.fm/archives/6810/comment-page-1#comments)
3. [2](https://kexue.fm/archives/6810/comment-page-2#comments)

[取消回复](https://kexue.fm/archives/6810#respond-post-6810)

你的大名

电子邮箱

个人网站（选填）

1\. 可以使用LaTeX代码，点击“预览效果”可查看效果；

2\. 可以通过点击评论楼层编号来引用该楼层；

3\. 网站可能会有点卡，如非确认评论失败，请 **不要重复点击提交**。

### 内容速览

[层中层](https://kexue.fm/archives/6810#%E5%B1%82%E4%B8%AD%E5%B1%82)
[拒绝重复劳动](https://kexue.fm/archives/6810#%E6%8B%92%E7%BB%9D%E9%87%8D%E5%A4%8D%E5%8A%B3%E5%8A%A8)
[OurLayer](https://kexue.fm/archives/6810#OurLayer)
[Mask](https://kexue.fm/archives/6810#Mask)
[证明你思考过](https://kexue.fm/archives/6810#%E8%AF%81%E6%98%8E%E4%BD%A0%E6%80%9D%E8%80%83%E8%BF%87)
[排除padding](https://kexue.fm/archives/6810#%E6%8E%92%E9%99%A4padding)
[Keras实现要点](https://kexue.fm/archives/6810#Keras%E5%AE%9E%E7%8E%B0%E8%A6%81%E7%82%B9)
[结合：双向RNN](https://kexue.fm/archives/6810#%E7%BB%93%E5%90%88%EF%BC%9A%E5%8F%8C%E5%90%91RNN)
[小结](https://kexue.fm/archives/6810#%E5%B0%8F%E7%BB%93)

### 智能搜索

支持整句搜索！网站自动使用 [结巴分词](https://github.com/fxsjy/jieba) 进行分词，并结合ngrams排序算法给出合理的搜索结果。

### 热门标签

[生成模型](https://kexue.fm/tag/%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B/) [attention](https://kexue.fm/tag/attention/) [优化](https://kexue.fm/tag/%E4%BC%98%E5%8C%96/) [语言模型](https://kexue.fm/tag/%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/) [模型](https://kexue.fm/tag/%E6%A8%A1%E5%9E%8B/) [梯度](https://kexue.fm/tag/%E6%A2%AF%E5%BA%A6/) [网站](https://kexue.fm/tag/%E7%BD%91%E7%AB%99/) [概率](https://kexue.fm/tag/%E6%A6%82%E7%8E%87/) [矩阵](https://kexue.fm/tag/%E7%9F%A9%E9%98%B5/) [优化器](https://kexue.fm/tag/%E4%BC%98%E5%8C%96%E5%99%A8/) [转载](https://kexue.fm/tag/%E8%BD%AC%E8%BD%BD/) [微分方程](https://kexue.fm/tag/%E5%BE%AE%E5%88%86%E6%96%B9%E7%A8%8B/) [分析](https://kexue.fm/tag/%E5%88%86%E6%9E%90/) [天象](https://kexue.fm/tag/%E5%A4%A9%E8%B1%A1/) [深度学习](https://kexue.fm/tag/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/) [积分](https://kexue.fm/tag/%E7%A7%AF%E5%88%86/) [python](https://kexue.fm/tag/python/) [扩散](https://kexue.fm/tag/%E6%89%A9%E6%95%A3/) [力学](https://kexue.fm/tag/%E5%8A%9B%E5%AD%A6/) [无监督](https://kexue.fm/tag/%E6%97%A0%E7%9B%91%E7%9D%A3/) [几何](https://kexue.fm/tag/%E5%87%A0%E4%BD%95/) [节日](https://kexue.fm/tag/%E8%8A%82%E6%97%A5/) [生活](https://kexue.fm/tag/%E7%94%9F%E6%B4%BB/) [文本生成](https://kexue.fm/tag/%E6%96%87%E6%9C%AC%E7%94%9F%E6%88%90/) [数论](https://kexue.fm/tag/%E6%95%B0%E8%AE%BA/)

### 随机文章

- [【NASA每日一图】水星上的双环陨石坑](https://kexue.fm/archives/168)
- [费曼路径积分思想的发展(三)](https://kexue.fm/archives/1849)
- [你跳绳的时候，想过绳子的形状曲线是怎样的吗？](https://kexue.fm/archives/6784)
- [评论功能修复了](https://kexue.fm/archives/1884)
- [哥德巴赫猜想浅谈1](https://kexue.fm/archives/1727)
- [Transformer升级之路：21、MLA好在哪里?（下）](https://kexue.fm/archives/11111)
- [行列式的导数](https://kexue.fm/archives/2383)
- [智能家居之热水器零冷水技术原理浅析](https://kexue.fm/archives/9405)
- [钱老，一路走好！](https://kexue.fm/archives/238)
- [【生活杂记】用电饭锅来煮米汤](https://kexue.fm/archives/10240)

### 最近评论

- [Bin](https://kexue.fm/archives/1990/comment-page-2#comment-29105): 今天偶然从某个论坛看到有人推荐您的博客，定睛一看竟然是华师同院的往届师兄！看到这篇2013年的...
- [Rapture D](https://kexue.fm/archives/11530/comment-page-1#comment-29104): 我有一个问题，为什么不考虑亥姆霍兹定理和斯托克斯公式。
- [mofheka](https://kexue.fm/archives/11390/comment-page-1#comment-29103): 苏神是还在用jax是么？最近在做基于Google Pathway的理念做一个动态版的MPMD框...
- [长琴](https://kexue.fm/archives/11530/comment-page-1#comment-29102): 看懂这篇博客也不是一件容易的事情。
- [AlexLi](https://kexue.fm/archives/9257/comment-page-4#comment-29101): 苏老师，请教一下(7)式中将 $\\mu(x\_t)$ 传给 $p\_o$ 进行推理的操作。 $x\_...
- [tyler\_zxc](https://kexue.fm/archives/7921/comment-page-2#comment-29100): "Performer的思想是将标准的Attention线性化，所以为什么不干脆直接训练一个线性...
- [我](https://kexue.fm/archives/11494/comment-page-1#comment-29099): 似乎并非mHC提出矩阵的思想？之前hyper connection就是了
- [winter](https://kexue.fm/archives/10847/comment-page-1#comment-29098): 苏神您好，假如对于比较均匀的attention weightP，往往呈现long tail分布...
- [苏剑林](https://kexue.fm/archives/8512/comment-page-2#comment-29097): KL散度、JS散度、W距离啥的，都行啊，看你喜欢哪个
- [苏剑林](https://kexue.fm/archives/9119/comment-page-14#comment-29096): 没有绝对公平的对比方法，主要看你关心什么。比如，如果只关心推理成本和推理效果，那么有的方法可以...

### 友情链接

- [Cool Papers](https://papers.cool/)
- [数学研发](https://bbs.emath.ac.cn/)
- [Seatop](http://www.seatop.com.cn/)
- [Xiaoxia](https://xiaoxia.org/)
- [积分表-网络版](https://kexue.fm/sci/integral/index.html)
- [丝路博傲](http://blog.dvxj.com/)
- [数学之家](http://www.2math.cn/)
- [有趣天文奇观](http://interesting-sky.china-vo.org/)
- [TwistedW](http://www.twistedwg.com/)
- [godweiyang](https://godweiyang.com/)
- [AI柠檬](https://blog.ailemon.net/)
- [王登科-DK博客](https://greatdk.com/)
- [ESON](https://blog.eson.org/)
- [枫之羽](https://fzhiy.net/)
- [coding-zuo](https://coding-zuo.github.io/)
- [博科园](https://www.bokeyuan.net/)
- [孔皮皮的博客](https://www.kppkkp.top/)
- [运鹏的博客](https://yunpengtai.top/)
- [jiming.site](https://jiming.site/)
- [OmegaXYZ](https://www.omegaxyz.com/)
- [EAI猩球](https://www.robotech.ink/)
- [文举的博客](https://liwenju0.com/)
- [申请链接](https://kexue.fm/links.html)

[![署名-非商业用途-保持一致](https://kexue.fm/usr/themes/geekg/images/cc.gif)](http://creativecommons.org/licenses/by-nc-nd/2.5/cn/) 本站采用创作共用版权协议，要求署名、非商业用途和保持一致。转载本站内容必须也遵循“ [署名-非商业用途-保持一致](http://creativecommons.org/licenses/by-nc-nd/2.5/cn/)”的创作共用协议。



© 2009-2026 Scientific Spaces. All rights reserved. Theme by [laogui](http://www.laogui.com/). Powered by [Typecho](http://typecho.org/). 备案号: [粤ICP备09093259号-1/2](https://beian.miit.gov.cn/ "粤ICP备09093259号")。