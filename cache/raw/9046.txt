你的语言模型有没有“无法预测的词”？ - 科学空间|Scientific Spaces
![MobileSideBar](https://kexue.fm/usr/themes/geekg/images/slide-button.png "MobileSideBar")
## SEARCH
## MENU
* [打赏](https://kexue.fm/reward.html)
* [公式](https://kexue.fm/latex.html)
* [天象](https://kexue.fm/ac.html)
* [链接](https://kexue.fm/links.html)
* [时光](https://kexue.fm/me.html)
* [博览](https://kexue.fm/science.html)
* [归档](https://kexue.fm/content.html)
## CATEGORIES
* [千奇百怪](https://kexue.fm/category/Everything)
* [天文探索](https://kexue.fm/category/Astronomy)
* [数学研究](https://kexue.fm/category/Mathematics)
* [物理化学](https://kexue.fm/category/Phy-chem)
* [信息时代](https://kexue.fm/category/Big-Data)
* [生物自然](https://kexue.fm/category/Biology)
* [图片摄影](https://kexue.fm/category/Photograph)
* [问题百科](https://kexue.fm/category/Questions)
* [生活/情感](https://kexue.fm/category/Life-Feeling)
* [资源共享](https://kexue.fm/category/Resources)
## NEWPOSTS
* [让炼丹更科学一些（五）：基于梯度精...](https://kexue.fm/archives/11530)
* [让炼丹更科学一些（四）：新恒等式，...](https://kexue.fm/archives/11494)
* [为什么DeltaNet要加L2 N...](https://kexue.fm/archives/11486)
* [让炼丹更科学一些（三）：SGD的终...](https://kexue.fm/archives/11480)
* [让炼丹更科学一些（二）：将结论推广...](https://kexue.fm/archives/11469)
* [滑动平均视角下的权重衰减和学习率](https://kexue.fm/archives/11459)
* [生成扩散模型漫谈（三十一）：预测数...](https://kexue.fm/archives/11428)
* [Muon优化器指南：快速上手与关键细节](https://kexue.fm/archives/11416)
* [AdamW的Weight RMS的...](https://kexue.fm/archives/11404)
* [n个正态随机数的最大值的渐近估计](https://kexue.fm/archives/11390)
## COMMENTS
* [Rapture D: 我有一个问题，为什么不考虑亥姆霍兹定理和斯托克斯公式。](https://kexue.fm/archives/11530/comment-page-1#comment-29104)
* [mofheka: 苏神是还在用jax是么？最近在做基于Google Pathwa...](https://kexue.fm/archives/11390/comment-page-1#comment-29103)
* [长琴: 看懂这篇博客也不是一件容易的事情。](https://kexue.fm/archives/11530/comment-page-1#comment-29102)
* [AlexLi: 苏老师，请教一下(7)式中将 $\\mu(x\_t)$ 传给$p...](https://kexue.fm/archives/9257/comment-page-4#comment-29101)
* [tyler\_zxc: "Performer的思想是将标准的Attention线性化，...](https://kexue.fm/archives/7921/comment-page-2#comment-29100)
* [我: 似乎并非mHC提出矩阵的思想？之前hyper connecti...](https://kexue.fm/archives/11494/comment-page-1#comment-29099)
* [winter: 苏神您好，假如对于比较均匀的attention weightP...](https://kexue.fm/archives/10847/comment-page-1#comment-29098)
* [苏剑林: KL散度、JS散度、W距离啥的，都行啊，看你喜欢哪个](https://kexue.fm/archives/8512/comment-page-2#comment-29097)
* [苏剑林: 没有绝对公平的对比方法，主要看你关心什么。比如，如果只关心推理...](https://kexue.fm/archives/9119/comment-page-14#comment-29096)
* [苏剑林: 如果我有时间重新搭建博客，应该会用python自己写了，而不用...](https://kexue.fm/links.html/comment-page-6#comment-29095)
## USERLOGIN
* [登录](https://kexue.fm/admin/login.php)
[科学空间|Scientific Spaces](https://kexue.fm)
* [登录](https://kexue.fm/admin/login.php)
* [打赏](https://kexue.fm/reward.html)
* [公式](https://kexue.fm/latex.html)
* [天象](https://kexue.fm/ac.html)
* [链接](https://kexue.fm/links.html)
* [时光](https://kexue.fm/me.html)
* [博览](https://kexue.fm/science.html)
* [归档](https://kexue.fm/content.html)
渴望成为一个小飞侠* [![](https://kexue.fm/usr/themes/geekg/images/rss.png)
欢迎订阅](https://kexue.fm/feed)
* [![](https://kexue.fm/usr/themes/geekg/images/mail.png)
个性邮箱](https://kexue.fm/archives/119)
* [![](https://kexue.fm/usr/themes/geekg/images/Saturn.png)
天象信息](https://kexue.fm/ac.html)
* [![](https://kexue.fm/usr/themes/geekg/images/iss.png)
观测ISS](https://kexue.fm/archives/41)
* [![](https://kexue.fm/usr/themes/geekg/images/pi.png)
LaTeX](https://kexue.fm/latex.html)
* [![](https://kexue.fm/usr/themes/geekg/images/mlogo.png)
关于博主](https://kexue.fm/me.html)
欢迎访问“科学空间”，这里将与您共同探讨自然科学，回味人生百态；也期待大家的分享～* [**千奇百怪**Everything](https://kexue.fm/category/Everything)
* [**天文探索**Astronomy](https://kexue.fm/category/Astronomy)
* [**数学研究**Mathematics](https://kexue.fm/category/Mathematics)
* [**物理化学**Phy-chem](https://kexue.fm/category/Phy-chem)
* [**信息时代**Big-Data](https://kexue.fm/category/Big-Data)
* [**生物自然**Biology](https://kexue.fm/category/Biology)
* [**图片摄影**Photograph](https://kexue.fm/category/Photograph)
* [**问题百科**Questions](https://kexue.fm/category/Questions)
* [**生活/情感**Life-Feeling](https://kexue.fm/category/Life-Feeling)
* [**资源共享**Resources](https://kexue.fm/category/Resources)
* [**千奇百怪**](https://kexue.fm/category/Everything)
* [**天文探索**](https://kexue.fm/category/Astronomy)
* [**数学研究**](https://kexue.fm/category/Mathematics)
* [**物理化学**](https://kexue.fm/category/Phy-chem)
* [**信息时代**](https://kexue.fm/category/Big-Data)
* [**生物自然**](https://kexue.fm/category/Biology)
* [**图片摄影**](https://kexue.fm/category/Photograph)
* [**问题百科**](https://kexue.fm/category/Questions)
* [**生活/情感**](https://kexue.fm/category/Life-Feeling)
* [**资源共享**](https://kexue.fm/category/Resources)
[首页](https://kexue.fm)[信息时代](https://kexue.fm/category/Big-Data)你的语言模型有没有“无法预测的词”？
20Apr
# [你的语言模型有没有“无法预测的词”？](https://kexue.fm/archives/9046)
By苏剑林|2022-04-20|31149位读者|:
众所周知，分类模型通常都是先得到编码向量，然后接一个Dense层预测每个类别的概率，而预测时则是输出概率最大的类别。但大家是否想过这样一种可能：训练好的分类模型可能存在“无法预测的类别”，即不管输入是什么，都不可能预测出某个类别$k$，类别$k$永远不可能成为概率最大的那个。
当然，这种情况一般只出现在类别数远远超过编码向量维度的场景，常规的分类问题很少这么极端的。然而，我们知道语言模型本质上也是一个分类模型，它的类别数也就是词表的总大小，往往是远超过向量维度的，那么我们的语言模型是否有“无法预测的词”？（只考虑Greedy解码）
## 是否存在[#](#是否存在)
ACL2022的论文[《Low-Rank Softmax Can Have Unargmaxable Classes in Theory but Rarely in Practice》](https://papers.cool/arxiv/2203.06462)首先探究了这个问题，正如其标题所言，答案是“理论上存在但实际出现概率很小”。
首先我们来看“理论上存在”。为了证明其存在性，我们只需要具体地构建一个例子。设各个类别向量分为$\\boldsymbol{w}\_1,\\boldsymbol{w}\_2,\\cdots,\\boldsymbol{w}\_n\\in\\mathbb{R}^d$，偏置项为$b\_1,b\_2,\\cdots,b\_n$，假设类别$k$是可预测的，那么就存在$\\boldsymbol{z}\\in\\mathbb{R}^d$，同时满足
\\begin{equation}\\langle\\boldsymbol{w}\_k,\\boldsymbol{z}\\rangle + b\_k \> \\langle\\boldsymbol{w}\_i,\\boldsymbol{z}\\rangle + b\_i\\quad (\\forall i \\neq k)\\end{equation}
反过来，如果类别$k$不可预测，那么对于任意$\\boldsymbol{z}\\in\\mathbb{R}^d$，必须存在某个$i\\neq k$，满足
\\begin{equation}\\langle\\boldsymbol{w}\_k,\\boldsymbol{z}\\rangle + b\_k \\leq \\langle\\boldsymbol{w}\_i,\\boldsymbol{z}\\rangle + b\_i\\end{equation}
由于现在我们只需要举例子，所以简单起见我们先考虑无偏置项的情况，并设$k=n$，此时条件为$\\langle \\boldsymbol{w}\_i - \\boldsymbol{w}\_n, \\boldsymbol{z}\\rangle \\geq 0$，也就是说，任意向量$\\boldsymbol{z}$必然能找到向量$\\boldsymbol{w}\_i - \\boldsymbol{w}\_n$与之夹角小于等于90度。不难想象，当向量数大于空间维度、向量均匀分布在空间中时，这是有可能出现的，比如二维平面上的任意向量，就必然与$(0,1),(1,0),(0,-1),(-1,0)$之一的夹角小于90度，从而我们可以构造出例子：
\\begin{equation}\\left\\{\\begin{aligned}
&\bolds&\boldsymbol{w}\_5 = (1, 1) \\quad(\\boldsymbol{w}\_5\\text{可以随便选})\\\\
&\bolds&\boldsymbol{w}\_1 = (1, 1) + (0, 1) = (1, 2)\\\\
&\bolds&\boldsymbol{w}\_2 = (1, 1) + (1, 0) = (2, 1)\\\\
&\bolds&\boldsymbol{w}\_3 = (1, 1) + (0, -1) = (1, 0)\\\\
&\bolds&\boldsymbol{w}\_4 = (1, 1) + (-1, 0) = (0, 1)\\\\
\\end{aligned}\\right.\\end{equation}
在这个例子中，类别5就是不可预测的了，不信大家可以代入一些$\\boldsymbol{z}$试试。
## 怎么判断[#](#怎么判断)
现在我们已经确认了“无法预测的类别”是可能存在的，那么一个很自然的问题就是，对于一个训练好的模型，也就是给定$\\boldsymbol{w}\_1,\\boldsymbol{w}\_2,\\cdots,\\boldsymbol{w}\_n\\in\\mathbb{R}^d$和$b\_1,b\_2,\\cdots,b\_n$，怎么判断其中是否存在不可预测的类别呢？
根据前一节的描述，从解不等式的角度来看，如果类别$k$是可预测的，那么下述不等式组的解集就会非空
\\begin{equation}\\langle\\boldsymbol{w}\_k - \\boldsymbol{w}\_i,\\boldsymbol{z}\\rangle + (b\_k - b\_i) \> 0\\quad (\\forall i \\neq k)\\end{equation}
不失一般性，我们同样设$k=n$，并且记$\\Delta\\boldsymbol{w}\_i = \\boldsymbol{w}\_n - \\boldsymbol{w}\_i, \\Delta b\_i = b\_n - b\_i$，留意到
\\begin{equation}\\langle\\Delta\\boldsymbol{w}\_i,\\boldsymbol{z}\\rangle + \\Delta b\_i \> 0\\,(i = 1,2,\\cdots,n-1)\\quad\\Leftrightarrow\\quad \\min\_i \\langle\\Delta\\boldsymbol{w}\_i,\\boldsymbol{z}\\rangle + \\Delta b\_i \> 0\\end{equation}
所以，只要我们尽量最大化$\\min\\limits\_i \\langle\\Delta\\boldsymbol{w}\_i,\\boldsymbol{z}\\rangle + \\Delta b\_i$，如果最终结果是正的，那么类别$n$就是可预测的，否则就是不可预测的。如果之前读过[《多任务学习漫谈（二）：行梯度之事》](https://kexue.fm/archives/8896)的读者，就会发现该问题“似曾相识”，特别是如果没有偏置项的情况下，它跟多任务学习中寻找“帕累托最优”的过程是几乎一致的。
现在问题变为\\begin{equation}\\max\_{\\boldsymbol{z}} \\min\_i \\langle\\Delta\\boldsymbol{w}\_i,\\boldsymbol{z}\\rangle + \\Delta b\_i\\end{equation}
为了避免发散到无穷，我们可以加个约束$\\Vert \\boldsymbol{z}\\Vert\\leq r$：
\\begin{equation}\\max\_{\\Vert \\boldsymbol{z}\\Vert\\leq r} \\min\_i \\langle\\Delta\\boldsymbol{w}\_i,\\boldsymbol{z}\\rangle + \\Delta b\_i \\end{equation}
其中$r$是一个常数，只要$r$取得足够大，它就能跟实际情况足够吻合，因为神经网络的输出通常来说也是有界的。接下来的过程就跟[《多任务学习漫谈（二）：行梯度之事》](https://kexue.fm/archives/8896)的几乎一样了，首先引入
\\begin{equation}\\mathbb{P}^{n-1} = \\left\\{(\\alpha\_1,\\alpha\_2,\\cdots,\\alpha\_{n-1})\\left|\\alpha\_1,\\alpha\_2,\\cdots,\\alpha\_{n-1}\\geq 0, \\sum\_i \\alpha\_i = 1\\right.\\right\\}\\end{equation}
那么问题变成\\begin{equation}\\max\_{\\Vert \\boldsymbol{z}\\Vert\\leq r} \\min\_{\\alpha\\in\\mathbb{P}^{n-1}} \\left\\langle\\sum\_i \\alpha\_i \\Delta\\boldsymbol{w}\_i,\\boldsymbol{z}\\right\\rangle + \\sum\_i \\alpha\_i \\Delta b\_i\\end{equation}
根据冯·诺依曼的[Minimax定理](https://en.wikipedia.org/wiki/Minimax_theorem)，可以交换$\\max$和$\\min$的顺序
\\begin{equation}\\min\_{\\alpha\\in\\mathbb{P}^{n-1}} \\max\_{\\Vert \\boldsymbol{z}\\Vert\\leq r}\\left\\langle\\sum\_i \\alpha\_i \\Delta\\boldsymbol{w}\_i,\\boldsymbol{z}\\right\\rangle + \\sum\_i \\alpha\_i \\Delta b\_i\\end{equation}
很显然，$\\max$这一步在$\\Vert\\boldsymbol{z}\\Vert=r$且$\\boldsymbol{z}$跟$\\sum\\limits\_i \\alpha\_i \\Delta\\boldsymbol{w}\_i$同向时取到，结果为
\\begin{equation}\\min\_{\\alpha\\in\\mathbb{P}^{n-1}} r\\left\\Vert\\sum\_i \\alpha\_i \\Delta\\boldsymbol{w}\_i\\right\\Vert + \\sum\_i \\alpha\_i \\Delta b\_i\\end{equation}
当$r$足够大时，偏置项的影响就非常小了，所以这几乎就等价于没有偏置项的情形
\\begin{equation}\\min\_{\\alpha\\in\\mathbb{P}^{n-1}} \\left\\Vert\\sum\_i \\alpha\_i \\Delta\\boldsymbol{w}\_i\\right\\Vert\\end{equation}
最后的$\\min$的求解过程已经在[《多任务学习漫谈（二）：行梯度之事》](https://kexue.fm/archives/8896)中讨论过了，主要用到了[Frank-Wolfe算法](https://en.wikipedia.org/wiki/Frank–Wolfe_algorithm)，不再重复。
**（注：以上判别过程是笔者自己给出的，跟论文[《Low-Rank Softmax Can Have Unargmaxable Classes in Theory but Rarely in Practice》](https://papers.cool/arxiv/2203.06462)中的方法并不相同。）**
## 实践如何[#](#实践如何)
前面的讨论都是理论上的，那么实际的语言模型出现“无法预测的词”的概率大不大呢？原论文对一些训练好的语言模型和生成模型进行了检验，发现实际上出现的概率很小，比如下表中的机器翻译模型检验结果：[![机器翻译模型的检验结果](https://kexue.fm/usr/uploads/2022/04/3980694051.png)](https://kexue.fm/usr/uploads/2022/04/3980694051.png)
机器翻译模型的检验结果其实这不难理解，从前面的讨论中我们知道“无法预测的词”一般只出现在类别数远远大于向量维度的情况，也就是原论文标题中的“Low-Rank”。但由于“维度灾难”的原因，“远远大于”这个概念其实并非我们直观所想的那样，比如对于2维空间来说，类别数为4就可以称得上“远远大于”，但如果是200维空间，那么即便是类别数为40000也算不上“远远大于”。常见的语言模型向量维度基本上都有几百维，而词表顶多也就是数十万的级别，因此其实还是算不上“远远大于”，因此出现“无法预测的词”的概率就很小了。
另外，我们还可以证明，如果所有的$\\boldsymbol{w}\_i$互不相同但是模长都相等，那么是绝对不会出现“无法预测的词”，因此这种不可预测的情况只出现在$\\boldsymbol{w}\_i$模长差异较大的情况，而在当前主流的深度模型中，由于各种Normalization技术的应用，$\\boldsymbol{w}\_i$模长差异较大的情况很少出现了，这进一步降低了“无法预测的词”的出现概率了。
当然，还是文章开头说了，本文的“无法预测的词”指的是最大化预测，也就是Greedy Search，如果用Beam Search或者随机采样，那么即便存在“无法预测的词”，也依然是可能生成出来的。这个“无法预测的词”，更多是一个好玩但实用价值不大的理论概念了，
## 最后小结[#](#最后小结)
本文向大家介绍了一个没什么实用价值但是颇为有意思的现象：你的语言模型可能存在一些“无法预测的词”，它永远不可能成为概率最大者。***转载到请包括本文地址：** [https://kexue.fm/archives/9046](https://kexue.fm/archives/9046)*
***更详细的转载事宜请参考：*** [《科学空间FAQ》](https://kexue.fm/archives/6508#文章如何转载/引用)
**如果您还有什么疑惑或建议，欢迎在下方评论区继续讨论。**
**如果您觉得本文还不错，欢迎[分享](#share)/[打赏](#pay)本文。打赏并非要从中获得收益，而是希望知道科学空间获得了多少读者的真心关注。当然，如果你无视它，也不会影响你的阅读。再次表示欢迎和感谢！**
打赏![科学空间](https://kexue.fm/usr/themes/geekg/payment/wx.png)
微信打赏![科学空间](https://kexue.fm/usr/themes/geekg/payment/zfb.png)
支付宝打赏因为网站后台对打赏并无记录，因此欢迎在打赏时候备注留言。 你还可以[**点击这里**](http://mail.qq.com/cgi-bin/qm_share?t=qm_mailme&email=tN7d1drY3drrx8H0xcWa19vZ)或在下方评论区留言来告知你的建议或需求。
**如果您需要引用本文，请参考：**
苏剑林. (Apr. 20, 2022). 《你的语言模型有没有“无法预测的词”？》[Blog post]. Retrieved from[https://kexue.fm/archives/9046](https://kexue.fm/archives/9046)
@online{kexuefm-9046,
title={你的语言模型有没有“无法预测的词”？},
author={苏剑林},
year={2022},
month={Apr},
url={\\url{https://kexue.fm/archives/9046}},
}
分类：[信息时代](https://kexue.fm/category/Big-Data) 标签：[语言模型](https://kexue.fm/tag/语言模型/),[多任务](https://kexue.fm/tag/多任务/)[4 评论](https://kexue.fm/archives/9046#comments)
&lt;[GlobalPointer下的“KL散度”应该是怎样的？](https://kexue.fm/archives/9039)|[GAU-α：尝鲜体验快好省的下一代Attention](https://kexue.fm/archives/9052)&gt;
### 你也许还对下面的内容感兴趣* [Transformer升级之路：21、MLA好在哪里?（下）](https://kexue.fm/archives/11111)
* [Transformer升级之路：20、MLA好在哪里?（上）](https://kexue.fm/archives/10907)
* [Transformer升级之路：19、第二类旋转位置编码](https://kexue.fm/archives/10862)
* [Decoder-only的LLM为什么需要位置编码？](https://kexue.fm/archives/10347)
* [Monarch矩阵：计算高效的稀疏型矩阵分解](https://kexue.fm/archives/10249)
* [缓存与效果的极限拉扯：从MHA、MQA、GQA到MLA](https://kexue.fm/archives/10091)
* [时空之章：将Attention视为平方复杂度的RNN](https://kexue.fm/archives/10017)
* [我在Performer中发现了Transformer-VQ的踪迹](https://kexue.fm/archives/9862)
* [预训练一下，Transformer的长序列成绩还能涨不少！](https://kexue.fm/archives/9787)
* [脑洞大开：非线性RNN居然也可以并行计算？](https://kexue.fm/archives/9783)
[发表你的看法](#comment_form)
[allen](https://www.linkedin.com/in/allenyllee/)
April 20th, 2022
這個問題跟預測「未知類別」應該不一樣對吧？所謂的「預測未知類別問題」是指：通常預測的時候，類別是一個固定的set，比方說有10類，
但是當你給他一個不在這10類範圍裡的輸入，
模型還是會給出一個在這10類中的預測結果，
而不會告訴你「答案不屬於這10類」。
初看你文章的標題：「无法预测的词」我以為是不是在某種情況下，模型必須要用到不存在詞表裡的詞來表達一個意義...
不過看起來你說的不是這種。你說的「无法预测的词」，應該是指「已經存在詞表中，但是模型輸出幾乎不會選到它」對吧？其實我對前者比較有興趣，因為人類經常發現一些新概念無法用現有詞彙表達，就創造一個新詞來表達它。但這新詞甚至不在詞表上，若模型想要表達這個新概念所代表的意義，但它詞表裡沒有那個詞，那它究竟要如何表達？從輸出是經過softmax 的角度來看，模型只能給出機率最大的詞，但不會告訴我們，它想要輸出的詞是一個不存在現有詞表裡的詞的機率有多大。我想，如果能做到這點，那麼AI 就可能可以回答「自己不知道甚麼」，而不再是硬要給出一個答案。[回复评论](https://kexue.fm/archives/9046/comment-page-1?replyTo=18994#respond-post-9046)
[苏剑林](https://kexue.fm)发表于 April 22nd, 2022
你说的这个属于“拒识”了，也有挺多研究，多数需要一个域外数据来辅助训练的。我之前看过感觉比较靠谱的一篇是《Energy-based Out-of-distribution Detection》。
[回复评论](https://kexue.fm/archives/9046/comment-page-1?replyTo=19017#respond-post-9046)
KunteAlex
April 29th, 2022
无法预测的词是不是就是那些生僻字，即训练语料中出现的低频词？[回复评论](https://kexue.fm/archives/9046/comment-page-1?replyTo=19061#respond-post-9046)
[苏剑林](https://kexue.fm)发表于 May 5th, 2022
基本是的。[回复评论](https://kexue.fm/archives/9046/comment-page-1?replyTo=19068#respond-post-9046)
[取消回复](https://kexue.fm/archives/9046#respond-post-9046)
你的大名电子邮箱个人网站（选填）1. 可以使用LaTeX代码，点击“预览效果”可查看效果；
2. 可以通过点击评论楼层编号来引用该楼层；3. 网站可能会有点卡，如非确认评论失败，请**不要重复点击提交**。
********************
### 内容速览* [是否存在](#是否存在)
* [怎么判断](#怎么判断)
* [实践如何](#实践如何)
* [最后小结](#最后小结)
********************
### 智能搜索支持整句搜索！网站自动使用[结巴分词](https://github.com/fxsjy/jieba)进行分词，并结合ngrams排序算法给出合理的搜索结果。
********************
### 热门标签[生成模型](https://kexue.fm/tag/生成模型/)[attention](https://kexue.fm/tag/attention/)[优化](https://kexue.fm/tag/优化/)[语言模型](https://kexue.fm/tag/语言模型/)[模型](https://kexue.fm/tag/模型/)[梯度](https://kexue.fm/tag/梯度/)[网站](https://kexue.fm/tag/网站/)[概率](https://kexue.fm/tag/概率/)[矩阵](https://kexue.fm/tag/矩阵/)[优化器](https://kexue.fm/tag/优化器/)[转载](https://kexue.fm/tag/转载/)[微分方程](https://kexue.fm/tag/微分方程/)[分析](https://kexue.fm/tag/分析/)[天象](https://kexue.fm/tag/天象/)[深度学习](https://kexue.fm/tag/深度学习/)[积分](https://kexue.fm/tag/积分/)[python](https://kexue.fm/tag/python/)[扩散](https://kexue.fm/tag/扩散/)[力学](https://kexue.fm/tag/力学/)[无监督](https://kexue.fm/tag/无监督/)[几何](https://kexue.fm/tag/几何/)[节日](https://kexue.fm/tag/节日/)[生活](https://kexue.fm/tag/生活/)[文本生成](https://kexue.fm/tag/文本生成/)[数论](https://kexue.fm/tag/数论/)
********************
********************
### 随机文章* [实数集到无理数集的双射](https://kexue.fm/archives/2953)
* [关于维度公式“n &gt; 8.33 log N”的可用性分析](https://kexue.fm/archives/8711)
* [当时七夕笑牵牛](https://kexue.fm/archives/1705)
* [RoFormerV2：自然语言理解的极限探索](https://kexue.fm/archives/8998)
* [哥本哈根气候大会召开情况](https://kexue.fm/archives/300)
* [【中文分词系列】 7. 深度学习分词？只需一个词典！](https://kexue.fm/archives/4245)
* [从马尔科夫过程到主方程（推导过程）](https://kexue.fm/archives/4598)
* [用变分推断统一理解生成模型（VAE、GAN、AAE、ALI）](https://kexue.fm/archives/5716)
* [重新思考学习率与Batch Size（二）：平均场](https://kexue.fm/archives/11280)
* [Python的多进程编程技巧](https://kexue.fm/archives/4231)
********************
********************
### 最近评论* [Rapture D](https://kexue.fm/archives/11530/comment-page-1#comment-29104): 我有一个问题，为什么不考虑亥姆霍兹定理和斯托克斯公式。* [mofheka](https://kexue.fm/archives/11390/comment-page-1#comment-29103): 苏神是还在用jax是么？最近在做基于Google Pathway的理念做一个动态版的MPMD框...
* [长琴](https://kexue.fm/archives/11530/comment-page-1#comment-29102): 看懂这篇博客也不是一件容易的事情。* [AlexLi](https://kexue.fm/archives/9257/comment-page-4#comment-29101): 苏老师，请教一下(7)式中将 $\\mu(x\_t)$ 传给$p\_o$ 进行推理的操作。$x\_...
* [tyler\_zxc](https://kexue.fm/archives/7921/comment-page-2#comment-29100): "Performer的思想是将标准的Attention线性化，所以为什么不干脆直接训练一个线性...
* [我](https://kexue.fm/archives/11494/comment-page-1#comment-29099): 似乎并非mHC提出矩阵的思想？之前hyper connection就是了
* [winter](https://kexue.fm/archives/10847/comment-page-1#comment-29098): 苏神您好，假如对于比较均匀的attention weightP，往往呈现long tail分布...
* [苏剑林](https://kexue.fm/archives/8512/comment-page-2#comment-29097): KL散度、JS散度、W距离啥的，都行啊，看你喜欢哪个
* [苏剑林](https://kexue.fm/archives/9119/comment-page-14#comment-29096): 没有绝对公平的对比方法，主要看你关心什么。比如，如果只关心推理成本和推理效果，那么有的方法可以...
* [苏剑林](https://kexue.fm/links.html/comment-page-6#comment-29095): 如果我有时间重新搭建博客，应该会用python自己写了，而不用第三方架构，这样可玩性好很多。事...
********************
********************
### 友情链接* [Cool Papers](https://papers.cool)
* [数学研发](https://bbs.emath.ac.cn)
* [Seatop](http://www.seatop.com.cn/)
* [Xiaoxia](https://xiaoxia.org/)
* [积分表-网络版](https://kexue.fm/sci/integral/index.html)
* [丝路博傲](http://blog.dvxj.com/)
* [数学之家](http://www.2math.cn/)
* [有趣天文奇观](http://interesting-sky.china-vo.org/)
* [TwistedW](http://www.twistedwg.com/)
* [godweiyang](https://godweiyang.com/)
* [AI柠檬](https://blog.ailemon.net/)
* [王登科-DK博客](https://greatdk.com)
* [ESON](https://blog.eson.org/)
* [枫之羽](https://fzhiy.net/)
* [coding-zuo](https://coding-zuo.github.io/)
* [博科园](https://www.bokeyuan.net/)
* [孔皮皮的博客](https://www.kppkkp.top/)
* [运鹏的博客](https://yunpengtai.top/)
* [jiming.site](https://jiming.site/)
* [OmegaXYZ](https://www.omegaxyz.com/)
* [EAI猩球](https://www.robotech.ink/)
* [文举的博客](https://liwenju0.com/)
* [申请链接](https://kexue.fm/links.html)
********************
[![署名-非商业用途-保持一致](https://kexue.fm/usr/themes/geekg/images/cc.gif)](http://creativecommons.org/licenses/by-nc-nd/2.5/cn/)本站采用创作共用版权协议，要求署名、非商业用途和保持一致。转载本站内容必须也遵循“[署名-非商业用途-保持一致](http://creativecommons.org/licenses/by-nc-nd/2.5/cn/)”的创作共用协议。
©2009-2026 Scientific Spaces. All rights reserved. Theme by[laogui](http://www.laogui.com). Powered by[Typecho](http://typecho.org). 备案号:[粤ICP备09093259号-1/2](https://beian.miit.gov.cn/)。