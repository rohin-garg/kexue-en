为节约而生：从标准Attention到稀疏Attention - 科学空间|Scientific Spaces
![MobileSideBar](https://kexue.fm/usr/themes/geekg/images/slide-button.png "MobileSideBar")
## SEARCH
## MENU
* [打赏](https://kexue.fm/reward.html)
* [公式](https://kexue.fm/latex.html)
* [天象](https://kexue.fm/ac.html)
* [链接](https://kexue.fm/links.html)
* [时光](https://kexue.fm/me.html)
* [博览](https://kexue.fm/science.html)
* [归档](https://kexue.fm/content.html)
## CATEGORIES
* [千奇百怪](https://kexue.fm/category/Everything)
* [天文探索](https://kexue.fm/category/Astronomy)
* [数学研究](https://kexue.fm/category/Mathematics)
* [物理化学](https://kexue.fm/category/Phy-chem)
* [信息时代](https://kexue.fm/category/Big-Data)
* [生物自然](https://kexue.fm/category/Biology)
* [图片摄影](https://kexue.fm/category/Photograph)
* [问题百科](https://kexue.fm/category/Questions)
* [生活/情感](https://kexue.fm/category/Life-Feeling)
* [资源共享](https://kexue.fm/category/Resources)
## NEWPOSTS
* [滑动平均视角下的权重衰减和学习率](https://kexue.fm/archives/11459)
* [生成扩散模型漫谈（三十一）：预测数...](https://kexue.fm/archives/11428)
* [Muon优化器指南：快速上手与关键细节](https://kexue.fm/archives/11416)
* [AdamW的Weight RMS的...](https://kexue.fm/archives/11404)
* [n个正态随机数的最大值的渐近估计](https://kexue.fm/archives/11390)
* [流形上的最速下降：5. 对偶梯度下降](https://kexue.fm/archives/11388)
* [低精度Attention可能存在有...](https://kexue.fm/archives/11371)
* [MuP之上：1. 好模型的三个特征](https://kexue.fm/archives/11340)
* [随机矩阵的谱范数的快速估计](https://kexue.fm/archives/11335)
* [DiVeQ：一种非常简洁的VQ训练方案](https://kexue.fm/archives/11328)
## COMMENTS
* [沈天放: Stochastic Rounding一般怎么做的？主要是硬件...](https://kexue.fm/archives/11371/comment-page-1#comment-28982)
* [winter: 苏老师，请问deltanet能学习到full attn的att...](https://kexue.fm/archives/11033/comment-page-2#comment-28981)
* [喝一口可乐: 十分感谢苏神，不好意思未能及时回复，十分抱歉，我再重新表述下我...](https://kexue.fm/archives/10958/comment-page-3#comment-28980)
* [waaaa: 谢谢大佬的讲解，让我这个初学者了解了自编码器的作用。以前我从字...](https://kexue.fm/archives/3331/comment-page-2#comment-28979)
* [苏剑林: 我没参与iMF。不过iMF基本就是第二目标了，所以没必要另写博客了吧](https://kexue.fm/archives/10958/comment-page-3#comment-28978)
* [苏剑林: 点赞](https://kexue.fm/archives/11416/comment-page-1#comment-28977)
* [苏剑林: 这个很好理解吧，因为flow模型的loss里边包含行列式负对数...](https://kexue.fm/archives/5807/comment-page-6#comment-28976)
* [苏剑林: RoPE实际上在这篇文章的一年前就发现了，不过过当时在TPU上...](https://kexue.fm/archives/8130/comment-page-7#comment-28975)
* [苏剑林: 谢谢分享，我去好好补习一下。](https://kexue.fm/archives/11428/comment-page-1#comment-28974)
* [XuuuHF: 苏神您好！看了您很多文章，收获很多，十分感谢。最近在学习M...](https://kexue.fm/archives/10958/comment-page-3#comment-28973)
## USERLOGIN
* [登录](https://kexue.fm/admin/login.php)
[科学空间|Scientific Spaces](https://kexue.fm)
* [登录](https://kexue.fm/admin/login.php)
* [打赏](https://kexue.fm/reward.html)
* [公式](https://kexue.fm/latex.html)
* [天象](https://kexue.fm/ac.html)
* [链接](https://kexue.fm/links.html)
* [时光](https://kexue.fm/me.html)
* [博览](https://kexue.fm/science.html)
* [归档](https://kexue.fm/content.html)
渴望成为一个小飞侠* [![](https://kexue.fm/usr/themes/geekg/images/rss.png)
欢迎订阅](https://kexue.fm/feed)
* [![](https://kexue.fm/usr/themes/geekg/images/mail.png)
个性邮箱](https://kexue.fm/archives/119)
* [![](https://kexue.fm/usr/themes/geekg/images/Saturn.png)
天象信息](https://kexue.fm/ac.html)
* [![](https://kexue.fm/usr/themes/geekg/images/iss.png)
观测ISS](https://kexue.fm/archives/41)
* [![](https://kexue.fm/usr/themes/geekg/images/pi.png)
LaTeX](https://kexue.fm/latex.html)
* [![](https://kexue.fm/usr/themes/geekg/images/mlogo.png)
关于博主](https://kexue.fm/me.html)
欢迎访问“科学空间”，这里将与您共同探讨自然科学，回味人生百态；也期待大家的分享～* [**千奇百怪**Everything](https://kexue.fm/category/Everything)
* [**天文探索**Astronomy](https://kexue.fm/category/Astronomy)
* [**数学研究**Mathematics](https://kexue.fm/category/Mathematics)
* [**物理化学**Phy-chem](https://kexue.fm/category/Phy-chem)
* [**信息时代**Big-Data](https://kexue.fm/category/Big-Data)
* [**生物自然**Biology](https://kexue.fm/category/Biology)
* [**图片摄影**Photograph](https://kexue.fm/category/Photograph)
* [**问题百科**Questions](https://kexue.fm/category/Questions)
* [**生活/情感**Life-Feeling](https://kexue.fm/category/Life-Feeling)
* [**资源共享**Resources](https://kexue.fm/category/Resources)
* [**千奇百怪**](https://kexue.fm/category/Everything)
* [**天文探索**](https://kexue.fm/category/Astronomy)
* [**数学研究**](https://kexue.fm/category/Mathematics)
* [**物理化学**](https://kexue.fm/category/Phy-chem)
* [**信息时代**](https://kexue.fm/category/Big-Data)
* [**生物自然**](https://kexue.fm/category/Biology)
* [**图片摄影**](https://kexue.fm/category/Photograph)
* [**问题百科**](https://kexue.fm/category/Questions)
* [**生活/情感**](https://kexue.fm/category/Life-Feeling)
* [**资源共享**](https://kexue.fm/category/Resources)
[首页](https://kexue.fm)[信息时代](https://kexue.fm/category/Big-Data)为节约而生：从标准Attention到稀疏Attention
27Jul
# [为节约而生：从标准Attention到稀疏Attention](https://kexue.fm/archives/6853)
By苏剑林|2019-07-27|183611位读者|:
[![attention, please!](https://kexue.fm/usr/uploads/2019/07/1271870192.jpg)](https://kexue.fm/usr/uploads/2019/07/1271870192.jpg)
attention, please!
如今NLP领域，Attention大行其道，当然也不止NLP，在CV领域Attention也占有一席之地（Non Local、SAGAN等）。在18年初[《〈Attention is All You Need〉浅读（简介+代码）》](https://kexue.fm/archives/4765)一文中，我们就已经讨论过Attention机制，Attention的核心在于$\\boldsymbol{Q},\\boldsymbol{K},\\boldsymbol{V}$三个向量序列的交互和融合，其中$\\boldsymbol{Q},\\boldsymbol{K}$的交互给出了两两向量之间的某种相关度（权重），而最后的输出序列则是把$\\boldsymbol{V}$按照权重求和得到的。
显然，众多NLP&CV的成果已经充分肯定了Attention的有效性。本文我们将会介绍Attention的一些变体，这些变体的共同特点是——“为节约而生”——既节约时间，也节约显存。
## 背景简述[#](#背景简述)
[《Attention is All You Need》](https://papers.cool/arxiv/1706.03762)一文讨论的我们称之为“乘性Attention”，目前用得比较广泛的也就是这种Attention：
\\begin{equation}Attention(\\boldsymbol{Q},\\boldsymbol{K},\\boldsymbol{V}) = softmax\\left(\\frac{\\boldsymbol{Q}\\boldsymbol{K}^{\\top}}{\\sqrt{d\_k}}\\right)\\boldsymbol{V}\\end{equation}
另外还有加性Attention，但加性Attention并行不大容易实现（或者实现起来占用显存多），所以一般只用来将变长向量序列编码为固定长度的向量（取代简单的Pooling），而很少用来做序列到序列的编码。而在乘性Attention中，用得最广泛的当数Self Attention了，这种情况下$\\boldsymbol{Q},\\boldsymbol{K},\\boldsymbol{V}$都是同一个$\\boldsymbol{X}$经过线性变换之后的结果，这样一来输出结果就是跟$\\boldsymbol{X}$一样长的向量序列，并且能够直接捕捉$X$中任意两个向量的关联，而且易于并行，这都是Self Attention的优点。
然而，从理论上来讲，Self Attention的计算时间和显存占用量都是$\\mathcal{O}(n^2)$级别的（$n$是序列长度），这就意味着如果序列长度变成原来的2倍，显存占用量就是原来的4倍，计算时间也是原来的4倍。当然，假设并行核心数足够多的情况下，计算时间未必会增加到原来的4倍，但是显存的4倍却是实实在在的，无可避免，这也是微调Bert的时候时不时就来个OOM的原因了。
## 稀疏Attention[#](#稀疏Attention)
我们说Self Attention是$\\mathcal{O}(n^2)$的，那是因为它要对序列中的任意两个向量都要计算相关度，得到一个$n^2$大小的相关度矩阵：
[![标准Self Attention的注意力矩阵（左）和关联图示（右）](https://kexue.fm/usr/uploads/2019/07/775103900.png)](https://kexue.fm/usr/uploads/2019/07/775103900.png)
标准Self Attention的注意力矩阵（左）和关联图示（右）
在上图中，左边显示了注意力矩阵，右边显示了关联性，这表明每个元素都跟序列内所有元素有关联。所以，如果要节省显存，加快计算速度，那么一个基本的思路就是减少关联性的计算，也就是认为每个元素只跟序列内的一部分元素相关，这就是**稀疏Attention**的基本原理。本文所要介绍的稀疏Attention，源于OpenAI的论文[《Generating Long Sequences with Sparse Transformers》](https://papers.cool/arxiv/1904.10509)，但没有按照原论文的方式来介绍，而是用一种笔者认为更加自然的思路来介绍。
### Atrous Self Attention[#](<#Atrous Self Attention>)
第一个要引入的概念是Atrous Self Attention，中文可以称之为“膨胀自注意力”、“空洞自注意力”、“带孔自注意力”等。这个名称跟后面的Local Self Attention一样，都是笔者根据它的特性自行命名的，原论文[《Generating Long Sequences with Sparse Transformers》](https://papers.cool/arxiv/1904.10509)并没有出现过这两个概念，但我认为将它们单独引出来是有意义的。
很显然，Atrous Self Attention就是启发于“膨胀卷积（Atrous Convolution）”，如下右图所示，它对相关性进行了约束，强行要求每个元素只跟它相对距离为$k,2k,3k,\\dots$的元素关联，其中$k \> 1$是预先设定的超参数。从下左的注意力矩阵看，就是强行要求相对距离不是$k$的倍数的注意力为0（白色代表0）：
[![Atrous Self Attention的注意力矩阵（左）和关联图示（右）](https://kexue.fm/usr/uploads/2019/07/4107095412.png)](https://kexue.fm/usr/uploads/2019/07/4107095412.png)
Atrous Self Attention的注意力矩阵（左）和关联图示（右）
由于现在计算注意力是“跳着”来了，所以实际上每个元素只跟大约$n/k$个元素算相关性，这样一来理想情况下运行效率和显存占用都变成了$\\mathcal{O}(n^2/k)$，也就是说能直接降低到原来的$1/k$。
### Local Self Attention[#](<#Local Self Attention>)
另一个要引入的过渡概念是Local Self Attention，中文可称之为“局部自注意力”。其实自注意力机制在CV领域统称为“Non Local”，而显然Local Self Attention则要放弃全局关联，重新引入局部关联。具体来说也很简单，就是约束每个元素只与前后$k$个元素以及自身有关联，如下图所示：
[![Local Self Attention的注意力矩阵（左）和关联图示（右）](https://kexue.fm/usr/uploads/2019/07/713126535.png)](https://kexue.fm/usr/uploads/2019/07/713126535.png)
Local Self Attention的注意力矩阵（左）和关联图示（右）
从注意力矩阵来看，就是相对距离超过$k$的注意力都直接设为0。
其实Local Self Attention就跟普通卷积很像了，都是保留了一个$2k+1$大小的窗口，然后在窗口内进行一些运算，不同的是普通卷积是把窗口展平然后接一个全连接层得到输出，而现在是窗口内通过注意力来加权平均得到输出。对于Local Self Attention来说，每个元素只跟$2k+1$个元素算相关性，这样一来理想情况下运行效率和显存占用都变成了$\\mathcal{O}((2k+1)n)\\sim \\mathcal{O}(kn)$了，也就是说随着$n$而线性增长，这是一个很理想的性质——当然也直接牺牲了长程关联性。
### Sparse Self Attention[#](<#Sparse Self Attention>)
到此，就可以很自然地引入OpenAI的Sparse Self Attention了。我们留意到，Atrous Self Attention是带有一些洞的，而Local Self Attention正好填补了这些洞，所以一个简单的方式就是将Local Self Attention和Atrous Self Attention交替使用，两者累积起来，理论上也可以学习到全局关联性，也省了显存。
（简单画个草图就可以知道，假如第一层用Local Self Attention的话，那么输出的每个向量都融合了局部的几个输入向量，然后第二层用Atrous Self Attention，虽然它是跳着来，但是因为第一层的输出融合了局部的输入向量，所以第二层的输出理论上可以跟任意的输入向量相关，也就是说实现了长程关联。）
但是OpenAI没有这样做，它直接将两个Atrous Self Attention和Local Self Attention合并为一个，如下图：
[![Sparse Self Attention的注意力矩阵（左）和关联图示（右）](https://kexue.fm/usr/uploads/2019/07/1199615308.png)](https://kexue.fm/usr/uploads/2019/07/1199615308.png)
Sparse Self Attention的注意力矩阵（左）和关联图示（右）
从注意力矩阵上看就很容易理解了，就是除了相对距离不超过$k$的、相对距离为$k,2k,3k,\\dots$的注意力都设为0，这样一来Attention就具有“局部紧密相关和远程稀疏相关”的特性，这对很多任务来说可能是一个不错的先验，因为真正需要密集的长程关联的任务事实上是很少的。
## 代码实现[#](#代码实现)
上面的Atrous Self Attention、Local Self Attention、Sparse Self Attention都算是稀疏Attention，直观上来看就是注意力矩阵变得很稀疏了。那怎么实现它们呢？如果直接在注意力矩阵中对为零的部分进行mask的话，那在数学上（功能上）是没有问题的，但这样做并不能提速，也不能省显存。
### 官方实现[#](#官方实现)
OpenAI也开源了自己的实现，位于：[https://github.com/openai/sparse\_attention](https://github.com/openai/sparse_attention)
这是基于tensorflow的，还用到了它们自己的一个稀疏矩阵库[blocksparse](https://github.com/openai/blocksparse/)。不过这玩意似乎封装得很奇怪，我不知道怎么将它迁移到Keras，而且它用了很多Python 3的特性，不能直接用于Python 2。如果用Python 3和纯Tensorflow的朋友可以试试。
还有一个问题是OpenAI原论文主要是用稀疏Attention来生成超长序列，所以它在论文中和代码中都把注意力矩阵的所有上三角部分都mask了（避免用到未来信息），但未必所有用到稀疏Attention的都是生成场景，而且对于基本概念的介绍来说，这是不必要的，这也是笔者不按原论文的思路来介绍的原因之一。
### Keras实现[#](#Keras实现)
对于Keras，笔者根据自己构思的写法实现了上述三种稀疏Attention，并且和原来写过的Attention代码统一规范化了一下，还是放在原来的位置：
[https://github.com/bojone/attention/blob/master/attention\_keras.py](https://github.com/bojone/attention/blob/master/attention_keras.py)
经过实验，发现在笔者的写法中，这三种稀疏Attention相比全量Attention确实能节省一些内存，但遗憾的是，除了Atrous Self Attention外，剩下两种Attention的实现都不能提速，反而降低了一点速度，这是因为实现过程中没有充分利用稀疏性所致的，而OpenAI的blocksparse则是经过高度优化，而且是直接写的CUDA代码，这没法比。但不管速度如何，三种稀疏Attention功能上应该是没毛病的。
## 文章小结[#](#文章小结)
也没什么好总结的了，就介绍并实现了三种稀疏Attention。除了省显存外，稀疏的Attention应该能够更好地适应一些任务，毕竟大多数任务的关联主要都在局部的，而且是从局部到整体的形式。尤其是最后一个Sparse Self Attention所体现的“局部紧密相关和远程稀疏相关”，应当能满足大多数任务的特点，如果有相应任务的读者，不妨试用一下。
***转载到请包括本文地址：** [https://kexue.fm/archives/6853](https://kexue.fm/archives/6853)*
***更详细的转载事宜请参考：*** [《科学空间FAQ》](https://kexue.fm/archives/6508#文章如何转载/引用)
**如果您还有什么疑惑或建议，欢迎在下方评论区继续讨论。**
**如果您觉得本文还不错，欢迎[分享](#share)/[打赏](#pay)本文。打赏并非要从中获得收益，而是希望知道科学空间获得了多少读者的真心关注。当然，如果你无视它，也不会影响你的阅读。再次表示欢迎和感谢！**
打赏![科学空间](https://kexue.fm/usr/themes/geekg/payment/wx.png)
微信打赏![科学空间](https://kexue.fm/usr/themes/geekg/payment/zfb.png)
支付宝打赏因为网站后台对打赏并无记录，因此欢迎在打赏时候备注留言。 你还可以[**点击这里**](http://mail.qq.com/cgi-bin/qm_share?t=qm_mailme&email=tN7d1drY3drrx8H0xcWa19vZ)或在下方评论区留言来告知你的建议或需求。
**如果您需要引用本文，请参考：**
苏剑林. (Jul. 27, 2019). 《为节约而生：从标准Attention到稀疏Attention 》[Blog post]. Retrieved from[https://kexue.fm/archives/6853](https://kexue.fm/archives/6853)
@online{kexuefm-6853,
title={为节约而生：从标准Attention到稀疏Attention},
author={苏剑林},
year={2019},
month={Jul},
url={\\url{https://kexue.fm/archives/6853}},
}
分类：[信息时代](https://kexue.fm/category/Big-Data) 标签：[模型](https://kexue.fm/tag/模型/),[稀疏](https://kexue.fm/tag/稀疏/),[attention](https://kexue.fm/tag/attention/)[33 评论](https://kexue.fm/archives/6853#comments)
&lt;[思考：两个椭圆片能粘合成一个立体吗？](https://kexue.fm/archives/6818)|[Keras实现两个优化器：Lookahead和LazyOptimizer](https://kexue.fm/archives/6869)&gt;
### 你也许还对下面的内容感兴趣* [低精度Attention可能存在有偏的舍入误差](https://kexue.fm/archives/11371)
* [为什么线性注意力要加Short Conv？](https://kexue.fm/archives/11320)
* [QK-Clip：让Muon在Scaleup之路上更进一步](https://kexue.fm/archives/11126)
* [Transformer升级之路：21、MLA好在哪里?（下）](https://kexue.fm/archives/11111)
* [“对角+低秩”三角阵的高效求逆方法](https://kexue.fm/archives/11072)
* [线性注意力简史：从模仿、创新到反哺](https://kexue.fm/archives/11033)
* [MoE环游记：5、均匀分布的反思](https://kexue.fm/archives/10945)
* [Transformer升级之路：20、MLA好在哪里?（上）](https://kexue.fm/archives/10907)
* [Transformer升级之路：19、第二类旋转位置编码](https://kexue.fm/archives/10862)
* [矩阵的有效秩（Effective Rank）](https://kexue.fm/archives/10847)
[发表你的看法](#comment_form)
1. [&laquo;](https://kexue.fm/archives/6853/comment-page-1#comments)
2. [1](https://kexue.fm/archives/6853/comment-page-1#comments)
3. [2](https://kexue.fm/archives/6853/comment-page-2#comments)
peacerl
October 31st, 2022
苏神，想问一下，先局部再空洞的方案您后面有尝试吗，效果如何？[回复评论](https://kexue.fm/archives/6853/comment-page-2?replyTo=20228#respond-post-6853)
[苏剑林](https://kexue.fm)发表于 November 3rd, 2022
这么久远的文章，没印象了～[回复评论](https://kexue.fm/archives/6853/comment-page-2?replyTo=20253#respond-post-6853)
peacerl 发表于November 3rd, 2022
好的，谢谢回复，我来尝试一下hh
[回复评论](https://kexue.fm/archives/6853/comment-page-2?replyTo=20258#respond-post-6853)
ykk 发表于April 24th, 2023
有人用这个发了ECCV论文，还有一篇ICLR 2023也是这样
[回复评论](https://kexue.fm/archives/6853/comment-page-2?replyTo=21452#respond-post-6853)
顿吾发表于July 22nd, 2023
能说明一下这两篇论文名吗，我对这个挺感兴趣的，麻烦了[回复评论](https://kexue.fm/archives/6853/comment-page-2?replyTo=22333#respond-post-6853)
Orzlala 发表于March 24th, 2024
2024年的一篇CVPR就是苏神讲的稀疏自注意力--ACC-ViT : Atrous Convolution’s Comeback in Vision Transformers，眼光太超前了
[回复评论](https://kexue.fm/archives/6853/comment-page-2?replyTo=24004#respond-post-6853)
Orzlala 发表于March 24th, 2024
哥们，不会这篇文章就是你发的吧：ACC-ViT : Atrous Convolution’s Comeback in Vision Transformers
[回复评论](https://kexue.fm/archives/6853/comment-page-2?replyTo=24005#respond-post-6853)
[alex在成都](http://sxontheway.github.io)
June 27th, 2023
想请教一下苏神，如果只是想数值验证一下结果，不考虑效率。这些sparse
attention的方法是否等效于把softmax后的attention矩阵相应位置赋为0?
[回复评论](https://kexue.fm/archives/6853/comment-page-2?replyTo=22078#respond-post-6853)
[苏剑林](https://kexue.fm)发表于 June 28th, 2023
应该是softmax之前把相应的位置改为-np.inf
[回复评论](https://kexue.fm/archives/6853/comment-page-2?replyTo=22095#respond-post-6853)
小火车发表于August 30th, 2023
苏神说的对，我画蛇添足解释下。原理参考padding\_mask，softmax前要改成无穷小的一个值，这样softmax的时候该项（e的无穷小）约等于0,就实现了稀疏的效果。
[回复评论](https://kexue.fm/archives/6853/comment-page-2?replyTo=22607#respond-post-6853)
ZYZ
April 24th, 2024
一个typo， “右变显示了关联性”--\> “右边显示了关联性”[回复评论](https://kexue.fm/archives/6853/comment-page-2?replyTo=24194#respond-post-6853)
[苏剑林](https://kexue.fm)发表于 April 25th, 2024
感谢指出，已更正。[回复评论](https://kexue.fm/archives/6853/comment-page-2?replyTo=24206#respond-post-6853)
1. [&laquo;](https://kexue.fm/archives/6853/comment-page-1#comments)
2. [1](https://kexue.fm/archives/6853/comment-page-1#comments)
3. [2](https://kexue.fm/archives/6853/comment-page-2#comments)
[取消回复](https://kexue.fm/archives/6853#respond-post-6853)
你的大名电子邮箱个人网站（选填）1. 可以使用LaTeX代码，点击“预览效果”可查看效果；
2. 可以通过点击评论楼层编号来引用该楼层；3. 网站可能会有点卡，如非确认评论失败，请**不要重复点击提交**。
********************
### 内容速览4. [背景简述](#背景简述)
5. [稀疏Attention](#稀疏Attention)
6. [Atrous Self Attention](<#Atrous Self Attention>)
7. [Local Self Attention](<#Local Self Attention>)
8. [Sparse Self Attention](<#Sparse Self Attention>)
9. [代码实现](#代码实现)
10. [官方实现](#官方实现)
11. [Keras实现](#Keras实现)
12. [文章小结](#文章小结)
********************
### 智能搜索支持整句搜索！网站自动使用[结巴分词](https://github.com/fxsjy/jieba)进行分词，并结合ngrams排序算法给出合理的搜索结果。
********************
### 热门标签[生成模型](https://kexue.fm/tag/生成模型/)[attention](https://kexue.fm/tag/attention/)[优化](https://kexue.fm/tag/优化/)[语言模型](https://kexue.fm/tag/语言模型/)[模型](https://kexue.fm/tag/模型/)[网站](https://kexue.fm/tag/网站/)[梯度](https://kexue.fm/tag/梯度/)[概率](https://kexue.fm/tag/概率/)[矩阵](https://kexue.fm/tag/矩阵/)[转载](https://kexue.fm/tag/转载/)[优化器](https://kexue.fm/tag/优化器/)[微分方程](https://kexue.fm/tag/微分方程/)[分析](https://kexue.fm/tag/分析/)[天象](https://kexue.fm/tag/天象/)[深度学习](https://kexue.fm/tag/深度学习/)[积分](https://kexue.fm/tag/积分/)[python](https://kexue.fm/tag/python/)[扩散](https://kexue.fm/tag/扩散/)[力学](https://kexue.fm/tag/力学/)[无监督](https://kexue.fm/tag/无监督/)[几何](https://kexue.fm/tag/几何/)[节日](https://kexue.fm/tag/节日/)[生活](https://kexue.fm/tag/生活/)[文本生成](https://kexue.fm/tag/文本生成/)[数论](https://kexue.fm/tag/数论/)
********************
********************
### 随机文章* [【分享】兴隆山的双子座流星雨](https://kexue.fm/archives/3580)
* [【NASA每日一图】不规则的NGC 55](https://kexue.fm/archives/71)
* [相对论和量子力学的初探](https://kexue.fm/archives/1744)
* [“对角+低秩”三角阵的高效求逆方法](https://kexue.fm/archives/11072)
* [思考：两个椭圆片能粘合成一个立体吗？](https://kexue.fm/archives/6818)
* [一道自然数的数学题](https://kexue.fm/archives/35)
* [【翻译】庆祝希格斯玻色子的最终发现！](https://kexue.fm/archives/1661)
* [从SamplePairing到mixup：神奇的正则项](https://kexue.fm/archives/5693)
* [朋友们，来瓶汽水吧！有趣的换汽水问题](https://kexue.fm/archives/3495)
* [数学基本技艺之23、24（下）](https://kexue.fm/archives/2096)
********************
********************
### 最近评论* [沈天放](https://kexue.fm/archives/11371/comment-page-1#comment-28982): Stochastic Rounding一般怎么做的？主要是硬件电路里随机bit从哪里来，有的说...
* [winter](https://kexue.fm/archives/11033/comment-page-2#comment-28981): 苏老师，请问deltanet能学习到full attn的attn pattern吗？
* [喝一口可乐](https://kexue.fm/archives/10958/comment-page-3#comment-28980): 十分感谢苏神，不好意思未能及时回复，十分抱歉，我再重新表述下我的问题。在机械臂领域，我们会收集...
* [waaaa](https://kexue.fm/archives/3331/comment-page-2#comment-28979): 谢谢大佬的讲解，让我这个初学者了解了自编码器的作用。以前我从字面意义上理解vae，看到编码两个...
* [苏剑林](https://kexue.fm/archives/10958/comment-page-3#comment-28978): 我没参与iMF。不过iMF基本就是第二目标了，所以没必要另写博客了吧
* [苏剑林](https://kexue.fm/archives/11416/comment-page-1#comment-28977): 点赞* [苏剑林](https://kexue.fm/archives/5807/comment-page-6#comment-28976): 这个很好理解吧，因为flow模型的loss里边包含行列式负对数啊，如果不可逆，行列式为0，那么...
* [苏剑林](https://kexue.fm/archives/8130/comment-page-7#comment-28975): RoPE实际上在这篇文章的一年前就发现了，不过过当时在TPU上做实验，结果似乎有些异常，所以一...
* [苏剑林](https://kexue.fm/archives/11428/comment-page-1#comment-28974): 谢谢分享，我去好好补习一下。* [XuuuHF](https://kexue.fm/archives/10958/comment-page-3#comment-28973): 苏神您好！看了您很多文章，收获很多，十分感谢。最近在学习MeanFlow 这篇文章时，发现...
********************
********************
### 友情链接* [Cool Papers](https://papers.cool)
* [数学研发](https://bbs.emath.ac.cn)
* [Seatop](http://www.seatop.com.cn/)
* [Xiaoxia](https://xiaoxia.org/)
* [积分表-网络版](https://kexue.fm/sci/integral/index.html)
* [丝路博傲](http://blog.dvxj.com/)
* [数学之家](http://www.2math.cn/)
* [有趣天文奇观](http://interesting-sky.china-vo.org/)
* [TwistedW](http://www.twistedwg.com/)
* [godweiyang](https://godweiyang.com/)
* [AI柠檬](https://blog.ailemon.net/)
* [王登科-DK博客](https://greatdk.com)
* [ESON](https://blog.eson.org/)
* [枫之羽](https://fzhiy.net/)
* [Mathor's blog](https://wmathor.com/)
* [coding-zuo](https://coding-zuo.github.io/)
* [博科园](https://www.bokeyuan.net/)
* [孔皮皮的博客](https://www.kppkkp.top/)
* [运鹏的博客](https://yunpengtai.top/)
* [jiming.site](https://jiming.site/)
* [OmegaXYZ](https://www.omegaxyz.com/)
* [EAI猩球](https://www.robotech.ink/)
* [文举的博客](https://liwenju0.com/)
* [申请链接](https://kexue.fm/links.html)
********************
[![署名-非商业用途-保持一致](https://kexue.fm/usr/themes/geekg/images/cc.gif)](http://creativecommons.org/licenses/by-nc-nd/2.5/cn/)本站采用创作共用版权协议，要求署名、非商业用途和保持一致。转载本站内容必须也遵循“[署名-非商业用途-保持一致](http://creativecommons.org/licenses/by-nc-nd/2.5/cn/)”的创作共用协议。
©2009-2025 Scientific Spaces. All rights reserved. Theme by[laogui](http://www.laogui.com). Powered by[Typecho](http://typecho.org). 备案号:[粤ICP备09093259号-1/2](https://beian.miit.gov.cn/)。