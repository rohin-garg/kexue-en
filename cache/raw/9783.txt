## SEARCH

## MENU

- [打赏](https://kexue.fm/reward.html)
- [公式](https://kexue.fm/latex.html)
- [天象](https://kexue.fm/ac.html)
- [链接](https://kexue.fm/links.html)
- [时光](https://kexue.fm/me.html)
- [博览](https://kexue.fm/science.html)
- [归档](https://kexue.fm/content.html)

## CATEGORIES

- [千奇百怪](https://kexue.fm/category/Everything)
- [天文探索](https://kexue.fm/category/Astronomy)
- [数学研究](https://kexue.fm/category/Mathematics)
- [物理化学](https://kexue.fm/category/Phy-chem)
- [信息时代](https://kexue.fm/category/Big-Data)
- [生物自然](https://kexue.fm/category/Biology)
- [图片摄影](https://kexue.fm/category/Photograph)
- [问题百科](https://kexue.fm/category/Questions)
- [生活/情感](https://kexue.fm/category/Life-Feeling)
- [资源共享](https://kexue.fm/category/Resources)

## NEWPOSTS

- [为什么DeltaNet要加L2 N...](https://kexue.fm/archives/11486)
- [让炼丹更科学一些（三）：SGD的终...](https://kexue.fm/archives/11480)
- [让炼丹更科学一些（二）：将结论推广...](https://kexue.fm/archives/11469)
- [滑动平均视角下的权重衰减和学习率](https://kexue.fm/archives/11459)
- [生成扩散模型漫谈（三十一）：预测数...](https://kexue.fm/archives/11428)
- [Muon优化器指南：快速上手与关键细节](https://kexue.fm/archives/11416)
- [AdamW的Weight RMS的...](https://kexue.fm/archives/11404)
- [n个正态随机数的最大值的渐近估计](https://kexue.fm/archives/11390)
- [流形上的最速下降：5\. 对偶梯度下降](https://kexue.fm/archives/11388)
- [低精度Attention可能存在有...](https://kexue.fm/archives/11371)

## COMMENTS

- [kaiyuan: 看了“Linear Transformers Are Secr...](https://kexue.fm/archives/11486/comment-page-1#comment-29036)
- [sog: 好的，符号相同，搞混了呃](https://kexue.fm/archives/11469/comment-page-1#comment-29035)
- [kerry: 还没有通读完后面的系列，提出一些拙见。\
降低方差这一节把原本的...](https://kexue.fm/archives/9119/comment-page-14#comment-29034)
- [Kevin Yin: I wrote https://research.novela...](https://kexue.fm/archives/11158/comment-page-1#comment-29033)
- [罗: 公式(6)显示出来是不是有点小问题？](https://kexue.fm/archives/11480/comment-page-1#comment-29032)
- [cmlin: 本人对这方面不太熟悉，想了解这三个条件的意义及动机，且希望这系...](https://kexue.fm/archives/11340/comment-page-1#comment-29031)
- [喝一口可乐: 理解了，感谢苏神回复，数学上给出建模分析确实清晰了很多，再次感...](https://kexue.fm/archives/10958/comment-page-3#comment-29030)
- [CuddleSabe1: 感觉普通的 flow matching 可以看成 degrad...](https://kexue.fm/archives/10958/comment-page-1#comment-29029)
- [岁月如书: 受教了，感谢](https://kexue.fm/archives/11126/comment-page-3#comment-29028)
- [苏剑林: 是](https://kexue.fm/archives/11126/comment-page-3#comment-29027)

## USERLOGIN

- [登录](https://kexue.fm/admin/login.php)

[科学空间\|Scientific Spaces](https://kexue.fm)

- [登录](https://kexue.fm/admin/login.php)
- [打赏](https://kexue.fm/reward.html)
- [公式](https://kexue.fm/latex.html)
- [天象](https://kexue.fm/ac.html)
- [链接](https://kexue.fm/links.html)
- [时光](https://kexue.fm/me.html)
- [博览](https://kexue.fm/science.html)
- [归档](https://kexue.fm/content.html)

渴望成为一个小飞侠

- [欢迎订阅](https://kexue.fm/feed)
- [个性邮箱](https://kexue.fm/archives/119)
- [天象信息](https://kexue.fm/ac.html)
- [观测ISS](https://kexue.fm/archives/41)
- [LaTeX](https://kexue.fm/latex.html)
- [关于博主](https://kexue.fm/me.html)

欢迎访问“科学空间”，这里将与您共同探讨自然科学，回味人生百态；也期待大家的分享～

- [**千奇百怪** Everything](https://kexue.fm/category/Everything)
- [**天文探索** Astronomy](https://kexue.fm/category/Astronomy)
- [**数学研究** Mathematics](https://kexue.fm/category/Mathematics)
- [**物理化学** Phy-chem](https://kexue.fm/category/Phy-chem)
- [**信息时代** Big-Data](https://kexue.fm/category/Big-Data)
- [**生物自然** Biology](https://kexue.fm/category/Biology)
- [**图片摄影** Photograph](https://kexue.fm/category/Photograph)
- [**问题百科** Questions](https://kexue.fm/category/Questions)
- [**生活/情感** Life-Feeling](https://kexue.fm/category/Life-Feeling)
- [**资源共享** Resources](https://kexue.fm/category/Resources)

- [**千奇百怪**](https://kexue.fm/category/Everything)
- [**天文探索**](https://kexue.fm/category/Astronomy)
- [**数学研究**](https://kexue.fm/category/Mathematics)
- [**物理化学**](https://kexue.fm/category/Phy-chem)
- [**信息时代**](https://kexue.fm/category/Big-Data)
- [**生物自然**](https://kexue.fm/category/Biology)
- [**图片摄影**](https://kexue.fm/category/Photograph)
- [**问题百科**](https://kexue.fm/category/Questions)
- [**生活/情感**](https://kexue.fm/category/Life-Feeling)
- [**资源共享**](https://kexue.fm/category/Resources)

[首页](https://kexue.fm) [数学研究](https://kexue.fm/category/Mathematics) [信息时代](https://kexue.fm/category/Big-Data) 脑洞大开：非线性RNN居然也可以并行计算？

26Sep

# [脑洞大开：非线性RNN居然也可以并行计算？](https://kexue.fm/archives/9783)

By 苏剑林 \|
2023-09-26 \|
89987位读者\|

近年来，线性RNN由于其可并行训练以及常数推理成本等特性，吸引了一定研究人员的关注（例如笔者之前写的 [《Google新作试图“复活”RNN：RNN能否再次辉煌？》](https://kexue.fm/archives/9554)），这让RNN在Transformer遍地开花的潮流中仍有“一席之地”。然而，目前看来这“一席之地”只属于线性RNN，因为非线性RNN无法高效地并行训练，所以在架构之争中是“心有余而力不足”。

不过，一篇名为 [《Parallelizing Non-Linear Sequential Models over the Sequence Length》](https://papers.cool/arxiv/2309.12252) 的论文有不同的看法，它提出了一种迭代算法，宣传可以实现非线性RNN的并行训练！真有如此神奇？接下来我们一探究竟。

## 求不动点 [\#](https://kexue.fm/kexue.fm\#%E6%B1%82%E4%B8%8D%E5%8A%A8%E7%82%B9)

原论文对其方法做了非常一般的介绍，而且其侧重点是PDE和ODE，这里我们直接从RNN入手。考虑常见的简单非线性RNN：
\\begin{equation}x\_t = \\tanh(Ax\_{t-1} + u\_t)\\label{eq:rnn}\\end{equation}
由于$\\tanh$的存在，它只能串行计算。现在我们在两边都减去$Ax\_{t-1}$：
\\begin{equation}x\_t - Ax\_{t-1} = \\tanh(Ax\_{t-1} + u\_t) - Ax\_{t-1}\\end{equation}
当然，这改变不了它是非线性RNN的实质。然而我们可以发现，假如右端的$x\_{t-1}$换成像$u\_t$那样的给定向量，那么这就是一个线性RNN了，根据 [《Google新作试图“复活”RNN：RNN能否再次辉煌？》](https://kexue.fm/archives/9554) 的结果，它是可以并行计算的。此时，敏捷的读者可能已经猜到后面的步骤了——迭代求解！

首先，将上述RNN更改成
\\begin{equation}x\_t^{(n)} - Ax\_{t-1}^{(n)} = \\tanh(Ax\_{t-1}^{(n-1)} + u\_t) - Ax\_{t-1}^{(n-1)}\\label{eq:rnn-iter}\\end{equation}
从给定$x\_t^{(0)}$出发，反复迭代上式，理想情况下，它会收敛于一个不动点$x\_t^\*$，这就是原来非线性RNN的计算结果。当然，理论上通过式$\\eqref{eq:rnn-iter}$迭代的总计算量是比直接通过式$\\eqref{eq:rnn}$递归计算要大的，但由于每一步迭代都是可并行的线性RNN，并且如果收敛速度比较快时迭代步数不需要太多，那么总的耗时通常都会快于直接非线性RNN递归（尤其是序列长度很大时）。

## 简化形式 [\#](https://kexue.fm/kexue.fm\#%E7%AE%80%E5%8C%96%E5%BD%A2%E5%BC%8F)

事实上，非线性RNN之所以慢，无法并行计算还是次要的，最关键是它包含了大量的非element-wise运算，比如式$\\eqref{eq:rnn}$的$\\tanh$里边的矩阵运算$Ax\_{t-1}$；而线性RNN之所以快，除了它允许并行训练之外，更关键的是它能通过对角化来将矩阵乘法变换为element-wise的乘法——对于element-wise乘法来说，即便是串行计算也不会太慢。

当我们通过式$\\eqref{eq:rnn-iter}$将非线性RNN转为线性RNN的迭代之后，同样享受线性RNN可对角化的“待遇”，从而提高计算速度。具体来说，在复数域中将$A$对角化为$P\\Lambda P^{-1}$，那么式$\\eqref{eq:rnn-iter}$变为
\\begin{equation}x\_t^{(n)} - P\\Lambda P^{-1} x\_{t-1}^{(n)} = \\tanh(P\\Lambda P^{-1} x\_{t-1}^{(n-1)} + u\_t) - P\\Lambda P^{-1} x\_{t-1}^{(n-1)}\\end{equation}
两端都左乘$P^{-1}$：
\\begin{equation}P^{-1} x\_t^{(n)} - \\Lambda P^{-1} x\_{t-1}^{(n)} = P^{-1}\\tanh(P\\Lambda P^{-1} x\_{t-1}^{(n-1)} + u\_t) - \\Lambda P^{-1} x\_{t-1}^{(n-1)}\\end{equation}
令$y\_t = P^{-1} x\_t$，那么上式可以简化为
\\begin{equation}y\_t^{(n)} - \\Lambda y\_{t-1}^{(n)} = P^{-1}\\tanh(P\\Lambda y\_{t-1}^{(n-1)} + u\_t) - \\Lambda y\_{t-1}^{(n-1)}\\end{equation}
由于RNN之后一般都还要接个投影层，所以$x\_t = P y\_t$的$P$原则上可以合并到外接的投影层里边，也就是说，上式理论上具备跟原来的$\\eqref{eq:rnn}$具备同等的表达能力，但由于$\\Lambda$是对角阵，递归的计算量会明显降低。上式还出现了逆矩阵$P^{-1}$，不单计算量大，而且不利于优化，所以我们可以干脆将$P^{-1}$和$P\\Lambda$换成两个不相关的参数矩阵：
\\begin{equation}y\_t^{(n)} - \\Lambda y\_{t-1}^{(n)} = P\\tanh(Q y\_{t-1}^{(n-1)} + u\_t) - \\Lambda y\_{t-1}^{(n-1)}\\end{equation}
只要初始化是$PQ=\\Lambda$就行。

## 摄动思想 [\#](https://kexue.fm/kexue.fm\#%E6%91%84%E5%8A%A8%E6%80%9D%E6%83%B3)

假定$x\_t^{(0)}=0$，那么式$\\eqref{eq:rnn-iter}$其实就是将原本的非线性RNN就分解为一系列线性RNN：
\\begin{equation}\\begin{array}{c}
x\_t^{(1)} - Ax\_{t-1}^{(1)} = \\tanh(u\_t)\\\
x\_t^{(2)} - Ax\_{t-1}^{(2)} = \\tanh(Ax\_{t-1}^{(1)} + u\_t) - Ax\_{t-1}^{(1)} \\\
\\vdots \\\
x\_t^{(n)} - Ax\_{t-1}^{(n)} = \\tanh(Ax\_{t-1}^{(n-1)} + u\_t) - Ax\_{t-1}^{(n-1)} \\\
\\vdots \\\
\\end{array}\\label{eq:rnns}\\end{equation}
而假设$x\_{t-1},u\_t$都是小量，那么对式$\\eqref{eq:rnn}$右端利用$\\tanh x \\approx x$得到：
\\begin{equation}x\_t = \\tanh(Ax\_{t-1} + u\_t) \\approx Ax\_{t-1} + u\_t \\approx Ax\_{t-1} + \\tanh(u\_t)\\label{eq:rnn-approx}\\end{equation}
这正好是$\\eqref{eq:rnns}$中的第一个方程，因此如果假设成立，那么$x\_t^{(1)}$或许已经足够接近理想的$x\_t^\*$，后面的每一步迭代都在快速逼近它。从这里我们可以看出，“两边同时减去$Ax\_{t-1}$”是关键之处，这使得$\\eqref{eq:rnn-iter}$的第一步迭代就接近于原本非线性RNN的一阶线性近似，这可以提高收敛速度，也是数学物理中的经典操作，名曰“ [摄动](https://kexue.fm/tag/%E6%91%84%E5%8A%A8/)”。

## 加快收敛 [\#](https://kexue.fm/kexue.fm\#%E5%8A%A0%E5%BF%AB%E6%94%B6%E6%95%9B)

根据摄动法的思想，提高收敛速度的关键就是提高近似展开的精度，比如较为简单的改进是只假设$x\_{t-1}$是小量，那么根据一阶泰勒展开有（将$u\_t$视为列向量，这里的$\\circ$是Hadamard积分）
\\begin{equation}x\_t = \\tanh(Ax\_{t-1} + u\_t) \\approx \\tanh(u\_t) + (\\text{sech}^2 u\_t\\circ A)x\_{t-1}\\end{equation}
于是改进的结果就是式$\\eqref{eq:rnn-iter}$变为
\\begin{equation}x\_t^{(n)} - A\_t x\_{t-1}^{(n)} = \\tanh(Ax\_{t-1}^{(n-1)} + u\_t) - A\_t x\_{t-1}^{(n-1)}\\label{eq:iter-plus1}\\end{equation}
其中$A\_t = \\text{sech}^2 u\_t\\circ A$。更精细的改进是在每一步迭代时，都在前一步迭代结果的基础上进行展开：
\\begin{equation}\\begin{aligned}
x\_t =&\\, \\tanh(Ax\_{t-1} + u\_t) \\\
\\approx&\\, \\tanh(Ax\_{t-1}^{(n-1)} + u\_t) + (\\text{sech}^2 (Ax\_{t-1}^{(n-1)} + u\_t)\\circ A)(x\_{t-1} - x\_{t-1}^{(n-1)})
\\end{aligned}\\end{equation}
于是式$\\eqref{eq:rnn-iter}$变为
\\begin{equation}x\_t^{(n)} - A\_t^{(n)} x\_{t-1}^{(n)} = \\tanh(Ax\_{t-1}^{(n-1)} + u\_t) - A\_t^{(n)} x\_{t-1}^{(n-1)}\\label{eq:iter-plus2}\\end{equation}
其中$A\_t^{(n)}=\\text{sech}^2 (Ax\_{t-1}^{(n-1)} + u\_t)\\circ A$。最后的这个迭代格式，实际上就是求方程数值解的“ [牛顿法](https://en.wikipedia.org/wiki/Newton%27s_method)”，它具有二次收敛速度。

## 何必收敛 [\#](https://kexue.fm/kexue.fm\#%E4%BD%95%E5%BF%85%E6%94%B6%E6%95%9B)

理论上来说，$\\eqref{eq:iter-plus1}$、$\\eqref{eq:iter-plus2}$两个改进确实能提高收敛速度，然而它们使得每一步线性递归的矩阵$A$变得跟$t$甚至$n$有关了，这其实会大大增加并行的复杂度，也不能利用“ [简化形式](https://kexue.fm/kexue.fm#%E7%AE%80%E5%8C%96%E5%BD%A2%E5%BC%8F)”一节的对角化技巧来加速。另一方面，如果保持$\\eqref{eq:rnn-iter}$这样的迭代格式，虽然有诸多效率上的好处，但收敛方面确实无法得到很好的保障。

难道这两者的矛盾就无法调和了吗？事实上，按照笔者的观点，最直接的做法是“别去管它”——借助非线性RNN导出了$\\eqref{eq:rnn-iter}$后，就忘记原本的非线性RNN，将式$\\eqref{eq:rnn-iter}$作为基本模型。也就是说，何必忧虑式$\\eqref{eq:rnn-iter}$会不会收敛到原来的非线性RNN？直接将它作为新的出发点不好吗？梯度下降学到怎样的结果就是怎样的结果，如果梯度下降学到的结果是不收敛到原来的非线性RNN，那么就意味着不收敛到原来的RNN是更适合的。

抛开这一层思维束缚后，其实很多问题会变得豁然开朗起来。首先，即便是式$\\eqref{eq:iter-plus2}$在理论上拥有非常好的收敛速度，但也是有条件的，而且在深度学习的背景下，要保证这些条件会显得很奢侈。换言之，即便是式$\\eqref{eq:iter-plus2}$的收敛性也没有绝对保证，所以何必“五十步笑百步”去苛责式$\\eqref{eq:rnn-iter}$？其次，将式$\\eqref{eq:rnn-iter}$视为新的出发点后，我们可以将它单纯地理解为线性RNN的一种新用法，或者说解决线性RNN缺陷（比如线性RNN不是图灵完备的）的一个思路，这样操作性更强。

总的来说，不去管它的收敛性，似乎更能打破思维僵局，探索更一般的结果。

## 一般情形 [\#](https://kexue.fm/kexue.fm\#%E4%B8%80%E8%88%AC%E6%83%85%E5%BD%A2)

前面的“长篇大论”，都只围绕着简单的非线性RNN也就是式$\\eqref{eq:rnn}$进行讨论，对于更常用的LSTM、GRU，结果又如何呢？

以GRU为例，它原本的形式为
\\begin{equation}\\begin{aligned} z\_{t} & = \\sigma \\left( W\_{z} x\_{t} + U\_{z} h\_{t - 1} + b\_{z} \\right) \\\
r\_{t} & = \\sigma \\left( W\_{r} x\_{t} + U\_{r} h\_{t - 1} + b\_{r} \\right) \\\
\\hat{h}\_t & = \\tanh \\left( W\_{h} x\_{t} + U\_{h} (r\_t \\circ h\_{t - 1}) + b\_{c} \\right)\\\
h\_{t} & = \\left(1 - z\_{t}\\right) \\circ h\_{t - 1} + z\_{t} \\circ \\hat{h}\_t \\end{aligned}\\end{equation}
初始阶段，所有门控都可以近似视为$\\frac{1}{2}$，那么模仿式$\\eqref{eq:rnn-approx}$有
\\begin{equation}\\begin{aligned}
h\_{t} &\\, = \\left(1 - z\_{t}\\right) \\circ h\_{t - 1} + z\_{t} \\circ \\hat{h}\_t \\\
&\\, \\approx \\frac{1}{2} h\_{t - 1} + \\frac{1}{2} \\hat{h}\_t \\\
&\\, \\approx \\frac{1}{2} h\_{t - 1} + \\frac{1}{2} \\left(\\tanh ( W\_{h} x\_{t} + b\_{c} ) + \\frac{1}{2}U\_{h} h\_{t - 1}\\right) \\\
&\\, = \\frac{1}{2} \\left(I + \\frac{1}{2}U\_{h}\\right)h\_{t - 1} + \\frac{1}{2} \\tanh ( W\_{h} x\_{t} + b\_{c} ) \\\
\\end{aligned}\\end{equation}
所以可以选取$A=\\frac{1}{2} \\left(I + \\frac{1}{2}U\_{h}\\right)$，将GRU改写为迭代
\\begin{equation}\\begin{aligned} z\_{t}^{(n)} & = \\sigma \\left( W\_{z} x\_{t} + U\_{z} h\_{t - 1}^{(n-1)} + b\_{z} \\right) \\\
r\_{t}^{(n)} & = \\sigma \\left( W\_{r} x\_{t} + U\_{r} h\_{t - 1}^{(n-1)} + b\_{r} \\right) \\\
\\hat{h}\_t^{(n)} & = \\tanh \\left( W\_{h} x\_{t} + U\_{h} (r\_t^{(n)} \\circ h\_{t - 1}^{(n-1)}) + b\_{c} \\right)\\\
h\_{t}^{(n)} & = Ah\_{t-1}^{(n)} - Ah\_{t-1}^{(n - 1)} + \\left(1 - z\_{t}^{(n)}\\right) \\circ h\_{t - 1}^{(n-1)} + z\_{t}^{(n)} \\circ \\hat{h}\_t^{(n)} \\end{aligned}\\end{equation}

总的来说，这种将非线性RNN变为线性RNN迭代的转换，从实践的角度来看，就是以非线性RNN为引，导出一种多层线性RNN的参数共享和组合方法，迭代了几次，那么就有几层线性RNN的计算量。这样自然而言就引发了一个思考：除非可以证明GRU、LSTM等非线性RNN有绝对的优势，否则直接叠加几层“线性RNN+MLP”不好吗？

## 文章小结 [\#](https://kexue.fm/kexue.fm\#%E6%96%87%E7%AB%A0%E5%B0%8F%E7%BB%93)

本文简单探讨了非线性RNN的并行计算问题——通过数学物理中的“摄动”思想，我们可以将非线性RNN转化为线性RNN的迭代，从而利用线性RNN的可并行性来实现非线性RNN的并行。

_**转载到请包括本文地址：** [https://kexue.fm/archives/9783](https://kexue.fm/archives/9783)_

_**更详细的转载事宜请参考：**_ [《科学空间FAQ》](https://kexue.fm/archives/6508#%E6%96%87%E7%AB%A0%E5%A6%82%E4%BD%95%E8%BD%AC%E8%BD%BD/%E5%BC%95%E7%94%A8)

**如果您还有什么疑惑或建议，欢迎在下方评论区继续讨论。**

**如果您觉得本文还不错，欢迎 [分享](https://kexue.fm/kexue.fm#share)/ [打赏](https://kexue.fm/kexue.fm#pay) 本文。打赏并非要从中获得收益，而是希望知道科学空间获得了多少读者的真心关注。当然，如果你无视它，也不会影响你的阅读。再次表示欢迎和感谢！**

打赏

微信打赏

支付宝打赏

因为网站后台对打赏并无记录，因此欢迎在打赏时候备注留言。你还可以 [**点击这里**](http://mail.qq.com/cgi-bin/qm_share?t=qm_mailme&email=tN7d1drY3drrx8H0xcWa19vZ) 或在下方评论区留言来告知你的建议或需求。

**如果您需要引用本文，请参考：**

苏剑林. (Sep. 26, 2023). 《脑洞大开：非线性RNN居然也可以并行计算？ 》\[Blog post\]. Retrieved from [https://kexue.fm/archives/9783](https://kexue.fm/archives/9783)

@online{kexuefm-9783,
        title={脑洞大开：非线性RNN居然也可以并行计算？},
        author={苏剑林},
        year={2023},
        month={Sep},
        url={\\url{https://kexue.fm/archives/9783}},
}

分类： [数学研究](https://kexue.fm/category/Mathematics), [信息时代](https://kexue.fm/category/Big-Data)    标签： [方程](https://kexue.fm/tag/%E6%96%B9%E7%A8%8B/), [迭代](https://kexue.fm/tag/%E8%BF%AD%E4%BB%A3/), [语言模型](https://kexue.fm/tag/%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/), [RNN](https://kexue.fm/tag/RNN/)[27 评论](https://kexue.fm/archives/9783#comments)

< [自然数集中 N = ab + c 时 a + b + c 的最小值](https://kexue.fm/archives/9775) \| [预训练一下，Transformer的长序列成绩还能涨不少！](https://kexue.fm/archives/9787) >

### 你也许还对下面的内容感兴趣

- [为什么DeltaNet要加L2 Normalize？](https://kexue.fm/archives/11486)
- [为什么线性注意力要加Short Conv？](https://kexue.fm/archives/11320)
- [矩阵r次方根和逆r次方根的高效计算](https://kexue.fm/archives/11175)
- [矩阵平方根和逆平方根的高效计算](https://kexue.fm/archives/11158)
- [Transformer升级之路：21、MLA好在哪里?（下）](https://kexue.fm/archives/11111)
- [“对角+低秩”三角阵的高效求逆方法](https://kexue.fm/archives/11072)
- [通过msign来计算奇异值裁剪mclip（下）](https://kexue.fm/archives/11059)
- [线性注意力简史：从模仿、创新到反哺](https://kexue.fm/archives/11033)
- [通过msign来计算奇异值裁剪mclip（上）](https://kexue.fm/archives/11006)
- [msign算子的Newton-Schulz迭代（下）](https://kexue.fm/archives/10996)

[发表你的看法](https://kexue.fm/kexue.fm#comment_form)

hazdzz

September 26th, 2023

Google 最近有提出 TSMixer (https://openreview.net/forum?id=wbpxTuXgm0)。

[回复评论](https://kexue.fm/archives/9783/comment-page-1?replyTo=22802#respond-post-9783)

[苏剑林](https://kexue.fm) 发表于
September 28th, 2023

扫了一眼，似乎不是特别有意思。

[回复评论](https://kexue.fm/archives/9783/comment-page-1?replyTo=22821#respond-post-9783)

hazdzz

September 26th, 2023

最近這幾年有一個主張是透過 dynamical system 把 RNN 和 ODE/PDE/SDE 聯繫在一起。

[回复评论](https://kexue.fm/archives/9783/comment-page-1?replyTo=22803#respond-post-9783)

tsinglin 发表于
September 27th, 2023

您好，具体哪些 dynamical system？ 方便列一些吗

[回复评论](https://kexue.fm/archives/9783/comment-page-1?replyTo=22815#respond-post-9783)

[苏剑林](https://kexue.fm) 发表于
September 28th, 2023

这个联系比较自然吧。就本文提到的论文而言，它主要就是做ODE/PDE的，看上去做RNN只是顺便。

[回复评论](https://kexue.fm/archives/9783/comment-page-1?replyTo=22822#respond-post-9783)

[脑洞大开：非线性RNN居然也可以并行计算？ R11; AI 資訊](https://news.aitime.space/2023/09/56428/)

September 26th, 2023

\[...\]​Read More \[...\]

[回复评论](https://kexue.fm/archives/9783/comment-page-1?replyTo=22805#respond-post-9783)

hazdzz

September 29th, 2023

爲甚麼 公式 (7) 的中只需要初始化保證 $\\mathbf{P}\\mathbf{Q} = \\mathbf{\\Lambda}$ 就可以了？ Neural Networks 在訓練的過程中，weight tensor 不是會一直改變的嗎？$\\mathbf{P}\\mathbf{Q} = \\mathbf{\\Lambda}$ 在訓練過程中可能會無法保證也沒關係？

[回复评论](https://kexue.fm/archives/9783/comment-page-1?replyTo=22830#respond-post-9783)

[苏剑林](https://kexue.fm) 发表于
October 6th, 2023

如果训练过程不保证$\\boldsymbol{P}\\boldsymbol{Q} = \\boldsymbol{\\Lambda}$，可能是因为优化器觉得$\\boldsymbol{P}\\boldsymbol{Q} \\neq \\boldsymbol{\\Lambda}$更好，所以并没有什么关系。

[回复评论](https://kexue.fm/archives/9783/comment-page-1?replyTo=22845#respond-post-9783)

wangshuai

October 10th, 2023

hello，从感受野的角度来看，迭代的过程，$x\_t$要看到$x\_0$，不是整个不动点迭代要跑t个step吗，这和原来的rnn比，为什么能快呢？

[回复评论](https://kexue.fm/archives/9783/comment-page-1?replyTo=22873#respond-post-9783)

hazdzz 发表于
October 10th, 2023

可以用 prefix sum 來並行運算 RNN，這樣就比原先衹能迭代運算 RNN 要快很多。

[回复评论](https://kexue.fm/archives/9783/comment-page-1?replyTo=22874#respond-post-9783)

[苏剑林](https://kexue.fm) 发表于
October 12th, 2023

线性RNN是可以并行的（参考 [https://kexue.fm/archives/9554](https://kexue.fm/archives/9554) ），理想情况下复杂度是$\\mathcal{O}(\\log t)$，所以总的复杂度再乘以迭代步数，因此时间上有可能少于直接计算非线性RNN。

[回复评论](https://kexue.fm/archives/9783/comment-page-1?replyTo=22879#respond-post-9783)

jimmylihui

October 18th, 2023

这个(16)是怎么推导出来的,有大神能解释一下细节吗

[回复评论](https://kexue.fm/archives/9783/comment-page-1?replyTo=22907#respond-post-9783)

[苏剑林](https://kexue.fm) 发表于
October 21st, 2023

所有的中间变量都用上一步的迭代结果计算，然后用两端减去一阶近似的方式去构造最终的迭代格式，保证$^{(n-1)}$替换为$^{(n)}$时，退化为原本的非线性RNN。

[回复评论](https://kexue.fm/archives/9783/comment-page-1?replyTo=22922#respond-post-9783)

hazdzz

November 12th, 2023

令 $\\mathbf{P}$ 爲 learnable orthogonal matrix 即可，比如可以用 expRNN 裡提出的方法。

[回复评论](https://kexue.fm/archives/9783/comment-page-1?replyTo=23062#respond-post-9783)

[苏剑林](https://kexue.fm) 发表于
November 13th, 2023

这样计算复杂度又更大了？

[回复评论](https://kexue.fm/archives/9783/comment-page-1?replyTo=23071#respond-post-9783)

hazdzz 发表于
November 16th, 2023

還有一個辦法，在公式 (3) 中，用 $\\mathbf{A} = \\mathbf{F}\\mathbf{\\Lambda}\\mathbf{F}^{\\ast}$ 替代 $\\mathbf{A} = \\mathbf{P}\\mathbf{\\Lambda}\\mathbf{P}^{-1}$,其中 $\\mathbf{F}$ 是 DFT 矩陣。從 signal processing 的角度來看，$\\mathbf{P}^{-1}$ 和 $\\mathbf{P}$ 的作用是對 $\\mathbf{x}\_{t-1}$ 做 discrete orthogonal transform 和 inverse transform。用 DFT 矩陣替代之後，可以用 FFT 來加速運算。

[回复评论](https://kexue.fm/archives/9783/comment-page-1?replyTo=23083#respond-post-9783)

[苏剑林](https://kexue.fm) 发表于
November 20th, 2023

这样的话感觉干脆换用基于FFT的模型得了（捂脸）～

[回复评论](https://kexue.fm/archives/9783/comment-page-1?replyTo=23116#respond-post-9783)

hazdzz 发表于
November 22nd, 2023

說不定 FFT based Linear RNN 確實可以

[回复评论](https://kexue.fm/archives/9783/comment-page-1?replyTo=23152#respond-post-9783)

hazdzz 发表于
March 11th, 2024

還真有這樣的模型（https://openreview.net/forum?id=IxmWsm4xrua）

[回复评论](https://kexue.fm/archives/9783/comment-page-1?replyTo=23907#respond-post-9783)

[苏剑林](https://kexue.fm) 发表于
March 12th, 2024

这个就是广义的FFT呀，并不是反过来用FFT来加速RNN

[回复评论](https://kexue.fm/archives/9783/comment-page-1?replyTo=23918#respond-post-9783)

hazdzz 发表于
March 30th, 2024

不會。我最近在試類似的方法，時間複雜度和空間複雜度僅比O(NlogN)大一點點。

[回复评论](https://kexue.fm/archives/9783/comment-page-1?replyTo=24043#respond-post-9783)

ziyang

December 7th, 2023

有个不太懂的是，这种技术可以用于RNN的并行推理吗？还是只能用于并行训练

[回复评论](https://kexue.fm/archives/9783/comment-page-1?replyTo=23220#respond-post-9783)

[苏剑林](https://kexue.fm) 发表于
December 8th, 2023

理论上可以，但实际上有点难，因为无法提前预知结果长度。

[回复评论](https://kexue.fm/archives/9783/comment-page-1?replyTo=23238#respond-post-9783)

Roi

September 2nd, 2024

想问一下以(8)式为例，从step=n-1到step=n，右式的$x\_{t-1}^{n-1}$如何得到呢？前一个式子得到的是$x\_{t}^{n-1}-Ax\_{t-1}^{n-1}$，如果想要单独求出$x\_{t-1}^{n-1}$是不是还需要类似于递推或者开一个大数组从$x\_{1}^{n-1}$开始往后一个一个计算+存储？
如果是这样的话，对序列的初始值$x\_1$，(9)式有：$x\_1^{0}=tanh(u\_1)$（因为$x\_0 \\triangleq 0$），那就意味着$x\_1$是固定且无损的，不会随着迭代而更新，导致后面所有的$x\_{t}^{n}$都无法更新了？

[回复评论](https://kexue.fm/archives/9783/comment-page-1?replyTo=25146#respond-post-9783)

[苏剑林](https://kexue.fm) 发表于
September 6th, 2024

第一步，$x\_t^{(1)} - Ax\_{t-1}^{(1)} = \\tanh(u\_t)$，这就是个线性RNN，可以并行求出$x\_1^{(1)},x\_2^{(1)},x\_3^{(1)},\\cdots$；

第二步，$x\_t^{(2)} - Ax\_{t-1}^{(2)} = \\tanh(Ax\_{t-1}^{(1)} + u\_t) - Ax\_{t-1}^{(1)}$，其中所有的$x\_1^{(1)},x\_2^{(1)},x\_3^{(1)},\\cdots$已经在上一步求出，因此也可以并行求出$x\_1^{(2)},x\_2^{(2)},x\_3^{(2)},\\cdots$；

依此类推～

[回复评论](https://kexue.fm/archives/9783/comment-page-1?replyTo=25158#respond-post-9783)

Roi 发表于
September 8th, 2024

嗯嗯，这个逻辑乍一看很显然，但是感觉细想还是不太对。$n$迭代过程中$x^{(n)}$能够产生更新的原因还是因为真值在随着$n$的迭代的过程中在跟着$t$进行传递吧，这样理论上$n=t$才可以收敛？
第一步$x\_1^{(1)}=tanh(u\_1)$已经是真值$x\_1$了，而且这个真值每一步迭代都是这样的形式，不会改变（$x\_1$是首项，即$x\_0 \\triangleq 0$，因此$x\_1$的递推式和后面的不同）；第二步这个真值根据$x\_2^{(2)}-Ax\_1^{(2)}=tanh(Ax\_1^{(1)}+u\_2)-Ax\_1^{(1)}$，由于$x\_1^{(1)}=x\_1^{(2)}$，所以$x\_2^{(2)}=tanh(Ax\_1^{(1)}+u\_2)=tanh(Ax\_1+u\_2)$，也即正确的$x\_1$在迭代第二步$n=2$的时候传递到了$x\_2$处，使得$x\_2$也获得了真值。
之后类似，整体的迭代的收敛性其实是因为真值在随着$n$根据$t$来逐步往后传的，如果真值传不到（$n \\ll t\_{length}$），误差会很大吧？

[回复评论](https://kexue.fm/archives/9783/comment-page-1?replyTo=25174#respond-post-9783)

[苏剑林](https://kexue.fm) 发表于
September 9th, 2024

所以你的意思是想要表达这样做的误差可能很大，以及你为什么觉得误差大的一些看法？

如果这样的话，那这个问题确实无解了，要严格保证它收敛于原始非线性RNN还是要静心调试的，我更倾向于将它当成一种复杂线性RNN的构建方式，事后就当它线性RNN来用，保持训练和预测的一致性。

[回复评论](https://kexue.fm/archives/9783/comment-page-1?replyTo=25189#respond-post-9783)

[取消回复](https://kexue.fm/archives/9783#respond-post-9783)

你的大名

电子邮箱

个人网站（选填）

1\. 可以使用LaTeX代码，点击“预览效果”可查看效果；2. 可以通过点击评论楼层编号来引用该楼层；3. 网站可能会有点卡，如非确认评论失败，请 **不要重复点击提交**。

### 内容速览

[求不动点](https://kexue.fm/kexue.fm#%E6%B1%82%E4%B8%8D%E5%8A%A8%E7%82%B9)
[简化形式](https://kexue.fm/kexue.fm#%E7%AE%80%E5%8C%96%E5%BD%A2%E5%BC%8F)
[摄动思想](https://kexue.fm/kexue.fm#%E6%91%84%E5%8A%A8%E6%80%9D%E6%83%B3)
[加快收敛](https://kexue.fm/kexue.fm#%E5%8A%A0%E5%BF%AB%E6%94%B6%E6%95%9B)
[何必收敛](https://kexue.fm/kexue.fm#%E4%BD%95%E5%BF%85%E6%94%B6%E6%95%9B)
[一般情形](https://kexue.fm/kexue.fm#%E4%B8%80%E8%88%AC%E6%83%85%E5%BD%A2)
[文章小结](https://kexue.fm/kexue.fm#%E6%96%87%E7%AB%A0%E5%B0%8F%E7%BB%93)

### 智能搜索

支持整句搜索！网站自动使用 [结巴分词](https://github.com/fxsjy/jieba) 进行分词，并结合ngrams排序算法给出合理的搜索结果。

### 热门标签

[生成模型](https://kexue.fm/tag/%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B/) [attention](https://kexue.fm/tag/attention/) [优化](https://kexue.fm/tag/%E4%BC%98%E5%8C%96/) [语言模型](https://kexue.fm/tag/%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/) [模型](https://kexue.fm/tag/%E6%A8%A1%E5%9E%8B/) [网站](https://kexue.fm/tag/%E7%BD%91%E7%AB%99/) [梯度](https://kexue.fm/tag/%E6%A2%AF%E5%BA%A6/) [概率](https://kexue.fm/tag/%E6%A6%82%E7%8E%87/) [矩阵](https://kexue.fm/tag/%E7%9F%A9%E9%98%B5/) [优化器](https://kexue.fm/tag/%E4%BC%98%E5%8C%96%E5%99%A8/) [转载](https://kexue.fm/tag/%E8%BD%AC%E8%BD%BD/) [微分方程](https://kexue.fm/tag/%E5%BE%AE%E5%88%86%E6%96%B9%E7%A8%8B/) [分析](https://kexue.fm/tag/%E5%88%86%E6%9E%90/) [天象](https://kexue.fm/tag/%E5%A4%A9%E8%B1%A1/) [深度学习](https://kexue.fm/tag/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/) [积分](https://kexue.fm/tag/%E7%A7%AF%E5%88%86/) [python](https://kexue.fm/tag/python/) [扩散](https://kexue.fm/tag/%E6%89%A9%E6%95%A3/) [力学](https://kexue.fm/tag/%E5%8A%9B%E5%AD%A6/) [无监督](https://kexue.fm/tag/%E6%97%A0%E7%9B%91%E7%9D%A3/) [几何](https://kexue.fm/tag/%E5%87%A0%E4%BD%95/) [节日](https://kexue.fm/tag/%E8%8A%82%E6%97%A5/) [生活](https://kexue.fm/tag/%E7%94%9F%E6%B4%BB/) [文本生成](https://kexue.fm/tag/%E6%96%87%E6%9C%AC%E7%94%9F%E6%88%90/) [数论](https://kexue.fm/tag/%E6%95%B0%E8%AE%BA/)

### 随机文章

- [节省显存的重计算技巧也有了Keras版了](https://kexue.fm/archives/7367)
- [两本天体力学的旧书...](https://kexue.fm/archives/464)
- [《量子力学与路径积分》习题解答V0.5](https://kexue.fm/archives/3692)
- [【不可思议的Word2Vec】 3.提取关键词](https://kexue.fm/archives/4316)
- [科学空间：2010年9月重要天象](https://kexue.fm/archives/909)
- [生成扩散模型漫谈（二）：DDPM = 自回归式VAE](https://kexue.fm/archives/9152)
- [《新理解矩阵2》：矩阵是什么？](https://kexue.fm/archives/1768)
- [积分梯度：一种新颖的神经网络可视化方法](https://kexue.fm/archives/7533)
- [圆周率节快乐！\|\| 原来已经写了十年博客～](https://kexue.fm/archives/6469)
- [配置不同的学习率，LoRA还能再涨一点？](https://kexue.fm/archives/10001)

### 最近评论

- [kaiyuan](https://kexue.fm/archives/11486/comment-page-1#comment-29036): 看了“Linear Transformers Are Secretly Fast Weight...
- [sog](https://kexue.fm/archives/11469/comment-page-1#comment-29035): 好的，符号相同，搞混了呃
- [kerry](https://kexue.fm/archives/9119/comment-page-14#comment-29034): 还没有通读完后面的系列，提出一些拙见。
降低方差这一节把原本的目标“预测单步的noise”变成...
- [Kevin Yin](https://kexue.fm/archives/11158/comment-page-1#comment-29033): I wrote https://research.novelai.net/muonscale/...
- [罗](https://kexue.fm/archives/11480/comment-page-1#comment-29032): 公式(6)显示出来是不是有点小问题？
- [cmlin](https://kexue.fm/archives/11340/comment-page-1#comment-29031): 本人对这方面不太熟悉，想了解这三个条件的意义及动机，且希望这系列可以继续写下去。以下想发表一些...
- [喝一口可乐](https://kexue.fm/archives/10958/comment-page-3#comment-29030): 理解了，感谢苏神回复，数学上给出建模分析确实清晰了很多，再次感谢苏神回复！
- [CuddleSabe1](https://kexue.fm/archives/10958/comment-page-1#comment-29029): 感觉普通的 flow matching 可以看成 degrade-aware image de...
- [岁月如书](https://kexue.fm/archives/11126/comment-page-3#comment-29028): 受教了，感谢
- [苏剑林](https://kexue.fm/archives/11126/comment-page-3#comment-29027): 是

### 友情链接

- [Cool Papers](https://papers.cool)
- [数学研发](https://bbs.emath.ac.cn)
- [Seatop](http://www.seatop.com.cn/)
- [Xiaoxia](https://xiaoxia.org/)
- [积分表-网络版](https://kexue.fm/sci/integral/index.html)
- [丝路博傲](http://blog.dvxj.com/)
- [数学之家](http://www.2math.cn/)
- [有趣天文奇观](http://interesting-sky.china-vo.org/)
- [TwistedW](http://www.twistedwg.com/)
- [godweiyang](https://godweiyang.com/)
- [AI柠檬](https://blog.ailemon.net/)
- [王登科-DK博客](https://greatdk.com)
- [ESON](https://blog.eson.org/)
- [枫之羽](https://fzhiy.net/)
- [coding-zuo](https://coding-zuo.github.io/)
- [博科园](https://www.bokeyuan.net/)
- [孔皮皮的博客](https://www.kppkkp.top/)
- [运鹏的博客](https://yunpengtai.top/)
- [jiming.site](https://jiming.site/)
- [OmegaXYZ](https://www.omegaxyz.com/)
- [EAI猩球](https://www.robotech.ink/)
- [文举的博客](https://liwenju0.com/)
- [申请链接](https://kexue.fm/links.html)

本站采用创作共用版权协议，要求署名、非商业用途和保持一致。转载本站内容必须也遵循“ [署名-非商业用途-保持一致](http://creativecommons.org/licenses/by-nc-nd/2.5/cn/)”的创作共用协议。
© 2009-2025 Scientific Spaces. All rights reserved. Theme by [laogui](http://www.laogui.com). Powered by [Typecho](http://typecho.org). 备案号: [粤ICP备09093259号-1/2](https://beian.miit.gov.cn/)。