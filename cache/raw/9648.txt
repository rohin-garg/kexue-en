## SEARCH

## MENU

- [打赏](https://kexue.fm/reward.html)
- [公式](https://kexue.fm/latex.html)
- [天象](https://kexue.fm/ac.html)
- [链接](https://kexue.fm/links.html)
- [时光](https://kexue.fm/me.html)
- [博览](https://kexue.fm/science.html)
- [归档](https://kexue.fm/content.html)

## CATEGORIES

- [千奇百怪](https://kexue.fm/category/Everything)
- [天文探索](https://kexue.fm/category/Astronomy)
- [数学研究](https://kexue.fm/category/Mathematics)
- [物理化学](https://kexue.fm/category/Phy-chem)
- [信息时代](https://kexue.fm/category/Big-Data)
- [生物自然](https://kexue.fm/category/Biology)
- [图片摄影](https://kexue.fm/category/Photograph)
- [问题百科](https://kexue.fm/category/Questions)
- [生活/情感](https://kexue.fm/category/Life-Feeling)
- [资源共享](https://kexue.fm/category/Resources)

## NEWPOSTS

- [msign算子的Newton-Sc...](https://kexue.fm/archives/10996)
- [从无穷范数求导到等值振荡定理](https://kexue.fm/archives/10972)
- [生成扩散模型漫谈（三十）：从瞬时速...](https://kexue.fm/archives/10958)
- [MoE环游记：5、均匀分布的反思](https://kexue.fm/archives/10945)
- [msign算子的Newton-Sc...](https://kexue.fm/archives/10922)
- [Transformer升级之路：2...](https://kexue.fm/archives/10907)
- [一道概率不等式：盯着它到显然成立为止！](https://kexue.fm/archives/10902)
- [SVD的导数](https://kexue.fm/archives/10878)
- [智能家居之手搓一套能接入米家的零冷水装置](https://kexue.fm/archives/10869)
- [Transformer升级之路：1...](https://kexue.fm/archives/10862)

## COMMENTS

- [PengchengMa: 牛啊](https://kexue.fm/archives/10996/comment-page-1#comment-27811)
- [xczh: 已使用mean flow policy，一步推理效果确实惊人，...](https://kexue.fm/archives/10958/comment-page-1#comment-27810)
- [Cosine: 是不是因为shared experts每次都激活，而route...](https://kexue.fm/archives/10945/comment-page-1#comment-27809)
- [rpsun: 这样似乎与传统的经验正交函数之类的有相似之处。把样本的平均值减...](https://kexue.fm/archives/10699/comment-page-1#comment-27808)
- [贵阳机场接机: 怎么不更新啦](https://kexue.fm/archives/1490/comment-page-1#comment-27807)
- [czvzb: 具身智能模型目前主流也是在使用扩散和流匹配这类方法来预测动作。...](https://kexue.fm/archives/10958/comment-page-1#comment-27806)
- [Shawn\_yang: 不好意思，以为网页卡了0.0点了三下](https://kexue.fm/archives/10945/comment-page-1#comment-27805)
- [Shawn\_yang: 苏神，关于您所说的：“推理阶段可以事先预估Routed Exp...](https://kexue.fm/archives/10945/comment-page-1#comment-27804)
- [Shawn\_yang: 苏神，关于您所说的：“推理阶段可以事先预估Routed Exp...](https://kexue.fm/archives/10945/comment-page-1#comment-27803)
- [Shawn\_yang: 苏神，关于您所说的：“推理阶段可以事先预估Routed Exp...](https://kexue.fm/archives/10945/comment-page-1#comment-27802)

## USERLOGIN

- [登录](https://kexue.fm/admin/login.php)

[科学空间\|Scientific Spaces](https://kexue.fm)

- [登录](https://kexue.fm/admin/login.php)
- [打赏](https://kexue.fm/reward.html)
- [公式](https://kexue.fm/latex.html)
- [天象](https://kexue.fm/ac.html)
- [链接](https://kexue.fm/links.html)
- [时光](https://kexue.fm/me.html)
- [博览](https://kexue.fm/science.html)
- [归档](https://kexue.fm/content.html)

渴望成为一个小飞侠

- [欢迎订阅](https://kexue.fm/feed)
- [个性邮箱](https://kexue.fm/archives/119)
- [天象信息](https://kexue.fm/ac.html)
- [观测ISS](https://kexue.fm/archives/41)
- [LaTeX](https://kexue.fm/latex.html)
- [关于博主](https://kexue.fm/me.html)

欢迎访问“科学空间”，这里将与您共同探讨自然科学，回味人生百态；也期待大家的分享～

- [**千奇百怪** Everything](https://kexue.fm/category/Everything)
- [**天文探索** Astronomy](https://kexue.fm/category/Astronomy)
- [**数学研究** Mathematics](https://kexue.fm/category/Mathematics)
- [**物理化学** Phy-chem](https://kexue.fm/category/Phy-chem)
- [**信息时代** Big-Data](https://kexue.fm/category/Big-Data)
- [**生物自然** Biology](https://kexue.fm/category/Biology)
- [**图片摄影** Photograph](https://kexue.fm/category/Photograph)
- [**问题百科** Questions](https://kexue.fm/category/Questions)
- [**生活/情感** Life-Feeling](https://kexue.fm/category/Life-Feeling)
- [**资源共享** Resources](https://kexue.fm/category/Resources)

- [**千奇百怪**](https://kexue.fm/category/Everything)
- [**天文探索**](https://kexue.fm/category/Astronomy)
- [**数学研究**](https://kexue.fm/category/Mathematics)
- [**物理化学**](https://kexue.fm/category/Phy-chem)
- [**信息时代**](https://kexue.fm/category/Big-Data)
- [**生物自然**](https://kexue.fm/category/Biology)
- [**图片摄影**](https://kexue.fm/category/Photograph)
- [**问题百科**](https://kexue.fm/category/Questions)
- [**生活/情感**](https://kexue.fm/category/Life-Feeling)
- [**资源共享**](https://kexue.fm/category/Resources)

[首页](https://kexue.fm) [信息时代](https://kexue.fm/category/Big-Data) Naive Bayes is all you need ?

8Jun

# [Naive Bayes is all you need ?](https://kexue.fm/archives/9648)

By 苏剑林 \|
2023-06-08 \|
61484位读者\|

很抱歉，起了这么个具有标题党特征的题目。在写完 [《NBCE：使用朴素贝叶斯扩展LLM的Context处理长度》](https://kexue.fm/archives/9617) 之后，笔者就觉得朴素贝叶斯（Naive Bayes）跟Attention机制有很多相同的特征，后来再推导了一下发现，Attention机制其实可以看成是一种广义的、参数化的朴素贝叶斯。既然如此，“ [Attention is All You Need](https://kexue.fm/archives/4765)”不也就意味着“Naive Bayes is all you need”了？这就是本文标题的缘由。

接下来笔者将介绍自己的思考过程，分析如何从朴素贝叶斯角度来理解Attention机制。

## 朴素贝叶斯 [\#](https://kexue.fm/archives/9648\#%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF)

本文主要考虑语言模型，它要建模的是$p(x\_t\|x\_1,\\cdots,x\_{t-1})$。根据贝叶斯公式，我们有
\\begin{equation}p(x\_t\|x\_1,\\cdots,x\_{t-1}) = \\frac{p(x\_1,\\cdots,x\_{t-1}\|x\_t)p(x\_t)}{p(x\_1,\\cdots,x\_{t-1})}\\propto p(x\_1,\\cdots,x\_{t-1}\|x\_t)p(x\_t)\\end{equation}
根据独立假设$p(x\_1,\\cdots,x\_{t-1}\|x\_t) = \\prod\\limits\_{j=1}^{t-1} p(x\_j\|x\_t)$，我们有
\\begin{equation}p(x\_t\|x\_1,\\cdots,x\_{t-1}) \\propto \\prod\_{j=1}^{t-1} p(x\_j\|x\_t)p(x\_t)\\end{equation}
再次根据贝叶斯公式$p(x\_j\|x\_t)=\\frac{p(x\_t\|x\_j)p(x\_j)}{p(x\_t)}\\propto\\frac{p(x\_t\|x\_j)}{p(x\_t)}$，得到
\\begin{equation}p(x\_t\|x\_1,\\cdots,x\_{t-1}) \\propto \\frac{1}{\[p(x\_t)\]^{t-2}}\\prod\_{j=1}^{t-1} p(x\_t\|x\_j)\\end{equation}
两边取对数得到
\\begin{equation}\\log p(x\_t\|x\_1,\\cdots,x\_{t-1}) = \\sum\_{j=1}^{t-1}\\log p(x\_t\|x\_j) - (t - 2) \\log p(x\_t) + \\text{常数}\\end{equation}

## 一般化结果 [\#](https://kexue.fm/archives/9648\#%E4%B8%80%E8%88%AC%E5%8C%96%E7%BB%93%E6%9E%9C)

相同的推导我们在 [《NBCE：使用朴素贝叶斯扩展LLM的Context处理长度》](https://kexue.fm/archives/9617) 也进行过，跟该文章一样，我们将上式一般化为：
\\begin{equation}\\log p(x\_t\|x\_1,\\cdots,x\_{t-1}) = (1 + \\beta)\\mathcal{P}\[\\log p(x\_t\|x\_j)\] - \\beta \\log p(x\_t) + \\text{常数}\\end{equation}
这里的$\\beta$作为超参数来调，$\\mathcal{P}$是某种Pooling方式。接下来我们主要看$\\beta=0$、以加权平均为Pooling的例子，即
\\begin{equation}\\log p(x\_t\|x\_1,\\cdots,x\_{t-1}) = \\sum\_j a\_{t,j} \\log p(x\_t\|x\_j) + \\text{常数}\\label{eq:nb-core}\\end{equation}
这里的$a\_{t,j}$是$x\_{t-1}$与$x\_j$的函数。

可能有读者想问，这个一般化的式子还能算是朴素贝叶斯吗？笔者认为它可以作为广义的朴素贝叶斯来看待，因为朴素贝叶斯可以视为各个$\\log p(x\_t\|x\_j)$的等权平均，这里则是换成了更一般化的加权平均。不过，将$a\_{t,j}$选取为$x\_{t-1}$与$x\_j$的函数，突出了$x\_{t-1}$的地位，改善了朴素贝叶斯的无序性这一弊端。所以更准确来说，式$\\eqref{eq:nb-core}$是2-gram语言模型与朴素贝叶斯的结合。

## 注意力初现 [\#](https://kexue.fm/archives/9648\#%E6%B3%A8%E6%84%8F%E5%8A%9B%E5%88%9D%E7%8E%B0)

接下来，将$\\log p(x\_t\|x\_j)$进一步参数化，我们就可以得见Attention的形式了。不难发现，$p(x\_t\|x\_j)$实质上就是以前Word2Vec的Skip Gram模型，它的常规建模方式是“Embedding + 内积 + Softmax”，即
\\begin{equation}p(x\_t\|x\_j) = \\frac{e^{v(x\_j)\\cdot w(x\_t)}}{Z(x\_j)},\\quad Z(x\_j) = \\sum\_{x\_t\\in Vocab}e^{v(x\_j)\\cdot w(x\_t)}\\end{equation}
所以我们简单地认为
\\begin{equation}\\log p(x\_t\|x\_j) = v(x\_j)\\cdot w(x\_t) + \\text{常数}\\end{equation}
代入到式$\\eqref{eq:nb-core}$，得到
\\begin{equation}\\log p(x\_t\|x\_1,\\cdots,x\_{t-1}) = \\left(\\sum\_j a\_{t,j} v(x\_j)\\right)\\cdot w(x\_t) + \\text{常数}\\label{eq:nb-core-2}\\end{equation}
括号中的式子，我们将它单独拿出来，当作通常用特征融合运算，它其实就是常规的Attention。所以说，单层的Attention做语言模型，实则就是广义的朴素贝叶斯。

当然，这里我们还没有将$a\_{t,j}$确定下来。上一节我们说$a\_{t,j}$是$x\_{t-1}$与$x\_j$的函数，然后同时还要归一化（加权平均），所以比较简单的方式就是像Skip Gram一样“Embedding + 内积 + Softmax”：
\\begin{equation}a\_{t,j} = \\frac{e^{q(x\_{t-1})\\cdot k(x\_j)}}{Z\_t},\\quad Z\_t = \\sum\_{j=1}^{t-1} e^{q(x\_{t-1})\\cdot k(x\_j)}\\end{equation}
代入到式$\\eqref{eq:nb-core-2}$，就是目前最常用的Dot-Product Attention了。当然，这种方式不是唯一的，还有加性Attention等，选择Dot-Product的最主要原因是它可以在比较省显存的前提下实现并行。

## 层叠与残差 [\#](https://kexue.fm/archives/9648\#%E5%B1%82%E5%8F%A0%E4%B8%8E%E6%AE%8B%E5%B7%AE)

不管怎么参数化，单层的朴素贝叶斯能力总是有限的，所以需要进一步提高模型的复杂度。从神经网络的角度来看，提高模型复杂度的主要方式是增加深度，也就是层与层之间的堆叠。那么，从概率分布的角度如何理解这种堆叠呢？答案是隐变量模型。

所谓隐变量模型，就是引入隐变量$z\_1,z\_2,\\cdots,z\_{t-1}$，使得
\\begin{equation}p(x\_t\|x\_1,\\cdots,x\_{t-1}) = \\int p(x\_t\|z\_1,\\cdots,z\_{t-1})p(z\_1,\\cdots,z\_{t-1}\|x\_1,\\cdots,x\_{t-1})dz\_1 \\cdots dz\_{t-1}\\end{equation}
说白了，就是通过简单分布的叠加来拟合更复杂的分布，跟GMM（高斯混合模型）的思想是一致的。基于前面的讨论，$p(x\_t\|z\_1,\\cdots,z\_{t-1})$我们同样用朴素贝叶斯建模，即从特征层面就是单层Attention。而对于$p(z\_1,\\cdots,z\_{t-1}\|x\_1,\\cdots,x\_{t-1})$，我们按照自回归模型的特点，分解为
\\begin{equation}p(z\_1,\\cdots,z\_{t-1}\|x\_1,\\cdots,x\_{t-1}) = \\prod\_{k=1}^{t-1} p(z\_k\|x\_1,\\cdots,x\_k)\\end{equation}
这样每个$p(z\_k\|x\_1,\\cdots,x\_k)$形式上就跟$p(x\_t\|z\_1,\\cdots,z\_{t-1})$一样了，于是同样可以用朴素贝叶斯建模。简单起见，$z\_k$我们定义为连续型变量，$p(z\_k\|x\_1,\\cdots,x\_k)$则定义为 [狄拉克分布](https://kexue.fm/archives/1870)，于是积分可以直接算出来，结果就是两层Attention的堆叠了。

最后，Transfromer中还有一个关键成分是残差，实际上它就是将式$\\eqref{eq:nb-core}$一般化为
\\begin{equation}\\log p(x\_t\|x\_1,\\cdots,x\_{t-1}) = \\log p(x\_t\|x\_{t-1}) + \\sum\_j a\_{t,j} \\log p(x\_t\|x\_j) + \\text{常数}\\end{equation}
可以理解为一种突出了2-gram的地位的Pooling方式，算是一种先验。最后，还剩下的FeedForward层、LayerNorm层等，这些层不涉及token之间的交互，可以理解为是更复杂地参数化的朴素贝叶斯。

当然，这样笼统的解释看上去有些勉强，但笔者原本的想法，也不是精准地解释Transformer或Attention，而是期望是能从朴素贝叶斯角度来够获得一些关于长度外推的新思路。但很遗憾，目前笔者还没有得到预期的结果。然而，尽管看上去像是盲目的自恋，但笔者依然认为上述朴素贝叶斯和隐变量模型的视角还有进一步挖掘的潜力，比如看上去我们可以从朴素贝叶斯角度解释基于Attention的语言模型的In-Context Learning为啥会有效。

## 文章总概述 [\#](https://kexue.fm/archives/9648\#%E6%96%87%E7%AB%A0%E6%80%BB%E6%A6%82%E8%BF%B0)

本文阐述了朴素贝叶斯与Attention机制之间的关联，显示了Attention可被视为一种广义的朴素贝叶斯。从这个视角，我们还可以进一步地理解Attention中的层叠与残差等内容。

_**转载到请包括本文地址：** [https://kexue.fm/archives/9648](https://kexue.fm/archives/9648)_

_**更详细的转载事宜请参考：**_ [《科学空间FAQ》](https://kexue.fm/archives/6508#%E6%96%87%E7%AB%A0%E5%A6%82%E4%BD%95%E8%BD%AC%E8%BD%BD/%E5%BC%95%E7%94%A8)

**如果您还有什么疑惑或建议，欢迎在下方评论区继续讨论。**

**如果您觉得本文还不错，欢迎 [分享](https://kexue.fm/archives/9648#share)/ [打赏](https://kexue.fm/archives/9648#pay) 本文。打赏并非要从中获得收益，而是希望知道科学空间获得了多少读者的真心关注。当然，如果你无视它，也不会影响你的阅读。再次表示欢迎和感谢！**

打赏

微信打赏

支付宝打赏

因为网站后台对打赏并无记录，因此欢迎在打赏时候备注留言。你还可以 [**点击这里**](http://mail.qq.com/cgi-bin/qm_share?t=qm_mailme&email=tN7d1drY3drrx8H0xcWa19vZ) 或在下方评论区留言来告知你的建议或需求。

**如果您需要引用本文，请参考：**

苏剑林. (Jun. 08, 2023). 《Naive Bayes is all you need ? 》\[Blog post\]. Retrieved from [https://kexue.fm/archives/9648](https://kexue.fm/archives/9648)

@online{kexuefm-9648,
        title={Naive Bayes is all you need ?},
        author={苏剑林},
        year={2023},
        month={Jun},
        url={\\url{https://kexue.fm/archives/9648}},
}

分类： [信息时代](https://kexue.fm/category/Big-Data)    标签： [语言模型](https://kexue.fm/tag/%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/), [attention](https://kexue.fm/tag/attention/), [LLM](https://kexue.fm/tag/LLM/), [贝叶斯](https://kexue.fm/tag/%E8%B4%9D%E5%8F%B6%E6%96%AF/)[27 评论](https://kexue.fm/archives/9648#comments)

< [关于NBCE方法的一些补充说明和分析](https://kexue.fm/archives/9632) \| [梯度流：探索通向最小值之路](https://kexue.fm/archives/9660) >

### 你也许还对下面的内容感兴趣

- [Transformer升级之路：20、MLA究竟好在哪里？](https://kexue.fm/archives/10907)
- [Transformer升级之路：19、第二类旋转位置编码](https://kexue.fm/archives/10862)
- [细水长flow之TARFLOW：流模型满血归来？](https://kexue.fm/archives/10667)
- [“闭门造车”之多模态思路浅谈（三）：位置编码](https://kexue.fm/archives/10352)
- [Decoder-only的LLM为什么需要位置编码？](https://kexue.fm/archives/10347)
- [Monarch矩阵：计算高效的稀疏型矩阵分解](https://kexue.fm/archives/10249)
- [Transformer升级之路：18、RoPE的底数选择原则](https://kexue.fm/archives/10122)
- [缓存与效果的极限拉扯：从MHA、MQA、GQA到MLA](https://kexue.fm/archives/10091)
- [Transformer升级之路：17、多模态位置编码的简单思考](https://kexue.fm/archives/10040)
- [时空之章：将Attention视为平方复杂度的RNN](https://kexue.fm/archives/10017)

[发表你的看法](https://kexue.fm/archives/9648#comment_form)

mututu

June 8th, 2023

minior typo in Eq.(12):

$p(z\_{1}, \\cdots, z\_{t−1} \| x\_{1}, \\cdots ,z\_{t−1}) \\rightarrow p(z\_{1}, \\cdots, z\_{t−1} \| x\_{1}, \\cdots ,x\_{t−1})$

[回复评论](https://kexue.fm/archives/9648/comment-page-1?replyTo=21896#respond-post-9648)

[苏剑林](https://kexue.fm) 发表于
June 8th, 2023

fixed, thanks.

[回复评论](https://kexue.fm/archives/9648/comment-page-1?replyTo=21901#respond-post-9648)

Monstrous Moonshine

June 8th, 2023

前几天Hacker News上刷到了一样的title: https://arxiv.org/abs/2306.00238
Bytes Are All You Need: Transformers Operating Directly On File Bytes

[回复评论](https://kexue.fm/archives/9648/comment-page-1?replyTo=21907#respond-post-9648)

[苏剑林](https://kexue.fm) 发表于
June 8th, 2023

一个是bayes，一个是bytes，貌似差得比较远。。。

[回复评论](https://kexue.fm/archives/9648/comment-page-1?replyTo=21908#respond-post-9648)

Monstrous Moonshine 发表于
June 9th, 2023

哈哈，确实，看错了

[回复评论](https://kexue.fm/archives/9648/comment-page-1?replyTo=21928#respond-post-9648)

[Naive Bayes is all you need ? R11; AI 資訊](https://news.aitime.space/2023/06/3197/)

June 8th, 2023

\[...\]​Read More \[...\]

[回复评论](https://kexue.fm/archives/9648/comment-page-1?replyTo=21911#respond-post-9648)

GPT-4

June 8th, 2023

跟第一篇文章对比，公式（5）右边第一项少了一个上横线

[回复评论](https://kexue.fm/archives/9648/comment-page-1?replyTo=21912#respond-post-9648)

[苏剑林](https://kexue.fm) 发表于
June 12th, 2023

跟那篇文章的公式$(8)$对应。

[回复评论](https://kexue.fm/archives/9648/comment-page-1?replyTo=21941#respond-post-9648)

KeyDancer

June 9th, 2023

苏神好，想问个可能比较幼稚的问题，式(2)中的独立性假设是如何得到的呢？感觉对语言模型来说可能难以分解成独立的形式。

[回复评论](https://kexue.fm/archives/9648/comment-page-1?replyTo=21924#respond-post-9648)

wuhao 发表于
June 10th, 2023

可能在冗长的上下文中，马尔可夫性远不如上下字那么重要，在这种情况的下，朴素贝叶斯已经可以将其建模出来了。

[回复评论](https://kexue.fm/archives/9648/comment-page-1?replyTo=21931#respond-post-9648)

wuhao 发表于
June 10th, 2023

但这种建模对于语句块的顺序是无法展示的，还是缺失了一定的马尔可夫性导致的。

[回复评论](https://kexue.fm/archives/9648/comment-page-1?replyTo=21932#respond-post-9648)

[苏剑林](https://kexue.fm) 发表于
June 12th, 2023

不是如何得到独立性，而是作为一个近似；它的近似程度肯定是不够好的，所以后面提出了隐变量模型，提高近似程度。举个例子，就好比$e^x\\approx 1+x$，这是一个近似，为了提高这个近似精度，我们可以补充更多的项如$\\frac{1}{2}x^2$（残差），也可以用迭代（层叠）。

[回复评论](https://kexue.fm/archives/9648/comment-page-1?replyTo=21945#respond-post-9648)

Chatty Fish

June 9th, 2023

想请教一下苏神，毕竟naive bayes太有名了，以前也有人尝试过用来解决autoregressive model的问题，比如：
Shekofteh, Y., & Almasganj, F. (2013). Autoregressive modeling of speech trajectory transformed to the reconstructed phase space for ASR purposes. Digital Signal Processing, 23(6), 1923-1932.

还有一些其他的尝试，就不一一举例了。

考虑到transformer也是一个自回归模型，这之间又有什么诡异的联系呢？

[回复评论](https://kexue.fm/archives/9648/comment-page-1?replyTo=21926#respond-post-9648)

[苏剑林](https://kexue.fm) 发表于
June 12th, 2023

你这篇paper下载要收费，我暂未能阅读。

至于你后一个问题，是不是想问这个 [https://kexue.fm/archives/9648](https://kexue.fm/archives/9648) ？

[回复评论](https://kexue.fm/archives/9648/comment-page-1?replyTo=21947#respond-post-9648)

Chatty Fish 发表于
June 13th, 2023

这里有一个PDF的，你看你那边能不能下载：
https://d1wqtxts1xzle7.cloudfront.net/54361764/j.dsp.2013.06.01120170906-20541-12kc2al-libre.pdf?1504748542=&response-content-disposition=inline%3B+filename%3DAutoregressive\_modeling\_of\_speech\_trajec.pdf&Expires=1686670350&Signature=TAi2LfLqomKU1t50R8Hkgrx19ssJ5VJxXeBFoxWCjnnJsqkyoEIorrir9gSzPr9LutGaq~VExuCgE07umdorOF5n00-Cosr1yJktr--5HQitV62p0H3GYo5V3KV58MWoIsOX8Na1tZjafGTEmHDIawachPTeMlSDQsMdpc~xG3wGoW8r7JKbnoqWWsT6WKtCIF67M~e3d2WY3GDhWRAVBf2uPPARBGSw6QXvGw5uHo1KOBcPEB2zi-05FVRN0L3MazQDyd1Z68C6dQd8xzDWYcs7nbSlIY-2o8JO8tQF~mQ2g5dxFoOjcphAwC4EJGjoGGQ7BBWzQTd1t09W2~pYfQ\_\_&Key-Pair-Id=APKAJLOHF5GGSLRBV4ZA

[回复评论](https://kexue.fm/archives/9648/comment-page-1?replyTo=21966#respond-post-9648)

[苏剑林](https://kexue.fm) 发表于
June 15th, 2023

下不了。

看标题，似乎没看出它跟naive bayes的关系，你可以稍微介绍一下它的思路。

[回复评论](https://kexue.fm/archives/9648/comment-page-1?replyTo=21989#respond-post-9648)

Chatty Fish 发表于
June 16th, 2023

我通读了一下，这篇和你做的研究应该无关：

作者的目的是提取语音信号的有效特征。

特征提取方法应用于语音信号的帧上，包括以下五个步骤：

将帧中的语音样本归一化到该帧所属的均值和方差数值。因此，归一化后的语音样本具有零均值和单位标准差。

将归一化后的语音样本嵌入到重构相空间（RPS）中，使用嵌入维度 m = 8 和时间滞后参数为 6，从而为每个帧形成一个 m 维轨迹。

通过多元自回归（MVAR）方法评估语音轨迹在RPS中的 P 个系数矩阵（其中 P 是LP模型的阶数）。通过这种方式，为每个帧评估了维度为 m × m 的 P 个矩阵。

使用线性判别分析（LDA）进行维度降低，以获得最终的特征向量。LDA技术被用于同时去相关化和降低特征集的维度。

最后，使用所得特征向量作为输入，应用朴素贝叶斯分类器（NBC）对孤立音素进行分类。NBC计算每个测试样本属于不同音素类别的后验概率，并选择具有最高后验概率的类别作为预测结果。

他们是前面用Multivariate Autoregressive模型，然后用线性判别分析（LDA）进行降维，然后送到朴素贝叶斯分类器（NBC）里面去分类。

因为用到的类似的数学工具，分析连续语音又和NLP有有些类似，数学描述上和你有些公式比较象，读过后完全不同。

[回复评论](https://kexue.fm/archives/9648/comment-page-1?replyTo=21996#respond-post-9648)

[苏剑林](https://kexue.fm) 发表于
June 19th, 2023

好的谢谢。

[回复评论](https://kexue.fm/archives/9648/comment-page-1?replyTo=22010#respond-post-9648)

Chatty Fish

June 9th, 2023

苏神要不要粉丝中招几个研究生，让学生把相关论文都扫描一遍，否则自己干太累了，以免撞上类似的公式或者硬件的数学化表述，我自愿当一个。

[回复评论](https://kexue.fm/archives/9648/comment-page-1?replyTo=21927#respond-post-9648)

[苏剑林](https://kexue.fm) 发表于
June 12th, 2023

很惭愧，我没什么组织能力（捂脸），而且很多paper不自己读一下总感觉不放心。

[回复评论](https://kexue.fm/archives/9648/comment-page-1?replyTo=21948#respond-post-9648)

Santu 发表于
July 16th, 2023

paper是我自己读的我自己都不放心，就得看苏神的讲解（doge

[回复评论](https://kexue.fm/archives/9648/comment-page-1?replyTo=22246#respond-post-9648)

y1ny

June 12th, 2023

前段时间正巧也在研究用贝叶斯的框架来解释语言模型in context learning的一些学习和泛化现象的工作，当时看到这篇文章: An Explanation of In-context Learning as Implicit Bayesian Inference (https://arxiv.org/pdf/2111.02080.pdf)

感觉和苏神的直觉有相似的地方?

[回复评论](https://kexue.fm/archives/9648/comment-page-1?replyTo=21954#respond-post-9648)

[苏剑林](https://kexue.fm) 发表于
June 15th, 2023

谢谢推荐，我抽空读读。

[回复评论](https://kexue.fm/archives/9648/comment-page-1?replyTo=21981#respond-post-9648)

wind\_

June 16th, 2023

关于公式10，有两个疑问想请教一下：
1\. a\_tj，为什么是关于x\[t-1\]和x\[j\]的函数呢？为什么不是x\[t\]和x\[j\]的？
2\. Z\_t，为什么j是从1到t-1的？为什么不是整个词表长度上的？这个疑问，我同样觉得原始Attention中注意力score做softmax，是不是应该针对整个词表？

[回复评论](https://kexue.fm/archives/9648/comment-page-1?replyTo=21998#respond-post-9648)

[苏剑林](https://kexue.fm) 发表于
June 19th, 2023

1、$x\_t$是你要预测的，不是输入，最新的token我们顶多是$x\_{t-1}$；

2、我们想要的是对输入token进行加权平均，不是词表中所有token。

[回复评论](https://kexue.fm/archives/9648/comment-page-1?replyTo=22012#respond-post-9648)

trestad

September 5th, 2023

请教一下13式残差的设计：x是token序列，而transformer里面的残差结构是在层内，为什么残差在13式中被建模为logp(xt\|xt−1) 而不是与z有关？

[回复评论](https://kexue.fm/archives/9648/comment-page-1?replyTo=22639#respond-post-9648)

[苏剑林](https://kexue.fm) 发表于
September 6th, 2023

它只是给出单个层的形式，多个层的复合是应该改成$z$相关。

[回复评论](https://kexue.fm/archives/9648/comment-page-1?replyTo=22654#respond-post-9648)

[取消回复](https://kexue.fm/archives/9648#respond-post-9648)

你的大名

电子邮箱

个人网站（选填）

1\. 可以使用LaTeX代码，点击“预览效果”可查看效果；2. 可以通过点击评论楼层编号来引用该楼层；3. 网站可能会有点卡，如非确认评论失败，请不要重复点击提交。

### 内容速览

[朴素贝叶斯](https://kexue.fm/archives/9648#%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF)
[一般化结果](https://kexue.fm/archives/9648#%E4%B8%80%E8%88%AC%E5%8C%96%E7%BB%93%E6%9E%9C)
[注意力初现](https://kexue.fm/archives/9648#%E6%B3%A8%E6%84%8F%E5%8A%9B%E5%88%9D%E7%8E%B0)
[层叠与残差](https://kexue.fm/archives/9648#%E5%B1%82%E5%8F%A0%E4%B8%8E%E6%AE%8B%E5%B7%AE)
[文章总概述](https://kexue.fm/archives/9648#%E6%96%87%E7%AB%A0%E6%80%BB%E6%A6%82%E8%BF%B0)

### 智能搜索

支持整句搜索！网站自动使用 [结巴分词](https://github.com/fxsjy/jieba) 进行分词，并结合ngrams排序算法给出合理的搜索结果。

### 热门标签

[生成模型](https://kexue.fm/tag/%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B/) [attention](https://kexue.fm/tag/attention/) [优化](https://kexue.fm/tag/%E4%BC%98%E5%8C%96/) [语言模型](https://kexue.fm/tag/%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/) [模型](https://kexue.fm/tag/%E6%A8%A1%E5%9E%8B/) [网站](https://kexue.fm/tag/%E7%BD%91%E7%AB%99/) [概率](https://kexue.fm/tag/%E6%A6%82%E7%8E%87/) [梯度](https://kexue.fm/tag/%E6%A2%AF%E5%BA%A6/) [转载](https://kexue.fm/tag/%E8%BD%AC%E8%BD%BD/) [微分方程](https://kexue.fm/tag/%E5%BE%AE%E5%88%86%E6%96%B9%E7%A8%8B/) [天象](https://kexue.fm/tag/%E5%A4%A9%E8%B1%A1/) [矩阵](https://kexue.fm/tag/%E7%9F%A9%E9%98%B5/) [分析](https://kexue.fm/tag/%E5%88%86%E6%9E%90/) [深度学习](https://kexue.fm/tag/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/) [积分](https://kexue.fm/tag/%E7%A7%AF%E5%88%86/) [python](https://kexue.fm/tag/python/) [优化器](https://kexue.fm/tag/%E4%BC%98%E5%8C%96%E5%99%A8/) [力学](https://kexue.fm/tag/%E5%8A%9B%E5%AD%A6/) [无监督](https://kexue.fm/tag/%E6%97%A0%E7%9B%91%E7%9D%A3/) [扩散](https://kexue.fm/tag/%E6%89%A9%E6%95%A3/) [几何](https://kexue.fm/tag/%E5%87%A0%E4%BD%95/) [节日](https://kexue.fm/tag/%E8%8A%82%E6%97%A5/) [生活](https://kexue.fm/tag/%E7%94%9F%E6%B4%BB/) [文本生成](https://kexue.fm/tag/%E6%96%87%E6%9C%AC%E7%94%9F%E6%88%90/) [数论](https://kexue.fm/tag/%E6%95%B0%E8%AE%BA/)

### 随机文章

- [曾被嫌弃的预训练任务NSP，做出了优秀的Zero Shot效果](https://kexue.fm/archives/8671)
- [写在2009年终结之际...](https://kexue.fm/archives/333)
- [祝大家新春愉快！虎年飞跃！](https://kexue.fm/archives/436)
- [最近广告特别多...（严厉声明）](https://kexue.fm/archives/411)
- [熵不变性Softmax的一个快速推导](https://kexue.fm/archives/9034)
- [太阳中心的压强和温度](https://kexue.fm/archives/721)
- [文本情感分类（三）：分词 OR 不分词](https://kexue.fm/archives/3863)
- [2010年诺贝尔文学奖落户秘鲁](https://kexue.fm/archives/981)
- [Transformer升级之路：6、旋转位置编码的完备性分析](https://kexue.fm/archives/9403)
- [【生物总结】到细胞内旅游](https://kexue.fm/archives/592)

### 最近评论

- [PengchengMa](https://kexue.fm/archives/10996/comment-page-1#comment-27811): 牛啊
- [xczh](https://kexue.fm/archives/10958/comment-page-1#comment-27810): 已使用mean flow policy，一步推理效果确实惊人，性能跟多步推理的diffusio...
- [Cosine](https://kexue.fm/archives/10945/comment-page-1#comment-27809): 是不是因为shared experts每次都激活，而routed experts是依概率被选中...
- [rpsun](https://kexue.fm/archives/10699/comment-page-1#comment-27808): 这样似乎与传统的经验正交函数之类的有相似之处。把样本的平均值减掉之后做正交分解。那么如果单纯地...
- [贵阳机场接机](https://kexue.fm/archives/1490/comment-page-1#comment-27807): 怎么不更新啦
- [czvzb](https://kexue.fm/archives/10958/comment-page-1#comment-27806): 具身智能模型目前主流也是在使用扩散和流匹配这类方法来预测动作。
苏神推荐你看这几篇文章：
1....
- [Shawn\_yang](https://kexue.fm/archives/10945/comment-page-1#comment-27805): 不好意思，以为网页卡了0.0点了三下
- [Shawn\_yang](https://kexue.fm/archives/10945/comment-page-1#comment-27804): 苏神，关于您所说的：“推理阶段可以事先预估Routed Expert的实际分布，只要细致地进行...
- [Shawn\_yang](https://kexue.fm/archives/10945/comment-page-1#comment-27803): 苏神，关于您所说的：“推理阶段可以事先预估Routed Expert的实际分布，只要细致地进行...
- [Shawn\_yang](https://kexue.fm/archives/10945/comment-page-1#comment-27802): 苏神，关于您所说的：“推理阶段可以事先预估Routed Expert的实际分布，只要细致地进行...

### 友情链接

- [Cool Papers](https://papers.cool)
- [数学研发](https://bbs.emath.ac.cn)
- [Seatop](http://www.seatop.com.cn/)
- [Xiaoxia](https://xiaoxia.org/)
- [积分表-网络版](https://kexue.fm/sci/integral/index.html)
- [丝路博傲](http://blog.dvxj.com/)
- [ph4ntasy 饭特稀](http://www.ph4ntasy.com/)
- [数学之家](http://www.2math.cn/)
- [有趣天文奇观](http://interesting-sky.china-vo.org/)
- [TwistedW](http://www.twistedwg.com/)
- [godweiyang](https://godweiyang.com/)
- [AI柠檬](https://blog.ailemon.net/)
- [王登科-DK博客](https://greatdk.com)
- [ESON](https://blog.eson.org/)
- [枫之羽](https://fzhiy.net/)
- [Mathor's blog](https://wmathor.com/)
- [coding-zuo](https://coding-zuo.github.io/)
- [博科园](https://www.bokeyuan.net/)
- [孔皮皮的博客](https://www.kppkkp.top/)
- [运鹏的博客](https://yunpengtai.top/)
- [jiming.site](https://jiming.site/)
- [OmegaXYZ](https://www.omegaxyz.com/)
- [Blog by Eacls](https://www.eacls.top/)
- [EAI猩球](https://www.robotech.ink/)
- [文举的博客](https://liwenju0.com/)
- [用代码打点酱油](https://bruceyuan.com/)
- [申请链接](https://kexue.fm/links.html)

本站采用创作共用版权协议，要求署名、非商业用途和保持一致。转载本站内容必须也遵循“ [署名-非商业用途-保持一致](http://creativecommons.org/licenses/by-nc-nd/2.5/cn/)”的创作共用协议。
© 2009-2025 Scientific Spaces. All rights reserved. Theme by [laogui](http://www.laogui.com). Powered by [Typecho](http://typecho.org). 备案号: [粤ICP备09093259号-1/2](https://beian.miit.gov.cn/)。