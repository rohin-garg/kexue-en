## SEARCH

## MENU

- [打赏](https://kexue.fm/reward.html)
- [公式](https://kexue.fm/latex.html)
- [天象](https://kexue.fm/ac.html)
- [链接](https://kexue.fm/links.html)
- [时光](https://kexue.fm/me.html)
- [博览](https://kexue.fm/science.html)
- [归档](https://kexue.fm/content.html)

## CATEGORIES

- [千奇百怪](https://kexue.fm/category/Everything)
- [天文探索](https://kexue.fm/category/Astronomy)
- [数学研究](https://kexue.fm/category/Mathematics)
- [物理化学](https://kexue.fm/category/Phy-chem)
- [信息时代](https://kexue.fm/category/Big-Data)
- [生物自然](https://kexue.fm/category/Biology)
- [图片摄影](https://kexue.fm/category/Photograph)
- [问题百科](https://kexue.fm/category/Questions)
- [生活/情感](https://kexue.fm/category/Life-Feeling)
- [资源共享](https://kexue.fm/category/Resources)

## NEWPOSTS

- [通过msign来计算mclip（奇...](https://kexue.fm/archives/11006)
- [msign算子的Newton-Sc...](https://kexue.fm/archives/10996)
- [从无穷范数求导到等值振荡定理](https://kexue.fm/archives/10972)
- [生成扩散模型漫谈（三十）：从瞬时速...](https://kexue.fm/archives/10958)
- [MoE环游记：5、均匀分布的反思](https://kexue.fm/archives/10945)
- [msign算子的Newton-Sc...](https://kexue.fm/archives/10922)
- [Transformer升级之路：2...](https://kexue.fm/archives/10907)
- [一道概率不等式：盯着它到显然成立为止！](https://kexue.fm/archives/10902)
- [SVD的导数](https://kexue.fm/archives/10878)
- [智能家居之手搓一套能接入米家的零冷水装置](https://kexue.fm/archives/10869)

## COMMENTS

- [PengchengMa: 牛啊](https://kexue.fm/archives/10996/comment-page-1#comment-27811)
- [xczh: 已使用mean flow policy，一步推理效果确实惊人，...](https://kexue.fm/archives/10958/comment-page-1#comment-27810)
- [Cosine: 是不是因为shared experts每次都激活，而route...](https://kexue.fm/archives/10945/comment-page-1#comment-27809)
- [rpsun: 这样似乎与传统的经验正交函数之类的有相似之处。把样本的平均值减...](https://kexue.fm/archives/10699/comment-page-1#comment-27808)
- [贵阳机场接机: 怎么不更新啦](https://kexue.fm/archives/1490/comment-page-1#comment-27807)
- [czvzb: 具身智能模型目前主流也是在使用扩散和流匹配这类方法来预测动作。...](https://kexue.fm/archives/10958/comment-page-1#comment-27806)
- [Shawn\_yang: 苏神，关于您所说的：“推理阶段可以事先预估Routed Exp...](https://kexue.fm/archives/10945/comment-page-1#comment-27802)
- [OceanYU: 您好，关于由式（7）推导出高斯分布，我这里有一点问题，式（7）...](https://kexue.fm/archives/9164/comment-page-4#comment-27801)
- [jorjiang: 训练和prefill这个compute-bound阶段不做矩阵...](https://kexue.fm/archives/10907/comment-page-2#comment-27800)
- [amy: 苏老师，您有关注傅里叶旋转位置编码这篇工作吗，想知道您对这篇工...](https://kexue.fm/archives/10907/comment-page-2#comment-27799)

## USERLOGIN

- [登录](https://kexue.fm/admin/login.php)

[科学空间\|Scientific Spaces](https://kexue.fm)

- [登录](https://kexue.fm/admin/login.php)
- [打赏](https://kexue.fm/reward.html)
- [公式](https://kexue.fm/latex.html)
- [天象](https://kexue.fm/ac.html)
- [链接](https://kexue.fm/links.html)
- [时光](https://kexue.fm/me.html)
- [博览](https://kexue.fm/science.html)
- [归档](https://kexue.fm/content.html)

渴望成为一个小飞侠

- [欢迎订阅](https://kexue.fm/feed)
- [个性邮箱](https://kexue.fm/archives/119)
- [天象信息](https://kexue.fm/ac.html)
- [观测ISS](https://kexue.fm/archives/41)
- [LaTeX](https://kexue.fm/latex.html)
- [关于博主](https://kexue.fm/me.html)

欢迎访问“科学空间”，这里将与您共同探讨自然科学，回味人生百态；也期待大家的分享～

- [**千奇百怪** Everything](https://kexue.fm/category/Everything)
- [**天文探索** Astronomy](https://kexue.fm/category/Astronomy)
- [**数学研究** Mathematics](https://kexue.fm/category/Mathematics)
- [**物理化学** Phy-chem](https://kexue.fm/category/Phy-chem)
- [**信息时代** Big-Data](https://kexue.fm/category/Big-Data)
- [**生物自然** Biology](https://kexue.fm/category/Biology)
- [**图片摄影** Photograph](https://kexue.fm/category/Photograph)
- [**问题百科** Questions](https://kexue.fm/category/Questions)
- [**生活/情感** Life-Feeling](https://kexue.fm/category/Life-Feeling)
- [**资源共享** Resources](https://kexue.fm/category/Resources)

- [**千奇百怪**](https://kexue.fm/category/Everything)
- [**天文探索**](https://kexue.fm/category/Astronomy)
- [**数学研究**](https://kexue.fm/category/Mathematics)
- [**物理化学**](https://kexue.fm/category/Phy-chem)
- [**信息时代**](https://kexue.fm/category/Big-Data)
- [**生物自然**](https://kexue.fm/category/Biology)
- [**图片摄影**](https://kexue.fm/category/Photograph)
- [**问题百科**](https://kexue.fm/category/Questions)
- [**生活/情感**](https://kexue.fm/category/Life-Feeling)
- [**资源共享**](https://kexue.fm/category/Resources)

[首页](https://kexue.fm) [信息时代](https://kexue.fm/category/Big-Data) 【学习清单】最近比较重要的GAN进展论文

26Dec

# [【学习清单】最近比较重要的GAN进展论文](https://kexue.fm/archives/6240)

By 苏剑林 \|
2018-12-26 \|
73725位读者\|

这篇文章简单列举一下我认为最近这段时间中比较重要的GAN进展论文，这基本也是我在学习GAN的过程中主要去研究的论文清单。

## 生成模型之味 [\#](https://kexue.fm/archives/6240\#%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B%E4%B9%8B%E5%91%B3)

GAN是一个大坑，尤其像我这样的业余玩家，一头扎进去很久也很难有什么产出，尤其是各个大公司拼算力搞出来一个个大模型，个人几乎都没法玩了。但我总觉得，真的去碰了生成模型，才觉得自己碰到了真正的机器学习。这一点，不管在图像中还是文本中都是如此。所以，我还是愿意去关注生成模型。

当然，GAN不是生成模型的唯一选择，却是一个非常有趣的选择。在图像中至少有GAN、flow、pixelrnn/pixelcnn这几种选择，但要说潜力，我还是觉得GAN才是最具前景的，不单是因为效果，主要是因为它那对抗的思想。而在文本中，事实上seq2seq机制就是一个概率生成模型了，而pixelrnn这类模型，实际上就是模仿着seq2seq来做的，当然也有用GAN做文本生成的研究（不过基本上都涉及到了强化学习）。也就是说，其实在NLP中，生成模型也有很多成果，哪怕你主要是研究NLP的，也终将碰到生成模型。

好了，话不多说，还是赶紧把清单列一列，供大家参考，也作为自己的备忘。

## 拿效果来说话 [\#](https://kexue.fm/archives/6240\#%E6%8B%BF%E6%95%88%E6%9E%9C%E6%9D%A5%E8%AF%B4%E8%AF%9D)

### 话在前头 [\#](https://kexue.fm/archives/6240\#%E8%AF%9D%E5%9C%A8%E5%89%8D%E5%A4%B4)

不严谨地说，目前在GAN中，基本上都是效果说话。不管你多么完美的理论，只要你实验不能生成高清图，都很难被人接受；你再丑陋的结果，只要你实验效果够好，能生成高清大图，大家都得围着你转。

GAN模型的一个标志性事件，是NVIDIA去年搞出来的Progressive Growing GANs，它首次实现了1024\*1024的高清人脸生成。要知道，一般的GAN在生成128\*128人脸时就会有困难，所以1024分辨率的生成称得上是一个突破。而下面列举的一些论文，都是在自己的实验中做到了1024的人脸生成。单是这个实验结果，就值得我们去关注一下这些论文。

当然，生成1024图除了需要模型的进步，还需要很大的算力，因此一般人/实验室都很难做到。关注这些论文，并不是要我们去复现这么大的图生成，而是因为这些模型能生成这么大的图，必然有它值得我们借鉴的地方，甚至我们可以从中明白到GAN的瓶颈所在，从而让我们在自己的研究中少走弯路。

### 论文清单 [\#](https://kexue.fm/archives/6240\#%E8%AE%BA%E6%96%87%E6%B8%85%E5%8D%95)

**《Progressive Growing of GANs for Improved Quality, Stability, and Variation》** **论文地址：** [https://papers.cool/arxiv/1710.10196](https://papers.cool/arxiv/1710.10196) **参考实现：** [https://github.com/tkarras/progressive\_growing\_of\_gans](https://github.com/tkarras/progressive_growing_of_gans) **简单介绍：** 这篇文章就是前面说的首次实现了1024人脸生成的Progressive Growing GANs，简称PGGAN，来自NVIDIA。顾名思义，PGGAN通过一种渐进式的结构，实现了从低分辨率到高分辨率的过渡，从而能平滑地训练出高清模型出来。论文还提出了自己对正则化、归一化的一些理解和技巧，值得思考。当然，由于是渐进式的，所以相当于要串联地训练很多个模型，所以PGGAN很慢...

**《Which Training Methods for GANs do actually Converge?》** **论文地址：** [https://papers.cool/arxiv/1801.04406](https://papers.cool/arxiv/1801.04406) **参考实现：** [https://github.com/LMescheder/GAN\_stability](https://github.com/LMescheder/GAN_stability) **简单介绍：** 这篇文章有很多对GAN训练稳定性的数学推导，最终得到了比WGAN-GP更简单的梯度惩罚项，关注GAN训练稳定性的同学可以参考。除了1024人脸，这篇文章也做了很多其他数据集的实验，效果都挺不错，而且都是直接端到端训练，不需要渐进式结构。我唯一困惑的是，这个惩罚项，不就是 [WGAN-div](https://kexue.fm/archives/6139) 中的一个特例吗？为什么论文没有提到这一点？

**《IntroVAE: Introspective Variational Autoencoders for Photographic Image Synthesis》** **论文地址：** [https://papers.cool/arxiv/1807.06358](https://papers.cool/arxiv/1807.06358) **参考实现：**（目前还没有看到效果好的开源／复现）
**简单介绍：** 这是个会“反省”的VAE，通过对抗来改进了VAE，从而能生成高清图片，并且能同时得到编码器和生成器。除了能生成1024的高清图，更值得一提的是，这篇文章在构思上非常精妙。因为能同时得到编码器和生成器的模型不算独特，比如 [BiGAN](https://kexue.fm/archives/6214) 就能做到，但是IntroVAE独特之处在于它能直接利用了encoder作为判别器，不需要额外的判别器，也就是直接省去了1/3的参数量。这背后更深层次的原因，值得我们去细细分析和回味。

**《Large Scale GAN Training for High Fidelity Natural Image Synthesis》** **论文地址：** [https://papers.cool/arxiv/1809.11096](https://papers.cool/arxiv/1809.11096) **参考实现：** [https://github.com/AaronLeong/BigGAN-pytorch](https://github.com/AaronLeong/BigGAN-pytorch) **简单介绍：** 这就是大名鼎鼎的BigGAN。这篇文章虽然没有提供1024的人脸生成结果，但是它提供了128、256、512的自然场景图片的生成结果。要知道自然场景图片的生成可是比CelebA的人脸生成要难上很多倍，既然它连512的自然场景图片都可以生成了，我们自然不怀疑它能轻松生成1024的人脸。BigGAN在网上已经有很多科普介绍了，不再重复。论文还提出了自己的一些正则化技巧，并分享了大量的调参经验（调整哪些参数会有好的／坏的改变），非常值得参考。

**《Variational Discriminator Bottleneck: Improving Imitation Learning, Inverse RL, and GANs by Constraining Information Flow》** **论文地址：** [https://papers.cool/arxiv/1810.00821](https://papers.cool/arxiv/1810.00821) **参考实现：** [https://github.com/akanimax/Variational\_Discriminator\_Bottleneck](https://github.com/akanimax/Variational_Discriminator_Bottleneck) **简单介绍：** 这篇文章通过信息瓶颈来控制判别器的拟合能力，从而起到正则作用，稳定了GAN的训练。信息瓶颈的简介可以参考 [我的这篇文章](https://kexue.fm/archives/6181)。总的来说，在普通有监督训练中一切防止过拟合的手段，理论上都可以用在判别器中，而信息瓶颈也算是防止过拟合的一种手段。当然，从标题就可以知道，论文也不满足于只用在GAN中，除了1024的人脸图生成实验，论文还做了仿真学习、强化学习等实验。

**《A Style-Based Generator Architecture for Generative Adversarial Networks》** **论文地址：** [https://papers.cool/arxiv/1812.04948](https://papers.cool/arxiv/1812.04948) **参考实现：** [https://github.com/NVlabs/stylegan](https://github.com/NVlabs/stylegan) **简单介绍：** 这就是前几天发出来的新GAN生成器架构，被很多文章称之为GAN 2.0，依旧是NVIDIA，依旧是PGGAN的作者，依旧是PGGAN的模式，只不过生成器的架构换了，人家在一年前就已经生成了1024图，这次肯定也不例外了。这个新的生成器架构，据说是借鉴了风格迁移的模型，所以叫Style-Based Generator。我读了一下，其实它差不多就是条件GAN（CGAN）的架构，但是把条件和噪声互换了。简单来说，就是把噪声当作条件，把条件当作噪声，然后代入到CGAN中。看论文的效果图，这种思维上的转换的效果还是很不错的，我自己也试着实现了一下，能work，但是有点mode collapse，大家还是等开源吧。顺便一提的是，一年前也是PGGAN的作者给我们带来了CelebA HQ数据集，现在还是他们给我们带来了新数据集FFHQ。据说数据集和代码都将在明年一月开源，让我们拭目以待。

## 稳住训练再说 [\#](https://kexue.fm/archives/6240\#%E7%A8%B3%E4%BD%8F%E8%AE%AD%E7%BB%83%E5%86%8D%E8%AF%B4)

### 话在前头 [\#](https://kexue.fm/archives/6240\#%E8%AF%9D%E5%9C%A8%E5%89%8D%E5%A4%B4)

与有监督学习的任务不同，有监督学习中，一般只要设计好模型，然后有足够多的数据，足够的算力，就可以得到足够好的模型；但GAN从来都不是设计模型就完事了，它是一个理论、模型、优化一体的事情。从框架的角度来看，发展到WGAN后GAN的理论框架基本也就完备了，后面都只是不痛不痒的修补（包括我的 [GAN-QP](https://kexue.fm/archives/6163)）；从模型架构来看，DCGAN奠定了基础，后来发展的ResNet + Upsampling也成为了标准框架之一，至于刚出来的Style-Based Generator就不说了，所以说模型架构基本上也成熟了。那剩下的是什么呢？

是优化，也就是训练过程。我觉得，要想真正掌握GAN，就得仔细研究它的优化过程，也许得从动力学角度来仔细分析它的训练轨迹。这可能涉及到微分方程解的存在性、唯一性、稳定性等性质，也可能涉及到随机优化过程的知识。总而言之，需要把优化过程也纳入到GAN的分析中，GAN才可能真正完备起来。

下面的这些论文，从不同的角度分析了GAN的训练问题，并给出了自己的解决方案，值得一读。

### 论文清单 [\#](https://kexue.fm/archives/6240\#%E8%AE%BA%E6%96%87%E6%B8%85%E5%8D%95)

**《Stabilizing Training of Generative Adversarial Networks through Regularization》** **论文地址：** [https://papers.cool/arxiv/1705.09367](https://papers.cool/arxiv/1705.09367) **简单介绍：** 通过加噪声的方式推导出了GAN的正则项，推导过程理论上适用于一切f-GAN。从论文效果图看，结果还是不错的。

**《GANs Trained by a Two Time-Scale Update Rule Converge to a Local Nash Equilibrium》** **论文地址：** [https://papers.cool/arxiv/1706.08500](https://papers.cool/arxiv/1706.08500) **简单介绍：** 提出了TTUR的训练策略，大概意思就是：原来我们每次迭代都是用相同的学习率将判别器和生成器交替训练不同的次数，现在可以考虑用不同的学习率将各自训练一次，这样显然训练起来会更省时。不过我粗略看了一下，尽管论文理论多，但是它理论基础却是另外一篇现成的文章《stochastic approximation with two time scales》，可以说论文只是反复在用这个现成的理论基础，略微单调。

**《Which Training Methods for GANs do actually Converge?》** **论文地址：** [https://papers.cool/arxiv/1801.04406](https://papers.cool/arxiv/1801.04406) **简单介绍：** 在前面已经介绍过这篇文章了，但这里还是再放一次，因为实在是太经典～感觉是研究GAN训练稳定性必看的文章，从微分方程角度来理解GAN的训练问题。在稳定性分析的过程中，这篇文章主要还引用了两篇文章，一篇是它的“前传”（同一作者），叫做 [《The Numerics of GANs》](https://papers.cool/arxiv/1705.10461)，另一篇是 [《Gradient descent GAN optimization is locally stable》](https://papers.cool/arxiv/1706.04156)，都是经典之作。

**《Spectral Normalization for Generative Adversarial Networks》** **论文地址：** [https://papers.cool/arxiv/1802.05957](https://papers.cool/arxiv/1802.05957) **简单介绍：** 通过谱归一化给判别器实现L约束，应该说是目前实现L约束最漂亮的方法了，目前谱归一化也用得很广，所以值得一提。相关介绍也可以参考 [我之前的文章](https://kexue.fm/archives/6051)。

**《Improving the Improved Training of Wasserstein GANs: A Consistency Term and Its Dual Effect》** **论文地址：** [https://papers.cool/arxiv/1803.01541](https://papers.cool/arxiv/1803.01541) **简单介绍：** 往WGAN-GP中添加了一个新的正则项，这个正则项的想法很朴素，就是直接把L约束（差分形式）作为正则项，跟GAN-QP的判别器多出来的二次项差不多。看论文的曲线图，训练比纯WGAN-GP要稳定些。

## 欢迎继续补充 [\#](https://kexue.fm/archives/6240\#%E6%AC%A2%E8%BF%8E%E7%BB%A7%E7%BB%AD%E8%A1%A5%E5%85%85)

这次的论文清单就这么多了，刚好凑够了十篇。限于笔者阅读量，不排除有疏漏之处，如果还有其他推荐的，欢迎在评论中提出。

PS：只关心NLP的读者也不用无奈，很快就会有一些NLP的博文出来了（^\_^）

_**转载到请包括本文地址：** [https://kexue.fm/archives/6240](https://kexue.fm/archives/6240)_

_**更详细的转载事宜请参考：**_ [《科学空间FAQ》](https://kexue.fm/archives/6508#%E6%96%87%E7%AB%A0%E5%A6%82%E4%BD%95%E8%BD%AC%E8%BD%BD/%E5%BC%95%E7%94%A8)

**如果您还有什么疑惑或建议，欢迎在下方评论区继续讨论。**

**如果您觉得本文还不错，欢迎 [分享](https://kexue.fm/archives/6240#share)/ [打赏](https://kexue.fm/archives/6240#pay) 本文。打赏并非要从中获得收益，而是希望知道科学空间获得了多少读者的真心关注。当然，如果你无视它，也不会影响你的阅读。再次表示欢迎和感谢！**

打赏

微信打赏

支付宝打赏

因为网站后台对打赏并无记录，因此欢迎在打赏时候备注留言。你还可以 [**点击这里**](http://mail.qq.com/cgi-bin/qm_share?t=qm_mailme&email=tN7d1drY3drrx8H0xcWa19vZ) 或在下方评论区留言来告知你的建议或需求。

**如果您需要引用本文，请参考：**

苏剑林. (Dec. 26, 2018). 《【学习清单】最近比较重要的GAN进展论文 》\[Blog post\]. Retrieved from [https://kexue.fm/archives/6240](https://kexue.fm/archives/6240)

@online{kexuefm-6240,
        title={【学习清单】最近比较重要的GAN进展论文},
        author={苏剑林},
        year={2018},
        month={Dec},
        url={\\url{https://kexue.fm/archives/6240}},
}

分类： [信息时代](https://kexue.fm/category/Big-Data)    标签： [学习](https://kexue.fm/tag/%E5%AD%A6%E4%B9%A0/), [GAN](https://kexue.fm/tag/GAN/), [生成模型](https://kexue.fm/tag/%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B/)[12 评论](https://kexue.fm/archives/6240#comments)

< [从动力学角度看优化算法（二）：自适应学习率算法](https://kexue.fm/archives/6234) \| [2019年全年天象](https://kexue.fm/archives/6257) >

### 你也许还对下面的内容感兴趣

- [生成扩散模型漫谈（三十）：从瞬时速度到平均速度](https://kexue.fm/archives/10958)
- [Transformer升级之路：20、MLA究竟好在哪里？](https://kexue.fm/archives/10907)
- [生成扩散模型漫谈（二十九）：用DDPM来离散编码](https://kexue.fm/archives/10711)
- [细水长flow之TARFLOW：流模型满血归来？](https://kexue.fm/archives/10667)
- [生成扩散模型漫谈（二十八）：分步理解一致性模型](https://kexue.fm/archives/10633)
- [生成扩散模型漫谈（二十七）：将步长作为条件输入](https://kexue.fm/archives/10617)
- [生成扩散模型漫谈（二十六）：基于恒等式的蒸馏（下）](https://kexue.fm/archives/10567)
- [VQ的又一技巧：给编码表加一个线性变换](https://kexue.fm/archives/10519)
- [VQ的旋转技巧：梯度直通估计的一般推广](https://kexue.fm/archives/10489)
- [“闭门造车”之多模态思路浅谈（二）：自回归](https://kexue.fm/archives/10197)

[发表你的看法](https://kexue.fm/archives/6240#comment_form)

贾耕云

December 26th, 2018

哇，看到实验室学长的论文了，就是introVAE那篇。
我感觉还可以列一下关于mode collapse的文章，最近这方面论文也很多啊，比如nips的pacGAN

[回复评论](https://kexue.fm/archives/6240/comment-page-1?replyTo=10433#respond-post-6240)

[苏剑林](https://kexue.fm) 发表于
December 26th, 2018

pacGAN类似minibatch GAN，连同RSGAN，包括我自己写的T-GAN，我觉得这些都不算真正意义上的突破了。如果这些都列举出来，那就很多啦。

另外在我看来，mode collapse其实是GAN的固有问题，目前的纯GAN体系下很难解决，倒是IntroVAE、BiGAN这些路会有希望。

[回复评论](https://kexue.fm/archives/6240/comment-page-1?replyTo=10434#respond-post-6240)

[苏剑林](https://kexue.fm) 发表于
December 26th, 2018

对了，如果你能联系你的学长，跟他说一声他论文的$(9)$式KL散度的结果少了个负号～

[回复评论](https://kexue.fm/archives/6240/comment-page-1?replyTo=10435#respond-post-6240)

nanlihao

December 31st, 2018

博主你好，我想问个关于introvae的问题，就introvae而言，其中的discriminator是不是一个散度呢？感觉不知道怎么分析。。。

[回复评论](https://kexue.fm/archives/6240/comment-page-1?replyTo=10460#respond-post-6240)

[苏剑林](https://kexue.fm) 发表于
December 31st, 2018

是，以原来VAE的KL loss为discriminator。很遗憾，我没能复现introvae，所以我不好进一步提出自己的想法。

[回复评论](https://kexue.fm/archives/6240/comment-page-1?replyTo=10466#respond-post-6240)

jjtail

January 5th, 2019

NLP的博文等不及了

[回复评论](https://kexue.fm/archives/6240/comment-page-1?replyTo=10489#respond-post-6240)

[苏剑林](https://kexue.fm) 发表于
January 5th, 2019

总会有的，不着急。

[回复评论](https://kexue.fm/archives/6240/comment-page-1?replyTo=10495#respond-post-6240)

[无中生有炼丹术，生成逼真卡通人脸——DCGAN(对抗生成网络）实战 - 算法网](http://ddrv.cn/a/143741)

May 5th, 2019

\[...\] [https://spaces.ac.cn/archives/6240](https://spaces.ac.cn/archives/6240) http://speech.ee.ntu.edu.tw/~tlkagk/courses\_MLDS18.html https://arxiv.org/pdf/1511.06434.pdf https://arxiv.org/abs/1406.2661 https://github.com/eriklindernoren/Keras\[...\]

[回复评论](https://kexue.fm/archives/6240/comment-page-1?replyTo=11103#respond-post-6240)

zcj5918

March 6th, 2023

苏神您好，问个比较务虚的问题。我们学神经网络的时候，有时候我想根据自己的想法去coding一个模型，然后发现模型有各种各样的问题，调参或者调结构总是靠经验或者直觉，说白了就是靠猜。我就是不知道一些经典的模型结构别人是怎么想到的，比如是不是能够有什么通用的方法，一开始我们可能搭出了一个比较差的网络，然后通过看看每一层学到的权重分布，梯度什么的，然后根据这些信息，猜测到底效果差的原因是因为样本烂、样本少、还是少了bn，少了谱归一化，需要dropout。这方面有没有一些经验和总结呢？请指教！

[回复评论](https://kexue.fm/archives/6240/comment-page-1?replyTo=21058#respond-post-6240)

漠然 发表于
March 7th, 2023

之前和你差不多的想法，后来看了苏神的博客，尤其是对于优化目标的启发式探索（基于数学理论），让我感受到，关键点还是数学分析，而不是猜测。一个实践中很work的方案可能是你的一时灵感，但是仔细探究，总会找到其理论基础。我的感觉是从理论出发找到work的方案，比靠猜测或者灵感来验证ieda work的策略要务实得多，当然也难得多。
最后再次感谢苏神！

[回复评论](https://kexue.fm/archives/6240/comment-page-1?replyTo=21072#respond-post-6240)

zcj5918 发表于
March 7th, 2023

想系统地理一下思路，比如就你说的优化目标的启发式搜索什么的，因为我不是从数学系出身的，可能相对基础差些。比如做数学分析之类的工作，自己自学了一些矩阵分析、随机过程、泛函分析的课程（当然可能没有特别系统），发现最难的工作其实不是数学推导，而是合理的数学建模，往往只要直观上想清楚一件事情来龙去脉，能有一个数学模型描述它，后面就是翻书找工具，背不出公式也没关系，可以把能想到的所有可能的方法的书摊在地上，然后憋一下午就可能脑子里推通了。问题是有时候找不到一个合适的数学工具去描述我想要证明的结论或者想法。

“尤其是对于优化目标的启发式探索”，这些东西有没有系统性的教一些思维方法类的书，很多教材，感觉是直接给结论的，看懂了，但又似乎只是看懂了，并不能灵活转化。。。

[回复评论](https://kexue.fm/archives/6240/comment-page-1?replyTo=21073#respond-post-6240)

[苏剑林](https://kexue.fm) 发表于
March 7th, 2023

深度学习发展史，很多都是先得到了有效的实验结果，然后才去想办法解释，而且很多之前看上去合理的解释，后面也有被推翻的例子。

所以，开始阶段我们基本就只能接受，尤其是没有人引导的情况下，只能多读多接受。当你接受得足够多了，超出你的承受范围了，你就自然不会停留在纯粹接受的范围内，而是开始融会贯通，拥有自己的思路。之前我们数学分析的老师说过一句话，学习是从一个从薄到厚、再从厚到薄的过程，便是这个意思。

在这个过程中，自律且不自欺。

自律容易理解，坚持、毅力；不自欺的意思是，不要为了读论文的数量或者完成任务，自己骗自己说弄懂了，然后就去读下一篇。什么是弄懂，首先咬文嚼字理解论文的每一行字、每一个概念，然后能够复述甚至再创造就是弄懂了。这个过程看上去很难很费时间，但对于某个专业领域而言，只要你坚持几篇做到这个程度，后面就会逐渐加快了。

[回复评论](https://kexue.fm/archives/6240/comment-page-1?replyTo=21077#respond-post-6240)

[取消回复](https://kexue.fm/archives/6240#respond-post-6240)

你的大名

电子邮箱

个人网站（选填）

1\. 可以使用LaTeX代码，点击“预览效果”可查看效果；2. 可以通过点击评论楼层编号来引用该楼层；3. 网站可能会有点卡，如非确认评论失败，请不要重复点击提交。

### 内容速览

[生成模型之味](https://kexue.fm/archives/6240#%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B%E4%B9%8B%E5%91%B3)
[拿效果来说话](https://kexue.fm/archives/6240#%E6%8B%BF%E6%95%88%E6%9E%9C%E6%9D%A5%E8%AF%B4%E8%AF%9D)
[话在前头](https://kexue.fm/archives/6240#%E8%AF%9D%E5%9C%A8%E5%89%8D%E5%A4%B4)
[论文清单](https://kexue.fm/archives/6240#%E8%AE%BA%E6%96%87%E6%B8%85%E5%8D%95)
[稳住训练再说](https://kexue.fm/archives/6240#%E7%A8%B3%E4%BD%8F%E8%AE%AD%E7%BB%83%E5%86%8D%E8%AF%B4)
[话在前头](https://kexue.fm/archives/6240#%E8%AF%9D%E5%9C%A8%E5%89%8D%E5%A4%B4)
[论文清单](https://kexue.fm/archives/6240#%E8%AE%BA%E6%96%87%E6%B8%85%E5%8D%95)
[欢迎继续补充](https://kexue.fm/archives/6240#%E6%AC%A2%E8%BF%8E%E7%BB%A7%E7%BB%AD%E8%A1%A5%E5%85%85)

### 智能搜索

支持整句搜索！网站自动使用 [结巴分词](https://github.com/fxsjy/jieba) 进行分词，并结合ngrams排序算法给出合理的搜索结果。

### 热门标签

[生成模型](https://kexue.fm/tag/%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B/) [attention](https://kexue.fm/tag/attention/) [优化](https://kexue.fm/tag/%E4%BC%98%E5%8C%96/) [语言模型](https://kexue.fm/tag/%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/) [模型](https://kexue.fm/tag/%E6%A8%A1%E5%9E%8B/) [网站](https://kexue.fm/tag/%E7%BD%91%E7%AB%99/) [概率](https://kexue.fm/tag/%E6%A6%82%E7%8E%87/) [梯度](https://kexue.fm/tag/%E6%A2%AF%E5%BA%A6/) [转载](https://kexue.fm/tag/%E8%BD%AC%E8%BD%BD/) [微分方程](https://kexue.fm/tag/%E5%BE%AE%E5%88%86%E6%96%B9%E7%A8%8B/) [矩阵](https://kexue.fm/tag/%E7%9F%A9%E9%98%B5/) [天象](https://kexue.fm/tag/%E5%A4%A9%E8%B1%A1/) [分析](https://kexue.fm/tag/%E5%88%86%E6%9E%90/) [深度学习](https://kexue.fm/tag/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/) [积分](https://kexue.fm/tag/%E7%A7%AF%E5%88%86/) [python](https://kexue.fm/tag/python/) [优化器](https://kexue.fm/tag/%E4%BC%98%E5%8C%96%E5%99%A8/) [力学](https://kexue.fm/tag/%E5%8A%9B%E5%AD%A6/) [无监督](https://kexue.fm/tag/%E6%97%A0%E7%9B%91%E7%9D%A3/) [扩散](https://kexue.fm/tag/%E6%89%A9%E6%95%A3/) [几何](https://kexue.fm/tag/%E5%87%A0%E4%BD%95/) [节日](https://kexue.fm/tag/%E8%8A%82%E6%97%A5/) [生活](https://kexue.fm/tag/%E7%94%9F%E6%B4%BB/) [文本生成](https://kexue.fm/tag/%E6%96%87%E6%9C%AC%E7%94%9F%E6%88%90/) [数论](https://kexue.fm/tag/%E6%95%B0%E8%AE%BA/)

### 随机文章

- [400多本数学电子书籍（供下载）](https://kexue.fm/archives/402)
- [太阳系是稳定的吗？](https://kexue.fm/archives/1115)
- [朋友们，来瓶汽水吧！有趣的换汽水问题](https://kexue.fm/archives/3495)
- [星座计划“破产”，重返月球搁浅](https://kexue.fm/archives/388)
- [“未解之谜”：为何不讲中点矩形法则？](https://kexue.fm/archives/1668)
- [线性Transformer应该不是你要等的那个模型](https://kexue.fm/archives/8610)
- [【龟猫记】家里多了几只小动物](https://kexue.fm/archives/344)
- [基于Conv1D的光谱分类模型（一维序列分类）](https://kexue.fm/archives/5505)
- [Transformer升级之路：18、RoPE的底数选择原则](https://kexue.fm/archives/10122)
- [神奇的麦田圈坐标图片之谜](https://kexue.fm/archives/124)

### 最近评论

- [PengchengMa](https://kexue.fm/archives/10996/comment-page-1#comment-27811): 牛啊
- [xczh](https://kexue.fm/archives/10958/comment-page-1#comment-27810): 已使用mean flow policy，一步推理效果确实惊人，性能跟多步推理的diffusio...
- [Cosine](https://kexue.fm/archives/10945/comment-page-1#comment-27809): 是不是因为shared experts每次都激活，而routed experts是依概率被选中...
- [rpsun](https://kexue.fm/archives/10699/comment-page-1#comment-27808): 这样似乎与传统的经验正交函数之类的有相似之处。把样本的平均值减掉之后做正交分解。那么如果单纯地...
- [贵阳机场接机](https://kexue.fm/archives/1490/comment-page-1#comment-27807): 怎么不更新啦
- [czvzb](https://kexue.fm/archives/10958/comment-page-1#comment-27806): 具身智能模型目前主流也是在使用扩散和流匹配这类方法来预测动作。
苏神推荐你看这几篇文章：
1....
- [Shawn\_yang](https://kexue.fm/archives/10945/comment-page-1#comment-27802): 苏神，关于您所说的：“推理阶段可以事先预估Routed Expert的实际分布，只要细致地进行...
- [OceanYU](https://kexue.fm/archives/9164/comment-page-4#comment-27801): 您好，关于由式（7）推导出高斯分布，我这里有一点问题，式（7）只能保证关于x\_t-1是二次函数...
- [jorjiang](https://kexue.fm/archives/10907/comment-page-2#comment-27800): 训练和prefill这个compute-bound阶段不做矩阵吸收，这个用我这个解释更好理解了...
- [amy](https://kexue.fm/archives/10907/comment-page-2#comment-27799): 苏老师，您有关注傅里叶旋转位置编码这篇工作吗，想知道您对这篇工作的看法是什么，这篇工作可以wo...

### 友情链接

- [Cool Papers](https://papers.cool)
- [数学研发](https://bbs.emath.ac.cn)
- [Seatop](http://www.seatop.com.cn/)
- [Xiaoxia](https://xiaoxia.org/)
- [积分表-网络版](https://kexue.fm/sci/integral/index.html)
- [丝路博傲](http://blog.dvxj.com/)
- [ph4ntasy 饭特稀](http://www.ph4ntasy.com/)
- [数学之家](http://www.2math.cn/)
- [有趣天文奇观](http://interesting-sky.china-vo.org/)
- [TwistedW](http://www.twistedwg.com/)
- [godweiyang](https://godweiyang.com/)
- [AI柠檬](https://blog.ailemon.net/)
- [王登科-DK博客](https://greatdk.com)
- [ESON](https://blog.eson.org/)
- [枫之羽](https://fzhiy.net/)
- [Mathor's blog](https://wmathor.com/)
- [coding-zuo](https://coding-zuo.github.io/)
- [博科园](https://www.bokeyuan.net/)
- [孔皮皮的博客](https://www.kppkkp.top/)
- [运鹏的博客](https://yunpengtai.top/)
- [jiming.site](https://jiming.site/)
- [OmegaXYZ](https://www.omegaxyz.com/)
- [Blog by Eacls](https://www.eacls.top/)
- [EAI猩球](https://www.robotech.ink/)
- [文举的博客](https://liwenju0.com/)
- [用代码打点酱油](https://bruceyuan.com/)
- [申请链接](https://kexue.fm/links.html)

本站采用创作共用版权协议，要求署名、非商业用途和保持一致。转载本站内容必须也遵循“ [署名-非商业用途-保持一致](http://creativecommons.org/licenses/by-nc-nd/2.5/cn/)”的创作共用协议。
© 2009-2025 Scientific Spaces. All rights reserved. Theme by [laogui](http://www.laogui.com). Powered by [Typecho](http://typecho.org). 备案号: [粤ICP备09093259号-1/2](https://beian.miit.gov.cn/)。