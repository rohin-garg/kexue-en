## SEARCH

## MENU

- [打赏](https://kexue.fm/reward.html)
- [公式](https://kexue.fm/latex.html)
- [天象](https://kexue.fm/ac.html)
- [链接](https://kexue.fm/links.html)
- [时光](https://kexue.fm/me.html)
- [博览](https://kexue.fm/science.html)
- [归档](https://kexue.fm/content.html)

## CATEGORIES

- [千奇百怪](https://kexue.fm/category/Everything)
- [天文探索](https://kexue.fm/category/Astronomy)
- [数学研究](https://kexue.fm/category/Mathematics)
- [物理化学](https://kexue.fm/category/Phy-chem)
- [信息时代](https://kexue.fm/category/Big-Data)
- [生物自然](https://kexue.fm/category/Biology)
- [图片摄影](https://kexue.fm/category/Photograph)
- [问题百科](https://kexue.fm/category/Questions)
- [生活/情感](https://kexue.fm/category/Life-Feeling)
- [资源共享](https://kexue.fm/category/Resources)

## NEWPOSTS

- [msign的导数](https://kexue.fm/archives/11025)
- [通过msign来计算mclip（奇...](https://kexue.fm/archives/11006)
- [msign算子的Newton-Sc...](https://kexue.fm/archives/10996)
- [等值振荡定理：最优多项式逼近的充要条件](https://kexue.fm/archives/10972)
- [生成扩散模型漫谈（三十）：从瞬时速...](https://kexue.fm/archives/10958)
- [MoE环游记：5、均匀分布的反思](https://kexue.fm/archives/10945)
- [msign算子的Newton-Sc...](https://kexue.fm/archives/10922)
- [Transformer升级之路：2...](https://kexue.fm/archives/10907)
- [一道概率不等式：盯着它到显然成立为止！](https://kexue.fm/archives/10902)
- [SVD的导数](https://kexue.fm/archives/10878)

## COMMENTS

- [tll1945tll1937: 真心实意的向大家请教问题：看了文章“对齐全量微调！这是我看过最...](https://kexue.fm/archives/10266/comment-page-1#comment-27901)
- [oYo\_logan: \[comment=27017\]苏剑林\[/comment\]苏神，...](https://kexue.fm/archives/10757/comment-page-1#comment-27897)
- [z123: 在参数矩阵较多的CNN小模型上，Muon会明显慢于Adam，这...](https://kexue.fm/archives/10592/comment-page-1#comment-27896)
- [dry: 苏神好，一直有个疑问，ReFlow构建的ODE是$dx\_t/d...](https://kexue.fm/archives/10958/comment-page-2#comment-27895)
- [tyj: 感觉和之前的一篇文章很像，应该算是concurrent wor...](https://kexue.fm/archives/10958/comment-page-2#comment-27894)
- [li6626: 苏老师，Normalizing Flow有了新进展，论文链接:...](https://kexue.fm/archives/10667/comment-page-1#comment-27893)
- [tesslqy: Evans那本书感觉就挺好的，不过长了一点且有点难（姜萍拿来装...](https://kexue.fm/archives/4718/comment-page-1#comment-27891)
- [Bauree: 渴望大图，万分感谢！](https://kexue.fm/archives/443/comment-page-1#comment-27890)
- [tesslqy: 感觉应该是狄拉克测度，不过我也没系统学过Lebesgue以外的...](https://kexue.fm/archives/4187/comment-page-1#comment-27888)
- [tesslqy: 假设只有作者懂，但我不是作者，我懂了，假设不成立](https://kexue.fm/archives/4187/comment-page-1#comment-27887)

## USERLOGIN

- [登录](https://kexue.fm/admin/login.php)

[科学空间\|Scientific Spaces](https://kexue.fm)

- [登录](https://kexue.fm/admin/login.php)
- [打赏](https://kexue.fm/reward.html)
- [公式](https://kexue.fm/latex.html)
- [天象](https://kexue.fm/ac.html)
- [链接](https://kexue.fm/links.html)
- [时光](https://kexue.fm/me.html)
- [博览](https://kexue.fm/science.html)
- [归档](https://kexue.fm/content.html)

渴望成为一个小飞侠

- [欢迎订阅](https://kexue.fm/feed)
- [个性邮箱](https://kexue.fm/archives/119)
- [天象信息](https://kexue.fm/ac.html)
- [观测ISS](https://kexue.fm/archives/41)
- [LaTeX](https://kexue.fm/latex.html)
- [关于博主](https://kexue.fm/me.html)

欢迎访问“科学空间”，这里将与您共同探讨自然科学，回味人生百态；也期待大家的分享～

- [**千奇百怪** Everything](https://kexue.fm/category/Everything)
- [**天文探索** Astronomy](https://kexue.fm/category/Astronomy)
- [**数学研究** Mathematics](https://kexue.fm/category/Mathematics)
- [**物理化学** Phy-chem](https://kexue.fm/category/Phy-chem)
- [**信息时代** Big-Data](https://kexue.fm/category/Big-Data)
- [**生物自然** Biology](https://kexue.fm/category/Biology)
- [**图片摄影** Photograph](https://kexue.fm/category/Photograph)
- [**问题百科** Questions](https://kexue.fm/category/Questions)
- [**生活/情感** Life-Feeling](https://kexue.fm/category/Life-Feeling)
- [**资源共享** Resources](https://kexue.fm/category/Resources)

- [**千奇百怪**](https://kexue.fm/category/Everything)
- [**天文探索**](https://kexue.fm/category/Astronomy)
- [**数学研究**](https://kexue.fm/category/Mathematics)
- [**物理化学**](https://kexue.fm/category/Phy-chem)
- [**信息时代**](https://kexue.fm/category/Big-Data)
- [**生物自然**](https://kexue.fm/category/Biology)
- [**图片摄影**](https://kexue.fm/category/Photograph)
- [**问题百科**](https://kexue.fm/category/Questions)
- [**生活/情感**](https://kexue.fm/category/Life-Feeling)
- [**资源共享**](https://kexue.fm/category/Resources)

[首页](https://kexue.fm) [数学研究](https://kexue.fm/category/Mathematics) [信息时代](https://kexue.fm/category/Big-Data) VQ的又一技巧：给编码表加一个线性变换

6Nov

# [VQ的又一技巧：给编码表加一个线性变换](https://kexue.fm/archives/10519)

By 苏剑林 \|
2024-11-06 \|
39409位读者\|

在 [《VQ的旋转技巧：梯度直通估计的一般推广》](https://kexue.fm/archives/10489) 中，我们介绍了VQ（Vector Quantization）的Rotation Trick，它的思想是通过推广VQ的STE（Straight-Through Estimator）来为VQ设计更好的梯度，从而缓解VQ的编码表坍缩、编码表利用率低等问题。

无独有偶，昨天发布在arXiv上的论文 [《Addressing Representation Collapse in Vector Quantized Models with One Linear Layer》](https://papers.cool/arxiv/2411.02038) 提出了改善VQ的另一个技巧：给编码表加一个线性变换。这个技巧单纯改变了编码表的参数化方式，不改变VQ背后的理论框架，但实测效果非常优异，称得上是简单有效的经典案例。

## 基础 [\#](https://kexue.fm/archives/10519\#%E5%9F%BA%E7%A1%80)

由于在 [《VQ-VAE的简明介绍：量子化自编码器》](https://kexue.fm/archives/6760)、 [《简单得令人尴尬的FSQ：“四舍五入”超越了VQ-VAE》](https://kexue.fm/archives/9826) 等文章中我们已经多次介绍了VQ和VQ-VAE了，所以这里不再娓娓道来，直接给出普通AE和VQ-VAE的数学形式：
\\begin{align}
\\text{AE:}&\\qquad z = encoder(x),\\quad \\hat{x}=decoder(z),\\quad \\mathcal{L}=\\Vert x - \\hat{x}\\Vert^2 \\\\[12pt\]
\\text{VQ-VAE:}&\\qquad\\left\\{\\begin{aligned}
z =&\\, encoder(x)\\\\[5pt\]
z\_q =&\\, z + \\text{sg}\[q - z\],\\quad q = \\mathop{\\text{argmin}}\_{e\\in\\{e\_1,e\_2,\\cdots,e\_K\\}} \\Vert z - e\\Vert\\\
\\hat{x} =&\\, decoder(z\_q)\\\\[5pt\]
\\mathcal{L} =&\\, \\Vert x - \\hat{x}\\Vert^2 + \\beta\\Vert q - \\text{sg}\[z\]\\Vert^2 + \\gamma\\Vert z - \\text{sg}\[q\]\\Vert^2
\\end{aligned}\\right.\\label{eq:vqvae}
\\end{align}

再次强调老生常谈的一点：VQ-VAE不是VAE，它只是一个加上了VQ的AE，没有VAE的生成能力。而VQ则是将任意向量映射为编码表中与它最邻近的向量的操作，这个操作本身具有不可导的特性，所以通过STE来为encoder设计了梯度，并且新增了$\\beta,\\gamma$这两项损失，来为编码表提供梯度，同时也起到规整encoder表征的作用。

## 改动 [\#](https://kexue.fm/archives/10519\#%E6%94%B9%E5%8A%A8)

论文将自己所提方法称为SimVQ，但没有解释Sim是什么含义，笔者猜测Sim是Simple的缩写，因为SimVQ的改动确实太Simple了：
\\begin{equation}
\\text{SimVQ-VAE:}\\qquad\\left\\{\\begin{aligned}
z =&\\, encoder(x)\\\\[5pt\]
z\_q =&\\, z + \\text{sg}\[q\\color{red}{W} - z\],\\quad q = \\mathop{\\text{argmin}}\_{e\\in\\{e\_1,e\_2,\\cdots,e\_K\\}} \\Vert z - e\\color{red}{W}\\Vert\\\
\\hat{x} =&\\, decoder(z\_q)\\\\[5pt\]
\\mathcal{L} =&\\, \\Vert x - \\hat{x}\\Vert^2 + \\beta\\Vert q\\color{red}{W} - \\text{sg}\[z\]\\Vert^2 + \\gamma\\Vert z - \\text{sg}\[q\\color{red}{W}\]\\Vert^2\\end{aligned}\\right.
\\end{equation}
没错，就只是在编码表多乘了一个矩阵$W$，其他原封不动。

如果原本就是用式$\\eqref{eq:vqvae}$训练VQ的，那么SimVQ可以直接简单上；如果原本是用EMA来更新编码表的（即$\\beta=0$，然后用另外的滑动平均过程来更新编码表，这是VQ-VAE-2及后续一些模型的做法，在数学上等价于用SGD来优化编码表损失，而其他损失则可以用Adam等非SGD优化器），那么则需要取消这个操作，重新引入$\\beta$项来端到端优化。

可能马上有读者质疑：这不就是将编码表的参数化从$E$改为$EW$吗？$EW$可以合并成一个矩阵，等价于一个新的$E$，按道理不改变模型的理论能力？是的，SimVQ对模型能力来说是不变的，但对SGD、Adam来说却是变的，它会改变优化器的学习过程，从而影响学习结果的好坏。

## 实验 [\#](https://kexue.fm/archives/10519\#%E5%AE%9E%E9%AA%8C)

进一步思考和分析之前，我们先看看SimVQ的实验效果。SimVQ做了视觉和音频的实验，比较有代表性的是Table 1：

SimVQ的实验效果

根据论文的描述，SimVQ的代码就是在第一行VQGAN的代码上改的，改动就只有往VQ层插入了个线性变换，然后提升就非常显著了，不仅在相同编码表大小下达到了最优的重构质量，还能通过增加编码表大小进一步提高重构质量，这足以体现SimVQ的魅力——简单且有效。

笔者也在自己之前写的VQ-VAE代码上做了尝试，实测显示这个线性变换的加入，明显加速了VQ-VAE的收敛速度，并且最终的重构损失也有所降低。笔者还实验了$W$取对角阵的变体，这时候就相当于每个编码向量都element-wise地与一个参数向量（全一初始化）相乘，结果显示这样的变体也能起到相近的效果，介乎VQ与SimVQ之间。

## 分析 [\#](https://kexue.fm/archives/10519\#%E5%88%86%E6%9E%90)

直观来想，VQ对编码表的更新是比较“孤立”的，比如某个样本$z$被VQ为$q$，那么这个样本的梯度就只会影响$q$，不会影响编码表里的其他向量；但SimVQ不同，它不单会更新$q$，还会更新$W$，从几何意义上看，$W$就相当于编码表的基底，一旦更新$W$，那么整个编码表就会更新了。所以说，SimVQ使得整个编码表的“联动”更为密切，从而更有机会找到更优的解，而不是陷入“各自为政”的局部最优。

那为什么SimVQ能提高编码表的利用率呢？这个其实也不难理解。再次根据$W$是编码表基底的解释，如果编码表利用率过低，那么$W$就会出现“各向异性”，即基底偏向于那些被利用起来的编码，可是一旦基底发生这种变化，那么它的线性组合应该也是偏向于被利用起来的编码，从而利用率不会太低。说白了，可学习的基底会自动让自己的利用率变高，从而让整个编码表的利用率都提高起来。

我们也可以从数学公式角度来描述这个过程。假设优化器为SGD，那么VQ中编码$e\_i$的更新为
\\begin{equation}e\_i^{(t+1)} = e\_i^{(t)} - \\eta\\frac{\\partial \\mathcal{L}}{\\partial e\_i^{(t)}}\\end{equation}
这样如果当前批次中$e\_i$没有被选中，那么$\\frac{\\partial \\mathcal{L}}{\\partial e\_i^{(t)}}$为零，当前编码表就不更新了。但如果$e\_i$被参数化为$q\_i W$，那么
\\begin{equation}\\begin{aligned}
q\_i^{(t+1)} =&\\, q\_i^{(t)} - \\eta\\frac{\\partial \\mathcal{L}}{\\partial q\_i^{(t)}} = q\_i^{(t)} - \\eta \\frac{\\partial \\mathcal{L}}{\\partial e\_i^{(t)}} W^{(t)}{}^{\\top}\\\
W^{(t+1)} =&\\, W^{(t)} - \\eta\\frac{\\partial \\mathcal{L}}{\\partial W^{(t)}} = W^{(t)} - \\eta \\sum\_i q\_i^{(t)}{}^{\\top}\\frac{\\partial \\mathcal{L}}{\\partial e\_i^{(t)}} \\\
e\_i^{(t+1)}=&\\,q\_i^{(t+1)}W^{(t+1)}\\approx e\_i^{(t)} - \\eta\\left(\\frac{\\partial \\mathcal{L}}{\\partial e\_i^{(t)}} W^{(t)}{}^{\\top}W^{(t)} + q\_i^{(t)}\\sum\_i q\_i^{(t)}{}^{\\top}\\frac{\\partial \\mathcal{L}}{\\partial e\_i^{(t)}}\\right)
\\end{aligned}\\end{equation}
可以看到：

> 1、$W$是基于全体被选中的编码的梯度之和来更新的，所以它自然会更倾向于高利用率方向；
>
> 2、由于$q\_i^{(t)}\\sum\_i q\_i^{(t)}{}^{\\top}\\frac{\\partial \\mathcal{L}}{\\partial e\_i^{(t)}}$的存在，不管编码$i$有没有被选中，它的更新都几乎不会为零；
>
> 3、$q\_i^{(t)}\\sum\_i q\_i^{(t)}{}^{\\top}\\frac{\\partial \\mathcal{L}}{\\partial e\_i^{(t)}}$相当于是高利用率方向的投影，它使得每个编码都往高利用率方向走。

然而，物极必反，如果全体编码都使劲往高利用率方向走，那么反而可能会导致编码表坍缩（codebook collapse），因此SimVQ默认采用了一个很保守的策略：只更新$W$，所有的$q$在随机初始化后就不更新了，这样一来就几乎杜绝了编码表坍缩的可能性。好消息是，在适当的编码维度下，实验显示$q,W$都更新和只更新$W$的表现都差不多，所以读者可以按照自己的偏好选择具体的形式。

## 延伸 [\#](https://kexue.fm/archives/10519\#%E5%BB%B6%E4%BC%B8)

抛开VQ的背景，像SimVQ这种引入额外的参数但又在数学上等价，即不改变模型的理论拟合能力，只改变优化过程的动力学的做法，我们称为“过参数化（Overparameterization）”。

过参数化在神经网络中并不鲜见，比如现在模型的主流架构是Pre Norm即$x + f(\\text{RMSNorm}(x))$，RMSNorm最后所乘的$\\gamma$向量通常都是过参数化的，因为$f$的第一层通常就是线性变换，比如Attention是线性变换投影到Q、K、V，FFN是线性变换来升维，等等，这些模型在推理阶段$\\gamma$向量完全可以合并到$f$的线性变换中，但鲜有看到在训练阶段就把$\\gamma$去掉的做法。

这是因为不少人认为，深度学习模型之所以“好训”，过参数化有不可忽视的作用，因此贸然去掉已经充分验证的模型的过参数化风险很大。这里的“好训”，主要是指梯度下降这种理论上容易陷入局部最优的方法居然经常可以找到一个实际表现很好的解，这本身就是一件很不可思议的事情。还有 [《On the Optimization of Deep Networks: Implicit Acceleration by Overparameterization》](https://papers.cool/arxiv/1802.06509) 等工作，表明过参数化隐式地加速了训练，作用类似于SGD中的动量。

最后，VQ本质上可以理解为一种稀疏训练方案，所以SimVQ所带来的启发和改动，也许还能用于其他稀疏训练模型，比如MoE（Mixture of Experts）。当前的MoE训练方案中，Expert之间的更新也是比较独立的，只有被Router选中的Expert才会更新参数，那么是不是有可能像SimVQ一样，所有的Expert后都接一个共享参数的线性变换，用来提高Expert的利用效率？当然MoE本身跟VQ也有很多不同之处，这还只是个猜测。

## 小结 [\#](https://kexue.fm/archives/10519\#%E5%B0%8F%E7%BB%93)

本文介绍了VQ（Vector Quantization）的另一个训练技巧——SimVQ——只在VQ的编码表多加一个线性变换，无需其他改动，就能达到加速收敛、提升编码利用率、降低重构损失等效果，相当简单有效。

_**转载到请包括本文地址：** [https://kexue.fm/archives/10519](https://kexue.fm/archives/10519)_

_**更详细的转载事宜请参考：**_ [《科学空间FAQ》](https://kexue.fm/archives/6508#%E6%96%87%E7%AB%A0%E5%A6%82%E4%BD%95%E8%BD%AC%E8%BD%BD/%E5%BC%95%E7%94%A8)

**如果您还有什么疑惑或建议，欢迎在下方评论区继续讨论。**

**如果您觉得本文还不错，欢迎 [分享](https://kexue.fm/archives/10519#share)/ [打赏](https://kexue.fm/archives/10519#pay) 本文。打赏并非要从中获得收益，而是希望知道科学空间获得了多少读者的真心关注。当然，如果你无视它，也不会影响你的阅读。再次表示欢迎和感谢！**

打赏

微信打赏

支付宝打赏

因为网站后台对打赏并无记录，因此欢迎在打赏时候备注留言。你还可以 [**点击这里**](http://mail.qq.com/cgi-bin/qm_share?t=qm_mailme&email=tN7d1drY3drrx8H0xcWa19vZ) 或在下方评论区留言来告知你的建议或需求。

**如果您需要引用本文，请参考：**

苏剑林. (Nov. 06, 2024). 《VQ的又一技巧：给编码表加一个线性变换 》\[Blog post\]. Retrieved from [https://kexue.fm/archives/10519](https://kexue.fm/archives/10519)

@online{kexuefm-10519,
        title={VQ的又一技巧：给编码表加一个线性变换},
        author={苏剑林},
        year={2024},
        month={Nov},
        url={\\url{https://kexue.fm/archives/10519}},
}

分类： [数学研究](https://kexue.fm/category/Mathematics), [信息时代](https://kexue.fm/category/Big-Data)    标签： [生成模型](https://kexue.fm/tag/%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B/), [编码](https://kexue.fm/tag/%E7%BC%96%E7%A0%81/), [梯度](https://kexue.fm/tag/%E6%A2%AF%E5%BA%A6/), [离散化](https://kexue.fm/tag/%E7%A6%BB%E6%95%A3%E5%8C%96/)[21 评论](https://kexue.fm/archives/10519#comments)

< [低秩近似之路（四）：ID](https://kexue.fm/archives/10501) \| [当Batch Size增大时，学习率该如何随之变化？](https://kexue.fm/archives/10542) >

### 你也许还对下面的内容感兴趣

- [msign的导数](https://kexue.fm/archives/11025)
- [生成扩散模型漫谈（三十）：从瞬时速度到平均速度](https://kexue.fm/archives/10958)
- [Transformer升级之路：20、MLA究竟好在哪里？](https://kexue.fm/archives/10907)
- [SVD的导数](https://kexue.fm/archives/10878)
- [通过梯度近似寻找Normalization的替代品](https://kexue.fm/archives/10831)
- [MoE环游记：4、难处应当多投入](https://kexue.fm/archives/10815)
- [高阶muP：更简明但更高明的谱条件缩放](https://kexue.fm/archives/10795)
- [初探muP：超参数的跨模型尺度迁移规律](https://kexue.fm/archives/10770)
- [MoE环游记：3、换个思路来分配](https://kexue.fm/archives/10757)
- [Muon续集：为什么我们选择尝试Muon？](https://kexue.fm/archives/10739)

[发表你的看法](https://kexue.fm/archives/10519#comment_form)

youngsheen

November 6th, 2024

感谢苏神对我们工作的的介绍推广，也欢迎大家点个star。https://github.com/youngsheen/SimVQ。博客最后关于moe的猜想和我们不谋而合，在这个project启动之初我们就规划了moe的实验想要解决expert selection collapse的问题，因为vq和moe的框架实在太过相似。但是可惜的是，在我们初步的实验中并未在moe上奏效，再加上计算资源和时间紧张遂放弃了moe的部分。最近随着hunyuan moe的开源，又激起了我重启moe实验的想法haha～

[回复评论](https://kexue.fm/archives/10519/comment-page-1?replyTo=25703#respond-post-10519)

youngsheen 发表于
November 6th, 2024

补充一下，在主实验中我们没有优化q而是只优化了w，q从高斯采样完就固定了（虽然消融实验中同时优化qw也能work）。所以某种程度上来说他和vae有一种隐隐约约的联系，或许可以找到一个统一的方案合并vqvae和vae，但我还没想通。苏神怎么看simvq和vae的相似性。

[回复评论](https://kexue.fm/archives/10519/comment-page-1?replyTo=25704#respond-post-10519)

[苏剑林](https://kexue.fm) 发表于
November 7th, 2024

欢迎作者莅临指导！

1、MoE本身问题就比较多，可能还需要细细抽丝剥茧，一时半会确实不好出结果；

2、不过VQ跟MoE的训练技巧还是有明显差距的，MoE的主要目的是要省训练成本，训练期间不用碰到没有被select出来的expert，但VQ还是要跟全体code算距离，才能把q算出来；

3、关于只优化$W$的操作，我在博客也补充上去了，不过个人实测当code dim不大时，只优化$W$的效果貌似不如都优化。

[回复评论](https://kexue.fm/archives/10519/comment-page-1?replyTo=25710#respond-post-10519)

报告大王 发表于
November 28th, 2024

看这个论文的时候感觉方法很巧妙，我以为iclr能得高分呢。点进审稿意见看了下，审稿人也太离谱了，无了个大语。

[回复评论](https://kexue.fm/archives/10519/comment-page-1?replyTo=25851#respond-post-10519)

[苏剑林](https://kexue.fm) 发表于
December 1st, 2024

巧妙，但简单，而顶会的审稿人很难接受简单的东西，或者说，越简单的东西，他们越容易看懂，因此越容易提出各种稀奇古怪（乱七八糟）的问题并打低分。

[回复评论](https://kexue.fm/archives/10519/comment-page-1?replyTo=25882#respond-post-10519)

dt3t

November 10th, 2024

与 Mixture-of-Subspaces in Low-Rank Adaptation 这篇文章的方法异曲同工

[回复评论](https://kexue.fm/archives/10519/comment-page-1?replyTo=25728#respond-post-10519)

[苏剑林](https://kexue.fm) 发表于
November 15th, 2024

这个联系就有点勉强了。

[回复评论](https://kexue.fm/archives/10519/comment-page-1?replyTo=25768#respond-post-10519)

Kiro

November 20th, 2024

虽然不做这个方向，但是也去看了一下原文。
从结果来看，我感觉似乎C其实是weight vector，而W其实才是Codebook。C随机初始化后frozen，相当于从原始的one-hot式选择code变成了选择加权系数vector来融合code。

[回复评论](https://kexue.fm/archives/10519/comment-page-1?replyTo=25818#respond-post-10519)

[苏剑林](https://kexue.fm) 发表于
November 24th, 2024

非常棒，前几天我也刚好想到这一点，确实也可以理解为将code的原始编码从高维的one hot改为低维的随机采样，但这只能解释fixed住C的做法了，如果C也可训练，那还是本文的解释更适合一些。

[回复评论](https://kexue.fm/archives/10519/comment-page-1?replyTo=25838#respond-post-10519)

虚谷

December 10th, 2024

苏老师您好，跟您请教一个基本的问题，很多论文中证明codebook ultization接近100%，请问论文中的码本利用率的计算方式是什么，是统计训练过程中codebook中每一个ei都用到了吗，只要ei被使用过一次也算是有效统计吗？

[回复评论](https://kexue.fm/archives/10519/comment-page-1?replyTo=25932#respond-post-10519)

[苏剑林](https://kexue.fm) 发表于
December 11th, 2024

好像是这样，我没细看，你可能要自己确认一下。

[回复评论](https://kexue.fm/archives/10519/comment-page-1?replyTo=25955#respond-post-10519)

申骋昊 发表于
December 27th, 2024

他代码里就是在训练的时候只要用过了就算利用到了，其实我认为应该在验证集中统计，并且每次都从0开始统计。

[回复评论](https://kexue.fm/archives/10519/comment-page-1?replyTo=26108#respond-post-10519)

ghrua

December 11th, 2024

苏老师好，刚回顾了三篇你写的关于 VQ 的文章，收获很多。我想分享一个回读论文的时候发现一个有意思的点：

我是NLP出身，所以一个很自然的想法是用 Gumble-softmax 解决 argmin/argmax 问题。这是一个比 STE 以及 FSQ 更自然简单的方式实现 end-to-end training。如果加W可以work，我感觉Gumble也应该会可以work的，但是似乎VQVAE论文有提到说：

"Recently a few authors have suggested the use of a new continuous reparemetrisation based on the
so-called Concrete \[25\] or Gumbel-softmax \[19\] distribution, which is a continuous distribution and
has a temperature constant that can be annealed during training to converge to a discrete distribution
in the limit. In the beginning of training the variance of the gradients is low but biased, and towards
the end of training the variance becomes high but unbiased. "

这点让我还挺惊讶的。

[回复评论](https://kexue.fm/archives/10519/comment-page-1?replyTo=25957#respond-post-10519)

[苏剑林](https://kexue.fm) 发表于
December 16th, 2024

用Gumbel Softmax去训练VQ-VAE效果确实会变差，这一点我最近也在思考中～

[回复评论](https://kexue.fm/archives/10519/comment-page-1?replyTo=25986#respond-post-10519)

Chandery

December 14th, 2024

苏老师，看您的博客学的VAE，准备学习扩散模型，看了您的VQVAE，受益匪浅，但是没找到关于VQGAN的文章呀，这是LDM很关键的一步，是我找漏了嘛

[回复评论](https://kexue.fm/archives/10519/comment-page-1?replyTo=25982#respond-post-10519)

[苏剑林](https://kexue.fm) 发表于
December 16th, 2024

VQGAN就是VQVAE加上判别器和对抗Loss，这是很自然的思路，不需要新开一篇博客介绍了。

[回复评论](https://kexue.fm/archives/10519/comment-page-1?replyTo=25994#respond-post-10519)

Chandery 发表于
December 19th, 2024

噢噢噢谢谢苏老师！真的受益匪浅

[回复评论](https://kexue.fm/archives/10519/comment-page-1?replyTo=26029#respond-post-10519)

Chandery 发表于
December 19th, 2024

没有啊，VQGAN里面结合了transformer用作训练的，这一部分是我比较不能理解的地方

[回复评论](https://kexue.fm/archives/10519/comment-page-1?replyTo=26030#respond-post-10519)

[苏剑林](https://kexue.fm) 发表于
December 20th, 2024

ViT不是很普遍了吗

[回复评论](https://kexue.fm/archives/10519/comment-page-1?replyTo=26049#respond-post-10519)

Yuancheng Wang

May 19th, 2025

苏神您好！关于 VQ-VAE 的 codebook 优化，有两个问题想请教：

1: 若仅优化投影矩阵 $W$ 而固定 codebook $E$，是否会限制模型的表达能力？直接优化 $E$：可以学习任意的 $N \\times d$ 矩阵，表达空间应该更自由，固定 $E$ 仅优化 $W$：可表达的向量是不是受限于 $E$ 的初始选择。

2: 原文采用：
$$ z\_q = z + \\text{sg}\[z\_q - z\] $$
$$ q = \\text{argmin}\_{e\_i \\in E} \\\| z\_q - eW \\\| $$
能否推广为：
$$ q = \\text{argmin}\_{e\_i \\in E} \\\| z\_q - f(e, E) \\\| $$
其中 $f(e, E)$ 是某种基于全体 codebook 的变换（拍脑袋想，比如注意力），这样也可以让所有的 code 参与更新了，而且也不会有问题 1 里的限制。

[回复评论](https://kexue.fm/archives/10519/comment-page-1?replyTo=27638#respond-post-10519)

[苏剑林](https://kexue.fm) 发表于
May 28th, 2025

1、可能会，看实验吧，但感觉影响不会太大；

2、都可以试，但应该不建议太复杂的，否则成本太大。

[回复评论](https://kexue.fm/archives/10519/comment-page-1?replyTo=27698#respond-post-10519)

[取消回复](https://kexue.fm/archives/10519#respond-post-10519)

你的大名

电子邮箱

个人网站（选填）

1\. 可以使用LaTeX代码，点击“预览效果”可查看效果；2. 可以通过点击评论楼层编号来引用该楼层；3. 网站可能会有点卡，如非确认评论失败，请 **不要重复点击提交**。

### 内容速览

[基础](https://kexue.fm/archives/10519#%E5%9F%BA%E7%A1%80)
[改动](https://kexue.fm/archives/10519#%E6%94%B9%E5%8A%A8)
[实验](https://kexue.fm/archives/10519#%E5%AE%9E%E9%AA%8C)
[分析](https://kexue.fm/archives/10519#%E5%88%86%E6%9E%90)
[延伸](https://kexue.fm/archives/10519#%E5%BB%B6%E4%BC%B8)
[小结](https://kexue.fm/archives/10519#%E5%B0%8F%E7%BB%93)

### 智能搜索

支持整句搜索！网站自动使用 [结巴分词](https://github.com/fxsjy/jieba) 进行分词，并结合ngrams排序算法给出合理的搜索结果。

### 热门标签

[生成模型](https://kexue.fm/tag/%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B/) [attention](https://kexue.fm/tag/attention/) [优化](https://kexue.fm/tag/%E4%BC%98%E5%8C%96/) [语言模型](https://kexue.fm/tag/%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/) [模型](https://kexue.fm/tag/%E6%A8%A1%E5%9E%8B/) [网站](https://kexue.fm/tag/%E7%BD%91%E7%AB%99/) [概率](https://kexue.fm/tag/%E6%A6%82%E7%8E%87/) [梯度](https://kexue.fm/tag/%E6%A2%AF%E5%BA%A6/) [转载](https://kexue.fm/tag/%E8%BD%AC%E8%BD%BD/) [微分方程](https://kexue.fm/tag/%E5%BE%AE%E5%88%86%E6%96%B9%E7%A8%8B/) [矩阵](https://kexue.fm/tag/%E7%9F%A9%E9%98%B5/) [天象](https://kexue.fm/tag/%E5%A4%A9%E8%B1%A1/) [分析](https://kexue.fm/tag/%E5%88%86%E6%9E%90/) [深度学习](https://kexue.fm/tag/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/) [积分](https://kexue.fm/tag/%E7%A7%AF%E5%88%86/) [python](https://kexue.fm/tag/python/) [优化器](https://kexue.fm/tag/%E4%BC%98%E5%8C%96%E5%99%A8/) [力学](https://kexue.fm/tag/%E5%8A%9B%E5%AD%A6/) [无监督](https://kexue.fm/tag/%E6%97%A0%E7%9B%91%E7%9D%A3/) [扩散](https://kexue.fm/tag/%E6%89%A9%E6%95%A3/) [几何](https://kexue.fm/tag/%E5%87%A0%E4%BD%95/) [节日](https://kexue.fm/tag/%E8%8A%82%E6%97%A5/) [生活](https://kexue.fm/tag/%E7%94%9F%E6%B4%BB/) [文本生成](https://kexue.fm/tag/%E6%96%87%E6%9C%AC%E7%94%9F%E6%88%90/) [数论](https://kexue.fm/tag/%E6%95%B0%E8%AE%BA/)

### 随机文章

- [生成扩散模型漫谈（二十八）：分步理解一致性模型](https://kexue.fm/archives/10633)
- [流体静力平衡的应用](https://kexue.fm/archives/1964)
- [高斯型积分的微扰展开（三）](https://kexue.fm/archives/3280)
- [数学竞赛广东预赛\|组成三角形的概率](https://kexue.fm/archives/1477)
- [深度学习的互信息：无监督提取特征](https://kexue.fm/archives/6024)
- [FlatNCE：小批次对比学习效果差的原因竟是浮点误差？](https://kexue.fm/archives/8586)
- [【理解黎曼几何】6\. 曲率的计数与计算(Python)](https://kexue.fm/archives/4026)
- [生成扩散模型漫谈（二十五）：基于恒等式的蒸馏（上）](https://kexue.fm/archives/10085)
- [【分享】兴隆山的双子座流星雨](https://kexue.fm/archives/3580)
- [居然是他！奥巴马获得2009年诺贝尔和平奖！](https://kexue.fm/archives/188)

### 最近评论

- [tll1945tll1937](https://kexue.fm/archives/10266/comment-page-1#comment-27901): 真心实意的向大家请教问题：看了文章“对齐全量微调！这是我看过最精彩的LoRA改进（二）”，我实...
- [oYo\_logan](https://kexue.fm/archives/10757/comment-page-1#comment-27897): \[comment=27017\]苏剑林\[/comment\]苏神，想请教一下，我理解在一个batc...
- [z123](https://kexue.fm/archives/10592/comment-page-1#comment-27896): 在参数矩阵较多的CNN小模型上，Muon会明显慢于Adam，这方面有什么优化提速的方案吗？
- [dry](https://kexue.fm/archives/10958/comment-page-2#comment-27895): 苏神好，一直有个疑问，ReFlow构建的ODE是$dx\_t/dt=x\_1-x\_0$，为什么这并...
- [tyj](https://kexue.fm/archives/10958/comment-page-2#comment-27894): 感觉和之前的一篇文章很像，应该算是concurrent work： https://arxiv...
- [li6626](https://kexue.fm/archives/10667/comment-page-1#comment-27893): 苏老师，Normalizing Flow有了新进展，论文链接:https://arxiv.or...
- [tesslqy](https://kexue.fm/archives/4718/comment-page-1#comment-27891): Evans那本书感觉就挺好的，不过长了一点且有点难（姜萍拿来装的就是这本书，可见流传度之广
- [Bauree](https://kexue.fm/archives/443/comment-page-1#comment-27890): 渴望大图，万分感谢！
- [tesslqy](https://kexue.fm/archives/4187/comment-page-1#comment-27888): 感觉应该是狄拉克测度，不过我也没系统学过Lebesgue以外的测度只是有这个感觉
- [tesslqy](https://kexue.fm/archives/4187/comment-page-1#comment-27887): 假设只有作者懂，但我不是作者，我懂了，假设不成立

### 友情链接

- [Cool Papers](https://papers.cool)
- [数学研发](https://bbs.emath.ac.cn)
- [Seatop](http://www.seatop.com.cn/)
- [Xiaoxia](https://xiaoxia.org/)
- [积分表-网络版](https://kexue.fm/sci/integral/index.html)
- [丝路博傲](http://blog.dvxj.com/)
- [ph4ntasy 饭特稀](http://www.ph4ntasy.com/)
- [数学之家](http://www.2math.cn/)
- [有趣天文奇观](http://interesting-sky.china-vo.org/)
- [TwistedW](http://www.twistedwg.com/)
- [godweiyang](https://godweiyang.com/)
- [AI柠檬](https://blog.ailemon.net/)
- [王登科-DK博客](https://greatdk.com)
- [ESON](https://blog.eson.org/)
- [枫之羽](https://fzhiy.net/)
- [Mathor's blog](https://wmathor.com/)
- [coding-zuo](https://coding-zuo.github.io/)
- [博科园](https://www.bokeyuan.net/)
- [孔皮皮的博客](https://www.kppkkp.top/)
- [运鹏的博客](https://yunpengtai.top/)
- [jiming.site](https://jiming.site/)
- [OmegaXYZ](https://www.omegaxyz.com/)
- [Blog by Eacls](https://www.eacls.top/)
- [EAI猩球](https://www.robotech.ink/)
- [文举的博客](https://liwenju0.com/)
- [用代码打点酱油](https://bruceyuan.com/)
- [申请链接](https://kexue.fm/links.html)

本站采用创作共用版权协议，要求署名、非商业用途和保持一致。转载本站内容必须也遵循“ [署名-非商业用途-保持一致](http://creativecommons.org/licenses/by-nc-nd/2.5/cn/)”的创作共用协议。
© 2009-2025 Scientific Spaces. All rights reserved. Theme by [laogui](http://www.laogui.com). Powered by [Typecho](http://typecho.org). 备案号: [粤ICP备09093259号-1/2](https://beian.miit.gov.cn/)。