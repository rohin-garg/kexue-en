## SEARCH

## MENU

- [打赏](https://kexue.fm/reward.html)
- [公式](https://kexue.fm/latex.html)
- [天象](https://kexue.fm/ac.html)
- [链接](https://kexue.fm/links.html)
- [时光](https://kexue.fm/me.html)
- [博览](https://kexue.fm/science.html)
- [归档](https://kexue.fm/content.html)

## CATEGORIES

- [千奇百怪](https://kexue.fm/category/Everything)
- [天文探索](https://kexue.fm/category/Astronomy)
- [数学研究](https://kexue.fm/category/Mathematics)
- [物理化学](https://kexue.fm/category/Phy-chem)
- [信息时代](https://kexue.fm/category/Big-Data)
- [生物自然](https://kexue.fm/category/Biology)
- [图片摄影](https://kexue.fm/category/Photograph)
- [问题百科](https://kexue.fm/category/Questions)
- [生活/情感](https://kexue.fm/category/Life-Feeling)
- [资源共享](https://kexue.fm/category/Resources)

## NEWPOSTS

- [为什么Adam的Update RM...](https://kexue.fm/archives/11267)
- [重新思考学习率与Batch Siz...](https://kexue.fm/archives/11260)
- [Cool Papers更新：简单适...](https://kexue.fm/archives/11250)
- [流形上的最速下降：4\. Muon ...](https://kexue.fm/archives/11241)
- [ReLU/GeLU/Swish的一...](https://kexue.fm/archives/11233)
- [流形上的最速下降：3\. Muon ...](https://kexue.fm/archives/11221)
- [流形上的最速下降：2\. Muon ...](https://kexue.fm/archives/11215)
- [基于树莓派Zero2W搭建一个随身旁路由](https://kexue.fm/archives/11206)
- [流形上的最速下降：1\. SGD ...](https://kexue.fm/archives/11196)
- [矩阵r次方根和逆r次方根的高效计算](https://kexue.fm/archives/11175)

## COMMENTS

- [ameowcat: 苏神您好，有个问题想请教一下，最近扩散模型的推理优化有一篇文章...](https://kexue.fm/archives/10958/comment-page-3#comment-28529)
- [Eliot: 2 实现loss-free with budget应当是在当前...](https://kexue.fm/archives/10815/comment-page-1#comment-28528)
- [Eliot: 继续阅读这2份代码后，大概结论如下\
1 megatron-lm...](https://kexue.fm/archives/10815/comment-page-1#comment-28527)
- [lzyyzl: 帮忙解惑一下\
1 文中提到本文主题是求O=msign(M)的导...](https://kexue.fm/archives/11025/comment-page-1#comment-28525)
- [z: 牛](https://kexue.fm/archives/11267/comment-page-1#comment-28524)
- [Gusto: 按照个人理解将weight decay理解为损失函数中的惩罚项...](https://kexue.fm/archives/10739/comment-page-2#comment-28523)
- [hazdzz: 非常感谢您提到 pbSGD，这启发了我的毕业论文！](https://kexue.fm/archives/11196/comment-page-1#comment-28522)
- [苏剑林: 你是指xml代码？那不是乱码，feed就是xml格式，你要自己...](https://kexue.fm/content.html/comment-page-1#comment-28521)
- [苏剑林: 都是我人工整理的，只要论文集是公开可访问、没有反爬虫的，理论上...](https://kexue.fm/archives/11250/comment-page-1#comment-28520)
- [苏剑林: 按照平均budget来算一个静态scaling factor，...](https://kexue.fm/archives/10945/comment-page-1#comment-28519)

## USERLOGIN

- [登录](https://kexue.fm/admin/login.php)

[科学空间\|Scientific Spaces](https://kexue.fm)

- [登录](https://kexue.fm/admin/login.php)
- [打赏](https://kexue.fm/reward.html)
- [公式](https://kexue.fm/latex.html)
- [天象](https://kexue.fm/ac.html)
- [链接](https://kexue.fm/links.html)
- [时光](https://kexue.fm/me.html)
- [博览](https://kexue.fm/science.html)
- [归档](https://kexue.fm/content.html)

渴望成为一个小飞侠

- [欢迎订阅](https://kexue.fm/feed)
- [个性邮箱](https://kexue.fm/archives/119)
- [天象信息](https://kexue.fm/ac.html)
- [观测ISS](https://kexue.fm/archives/41)
- [LaTeX](https://kexue.fm/latex.html)
- [关于博主](https://kexue.fm/me.html)

欢迎访问“科学空间”，这里将与您共同探讨自然科学，回味人生百态；也期待大家的分享～

- [**千奇百怪** Everything](https://kexue.fm/category/Everything)
- [**天文探索** Astronomy](https://kexue.fm/category/Astronomy)
- [**数学研究** Mathematics](https://kexue.fm/category/Mathematics)
- [**物理化学** Phy-chem](https://kexue.fm/category/Phy-chem)
- [**信息时代** Big-Data](https://kexue.fm/category/Big-Data)
- [**生物自然** Biology](https://kexue.fm/category/Biology)
- [**图片摄影** Photograph](https://kexue.fm/category/Photograph)
- [**问题百科** Questions](https://kexue.fm/category/Questions)
- [**生活/情感** Life-Feeling](https://kexue.fm/category/Life-Feeling)
- [**资源共享** Resources](https://kexue.fm/category/Resources)

- [**千奇百怪**](https://kexue.fm/category/Everything)
- [**天文探索**](https://kexue.fm/category/Astronomy)
- [**数学研究**](https://kexue.fm/category/Mathematics)
- [**物理化学**](https://kexue.fm/category/Phy-chem)
- [**信息时代**](https://kexue.fm/category/Big-Data)
- [**生物自然**](https://kexue.fm/category/Biology)
- [**图片摄影**](https://kexue.fm/category/Photograph)
- [**问题百科**](https://kexue.fm/category/Questions)
- [**生活/情感**](https://kexue.fm/category/Life-Feeling)
- [**资源共享**](https://kexue.fm/category/Resources)

[首页](https://kexue.fm) [数学研究](https://kexue.fm/category/Mathematics) 从几何视角来理解模型参数的初始化策略

16Jan

# [从几何视角来理解模型参数的初始化策略](https://kexue.fm/archives/7180)

By 苏剑林 \|
2020-01-16 \|
138311位读者\|

对于复杂模型来说，参数的初始化显得尤为重要。糟糕的初始化，很多时候已经不单是模型效果变差的问题了，还更有可能是模型根本训练不动或者不收敛。在深度学习中常见的自适应初始化策略是Xavier初始化，它是从正态分布$\\mathcal{N}\\left(0,\\frac{2}{fan\_{in} + fan\_{out}}\\right)$中随机采样而构成的初始权重，其中$fan\_{in}$是输入的维度而$fan\_{out}$是输出的维度。其他初始化策略基本上也类似，只不过假设有所不同，导致最终形式略有差别。

标准的初始化策略的推导是基于概率统计的，大概的思路是假设输入数据的均值为0、方差为1，然后期望输出数据也保持均值为0、方差为1，然后推导出初始变换应该满足的均值和方差条件。这个过程理论上没啥问题，但在笔者看来依然不够直观，而且推导过程的假设有点多。本文则希望能从 **几何视角** 来理解模型的初始化方法，给出一个更直观的推导过程。

## 信手拈来的正交 [\#](https://kexue.fm/kexue.fm\#%E4%BF%A1%E6%89%8B%E6%8B%88%E6%9D%A5%E7%9A%84%E6%AD%A3%E4%BA%A4)

前者时间笔者写了 [《n维空间下两个随机向量的夹角分布》](https://kexue.fm/archives/7076)，其中的一个推论是

> **推论1**： 高维空间中的任意两个随机向量几乎都是垂直的。

事实上，推论1正是本文整个几何视角的出发点！它的一个更进一步的推论是：

> **推论2**： 从$\\mathcal{N}(0, 1/n)$中随机选取$n^2$个数，组成一个$n\\times n$的矩阵，这个矩阵近似为正交矩阵，且$n$越大，近似程度越好。

不信的读者也可以数值验证一下：

```
import numpy as np

n = 100
W = np.random.randn(n, n) / np.sqrt(n)
X = np.dot(W.T, W) # 矩阵乘以自身的转置
print(X) # 看看是否接近单位阵
print(np.square(X - np.eye(n)).mean()) # 计算与单位阵的mse

```

相信对于多数读者来说，第一次看到这个推论2或多或少都会觉得惊讶。正交矩阵是指满足$\\boldsymbol{W}^{\\top}\\boldsymbol{W}=\\boldsymbol{I}$的矩阵，也就是说它的逆等于转置。一般矩阵的逆和转置的求解难度差得不是一点点，所以给我们的感觉是“逆=转置”是一个很苛刻的条件才对，但推论2却告诉我们随机采样而来的矩阵就已经接近正交矩阵了，不得不说有点反直觉。当初笔者刚意识到这一点时，也是感觉挺惊讶的。

## 其实也没那么难理解 [\#](https://kexue.fm/kexue.fm\#%E5%85%B6%E5%AE%9E%E4%B9%9F%E6%B2%A1%E9%82%A3%E4%B9%88%E9%9A%BE%E7%90%86%E8%A7%A3)

不过，当我们习惯了推论1“高维空间中的任意两个随机向量几乎都是垂直的”这个事实后，我们确实可以很快地理解并导出这个结果。快速推导的时候，我们可以先考虑标准正态分布$\\mathcal{N}(0,1)$，注意到推论1要求采样的方向均匀，而标准正态分布正好满足这个要求。从$\\mathcal{N}(0,1)$中采样到一个$n\\times n$的矩阵，我们可以把它看成是$n$个$n$维向量，那既然这$n$个向量都是随机向量，所以它们两两之间自然就接近正交了。

当然，两两正交还不是正交矩阵，因为正交矩阵还要求每个向量的模长为1，而我们有$\\mathbb{E}\_{x\\sim \\mathcal{N}(0,1)}\\left\[x^2\\right\]=1$，所以这意味着从$\\mathcal{N}(0,1)$中采样出的$n$维向量模长近似为$\\sqrt{n}$，所以为了接近正交，还需要将每个元素除以$\\sqrt{n}$，这等价于采样方差由1变成了$1/n$。

此外，采样分布还不一定要是正态分布，比如均匀分布$U\\left\[-\\sqrt{3/n}, \\sqrt{3/n}\\right\]$也行。事实上我们有

> **推论3**： 从任意的均值为0、方差为$1/n$的分布$p(x)$中独立重复采样出来的$n\\times n$矩阵，都接近正交矩阵。

我们可以从一个更数学化的角度来理解推论3：假设$\\boldsymbol{x}=(x\_1,x\_2,\\dots,x\_n),\\boldsymbol{y}=(y\_1,y\_2,\\dots,y\_n)$都是从$p(x)$中采样出来的，那么有
\\begin{equation}\\begin{aligned}\\langle \\boldsymbol{x}, \\boldsymbol{y}\\rangle =&\\, n\\times \\frac{1}{n}\\sum\_{k=1}^n x\_k y\_k\\\
\\approx&\\, n\\times \\mathbb{E}\_{x\\sim p(x),y\\sim p(x)}\[xy\]\\\
=&\\, n\\times \\mathbb{E}\_{x\\sim p(x)}\[x\]\\times \\mathbb{E}\_{y\\sim p(x)}\[y\]\\\
=&\\,0\\end{aligned}\\end{equation}
以及
\\begin{equation}\\begin{aligned}\\Vert\\boldsymbol{x}\\Vert^2 =&\\, n\\times \\frac{1}{n}\\sum\_{k=1}^n x\_k^2\\\
\\approx&\\, n\\times \\mathbb{E}\_{x\\sim p(x)}\\left\[x^2\\right\]\\\
=&\\, n\\times \\left(\\mu^2 + \\sigma^2\\right)\\\
=&\\,1\\end{aligned}\\end{equation}
所以任意两个向量都是接近正交归一的，因此采样出来的矩阵也接近正交矩阵。

## 现在可以说初始化了 [\#](https://kexue.fm/kexue.fm\#%E7%8E%B0%E5%9C%A8%E5%8F%AF%E4%BB%A5%E8%AF%B4%E5%88%9D%E5%A7%8B%E5%8C%96%E4%BA%86)

说了那么多正交矩阵的内容，其实本质上都是为理解初始化方法的几何意义做铺垫。如果读者对线性代数还有印象的话，那么应该还记得正交矩阵的重要意义在于它在变换过程中保持了向量的模长不变。用数学公式来表达，就是设$\\boldsymbol{W}\\in \\mathbb{R}^{n\\times n}$是一个正交矩阵，而$\\boldsymbol{x}\\in\\mathbb{R}^n$是任意向量，则$\\boldsymbol{x}$的模长等于$\\boldsymbol{W}\\boldsymbol{x}$的模长：
\\begin{equation}\\Vert\\boldsymbol{W}\\boldsymbol{x}\\Vert^2 = \\boldsymbol{x}^{\\top}\\boldsymbol{W}^{\\top}\\boldsymbol{W}\\boldsymbol{x}=\\boldsymbol{x}^{\\top}\\boldsymbol{x}=\\Vert\\boldsymbol{x}\\Vert^2\\end{equation}
考虑全连接层：
\\begin{equation}\\boldsymbol{y}=\\boldsymbol{W}\\boldsymbol{x} + \\boldsymbol{b}\\end{equation}
深度学习模型本身上就是一个个全连接层的嵌套，所以为了使模型最后的输出不至于在初始化阶段就过于“膨胀”或者“退化”，一个想法就是让模型在初始化时能保持模长不变。

这个想法形成的一个自然的初始化策略就是“以全零初始化$\\boldsymbol{b}$，以随机正交矩阵初始化$\\boldsymbol{W}$”。而推论2就已经告诉我们，从$\\mathcal{N}(0, 1/n)$采样而来的$n\\times n$矩阵就已经接近正交矩阵了，所以我们可以从$\\mathcal{N}(0, 1/n)$采样来初始化$\\boldsymbol{W}$。这便是Xavier初始化策略了，有些框架也叫Glorot初始化，因为作者叫Xavier Glorot～此外，采样分布也不一定是$\\mathcal{N}(0, 1/n)$，前面推论3说了你可以从任意均值为0、方差为$1/n$的分布中采样。

上面说的是输入和输出维度都是$n$的情况，如果输入是$n$维，输出是$m$维呢？这时候$\\boldsymbol{W}\\in\\mathbb{R}^{m\\times n}$，保持$\\boldsymbol{W}\\boldsymbol{x}$模长不变的条件依然是$\\boldsymbol{W}^{\\top}\\boldsymbol{W}=\\boldsymbol{I}$。然而，当$m < n$时，这是不可能的；当$m \\geq n$时，这是有可能成立的，并且根据前面相似的推导，我们可以得到

> **推论4**： 当$m \\geq n$时，从任意的均值为0、方差为$1/m$的分布$p(x)$中独立重复采样出来的$m\\times n$矩阵，近似满足$\\boldsymbol{W}^{\\top}\\boldsymbol{W}=\\boldsymbol{I}$。

所以，如果$m > n$，那么只需要把采样分布的方差改为$1/m$就好，至于$m < n$时，虽然没有直接的推导，但仍然可以沿用这个做法，毕竟合理的策略应该是普适的。注意，这个改动跟Xavier初始化的原始设计有点不一样，它是“LeCun初始化”的对偶版本（LeCun初始化方差是$1/n$），而Xavier初始化的方差则是$2/(m+n)$，这平均了前向传播和反向传播的直觉做法，而我们这里主要考虑的是前向传播。

可能还会有读者疑问：你这里只是考虑了没有激活函数的场景，就算$\\boldsymbol{y}$的模长跟$\\boldsymbol{x}$一样，但$\\boldsymbol{y}$经过激活函数后就不一样了。确实是存在这样的情况，而且这时候只能针对具体问题具体分析。比如$\\tanh(x)$在$x$比较小的时候有$\\tanh(x)\\approx x$，所以可以认为Xavier初始化直接适用于$\\tanh$激活；再比如$\\text{relu}$时可以认为$\\text{relu}(\\boldsymbol{y})$会有大约一半的元素被置零，所以模长大约变为原来的$1/\\sqrt{2}$，而要保持模长不变，可以让$\\boldsymbol{W}$乘上$\\sqrt{2}$，也就是说初始化方差从$1/m$变成$2/m$，这就是何恺明大神提出来的针对$\\text{relu}$的初始化策略。

当然，事实上很难针对每一个激活函数都做好方差的调整，所以一个更通用的做法就是直接在激活函数后面加上一个类似Layer Normalization的操作，直接显式地恢复模长。这时候就轮到各种Normalization技巧登场了～（欢迎继续阅读旧作 [《BN究竟起了什么作用？一个闭门造车的分析》](https://kexue.fm/archives/6992)。）

## 还有一点小结 [\#](https://kexue.fm/kexue.fm\#%E8%BF%98%E6%9C%89%E4%B8%80%E7%82%B9%E5%B0%8F%E7%BB%93)

本文主要是从“高维空间中的任意两个随机向量几乎都是垂直的”这个结论推导出“均值为0、方差为$1/n$的任意$n\\times n$矩阵接近于正交矩阵”，继而给出了相关初始化策略的一个几何视角。窃以为这个几何视角相比纯统计视角要更直观易懂一些。

_**转载到请包括本文地址：** [https://kexue.fm/archives/7180](https://kexue.fm/archives/7180)_

_**更详细的转载事宜请参考：**_ [《科学空间FAQ》](https://kexue.fm/archives/6508#%E6%96%87%E7%AB%A0%E5%A6%82%E4%BD%95%E8%BD%AC%E8%BD%BD/%E5%BC%95%E7%94%A8)

**如果您还有什么疑惑或建议，欢迎在下方评论区继续讨论。**

**如果您觉得本文还不错，欢迎 [分享](https://kexue.fm/kexue.fm#share)/ [打赏](https://kexue.fm/kexue.fm#pay) 本文。打赏并非要从中获得收益，而是希望知道科学空间获得了多少读者的真心关注。当然，如果你无视它，也不会影响你的阅读。再次表示欢迎和感谢！**

打赏

微信打赏

支付宝打赏

因为网站后台对打赏并无记录，因此欢迎在打赏时候备注留言。你还可以 [**点击这里**](http://mail.qq.com/cgi-bin/qm_share?t=qm_mailme&email=tN7d1drY3drrx8H0xcWa19vZ) 或在下方评论区留言来告知你的建议或需求。

**如果您需要引用本文，请参考：**

苏剑林. (Jan. 16, 2020). 《从几何视角来理解模型参数的初始化策略 》\[Blog post\]. Retrieved from [https://kexue.fm/archives/7180](https://kexue.fm/archives/7180)

@online{kexuefm-7180,
        title={从几何视角来理解模型参数的初始化策略},
        author={苏剑林},
        year={2020},
        month={Jan},
        url={\\url{https://kexue.fm/archives/7180}},
}

分类： [数学研究](https://kexue.fm/category/Mathematics)    标签： [模型](https://kexue.fm/tag/%E6%A8%A1%E5%9E%8B/), [概率](https://kexue.fm/tag/%E6%A6%82%E7%8E%87/), [几何](https://kexue.fm/tag/%E5%87%A0%E4%BD%95/), [优化](https://kexue.fm/tag/%E4%BC%98%E5%8C%96/)[42 评论](https://kexue.fm/archives/7180#comments)

< [Self-Orthogonality Module：一个即插即用的核正交化模块](https://kexue.fm/archives/7169) \| [抛开约束，增强模型：一行代码提升albert表现](https://kexue.fm/archives/7187) >

### 你也许还对下面的内容感兴趣

- [QK-Clip：让Muon在Scaleup之路上更进一步](https://kexue.fm/archives/11126)
- [Transformer升级之路：21、MLA好在哪里?（下）](https://kexue.fm/archives/11111)
- [MoE环游记：5、均匀分布的反思](https://kexue.fm/archives/10945)
- [Transformer升级之路：20、MLA好在哪里?（上）](https://kexue.fm/archives/10907)
- [一道概率不等式：盯着它到显然成立为止！](https://kexue.fm/archives/10902)
- [MoE环游记：4、难处应当多投入](https://kexue.fm/archives/10815)
- [MoE环游记：1、从几何意义出发](https://kexue.fm/archives/10699)
- [三个球的交点坐标（三球交会定位）](https://kexue.fm/archives/10684)
- [为什么梯度裁剪的默认模长是1？](https://kexue.fm/archives/10657)
- [从谱范数梯度到新式权重衰减的思考](https://kexue.fm/archives/10648)

[发表你的看法](https://kexue.fm/kexue.fm#comment_form)

1. [«](https://kexue.fm/archives/7180/comment-page-1#comments)
2. [1](https://kexue.fm/archives/7180/comment-page-1#comments)
3. [2](https://kexue.fm/archives/7180/comment-page-2#comments)

wa007

April 6th, 2022

请教苏神~~
Xavier 初始化的出发点是保证 输入向量和输出向量 方差一致，以保证输入和输出向量的稀疏性一致，从以保证梯度的稳定。与苏神的「让模型在初始化时能保持模长不变」想法不太一样。
「方差一致」和「模长不变」这两者会有什么深层的联系吗？不知道苏神对此有没有什么看法~~

[回复评论](https://kexue.fm/archives/7180/comment-page-2?replyTo=18868#respond-post-7180)

[苏剑林](https://kexue.fm) 发表于
April 7th, 2022

根据公式
$$\\mathbb{E}\[x^2\]=\\mathbb{E}\[x\]^2 + \\mathbb{V}ar\[x\]=\\mu^2+\\sigma^2$$
当均值为0时，二阶矩$\\mathbb{E}\[x^2\]$跟方差是等价的。

而二阶矩
$$\\mathbb{E}\[x^2\]\\approx \\frac{1}{n}\\sum\_{i=1}^n x\_i^2 = \\frac{1}{n}\\Vert(x\_1,x\_2,\\cdots,x\_n)\\Vert^2$$
也就是说二阶矩跟向量模长相当于只差一个倍数关系。

[回复评论](https://kexue.fm/archives/7180/comment-page-2?replyTo=18882#respond-post-7180)

hazdzz

May 20th, 2023

我剛看到有一篇 paper （https://arxiv.org/abs/2305.09828）用你提出的推論4提出了一個 initialization 用於 self-attention。

[回复评论](https://kexue.fm/archives/7180/comment-page-2?replyTo=21685#respond-post-7180)

[苏剑林](https://kexue.fm) 发表于
May 20th, 2023

这个是通过扰动单位阵并SVD分解来构建初始化，貌似跟我这个联系不大～

[回复评论](https://kexue.fm/archives/7180/comment-page-2?replyTo=21692#respond-post-7180)

笑雨

July 2nd, 2024

都是很多奇奇怪怪的角度，感性的理科思维。真是厉害。我们一般人感觉不到，也不愿意去感觉，脑瓜居然可以整天想这些

[回复评论](https://kexue.fm/archives/7180/comment-page-2?replyTo=24679#respond-post-7180)

笑雨 发表于
July 2nd, 2024

我再次愿意相信：存在即是道理。发现了现象，就可以思考后面是否有什么道道。可是客观自己数学不好，动手不勤，慢，懒，疑

[回复评论](https://kexue.fm/archives/7180/comment-page-2?replyTo=24680#respond-post-7180)

jorjiang

December 27th, 2024

W.T @ W=I是\|\|Wx\|\| = \|\|x\|\|模长不变的充分条件而不是必要条件。
实际上让\|\|Wx\|\| = \|\|x\|\|显然是可以实现的，而且不难

[回复评论](https://kexue.fm/archives/7180/comment-page-2?replyTo=26111#respond-post-7180)

[苏剑林](https://kexue.fm) 发表于
January 7th, 2025

如果是恒等成立，那么就是充要条件。

因为$\\Vert\\boldsymbol{W}\\boldsymbol{x}\\Vert=\\Vert\\boldsymbol{x}\\Vert$等价于$\\boldsymbol{x}^{\\top}(\\boldsymbol{W}^{\\top}\\boldsymbol{W}-\\boldsymbol{I})\\boldsymbol{x}=0$，如果该式对于任意向量$\\boldsymbol{x}$恒成立，那么$\\boldsymbol{x}$换成任意矩阵也恒成立，于是代入$\\boldsymbol{x}=\\boldsymbol{I}$，就得到$\\boldsymbol{W}^{\\top}\\boldsymbol{W}-\\boldsymbol{I}=0$。

[回复评论](https://kexue.fm/archives/7180/comment-page-2?replyTo=26172#respond-post-7180)

wentao 发表于
July 31st, 2025

是不是可以这么理解：对于上述方程，需要对m维空间中的向量恒成立，则x.⊤@(W.⊤@W−I)的零空间需要是m维的，因此x.⊤@(W.⊤@W−I)的列空间只能是0维空间，因此W.⊤@W−I只能是0矩阵。

[回复评论](https://kexue.fm/archives/7180/comment-page-2?replyTo=28273#respond-post-7180)

[苏剑林](https://kexue.fm) 发表于
August 3rd, 2025

噢，可以，这样有点把问题复杂化了，不过你能理解就好。

[回复评论](https://kexue.fm/archives/7180/comment-page-2?replyTo=28286#respond-post-7180)

huleeee

May 14th, 2025

苏神你好，拜读了您上面演示的推导，浅显易懂，收获颇丰，但是有一个地方不是很明辩，您在提到正交矩阵$W$对于模长的报错作用时提到：“深度学习模型本身上就是一个个全连接层的嵌套，所以为了使模型最后的输出不至于在初始化阶段就过于“膨胀”或者“退化”，一个想法就是让模型在初始化时能保持模长不变。”这里提到初始化阶段的输出出现“膨胀”或者“退化”，究竟是什么意思？我理解的意思是，输出的均值出现偏移，方差出现较大变化，但是为什么要保持这种属性呢？

[回复评论](https://kexue.fm/archives/7180/comment-page-2?replyTo=27597#respond-post-7180)

jorjiang 发表于
May 15th, 2025

膨胀、退化对应梯度爆炸、消失

[回复评论](https://kexue.fm/archives/7180/comment-page-2?replyTo=27606#respond-post-7180)

[苏剑林](https://kexue.fm) 发表于
May 17th, 2025

就是输出向量的模长变大或缩小。

[回复评论](https://kexue.fm/archives/7180/comment-page-2?replyTo=27619#respond-post-7180)

1. [«](https://kexue.fm/archives/7180/comment-page-1#comments)
2. [1](https://kexue.fm/archives/7180/comment-page-1#comments)
3. [2](https://kexue.fm/archives/7180/comment-page-2#comments)

[取消回复](https://kexue.fm/archives/7180#respond-post-7180)

你的大名

电子邮箱

个人网站（选填）

1\. 可以使用LaTeX代码，点击“预览效果”可查看效果；2. 可以通过点击评论楼层编号来引用该楼层；3. 网站可能会有点卡，如非确认评论失败，请 **不要重复点击提交**。

### 内容速览

[信手拈来的正交](https://kexue.fm/kexue.fm#%E4%BF%A1%E6%89%8B%E6%8B%88%E6%9D%A5%E7%9A%84%E6%AD%A3%E4%BA%A4)
[其实也没那么难理解](https://kexue.fm/kexue.fm#%E5%85%B6%E5%AE%9E%E4%B9%9F%E6%B2%A1%E9%82%A3%E4%B9%88%E9%9A%BE%E7%90%86%E8%A7%A3)
[现在可以说初始化了](https://kexue.fm/kexue.fm#%E7%8E%B0%E5%9C%A8%E5%8F%AF%E4%BB%A5%E8%AF%B4%E5%88%9D%E5%A7%8B%E5%8C%96%E4%BA%86)
[还有一点小结](https://kexue.fm/kexue.fm#%E8%BF%98%E6%9C%89%E4%B8%80%E7%82%B9%E5%B0%8F%E7%BB%93)

### 智能搜索

支持整句搜索！网站自动使用 [结巴分词](https://github.com/fxsjy/jieba) 进行分词，并结合ngrams排序算法给出合理的搜索结果。

### 热门标签

[生成模型](https://kexue.fm/tag/%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B/) [attention](https://kexue.fm/tag/attention/) [优化](https://kexue.fm/tag/%E4%BC%98%E5%8C%96/) [语言模型](https://kexue.fm/tag/%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/) [模型](https://kexue.fm/tag/%E6%A8%A1%E5%9E%8B/) [网站](https://kexue.fm/tag/%E7%BD%91%E7%AB%99/) [概率](https://kexue.fm/tag/%E6%A6%82%E7%8E%87/) [梯度](https://kexue.fm/tag/%E6%A2%AF%E5%BA%A6/) [转载](https://kexue.fm/tag/%E8%BD%AC%E8%BD%BD/) [矩阵](https://kexue.fm/tag/%E7%9F%A9%E9%98%B5/) [微分方程](https://kexue.fm/tag/%E5%BE%AE%E5%88%86%E6%96%B9%E7%A8%8B/) [分析](https://kexue.fm/tag/%E5%88%86%E6%9E%90/) [优化器](https://kexue.fm/tag/%E4%BC%98%E5%8C%96%E5%99%A8/) [天象](https://kexue.fm/tag/%E5%A4%A9%E8%B1%A1/) [深度学习](https://kexue.fm/tag/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/) [积分](https://kexue.fm/tag/%E7%A7%AF%E5%88%86/) [python](https://kexue.fm/tag/python/) [力学](https://kexue.fm/tag/%E5%8A%9B%E5%AD%A6/) [无监督](https://kexue.fm/tag/%E6%97%A0%E7%9B%91%E7%9D%A3/) [扩散](https://kexue.fm/tag/%E6%89%A9%E6%95%A3/) [几何](https://kexue.fm/tag/%E5%87%A0%E4%BD%95/) [节日](https://kexue.fm/tag/%E8%8A%82%E6%97%A5/) [生活](https://kexue.fm/tag/%E7%94%9F%E6%B4%BB/) [文本生成](https://kexue.fm/tag/%E6%96%87%E6%9C%AC%E7%94%9F%E6%88%90/) [数论](https://kexue.fm/tag/%E6%95%B0%E8%AE%BA/)

### 随机文章

- [fashion-mnist的gan玩具](https://kexue.fm/archives/4540)
- [【理科生读小说】来谈谈“四两拨千斤”](https://kexue.fm/archives/5048)
- [《新理解矩阵2》：矩阵是什么？](https://kexue.fm/archives/1768)
- [再谈非方阵的行列式](https://kexue.fm/archives/6096)
- [生活中的趣味数学：同一天生日概率有多大](https://kexue.fm/archives/40)
- [从Knotsevich在黑板上写的级数题目谈起](https://kexue.fm/archives/3229)
- [【NASA每日一图】壮观的银河系](https://kexue.fm/archives/137)
- [【NASA每日一图】太阳系中的木卫三](https://kexue.fm/archives/130)
- [【个人翻译】变暖的地球对冷血动物来说过热？](https://kexue.fm/archives/11)
- [Transformer升级之路：12、无限外推的ReRoPE？](https://kexue.fm/archives/9708)

### 最近评论

- [ameowcat](https://kexue.fm/archives/10958/comment-page-3#comment-28529): 苏神您好，有个问题想请教一下，最近扩散模型的推理优化有一篇文章也是使用ode：https://...
- [Eliot](https://kexue.fm/archives/10815/comment-page-1#comment-28528): 2 实现loss-free with budget应当是在当前Megatron-LM基础上应当...
- [Eliot](https://kexue.fm/archives/10815/comment-page-1#comment-28527): 继续阅读这2份代码后，大概结论如下
1 megatron-lm应当只实现了经典的loss-fr...
- [lzyyzl](https://kexue.fm/archives/11025/comment-page-1#comment-28525): 帮忙解惑一下
1 文中提到本文主题是求O=msign(M)的导数。将∇ML表示为∇OL的函数也...
- [z](https://kexue.fm/archives/11267/comment-page-1#comment-28524): 牛
- [Gusto](https://kexue.fm/archives/10739/comment-page-2#comment-28523): 按照个人理解将weight decay理解为损失函数中的惩罚项的话，为什么weight dec...
- [hazdzz](https://kexue.fm/archives/11196/comment-page-1#comment-28522): 非常感谢您提到 pbSGD，这启发了我的毕业论文！
- [苏剑林](https://kexue.fm/content.html/comment-page-1#comment-28521): 你是指xml代码？那不是乱码，feed就是xml格式，你要自己找工具订阅。
- [苏剑林](https://kexue.fm/archives/11250/comment-page-1#comment-28520): 都是我人工整理的，只要论文集是公开可访问、没有反爬虫的，理论上都可以，当然实际上行不行，还得看...
- [苏剑林](https://kexue.fm/archives/10945/comment-page-1#comment-28519): 按照平均budget来算一个静态scaling factor，好像也是一个挺合理的事情？

### 友情链接

- [Cool Papers](https://papers.cool)
- [数学研发](https://bbs.emath.ac.cn)
- [Seatop](http://www.seatop.com.cn/)
- [Xiaoxia](https://xiaoxia.org/)
- [积分表-网络版](https://kexue.fm/sci/integral/index.html)
- [丝路博傲](http://blog.dvxj.com/)
- [数学之家](http://www.2math.cn/)
- [有趣天文奇观](http://interesting-sky.china-vo.org/)
- [TwistedW](http://www.twistedwg.com/)
- [godweiyang](https://godweiyang.com/)
- [AI柠檬](https://blog.ailemon.net/)
- [王登科-DK博客](https://greatdk.com)
- [ESON](https://blog.eson.org/)
- [枫之羽](https://fzhiy.net/)
- [Mathor's blog](https://wmathor.com/)
- [coding-zuo](https://coding-zuo.github.io/)
- [博科园](https://www.bokeyuan.net/)
- [孔皮皮的博客](https://www.kppkkp.top/)
- [运鹏的博客](https://yunpengtai.top/)
- [jiming.site](https://jiming.site/)
- [OmegaXYZ](https://www.omegaxyz.com/)
- [EAI猩球](https://www.robotech.ink/)
- [文举的博客](https://liwenju0.com/)
- [用代码打点酱油](https://bruceyuan.com/)
- [Zhang's blog](https://armcvai.cn/)
- [申请链接](https://kexue.fm/links.html)

本站采用创作共用版权协议，要求署名、非商业用途和保持一致。转载本站内容必须也遵循“ [署名-非商业用途-保持一致](http://creativecommons.org/licenses/by-nc-nd/2.5/cn/)”的创作共用协议。
© 2009-2025 Scientific Spaces. All rights reserved. Theme by [laogui](http://www.laogui.com). Powered by [Typecho](http://typecho.org). 备案号: [粤ICP备09093259号-1/2](https://beian.miit.gov.cn/)。