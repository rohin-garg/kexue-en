## SEARCH

## MENU

- [打赏](https://kexue.fm/reward.html)
- [公式](https://kexue.fm/latex.html)
- [天象](https://kexue.fm/ac.html)
- [链接](https://kexue.fm/links.html)
- [时光](https://kexue.fm/me.html)
- [博览](https://kexue.fm/science.html)
- [归档](https://kexue.fm/content.html)

## CATEGORIES

- [千奇百怪](https://kexue.fm/category/Everything)
- [天文探索](https://kexue.fm/category/Astronomy)
- [数学研究](https://kexue.fm/category/Mathematics)
- [物理化学](https://kexue.fm/category/Phy-chem)
- [信息时代](https://kexue.fm/category/Big-Data)
- [生物自然](https://kexue.fm/category/Biology)
- [图片摄影](https://kexue.fm/category/Photograph)
- [问题百科](https://kexue.fm/category/Questions)
- [生活/情感](https://kexue.fm/category/Life-Feeling)
- [资源共享](https://kexue.fm/category/Resources)

## NEWPOSTS

- [“对角+低秩”三角阵的高效求逆方法](https://kexue.fm/archives/11072)
- [通过msign来计算奇异值裁剪mc...](https://kexue.fm/archives/11059)
- [矩阵符号函数mcsgn能计算什么？](https://kexue.fm/archives/11056)
- [线性注意力简史：从模仿、创新到反哺](https://kexue.fm/archives/11033)
- [msign的导数](https://kexue.fm/archives/11025)
- [通过msign来计算奇异值裁剪mc...](https://kexue.fm/archives/11006)
- [msign算子的Newton-Sc...](https://kexue.fm/archives/10996)
- [等值振荡定理：最优多项式逼近的充要条件](https://kexue.fm/archives/10972)
- [生成扩散模型漫谈（三十）：从瞬时速...](https://kexue.fm/archives/10958)
- [MoE环游记：5、均匀分布的反思](https://kexue.fm/archives/10945)

## COMMENTS

- [kw: 把所有M直接换成全1矩阵就行吧，比如DeltaNet变成$(Q...](https://kexue.fm/archives/11033/comment-page-1#comment-28057)
- [WB: 非常清楚的blog。我有一个小问题想问一下，推导的时候用的是不...](https://kexue.fm/archives/10795/comment-page-1#comment-28056)
- [liangzhh: 谢谢大佬的分享，感觉中间有两个手误敲错，式(9)最后应该是加号...](https://kexue.fm/archives/11072/comment-page-1#comment-28055)
- [lidhrandom: Equation 3的等号右侧第二项的第一个${\\Lambda...](https://kexue.fm/archives/11072/comment-page-1#comment-28054)
- [Kuo: 在 $PaTH$ 论文章节 \`UT Transform for...](https://kexue.fm/archives/11033/comment-page-1#comment-28053)
- [Fanhao: 假定Hessian阵正定，那不是意味着$L(\\theta)$是...](https://kexue.fm/archives/10542/comment-page-1#comment-28052)
- [曲笑一: 对于第一个疑问，我看到分布式的版本已经开源。我在想如果将每个梯...](https://kexue.fm/archives/10739/comment-page-2#comment-28051)
- [曲笑一: 苏老师您好，阅读了您关于Muon系列的博客，受益匪浅。在此有两...](https://kexue.fm/archives/10739/comment-page-2#comment-28050)
- [tll1945tll1937: 老师，您好，向您请教一个问题：会不会因为LoRA中用到的梯度的...](https://kexue.fm/archives/10266/comment-page-1#comment-28049)
- [香蕉大王: 还是刚刚flow matching的例子$\\frac{d x\_...](https://kexue.fm/archives/9280/comment-page-2#comment-28048)

## USERLOGIN

- [登录](https://kexue.fm/admin/login.php)

[科学空间\|Scientific Spaces](https://kexue.fm)

- [登录](https://kexue.fm/admin/login.php)
- [打赏](https://kexue.fm/reward.html)
- [公式](https://kexue.fm/latex.html)
- [天象](https://kexue.fm/ac.html)
- [链接](https://kexue.fm/links.html)
- [时光](https://kexue.fm/me.html)
- [博览](https://kexue.fm/science.html)
- [归档](https://kexue.fm/content.html)

渴望成为一个小飞侠

- [欢迎订阅](https://kexue.fm/feed)
- [个性邮箱](https://kexue.fm/archives/119)
- [天象信息](https://kexue.fm/ac.html)
- [观测ISS](https://kexue.fm/archives/41)
- [LaTeX](https://kexue.fm/latex.html)
- [关于博主](https://kexue.fm/me.html)

欢迎访问“科学空间”，这里将与您共同探讨自然科学，回味人生百态；也期待大家的分享～

- [**千奇百怪** Everything](https://kexue.fm/category/Everything)
- [**天文探索** Astronomy](https://kexue.fm/category/Astronomy)
- [**数学研究** Mathematics](https://kexue.fm/category/Mathematics)
- [**物理化学** Phy-chem](https://kexue.fm/category/Phy-chem)
- [**信息时代** Big-Data](https://kexue.fm/category/Big-Data)
- [**生物自然** Biology](https://kexue.fm/category/Biology)
- [**图片摄影** Photograph](https://kexue.fm/category/Photograph)
- [**问题百科** Questions](https://kexue.fm/category/Questions)
- [**生活/情感** Life-Feeling](https://kexue.fm/category/Life-Feeling)
- [**资源共享** Resources](https://kexue.fm/category/Resources)

- [**千奇百怪**](https://kexue.fm/category/Everything)
- [**天文探索**](https://kexue.fm/category/Astronomy)
- [**数学研究**](https://kexue.fm/category/Mathematics)
- [**物理化学**](https://kexue.fm/category/Phy-chem)
- [**信息时代**](https://kexue.fm/category/Big-Data)
- [**生物自然**](https://kexue.fm/category/Biology)
- [**图片摄影**](https://kexue.fm/category/Photograph)
- [**问题百科**](https://kexue.fm/category/Questions)
- [**生活/情感**](https://kexue.fm/category/Life-Feeling)
- [**资源共享**](https://kexue.fm/category/Resources)

[首页](https://kexue.fm) [数学研究](https://kexue.fm/category/Mathematics) [信息时代](https://kexue.fm/category/Big-Data) 语言模型输出端共享Embedding的重新探索

20Jul

# [语言模型输出端共享Embedding的重新探索](https://kexue.fm/archives/9698)

By 苏剑林 \|
2023-07-20 \|
41706位读者\|

预训练刚兴起时，在语言模型的输出端重用Embedding权重是很常见的操作，比如BERT、第一版的T5、早期的GPT，都使用了这个操作，这是因为当模型主干部分不大且词表很大时，Embedding层的参数量很可观，如果输出端再新增一个独立的同样大小的权重矩阵的话，会导致显存消耗的激增。不过随着模型参数规模的增大，Embedding层的占比相对变小了，加之 [《Rethinking embedding coupling in pre-trained language models》](https://papers.cool/arxiv/2010.12821) 等研究表明共享Embedding可能会有些负面影响，所以现在共享Embedding的做法已经越来越少了。

本文旨在分析在共享Embedding权重时可能遇到的问题，并探索如何更有效地进行初始化和参数化。尽管共享Embedding看起来已经“过时”，但这依然不失为一道有趣的研究题目。

## 共享权重 [\#](https://kexue.fm/archives/9698\#%E5%85%B1%E4%BA%AB%E6%9D%83%E9%87%8D)

在语言模型的输出端重用Embedding权重的做法，英文称之为“Tied Embeddings”或者“Coupled Embeddings”，其思想主要是Embedding矩阵跟输出端转换到logits的投影矩阵大小是相同的（只差个转置），并且由于这个参数矩阵比较大，所以为了避免不必要的浪费，干脆共用同一个权重，如下图所示：

共享Embedding权重的Transformer示意图

共享Embedding最直接的后果可能是——它会导致预训练的初始损失非常大。这是因为我们通常会使用类似 [DeepNorm](https://kexue.fm/archives/8978) 的技术来降低训练难度，它们都是将模型的残差分支初始化得接近于零。换言之，模型在初始阶段近似于一个恒等函数，这使得初始模型相当于共享Embedding的2-gram模型。接下来我们将推导这样的2-gram模型损失大的原因，以及分析一些解决方案。

## 准备工作 [\#](https://kexue.fm/archives/9698\#%E5%87%86%E5%A4%87%E5%B7%A5%E4%BD%9C)

在正式开始推导之前，我们需要准备一些基础结论。

首先，要明确的是，我们主要对初始阶段的结果进行分析，此时的权重都是从某个“均值为0、方差为$\\sigma^2$”的分布中独立同分布地采样出来的，这允许我们通过期望来估计某些求和结果。比如对于$\\boldsymbol{w}=(w\_1,w\_2,\\cdots,w\_d)$，我们有
\\begin{equation}\\mathbb{E}\\left\[\\Vert \\boldsymbol{w}\\Vert^2\\right\] = \\mathbb{E}\\left\[\\sum\_i w\_i^2\\right\] = \\sum\_i \\mathbb{E}\\left\[w\_i^2\\right\] = d\\sigma^2\\label{eq:norm}\\end{equation}
因此可以取$\\Vert \\boldsymbol{w}\\Vert\\approx \\sqrt{d}\\sigma$。那么误差有多大呢？我们可以通过它的方差来感知。为此，我们先求它的二阶矩：
\\begin{equation}\\begin{aligned}\\mathbb{E}\\left\[\\Vert \\boldsymbol{w}\\Vert^4\\right\] =&\\, \\mathbb{E}\\left\[\\left(\\sum\_i w\_i^2\\right)^2\\right\] = \\mathbb{E}\\left\[\\sum\_i w\_i^4 + \\sum\_{i,j\|i\\neq j} w\_i^2 w\_j^2\\right\] \\\
=&\\, \\sum\_i \\mathbb{E}\\left\[w\_i^4\\right\] + \\sum\_{i,j\|i\\neq j} \\mathbb{E}\\left\[w\_i^2\\right\] \\mathbb{E}\\left\[w\_j^2\\right\] \\\
=&\\, d\\,\\mathbb{E}\\left\[w^4\\right\] + d(d-1) \\sigma^4 \\\
\\end{aligned}\\end{equation}
如果采样分布是正态分布，那么可以直接算出$\\mathbb{E}\\left\[w^4\\right\]=3\\sigma^4$，所以
\\begin{equation}\\mathbb{V}ar\\left\[\\Vert \\boldsymbol{w}\\Vert^2\\right\] = \\mathbb{E}\\left\[\\Vert \\boldsymbol{w}\\Vert^4\\right\] - \\mathbb{E}\\left\[\\Vert \\boldsymbol{w}\\Vert^2\\right\]^2 = 2d\\sigma^4\\end{equation}
这个方差大小也代表着$\\Vert \\boldsymbol{w}\\Vert\\approx \\sqrt{d}\\sigma$的近似程度，也就是说原本的采样方差$\\sigma^2$越小，那么近似程度越高。特别地，常见的采样方差是$1/d$（对应$\\Vert \\boldsymbol{w}\\Vert\\approx 1$，即单位向量），那么代入上式得到$2/d$，意味着维度越高近似程度越高。此外，如果采样分布不是正态分布，可以另外重新计算$\\mathbb{E}\\left\[w^4\\right\]$，或者直接将正态分布的结果作为参考结果，反正都只是一个估算罢了。

如果$\\boldsymbol{v}=(v\_1,v\_2,\\cdots,v\_d)$是另一个独立同分布向量，那么我们可以用同样的方法估计内积，结果是
\\begin{equation}\\mathbb{E}\\left\[\\boldsymbol{w}\\cdot\\boldsymbol{v}\\right\] = \\mathbb{E}\\left\[\\sum\_i w\_i v\_i\\right\] = \\sum\_i \\mathbb{E}\\left\[w\_i\\right\] \\mathbb{E}\\left\[v\_i\\right\] = 0\\label{eq:dot}\\end{equation}
以及
\\begin{equation}\\begin{aligned}\\mathbb{E}\\left\[(\\boldsymbol{w}\\cdot\\boldsymbol{v})^2\\right\] =&\\, \\mathbb{E}\\left\[\\left(\\sum\_i w\_i v\_i\\right)^2\\right\] = \\mathbb{E}\\left\[\\sum\_i w\_i^2 v\_i^2 + \\sum\_{i,j\|i\\neq j} w\_i v\_i w\_j v\_j\\right\] \\\
=&\\, \\sum\_i \\mathbb{E}\\left\[w\_i^2\\right\]\\mathbb{E}\\left\[w\_j^2\\right\] + \\sum\_{i,j\|i\\neq j} \\mathbb{E}\\left\[w\_i\\right\]\\mathbb{E}\\left\[v\_i\\right\]\\mathbb{E}\\left\[w\_j\\right\]\\mathbb{E}\\left\[v\_j\\right\] \\\
=&\\, d \\sigma^4 \\\
\\end{aligned}\\end{equation}
同样地，取$\\sigma^2=1/d$的话，那么方差是$1/d^3$，维度越高近似程度越高。以上两个结果可以说是 [《n维空间下两个随机向量的夹角分布》](https://kexue.fm/archives/7076)、 [《让人惊叹的Johnson-Lindenstrauss引理：理论篇》](https://kexue.fm/archives/8679) 中的结论的统计版本。

## 损失分析 [\#](https://kexue.fm/archives/9698\#%E6%8D%9F%E5%A4%B1%E5%88%86%E6%9E%90)

对语言模型来说，最终要输出一个逐token的$n$元分布，这里$n$是词表大小。假设我们直接输出均匀分布，也就是每个token的概率都是$1/n$，那么不难计算交叉熵损失将会是$\\log n$。这也就意味着，合理的初始化不应该使得初始损失明显超过$\\log n$，因为$\\log n$代表了最朴素的均匀分布，明显超过$\\log n$等价于说远远不如均匀分布，就好比是故意犯错，并不合理。

那么，为什么共享Embedding会出现这种情况呢？假设初始Embedding是$\\{\\boldsymbol{w}\_1,\\boldsymbol{w}\_2,\\cdots,\\boldsymbol{w}\_n\\}$，前面已经说了，初始阶段残差分支接近于零，所以输入输入token $i$，模型输出就是经过Normalization之后的Embedding $\\boldsymbol{w}\_i$。常见的Normalization就是Layer Norm或者RMS Norm，由于初始化分布是零均值的，所以Layer Norm跟RMS Norm大致等价，因此输出是
\\begin{equation}\\frac{\\boldsymbol{w}\_i}{\\Vert\\boldsymbol{w}\_i\\Vert \\big/\\sqrt{d}} = \\frac{\\boldsymbol{w}\_i}{\\sigma}\\end{equation}
接下来重用Embedding，内积然后Softmax，所建立的分布实质是
\\begin{equation}p(j\|i) = \\frac{e^{\\boldsymbol{w}\_i\\cdot \\boldsymbol{w}\_j / \\sigma}}{\\sum\\limits\_k e^{\\boldsymbol{w}\_i\\cdot \\boldsymbol{w}\_k / \\sigma}}\\end{equation}
对应的损失函数就是
\\begin{equation}-\\log p(j\|i) = \\log \\sum\\limits\_k e^{\\boldsymbol{w}\_i\\cdot \\boldsymbol{w}\_k / \\sigma} - \\boldsymbol{w}\_i\\cdot \\boldsymbol{w}\_j \\big/ \\sigma\\end{equation}
语言模型任务是为了预测下一个token，而我们知道自然句子中叠词的比例很小，所以基本上可以认为$j\\neq i$，那么根据结果$\\eqref{eq:dot}$就有$\\boldsymbol{w}\_i\\cdot \\boldsymbol{w}\_j\\approx 0$。所以，初始损失函数是
\\begin{equation}-\\log p(j\|i) \\approx \\log \\sum\_k e^{\\boldsymbol{w}\_i\\cdot \\boldsymbol{w}\_k / \\sigma}=\\log \\left(e^{\\boldsymbol{w}\_i\\cdot \\boldsymbol{w}\_i / \\sigma} + \\sum\\limits\_{k\|k\\neq i} e^{\\boldsymbol{w}\_i\\cdot \\boldsymbol{w}\_k / \\sigma}\\right)\\approx\\log \\left(e^{d \\sigma} + (n-1)\\right)\\label{eq:loss}\\end{equation}
后面的$\\approx$再次用到了式$\\eqref{eq:norm}$和式$\\eqref{eq:dot}$。常见的初始化方差$\\sigma^2$，或者是一个常数，或者是$1/d$（此时$e^{d \\sigma}=e^{\\sqrt{d}}$），不管是哪一种，当$d$较大时，都导致$e^{d \\sigma}$占主导，于是损失将会是$\\log e^{d\\sigma}=d\\sigma$级别，这很容易就超过了均匀分布的$\\log n$。

## 一些对策 [\#](https://kexue.fm/archives/9698\#%E4%B8%80%E4%BA%9B%E5%AF%B9%E7%AD%96)

根据上述推导结果，我们就可以针对性地设计一些对策了。比较直接的方案是调整初始化，根据式$\\eqref{eq:loss}$，我们只需要让$e^{d\\sigma}=n$，那么初始损失就是变成$\\log n$级别的，也就是说初始化的标准差要改为$\\sigma=(\\log n)/d$。

一般来说，我们会希望参数的初始化方差尽量大一些，这样梯度相对来说没那么容易下溢，而$\\sigma=(\\log n)/d$有时候会显得过小了。为此，我们可以换一种思路：很明显，式$\\eqref{eq:loss}$之所以会偏大，是因为出现了$e^{\\boldsymbol{w}\_i\\cdot \\boldsymbol{w}\_i / \\sigma}$，由于两个$\\boldsymbol{w}\_i$相同，它们内积变成了模长，从而变得很大，如果能让它们不同，那么就不会出现这一个占主导的项了。

为此，最简单的方法自然是干脆不共享Embedding，此时是$e^{\\boldsymbol{w}\_i\\cdot \\boldsymbol{v}\_i / \\sigma}$而不是$e^{\\boldsymbol{w}\_i\\cdot \\boldsymbol{w}\_i / \\sigma}$，用$\\eqref{eq:dot}$而不是$\\eqref{eq:norm}$作为近似，于是式$\\eqref{eq:loss}$渐近于$\\log n$。如果还想保留共享Embedding，我们可以在最后的Normalization之后，再接一个正交初始化的投影层，这样$e^{\\boldsymbol{w}\_i\\cdot \\boldsymbol{w}\_i / \\sigma}$变成了$e^{(\\boldsymbol{w}\_i\\boldsymbol{P})\\cdot \\boldsymbol{w}\_i / \\sigma}$，根据 [Johnson-Lindenstrauss引理](https://kexue.fm/archives/8679)，经过随机投影的向量近似于独立向量了，所以也近似于不共享的情况，这其实就是BERT的解决办法。特别地，这个投影层还可以一般化地加上bias和激活函数。

如果一丁点额外参数都不想引入，那么可以考虑在Normalization之后“打乱”$\\boldsymbol{w}\_i$的各个维度，比如
\\begin{equation}\\mathcal{S}\[\\boldsymbol{w}\] = \\boldsymbol{w}\[d/2:\]\\circ\\boldsymbol{w}\[:d/2\]\\end{equation}
这里的$\\circ$是拼接操作，那么$\\mathcal{S}\[\\boldsymbol{w}\_i\]$和$\\boldsymbol{w}\_i$也接近正交了，内积自然也约等于0。这相当于（在初始阶段）将原来的$n\\times d$的Embedding矩阵劈开为两个$n\\times (d/2)$的矩阵然后构建不共享Embedding的2-gram模型。另外，我们还可以考虑其他打乱操作，比如 [ShuffleNet](https://papers.cool/arxiv/1707.01083) 中的先reshape，然后transpose再reshape回来。

在笔者的实验中，直接改初始化标准差为$\\sigma=(\\log n)/d$收敛速度是最慢的，其余方法收敛速度差不多，至于最终效果，所有方法似乎都差不多。

## 文章小结 [\#](https://kexue.fm/archives/9698\#%E6%96%87%E7%AB%A0%E5%B0%8F%E7%BB%93)

本文重温了语言模型输出端共享Embedding权重的操作，推导了直接重用Embedding来投影输出可能会导致损失过大的可能性，并探讨了一些解决办法。

_**转载到请包括本文地址：** [https://kexue.fm/archives/9698](https://kexue.fm/archives/9698)_

_**更详细的转载事宜请参考：**_ [《科学空间FAQ》](https://kexue.fm/archives/6508#%E6%96%87%E7%AB%A0%E5%A6%82%E4%BD%95%E8%BD%AC%E8%BD%BD/%E5%BC%95%E7%94%A8)

**如果您还有什么疑惑或建议，欢迎在下方评论区继续讨论。**

**如果您觉得本文还不错，欢迎 [分享](https://kexue.fm/archives/9698#share)/ [打赏](https://kexue.fm/archives/9698#pay) 本文。打赏并非要从中获得收益，而是希望知道科学空间获得了多少读者的真心关注。当然，如果你无视它，也不会影响你的阅读。再次表示欢迎和感谢！**

打赏

微信打赏

支付宝打赏

因为网站后台对打赏并无记录，因此欢迎在打赏时候备注留言。你还可以 [**点击这里**](http://mail.qq.com/cgi-bin/qm_share?t=qm_mailme&email=tN7d1drY3drrx8H0xcWa19vZ) 或在下方评论区留言来告知你的建议或需求。

**如果您需要引用本文，请参考：**

苏剑林. (Jul. 20, 2023). 《语言模型输出端共享Embedding的重新探索 》\[Blog post\]. Retrieved from [https://kexue.fm/archives/9698](https://kexue.fm/archives/9698)

@online{kexuefm-9698,
        title={语言模型输出端共享Embedding的重新探索},
        author={苏剑林},
        year={2023},
        month={Jul},
        url={\\url{https://kexue.fm/archives/9698}},
}

分类： [数学研究](https://kexue.fm/category/Mathematics), [信息时代](https://kexue.fm/category/Big-Data)    标签： [语言模型](https://kexue.fm/tag/%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/), [初始化](https://kexue.fm/tag/%E5%88%9D%E5%A7%8B%E5%8C%96/)[12 评论](https://kexue.fm/archives/9698#comments)

< [当生成模型肆虐：互联网将有“疯牛病”之忧？](https://kexue.fm/archives/9687) \| [Transformer升级之路：11、将β进制位置进行到底](https://kexue.fm/archives/9706) >

### 你也许还对下面的内容感兴趣

- [Transformer升级之路：20、MLA好在哪里?（上）](https://kexue.fm/archives/10907)
- [Transformer升级之路：19、第二类旋转位置编码](https://kexue.fm/archives/10862)
- [Decoder-only的LLM为什么需要位置编码？](https://kexue.fm/archives/10347)
- [Monarch矩阵：计算高效的稀疏型矩阵分解](https://kexue.fm/archives/10249)
- [缓存与效果的极限拉扯：从MHA、MQA、GQA到MLA](https://kexue.fm/archives/10091)
- [时空之章：将Attention视为平方复杂度的RNN](https://kexue.fm/archives/10017)
- [我在Performer中发现了Transformer-VQ的踪迹](https://kexue.fm/archives/9862)
- [预训练一下，Transformer的长序列成绩还能涨不少！](https://kexue.fm/archives/9787)
- [脑洞大开：非线性RNN居然也可以并行计算？](https://kexue.fm/archives/9783)
- [大词表语言模型在续写任务上的一个问题及对策](https://kexue.fm/archives/9762)

[发表你的看法](https://kexue.fm/archives/9698#comment_form)

[语言模型输出端共享embedding的重新探索 R11; AI 資訊](https://news.aitime.space/2023/07/11732/)

July 21st, 2023

\[...\]​Read More \[...\]

[回复评论](https://kexue.fm/archives/9698/comment-page-1?replyTo=22309#respond-post-9698)

吴傲 发表于
July 21st, 2023

请问苏神这里说的”至于最终效果，所有方法似乎都差不多。“
和原始共享embedding有提升么，有多大提升呢？

[回复评论](https://kexue.fm/archives/9698/comment-page-1?replyTo=22319#respond-post-9698)

yvan 发表于
July 21st, 2023

llama就没有共享，不共享效果应该会更好吧，毕竟增加了参数

[回复评论](https://kexue.fm/archives/9698/comment-page-1?replyTo=22321#respond-post-9698)

[苏剑林](https://kexue.fm) 发表于
July 22nd, 2023

我个人的实验没提升。

[回复评论](https://kexue.fm/archives/9698/comment-page-1?replyTo=22329#respond-post-9698)

游狸态

July 21st, 2023

输出端重用Embedding权重的话，形式上是原Embedding的转置; 但是如果将embedding理解为一种翻译器，我更想把它处理为原Embedding的伪逆，相当于一种回译。

[回复评论](https://kexue.fm/archives/9698/comment-page-1?replyTo=22318#respond-post-9698)

[苏剑林](https://kexue.fm) 发表于
July 22nd, 2023

初始阶段Embedding是零均值独立同分布初始化的，这样得到的矩阵接近正交矩阵，所以它的转置实质上就接近它的伪逆（的若干倍）。所以恰恰相反，共享Embedding出现loss很大的原因，正好是因为输出初始化成了伪逆，原因文章也分析了，我们的任务是语言模型，叠词很少，所以伪逆（或者说回译）初始化反而导致了初始化全错，比随机蒙还不如。

[回复评论](https://kexue.fm/archives/9698/comment-page-1?replyTo=22328#respond-post-9698)

Lee Chuai 发表于
November 4th, 2023

叠词，精辟！

[回复评论](https://kexue.fm/archives/9698/comment-page-1?replyTo=23001#respond-post-9698)

w1zard

July 31st, 2023

按照文中的分析，bert在transformer结构后面接了一个hidden\_size x hidden\_size的linear层，理论上可以规避这一问题？

[回复评论](https://kexue.fm/archives/9698/comment-page-1?replyTo=22392#respond-post-9698)

[苏剑林](https://kexue.fm) 发表于
August 4th, 2023

是的，本文已经说了，bert的做法已经规避了这个问题。

[回复评论](https://kexue.fm/archives/9698/comment-page-1?replyTo=22414#respond-post-9698)

gelthin

January 24th, 2024

苏神，这篇文章的分类类别是不是少了信息时代，在信息时代往下看找不到这篇。

[回复评论](https://kexue.fm/archives/9698/comment-page-1?replyTo=23588#respond-post-9698)

[苏剑林](https://kexue.fm) 发表于
January 24th, 2024

归到了数学研究类别了～我新增一下吧

[回复评论](https://kexue.fm/archives/9698/comment-page-1?replyTo=23593#respond-post-9698)

[探秘Transformer系列之（7）R12; embedding \| 呱唧呱唧网](https://www.itfaba.com/jishufenxian/204527.html)

February 28th, 2025

\[...\]ALBERT: A Lite BERT for Self-supervised Learning of Language Representations\[C\]// International Conference on Learning Representations Distributed Representations of Words and Phrases and their Compos\[...\]

[回复评论](https://kexue.fm/archives/9698/comment-page-1?replyTo=26800#respond-post-9698)

[取消回复](https://kexue.fm/archives/9698#respond-post-9698)

你的大名

电子邮箱

个人网站（选填）

1\. 可以使用LaTeX代码，点击“预览效果”可查看效果；2. 可以通过点击评论楼层编号来引用该楼层；3. 网站可能会有点卡，如非确认评论失败，请 **不要重复点击提交**。

### 内容速览

[共享权重](https://kexue.fm/archives/9698#%E5%85%B1%E4%BA%AB%E6%9D%83%E9%87%8D)
[准备工作](https://kexue.fm/archives/9698#%E5%87%86%E5%A4%87%E5%B7%A5%E4%BD%9C)
[损失分析](https://kexue.fm/archives/9698#%E6%8D%9F%E5%A4%B1%E5%88%86%E6%9E%90)
[一些对策](https://kexue.fm/archives/9698#%E4%B8%80%E4%BA%9B%E5%AF%B9%E7%AD%96)
[文章小结](https://kexue.fm/archives/9698#%E6%96%87%E7%AB%A0%E5%B0%8F%E7%BB%93)

### 智能搜索

支持整句搜索！网站自动使用 [结巴分词](https://github.com/fxsjy/jieba) 进行分词，并结合ngrams排序算法给出合理的搜索结果。

### 热门标签

[生成模型](https://kexue.fm/tag/%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B/) [attention](https://kexue.fm/tag/attention/) [优化](https://kexue.fm/tag/%E4%BC%98%E5%8C%96/) [语言模型](https://kexue.fm/tag/%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/) [模型](https://kexue.fm/tag/%E6%A8%A1%E5%9E%8B/) [网站](https://kexue.fm/tag/%E7%BD%91%E7%AB%99/) [概率](https://kexue.fm/tag/%E6%A6%82%E7%8E%87/) [梯度](https://kexue.fm/tag/%E6%A2%AF%E5%BA%A6/) [转载](https://kexue.fm/tag/%E8%BD%AC%E8%BD%BD/) [矩阵](https://kexue.fm/tag/%E7%9F%A9%E9%98%B5/) [微分方程](https://kexue.fm/tag/%E5%BE%AE%E5%88%86%E6%96%B9%E7%A8%8B/) [天象](https://kexue.fm/tag/%E5%A4%A9%E8%B1%A1/) [分析](https://kexue.fm/tag/%E5%88%86%E6%9E%90/) [深度学习](https://kexue.fm/tag/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/) [积分](https://kexue.fm/tag/%E7%A7%AF%E5%88%86/) [python](https://kexue.fm/tag/python/) [优化器](https://kexue.fm/tag/%E4%BC%98%E5%8C%96%E5%99%A8/) [力学](https://kexue.fm/tag/%E5%8A%9B%E5%AD%A6/) [无监督](https://kexue.fm/tag/%E6%97%A0%E7%9B%91%E7%9D%A3/) [扩散](https://kexue.fm/tag/%E6%89%A9%E6%95%A3/) [几何](https://kexue.fm/tag/%E5%87%A0%E4%BD%95/) [节日](https://kexue.fm/tag/%E8%8A%82%E6%97%A5/) [生活](https://kexue.fm/tag/%E7%94%9F%E6%B4%BB/) [文本生成](https://kexue.fm/tag/%E6%96%87%E6%9C%AC%E7%94%9F%E6%88%90/) [数论](https://kexue.fm/tag/%E6%95%B0%E8%AE%BA/)

### 随机文章

- [生活拾零...](https://kexue.fm/archives/139)
- [《新理解矩阵3》：行列式的点滴](https://kexue.fm/archives/1770)
- [柯西命题：盯着它到显然成立为止！](https://kexue.fm/archives/3272)
- [近乎完美地解决MathJax与Marked的冲突](https://kexue.fm/archives/10332)
- [不在家的国庆](https://kexue.fm/archives/2106)
- [2016年全年天象](https://kexue.fm/archives/4171)
- [遐思1：n次代数方程的解可以这样表示吗？](https://kexue.fm/archives/1367)
- [《向量》系列——5.平面向量微分方程与复数](https://kexue.fm/archives/963)
- [旁门左道之如何让Python的重试代码更加优雅](https://kexue.fm/archives/9938)
- [【NASA每日一图】水星上的双环陨石坑](https://kexue.fm/archives/168)

### 最近评论

- [kw](https://kexue.fm/archives/11033/comment-page-1#comment-28057): 把所有M直接换成全1矩阵就行吧，比如DeltaNet变成$(QK^⊤)(I+KK^⊤⊙(1-I...
- [WB](https://kexue.fm/archives/10795/comment-page-1#comment-28056): 非常清楚的blog。我有一个小问题想问一下，推导的时候用的是不等式（10），这里左边O(1)，...
- [liangzhh](https://kexue.fm/archives/11072/comment-page-1#comment-28055): 谢谢大佬的分享，感觉中间有两个手误敲错，式(9)最后应该是加号，另外是chunk而不是chuck吧？
- [lidhrandom](https://kexue.fm/archives/11072/comment-page-1#comment-28054): Equation 3的等号右侧第二项的第一个${\\Lambda^{-1}}$疑似不应取逆
- [Kuo](https://kexue.fm/archives/11033/comment-page-1#comment-28053): 在 $PaTH$ 论文章节 \`UT Transform for Products of Hou...
- [Fanhao](https://kexue.fm/archives/10542/comment-page-1#comment-28052): 假定Hessian阵正定，那不是意味着$L(\\theta)$是$\\theta$的凸函数吗？这一...
- [曲笑一](https://kexue.fm/archives/10739/comment-page-2#comment-28051): 对于第一个疑问，我看到分布式的版本已经开源。我在想如果将每个梯度矩阵G拆分为N\*N,再利用mu...
- [曲笑一](https://kexue.fm/archives/10739/comment-page-2#comment-28050): 苏老师您好，阅读了您关于Muon系列的博客，受益匪浅。在此有两个疑问想请教您：第一个问题是，M...
- [tll1945tll1937](https://kexue.fm/archives/10266/comment-page-1#comment-28049): 老师，您好，向您请教一个问题：会不会因为LoRA中用到的梯度的维度仅仅是全参数微调中梯度的维度...
- [香蕉大王](https://kexue.fm/archives/9280/comment-page-2#comment-28048): 还是刚刚flow matching的例子$\\frac{d x\_t}{dt} = v\_\\thet...

### 友情链接

- [Cool Papers](https://papers.cool)
- [数学研发](https://bbs.emath.ac.cn)
- [Seatop](http://www.seatop.com.cn/)
- [Xiaoxia](https://xiaoxia.org/)
- [积分表-网络版](https://kexue.fm/sci/integral/index.html)
- [丝路博傲](http://blog.dvxj.com/)
- [ph4ntasy 饭特稀](http://www.ph4ntasy.com/)
- [数学之家](http://www.2math.cn/)
- [有趣天文奇观](http://interesting-sky.china-vo.org/)
- [TwistedW](http://www.twistedwg.com/)
- [godweiyang](https://godweiyang.com/)
- [AI柠檬](https://blog.ailemon.net/)
- [王登科-DK博客](https://greatdk.com)
- [ESON](https://blog.eson.org/)
- [枫之羽](https://fzhiy.net/)
- [Mathor's blog](https://wmathor.com/)
- [coding-zuo](https://coding-zuo.github.io/)
- [博科园](https://www.bokeyuan.net/)
- [孔皮皮的博客](https://www.kppkkp.top/)
- [运鹏的博客](https://yunpengtai.top/)
- [jiming.site](https://jiming.site/)
- [OmegaXYZ](https://www.omegaxyz.com/)
- [Blog by Eacls](https://www.eacls.top/)
- [EAI猩球](https://www.robotech.ink/)
- [文举的博客](https://liwenju0.com/)
- [用代码打点酱油](https://bruceyuan.com/)
- [申请链接](https://kexue.fm/links.html)

本站采用创作共用版权协议，要求署名、非商业用途和保持一致。转载本站内容必须也遵循“ [署名-非商业用途-保持一致](http://creativecommons.org/licenses/by-nc-nd/2.5/cn/)”的创作共用协议。
© 2009-2025 Scientific Spaces. All rights reserved. Theme by [laogui](http://www.laogui.com). Powered by [Typecho](http://typecho.org). 备案号: [粤ICP备09093259号-1/2](https://beian.miit.gov.cn/)。