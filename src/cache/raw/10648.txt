## SEARCH

## MENU

- [打赏](https://kexue.fm/reward.html)
- [公式](https://kexue.fm/latex.html)
- [天象](https://kexue.fm/ac.html)
- [链接](https://kexue.fm/links.html)
- [时光](https://kexue.fm/me.html)
- [博览](https://kexue.fm/science.html)
- [归档](https://kexue.fm/content.html)

## CATEGORIES

- [千奇百怪](https://kexue.fm/category/Everything)
- [天文探索](https://kexue.fm/category/Astronomy)
- [数学研究](https://kexue.fm/category/Mathematics)
- [物理化学](https://kexue.fm/category/Phy-chem)
- [信息时代](https://kexue.fm/category/Big-Data)
- [生物自然](https://kexue.fm/category/Biology)
- [图片摄影](https://kexue.fm/category/Photograph)
- [问题百科](https://kexue.fm/category/Questions)
- [生活/情感](https://kexue.fm/category/Life-Feeling)
- [资源共享](https://kexue.fm/category/Resources)

## NEWPOSTS

- [msign的导数](https://kexue.fm/archives/11025)
- [通过msign来计算mclip（奇...](https://kexue.fm/archives/11006)
- [msign算子的Newton-Sc...](https://kexue.fm/archives/10996)
- [等值振荡定理：最优多项式逼近的充要条件](https://kexue.fm/archives/10972)
- [生成扩散模型漫谈（三十）：从瞬时速...](https://kexue.fm/archives/10958)
- [MoE环游记：5、均匀分布的反思](https://kexue.fm/archives/10945)
- [msign算子的Newton-Sc...](https://kexue.fm/archives/10922)
- [Transformer升级之路：2...](https://kexue.fm/archives/10907)
- [一道概率不等式：盯着它到显然成立为止！](https://kexue.fm/archives/10902)
- [SVD的导数](https://kexue.fm/archives/10878)

## COMMENTS

- [1: 1\[comment=14986\]Daniel Cheng\[/c...](https://kexue.fm/reward.html/comment-page-1#comment-27905)
- [1: 1\[comment=14986\]Daniel Cheng\[/c...](https://kexue.fm/reward.html/comment-page-1#comment-27904)
- [1: 1\[comment=25585\]Evan-wyl\[/comme...](https://kexue.fm/links.html/comment-page-6#comment-27903)
- [1: 1\[comment=25585\]Evan-wyl\[/comme...](https://kexue.fm/links.html/comment-page-6#comment-27902)
- [tll1945tll1937: 真心实意的向大家请教问题：看了文章“对齐全量微调！这是我看过最...](https://kexue.fm/archives/10266/comment-page-1#comment-27901)
- [oYo\_logan: \[comment=27017\]苏剑林\[/comment\]苏神，...](https://kexue.fm/archives/10757/comment-page-1#comment-27897)
- [z123: 在参数矩阵较多的CNN小模型上，Muon会明显慢于Adam，这...](https://kexue.fm/archives/10592/comment-page-1#comment-27896)
- [dry: 苏神好，一直有个疑问，ReFlow构建的ODE是$dx\_t/d...](https://kexue.fm/archives/10958/comment-page-2#comment-27895)
- [tyj: 感觉和之前的一篇文章很像，应该算是concurrent wor...](https://kexue.fm/archives/10958/comment-page-2#comment-27894)
- [li6626: 苏老师，Normalizing Flow有了新进展，论文链接:...](https://kexue.fm/archives/10667/comment-page-1#comment-27893)

## USERLOGIN

- [登录](https://kexue.fm/admin/login.php)

[科学空间\|Scientific Spaces](https://kexue.fm)

- [登录](https://kexue.fm/admin/login.php)
- [打赏](https://kexue.fm/reward.html)
- [公式](https://kexue.fm/latex.html)
- [天象](https://kexue.fm/ac.html)
- [链接](https://kexue.fm/links.html)
- [时光](https://kexue.fm/me.html)
- [博览](https://kexue.fm/science.html)
- [归档](https://kexue.fm/content.html)

渴望成为一个小飞侠

- [欢迎订阅](https://kexue.fm/feed)
- [个性邮箱](https://kexue.fm/archives/119)
- [天象信息](https://kexue.fm/ac.html)
- [观测ISS](https://kexue.fm/archives/41)
- [LaTeX](https://kexue.fm/latex.html)
- [关于博主](https://kexue.fm/me.html)

欢迎访问“科学空间”，这里将与您共同探讨自然科学，回味人生百态；也期待大家的分享～

- [**千奇百怪** Everything](https://kexue.fm/category/Everything)
- [**天文探索** Astronomy](https://kexue.fm/category/Astronomy)
- [**数学研究** Mathematics](https://kexue.fm/category/Mathematics)
- [**物理化学** Phy-chem](https://kexue.fm/category/Phy-chem)
- [**信息时代** Big-Data](https://kexue.fm/category/Big-Data)
- [**生物自然** Biology](https://kexue.fm/category/Biology)
- [**图片摄影** Photograph](https://kexue.fm/category/Photograph)
- [**问题百科** Questions](https://kexue.fm/category/Questions)
- [**生活/情感** Life-Feeling](https://kexue.fm/category/Life-Feeling)
- [**资源共享** Resources](https://kexue.fm/category/Resources)

- [**千奇百怪**](https://kexue.fm/category/Everything)
- [**天文探索**](https://kexue.fm/category/Astronomy)
- [**数学研究**](https://kexue.fm/category/Mathematics)
- [**物理化学**](https://kexue.fm/category/Phy-chem)
- [**信息时代**](https://kexue.fm/category/Big-Data)
- [**生物自然**](https://kexue.fm/category/Biology)
- [**图片摄影**](https://kexue.fm/category/Photograph)
- [**问题百科**](https://kexue.fm/category/Questions)
- [**生活/情感**](https://kexue.fm/category/Life-Feeling)
- [**资源共享**](https://kexue.fm/category/Resources)

[首页](https://kexue.fm) [数学研究](https://kexue.fm/category/Mathematics) [信息时代](https://kexue.fm/category/Big-Data) 从谱范数梯度到新式权重衰减的思考

25Dec

# [从谱范数梯度到新式权重衰减的思考](https://kexue.fm/archives/10648)

By 苏剑林 \|
2024-12-25 \|
25004位读者\|

在文章 [《Muon优化器赏析：从向量到矩阵的本质跨越》](https://kexue.fm/archives/10592) 中，我们介绍了一个名为“Muon”的新优化器，其中一个理解视角是作为谱范数正则下的最速梯度下降，这似乎揭示了矩阵参数的更本质的优化方向。众所周知，对于矩阵参数我们经常也会加权重衰减（Weight Decay），它可以理解为$F$范数平方的梯度，那么从Muon的视角看，通过谱范数平方的梯度来构建新的权重衰减，会不会能起到更好的效果呢？

那么问题来了，谱范数的梯度或者说导数长啥样呢？用它来设计的新权重衰减又是什么样的？接下来我们围绕这些问题展开。

## 基础回顾 [\#](https://kexue.fm/archives/10648\#%E5%9F%BA%E7%A1%80%E5%9B%9E%E9%A1%BE)

谱范数（Spectral Norm），又称“$2$范数”，是最常用的矩阵范数之一，相比更简单的$F$范数（Frobenius Norm），它往往能揭示一些与矩阵乘法相关的更本质的信号，这是因为它定义上就跟矩阵乘法相关：对于矩阵参数$\\boldsymbol{W}\\in\\mathbb{R}^{n\\times m}$，它的谱范数定义为
\\begin{equation}\\Vert\\boldsymbol{W}\\Vert\_2 \\triangleq \\max\_{\\Vert\\boldsymbol{x}\\Vert=1} \\Vert\\boldsymbol{W}\\boldsymbol{x}\\Vert\\end{equation}
这里$\\boldsymbol{x}\\in\\mathbb{R}^m$是列向量，右端的$\\Vert\\Vert$是向量的模长（欧氏范数）。换个角度看，谱范数就是使得下面不等式对$\\forall \\boldsymbol{x}\\in\\mathbb{R}^m$恒成立的最小常数$C$：
\\begin{equation}\\Vert\\boldsymbol{W}\\boldsymbol{x}\\Vert \\leq C\\Vert\\boldsymbol{x}\\Vert\\end{equation}
不难证明，当$C$取$F$范数$\\Vert W\\Vert\_F$时，上式也是恒成立的，所以可以写出$\\Vert \\boldsymbol{W}\\Vert\_2\\leq \\Vert \\boldsymbol{W}\\Vert\_F$（因为$\\Vert \\boldsymbol{W}\\Vert\_F$只是让上式恒成立的其中一个$C$，而$\\Vert \\boldsymbol{W}\\Vert\_2$则是最小的那个$C$）。这个结论也表明，如果我们想要控制输出的幅度，以谱范数作为正则项要比$F$范数更为精准。

早在6年前的 [《深度学习中的Lipschitz约束：泛化与生成模型》](https://kexue.fm/archives/6051) 中，我们就讨论过谱范数，当时的应用场景有两个：一是WGAN对判别器明确提出了Lipschitz约束，而实现方式之一就是基于谱范数的归一化；二是有一些工作表明，谱范数作为正则项，相比$F$范数正则有更好的性能。

## 梯度推导 [\#](https://kexue.fm/archives/10648\#%E6%A2%AF%E5%BA%A6%E6%8E%A8%E5%AF%BC)

现在让我们进入正题，尝试推导谱范数的梯度$\\nabla\_{\\boldsymbol{W}} \\Vert\\boldsymbol{W}\\Vert\_2$。我们知道，谱范数在数值上等于它的最大奇异值，对此我们在 [《低秩近似之路（二）：SVD》](https://kexue.fm/archives/10407) 的“ [矩阵范数](https://kexue.fm/archives/10407#%E7%9F%A9%E9%98%B5%E8%8C%83%E6%95%B0)”一节有过证明。这意味着，如果$\\boldsymbol{W}$可以SVD为$\\sum\\limits\_{i=1}^{\\min(n,m)}\\sigma\_i \\boldsymbol{u}\_i\\boldsymbol{v}\_i^{\\top}$，那么
\\begin{equation}\\Vert\\boldsymbol{W}\\Vert\_2 = \\sigma\_1 = \\boldsymbol{u}\_1^{\\top}\\boldsymbol{W}\\boldsymbol{v}\_1\\end{equation}
其中$\\sigma\_1 \\geq \\sigma\_2 \\geq \\cdots \\geq \\sigma\_{\\min(n,m)} \\geq 0$是$\\boldsymbol{W}$的奇异值。对两边求微分，我们得到
\\begin{equation}d\\Vert\\boldsymbol{W}\\Vert\_2 = d\\boldsymbol{u}\_1^{\\top}\\boldsymbol{W}\\boldsymbol{v}\_1 + \\boldsymbol{u}\_1^{\\top}d\\boldsymbol{W}\\boldsymbol{v}\_1 + \\boldsymbol{u}\_1^{\\top}\\boldsymbol{W}d\\boldsymbol{v}\_1\\end{equation}
留意到
\\begin{equation}d\\boldsymbol{u}\_1^{\\top}\\boldsymbol{W}\\boldsymbol{v}\_1 = d\\boldsymbol{u}\_1^{\\top}\\sum\_{i=1}^{\\min(n,m)}\\sigma\_i \\boldsymbol{u}\_i\\boldsymbol{v}\_i^{\\top}\\boldsymbol{v}\_1 = d\\boldsymbol{u}\_1^{\\top}\\sigma\_1 \\boldsymbol{u}\_1 = \\frac{1}{2}\\sigma\_1 d(\\Vert\\boldsymbol{u}\_1\\Vert^2)=0\\end{equation}
同理$\\boldsymbol{u}\_1^{\\top}\\boldsymbol{W}d\\boldsymbol{v}\_1=0$，所以
\\begin{equation}d\\Vert\\boldsymbol{W}\\Vert\_2 = \\boldsymbol{u}\_1^{\\top}d\\boldsymbol{W}\\boldsymbol{v}\_1 = \\text{Tr}((\\boldsymbol{u}\_1 \\boldsymbol{v}\_1^{\\top})^{\\top} d\\boldsymbol{W}) \\quad\\Rightarrow\\quad \\nabla\_{\\boldsymbol{W}}\\Vert\\boldsymbol{W}\\Vert\_2 = \\boldsymbol{u}\_1 \\boldsymbol{v}\_1^{\\top}\\end{equation}
注意，这个证明过程有一个关键条件是$\\sigma\_1 > \\sigma\_2$，因为如果$\\sigma\_1=\\sigma\_2$的话，$\\Vert\\boldsymbol{W}\\Vert\_2$既可以表示成$\\boldsymbol{u}\_1^{\\top}\\boldsymbol{W}\\boldsymbol{v}\_1$又可以表示成$\\boldsymbol{u}\_2^{\\top}\\boldsymbol{W}\\boldsymbol{v}\_2$，用同样方法求出的梯度分别是$\\boldsymbol{u}\_1 \\boldsymbol{v}\_1^{\\top}$和$\\boldsymbol{u}\_2 \\boldsymbol{v}\_2^{\\top}$，结果不唯一意味着梯度不存在。当然，从实践角度看，两个数完全相等的概率是很小的，因此可以忽略这一点。

（注：这里的证明过程参考了Stack Exchange上的 [回答](https://math.stackexchange.com/a/3000223)，但该回答里面没有证明$d\\boldsymbol{u}\_1^{\\top}\\boldsymbol{W}\\boldsymbol{v}\_1=0$和$\\boldsymbol{u}\_1^{\\top}\\boldsymbol{W}d\\boldsymbol{v}\_1=0$，这部分由笔者补充完整。）

## 权重衰减 [\#](https://kexue.fm/archives/10648\#%E6%9D%83%E9%87%8D%E8%A1%B0%E5%87%8F)

根据这个结果以及链式法则，我们有
\\begin{equation}\\nabla\_{\\boldsymbol{W}}\\left(\\frac{1}{2}\\Vert\\boldsymbol{W}\\Vert\_2^2\\right) = \\Vert\\boldsymbol{W}\\Vert\_2\\nabla\_{\\boldsymbol{W}}\\Vert\\boldsymbol{W}\\Vert\_2 = \\sigma\_1 \\boldsymbol{u}\_1 \\boldsymbol{v}\_1^{\\top}\\label{eq:grad-2-2}\\end{equation}
对比$F$范数下的结果：
\\begin{equation}\\nabla\_{\\boldsymbol{W}}\\left(\\frac{1}{2}\\Vert\\boldsymbol{W}\\Vert\_F^2\\right) = \\boldsymbol{W} = \\sum\_{i=1}^{\\min(n,m)}\\sigma\_i \\boldsymbol{u}\_i \\boldsymbol{v}\_i^{\\top}\\end{equation}
这样对比着看就很清晰了：$F$范数平方作为正则项所得出的权重衰减，同时惩罚全体奇异值；而谱范数平方对应的权重衰减，只惩罚最大奇异值。如果我们目的是压缩输出的大小，那么压缩最大奇异值是“刚刚好”的做法，压缩全体奇异值虽然可能达到相近的目的，但同时也可能压缩参数的表达能力。

根据“ [Eckart-Young-Mirsky定理](https://kexue.fm/archives/10407#%E8%BF%91%E4%BC%BC%E5%AE%9A%E7%90%86)”，式$\\eqref{eq:grad-2-2}$最右侧的结果还有一个含义，就是$\\boldsymbol{W}$矩阵的“最优1秩近似”。也就是说，谱范数的权重衰减将每一步减去它自身的操作，改为每一步减去它的最优1秩近似，弱化了惩罚力度，当然某种程度上也让惩罚更加“直击本质”。

## 数值计算 [\#](https://kexue.fm/archives/10648\#%E6%95%B0%E5%80%BC%E8%AE%A1%E7%AE%97)

对于实践来说，最关键的问题来了：怎么计算$\\sigma\_1 \\boldsymbol{u}\_1 \\boldsymbol{v}\_1^{\\top}$呢？SVD当然是最简单直接的方案，但计算复杂度无疑也是最高的，我们必须找到更高效的计算途径。

不失一般性，设$n\\geq m$。首先注意到
\\begin{equation}\\sigma\_1 \\boldsymbol{u}\_1 \\boldsymbol{v}\_1^{\\top} = \\sum\_{i=1}^m\\sigma\_i \\boldsymbol{u}\_i \\boldsymbol{v}\_i^{\\top} \\boldsymbol{v}\_1 \\boldsymbol{v}\_1^{\\top} = \\boldsymbol{W}\\boldsymbol{v}\_1 \\boldsymbol{v}\_1^{\\top}\\end{equation}
由此可见计算$\\sigma\_1 \\boldsymbol{u}\_1 \\boldsymbol{v}\_1^{\\top}$只需要知道$\\boldsymbol{v}\_1$，然后根据我们在 [《低秩近似之路（二）：SVD》](https://kexue.fm/archives/10407#%E5%A5%87%E5%BC%82%E5%88%86%E8%A7%A3) 中的讨论，$\\boldsymbol{v}\_1$实际上是矩阵$\\boldsymbol{W}^{\\top}\\boldsymbol{W}$的最大特征值对应的特征向量。这样一来，我们便将问题从一般矩阵$\\boldsymbol{W}$的SVD转化成了实对称矩阵$\\boldsymbol{W}^{\\top}\\boldsymbol{W}$的特征值分解，这其实已经降低复杂度了，因为特征值分解通常要比SVD明显快。

如果还觉得慢，那么我们就需要请出很多特征值分解算法背后的原理——“ [幂迭代（Power Iteration）](https://en.wikipedia.org/wiki/Power_iteration)”：

> 当$\\sigma\_1 > \\sigma\_2$时，迭代
> \\begin{equation}\\boldsymbol{x}\_{t+1} = \\frac{\\boldsymbol{W}^{\\top}\\boldsymbol{W}\\boldsymbol{x}\_t}{\\Vert\\boldsymbol{W}^{\\top}\\boldsymbol{W}\\boldsymbol{x}\_t\\Vert}\\end{equation}
> 以$(\\sigma\_2/\\sigma\_1)^{2t}$的速度收敛至$\\boldsymbol{v}\_1$。

幂迭代每步只需要算两次“矩阵-向量”乘法，复杂度是$\\mathcal{O}(nm)$，$t$步迭代的总复杂度是$\\mathcal{O}(tnm)$，非常理想，缺点是$\\sigma\_1,\\sigma\_2$接近时收敛会比较慢。但幂迭代的实际表现往往比理论想象更好用，早期很多工作甚至只迭代一次就得到不错的效果，因为$\\sigma\_1,\\sigma\_2$接近表明两者及其特征向量一定程度上可替换，而幂迭代即便没完全收敛，得到的也是两者特征向量的一个平均，这也完全够用了。

## 迭代证明 [\#](https://kexue.fm/archives/10648\#%E8%BF%AD%E4%BB%A3%E8%AF%81%E6%98%8E)

这一节我们来完成幂迭代的证明。不难看出，幂迭代可以等价地写成
\\begin{equation}\\lim\_{t\\to\\infty} \\frac{(\\boldsymbol{W}^{\\top}\\boldsymbol{W})^t \\boldsymbol{x}\_0}{\\Vert(\\boldsymbol{W}^{\\top}\\boldsymbol{W})^t \\boldsymbol{x}\_0\\Vert} = \\boldsymbol{v}\_1\\end{equation}
为了证明这个极限，我们从$\\boldsymbol{W}=\\sum\\limits\_{i=1}^m\\sigma\_i \\boldsymbol{u}\_i\\boldsymbol{v}\_i^{\\top}$出发，代入计算可得
\\begin{equation}\\boldsymbol{W}^{\\top}\\boldsymbol{W} = \\sum\_{i=1}^m\\sigma\_i^2 \\boldsymbol{v}\_i\\boldsymbol{v}\_i^{\\top},\\qquad(\\boldsymbol{W}^{\\top}\\boldsymbol{W})^t = \\sum\_{i=1}^m\\sigma\_i^{2t} \\boldsymbol{v}\_i\\boldsymbol{v}\_i^{\\top}\\end{equation}
由于$\\boldsymbol{v}\_1,\\boldsymbol{v}\_2,\\cdots,\\boldsymbol{v}\_m$是$\\mathbb{R}^m$的一组标准正交基，所以$\\boldsymbol{x}\_0$可以写成$\\sum\\limits\_{j=1}^m c\_j \\boldsymbol{v}\_j$，于是我们有
\\begin{equation}(\\boldsymbol{W}^{\\top}\\boldsymbol{W})^t \\boldsymbol{x}\_0 = \\sum\_{i=1}^m\\sigma\_i^{2t} \\boldsymbol{v}\_i\\boldsymbol{v}\_i^{\\top}\\sum\_{j=1}^m c\_j \\boldsymbol{v}\_j = \\sum\_{i=1}^m\\sum\_{j=1}^m c\_j\\sigma\_i^{2t} \\boldsymbol{v}\_i\\underbrace{\\boldsymbol{v}\_i^{\\top} \\boldsymbol{v}\_j}\_{=\\delta\_{i,j}} = \\sum\_{i=1}^m c\_i\\sigma\_i^{2t} \\boldsymbol{v}\_i\\end{equation}
以及
\\begin{equation}\\Vert(\\boldsymbol{W}^{\\top}\\boldsymbol{W})^t \\boldsymbol{x}\_0\\Vert = \\left\\Vert \\sum\_{i=1}^m c\_i\\sigma\_i^{2t} \\boldsymbol{v}\_i\\right\\Vert = \\sqrt{\\sum\_{i=1}^m c\_i^2\\sigma\_i^{4t}}\\end{equation}
由于随机初始化的缘故，$c\_1=0$的概率是非常小的，所以我们可以认为$c\_1\\neq 0$，那么
\\begin{equation}\\frac{(\\boldsymbol{W}^{\\top}\\boldsymbol{W})^t \\boldsymbol{x}\_0}{\\Vert(\\boldsymbol{W}^{\\top}\\boldsymbol{W})^t \\boldsymbol{x}\_0\\Vert} = \\frac{\\sum\\limits\_{i=1}^m c\_i\\sigma\_i^{2t} \\boldsymbol{v}\_i}{\\sqrt{\\sum\\limits\_{i=1}^m c\_i^2\\sigma\_i^{4t}}} = \\frac{\\boldsymbol{v}\_1 + \\sum\\limits\_{i=2}^m (c\_i/c\_1)(\\sigma\_i/\\sigma\_1)^{2t} \\boldsymbol{v}\_i}{\\sqrt{1 + \\sum\\limits\_{i=2}^m (c\_i/c\_1)^2(\\sigma\_i/\\sigma\_1)^{4t}}}\\end{equation}
当$\\sigma\_1 > \\sigma\_2$时，所有的$\\sigma\_i/\\sigma\_1(i\\geq 2)$都小于1，因此当$t\\to \\infty$时对应项都变成了0，最后的极限是$\\boldsymbol{v}\_1$。

## 相关工作 [\#](https://kexue.fm/archives/10648\#%E7%9B%B8%E5%85%B3%E5%B7%A5%E4%BD%9C)

最早提出谱范数正则的论文，应该是2017年的 [《Spectral Norm Regularization for Improving the Generalizability of Deep Learning》](https://papers.cool/arxiv/1705.10941)，里边对比了权重衰减、对抗训练、谱范数正则等方法，发现谱范数正则在泛化性能方面表现最好。

论文当时的做法，并不是像本文一样求出$\\nabla\_{\\boldsymbol{W}}\\Vert\\boldsymbol{W}\\Vert\_2^2 = 2\\sigma\_1\\boldsymbol{u}\_1 \\boldsymbol{v}\_1^{\\top}$，而是直接通过幂迭代来估计$\\Vert\\boldsymbol{W}\\Vert\_2$，然后将$\\Vert\\boldsymbol{W}\\Vert\_2^2$加权到损失函数中，让优化器自己去求梯度，这样做效率上稍差一些，并且也不好以权重衰减的形式跟优化器解耦开来。本文的做法相对来说更加灵活一些，允许我们像AdamW一样，将权重衰减独立于主损失函数的优化之外。

当然，从今天LLM的视角来看，当初的这些实验最大问题就是规模都太小了，很难有足够的说服力，不过鉴于谱范数的Muon优化器“珠玉在前”，笔者认为还是值得重新思考和尝试一下谱范数权重衰减。当然，不管是$F$范数还是谱范数的权重衰减，这些面向“泛化”的技术往往也有一些运气成份在里边，大家平常心期待就好。

个人在语言模型的初步实验结果显示，Loss层面可能会有微弱的提升（希望不是幻觉，当然再不济也没有出现变差的现象）。实验过程就是用幂迭代求出$\\boldsymbol{v}\_1$的近似值（初始化为全一向量，迭代10次），然后将原来的权重衰减$-\\lambda \\boldsymbol{W}$改为$-\\lambda \\boldsymbol{W}\\boldsymbol{v}\_1\\boldsymbol{v}\_1^{\\top}$，$\\lambda$的取值不做改变。

## 文章小结 [\#](https://kexue.fm/archives/10648\#%E6%96%87%E7%AB%A0%E5%B0%8F%E7%BB%93)

本文推导了谱范数的梯度，由此导出了一种新的权重衰减，并分享了笔者对它的思考。

_**转载到请包括本文地址：** [https://kexue.fm/archives/10648](https://kexue.fm/archives/10648)_

_**更详细的转载事宜请参考：**_ [《科学空间FAQ》](https://kexue.fm/archives/6508#%E6%96%87%E7%AB%A0%E5%A6%82%E4%BD%95%E8%BD%AC%E8%BD%BD/%E5%BC%95%E7%94%A8)

**如果您还有什么疑惑或建议，欢迎在下方评论区继续讨论。**

**如果您觉得本文还不错，欢迎 [分享](https://kexue.fm/archives/10648#share)/ [打赏](https://kexue.fm/archives/10648#pay) 本文。打赏并非要从中获得收益，而是希望知道科学空间获得了多少读者的真心关注。当然，如果你无视它，也不会影响你的阅读。再次表示欢迎和感谢！**

打赏

微信打赏

支付宝打赏

因为网站后台对打赏并无记录，因此欢迎在打赏时候备注留言。你还可以 [**点击这里**](http://mail.qq.com/cgi-bin/qm_share?t=qm_mailme&email=tN7d1drY3drrx8H0xcWa19vZ) 或在下方评论区留言来告知你的建议或需求。

**如果您需要引用本文，请参考：**

苏剑林. (Dec. 25, 2024). 《从谱范数梯度到新式权重衰减的思考 》\[Blog post\]. Retrieved from [https://kexue.fm/archives/10648](https://kexue.fm/archives/10648)

@online{kexuefm-10648,
        title={从谱范数梯度到新式权重衰减的思考},
        author={苏剑林},
        year={2024},
        month={Dec},
        url={\\url{https://kexue.fm/archives/10648}},
}

分类： [数学研究](https://kexue.fm/category/Mathematics), [信息时代](https://kexue.fm/category/Big-Data)    标签： [矩阵](https://kexue.fm/tag/%E7%9F%A9%E9%98%B5/), [优化](https://kexue.fm/tag/%E4%BC%98%E5%8C%96/), [梯度](https://kexue.fm/tag/%E6%A2%AF%E5%BA%A6/), [优化器](https://kexue.fm/tag/%E4%BC%98%E5%8C%96%E5%99%A8/), [谱范数](https://kexue.fm/tag/%E8%B0%B1%E8%8C%83%E6%95%B0/)[1 评论](https://kexue.fm/archives/10648#comments)

< [生成扩散模型漫谈（二十八）：分步理解一致性模型](https://kexue.fm/archives/10633) \| [为什么梯度裁剪的默认模长是1？](https://kexue.fm/archives/10657) >

### 你也许还对下面的内容感兴趣

- [msign的导数](https://kexue.fm/archives/11025)
- [通过msign来计算mclip（奇异值裁剪）](https://kexue.fm/archives/11006)
- [msign算子的Newton-Schulz迭代（下）](https://kexue.fm/archives/10996)
- [MoE环游记：5、均匀分布的反思](https://kexue.fm/archives/10945)
- [msign算子的Newton-Schulz迭代（上）](https://kexue.fm/archives/10922)
- [Transformer升级之路：20、MLA究竟好在哪里？](https://kexue.fm/archives/10907)
- [SVD的导数](https://kexue.fm/archives/10878)
- [矩阵的有效秩（Effective Rank）](https://kexue.fm/archives/10847)
- [通过梯度近似寻找Normalization的替代品](https://kexue.fm/archives/10831)
- [MoE环游记：4、难处应当多投入](https://kexue.fm/archives/10815)

[发表你的看法](https://kexue.fm/archives/10648#comment_form)

Kuo

February 26th, 2025

从SVD到lipschitz到muon，融会贯通靠谱，漂亮！

[回复评论](https://kexue.fm/archives/10648/comment-page-1?replyTo=26773#respond-post-10648)

[取消回复](https://kexue.fm/archives/10648#respond-post-10648)

你的大名

电子邮箱

个人网站（选填）

1\. 可以使用LaTeX代码，点击“预览效果”可查看效果；2. 可以通过点击评论楼层编号来引用该楼层；3. 网站可能会有点卡，如非确认评论失败，请 **不要重复点击提交**。

### 内容速览

[基础回顾](https://kexue.fm/archives/10648#%E5%9F%BA%E7%A1%80%E5%9B%9E%E9%A1%BE)
[梯度推导](https://kexue.fm/archives/10648#%E6%A2%AF%E5%BA%A6%E6%8E%A8%E5%AF%BC)
[权重衰减](https://kexue.fm/archives/10648#%E6%9D%83%E9%87%8D%E8%A1%B0%E5%87%8F)
[数值计算](https://kexue.fm/archives/10648#%E6%95%B0%E5%80%BC%E8%AE%A1%E7%AE%97)
[迭代证明](https://kexue.fm/archives/10648#%E8%BF%AD%E4%BB%A3%E8%AF%81%E6%98%8E)
[相关工作](https://kexue.fm/archives/10648#%E7%9B%B8%E5%85%B3%E5%B7%A5%E4%BD%9C)
[文章小结](https://kexue.fm/archives/10648#%E6%96%87%E7%AB%A0%E5%B0%8F%E7%BB%93)

### 智能搜索

支持整句搜索！网站自动使用 [结巴分词](https://github.com/fxsjy/jieba) 进行分词，并结合ngrams排序算法给出合理的搜索结果。

### 热门标签

[生成模型](https://kexue.fm/tag/%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B/) [attention](https://kexue.fm/tag/attention/) [优化](https://kexue.fm/tag/%E4%BC%98%E5%8C%96/) [语言模型](https://kexue.fm/tag/%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/) [模型](https://kexue.fm/tag/%E6%A8%A1%E5%9E%8B/) [网站](https://kexue.fm/tag/%E7%BD%91%E7%AB%99/) [概率](https://kexue.fm/tag/%E6%A6%82%E7%8E%87/) [梯度](https://kexue.fm/tag/%E6%A2%AF%E5%BA%A6/) [转载](https://kexue.fm/tag/%E8%BD%AC%E8%BD%BD/) [微分方程](https://kexue.fm/tag/%E5%BE%AE%E5%88%86%E6%96%B9%E7%A8%8B/) [矩阵](https://kexue.fm/tag/%E7%9F%A9%E9%98%B5/) [天象](https://kexue.fm/tag/%E5%A4%A9%E8%B1%A1/) [分析](https://kexue.fm/tag/%E5%88%86%E6%9E%90/) [深度学习](https://kexue.fm/tag/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/) [积分](https://kexue.fm/tag/%E7%A7%AF%E5%88%86/) [python](https://kexue.fm/tag/python/) [优化器](https://kexue.fm/tag/%E4%BC%98%E5%8C%96%E5%99%A8/) [力学](https://kexue.fm/tag/%E5%8A%9B%E5%AD%A6/) [无监督](https://kexue.fm/tag/%E6%97%A0%E7%9B%91%E7%9D%A3/) [扩散](https://kexue.fm/tag/%E6%89%A9%E6%95%A3/) [几何](https://kexue.fm/tag/%E5%87%A0%E4%BD%95/) [节日](https://kexue.fm/tag/%E8%8A%82%E6%97%A5/) [生活](https://kexue.fm/tag/%E7%94%9F%E6%B4%BB/) [文本生成](https://kexue.fm/tag/%E6%96%87%E6%9C%AC%E7%94%9F%E6%88%90/) [数论](https://kexue.fm/tag/%E6%95%B0%E8%AE%BA/)

### 随机文章

- [RSGAN：对抗模型中的“图灵测试”思想](https://kexue.fm/archives/6110)
- [《自然极值》系列——1.前言](https://kexue.fm/archives/1065)
- [EMO：基于最优传输思想设计的分类损失函数](https://kexue.fm/archives/9797)
- [“让Keras更酷一些！”：随意的输出和灵活的归一化](https://kexue.fm/archives/6311)
- [更别致的词向量模型(一)：simpler glove](https://kexue.fm/archives/4667)
- [GAU-α：尝鲜体验快好省的下一代Attention](https://kexue.fm/archives/9052)
- [奥赛版《春天里》](https://kexue.fm/archives/1418)
- [泛化性乱弹：从随机噪声、梯度惩罚到虚拟对抗训练](https://kexue.fm/archives/7466)
- [msign算子的Newton-Schulz迭代（下）](https://kexue.fm/archives/10996)
- [生成扩散模型漫谈（二十三）：信噪比与大图生成（下）](https://kexue.fm/archives/10055)

### 最近评论

- [1](https://kexue.fm/reward.html/comment-page-1#comment-27905): 1\[comment=14986\]Daniel Cheng\[/comment\]\[comment=...
- [1](https://kexue.fm/reward.html/comment-page-1#comment-27904): 1\[comment=14986\]Daniel Cheng\[/comment\]\[comment=...
- [1](https://kexue.fm/links.html/comment-page-6#comment-27903): 1\[comment=25585\]Evan-wyl\[/comment\]\[comment=2564...
- [1](https://kexue.fm/links.html/comment-page-6#comment-27902): 1\[comment=25585\]Evan-wyl\[/comment\]\[comment=2564...
- [tll1945tll1937](https://kexue.fm/archives/10266/comment-page-1#comment-27901): 真心实意的向大家请教问题：看了文章“对齐全量微调！这是我看过最精彩的LoRA改进（二）”，我实...
- [oYo\_logan](https://kexue.fm/archives/10757/comment-page-1#comment-27897): \[comment=27017\]苏剑林\[/comment\]苏神，想请教一下，我理解在一个batc...
- [z123](https://kexue.fm/archives/10592/comment-page-1#comment-27896): 在参数矩阵较多的CNN小模型上，Muon会明显慢于Adam，这方面有什么优化提速的方案吗？
- [dry](https://kexue.fm/archives/10958/comment-page-2#comment-27895): 苏神好，一直有个疑问，ReFlow构建的ODE是$dx\_t/dt=x\_1-x\_0$，为什么这并...
- [tyj](https://kexue.fm/archives/10958/comment-page-2#comment-27894): 感觉和之前的一篇文章很像，应该算是concurrent work： https://arxiv...
- [li6626](https://kexue.fm/archives/10667/comment-page-1#comment-27893): 苏老师，Normalizing Flow有了新进展，论文链接:https://arxiv.or...

### 友情链接

- [Cool Papers](https://papers.cool)
- [数学研发](https://bbs.emath.ac.cn)
- [Seatop](http://www.seatop.com.cn/)
- [Xiaoxia](https://xiaoxia.org/)
- [积分表-网络版](https://kexue.fm/sci/integral/index.html)
- [丝路博傲](http://blog.dvxj.com/)
- [ph4ntasy 饭特稀](http://www.ph4ntasy.com/)
- [数学之家](http://www.2math.cn/)
- [有趣天文奇观](http://interesting-sky.china-vo.org/)
- [TwistedW](http://www.twistedwg.com/)
- [godweiyang](https://godweiyang.com/)
- [AI柠檬](https://blog.ailemon.net/)
- [王登科-DK博客](https://greatdk.com)
- [ESON](https://blog.eson.org/)
- [枫之羽](https://fzhiy.net/)
- [Mathor's blog](https://wmathor.com/)
- [coding-zuo](https://coding-zuo.github.io/)
- [博科园](https://www.bokeyuan.net/)
- [孔皮皮的博客](https://www.kppkkp.top/)
- [运鹏的博客](https://yunpengtai.top/)
- [jiming.site](https://jiming.site/)
- [OmegaXYZ](https://www.omegaxyz.com/)
- [Blog by Eacls](https://www.eacls.top/)
- [EAI猩球](https://www.robotech.ink/)
- [文举的博客](https://liwenju0.com/)
- [用代码打点酱油](https://bruceyuan.com/)
- [申请链接](https://kexue.fm/links.html)

本站采用创作共用版权协议，要求署名、非商业用途和保持一致。转载本站内容必须也遵循“ [署名-非商业用途-保持一致](http://creativecommons.org/licenses/by-nc-nd/2.5/cn/)”的创作共用协议。
© 2009-2025 Scientific Spaces. All rights reserved. Theme by [laogui](http://www.laogui.com). Powered by [Typecho](http://typecho.org). 备案号: [粤ICP备09093259号-1/2](https://beian.miit.gov.cn/)。