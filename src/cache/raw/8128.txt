## SEARCH

## MENU

- [打赏](https://kexue.fm/reward.html)
- [公式](https://kexue.fm/latex.html)
- [天象](https://kexue.fm/ac.html)
- [链接](https://kexue.fm/links.html)
- [时光](https://kexue.fm/me.html)
- [博览](https://kexue.fm/science.html)
- [归档](https://kexue.fm/content.html)

## CATEGORIES

- [千奇百怪](https://kexue.fm/category/Everything)
- [天文探索](https://kexue.fm/category/Astronomy)
- [数学研究](https://kexue.fm/category/Mathematics)
- [物理化学](https://kexue.fm/category/Phy-chem)
- [信息时代](https://kexue.fm/category/Big-Data)
- [生物自然](https://kexue.fm/category/Biology)
- [图片摄影](https://kexue.fm/category/Photograph)
- [问题百科](https://kexue.fm/category/Questions)
- [生活/情感](https://kexue.fm/category/Life-Feeling)
- [资源共享](https://kexue.fm/category/Resources)

## NEWPOSTS

- [线性注意力简史：从模仿、创新到反哺](https://kexue.fm/archives/11033)
- [msign的导数](https://kexue.fm/archives/11025)
- [通过msign来计算mclip（奇...](https://kexue.fm/archives/11006)
- [msign算子的Newton-Sc...](https://kexue.fm/archives/10996)
- [等值振荡定理：最优多项式逼近的充要条件](https://kexue.fm/archives/10972)
- [生成扩散模型漫谈（三十）：从瞬时速...](https://kexue.fm/archives/10958)
- [MoE环游记：5、均匀分布的反思](https://kexue.fm/archives/10945)
- [msign算子的Newton-Sc...](https://kexue.fm/archives/10922)
- [Transformer升级之路：2...](https://kexue.fm/archives/10907)
- [一道概率不等式：盯着它到显然成立为止！](https://kexue.fm/archives/10902)

## COMMENTS

- [石子131: 也许可以尝试把热水管的回水管的开关阀做成用户的手动阀，在热水管...](https://kexue.fm/archives/9405/comment-page-2#comment-27955)
- [Kuo: 我的理解，这是一个迭代过程，注意下标K是指condition还...](https://kexue.fm/archives/10795/comment-page-1#comment-27954)
- [musicfish1973: 好的设计都是相似的,haha](https://kexue.fm/archives/8009/comment-page-1#comment-27953)
- [忍者猫: 这优化器的作者真的应该给你打钱](https://kexue.fm/archives/10592/comment-page-2#comment-27952)
- [Chaofa Yuan: 写得太好了](https://kexue.fm/archives/11033/comment-page-1#comment-27951)
- [Skyler Lin: respect苏神！](https://kexue.fm/archives/11033/comment-page-1#comment-27949)
- [宋佳铭: 对，个人感觉mean flow就是continuous tim...](https://kexue.fm/archives/10958/comment-page-1#comment-27947)
- [宋佳铭: 的确，对sg这个事情我感觉如果是用‘归纳’法做是不太能避免的，...](https://kexue.fm/archives/10958/comment-page-1#comment-27946)
- [MoFHeka: 苏老师您好，请问一下这套结论在稀疏参数上应该如何应用？比如大规...](https://kexue.fm/archives/10542/comment-page-1#comment-27945)
- [苏剑林: Temp LoRA倒是有印象，其实思想是一样的，如果我单独开一...](https://kexue.fm/archives/11033/comment-page-1#comment-27944)

## USERLOGIN

- [登录](https://kexue.fm/admin/login.php)

[科学空间\|Scientific Spaces](https://kexue.fm)

- [登录](https://kexue.fm/admin/login.php)
- [打赏](https://kexue.fm/reward.html)
- [公式](https://kexue.fm/latex.html)
- [天象](https://kexue.fm/ac.html)
- [链接](https://kexue.fm/links.html)
- [时光](https://kexue.fm/me.html)
- [博览](https://kexue.fm/science.html)
- [归档](https://kexue.fm/content.html)

渴望成为一个小飞侠

- [欢迎订阅](https://kexue.fm/feed)
- [个性邮箱](https://kexue.fm/archives/119)
- [天象信息](https://kexue.fm/ac.html)
- [观测ISS](https://kexue.fm/archives/41)
- [LaTeX](https://kexue.fm/latex.html)
- [关于博主](https://kexue.fm/me.html)

欢迎访问“科学空间”，这里将与您共同探讨自然科学，回味人生百态；也期待大家的分享～

- [**千奇百怪** Everything](https://kexue.fm/category/Everything)
- [**天文探索** Astronomy](https://kexue.fm/category/Astronomy)
- [**数学研究** Mathematics](https://kexue.fm/category/Mathematics)
- [**物理化学** Phy-chem](https://kexue.fm/category/Phy-chem)
- [**信息时代** Big-Data](https://kexue.fm/category/Big-Data)
- [**生物自然** Biology](https://kexue.fm/category/Biology)
- [**图片摄影** Photograph](https://kexue.fm/category/Photograph)
- [**问题百科** Questions](https://kexue.fm/category/Questions)
- [**生活/情感** Life-Feeling](https://kexue.fm/category/Life-Feeling)
- [**资源共享** Resources](https://kexue.fm/category/Resources)

- [**千奇百怪**](https://kexue.fm/category/Everything)
- [**天文探索**](https://kexue.fm/category/Astronomy)
- [**数学研究**](https://kexue.fm/category/Mathematics)
- [**物理化学**](https://kexue.fm/category/Phy-chem)
- [**信息时代**](https://kexue.fm/category/Big-Data)
- [**生物自然**](https://kexue.fm/category/Biology)
- [**图片摄影**](https://kexue.fm/category/Photograph)
- [**问题百科**](https://kexue.fm/category/Questions)
- [**生活/情感**](https://kexue.fm/category/Life-Feeling)
- [**资源共享**](https://kexue.fm/category/Resources)

[首页](https://kexue.fm) [数学研究](https://kexue.fm/category/Mathematics) [信息时代](https://kexue.fm/category/Big-Data) Seq2Seq重复解码现象的理论分析尝试

26Jan

# [Seq2Seq重复解码现象的理论分析尝试](https://kexue.fm/archives/8128)

By 苏剑林 \|
2021-01-26 \|
39282位读者\|

去年笔者写过博文 [《如何应对Seq2Seq中的“根本停不下来”问题？》](https://kexue.fm/archives/7500)，里边介绍了一篇论文中对Seq2Seq解码不停止现象的处理，并指出那篇论文只是提了一些应对该问题的策略，并没有提供原理上的理解。近日，笔者在Arixv读到了AAAI 2021的一篇名为 [《A Theoretical Analysis of the Repetition Problem in Text Generation》](https://papers.cool/arxiv/2012.14660) 的论文，里边从理论上分析了Seq2Seq重复解码现象。从本质上来看，重复解码和解码不停止其实都是同理的，所以这篇新论文算是填补了前面那篇论文的空白。

经过学习，笔者发现该论文确实有不少可圈可点之处，值得一读。笔者对原论文中的分析过程做了一些精简、修正和推广，将结果记录成此文，供大家参考。此外，抛开问题背景不讲，读者也可以将本文当成一节矩阵分析习题课，供大家复习线性代数哈～

## 基本思路 [\#](https://kexue.fm/archives/8128\#%E5%9F%BA%E6%9C%AC%E6%80%9D%E8%B7%AF)

所谓重复解码，指的是解码结果出现重复的片段，比如解码结果为“A B C D B C D B C D E F”，那么“B C D”就是重复片段了，因此这个解码结果就出现了重复解码现象。简单起见，如果解码过程中子序列$s=\[w\_1,w\_2,\\cdots,w\_n\]$后面接着的子序列是$t=\[w\_1,w\_2,\\cdots,w\_n,w\_1\]$，我们就称$\[w\_1,w\_2,\\cdots,w\_n\]$为一个“重复子序列”，而我们现在要做的事情，就是要分析解码过程中出现重复子序列的概率。

可能有读者疑问，为什么$t$的最后要多加一个$w\_1$？从后面的过程中我们可以明白到，这个其实只是为了分析上的方便，并没有什么必然性。我们希望得到的是一个有代表性的定量指标来衡量这个重复解码问题，最好还能从中能获得一些改进的思路，至于这个指标的具体细节，我们可以不用太在意。将研究目标量化是非常重要的，只有把目标量化后，我们才能更好地把握改进的方向，也才能去比较不同的方法优劣。不然就算吵得面红耳赤的，也终究无法得到个结论出来。

为了得到这样的一个指标，我们接下来先从简单的二元解码出发，得到一些有代表性的结果，然后看它能否推广到一般的自回归解码器中去。

## 二元解码 [\#](https://kexue.fm/archives/8128\#%E4%BA%8C%E5%85%83%E8%A7%A3%E7%A0%81)

一般的自回归模型形式为：
\\begin{equation}p(\\boldsymbol{y}\|\\boldsymbol{x}) = \\prod\_{t=1}^l p(y\_t\|\\boldsymbol{y}\_{< t}, \\boldsymbol{x})\\end{equation}
也就是说，位置$t$的解码不仅依赖于输入$\\boldsymbol{x}$，还依赖于$t$之前已经获得的所有解码结果。而简单起见，我们先考虑一种简单的情况，假设每一步解码只依赖于前一时刻的结果，即：
\\begin{equation}p(\\boldsymbol{y}\|\\boldsymbol{x}) = \\prod\_{t=1}^l p(y\_t\|y\_{t-1}, \\boldsymbol{x})\\end{equation}
这样一来，对于固定的输入$\\boldsymbol{x}$，解码器事实上就只是一个$n\\times n$的转移矩阵$\\boldsymbol{P}=(P\_{i,j})$，其中$P\_{i,j}$表示从$i$后面接$j$的概率，$n$代表词表大小。这样的解码器叫做二元文法模型、2-gram模型、马尔可夫模型，等等。我们还需要一个终止标记 ，遇到 就停止解码，所以实际上转移矩阵是$(n+1)\\times (n+1)$才对，但是我们考虑重复解码都是在终止之前的，所以只需要考虑除去 的$n\\times n$部分就行了。

我们要计算的是重复子序列的出现概率，假如以$\[i, j, k\]$是一个三元重复子序列，那么它的出现概率就是序列$\[i, j, k, i, j, k, i\]$出现的概率：
\\begin{equation}P\_{i,j}P\_{j,k}P\_{k,i}P\_{i,j}P\_{j,k}P\_{k,i}=P\_{i,j}^2 P\_{j,k}^2 P\_{k,i}^2\\end{equation}
因此所有的三元重复子序列的概率为
\\begin{equation}\\sum\_{i,j,k} P\_{i,j}^2 P\_{j,k}^2 P\_{k,i}^2 = \\text{Tr}\\,(\\boldsymbol{P}\\otimes\\boldsymbol{P})^3\\end{equation}
这里的$\\otimes$表示逐位元素对应相乘，而$\\text{Tr}$则是矩阵的迹，即对角线元素之和。最后，我们将所有长度的重复子序列概率都加起来：
\\begin{equation}R = \\sum\_{k=1}^{\\infty}\\text{Tr}\\,(\\boldsymbol{P}\\otimes\\boldsymbol{P})^k = \\text{Tr}\\,\\left(\\sum\_{k=1}^{\\infty}(\\boldsymbol{P}\\otimes\\boldsymbol{P})^k\\right)\\label{eq:r}\\end{equation}
这个就是二元解码器出现重复解码的概率。当然目前它还只是一个理论公式，不过它是我们重要的出发点。我们将分别推导它的上下界，以获得更具有启发性的结果。

## 一个下界 [\#](https://kexue.fm/archives/8128\#%E4%B8%80%E4%B8%AA%E4%B8%8B%E7%95%8C)

直接看式$\\eqref{eq:r}$不好看出点啥，我们可以先推导它一个更加直观一点的下界。还是以三元重复子序列为例，利用均值不等式我们可以得到：
\\begin{equation}
\\sum\_{i,j,k} P\_{i,j}^2 P\_{j,k}^2 P\_{k,i}^2 = n^3\\times\\frac{\\sum\_{i,j,k} P\_{i,j}^2 P\_{j,k}^2 P\_{k,i}^2}{n^3}\\geq n^3\\times\\left(\\frac{\\sum\_{i,j,k} P\_{i,j} P\_{j,k} P\_{k,i}}{n^3}\\right)^2 = \\frac{(\\text{Tr}\\, \\boldsymbol{P}^3)^2}{n^3}
\\end{equation}
事实上，我们还可以做得更精细一些。假设矩阵$\\boldsymbol{P}$有一些元素为0，那么$P\_{i,j}^2 P\_{j,k}^2 P\_{k,i}^2$中的非零元素的个数就不是$n^3$了，我们假设非零元素个数为$N\_3(\\boldsymbol{P}) < n^3$，那么我们在利用均值不等式的时候，可以只对非零元素进行，结果是将上述的$n^3$换为$N\_3(\\boldsymbol{P})$：
\\begin{equation}
\\sum\_{i,j,k} P\_{i,j}^2 P\_{j,k}^2 P\_{k,i}^2 \\geq \\frac{(\\text{Tr}\\, \\boldsymbol{P}^3)^2}{N\_3(\\boldsymbol{P})}
\\end{equation}
$N\_3(\\boldsymbol{P})$的直接计算比较困难，没有一般通项公式，但我们可以做个简单估算：设$\\boldsymbol{P}$的非零元素的比例为$\\zeta$，也就是非零元素个数为$\\zeta n^2$，那么我们可以认为$P\_{i,j}^2 P\_{j,k}^2 P\_{k,i}^2$的非零元素比例近似为$\\zeta^3$，而总的排列数为$n^3$，所以我们可以认为$N\_3(\\boldsymbol{P})\\sim \\zeta^3 n^3$，或者一般地$N\_k(\\boldsymbol{P})\\sim \\zeta^k n^k$。注意可以举例说明这个估计既不能保证是上界，也不能保证是下界，所以将$N\_3(\\boldsymbol{P})$替换为$\\zeta^3 n^3$后，我们无法保证上述不等号的成立。不过，如果我们愿意相信$\\zeta^3 n^3$是一个足够好的近似，我们我们依然可以（怀着忐忑而又坚定的信念）写下
\\begin{equation}
\\sum\_{i,j,k} P\_{i,j}^2 P\_{j,k}^2 P\_{k,i}^2 \\geq \\frac{(\\text{Tr}\\, \\boldsymbol{P}^3)^2}{\\zeta^3 n^3}
\\end{equation}
以及
\\begin{equation}R = \\sum\_{k=1}^{\\infty}\\text{Tr}\\,(\\boldsymbol{P}\\otimes\\boldsymbol{P})^k \\geq \\sum\_{k=1}^{\\infty} \\frac{(\\text{Tr}\\, \\boldsymbol{P}^k)^2}{\\zeta^k n^k}\\label{eq:r-2}\\end{equation}
或者我们干脆不关心不等号，而是将最右面的结果视为$R$的一个估计。

## 原文下界 [\#](https://kexue.fm/archives/8128\#%E5%8E%9F%E6%96%87%E4%B8%8B%E7%95%8C)

对于希望对着本文读原论文的读者，此时可能会有点懵了，因为不管是式$\\eqref{eq:r}$还是式$\\eqref{eq:r-2}$，都在原论文中找不到对应。事实上，原文并没有给出精确的式$\\eqref{eq:r}$，也没有给出估计式$\\eqref{eq:r-2}$，而是给出了另一个估计式，它也可以作为式$\\eqref{eq:r}$的下界推导出来。

同样利用均值不等式，我们有
\\begin{equation}\\begin{aligned}
\\sum\_{i,j,k} P\_{i,j}^2 P\_{j,k}^2 P\_{k,i}^2 =&\\, \\sum\_{i} \\sum\_{j,k} P\_{i,j}^2 P\_{j,k}^2 P\_{k,i}^2= \\sum\_{i} n^2\\times\\frac{\\sum\_{j,k} P\_{i,j}^2 P\_{j,k}^2 P\_{k,i}^2}{n^2}\\\
\\geq&\\, \\sum\_{i} n^2\\times\\left(\\frac{\\sum\_{j,k} P\_{i,j} P\_{j,k} P\_{k,i}}{n^2}\\right)^2 = \\frac{\\text{Tr}\\, (\\boldsymbol{P}^3\\otimes \\boldsymbol{P}^3)}{n^2}
\\end{aligned}\\end{equation}
类似地，可以引入非零元素个数的技巧来提高估计精度，非零率依然是$\\zeta^3$，而这次求和的总数是$n^2$，因此非零排列数约为$\\zeta^3 n^2$，所以我们（依旧是怀着忐忑而又坚定的信念）写下：
\\begin{equation}
\\sum\_{i,j,k} P\_{i,j}^2 P\_{j,k}^2 P\_{k,i}^2 \\geq \\frac{\\text{Tr}\\, (\\boldsymbol{P}^3\\otimes \\boldsymbol{P}^3)}{\\zeta^3 n^2}\\end{equation}
以及
\\begin{equation}R = \\sum\_{k=1}^{\\infty}\\text{Tr}\\,(\\boldsymbol{P}\\otimes\\boldsymbol{P})^k \\geq \\sum\_{k=1}^{\\infty}\\frac{\\text{Tr}\\, (\\boldsymbol{P}^k\\otimes \\boldsymbol{P}^k)}{\\zeta^k n^{k-1}}\\label{eq:r-3}\\end{equation}
这基本就是原论文中的“定义2.3”了，跟原论文不同的是：

> 1、原论文算得是平均到每个字词的概率，所以需要多除以一个$n$，因此它的分母是$n^k$；
>
> 2、原论文求迹的是$\\boldsymbol{P}^{2k}$而不是$\\boldsymbol{P}^k\\otimes \\boldsymbol{P}^k$，事实上这是原论文的错误，它在推导过程中把$(\\boldsymbol{P}^k)\_{i,i}^2$当成了$(\\boldsymbol{P}^{2k})\_{i,i}$，事实上它们是不等的，本文的式$\\eqref{eq:r-3}$才是正确的结果。

## 初步结论 [\#](https://kexue.fm/archives/8128\#%E5%88%9D%E6%AD%A5%E7%BB%93%E8%AE%BA)

其实不管是式$\\eqref{eq:r-2}$还是式$\\eqref{eq:r-3}$，形式都差不多，我们都可以用它来得出一些结论。此时，可能有些读者会疑惑：我们一般所用的模型的概率分布都是softmax出来的，softmax的结果都不等于0，所以$\\zeta$应该是恒等1，因此引入$\\zeta$似乎没有没有什么价值？

并非如此。的确，softmax出来的概率分布不会有严格等于0的情况，但是我们的解码算法，通常却会将它们强制置零！在文章 [《如何应对Seq2Seq中的“根本停不下来”问题？》](https://kexue.fm/archives/7500) 中我们就罗列了文本生成常用的解码算法，主要包括随机采样和确定性解码两种，其中随机采样分为直接随机采样、Top-k随机采样、Top-p随机采样，而确定性解码则包括Greedy Search、Beam Search两种，在这五种不同的解码算法中，除了最不常用的直接随机采样外，其余四种都是强行只保留若干个最优结果来作为候选值，这样就相当于直接截断了转移矩阵，大大降低了非零概率$\\zeta$。

比如最极端的Greedy Search，容易推出它实际上对应着最小的非零概率$\\zeta=1/n$，由于$\\zeta$是在分母中，所以$\\zeta$的缩小意味着重复率$R$的增加，这就告诉我们Greedy Search的重复解码风险是相当高的。尽管目前的结论仅仅是在二元解码模型的假设下得出的，但Greedy Search的重复解码确实是我们经常观察到的现象，所以这结论与解释确实已经有代表性了。

## 一个上界 [\#](https://kexue.fm/archives/8128\#%E4%B8%80%E4%B8%AA%E4%B8%8A%E7%95%8C)

有了下界，怎么可以没有上界呢？下界能帮助我们解释一些实验现象，而上界则可以给我们提供改进的思路。

为了推导上界，我们利用到如下两个结论：

> 1、矩阵的迹等于它所有特征值之和；
>
> 2、如果$\\lambda\_1(\\boldsymbol{A})\\geq\\lambda\_2(\\boldsymbol{A})\\geq\\cdots\\geq\\lambda\_n(\\boldsymbol{A})$是矩阵$\\boldsymbol{A}$的所有特征值，那么$\\lambda\_1^k(\\boldsymbol{A})\\geq\\lambda\_2^k(\\boldsymbol{A})\\geq\\cdots\\geq\\lambda\_n^k(\\boldsymbol{A})$是矩阵$\\boldsymbol{A}^k$的所有特征值。

所以，我们可以推导：
\\begin{equation}\\begin{aligned}
R =&\\, \\sum\_{k=1}^{\\infty}\\text{Tr}\\,(\\boldsymbol{P}\\otimes\\boldsymbol{P})^k = \\sum\_{k=1}^{\\infty}\\sum\_{i=1}^n\\lambda\_i\\left((\\boldsymbol{P}\\otimes\\boldsymbol{P})^k\\right)\\\
=&\\, \\sum\_{k=1}^{\\infty}\\sum\_{i=1}^n\\lambda\_i^k\\left(\\boldsymbol{P}\\otimes\\boldsymbol{P}\\right) = \\sum\_{i=1}^n \\sum\_{k=1}^{\\infty}\\lambda\_i^k\\left(\\boldsymbol{P}\\otimes\\boldsymbol{P}\\right) \\\
=&\\, \\sum\_{i=1}^n \\frac{\\lambda\_i \\left(\\boldsymbol{P}\\otimes\\boldsymbol{P}\\right)}{1 - \\lambda\_i \\left(\\boldsymbol{P}\\otimes\\boldsymbol{P}\\right)}
\\end{aligned}\\label{eq:r-4}\\end{equation}
上述过程用到了级数$\\frac{x}{1-x}=\\sum\_{k=1}^{\\infty} x^k$，该级数只有在$\|x\| < 1$才收敛，而很巧的是，我们可以证明$\\boldsymbol{P}\\otimes\\boldsymbol{P}$的特征根绝对值必然不大于1，且通常都小于1：由于$\\boldsymbol{P}$是转移矩阵，因此它的每一行之和都为1，因此$\\boldsymbol{P}\\otimes\\boldsymbol{P}$的每一行之和都小于等于1，设$\\lambda$、$\\boldsymbol{x}$是它的特征值和特征向量，那么$(\\boldsymbol{P}\\otimes\\boldsymbol{P})\\boldsymbol{x}=\\lambda \\boldsymbol{x}$，不失一般性，设$\\boldsymbol{x}$绝对值最大的元素为$x\_1$，$\\boldsymbol{P}\\otimes\\boldsymbol{P}$的第一个行向量为$\\boldsymbol{q}\_1^{\\top}$，那么我们有$\|\\lambda\| \|x\_1\| = \|\\boldsymbol{q}\_1^{\\top}\\boldsymbol{x}\| \\leq \|x\_1\|$，从而$\|\\lambda\| \\leq 1$，并且等号成立的条件还是比较苛刻的，所以通常来说都是$\|\\lambda\| < 1$。

注意函数$\\frac{x}{1-x}$在$\[-1,1)$区间是单调递增的，所以式$\\eqref{eq:r-4}$中占主导的是第一项$\\frac{\\lambda\_1 \\left(\\boldsymbol{P}\\otimes\\boldsymbol{P}\\right)}{1 - \\lambda\_1 \\left(\\boldsymbol{P}\\otimes\\boldsymbol{P}\\right)}$，如果非要给整体弄一个上界的话，那么可以是$\\frac{n \\lambda\_1 \\left(\\boldsymbol{P}\\otimes\\boldsymbol{P}\\right)}{1 - \\lambda\_1 \\left(\\boldsymbol{P}\\otimes\\boldsymbol{P}\\right)}$。

## 再次结论 [\#](https://kexue.fm/archives/8128\#%E5%86%8D%E6%AC%A1%E7%BB%93%E8%AE%BA)

由此可见，如果想要降低重复率$R$，那么我们需要想办法降低矩阵$\\boldsymbol{P}\\otimes\\boldsymbol{P}$的最大特征值。$\\boldsymbol{P}\\otimes\\boldsymbol{P}$是一个非负矩阵，根据非负矩阵的“Frobenius介值定理”，我们有：
\\begin{equation}\\min\_i \\sum\_j P\_{i,j}^2 \\leq \\lambda\_1 (\\boldsymbol{P}\\otimes\\boldsymbol{P}) \\leq \\max\_i \\sum\_j P\_{i,j}^2\\end{equation}
关于Frobenius介值定理，基本上在任何一本矩阵分析的书上都有介绍，它说的是“非负矩阵的最大特征值在它每一行的和的最小值于最大值之间”。现在我们知道，为了降低$\\boldsymbol{P}\\otimes\\boldsymbol{P}$的最大特征值，我们需要想办法降低它的每一行之和，即$\\sum\_j P\_{i,j}^2$，并且由于均值不等式
\\begin{equation}\\sum\_j P\_{i,j}^2\\geq n\\left(\\frac{\\sum\_j P\_{i,j}}{n}\\right)^2 = \\frac{1}{n}\\end{equation}
知它的最小值为$1/n$，在$P\_{i,1}=P\_{i,2}=\\cdots=P\_{i,n}$时取到，因此最终我们得出结论： **要降低最大特征值，就要使得矩阵$\\boldsymbol{P}$每一行尽可能均匀，换言之，要降低$\\boldsymbol{P}$每一行的方差。**

怎么降低方差呢？很简单，不能出现过高的概率值即可，比如某一行接近one hot的形式，那么平方之后依然接近one hot的形式，那么求和就接近1，远远大于理论最小值$1/n$。什么情况下会出现过高的概率值呢？也不难理解，就是某个字词后面可以接的字词很少，甚至只有1个候选值的时候，比如“忐”几乎只能接“忑”，那么$P\_{i=\\text{忐},j=\\text{忑}}$就相当高，“矩”后面大概接“阵”、“形”比较多，所以“矩”那一行的方差也不小。那怎么才能不出现这种过高的概率值呢？很简单，将高概率值的合并起来，当作一个新词来看待就行了，比如“忐忑”合并为一个词，那么“忐”那一行就不存在了，也就无所谓方差大了。同理，“矩形”、“矩阵”也应该合并为一个词比较好。

所以，说白了这就告诉我们，对于文本生成任务来说，以词为单位比以字为单位更加靠谱（更不容易出现重复解码）。适当地合并一些相关程度比较高的词作为新词加入到词表中，降低转移矩阵的方差，有助于降低重复解码的风险，原论文还给这个操作起了个很高端的名字，叫做Rebalanced Encoding Algorithm，事实上就是这个意思。我们之前词颗粒度的WoBERT在生成任务上比字颗粒度的BERT做得更好，也算是这个结论的验证了吧（参考 [《提速不掉点：基于词颗粒度的中文WoBERT》](https://kexue.fm/archives/7758)）。

## 一般解码 [\#](https://kexue.fm/archives/8128\#%E4%B8%80%E8%88%AC%E8%A7%A3%E7%A0%81)

那这个证明过程容易推广到一般的自回归模型中吗？很遗憾，并不容易。对于一般的自回归模型来说，它相当于每一步的$\\boldsymbol{P}$都是不一样的，因此只要模型的性能足够好，其实基本上不会出现重复解码，事实上经过充分预训练的生成式模型，确实很少出现重复解码了。但是，我们又能观察到，哪怕是一般的自回归解码，偶尔也能观察到重复解码现象，尤其是没有经过预训练的模型，这又该怎么解释呢？

前面的小节是基于二元解码模型的，结论是二元解码模型确实容易出现重复解码，那么我们或许可以反过来想，一般的自回归模型出现重复解码现象，是因为它此时退化为了二元解码模型？对于难度比较高的输入，模型可能无法精细捕捉好每一步的转移概率，从而只能将转移矩阵退化为二元解码，这是有可能的。

那么原论文对这一块又是怎么处理的呢？其实也差不多这样。原论文假设一般的自回归模型的转移矩阵，只是在二元解码的转移矩阵$\\boldsymbol{P}$的基础上加了个特定时刻的扰动$\\tilde{\\boldsymbol{P}}\_t=\\boldsymbol{P}+\\boldsymbol{Q}\_t$，然后指出在$\\boldsymbol{Q}\_t$足够小的时候它跟二元解码的差距也足够小（有点像废话），因此二元解码的结果也能代表一般自回归模型了。所以，对一般的自回归模型来说，我们确实很无力了，只能用这种想法跟它沾点边了～

## 文章小结 [\#](https://kexue.fm/archives/8128\#%E6%96%87%E7%AB%A0%E5%B0%8F%E7%BB%93)

本文是对Seq2Seq重复解码现象的一次理论分析尝试，主要的篇幅是针对二元解码模型得出一些定量的结果，并且发现这些结果确实能解释一些现象，并且还能带来一些改进的思路，最后比较“勉强”地将二元解码与一般的自回归模型联系了起来。本文在思路上受启发于论文 [《A Theoretical Analysis of the Repetition Problem in Text Generation》](https://papers.cool/arxiv/2012.14660)，但推导过程都是自己闭门造车的，公式定义也跟原论文略有不同，但总体而言结论是一致的，还请读者自行辨别，如果谬误，敬请斧正。

_**转载到请包括本文地址：** [https://kexue.fm/archives/8128](https://kexue.fm/archives/8128)_

_**更详细的转载事宜请参考：**_ [《科学空间FAQ》](https://kexue.fm/archives/6508#%E6%96%87%E7%AB%A0%E5%A6%82%E4%BD%95%E8%BD%AC%E8%BD%BD/%E5%BC%95%E7%94%A8)

**如果您还有什么疑惑或建议，欢迎在下方评论区继续讨论。**

**如果您觉得本文还不错，欢迎 [分享](https://kexue.fm/archives/8128#share)/ [打赏](https://kexue.fm/archives/8128#pay) 本文。打赏并非要从中获得收益，而是希望知道科学空间获得了多少读者的真心关注。当然，如果你无视它，也不会影响你的阅读。再次表示欢迎和感谢！**

打赏

微信打赏

支付宝打赏

因为网站后台对打赏并无记录，因此欢迎在打赏时候备注留言。你还可以 [**点击这里**](http://mail.qq.com/cgi-bin/qm_share?t=qm_mailme&email=tN7d1drY3drrx8H0xcWa19vZ) 或在下方评论区留言来告知你的建议或需求。

**如果您需要引用本文，请参考：**

苏剑林. (Jan. 26, 2021). 《Seq2Seq重复解码现象的理论分析尝试 》\[Blog post\]. Retrieved from [https://kexue.fm/archives/8128](https://kexue.fm/archives/8128)

@online{kexuefm-8128,
        title={Seq2Seq重复解码现象的理论分析尝试},
        author={苏剑林},
        year={2021},
        month={Jan},
        url={\\url{https://kexue.fm/archives/8128}},
}

分类： [数学研究](https://kexue.fm/category/Mathematics), [信息时代](https://kexue.fm/category/Big-Data)    标签： [矩阵](https://kexue.fm/tag/%E7%9F%A9%E9%98%B5/), [语言模型](https://kexue.fm/tag/%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/), [文本生成](https://kexue.fm/tag/%E6%96%87%E6%9C%AC%E7%94%9F%E6%88%90/), [解码](https://kexue.fm/tag/%E8%A7%A3%E7%A0%81/)[5 评论](https://kexue.fm/archives/8128#comments)

< [【搜出来的文本】⋅（三）基于BERT的文本采样](https://kexue.fm/archives/8119) \| [让研究人员绞尽脑汁的Transformer位置编码](https://kexue.fm/archives/8130) >

### 你也许还对下面的内容感兴趣

- [msign的导数](https://kexue.fm/archives/11025)
- [通过msign来计算mclip（奇异值裁剪）](https://kexue.fm/archives/11006)
- [Transformer升级之路：20、MLA究竟好在哪里？](https://kexue.fm/archives/10907)
- [SVD的导数](https://kexue.fm/archives/10878)
- [Transformer升级之路：19、第二类旋转位置编码](https://kexue.fm/archives/10862)
- [矩阵的有效秩（Effective Rank）](https://kexue.fm/archives/10847)
- [Muon续集：为什么我们选择尝试Muon？](https://kexue.fm/archives/10739)
- [低秩近似之路（五）：CUR](https://kexue.fm/archives/10662)
- [从谱范数梯度到新式权重衰减的思考](https://kexue.fm/archives/10648)
- [Muon优化器赏析：从向量到矩阵的本质跨越](https://kexue.fm/archives/10592)

[发表你的看法](https://kexue.fm/archives/8128#comment_form)

spzhuang

January 26th, 2021

全文都看过，我也一一验证了文章中出现的数学公式，基本都没有出入。除了不知道如何得出”假如$P$的非零比例为$\\zeta$“,那么$P\_{ij}P\_{jk}P\_{ki}$非零比例为$\\zeta^3$,这一推论以外。

[回复评论](https://kexue.fm/archives/8128/comment-page-1?replyTo=15391#respond-post-8128)

[苏剑林](https://kexue.fm) 发表于
January 26th, 2021

谢谢检验。这个比例仅仅是一个直观估计：我们将它当做随机事件，因为单个$P\_{i, j}$非零的概率是$\\zeta$，所以三项同时非零的概率就是$\\zeta^3$。由于我们是遍历求和，所以这个估计应该是有代表性的。

[回复评论](https://kexue.fm/archives/8128/comment-page-1?replyTo=15392#respond-post-8128)

spzhuang 发表于
January 27th, 2021

谢谢苏神解释，我也有在科学空间微信群里面，共勉。

[回复评论](https://kexue.fm/archives/8128/comment-page-1?replyTo=15405#respond-post-8128)

Ran

January 26th, 2021

赞，这种研究比模型炼丹有意义太多了

[回复评论](https://kexue.fm/archives/8128/comment-page-1?replyTo=15394#respond-post-8128)

[Seq2Seq Repeated Decoding Retrace - Technology Blog](https://aivgg.com/seq2seq-repeated-decoding-retrace/)

February 1st, 2022

\[...\]:: Seq2Seq Reciprocal Analysis Attempt\[...\]

[回复评论](https://kexue.fm/archives/8128/comment-page-1?replyTo=18348#respond-post-8128)

[取消回复](https://kexue.fm/archives/8128#respond-post-8128)

你的大名

电子邮箱

个人网站（选填）

1\. 可以使用LaTeX代码，点击“预览效果”可查看效果；2. 可以通过点击评论楼层编号来引用该楼层；3. 网站可能会有点卡，如非确认评论失败，请 **不要重复点击提交**。

### 内容速览

[基本思路](https://kexue.fm/archives/8128#%E5%9F%BA%E6%9C%AC%E6%80%9D%E8%B7%AF)
[二元解码](https://kexue.fm/archives/8128#%E4%BA%8C%E5%85%83%E8%A7%A3%E7%A0%81)
[一个下界](https://kexue.fm/archives/8128#%E4%B8%80%E4%B8%AA%E4%B8%8B%E7%95%8C)
[原文下界](https://kexue.fm/archives/8128#%E5%8E%9F%E6%96%87%E4%B8%8B%E7%95%8C)
[初步结论](https://kexue.fm/archives/8128#%E5%88%9D%E6%AD%A5%E7%BB%93%E8%AE%BA)
[一个上界](https://kexue.fm/archives/8128#%E4%B8%80%E4%B8%AA%E4%B8%8A%E7%95%8C)
[再次结论](https://kexue.fm/archives/8128#%E5%86%8D%E6%AC%A1%E7%BB%93%E8%AE%BA)
[一般解码](https://kexue.fm/archives/8128#%E4%B8%80%E8%88%AC%E8%A7%A3%E7%A0%81)
[文章小结](https://kexue.fm/archives/8128#%E6%96%87%E7%AB%A0%E5%B0%8F%E7%BB%93)

### 智能搜索

支持整句搜索！网站自动使用 [结巴分词](https://github.com/fxsjy/jieba) 进行分词，并结合ngrams排序算法给出合理的搜索结果。

### 热门标签

[生成模型](https://kexue.fm/tag/%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B/) [attention](https://kexue.fm/tag/attention/) [优化](https://kexue.fm/tag/%E4%BC%98%E5%8C%96/) [语言模型](https://kexue.fm/tag/%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/) [模型](https://kexue.fm/tag/%E6%A8%A1%E5%9E%8B/) [网站](https://kexue.fm/tag/%E7%BD%91%E7%AB%99/) [概率](https://kexue.fm/tag/%E6%A6%82%E7%8E%87/) [梯度](https://kexue.fm/tag/%E6%A2%AF%E5%BA%A6/) [转载](https://kexue.fm/tag/%E8%BD%AC%E8%BD%BD/) [微分方程](https://kexue.fm/tag/%E5%BE%AE%E5%88%86%E6%96%B9%E7%A8%8B/) [矩阵](https://kexue.fm/tag/%E7%9F%A9%E9%98%B5/) [天象](https://kexue.fm/tag/%E5%A4%A9%E8%B1%A1/) [分析](https://kexue.fm/tag/%E5%88%86%E6%9E%90/) [深度学习](https://kexue.fm/tag/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/) [积分](https://kexue.fm/tag/%E7%A7%AF%E5%88%86/) [python](https://kexue.fm/tag/python/) [优化器](https://kexue.fm/tag/%E4%BC%98%E5%8C%96%E5%99%A8/) [力学](https://kexue.fm/tag/%E5%8A%9B%E5%AD%A6/) [无监督](https://kexue.fm/tag/%E6%97%A0%E7%9B%91%E7%9D%A3/) [扩散](https://kexue.fm/tag/%E6%89%A9%E6%95%A3/) [几何](https://kexue.fm/tag/%E5%87%A0%E4%BD%95/) [节日](https://kexue.fm/tag/%E8%8A%82%E6%97%A5/) [生活](https://kexue.fm/tag/%E7%94%9F%E6%B4%BB/) [文本生成](https://kexue.fm/tag/%E6%96%87%E6%9C%AC%E7%94%9F%E6%88%90/) [数论](https://kexue.fm/tag/%E6%95%B0%E8%AE%BA/)

### 随机文章

- [一道比较函数大小的题目](https://kexue.fm/archives/1395)
- [隐藏在动量中的梯度累积：少更新几步，效果反而更好？](https://kexue.fm/archives/8634)
- [《费恩曼物理讲义》在线版](https://kexue.fm/archives/2231)
- [msign算子的Newton-Schulz迭代（上）](https://kexue.fm/archives/10922)
- [古老的火山爆发造成地球冰期？](https://kexue.fm/archives/13)
- [\[更新\]将向量乘法“退化”到复数](https://kexue.fm/archives/1188)
- [2010年诺贝尔化学奖出炉,美日科学家分享](https://kexue.fm/archives/979)
- [《向量》系列——1.向心力公式证明](https://kexue.fm/archives/701)
- [【NASA每日一图】飞马座的星系](https://kexue.fm/archives/60)
- [我们可以无损放大一个Transformer模型吗（一）](https://kexue.fm/archives/8444)

### 最近评论

- [石子131](https://kexue.fm/archives/9405/comment-page-2#comment-27955): 也许可以尝试把热水管的回水管的开关阀做成用户的手动阀，在热水管临近回水管、手动阀靠近热水管侧加...
- [Kuo](https://kexue.fm/archives/10795/comment-page-1#comment-27954): 我的理解，这是一个迭代过程，注意下标K是指condition还是desideratum
- [musicfish1973](https://kexue.fm/archives/8009/comment-page-1#comment-27953): 好的设计都是相似的,haha
- [忍者猫](https://kexue.fm/archives/10592/comment-page-2#comment-27952): 这优化器的作者真的应该给你打钱
- [Chaofa Yuan](https://kexue.fm/archives/11033/comment-page-1#comment-27951): 写得太好了
- [Skyler Lin](https://kexue.fm/archives/11033/comment-page-1#comment-27949): respect苏神！
- [宋佳铭](https://kexue.fm/archives/10958/comment-page-1#comment-27947): 对，个人感觉mean flow就是continuous time CTM
- [宋佳铭](https://kexue.fm/archives/10958/comment-page-1#comment-27946): 的确，对sg这个事情我感觉如果是用‘归纳’法做是不太能避免的，因为毕竟是用步长短的模型去约束步...
- [MoFHeka](https://kexue.fm/archives/10542/comment-page-1#comment-27945): 苏老师您好，请问一下这套结论在稀疏参数上应该如何应用？比如大规模稀疏Embedding，每个B...
- [苏剑林](https://kexue.fm/archives/11033/comment-page-1#comment-27944): Temp LoRA倒是有印象，其实思想是一样的，如果我单独开一篇文章介绍TTT的话，应该会提到...

### 友情链接

- [Cool Papers](https://papers.cool)
- [数学研发](https://bbs.emath.ac.cn)
- [Seatop](http://www.seatop.com.cn/)
- [Xiaoxia](https://xiaoxia.org/)
- [积分表-网络版](https://kexue.fm/sci/integral/index.html)
- [丝路博傲](http://blog.dvxj.com/)
- [ph4ntasy 饭特稀](http://www.ph4ntasy.com/)
- [数学之家](http://www.2math.cn/)
- [有趣天文奇观](http://interesting-sky.china-vo.org/)
- [TwistedW](http://www.twistedwg.com/)
- [godweiyang](https://godweiyang.com/)
- [AI柠檬](https://blog.ailemon.net/)
- [王登科-DK博客](https://greatdk.com)
- [ESON](https://blog.eson.org/)
- [枫之羽](https://fzhiy.net/)
- [Mathor's blog](https://wmathor.com/)
- [coding-zuo](https://coding-zuo.github.io/)
- [博科园](https://www.bokeyuan.net/)
- [孔皮皮的博客](https://www.kppkkp.top/)
- [运鹏的博客](https://yunpengtai.top/)
- [jiming.site](https://jiming.site/)
- [OmegaXYZ](https://www.omegaxyz.com/)
- [Blog by Eacls](https://www.eacls.top/)
- [EAI猩球](https://www.robotech.ink/)
- [文举的博客](https://liwenju0.com/)
- [用代码打点酱油](https://bruceyuan.com/)
- [申请链接](https://kexue.fm/links.html)

本站采用创作共用版权协议，要求署名、非商业用途和保持一致。转载本站内容必须也遵循“ [署名-非商业用途-保持一致](http://creativecommons.org/licenses/by-nc-nd/2.5/cn/)”的创作共用协议。
© 2009-2025 Scientific Spaces. All rights reserved. Theme by [laogui](http://www.laogui.com). Powered by [Typecho](http://typecho.org). 备案号: [粤ICP备09093259号-1/2](https://beian.miit.gov.cn/)。