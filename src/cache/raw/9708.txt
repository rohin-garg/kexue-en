![MobileSideBar](https://kexue.fm/usr/themes/geekg/images/slide-button.png)

## SEARCH

## MENU

- [打赏](https://kexue.fm/reward.html)
- [公式](https://kexue.fm/latex.html)
- [天象](https://kexue.fm/ac.html)
- [链接](https://kexue.fm/links.html)
- [时光](https://kexue.fm/me.html)
- [博览](https://kexue.fm/science.html)
- [归档](https://kexue.fm/content.html)

## CATEGORIES

- [千奇百怪](https://kexue.fm/category/Everything)
- [天文探索](https://kexue.fm/category/Astronomy)
- [数学研究](https://kexue.fm/category/Mathematics)
- [物理化学](https://kexue.fm/category/Phy-chem)
- [信息时代](https://kexue.fm/category/Big-Data)
- [生物自然](https://kexue.fm/category/Biology)
- [图片摄影](https://kexue.fm/category/Photograph)
- [问题百科](https://kexue.fm/category/Questions)
- [生活/情感](https://kexue.fm/category/Life-Feeling)
- [资源共享](https://kexue.fm/category/Resources)

## NEWPOSTS

- [通向概率分布之路：盘点Softma...](https://kexue.fm/archives/10145)
- [重温SSM（二）：HiPPO的一些...](https://kexue.fm/archives/10137)
- [Transformer升级之路：1...](https://kexue.fm/archives/10122)
- [重温SSM（一）：线性系统和HiP...](https://kexue.fm/archives/10114)
- [缓存与效果的极限拉扯：从MHA、M...](https://kexue.fm/archives/10091)
- [Cool Papers更新：简单搭...](https://kexue.fm/archives/10088)
- [以蒸馏的名义：“从去噪自编码器到生...](https://kexue.fm/archives/10085)
- [生成扩散模型漫谈（二十四）：少走捷...](https://kexue.fm/archives/10077)
- [生成扩散模型漫谈（二十三）：信噪比...](https://kexue.fm/archives/10055)
- [生成扩散模型漫谈（二十二）：信噪比...](https://kexue.fm/archives/10047)

## COMMENTS

- [silingtong: 大神,请教一下，为什么HF的deepseek的apply\_ro...](https://kexue.fm/archives/10091/comment-page-2#comment-24560)
- [lxl-: 请教一个问题，一些加速模型（如 SDXL-turbo等）的 C...](https://kexue.fm/archives/9257/comment-page-3#comment-24559)
- [Wen Fei: 您好，苏神，我有一点不明白。l2 norm对所有token和b...](https://kexue.fm/archives/9859/comment-page-1#comment-24558)
- [Wen Fei: 现在能讲一下 质量的来源了嘛？ 在相对论里，质量和能量是一样的...](https://kexue.fm/archives/2036/comment-page-1#comment-24557)
- [bill: 有人使用VQGAN的训练方法对比过VQ和FSQ吗？这应该是FS...](https://kexue.fm/archives/9826/comment-page-2#comment-24556)
- [NirVa: 为啥不做利用cookie保存star？](https://kexue.fm/archives/9978/comment-page-1#comment-24555)
- [lcz: 苏神，为什么“找一个在整个实数域上都单调递增的函数，而且增长速...](https://kexue.fm/archives/3290/comment-page-2#comment-24554)
- [周名远: Kiro: 很高兴你通过实践验证了SiD的稳定性。SDXL的d...](https://kexue.fm/archives/10085/comment-page-1#comment-24553)
- [苏剑林: 刚刷到这篇paper，它是每个像素都视为一个token，这种做...](https://kexue.fm/archives/9984/comment-page-2#comment-24552)
- [苏剑林: 谢谢，已更正。](https://kexue.fm/archives/10114/comment-page-1#comment-24551)

## USERLOGIN

- [登录](https://kexue.fm/admin/login.php)

[科学空间\|Scientific Spaces](https://kexue.fm)

- [登录](https://kexue.fm/admin/login.php)
- [打赏](https://kexue.fm/reward.html)
- [公式](https://kexue.fm/latex.html)
- [天象](https://kexue.fm/ac.html)
- [链接](https://kexue.fm/links.html)
- [时光](https://kexue.fm/me.html)
- [博览](https://kexue.fm/science.html)
- [归档](https://kexue.fm/content.html)

渴望成为一个小飞侠

- [![](https://kexue.fm/usr/themes/geekg/images/rss.png)\
\
欢迎订阅](https://kexue.fm/feed)
- [![](https://kexue.fm/usr/themes/geekg/images/mail.png)\
\
个性邮箱](https://kexue.fm/archives/119)
- [![](https://kexue.fm/usr/themes/geekg/images/Saturn.png)\
\
天象信息](https://kexue.fm/ac.html)
- [![](https://kexue.fm/usr/themes/geekg/images/iss.png)\
\
观测ISS](https://kexue.fm/archives/41)
- [![](https://kexue.fm/usr/themes/geekg/images/pi.png)\
\
LaTeX](https://kexue.fm/latex.html)
- [![](https://kexue.fm/usr/themes/geekg/images/mlogo.png)\
\
关于博主](https://kexue.fm/me.html)

欢迎访问“科学空间”，这里将与您共同探讨自然科学，回味人生百态；也期待大家的分享～

- [**千奇百怪** Everything](https://kexue.fm/category/Everything)
- [**天文探索** Astronomy](https://kexue.fm/category/Astronomy)
- [**数学研究** Mathematics](https://kexue.fm/category/Mathematics)
- [**物理化学** Phy-chem](https://kexue.fm/category/Phy-chem)
- [**信息时代** Big-Data](https://kexue.fm/category/Big-Data)
- [**生物自然** Biology](https://kexue.fm/category/Biology)
- [**图片摄影** Photograph](https://kexue.fm/category/Photograph)
- [**问题百科** Questions](https://kexue.fm/category/Questions)
- [**生活/情感** Life-Feeling](https://kexue.fm/category/Life-Feeling)
- [**资源共享** Resources](https://kexue.fm/category/Resources)

- [**千奇百怪**](https://kexue.fm/category/Everything)
- [**天文探索**](https://kexue.fm/category/Astronomy)
- [**数学研究**](https://kexue.fm/category/Mathematics)
- [**物理化学**](https://kexue.fm/category/Phy-chem)
- [**信息时代**](https://kexue.fm/category/Big-Data)
- [**生物自然**](https://kexue.fm/category/Biology)
- [**图片摄影**](https://kexue.fm/category/Photograph)
- [**问题百科**](https://kexue.fm/category/Questions)
- [**生活/情感**](https://kexue.fm/category/Life-Feeling)
- [**资源共享**](https://kexue.fm/category/Resources)

[首页](https://kexue.fm) [信息时代](https://kexue.fm/category/Big-Data) Transformer升级之路：12、无限外推的ReRoPE？

7Aug

# [Transformer升级之路：12、无限外推的ReRoPE？](https://kexue.fm/archives/9708)

By 苏剑林 \|
2023-08-07 \|
42460位读者\|

自从在 [《Transformer升级之路：11、将β进制位置进行到底》](https://kexue.fm/archives/9706) 中引入混合进制的思路进一步推广了NTK-aware Scaled RoPE后，笔者感觉类似思路的效果已经达到了上限，想要更大幅度的提升就必须另辟蹊径了。这时候笔者想起了此前构思过的一个思路，该思路由于复杂度较高所以被搁置下了，既然现在已经遇到了瓶颈，那么“唯一的办法就是最好的办法”，于是便将它重拾起来。

万万没想到的是，尽管该方法增加了一些推理复杂度，但它的实验效果却惊人地好——甚至隐约有无限的长度外推能力！因此，笔者迫不及待地撰写了本文来分享该方法。由于形式上跟ReLU激活函数的相似性，所以笔者将该方法命名为“ReRoPE (Rectified Rotary Position Embeddings)”。

## 重温 [\#](https://kexue.fm/archives/9708\#%E9%87%8D%E6%B8%A9)

我们知道， [RoPE](https://kexue.fm/archives/8265) 形式上是一种绝对位置编码，但实际上给Attention带来的是相对位置信息，即如下的 [Toeplitz矩阵](https://en.wikipedia.org/wiki/Toeplitz_matrix)：

\\begin{equation}\\begin{pmatrix}0 & \\\

1 & 0 & \\\

2 & 1 & 0 &\\\

3 & 2 & 1 & 0 & \\\

\\ddots & 3 & 2 & 1 & 0 & \\\

\\ddots & \\ddots & 3 & 2 & 1 & 0 & \\\

\\ddots & \\ddots & \\ddots & \\ddots & \\ddots & \\ddots & \\ddots \\\

\\tiny{L - 2} & \\ddots & \\ddots & \\ddots & \\ddots & \\ddots & \\ddots & \\ddots \\\

\\tiny{L - 1} & \\tiny{L - 2} & \\ddots & \\ddots & \\ddots & 3 & 2 & 1 & 0 & \\\

\\end{pmatrix}\\label{eq:rope}\\end{equation}

这里的$L$是当前样本长度。当$L$明显超出了训练长度时，多出来的位置由于没有被训练过，所以无法保证效果，这就是直接外推（Length Extrapolation）表现通常比较差的原因。

后来，研究人员提出了位置内插（Position Interpolation），它相当于将相对位置矩阵改为：

\\begin{equation}\\begin{pmatrix}0 & \\\

\\frac{1}{k} & 0 & \\\

\\frac{2}{k} & \\frac{1}{k} & 0 &\\\

\\frac{3}{k} & \\frac{2}{k} & \\frac{1}{k} & 0 & \\\

\\ddots & \\frac{3}{k} & \\frac{2}{k} & \\frac{1}{k} & 0 & \\\

\\ddots & \\ddots & \\frac{3}{k} & \\frac{2}{k} & \\frac{1}{k} & 0 & \\\

\\ddots & \\ddots & \\ddots & \\ddots & \\ddots & \\ddots & \\ddots \\\

\\tiny{\\frac{L-2}{k}} & \\ddots & \\ddots & \\ddots & \\ddots & \\ddots & \\ddots & \\ddots \\\

\\tiny{\\frac{L-1}{k}} & \\tiny{\\frac{L-1}{k}} & \\ddots & \\ddots & \\ddots & \\frac{3}{k} & \\frac{2}{k} & \\frac{1}{k} & 0 & \\\

\\end{pmatrix}\\end{equation}

这样一来，只要调整$k$，就可以保证最大的相对位置也不超过训练长度，因此避免了外推。然而，它使得位置信息更加“拥挤”了，所以还需要进行一定步数的微调才能让模型重新工作。而也正因为避免了外推，所以它所需要的微调步数相比直接外推要少得多（神经网络往往更擅长内插而不是外推）。

至于后面提出的NTK-aware Scaled RoPE，则是“剑走偏锋”，巧妙地将外推压力平摊到每一个维度上，所以它不微调也能有不错的效果，但它终究还是依赖外推，这是神经网络不擅长的事情，所以效果存在上限，在笔者的实验中，它的Long Context表现还无法很接近训练效果。

## 融合 [\#](https://kexue.fm/archives/9708\#%E8%9E%8D%E5%90%88)

我们也可以从语言模型的局域性来考察这些方法。所谓局域性，是指语言模型在推断下一个token时，明显更依赖于邻近的token。直接外推保持了局域性（0附近位置编码不变），效果差是因为引入了超出训练长度的位置编码；位置内插虽然没有外推位置编码，但扰乱了局域性（0附近位置编码被压缩为$1/k$），所以不微调效果也不好；而NTK-aware Scaled RoPE通过“高频外推、低频内插”隐含了两者优点，保证了局域性，又没有明显外推位置编码，所以不微调也有不错的效果。

有没有能更直接地结合外推和内插的方法呢？有，我们可以设定一个窗口大小$w$，在窗口内我们使用大小为$1$的位置间隔，在窗口外我们使用大小为$1/k$的位置间隔，整个相对位置矩阵如下：

\\begin{equation}\\begin{pmatrix}

\\color{red}{0} & \\\

\\color{red}{1} & \\color{red}{0} & \\\

\\color{red}{2} & \\color{red}{1} & \\color{red}{0} & \\\

\\color{red}{\\ddots} & \\color{red}{2} & \\color{red}{1} & \\color{red}{0} & \\\

\\color{red}{\\tiny{w - 1}} & \\color{red}{\\ddots} & \\color{red}{2} & \\color{red}{1} & \\color{red}{0} & \\\

\\color{green}{w} & \\color{red}{\\tiny{w - 1}} & \\color{red}{\\ddots} & \\color{red}{2} & \\color{red}{1} & \\color{red}{0} & \\\

\\color{green}{\\tiny{w + \\frac{1}{k}}} & \\color{green}{w} & \\color{red}{\\ddots} & \\color{red}{\\ddots} & \\color{red}{2} & \\color{red}{1} & \\color{red}{0} & \\\

\\color{green}{\\tiny{w + \\frac{2}{k}}} & \\color{green}{\\tiny{w + \\frac{1}{k}}} & \\color{green}{\\ddots} & \\color{red}{\\ddots} & \\color{red}{\\ddots} & \\color{red}{2} & \\color{red}{1} & \\color{red}{0} & \\\

\\color{green}{\\ddots} & \\color{green}{\\tiny{w + \\frac{2}{k}}} & \\color{green}{\\ddots} & \\color{green}{\\ddots} & \\color{red}{\\ddots} & \\color{red}{\\ddots} & \\color{red}{2} & \\color{red}{1} & \\color{red}{0} & \\\

\\color{green}{\\ddots} & \\color{green}{\\ddots} & \\color{green}{\\ddots} & \\color{green}{\\ddots} & \\color{green}{\\ddots} & \\color{red}{\\ddots} & \\color{red}{\\ddots} & \\color{red}{\\ddots} & \\color{red}{\\ddots} & \\color{red}{\\ddots} & \\\

\\color{green}{\\ddots} & \\color{green}{\\ddots} & \\color{green}{\\ddots} & \\color{green}{\\tiny{w + \\frac{2}{k}}} & \\color{green}{\\tiny{w + \\frac{1}{k}}} & \\color{green}{w} & \\color{red}{\\tiny{w - 1}} & \\color{red}{\\ddots} & \\color{red}{2} & \\color{red}{1} & \\color{red}{0} & \\\

\\color{green}{\\tiny{w + \\frac{L-1-w}{k}}} & \\color{green}{\\ddots} & \\color{green}{\\ddots} & \\color{green}{\\ddots} & \\color{green}{\\tiny{w + \\frac{2}{k}}} & \\color{green}{\\tiny{w + \\frac{1}{k}}} & \\color{green}{w} & \\color{red}{\\tiny{w - 1}} & \\color{red}{\\ddots} & \\color{red}{2} & \\color{red}{1} & \\color{red}{0} & \\\

\\end{pmatrix}\\label{eq:leaky-rerope}\\end{equation}

只要$w$小于训练长度，那么通过控制$k$，我们就可以在精确保持了局域性的前提下，使得所有位置编码不超过训练长度，简单直接地结合了直接外推和位置内插。

特别地，矩阵$\\eqref{eq:leaky-rerope}$还有一个特别的case：当$k\\to\\infty$时，它简化为

\\begin{equation}\\begin{pmatrix}

\\color{red}{0} & \\\

\\color{red}{1} & \\color{red}{0} & \\\

\\color{red}{2} & \\color{red}{1} & \\color{red}{0} & \\\

\\color{red}{\\ddots} & \\color{red}{2} & \\color{red}{1} & \\color{red}{0} & \\\

\\color{red}{\\tiny{w - 1}} & \\color{red}{\\ddots} & \\color{red}{2} & \\color{red}{1} & \\color{red}{0} & \\\

\\color{green}{w} & \\color{red}{\\tiny{w - 1}} & \\color{red}{\\ddots} & \\color{red}{2} & \\color{red}{1} & \\color{red}{0} & \\\

\\color{green}{w} & \\color{green}{w} & \\color{red}{\\ddots} & \\color{red}{\\ddots} & \\color{red}{2} & \\color{red}{1} & \\color{red}{0} & \\\

\\color{green}{w} & \\color{green}{w} & \\color{green}{\\ddots} & \\color{red}{\\ddots} & \\color{red}{\\ddots} & \\color{red}{2} & \\color{red}{1} & \\color{red}{0} & \\\

\\color{green}{\\ddots} & \\color{green}{w} & \\color{green}{\\ddots} & \\color{green}{\\ddots} & \\color{red}{\\ddots} & \\color{red}{\\ddots} & \\color{red}{2} & \\color{red}{1} & \\color{red}{0} & \\\

\\color{green}{\\ddots} & \\color{green}{\\ddots} & \\color{green}{\\ddots} & \\color{green}{\\ddots} & \\color{green}{\\ddots} & \\color{red}{\\ddots} & \\color{red}{\\ddots} & \\color{red}{\\ddots} & \\color{red}{\\ddots} & \\color{red}{\\ddots} & \\\

\\color{green}{\\ddots} & \\color{green}{\\ddots} & \\color{green}{\\ddots} & \\color{green}{w} & \\color{green}{w} & \\color{green}{w} & \\color{red}{\\tiny{w - 1}} & \\color{red}{\\ddots} & \\color{red}{2} & \\color{red}{1} & \\color{red}{0} & \\\

\\color{green}{w} & \\color{green}{\\ddots} & \\color{green}{\\ddots} & \\color{green}{\\ddots} & \\color{green}{w} & \\color{green}{w} & \\color{green}{w} & \\color{red}{\\tiny{w - 1}} & \\color{red}{\\ddots} & \\color{red}{2} & \\color{red}{1} & \\color{red}{0} & \\\

\\end{pmatrix}\\label{eq:rerope}\\end{equation}

在这个case下，不管输入长度是多少，它的位置编码范围都不超过$w$，所以这是一种有可能支持任意长度的Context的方案！

形式上，矩阵$\\eqref{eq:rerope}$、$\\eqref{eq:leaky-rerope}$与标准RoPE矩阵$\\eqref{eq:rope}$的关系，就相当于ReLU、Leaky ReLU与Linear的关系，所以笔者将$\\eqref{eq:rerope}$称为“ReRoPE（Rectified RoPE）”，将$\\eqref{eq:leaky-rerope}$称为“Leaky ReRoPE”。

## 计算 [\#](https://kexue.fm/archives/9708\#%E8%AE%A1%E7%AE%97)

其实，类似的思路并不难想到，以往基于Attention Bias的相对位置编码（比如 [经典相对位置编码](https://kexue.fm/archives/8130#%E7%BB%8F%E5%85%B8%E5%BC%8F)、 [T5位置编码](https://kexue.fm/archives/8130#T5%E5%BC%8F)）经常会出现这样的分块运算。然而跟这些相对位置编码不同，在RoPE中实现这样的分块运算会明显增加计算量，这也是该思路会被笔者搁置的主要原因。

怎么理解增加计算量呢？我们知道RoPE是“通过绝对位置实现相对位置”，这样只能得到线性的相对位置，而矩阵$\\eqref{eq:leaky-rerope}$、$\\eqref{eq:rerope}$是非线性的（或者说分段线性的），要实现它只能算两次Attention矩阵，然后组合起来。具体来说，首先用标准的RoPE计算一次Attention矩阵（Softmax之前）

\\begin{equation}a\_{i,j}^{(1)} = \\left(\\boldsymbol{\\mathcal{R}}^i\\boldsymbol{q}\_i\\right)^{\\top}\\left(\\boldsymbol{\\mathcal{R}}^j\\boldsymbol{k}\_j\\right) = \\boldsymbol{q}\_i^{\\top}\\boldsymbol{\\mathcal{R}}^{j-i}\\boldsymbol{k}\_j\\end{equation}

这里第一个等号是实现方式，第二个等号是等效结果，其中$\\boldsymbol{\\mathcal{R}}$就是RoPE的旋转矩阵，简单起见我们省略了Attention的scale因子。接着，我们需要计算间隔为$1/k$的RoPE的Attention矩阵（Leaky ReRoPE）：

\\begin{equation}a\_{i,j}^{(2)} = \\left(\\boldsymbol{\\mathcal{R}}^{(i-w)/k+w}\\boldsymbol{q}\_i\\right)^{\\top}\\left(\\boldsymbol{\\mathcal{R}}^{j/k}\\boldsymbol{k}\_j\\right) = \\boldsymbol{q}\_i^{\\top}\\boldsymbol{\\mathcal{R}}^{(j-i+w)/k-w}\\boldsymbol{k}\_j\\end{equation}

如果是ReRoPE，那么简单一些：

\\begin{equation}a\_{i,j}^{(2)} = \\left(\\boldsymbol{\\mathcal{R}}^w\\boldsymbol{q}\_i\\right)^{\\top}\\boldsymbol{k}\_j = \\boldsymbol{q}\_i^{\\top}\\boldsymbol{\\mathcal{R}}^w\\boldsymbol{k}\_j\\end{equation}

最后，根据$i - j < w$这个条件，将它们合并起来：

\\begin{equation}a\_{i,j} = \\left\\{\\begin{aligned}

&a\_{i,j}^{(1)},\\quad (i - j < w) \\\\[8pt\] &a\_{i,j}^{(2)}, \\quad (i - j \\geq w)

\\end{aligned}\\right.\\end{equation}

不管是ReRoPE还是Leaky ReRoPE，都不可避免地计算两次Attention矩阵（如果有更高效的实现方法，请赐教），这便是增加的计算量之一。此外，需要自定义计算Attention矩阵也导致了不能直接套用现成的flash attention实现，因此相对之下又增加了计算成本。

另一方面，同样是由于非线性的相对位置，所以在自回归解码时，Key序列的cache只能存RoPE之前的，然后在每步解码时给整个Key序列补上对应的RoPE，这样的改动也会增加推理计算量。唯一的好消息是，在token by token解码时，从第二步开始Query序列的长度就为1，此时只需要为Key序列定制RoPE，那么可以只算一次Attention矩阵：

\\begin{equation}a\_{i,j} = \\left\\{\\begin{aligned}

&\\boldsymbol{q}\_i^{\\top}\\left(\\boldsymbol{\\mathcal{R}}^{\\max(j-i,-w)}\\boldsymbol{k}\_j\\right), \\quad(\\text{ReRoPE})\\\\[8pt\]

&\\boldsymbol{q}\_i^{\\top}\\left(\\boldsymbol{\\mathcal{R}}^{\\max(j-i,(j-i+w)/k-w)}\\boldsymbol{k}\_j\\right), \\quad(\\text{Leaky ReRoPE})

\\end{aligned}\\right.\\end{equation}

## 实验 [\#](https://kexue.fm/archives/9708\#%E5%AE%9E%E9%AA%8C)

继续沿着 [《Transformer升级之路：11、将β进制位置进行到底》](https://kexue.fm/archives/9706) 的设置，我们对ReRoPE进行了实验，效果如下表：

\\begin{array}{c\|cc}

\\hline

\\text{测试长度} & 512(\\text{训练}) & 4096(\\text{重复}) & 4096(\\text{不重复})\\\

\\hline

\\text{Baseline} & 49.41\\% & 24.17\\% & 23.16\\% \\\

\\text{Baseline-}\\log n & 49.40\\% & 24.60\\% & 24.02\\% \\\

\\hline

\\text{PI-RoPE} & 49.41\\% & 15.04\\% & 13.54\\% \\\

\\text{PI-RoPE-}\\log n & 49.40\\% & 14.99\\% & 16.51\\% \\\

\\hline

\\text{NTK-RoPE-old} & 49.41\\% & 51.28\\% & 39.27\\% \\\

\\text{NTK-RoPE-}\\log n\\text{-old} & 49.40\\% & 61.71\\% & 43.75\\% \\\

\\hline

\\text{NTK-RoPE-fixed} & 49.41\\% & 51.86\\% & 39.61\\% \\\

\\text{NTK-RoPE-}\\log n^{\\color{red}{\\dagger}}\\text{-fixed} & 49.41\\% & 55.94\\% & 41.11\\% \\\

\\text{NTK-RoPE-}\\log n\\text{-fixed} & 49.40\\% & 62.85\\% & 44.14\\% \\\

\\text{NTK-RoPE-mixed} & 49.41\\% & 53.09\\% & 40.12\\% \\\

\\text{NTK-RoPE-}\\log n^{\\color{red}{\\dagger}}\\text{-mixed} & 49.41\\% & 59.11\\% & 42.38\\% \\\

\\text{NTK-RoPE-}\\log n\\text{-mixed} & 49.40\\% & 68.91\\% & 45.41\\% \\\

\\hline

\\text{ReRoPE-w256} & 49.41\\% & 77.90\\% & 48.48\\% \\\

\\text{ReRoPE-w256-}\\log n^{\\color{red}{\\dagger}} & 49.41\\% & 82.40\\% & 48.85\\% \\\

\\text{ReRoPE-w256-}\\log n & 49.40\\% & \\boldsymbol{85.12\\%} & \\boldsymbol{49.07\\%} \\\

\\hline

\\text{HFWA} & 48.70\\% & 80.84\\% & 48.15\\% \\\

\\hline

\\end{array}

正如文章开头所说，ReRoPE不微调外推的效果可谓出奇地好，不仅明显超越了此前最优的NTK-RoPE-mixed，还明显超过了从零预训练的 [HFWA](https://kexue.fm/archives/9603)！这里的$\\text{w256}$指的$w=256$，$\\log n^{\\color{red}{\\dagger}}$是指预训练没有加入$\\log n$缩放（比如LLAMA），测试阶段每个$\\boldsymbol{q}\_n$都乘上$\\max(1, \\log\_{\\text{maxlen}} n)$，$\\log n$则是指预训练就加入了$\\log n$缩放因子。

以下是一些消融实验，显示出ReRoPE关于$w$还是很鲁棒的，最优值大致是训练长度的$1/4\\sim 1/2$左右：

\\begin{array}{c\|cc}

\\hline

\\text{测试长度} & 512(\\text{训练}) & 4096(\\text{重复}) & 4096(\\text{不重复})\\\

\\hline

\\text{ReRoPE-w64} & 49.41\\% & 69.39\\% & 45.19\\% \\\

\\text{ReRoPE-w64-}\\log n^{\\color{red}{\\dagger}} & 49.41\\% & 78.58\\% & 47.42\\% \\\

\\text{ReRoPE-w64-}\\log n & 49.40\\% & 84.38\\% & 48.14\\% \\\

\\hline

\\text{ReRoPE-w128} & 49.41\\% & 76.11\\% & 47.82\\% \\\

\\text{ReRoPE-w128-}\\log n^{\\color{red}{\\dagger}} & 49.41\\% & 82.28\\% & 48.78\\% \\\

\\text{ReRoPE-w128-}\\log n & 49.40\\% & \\boldsymbol{85.47\\%} & 48.87\\% \\\

\\hline

\\text{ReRoPE-w256} & 49.41\\% & 77.90\\% & 48.48\\% \\\

\\text{ReRoPE-w256-}\\log n^{\\color{red}{\\dagger}} & 49.41\\% & 82.40\\% & 48.85\\% \\\

\\text{ReRoPE-w256-}\\log n & 49.40\\% & 85.12\\% & \\boldsymbol{49.07\\%} \\\

\\hline

\\text{ReRoPE-w384} & 49.41\\% & 70.72\\% & 48.15\\% \\\

\\text{ReRoPE-w384-}\\log n^{\\color{red}{\\dagger}} & 49.41\\% & 76.42\\% & 48.31\\% \\\

\\text{ReRoPE-w384-}\\log n & 49.40\\% & 83.24\\% & 48.62\\% \\\

\\hline

\\text{ReRoPE-w512} & 49.41\\% & 7.09\\% & 8.25\\% \\\

\\text{ReRoPE-w512-}\\log n^{\\color{red}{\\dagger}} & 49.41\\% & 7.08\\% & 8.25\\% \\\

\\text{ReRoPE-w512-}\\log n & 49.40\\% & 15.84\\% & 10.83\\% \\\

\\hline

\\end{array}

下表则对比了ReRoPE和Leaky ReRoPE：

\\begin{array}{c\|cc}

\\hline

\\text{测试长度} & 512(\\text{训练}) & 4096(\\text{重复}) & 4096(\\text{不重复})\\\

\\hline

\\text{ReRoPE-w128-}\\log n & 49.40\\% & \\boldsymbol{85.47\\%} & 48.87\\% \\\

\\text{Leaky ReRoPE-w128-k64-}\\log n & 49.40\\% & 85.29\\% & 48.96\\% \\\

\\text{Leaky ReRoPE-w128-k32-}\\log n & 49.40\\% & 85.31\\% & 49.03\\% \\\

\\text{Leaky ReRoPE-w128-k16-}\\log n & 49.40\\% & 85.15\\% & \\boldsymbol{49.10\\%} \\\

\\text{Leaky ReRoPE-w128-k8-}\\log n & 49.40\\% & 80.00\\% & 48.11\\% \\\

\\hline

\\text{ReRoPE-w256-}\\log n & 49.40\\% & 85.12\\% & 49.07\\% \\\

\\text{Leaky ReRoPE-w256-k64-}\\log n & 49.40\\% & 84.60\\% & 49.03\\% \\\

\\text{Leaky ReRoPE-w256-k32-}\\log n & 49.40\\% & 84.30\\% & 48.97\\% \\\

\\text{Leaky ReRoPE-w256-k16-}\\log n & 49.40\\% & 83.59\\% & 48.87\\% \\\

\\text{Leaky ReRoPE-w256-k8-}\\log n & 49.40\\% & 69.80\\% & 45.72\\% \\\

\\hline

\\end{array}

作为ReRoPE的一般化，经过精调的Leaky ReRoPE是有机会超过ReRoPE的，但提升很微弱。此外，当$k$取有限值时，能处理的最大长度也是有限的，因为我们不能提前知道要生成的总长度，所以只能预设一个足够大的$k$，但设定为有限值之后，当输入足够长时，就会因为位置编码超出训练长度而效果大幅下降，相比之下ReRoPE则不会有这个风险。总的来说，精调Leaky ReRoPE相比ReRoPE的价值似乎不大。

以上实验结果都只是在1亿参数的GAU模型上测试的，下面给出基于llama2-13b的测试结果（指标是loss，越小越好），它代表了在真正的LLM表现：

\\begin{array}{c\|cc}

\\hline

\\text{测试长度} & 4096(\\text{训练}) & 8192 & 16384\\\

\\hline

\\text{RoPE} & 1.4967 & 8.8615 & \\text{-} \\\

\\text{NTK-RoPE} & 1.6081 & 1.5417 & 1.5163 \\\

\\text{ReRoPE} & 1.4996 & 1.4267 & 1.4001 \\\

\\hline

\\end{array}

可以看到，ReRoPE真正做到了几乎不损训练效果（RoPE-4096代表训练效果），并且满足“longer context, lower loss”的理想特点（更多的context应该更加有助于预测）。此外，笔者也在OpenBuddy开源的LLAMA2-13b微调模型上测试了chat的效果，自我感觉还不错（最多测试过20k tokens的Context）。

最后，分享笔者在transformers的LLAMA模型基础上实现ReRoPE和Leaky ReRoPE的代码，读者也可以自行加载LLAMA系列模型进行测试：

> **Github： [https://github.com/bojone/rerope](https://github.com/bojone/rerope)**

## 小结 [\#](https://kexue.fm/archives/9708\#%E5%B0%8F%E7%BB%93)

在这篇文章中，笔者提出了ReRoPE (Rectified RoPE)，它同样是一种RoPE的后处理方案，实验结果显示它的不微调长度外推能力不仅明显超过了此前的NTK-aware Scaled RoPE，甚至还超过了之前专门设计的需要从零训练的HFWA。此外，不同于NTK-aware Scaled RoPE在超过某个长度后能力会大幅下降，ReRoPE似乎在任意长度下都表现良好。除了对比实验外，文章还给出了基于transformers-llama的参考实现，有兴趣的读者可以自行测试。

_**转载到请包括本文地址：** [https://kexue.fm/archives/9708](https://kexue.fm/archives/9708)_

_**更详细的转载事宜请参考：**_ [《科学空间FAQ》](https://kexue.fm/archives/6508#%E6%96%87%E7%AB%A0%E5%A6%82%E4%BD%95%E8%BD%AC%E8%BD%BD/%E5%BC%95%E7%94%A8)

**如果您还有什么疑惑或建议，欢迎在下方评论区继续讨论。**

**如果您觉得本文还不错，欢迎 [分享](https://kexue.fm/archives/9708#share)/ [打赏](https://kexue.fm/archives/9708#pay) 本文。打赏并非要从中获得收益，而是希望知道科学空间获得了多少读者的真心关注。当然，如果你无视它，也不会影响你的阅读。再次表示欢迎和感谢！**

打赏

![科学空间](https://kexue.fm/usr/themes/geekg/payment/wx.png)

微信打赏

![科学空间](https://kexue.fm/usr/themes/geekg/payment/zfb.png)

支付宝打赏

因为网站后台对打赏并无记录，因此欢迎在打赏时候备注留言。

你还可以 [**点击这里**](http://mail.qq.com/cgi-bin/qm_share?t=qm_mailme&email=tN7d1drY3drrx8H0xcWa19vZ) 或在下方评论区留言来告知你的建议或需求。

**如果您需要引用本文，请参考：**

苏剑林. (Aug. 07, 2023). 《Transformer升级之路：12、无限外推的ReRoPE？ 》\[Blog post\]. Retrieved from [https://kexue.fm/archives/9708](https://kexue.fm/archives/9708)

@online{kexuefm-9708,

        title={Transformer升级之路：12、无限外推的ReRoPE？},

        author={苏剑林},

        year={2023},

        month={Aug},

        url={\\url{https://kexue.fm/archives/9708}},

}

分类： [信息时代](https://kexue.fm/category/Big-Data)    标签： [attention](https://kexue.fm/tag/attention/), [位置编码](https://kexue.fm/tag/%E4%BD%8D%E7%BD%AE%E7%BC%96%E7%A0%81/), [泛化](https://kexue.fm/tag/%E6%B3%9B%E5%8C%96/), [外推](https://kexue.fm/tag/%E5%A4%96%E6%8E%A8/), [rope](https://kexue.fm/tag/rope/)[45 评论](https://kexue.fm/archives/9708#comments)

< [Transformer升级之路：11、将β进制位置进行到底](https://kexue.fm/archives/9706) \| [Transformer升级之路：13、逆用Leaky ReRoPE](https://kexue.fm/archives/9728) >

### 你也许还对下面的内容感兴趣

- [Transformer升级之路：18、RoPE的底数选择原则](https://kexue.fm/archives/10122)
- [缓存与效果的极限拉扯：从MHA、MQA、GQA到MLA](https://kexue.fm/archives/10091)
- [Transformer升级之路：17、多模态位置编码的简单思考](https://kexue.fm/archives/10040)
- [时空之章：将Attention视为平方复杂度的RNN](https://kexue.fm/archives/10017)
- [“闭门造车”之多模态模型方案浅谈](https://kexue.fm/archives/9984)
- [Transformer升级之路：16、“复盘”长度外推技术](https://kexue.fm/archives/9948)
- [注意力机制真的可以“集中注意力”吗？](https://kexue.fm/archives/9889)
- [我在Performer中发现了Transformer-VQ的踪迹](https://kexue.fm/archives/9862)
- [Transformer升级之路：15、Key归一化助力长度外推](https://kexue.fm/archives/9859)
- [VQ一下Key，Transformer的复杂度就变成线性了](https://kexue.fm/archives/9844)

[发表你的看法](https://kexue.fm/archives/9708#comment_form)

1. [«](https://kexue.fm/archives/9708/comment-page-2#comments)
2. [1](https://kexue.fm/archives/9708/comment-page-1#comments)
3. [2](https://kexue.fm/archives/9708/comment-page-2#comments)
4. [3](https://kexue.fm/archives/9708/comment-page-3#comments)

[Transformer升级之路：15、Key归一化助力长度外推 R11; AI 資訊](https://news.aitime.space/2023/11/69309/)

November 20th, 2023

\[...\]大体上，我们可以将目前Transformer的长度外推技术分为两类：一类是事后修改，比如NTK-RoPE、YaRN、ReRoPE等，这类方法的特点是直接修改推理模型，无需微调就能达到一定的长度外推效果，但缺点是它们都无法保持模型在训练长度内的恒等性；另一类自然是事前修改，如ALIBI、KERPLE、XPOS以及HWFA等，它们可以不加改动地实现一定的长度外推，但相应的改动需要在训练之前就引入，因此\[...\]

[回复评论](https://kexue.fm/archives/9708/comment-page-3?replyTo=23103#respond-post-9708)

ltekills3g

January 3rd, 2024

挺好奇为什么ReRope会比NTK-RoPE-mixed还要好？看起来ReRope损失了很多>w时候相对位置的信息啊？

难道单纯是因为mixed分摊仍然很难合理化（距离越远训练越少），而ReRope这里可以充分训练所有的>w的远距离吗？

[回复评论](https://kexue.fm/archives/9708/comment-page-3?replyTo=23436#respond-post-9708)

[苏剑林](https://kexue.fm) 发表于
January 5th, 2024

因为ReRoPE局部保持得更好，说明局部也是相当重要的。

[回复评论](https://kexue.fm/archives/9708/comment-page-3?replyTo=23457#respond-post-9708)

1. [«](https://kexue.fm/archives/9708/comment-page-2#comments)
2. [1](https://kexue.fm/archives/9708/comment-page-1#comments)
3. [2](https://kexue.fm/archives/9708/comment-page-2#comments)
4. [3](https://kexue.fm/archives/9708/comment-page-3#comments)

[取消回复](https://kexue.fm/archives/9708#respond-post-9708)

你的大名

电子邮箱

个人网站（选填）

1\. 可以在评论中使用LaTeX代码，点击“预览效果”可即时查看效果，点击 [这里](https://kexue.fm/content.html) 可以查看更多内容；

2\. 可以通过点击评论楼层编号来引用该楼层；

3\. **提交评论之前，建议复制一下评论内容，避免提交失败导致辛苦打的字没了。**

### 内容速览

[重温](https://kexue.fm/archives/9708#%E9%87%8D%E6%B8%A9)
[融合](https://kexue.fm/archives/9708#%E8%9E%8D%E5%90%88)
[计算](https://kexue.fm/archives/9708#%E8%AE%A1%E7%AE%97)
[实验](https://kexue.fm/archives/9708#%E5%AE%9E%E9%AA%8C)
[小结](https://kexue.fm/archives/9708#%E5%B0%8F%E7%BB%93)

### 智能搜索

支持整句搜索！网站自动使用 [结巴分词](https://github.com/fxsjy/jieba) 进行分词，并结合ngrams排序算法给出合理的搜索结果。

### 热门标签

[生成模型](https://kexue.fm/tag/%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B/) [attention](https://kexue.fm/tag/attention/) [模型](https://kexue.fm/tag/%E6%A8%A1%E5%9E%8B/) [语言模型](https://kexue.fm/tag/%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/) [优化](https://kexue.fm/tag/%E4%BC%98%E5%8C%96/) [概率](https://kexue.fm/tag/%E6%A6%82%E7%8E%87/) [网站](https://kexue.fm/tag/%E7%BD%91%E7%AB%99/) [转载](https://kexue.fm/tag/%E8%BD%AC%E8%BD%BD/) [微分方程](https://kexue.fm/tag/%E5%BE%AE%E5%88%86%E6%96%B9%E7%A8%8B/) [天象](https://kexue.fm/tag/%E5%A4%A9%E8%B1%A1/) [深度学习](https://kexue.fm/tag/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/) [积分](https://kexue.fm/tag/%E7%A7%AF%E5%88%86/) [分析](https://kexue.fm/tag/%E5%88%86%E6%9E%90/) [python](https://kexue.fm/tag/python/) [梯度](https://kexue.fm/tag/%E6%A2%AF%E5%BA%A6/) [无监督](https://kexue.fm/tag/%E6%97%A0%E7%9B%91%E7%9D%A3/) [力学](https://kexue.fm/tag/%E5%8A%9B%E5%AD%A6/) [节日](https://kexue.fm/tag/%E8%8A%82%E6%97%A5/) [几何](https://kexue.fm/tag/%E5%87%A0%E4%BD%95/) [文本生成](https://kexue.fm/tag/%E6%96%87%E6%9C%AC%E7%94%9F%E6%88%90/) [数论](https://kexue.fm/tag/%E6%95%B0%E8%AE%BA/) [矩阵](https://kexue.fm/tag/%E7%9F%A9%E9%98%B5/) [GAN](https://kexue.fm/tag/GAN/) [生活](https://kexue.fm/tag/%E7%94%9F%E6%B4%BB/) [扩散](https://kexue.fm/tag/%E6%89%A9%E6%95%A3/)

### 随机文章

- [祝大家马年快乐！](https://kexue.fm/archives/2339)
- [\[更正\]一道经典不等式的美妙证明](https://kexue.fm/archives/1420)
- [《量子力学与路径积分》习题解答V0.2](https://kexue.fm/archives/3476)
- [《虚拟的实在(4)》——质量是什么](https://kexue.fm/archives/2036)
- [温馨\|生活一角](https://kexue.fm/archives/144)
- [生成扩散模型漫谈（十三）：从万有引力到扩散模型](https://kexue.fm/archives/9305)
- [强大的整数数列网站OEIS](https://kexue.fm/archives/2765)
- [开学啦！咱们来做完形填空～（讯飞杯）](https://kexue.fm/archives/4564)
- [关于交错级数的审敛法则](https://kexue.fm/archives/159)
- [互怼的艺术：从零直达WGAN-GP](https://kexue.fm/archives/4439)

### 最近评论

- [silingtong](https://kexue.fm/archives/10091/comment-page-2#comment-24560): 大神,请教一下，为什么HF的deepseek的apply\_rotary\_pos\_emb函数中，...
- [lxl-](https://kexue.fm/archives/9257/comment-page-3#comment-24559): 请教一个问题，一些加速模型（如 SDXL-turbo等）的 CFG 都非常小，反向关键词都不起...
- [Wen Fei](https://kexue.fm/archives/9859/comment-page-1#comment-24558): 您好，苏神，我有一点不明白。l2 norm对所有token和batch求平均？那推理过程呢，t...
- [Wen Fei](https://kexue.fm/archives/2036/comment-page-1#comment-24557): 现在能讲一下 质量的来源了嘛？ 在相对论里，质量和能量是一样的东西，应该是不同坐标系下的投影。...
- [bill](https://kexue.fm/archives/9826/comment-page-2#comment-24556): 有人使用VQGAN的训练方法对比过VQ和FSQ吗？这应该是FSQ论文的实现方式。我训练出来无论...
- [NirVa](https://kexue.fm/archives/9978/comment-page-1#comment-24555): 为啥不做利用cookie保存star？
- [lcz](https://kexue.fm/archives/3290/comment-page-2#comment-24554): 苏神，为什么“找一个在整个实数域上都单调递增的函数，而且增长速度要快于线性增长，然后求和，最后...
- [周名远](https://kexue.fm/archives/10085/comment-page-1#comment-24553): Kiro: 很高兴你通过实践验证了SiD的稳定性。SDXL的distill目前实验还没具体开展...
- [苏剑林](https://kexue.fm/archives/9984/comment-page-2#comment-24552): 刚刷到这篇paper，它是每个像素都视为一个token，这种做法远比我说的激进，而且它自己越承...
- [苏剑林](https://kexue.fm/archives/10114/comment-page-1#comment-24551): 谢谢，已更正。

### 友情链接

- [Cool Papers](https://papers.cool)
- [数学研发](https://bbs.emath.ac.cn)
- [Seatop](http://www.seatop.com.cn/)
- [Xiaoxia](https://xiaoxia.org/)
- [积分表-网络版](https://kexue.fm/sci/integral/index.html)
- [丝路博傲](http://blog.dvxj.com/)
- [ph4ntasy 饭特稀](http://www.ph4ntasy.com/)
- [数学之家](http://www.2math.cn/)
- [有趣天文奇观](http://interesting-sky.china-vo.org/)
- [bsky](https://bsky.spaces.ac.cn/)
- [TwistedW](http://www.twistedwg.com/)
- [godweiyang](https://godweiyang.com/)
- [AI柠檬](https://blog.ailemon.net/)
- [王登科-DK博客](https://greatdk.com)
- [ESON](https://blog.eson.org/)
- [枫之羽](https://fzhiy.net/)
- [Mathor's blog](https://wmathor.com/)
- [孙云增的博客](https://sunyunzeng.com/)
- [coding-zuo](https://coding-zuo.github.io/)
- [博科园](https://www.bokeyuan.net/)
- [孔皮皮的博客](https://www.kppkkp.top/)
- [运鹏的博客](https://yunpengtai.top/)
- [jiming.site](https://jiming.site/)
- [OmegaXYZ](https://www.omegaxyz.com/)
- [Blog by Eacls](https://www.eacls.top/)
- [申请链接](https://kexue.fm/links.html)

[![署名-非商业用途-保持一致](https://kexue.fm/usr/themes/geekg/images/cc.gif)](http://creativecommons.org/licenses/by-nc-nd/2.5/cn/) 本站采用创作共用版权协议，要求署名、非商业用途和保持一致。转载本站内容必须也遵循“ [署名-非商业用途-保持一致](http://creativecommons.org/licenses/by-nc-nd/2.5/cn/)”的创作共用协议。

© 2009-2024 Scientific Spaces. All rights reserved. Theme by [laogui](http://www.laogui.com). Powered by [Typecho](http://typecho.org). 备案号: [粤ICP备09093259号-1/2](https://beian.miit.gov.cn/)。