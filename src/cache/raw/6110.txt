Processing math: 0%

![MobileSideBar](https://kexue.fm/usr/themes/geekg/images/slide-button.png)

## SEARCH

## MENU

- [打赏](https://kexue.fm/reward.html)
- [公式](https://kexue.fm/latex.html)
- [天象](https://kexue.fm/ac.html)
- [链接](https://kexue.fm/links.html)
- [时光](https://kexue.fm/me.html)
- [博览](https://kexue.fm/science.html)
- [归档](https://kexue.fm/content.html)

## CATEGORIES

- [千奇百怪](https://kexue.fm/category/Everything)
- [天文探索](https://kexue.fm/category/Astronomy)
- [数学研究](https://kexue.fm/category/Mathematics)
- [物理化学](https://kexue.fm/category/Phy-chem)
- [信息时代](https://kexue.fm/category/Big-Data)
- [生物自然](https://kexue.fm/category/Biology)
- [图片摄影](https://kexue.fm/category/Photograph)
- [问题百科](https://kexue.fm/category/Questions)
- [生活/情感](https://kexue.fm/category/Life-Feeling)
- [资源共享](https://kexue.fm/category/Resources)

## NEWPOSTS

- [让炼丹更科学一些（五）：基于梯度精...](https://kexue.fm/archives/11530)
- [让炼丹更科学一些（四）：新恒等式，...](https://kexue.fm/archives/11494)
- [为什么DeltaNet要加L2 N...](https://kexue.fm/archives/11486)
- [让炼丹更科学一些（三）：SGD的终...](https://kexue.fm/archives/11480)
- [让炼丹更科学一些（二）：将结论推广...](https://kexue.fm/archives/11469)
- [滑动平均视角下的权重衰减和学习率](https://kexue.fm/archives/11459)
- [生成扩散模型漫谈（三十一）：预测数...](https://kexue.fm/archives/11428)
- [Muon优化器指南：快速上手与关键细节](https://kexue.fm/archives/11416)
- [AdamW的Weight RMS的...](https://kexue.fm/archives/11404)
- [n个正态随机数的最大值的渐近估计](https://kexue.fm/archives/11390)

## COMMENTS

- [Bin: 今天偶然从某个论坛看到有人推荐您的博客，定睛一看竟然是华师同院...](https://kexue.fm/archives/1990/comment-page-2#comment-29105)
- [Rapture D: 我有一个问题，为什么不考虑亥姆霍兹定理和斯托克斯公式。](https://kexue.fm/archives/11530/comment-page-1#comment-29104)
- [mofheka: 苏神是还在用jax是么？最近在做基于Google Pathwa...](https://kexue.fm/archives/11390/comment-page-1#comment-29103)
- [长琴: 看懂这篇博客也不是一件容易的事情。](https://kexue.fm/archives/11530/comment-page-1#comment-29102)
- [AlexLi: 苏老师，请教一下(7)式中将 \\mu(x\_t) 传给 $p...](https://kexue.fm/archives/9257/comment-page-4#comment-29101)
- [tyler\_zxc: "Performer的思想是将标准的Attention线性化，...](https://kexue.fm/archives/7921/comment-page-2#comment-29100)
- [我: 似乎并非mHC提出矩阵的思想？之前hyper connecti...](https://kexue.fm/archives/11494/comment-page-1#comment-29099)
- [winter: 苏神您好，假如对于比较均匀的attention weightP...](https://kexue.fm/archives/10847/comment-page-1#comment-29098)
- [苏剑林: KL散度、JS散度、W距离啥的，都行啊，看你喜欢哪个](https://kexue.fm/archives/8512/comment-page-2#comment-29097)
- [苏剑林: 没有绝对公平的对比方法，主要看你关心什么。比如，如果只关心推理...](https://kexue.fm/archives/9119/comment-page-14#comment-29096)

## USERLOGIN

- [登录](https://kexue.fm/admin/login.php)

[科学空间\|Scientific Spaces](https://kexue.fm/)

- [登录](https://kexue.fm/admin/login.php)
- [打赏](https://kexue.fm/reward.html)
- [公式](https://kexue.fm/latex.html)
- [天象](https://kexue.fm/ac.html)
- [链接](https://kexue.fm/links.html)
- [时光](https://kexue.fm/me.html)
- [博览](https://kexue.fm/science.html)
- [归档](https://kexue.fm/content.html)

渴望成为一个小飞侠

- [![](https://kexue.fm/usr/themes/geekg/images/rss.png)\\
\\
欢迎订阅](https://kexue.fm/feed)
- [![](https://kexue.fm/usr/themes/geekg/images/mail.png)\\
\\
个性邮箱](https://kexue.fm/archives/119)
- [![](https://kexue.fm/usr/themes/geekg/images/Saturn.png)\\
\\
天象信息](https://kexue.fm/ac.html)
- [![](https://kexue.fm/usr/themes/geekg/images/iss.png)\\
\\
观测ISS](https://kexue.fm/archives/41)
- [![](https://kexue.fm/usr/themes/geekg/images/pi.png)\\
\\
LaTeX](https://kexue.fm/latex.html)
- [![](https://kexue.fm/usr/themes/geekg/images/mlogo.png)\\
\\
关于博主](https://kexue.fm/me.html)

欢迎访问“科学空间”，这里将与您共同探讨自然科学，回味人生百态；也期待大家的分享～

- [**千奇百怪** Everything](https://kexue.fm/category/Everything)
- [**天文探索** Astronomy](https://kexue.fm/category/Astronomy)
- [**数学研究** Mathematics](https://kexue.fm/category/Mathematics)
- [**物理化学** Phy-chem](https://kexue.fm/category/Phy-chem)
- [**信息时代** Big-Data](https://kexue.fm/category/Big-Data)
- [**生物自然** Biology](https://kexue.fm/category/Biology)
- [**图片摄影** Photograph](https://kexue.fm/category/Photograph)
- [**问题百科** Questions](https://kexue.fm/category/Questions)
- [**生活/情感** Life-Feeling](https://kexue.fm/category/Life-Feeling)
- [**资源共享** Resources](https://kexue.fm/category/Resources)

- [**千奇百怪**](https://kexue.fm/category/Everything)
- [**天文探索**](https://kexue.fm/category/Astronomy)
- [**数学研究**](https://kexue.fm/category/Mathematics)
- [**物理化学**](https://kexue.fm/category/Phy-chem)
- [**信息时代**](https://kexue.fm/category/Big-Data)
- [**生物自然**](https://kexue.fm/category/Biology)
- [**图片摄影**](https://kexue.fm/category/Photograph)
- [**问题百科**](https://kexue.fm/category/Questions)
- [**生活/情感**](https://kexue.fm/category/Life-Feeling)
- [**资源共享**](https://kexue.fm/category/Resources)

[首页](https://kexue.fm/) [信息时代](https://kexue.fm/category/Big-Data) RSGAN：对抗模型中的“图灵测试”思想

22Oct

# [RSGAN：对抗模型中的“图灵测试”思想](https://kexue.fm/archives/6110)

By 苏剑林 \|
2018-10-22 \|
166076位读者 \|

这两天无意间发现一个非常有意义的工作，称为“相对GAN”，简称RSGAN，来自文章 [《The relativistic discriminator: a key element missing from standard GAN》](https://papers.cool/arxiv/1807.00734)，据说该文章还得到了GAN创始人Goodfellow的点赞。这篇文章提出了用相对的判别器来取代标准GAN原有的判别器，使得生成器的收敛更为迅速，训练更为稳定。

可惜的是，这篇文章仅仅从训练和实验角度对结果进行了论述，并没有进行更深入的分析，以至于不少人觉得这只是GAN训练的一个trick。但是在笔者来看，RSGAN具有更为深刻的含义，甚至可以看成它已经开创了一个新的GAN流派。所以，笔者决定对RSGAN模型及其背后的内涵做一个基本的介绍。不过需要指出的是，除了结果一样之外，本文的介绍过程跟原论文相比几乎没有重合之处。

## “图灵测试”思想 [\#](https://kexue.fm/archives/6110\#%E2%80%9C%E5%9B%BE%E7%81%B5%E6%B5%8B%E8%AF%95%E2%80%9D%E6%80%9D%E6%83%B3)

### SGAN [\#](https://kexue.fm/archives/6110\#SGAN)

SGAN就是标准的GAN（Standard GAN）。就算没有做过GAN研究的读者，相信也从各种渠道了解到GAN的大概原理：“造假者”不断地进行造假，试图愚弄“鉴别者”；“鉴别者”不断提高鉴别技术，以分辨出真品和赝品。两者相互竞争，共同进步，直到“鉴别者”无法分辨出真、赝品了，“造假者”就功成身退了。

在建模时，通过交替训练实现这个过程：固定生成器，训练一个判别器（二分类模型），将真实样本输出1，将伪造样本输出0；然后固定判别器，训练生成器让伪造样本尽可能输出1，后面这一步不需要真实样本参与。

### 问题所在 [\#](https://kexue.fm/archives/6110\#%E9%97%AE%E9%A2%98%E6%89%80%E5%9C%A8)

然而，这个建模过程似乎对判别器的要求过于苛刻了，因为判别器是孤立运作的：训练生成器时，真实样本没有参与，所以判别器必须把关于真实样本的所有属性记住，这样才能指导生成器生成更真实的样本。

在生活实际中，我们并不是这样做的，所谓“没有对比就没有伤害，没有伤害就没有进步”，我们很多时候是根据真、赝品的对比来分辨的。比如识别一张假币，可能需要把它跟一张真币对比一下；识别山寨手机，只需要将它跟正版手机对比一下就行了；等等。类似地，如果要想把赝品造得更真，那么需要把真品放在一旁不断地进行对比改进，而不是单单凭借“记忆”中的真品来改进。

“对比”能让我们更容易识别出真、赝品出来，从而更好地制造赝品。而在人工智能领域，我们知道有非常著名的“图灵测试”，指的是测试者在无法预知的情况下同时跟机器人和人进行交流，如果测试者无法成功分别出人和机器人，那么说明这个机器人已经（在某个方面）具有人的智能了。“图灵测试”也强调了对比的重要性，如果机器人和人混合起来后就无法分辨了，那么说明机器人已经成功了。

接下来我们将会看到，RSGAN就是基于“图灵测试”的思想的：如果鉴别器无法鉴别出混合的真假图片，那么生成器就成功了；而为了生成更好的图片，生成器也需要直接借助于真实图片。

## RSGAN基本框架 [\#](https://kexue.fm/archives/6110\#RSGAN%E5%9F%BA%E6%9C%AC%E6%A1%86%E6%9E%B6)

### SGAN分析 [\#](https://kexue.fm/archives/6110\#SGAN%E5%88%86%E6%9E%90)

首先，我们来回顾一下标准的GAN的流程。设真实样本分布为\\tilde{p}(x)，伪造样本分布为q(x)，那么固定生成器后，我们来优化判别器T(x)：

\\begin{equation}\\min\_{T}-\\mathbb{E}\_{x\\sim \\tilde{p}(x)}\[\\log \\sigma(T(x))\] - \\mathbb{E}\_{x\\sim q(x)}\[\\log(1-\\sigma(T(x)))\]\\label{eq:sgan-d}\\end{equation}

这里的\\sigma就是sigmoid激活函数。然后固定判别器，我们优化生成器G(z)：

\\begin{equation}\\min\_{G}\\mathbb{E}\_{x=G(z),z\\sim q(z)}\[h(T(x))\]\\label{eq:sgan-g}\\end{equation}

注意这里我们有个不确定h，我们马上就来分析它。

从\\eqref{eq:sgan-d}我们可以解出判别器的最优解满足（后面有补充证明）

\\begin{equation}\\frac{\\tilde{p}(x)}{q(x)}=\\frac{\\sigma(T(x))}{1 - \\sigma(T(x))} = e^{T(x)}\\end{equation}

代入\\eqref{eq:sgan-g}，可以发现结果为

\\begin{equation}\\min\_{G}\\mathbb{E}\_{x=G(z),z\\sim q(z)}\\left\[h\\left(\\log\\frac{\\tilde{p}(x)}{q(x)}\\right)\\right\]=\\min\_{G}\\int q(x)\\left\[h\\left(\\log\\frac{\\tilde{p}(x)}{q(x)}\\right)\\right\]dx
\\end{equation}

写成最后一个等式，是因为只需要设f(t)=h(\\log(t))，就能够看出它具有f散度的形式。也就是说，最小化\\eqref{eq:sgan-g}就是在最小化对应的f散度。关于f散度，可以参数我之前写的 [《f-GAN简介：GAN模型的生产车间》](https://kexue.fm/archives/6016)。f散度中的f的本质要求是f是一个凸函数，所以只需要选择h使得h(\\log(t))为凸函数就行。最简单的情况是h(t)=-t，对应h(\\log(t))=-\\log t为凸函数，这时候\\eqref{eq:sgan-g}为

\\begin{equation}\\min\_{G}\\mathbb{E}\_{x=G(z),z\\sim q(z)}\[-T(x)\]\\end{equation}

类似的选择有很多，比如当h(t)=-\\log \\sigma(t)时，h(\\log(t))=\\log \\left(1+\\frac{1}{t}\\right)也是凸函数（t > 0时），所以

\\begin{equation}\\min\_{G}\\mathbb{E}\_{x=G(z),z\\sim q(z)}\[-\\log\\sigma(T(x))\]\\end{equation}

也是一个合理的选择，它便是GAN常用的生成器loss之一。类似地还有h(t)=\\log(1-\\sigma(t))，这些选择就不枚举了。

### RSGAN目标 [\#](https://kexue.fm/archives/6110\#RSGAN%E7%9B%AE%E6%A0%87)

这里，我们先直接给出RSGAN的优化目标：固定生成器后，我们来优化判别器T(x)：

\\begin{equation}\\min\_{T}-\\mathbb{E}\_{x\_r\\sim \\tilde{p}(x), x\_f\\sim q(x)}\[\\log \\sigma(T(x\_r)-T(x\_f))\]\\label{eq:rsgan-d}\\end{equation}

这里的\\sigma就是sigmoid激活函数。然后固定判别器，我们优化生成器G(z)：

\\begin{equation}\\min\_{G}\\mathbb{E}\_{x\_r\\sim \\tilde{p}(x), x\_f=G(z),z\\sim q(z)}\[h(T(x\_f) - T(x\_r))\]\\label{eq:rsgan-g}\\end{equation}

跟SGAN一样，我们这里保留了一般的h，h的要求跟前面的SGAN的讨论一致。而RSGAN原论文的选择是

\\begin{equation}\\min\_{G}-\\mathbb{E}\_{x\_r\\sim \\tilde{p}(x), x\_f=G(z),z\\sim q(z)}\[\\log\\sigma(T(x\_f) - T(x\_r))\]\\end{equation}

看上去就是把 **SGAN的判别器的两项换成一个相对判别器** 了，相关的分析结果有什么变化呢？

### 理论结果 [\#](https://kexue.fm/archives/6110\#%E7%90%86%E8%AE%BA%E7%BB%93%E6%9E%9C)

通过变分法（后面有补充证明）可以得到，\\eqref{eq:rsgan-d}的最优解为

\\begin{equation}\\frac{\\tilde{p}(x\_r)q(x\_f)}{\\tilde{p}(x\_f)q(x\_r)}=\\frac{\\sigma(T(x\_r)-T(x\_f))}{\\sigma(T(x\_f)-T(x\_r))}=e^{T(x\_r)-T(x\_f)}\\end{equation}

代入到\\eqref{eq:rsgan-g}，结果是

\\begin{equation}\\begin{aligned}&\\min\_{G}\\mathbb{E}\_{x\_r\\sim \\tilde{p}(x), x\_f=G(z),z\\sim q(z)}\\left\[h\\left(\\log\\frac{\\tilde{p}(x\_f)q(x\_r)}{\\tilde{p}(x\_r)q(x\_f)}\\right)\\right\]\\\
=&\\min\_{G}\\iint \\tilde{p}(x\_r)q(x\_f)\\left\[h\\left(\\log\\frac{\\tilde{p}(x\_f)q(x\_r)}{\\tilde{p}(x\_r)q(x\_f)}\\right)\\right\] dx\_r dx\_f\\end{aligned}\\label{eq:rsgan-gg}\\end{equation}

这个结果便是整个RSGAN的精华所在了，它 **优化的是\\tilde{p}(x\_r)q(x\_f)与\\tilde{p}(x\_f)q(x\_r)的f散度**！

这是什么意思呢？它就是说，假如我从真实样本采样一个x\_r出来，从伪造样本采样一个x\_f出来，然后将它们交换一下，把假的当成真，真的当成假，那么还能分辨出来吗？换言之：\\tilde{p}(x\_f)q(x\_r)有大变化吗？

假如没有什么变化，那就说明真假样本已经无法分辨了，训练成功，假如还能分辨出来，说明还需要借助真实样本来改善伪造样本。所以，式\\eqref{eq:rsgan-gg}就是RSGAN中的“图灵测试”思想的体现：打乱了数据，是否还能分辨出来？

### 模型效果分析 [\#](https://kexue.fm/archives/6110\#%E6%A8%A1%E5%9E%8B%E6%95%88%E6%9E%9C%E5%88%86%E6%9E%90)

作者在原论文中还提出了一个RaSGAN，a是average的意思，就是用整个batch的平均来代替单一的真/伪样本。但我觉得这不是一个特别优雅的做法，而且论文也表明RaSGAN的效果并非总是比RSGAN要好，所以这就不介绍了，有兴趣的读者看看原论文即可。

至于效果，论文中的效果列表显示，RSGAN在不少任务上都提升了模型的生成质量，但这并非总是这样，平均而言有轻微的提升吧。作者特别指出的是RSGAN能够加快生成器的训练速度，我个人也实验了一下，比SGAN、SNGAN都要快一些。

我的参考代码：

[https://github.com/bojone/gan/blob/master/keras/rsgan\_sn\_celeba.py](https://github.com/bojone/gan/blob/master/keras/rsgan_sn_celeba.py)

借用 [MingtaoGuo](https://github.com/MingtaoGuo/DCGAN_WGAN_WGAN-GP_LSGAN_SNGAN_RSGAN_RaSGAN_TensorFlow) 的一张图来对比RSGAN的收敛速度：

[![RSGAN收敛速度对比](https://kexue.fm/usr/uploads/2018/10/2687059029.jpg)](https://kexue.fm/usr/uploads/2018/10/2687059029.jpg "点击查看原图")

RSGAN收敛速度对比

从直观来看，RSGAN更快是因为在训练生成器时也借用了真实样本的信息，而不仅仅通过判别器的“记忆”；从理论上看，通过T(x\_r)、T(x\_f)作差的方式，使得判别器只依赖于它们的相对值，从而简单地改善了判别器T可能存在的偏置情况，使得梯度更加稳定。甚至我觉得，把真实样本也引入到生成器的训练中，有可能（没仔细证明）提升伪造样本的多样性，因为有了各种真实样本来对比，模型如果只生成单一样本，也很难满足判别器的对比判别标准。

## 相关话题讨论 [\#](https://kexue.fm/archives/6110\#%E7%9B%B8%E5%85%B3%E8%AF%9D%E9%A2%98%E8%AE%A8%E8%AE%BA)

### 简单总结 [\#](https://kexue.fm/archives/6110\#%E7%AE%80%E5%8D%95%E6%80%BB%E7%BB%93)

总的来说，我觉得RSGAN是对GAN的改进是从思想上做了改变的，也许RSGAN的作者也没有留意到这一点。

我们经常说，WGAN是GAN之后的一大突破，这没错，但这个突破是理论上的，而在思想上还是一样，都是在减少两个分布的距离，只不过以前用JS散度可能有各种问题，而WGAN换用了Wasserstein距离。我觉得RSGAN更像是一种思想上的突破——转化为真假样本混淆之后的分辨——尽管效果未必有大的进步。（当然你要是说大家最终的效果都是拉近了分布距离，那我也没话说^\_^）

RSGAN的一些提升是容易重现的，当然由于不是各种任务都有提升，所以也有人诟病这不过是GAN训练的一个trick。这些评论见仁见智吧，不妨碍我对这篇论文的赞赏和研究。

对了，顺便说一下，作者 [Alexia Jolicoeur-Martineau](https://scholar.google.com/citations?user=0qytQ1oAAAAJ&hl=en) 是犹太人总医院（Jewish General Hospital）的一名女生物统计学家，论文中的结果是她只用一颗1060跑出来的（ [出处在这里](https://www.reddit.com/r/MachineLearning/comments/8vr9am/r_the_relativistic_discriminator_a_key_element/e1ru76p)）。我突然也为我只有一颗1060感到自豪了...（然而我有1060但我并没有paper～）

### 延伸讨论 [\#](https://kexue.fm/archives/6110\#%E5%BB%B6%E4%BC%B8%E8%AE%A8%E8%AE%BA)

最后胡扯一些延伸的话题。

首先，可以留意到，WGAN的判别器loss本身就是两项的差的形式，也就是说WGAN的判别器就是一个相对判别器，作者认为这是WGAN效果好的重要原因。

这样看上去WGAN跟RSGAN本身就有一些交集，但我有个更进一步的想法，就是基于\\tilde{p}(x\_r)q(x\_f)与\\tilde{p}(x\_f)q(x\_r)的比较能否完全换用Wasserstein距离来进行？我们知道WGAN的生成器训练目标也是跟真实样本没关系的，怎么更好地将真实样本的信息引入到WGAN的生成器中去？

还有一个问题，就是目前作差仅仅是判别器最后输出的标量作差，那么能不能是判别器的某个隐藏层作差，然后算个mse或者再接几层神经网络？。总之，我觉得这个模型的事情应该还没完...

### 补充证明 [\#](https://kexue.fm/archives/6110\#%E8%A1%A5%E5%85%85%E8%AF%81%E6%98%8E)

> **1、\\eqref{eq:sgan-d}的最优解**
>
> \\begin{equation}\\begin{aligned}&-\\mathbb{E}\_{x\\sim \\tilde{p}(x)}\[\\log \\sigma(T(x))\] - \\mathbb{E}\_{x\\sim q(x)}\[\\log(1-\\sigma(T(x)))\]\\\
> =&-\\int \\Big(\\tilde{p}(x) \\log \\sigma(T(x)) + q(x) \\log(1-\\sigma(T(x))) \\Big)dx\\end{aligned}\\end{equation}
>
> 变分用\\delta表示，跟微分基本一样：
>
> \\begin{equation}\\begin{aligned}&\\delta \\int \\Big(\\tilde{p}(x) \\log \\sigma(T(x)) + q(x) \\log(1-\\sigma(T(x))) \\Big)dx\\\
> =& \\int \\left(\\tilde{p}(x) \\frac{\\delta \\sigma(T(x))}{\\sigma(T(x))} + q(x) \\frac{-\\delta \\sigma(T(x))}{1-\\sigma(T(x))} \\right)dx\\\
> =& \\int \\left(\\tilde{p}(x) \\frac{1}{\\sigma(T(x))} - q(x) \\frac{1}{1-\\sigma(T(x))} \\right)\\delta \\sigma(T(x)) dx
> \\end{aligned}\\end{equation}
>
> 极值在变分为0时取到，而\\delta \\sigma(T(x))代表任意增量，所以如果上式恒为0，意味着括号内的部分恒为0，即
>
> \\begin{equation}\\tilde{p}(x) \\frac{1}{\\sigma(T(x))} = q(x) \\frac{1}{1-\\sigma(T(x))}\\end{equation}
>
> **2、\\eqref{eq:rsgan-d}的最优解**
>
> \\begin{equation}\\begin{aligned}&-\\mathbb{E}\_{x\_r\\sim \\tilde{p}(x), x\_f\\sim q(x)}\[\\log \\sigma(T(x\_r)-T(x\_f))\]\\\
> =&-\\iint \\tilde{p}(x\_r)q(x\_f)\\log \\sigma(T(x\_r)-T(x\_f)) dx\_r dx\_f\\end{aligned}\\end{equation}
>
> 变分上式：
>
> \\begin{equation}\\begin{aligned}&\\delta \\iint \\tilde{p}(x\_r)q(x\_f)\\log \\sigma(T(x\_r)-T(x\_f)) dx\_r dx\_f\\\
> =& \\iint \\tilde{p}(x\_r)q(x\_f)\\frac{\\delta \\sigma(T(x\_r)-T(x\_f))}{\\sigma(T(x\_r)-T(x\_f))} dx\_r dx\_f\\quad\[\\text{接下来利用}\\sigma'(x)=\\sigma(x)\\sigma(-x)\]\\\
> =& \\iint \\tilde{p}(x\_r)q(x\_f)\\sigma(T(x\_f)-T(x\_r)) \\times (\\delta T(x\_r)-\\delta T(x\_f)) dx\_r dx\_f\\\
> =& \\iint \\tilde{p}(x\_r)q(x\_f)\\sigma(T(x\_f)-T(x\_r)) \\delta T(x\_r) dx\_r dx\_f \\quad\[\\text{接下来交换第二项的}x\_r,x\_f\]\\\
> &\\qquad - \\iint \\tilde{p}(x\_r)q(x\_f)\\sigma(T(x\_f)-T(x\_r)) \\delta T(x\_f) dx\_r dx\_f\\\
> =& \\iint \\tilde{p}(x\_r)q(x\_f)\\sigma(T(x\_f)-T(x\_r)) \\delta T(x\_r) dx\_r dx\_f \\\
> &\\qquad - \\iint \\tilde{p}(x\_f)q(x\_r)\\sigma(T(x\_r)-T(x\_f)) \\delta T(x\_r) dx\_f dx\_r\\\
> =& \\iint \\Big\[\\tilde{p}(x\_r)q(x\_f)\\sigma(T(x\_f)-T(x\_r)) \\\\
> &\\qquad\\qquad- \\tilde{p}(x\_f)q(x\_r)\\sigma(T(x\_r)-T(x\_f))\\Big\] \\delta T(x\_r) dx\_r dx\_f
> \\end{aligned}\\end{equation}
>
> 极值在变分为0时取到，所以方括号内的部分恒为0，即
>
> \\begin{equation}\\tilde{p}(x\_r)q(x\_f)\\sigma(T(x\_f)-T(x\_r))=\\tilde{p}(x\_f)q(x\_r)\\sigma(T(x\_r)-T(x\_f))\\end{equation}

_**转载到请包括本文地址：** [https://kexue.fm/archives/6110](https://kexue.fm/archives/6110 "RSGAN：对抗模型中的“图灵测试”思想")_

_**更详细的转载事宜请参考：**_ [《科学空间FAQ》](https://kexue.fm/archives/6508#%E6%96%87%E7%AB%A0%E5%A6%82%E4%BD%95%E8%BD%AC%E8%BD%BD/%E5%BC%95%E7%94%A8 "《科学空间FAQ》")

**如果您还有什么疑惑或建议，欢迎在下方评论区继续讨论。**

**如果您觉得本文还不错，欢迎 [分享](https://kexue.fm/archives/6110#share)/ [打赏](https://kexue.fm/archives/6110#pay) 本文。打赏并非要从中获得收益，而是希望知道科学空间获得了多少读者的真心关注。当然，如果你无视它，也不会影响你的阅读。再次表示欢迎和感谢！**

打赏

![科学空间](https://kexue.fm/usr/themes/geekg/payment/wx.png)

微信打赏

![科学空间](https://kexue.fm/usr/themes/geekg/payment/zfb.png)

支付宝打赏

因为网站后台对打赏并无记录，因此欢迎在打赏时候备注留言。

你还可以 [**点击这里**](http://mail.qq.com/cgi-bin/qm_share?t=qm_mailme&email=tN7d1drY3drrx8H0xcWa19vZ) 或在下方评论区留言来告知你的建议或需求。

**如果您需要引用本文，请参考：**

苏剑林. (Oct. 22, 2018). 《RSGAN：对抗模型中的“图灵测试”思想 》\[Blog post\]. Retrieved from [https://kexue.fm/archives/6110](https://kexue.fm/archives/6110)

@online{kexuefm-6110,

         title={RSGAN：对抗模型中的“图灵测试”思想},

         author={苏剑林},

         year={2018},

         month={Oct},

         url={\\url{https://kexue.fm/archives/6110}},

}


分类： [信息时代](https://kexue.fm/category/Big-Data)    标签： [概率](https://kexue.fm/tag/%E6%A6%82%E7%8E%87/), [无监督](https://kexue.fm/tag/%E6%97%A0%E7%9B%91%E7%9D%A3/), [GAN](https://kexue.fm/tag/GAN/), [生成模型](https://kexue.fm/tag/%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B/)[39 评论](https://kexue.fm/archives/6110#comments)

< [再谈非方阵的行列式](https://kexue.fm/archives/6096 "再谈非方阵的行列式") \| [缅怀金庸 \| 愿你登上10930小行星继续翱翔](https://kexue.fm/archives/6131 "缅怀金庸 | 愿你登上10930小行星继续翱翔") >

### 你也许还对下面的内容感兴趣

- [生成扩散模型漫谈（三十一）：预测数据而非噪声](https://kexue.fm/archives/11428 "生成扩散模型漫谈（三十一）：预测数据而非噪声")
- [n个正态随机数的最大值的渐近估计](https://kexue.fm/archives/11390 "n个正态随机数的最大值的渐近估计")
- [DiVeQ：一种非常简洁的VQ训练方案](https://kexue.fm/archives/11328 "DiVeQ：一种非常简洁的VQ训练方案")
- [为什么线性注意力要加Short Conv？](https://kexue.fm/archives/11320 "为什么线性注意力要加Short Conv？")
- [Transformer升级之路：21、MLA好在哪里?（下）](https://kexue.fm/archives/11111 "Transformer升级之路：21、MLA好在哪里?（下）")
- [线性注意力简史：从模仿、创新到反哺](https://kexue.fm/archives/11033 "线性注意力简史：从模仿、创新到反哺")
- [生成扩散模型漫谈（三十）：从瞬时速度到平均速度](https://kexue.fm/archives/10958 "生成扩散模型漫谈（三十）：从瞬时速度到平均速度")
- [Transformer升级之路：20、MLA好在哪里?（上）](https://kexue.fm/archives/10907 "Transformer升级之路：20、MLA好在哪里?（上）")
- [一道概率不等式：盯着它到显然成立为止！](https://kexue.fm/archives/10902 "一道概率不等式：盯着它到显然成立为止！")
- [生成扩散模型漫谈（二十九）：用DDPM来离散编码](https://kexue.fm/archives/10711 "生成扩散模型漫谈（二十九）：用DDPM来离散编码")

[发表你的看法](https://kexue.fm/archives/6110#comment_form)

1. [«](https://kexue.fm/archives/6110/comment-page-1#comments)
2. [1](https://kexue.fm/archives/6110/comment-page-1#comments)
3. [2](https://kexue.fm/archives/6110/comment-page-2#comments)

mcmc

November 22nd, 2018

这个似乎可以向半监督多实例学习的方向展开。

[回复评论](https://kexue.fm/archives/6110/comment-page-2?replyTo=10179#respond-post-6110)

[Hotel](http://hotelll.github.io/) 发表于
December 2nd, 2021

有点对比学习的感觉

[回复评论](https://kexue.fm/archives/6110/comment-page-2?replyTo=17945#respond-post-6110)

Duan Yu

November 29th, 2018

生成器如果是相对的话，真实样本xr跟G无关，那公式8、9里面的T(xr)这部分是不会对G产生梯度的呀？

[回复评论](https://kexue.fm/archives/6110/comment-page-2?replyTo=10221#respond-post-6110)

[苏剑林](https://kexue.fm/) 发表于
November 29th, 2018

\\log\\sigma(T(x\_f) - T(x\_r))的导数跟x\_r无关？你再算算？

[回复评论](https://kexue.fm/archives/6110/comment-page-2?replyTo=10226#respond-post-6110)

roger

December 29th, 2018

看了您的文章我对RSGAN了解的更透彻了。之前看到有人复现了该论文的代码，似乎和您说的有些出入，因此想向您请教一个问题。https://blog.csdn.net/Geoffrey\_MT/article/details/81198504中RSGAN的loss直接将D（x）等同于T（x）了，但是按我的理解文中的判别器D（xr）=sigmoid\[T（xr）-T（xf）\],不知道您怎么看？

[回复评论](https://kexue.fm/archives/6110/comment-page-2?replyTo=10458#respond-post-6110)

[苏剑林](https://kexue.fm/) 发表于
December 31st, 2018

没明白你的意思，这个链接中不就相当于你说的“D（xr）=sigmoid\[T（xr）-T（xf）\]”了吗？

[回复评论](https://kexue.fm/archives/6110/comment-page-2?replyTo=10464#respond-post-6110)

ghoshaw

March 14th, 2019

生成器里加入真实样本的信息的方法：现在训练一般都有一个L1 loss判断生成样本和GT的差距，这个对上面的理论分析有什么影响呢？

[回复评论](https://kexue.fm/archives/6110/comment-page-2?replyTo=10811#respond-post-6110)

小杰

June 11th, 2019

这确实是一个新的流派啊，哈哈

[回复评论](https://kexue.fm/archives/6110/comment-page-2?replyTo=11351#respond-post-6110)

纯洁的小火车

February 13th, 2020

站在WGAN的角度

我的理解是WGAN寻找一个(P(X),Q(Y))的联合分布表述样本分布的差异，然后优化。而RSGAN直接用了P(X)\*Q(Y)作为联合分布函数，但是RSGAN自己不考虑这些层面，他后续的优化，就是让站在WGAN角度看的P(X)\*Q(Y)联合分布形成的一个矩阵的主对角线两边越来越对称，那么就意味着两个分布差异越来越小了。

[回复评论](https://kexue.fm/archives/6110/comment-page-2?replyTo=12858#respond-post-6110)

[苏剑林](https://kexue.fm/) 发表于
February 13th, 2020

不错的视角～

[回复评论](https://kexue.fm/archives/6110/comment-page-2?replyTo=12868#respond-post-6110)

李梓强

April 25th, 2020

现在确实有很多中间层正则化的方法，通过比较生成图片和真实图片的语义特征来限制生成器的空间大小

[回复评论](https://kexue.fm/archives/6110/comment-page-2?replyTo=13219#respond-post-6110)

[zhb](https://github.com/zhb2000)

February 20th, 2024

试了一下这个“相对 GAN”的目标函数，似乎很容易出现严重的模式坍塌，坍塌得比标准 GAN 还要严重。Github 仓库的 issue 里也有人提到这个问题。

[回复评论](https://kexue.fm/archives/6110/comment-page-2?replyTo=23711#respond-post-6110)

[苏剑林](https://kexue.fm/) 发表于
February 21st, 2024

当时我测试的时候，应该没什么大问题，但也没什么改进就是了。

[回复评论](https://kexue.fm/archives/6110/comment-page-2?replyTo=23735#respond-post-6110)

swwww

January 11th, 2025

《he GAN is dead; long live the GAN!A Modern Baseline GAN》https://arxiv.org/pdf/2501.05441 最新的这篇R3GAN就是在本文的RSGAN的基础上增加了一些正则化和工程上的改进，取得了比DDPM更好的效果，不知道苏神有没有兴趣分析一下

[回复评论](https://kexue.fm/archives/6110/comment-page-2?replyTo=26227#respond-post-6110)

[苏剑林](https://kexue.fm/) 发表于
January 13th, 2025

这篇文章就理论来说没有太多新的东西，主要是结合近年来DDPM的经验，挑出了GAN的一个比较稳定的排列组合。

从我个人的角度看，GAN有可能成为优秀的生成模型，但很难成为未来多模态LLM预训练的选择。因为min-max这种训练模式有着比较明显的“起点-终点”，而预训练讲求的是终生学习。

[回复评论](https://kexue.fm/archives/6110/comment-page-2?replyTo=26258#respond-post-6110)

1. [«](https://kexue.fm/archives/6110/comment-page-1#comments)
2. [1](https://kexue.fm/archives/6110/comment-page-1#comments)
3. [2](https://kexue.fm/archives/6110/comment-page-2#comments)

[取消回复](https://kexue.fm/archives/6110#respond-post-6110)

你的大名

电子邮箱

个人网站（选填）

1\. 可以使用LaTeX代码，点击“预览效果”可查看效果；

2\. 可以通过点击评论楼层编号来引用该楼层；

3\. 网站可能会有点卡，如非确认评论失败，请 **不要重复点击提交**。

### 内容速览

[“图灵测试”思想](https://kexue.fm/archives/6110#%E2%80%9C%E5%9B%BE%E7%81%B5%E6%B5%8B%E8%AF%95%E2%80%9D%E6%80%9D%E6%83%B3)
[SGAN](https://kexue.fm/archives/6110#SGAN)
[问题所在](https://kexue.fm/archives/6110#%E9%97%AE%E9%A2%98%E6%89%80%E5%9C%A8)
[RSGAN基本框架](https://kexue.fm/archives/6110#RSGAN%E5%9F%BA%E6%9C%AC%E6%A1%86%E6%9E%B6)
[SGAN分析](https://kexue.fm/archives/6110#SGAN%E5%88%86%E6%9E%90)
[RSGAN目标](https://kexue.fm/archives/6110#RSGAN%E7%9B%AE%E6%A0%87)
[理论结果](https://kexue.fm/archives/6110#%E7%90%86%E8%AE%BA%E7%BB%93%E6%9E%9C)
[模型效果分析](https://kexue.fm/archives/6110#%E6%A8%A1%E5%9E%8B%E6%95%88%E6%9E%9C%E5%88%86%E6%9E%90)
[相关话题讨论](https://kexue.fm/archives/6110#%E7%9B%B8%E5%85%B3%E8%AF%9D%E9%A2%98%E8%AE%A8%E8%AE%BA)
[简单总结](https://kexue.fm/archives/6110#%E7%AE%80%E5%8D%95%E6%80%BB%E7%BB%93)
[延伸讨论](https://kexue.fm/archives/6110#%E5%BB%B6%E4%BC%B8%E8%AE%A8%E8%AE%BA)
[补充证明](https://kexue.fm/archives/6110#%E8%A1%A5%E5%85%85%E8%AF%81%E6%98%8E)

### 智能搜索

支持整句搜索！网站自动使用 [结巴分词](https://github.com/fxsjy/jieba) 进行分词，并结合ngrams排序算法给出合理的搜索结果。

### 热门标签

[生成模型](https://kexue.fm/tag/%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B/) [attention](https://kexue.fm/tag/attention/) [优化](https://kexue.fm/tag/%E4%BC%98%E5%8C%96/) [语言模型](https://kexue.fm/tag/%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/) [模型](https://kexue.fm/tag/%E6%A8%A1%E5%9E%8B/) [梯度](https://kexue.fm/tag/%E6%A2%AF%E5%BA%A6/) [网站](https://kexue.fm/tag/%E7%BD%91%E7%AB%99/) [概率](https://kexue.fm/tag/%E6%A6%82%E7%8E%87/) [矩阵](https://kexue.fm/tag/%E7%9F%A9%E9%98%B5/) [优化器](https://kexue.fm/tag/%E4%BC%98%E5%8C%96%E5%99%A8/) [转载](https://kexue.fm/tag/%E8%BD%AC%E8%BD%BD/) [微分方程](https://kexue.fm/tag/%E5%BE%AE%E5%88%86%E6%96%B9%E7%A8%8B/) [分析](https://kexue.fm/tag/%E5%88%86%E6%9E%90/) [天象](https://kexue.fm/tag/%E5%A4%A9%E8%B1%A1/) [深度学习](https://kexue.fm/tag/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/) [积分](https://kexue.fm/tag/%E7%A7%AF%E5%88%86/) [python](https://kexue.fm/tag/python/) [扩散](https://kexue.fm/tag/%E6%89%A9%E6%95%A3/) [力学](https://kexue.fm/tag/%E5%8A%9B%E5%AD%A6/) [无监督](https://kexue.fm/tag/%E6%97%A0%E7%9B%91%E7%9D%A3/) [几何](https://kexue.fm/tag/%E5%87%A0%E4%BD%95/) [节日](https://kexue.fm/tag/%E8%8A%82%E6%97%A5/) [生活](https://kexue.fm/tag/%E7%94%9F%E6%B4%BB/) [文本生成](https://kexue.fm/tag/%E6%96%87%E6%9C%AC%E7%94%9F%E6%88%90/) [数论](https://kexue.fm/tag/%E6%95%B0%E8%AE%BA/)

### 随机文章

- [在bert4keras中使用混合精度和XLA加速训练](https://kexue.fm/archives/9059)
- [三味Capsule：矩阵Capsule与EM路由](https://kexue.fm/archives/5155)
- [洗手盆里的学问](https://kexue.fm/archives/2031)
- [从动力学角度看优化算法（四）：GAN的第三个阶段](https://kexue.fm/archives/6583)
- [Performer：用随机投影将Attention的复杂度线性化](https://kexue.fm/archives/7921)
- [《自然极值》系列——8.极值分析](https://kexue.fm/archives/1134)
- [数值方法解方程之终极算法](https://kexue.fm/archives/590)
- [《向量》系列——2.曲率半径](https://kexue.fm/archives/714)
- [从Knotsevich在黑板上写的级数题目谈起](https://kexue.fm/archives/3229)
- [【学习清单】最近比较重要的GAN进展论文](https://kexue.fm/archives/6240)

### 最近评论

- [Bin](https://kexue.fm/archives/1990/comment-page-2#comment-29105): 今天偶然从某个论坛看到有人推荐您的博客，定睛一看竟然是华师同院的往届师兄！看到这篇2013年的...
- [Rapture D](https://kexue.fm/archives/11530/comment-page-1#comment-29104): 我有一个问题，为什么不考虑亥姆霍兹定理和斯托克斯公式。
- [mofheka](https://kexue.fm/archives/11390/comment-page-1#comment-29103): 苏神是还在用jax是么？最近在做基于Google Pathway的理念做一个动态版的MPMD框...
- [长琴](https://kexue.fm/archives/11530/comment-page-1#comment-29102): 看懂这篇博客也不是一件容易的事情。
- [AlexLi](https://kexue.fm/archives/9257/comment-page-4#comment-29101): 苏老师，请教一下(7)式中将 \\mu(x\_t) 传给 p\_o 进行推理的操作。 $x\_...
- [tyler\_zxc](https://kexue.fm/archives/7921/comment-page-2#comment-29100): "Performer的思想是将标准的Attention线性化，所以为什么不干脆直接训练一个线性...
- [我](https://kexue.fm/archives/11494/comment-page-1#comment-29099): 似乎并非mHC提出矩阵的思想？之前hyper connection就是了
- [winter](https://kexue.fm/archives/10847/comment-page-1#comment-29098): 苏神您好，假如对于比较均匀的attention weightP，往往呈现long tail分布...
- [苏剑林](https://kexue.fm/archives/8512/comment-page-2#comment-29097): KL散度、JS散度、W距离啥的，都行啊，看你喜欢哪个
- [苏剑林](https://kexue.fm/archives/9119/comment-page-14#comment-29096): 没有绝对公平的对比方法，主要看你关心什么。比如，如果只关心推理成本和推理效果，那么有的方法可以...

### 友情链接

- [Cool Papers](https://papers.cool/)
- [数学研发](https://bbs.emath.ac.cn/)
- [Seatop](http://www.seatop.com.cn/)
- [Xiaoxia](https://xiaoxia.org/)
- [积分表-网络版](https://kexue.fm/sci/integral/index.html)
- [丝路博傲](http://blog.dvxj.com/)
- [数学之家](http://www.2math.cn/)
- [有趣天文奇观](http://interesting-sky.china-vo.org/)
- [TwistedW](http://www.twistedwg.com/)
- [godweiyang](https://godweiyang.com/)
- [AI柠檬](https://blog.ailemon.net/)
- [王登科-DK博客](https://greatdk.com/)
- [ESON](https://blog.eson.org/)
- [枫之羽](https://fzhiy.net/)
- [coding-zuo](https://coding-zuo.github.io/)
- [博科园](https://www.bokeyuan.net/)
- [孔皮皮的博客](https://www.kppkkp.top/)
- [运鹏的博客](https://yunpengtai.top/)
- [jiming.site](https://jiming.site/)
- [OmegaXYZ](https://www.omegaxyz.com/)
- [EAI猩球](https://www.robotech.ink/)
- [文举的博客](https://liwenju0.com/)
- [申请链接](https://kexue.fm/links.html)

[![署名-非商业用途-保持一致](https://kexue.fm/usr/themes/geekg/images/cc.gif)](http://creativecommons.org/licenses/by-nc-nd/2.5/cn/) 本站采用创作共用版权协议，要求署名、非商业用途和保持一致。转载本站内容必须也遵循“ [署名-非商业用途-保持一致](http://creativecommons.org/licenses/by-nc-nd/2.5/cn/)”的创作共用协议。



© 2009-2026 Scientific Spaces. All rights reserved. Theme by [laogui](http://www.laogui.com/). Powered by [Typecho](http://typecho.org/). 备案号: [粤ICP备09093259号-1/2](https://beian.miit.gov.cn/ "粤ICP备09093259号")。