## SEARCH

## MENU

- [打赏](https://kexue.fm/reward.html)
- [公式](https://kexue.fm/latex.html)
- [天象](https://kexue.fm/ac.html)
- [链接](https://kexue.fm/links.html)
- [时光](https://kexue.fm/me.html)
- [博览](https://kexue.fm/science.html)
- [归档](https://kexue.fm/content.html)

## CATEGORIES

- [千奇百怪](https://kexue.fm/category/Everything)
- [天文探索](https://kexue.fm/category/Astronomy)
- [数学研究](https://kexue.fm/category/Mathematics)
- [物理化学](https://kexue.fm/category/Phy-chem)
- [信息时代](https://kexue.fm/category/Big-Data)
- [生物自然](https://kexue.fm/category/Biology)
- [图片摄影](https://kexue.fm/category/Photograph)
- [问题百科](https://kexue.fm/category/Questions)
- [生活/情感](https://kexue.fm/category/Life-Feeling)
- [资源共享](https://kexue.fm/category/Resources)

## NEWPOSTS

- [通过msign来计算mclip（奇...](https://kexue.fm/archives/11006)
- [msign算子的Newton-Sc...](https://kexue.fm/archives/10996)
- [等值振荡定理：最优多项式逼近的充要条件](https://kexue.fm/archives/10972)
- [生成扩散模型漫谈（三十）：从瞬时速...](https://kexue.fm/archives/10958)
- [MoE环游记：5、均匀分布的反思](https://kexue.fm/archives/10945)
- [msign算子的Newton-Sc...](https://kexue.fm/archives/10922)
- [Transformer升级之路：2...](https://kexue.fm/archives/10907)
- [一道概率不等式：盯着它到显然成立为止！](https://kexue.fm/archives/10902)
- [SVD的导数](https://kexue.fm/archives/10878)
- [智能家居之手搓一套能接入米家的零冷水装置](https://kexue.fm/archives/10869)

## COMMENTS

- [Chenguang Wang: 苏神好，我有个疑问，既然瞬时速度与时间t无关，那么对于样本$x...](https://kexue.fm/archives/10958/comment-page-1#comment-27821)
- [陈少龙: 苏神，您好！看了您的分享受益匪浅。但是我有个担心和疑问，您这里...](https://kexue.fm/archives/10907/comment-page-2#comment-27813)
- [PengchengMa: 牛啊](https://kexue.fm/archives/10996/comment-page-1#comment-27811)
- [xczh: 已使用mean flow policy，一步推理效果确实惊人，...](https://kexue.fm/archives/10958/comment-page-1#comment-27810)
- [Cosine: 是不是因为shared experts每次都激活，而route...](https://kexue.fm/archives/10945/comment-page-1#comment-27809)
- [rpsun: 这样似乎与传统的经验正交函数之类的有相似之处。把样本的平均值减...](https://kexue.fm/archives/10699/comment-page-1#comment-27808)
- [贵阳机场接机: 怎么不更新啦](https://kexue.fm/archives/1490/comment-page-1#comment-27807)
- [czvzb: 具身智能模型目前主流也是在使用扩散和流匹配这类方法来预测动作。...](https://kexue.fm/archives/10958/comment-page-1#comment-27806)
- [Shawn\_yang: 苏神，关于您所说的：“推理阶段可以事先预估Routed Exp...](https://kexue.fm/archives/10945/comment-page-1#comment-27802)
- [OceanYU: 您好，关于由式（7）推导出高斯分布，我这里有一点问题，式（7）...](https://kexue.fm/archives/9164/comment-page-4#comment-27801)

## USERLOGIN

- [登录](https://kexue.fm/admin/login.php)

[科学空间\|Scientific Spaces](https://kexue.fm)

- [登录](https://kexue.fm/admin/login.php)
- [打赏](https://kexue.fm/reward.html)
- [公式](https://kexue.fm/latex.html)
- [天象](https://kexue.fm/ac.html)
- [链接](https://kexue.fm/links.html)
- [时光](https://kexue.fm/me.html)
- [博览](https://kexue.fm/science.html)
- [归档](https://kexue.fm/content.html)

渴望成为一个小飞侠

- [欢迎订阅](https://kexue.fm/feed)
- [个性邮箱](https://kexue.fm/archives/119)
- [天象信息](https://kexue.fm/ac.html)
- [观测ISS](https://kexue.fm/archives/41)
- [LaTeX](https://kexue.fm/latex.html)
- [关于博主](https://kexue.fm/me.html)

欢迎访问“科学空间”，这里将与您共同探讨自然科学，回味人生百态；也期待大家的分享～

- [**千奇百怪** Everything](https://kexue.fm/category/Everything)
- [**天文探索** Astronomy](https://kexue.fm/category/Astronomy)
- [**数学研究** Mathematics](https://kexue.fm/category/Mathematics)
- [**物理化学** Phy-chem](https://kexue.fm/category/Phy-chem)
- [**信息时代** Big-Data](https://kexue.fm/category/Big-Data)
- [**生物自然** Biology](https://kexue.fm/category/Biology)
- [**图片摄影** Photograph](https://kexue.fm/category/Photograph)
- [**问题百科** Questions](https://kexue.fm/category/Questions)
- [**生活/情感** Life-Feeling](https://kexue.fm/category/Life-Feeling)
- [**资源共享** Resources](https://kexue.fm/category/Resources)

- [**千奇百怪**](https://kexue.fm/category/Everything)
- [**天文探索**](https://kexue.fm/category/Astronomy)
- [**数学研究**](https://kexue.fm/category/Mathematics)
- [**物理化学**](https://kexue.fm/category/Phy-chem)
- [**信息时代**](https://kexue.fm/category/Big-Data)
- [**生物自然**](https://kexue.fm/category/Biology)
- [**图片摄影**](https://kexue.fm/category/Photograph)
- [**问题百科**](https://kexue.fm/category/Questions)
- [**生活/情感**](https://kexue.fm/category/Life-Feeling)
- [**资源共享**](https://kexue.fm/category/Resources)

[首页](https://kexue.fm) [信息时代](https://kexue.fm/category/Big-Data) Google新作Synthesizer：我们还不够了解自注意力

25May

# [Google新作Synthesizer：我们还不够了解自注意力](https://kexue.fm/archives/7430)

By 苏剑林 \|
2020-05-25 \|
114355位读者\|

> 深度学习这个箱子，远比我们想象的要黑。

## 写在开头 [\#](https://kexue.fm/archives/7430\#%E5%86%99%E5%9C%A8%E5%BC%80%E5%A4%B4)

据说物理学家费曼说过一句话\[ [来源](https://en.wikiquote.org/wiki/Talk:Richard_Feynman#%22If_you_think_you_understand_quantum_mechanics,_you_don't_understand_quantum_mechanics.%22)\]：“谁要是说他懂得量子力学，那他就是真的不懂量子力学。”我现在越来越觉得，这句话中的“量子力学”也可以替换为“深度学习”。尽管深度学习已经在越来越多的领域证明了其有效性，但我们对它的解释性依然相当无力。当然，这几年来已经有不少工作致力于打开深度学习这个黑箱，但是很无奈，这些工作基本都是“马后炮”式的，也就是在已有的实验结果基础上提出一些勉强能说服自己的解释，无法做到自上而下的构建和理解模型的原理，更不用说提出一些前瞻性的预测。

本文关注的是自注意力机制。直观上来看，自注意力机制算是解释性比较强的模型之一了，它通过自己与自己的Attention来自动捕捉了token与token之间的关联，事实上在 [《Attention is All You Need》](https://papers.cool/arxiv/1706.03762) 那篇论文中，就给出了如下的看上去挺合理的可视化效果：

《Attention is All You Need》一文中对Attention的可视化例子

但自注意力机制真的是这样生效的吗？这种“token对token”的注意力是必须的吗？前不久Google的新论文 [《Synthesizer: Rethinking Self-Attention in Transformer Models》](https://papers.cool/arxiv/2005.00743) 对自注意力机制做了一些“异想天开”的探索，里边的结果也许会颠覆我们对自注意力的认知。

## 自注意力 [\#](https://kexue.fm/archives/7430\#%E8%87%AA%E6%B3%A8%E6%84%8F%E5%8A%9B)

自注意力模型的流行，始于2017年Google发表的 [《Attention is All You Need》](https://papers.cool/arxiv/1706.03762) 一文，关于它的科普读者还可以参考笔者旧作 [《Attention is All You Need》浅读（简介+代码）](https://kexue.fm/archives/4765)。它的基础是Scaled-Dot Attention，定义如下：
\\begin{equation}Attention(\\boldsymbol{Q},\\boldsymbol{K},\\boldsymbol{V}) = softmax\\left(\\frac{\\boldsymbol{Q}\\boldsymbol{K}^{\\top}}{\\sqrt{d\_k}}\\right)\\boldsymbol{V}\\end{equation}
其中$\\boldsymbol{Q}\\in\\mathbb{R}^{n\\times d\_k}, \\boldsymbol{K}\\in\\mathbb{R}^{m\\times d\_k}, \\boldsymbol{V}\\in\\mathbb{R}^{m\\times d\_v}$，softmax则是在$m$的那一维进行归一化。而自注意力，则是对于同一个$\\boldsymbol{X}\\in \\mathbb{R}^{n\\times d}$，通过不同的投影矩阵$\\boldsymbol{W}\_q,\\boldsymbol{W}\_k,\\boldsymbol{W}\_v\\in\\mathbb{R}^{d\\times d'}$得到$\\boldsymbol{Q}=\\boldsymbol{X}\\boldsymbol{W}\_q,\\boldsymbol{K}=\\boldsymbol{X}\\boldsymbol{W}\_k,\\boldsymbol{V}=\\boldsymbol{X}\\boldsymbol{W}\_v$，然后再做Attention，即
\\begin{equation}\\begin{aligned}
SelfAttention(\\boldsymbol{X}) =&\\, Attention(\\boldsymbol{X}\\boldsymbol{W}\_q, \\boldsymbol{X}\\boldsymbol{W}\_k, \\boldsymbol{X}\\boldsymbol{W}\_v)\\\
=&\\, softmax\\left(\\frac{\\boldsymbol{X}\\boldsymbol{W}\_q \\boldsymbol{W}\_k^{\\top}\\boldsymbol{X}^{\\top}}{\\sqrt{d\_k}}\\right)\\boldsymbol{X}\\boldsymbol{W}\_v&
\\end{aligned}\\end{equation}
至于Multi-Head Attention，则不过是Attention运算在不同的参数下重复多次然后将多个输出拼接起来，属于比较朴素的增强。而关于它的进一步推广，则可以参考 [《突破瓶颈，打造更强大的Transformer》](https://kexue.fm/archives/7325)。

## 天马行空 [\#](https://kexue.fm/archives/7430\#%E5%A4%A9%E9%A9%AC%E8%A1%8C%E7%A9%BA)

本质上来看，自注意力就是通过一个$n\\times n$的矩阵$\\boldsymbol{A}$和$d\\times d'$的矩阵$\\boldsymbol{W}\_v$，将原本是$n\\times d$的矩阵$\\boldsymbol{X}$，变成了$n\\times d'$的矩阵$\\boldsymbol{A}\\boldsymbol{X}\\boldsymbol{W}\_v$。其中矩阵$\\boldsymbol{A}$是动态生成的，即
\\begin{equation}\\boldsymbol{A}=softmax\\left(\\boldsymbol{B}\\right),\\quad\\boldsymbol{B}=\\frac{\\boldsymbol{X}\\boldsymbol{W}\_q \\boldsymbol{W}\_k^{\\top}\\boldsymbol{X}^{\\top}}{\\sqrt{d\_k}}\\end{equation}
对于矩阵$\\boldsymbol{B}$，本质上来说它就是$\\boldsymbol{X}$里边两两向量的内积组合，所以我们称它为“token对token”的Attention。

Synthesizer自注意力与标准自注意力的对比

那么，就到了前面提出的问题：“token对token”是必须的吗？能不能通过其他方式来生成这个矩阵$\\boldsymbol{B}$？Google的这篇论文正是“天马行空”了几种新的形式并做了实验，这些形式统称为Synthesizer。

### Dense形式 [\#](https://kexue.fm/archives/7430\#Dense%E5%BD%A2%E5%BC%8F)

第一种形式在原论文中称为Dense：$\\boldsymbol{B}$需要是$n\\times n$大小的，而$\\boldsymbol{X}$是$n\\times d$的，所以只需要一个$d\\times n$的变换矩阵$\\boldsymbol{W}\_a$就可以将它变成$n\\times n$了，即
\\begin{equation}\\boldsymbol{B}=\\boldsymbol{X}\\boldsymbol{W}\_a\\end{equation}
这其实就相当于把$\\boldsymbol{K}$固定为常数矩阵$\\boldsymbol{W}\_a^{\\top}$了。当然，原论文还做得更复杂一些，用到了两层Dense层：
\\begin{equation}\\boldsymbol{B}=\\text{relu}\\left(\\boldsymbol{X}\\boldsymbol{W}\_1 + \\boldsymbol{b}\_1\\right)\\boldsymbol{W}\_2 + \\boldsymbol{b}\_2\\end{equation}
但思想上并没有什么变化。

### Random形式 [\#](https://kexue.fm/archives/7430\#Random%E5%BD%A2%E5%BC%8F)

刚才说Dense形式相当于把$\\boldsymbol{K}$固定为常数矩阵，我们还能不能更“异想天开”一些：把$\\boldsymbol{Q}$固定为常数矩阵？这时候整个$\\boldsymbol{B}$相当于是一个常数矩阵，即
\\begin{equation}\\boldsymbol{B}=\\boldsymbol{R}\\end{equation}
原论文中还真是实验了这种形式，称之为Random，顾名思义，就是$\\boldsymbol{B}$是随机初始化的，然后可以选择随训练更新或不更新。据原论文描述，固定形式的Attention首次出现在论文 [《Fixed Encoder Self-Attention Patterns in Transformer-Based Machine Translation》](https://papers.cool/arxiv/2002.10260)，不同点是那里的Attention矩阵是由一个函数算出来的，而Google这篇论文则是完全随机初始化的。从形式上看，Random实际上就相当于可分离卷积（Depthwise Separable Convolution）运算。

### 低秩分解 [\#](https://kexue.fm/archives/7430\#%E4%BD%8E%E7%A7%A9%E5%88%86%E8%A7%A3)

上面两种新形式，往往会面对着参数过多的问题，所以很自然地就想到通过低秩分解来降低参数量。对于Dense和Random，原论文也提出并验证了对应的低秩分解形式，分别称为Factorized Dense和Factorized Random。

Factorized Dense通过Dense的方式，生成两个$n\\times a, n\\times b$的矩阵$\\boldsymbol{B}\_1,\\boldsymbol{B}\_2$，其中$ab=n$；然后将$\\boldsymbol{B}\_1$重复$b$次、然后将$\\boldsymbol{B}\_2$重复$a$次，得到对应的$n\\times n$矩阵$\\tilde{\\boldsymbol{B}}\_1,\\tilde{\\boldsymbol{B}}\_2$，最后将它们逐位相乘（个人感觉相乘之前$\\tilde{\\boldsymbol{B}}\_2$应该要转置一下比较合理，但原论文并没有提及），合成一个$n\\times n$的矩阵：
\\begin{equation}\\boldsymbol{B}=\\tilde{\\boldsymbol{B}}\_1 \\otimes \\tilde{\\boldsymbol{B}}\_2\\end{equation}

至于Factorized Random就很好理解了，本来是一整个$n\\times n$的矩阵$\\boldsymbol{R}$，现在变成两个$n\\times k$的矩阵$\\boldsymbol{R}\_1,\\boldsymbol{R}\_2$，然后
\\begin{equation}\\boldsymbol{B}=\\boldsymbol{R}\_1\\boldsymbol{R}\_2^{\\top} \\end{equation}

### 混合模式 [\#](https://kexue.fm/archives/7430\#%E6%B7%B7%E5%90%88%E6%A8%A1%E5%BC%8F)

到目前为止，连同标准的自注意力，我们有5种不同的生成矩阵$\\boldsymbol{B}$的方案，它们也可以混合起来，即
\\begin{equation}\\boldsymbol{B}=\\sum\_{i=1}^N \\alpha\_i \\boldsymbol{B}\_i\\end{equation}
其中$\\boldsymbol{B}\_i$是不同形式的自注意力矩阵，而$\\sum\\limits\_{i=1}^N \\alpha\_i=1$是可学习参数。

## 结果分析 [\#](https://kexue.fm/archives/7430\#%E7%BB%93%E6%9E%9C%E5%88%86%E6%9E%90)

前面介绍了统称为Synthesizer的几种新型自注意力形式，它们的共同特点是没有保持“token对token”形式，尤其是Random，则完全抛弃了原有注意力的动态特点，变成了静态的矩阵。

那么，这些新型自注意力的效果如何呢？它们又怎样冲击我们对自注意力机制的认识呢？

### 机器翻译 [\#](https://kexue.fm/archives/7430\#%E6%9C%BA%E5%99%A8%E7%BF%BB%E8%AF%91)

第一个评测任务是机器翻译，详细地比较了各种自注意力形式的效果：

Synthesizer在机器翻译任务上的表现对比

不知道读者怎么想，反正Synthesizer的这些结果是冲击了笔者对自注意力的认知的。表格显示，除了固定的Random外，所有的自注意力形式表现基本上都差不多，而且就算是固定的Random也有看得过去的效果，这表明我们以往对自注意力的认知和解释都太过片面了，并没有揭示自注意力生效的真正原因。

### 摘要对话 [\#](https://kexue.fm/archives/7430\#%E6%91%98%E8%A6%81%E5%AF%B9%E8%AF%9D)

接下来在摘要和对话生成任务上的结果：

Synthesizer在摘要和对话任务上的表现对比

在自动摘要这个任务上，标准注意力效果比较好，但是对话生成这个任务上，结果则反过来：标准的自注意力是最差的，Dense（D）和Random（R）是最好的，而当Dense和Random混合了标准的自注意力后（即 D+V 和 R+V），效果也变差了。这说明标准注意力并没有什么“独占鳌头”的优势，而几个Synthesizer看起来是标准注意力的“退化”，但事实上它们互不从属，各有优势。

### 预训练+微调 [\#](https://kexue.fm/archives/7430\#%E9%A2%84%E8%AE%AD%E7%BB%83+%E5%BE%AE%E8%B0%83)

最后，对于我们这些普通读者来说，可能比较关心是“预训练+微调”的效果怎样，也就是说，将BERT之类的模型的自注意力替换之后表现如何？原论文确实也做了这个实验，不过Baseline不是BERT而是T5，结果如下：

Synthesizer在“预训练+微调”的表现对比

在这个结果中，相比标准自注意力，Dense和Random就显得逊色了，这表明Dense和Random也许会在单一任务上表现得比较好，而迁移能力则比较弱。但是不能否定的是，像Random这样的自注意力，由于直接省去了$\\boldsymbol{Q}\\boldsymbol{K}^{\\top}$这个矩阵运算，因此计算效率会有明显提升，因此如果能想法子解决这个迁移性问题，说不准Transformer模型家族将会迎来大换血。

## 文末小结 [\#](https://kexue.fm/archives/7430\#%E6%96%87%E6%9C%AB%E5%B0%8F%E7%BB%93)

本文介绍了Google的新工作Synthesizer，它是对目前流行的自注意力机制的反思和探索。论文中提出了几种新型的自注意力机制，并做了相当充分的实验，而实验结果很可能会冲击我们对自注意力机制的已有认知，值得大家读读～

_**转载到请包括本文地址：** [https://kexue.fm/archives/7430](https://kexue.fm/archives/7430)_

_**更详细的转载事宜请参考：**_ [《科学空间FAQ》](https://kexue.fm/archives/6508#%E6%96%87%E7%AB%A0%E5%A6%82%E4%BD%95%E8%BD%AC%E8%BD%BD/%E5%BC%95%E7%94%A8)

**如果您还有什么疑惑或建议，欢迎在下方评论区继续讨论。**

**如果您觉得本文还不错，欢迎 [分享](https://kexue.fm/archives/7430#share)/ [打赏](https://kexue.fm/archives/7430#pay) 本文。打赏并非要从中获得收益，而是希望知道科学空间获得了多少读者的真心关注。当然，如果你无视它，也不会影响你的阅读。再次表示欢迎和感谢！**

打赏

微信打赏

支付宝打赏

因为网站后台对打赏并无记录，因此欢迎在打赏时候备注留言。你还可以 [**点击这里**](http://mail.qq.com/cgi-bin/qm_share?t=qm_mailme&email=tN7d1drY3drrx8H0xcWa19vZ) 或在下方评论区留言来告知你的建议或需求。

**如果您需要引用本文，请参考：**

苏剑林. (May. 25, 2020). 《Google新作Synthesizer：我们还不够了解自注意力 》\[Blog post\]. Retrieved from [https://kexue.fm/archives/7430](https://kexue.fm/archives/7430)

@online{kexuefm-7430,
        title={Google新作Synthesizer：我们还不够了解自注意力},
        author={苏剑林},
        year={2020},
        month={May},
        url={\\url{https://kexue.fm/archives/7430}},
}

分类： [信息时代](https://kexue.fm/category/Big-Data)    标签： [语言模型](https://kexue.fm/tag/%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/), [attention](https://kexue.fm/tag/attention/)[35 评论](https://kexue.fm/archives/7430#comments)

< [鱼与熊掌兼得：融合检索和生成的SimBERT模型](https://kexue.fm/archives/7427) \| [泛化性乱弹：从随机噪声、梯度惩罚到虚拟对抗训练](https://kexue.fm/archives/7466) >

### 你也许还对下面的内容感兴趣

- [Transformer升级之路：20、MLA究竟好在哪里？](https://kexue.fm/archives/10907)
- [Transformer升级之路：19、第二类旋转位置编码](https://kexue.fm/archives/10862)
- [细水长flow之TARFLOW：流模型满血归来？](https://kexue.fm/archives/10667)
- [“闭门造车”之多模态思路浅谈（三）：位置编码](https://kexue.fm/archives/10352)
- [Decoder-only的LLM为什么需要位置编码？](https://kexue.fm/archives/10347)
- [Monarch矩阵：计算高效的稀疏型矩阵分解](https://kexue.fm/archives/10249)
- [Transformer升级之路：18、RoPE的底数选择原则](https://kexue.fm/archives/10122)
- [缓存与效果的极限拉扯：从MHA、MQA、GQA到MLA](https://kexue.fm/archives/10091)
- [Transformer升级之路：17、多模态位置编码的简单思考](https://kexue.fm/archives/10040)
- [时空之章：将Attention视为平方复杂度的RNN](https://kexue.fm/archives/10017)

[发表你的看法](https://kexue.fm/archives/7430#comment_form)

1. [«](https://kexue.fm/archives/7430/comment-page-1#comments)
2. [1](https://kexue.fm/archives/7430/comment-page-1#comments)
3. [2](https://kexue.fm/archives/7430/comment-page-2#comments)

wp

July 11th, 2022

苏神，式（9）的$\\alpha$是随机初始化还是由函数计算出来的？式（9）可以看成矩阵$B$的基矩阵的线性组合吗？

[回复评论](https://kexue.fm/archives/7430/comment-page-2?replyTo=19445#respond-post-7430)

[苏剑林](https://kexue.fm) 发表于
July 12th, 2022

随即初始化。

[回复评论](https://kexue.fm/archives/7430/comment-page-2?replyTo=19453#respond-post-7430)

sizer

August 18th, 2022

苏神，synthesizer的random形式的attention，是否可以理解成一种bias呢？类似于一个linear层的bias，只不过变成了矩阵的形式。

[回复评论](https://kexue.fm/archives/7430/comment-page-2?replyTo=19646#respond-post-7430)

[苏剑林](https://kexue.fm) 发表于
August 19th, 2022

为什么不是kernel而是bias？

[回复评论](https://kexue.fm/archives/7430/comment-page-2?replyTo=19653#respond-post-7430)

wn

April 13th, 2023

苏神，synthesizer的random形式的attention引入了n\*n的矩阵，但是N是一个不确定的值，怎么来事现初始化呢

[回复评论](https://kexue.fm/archives/7430/comment-page-2?replyTo=21390#respond-post-7430)

[苏剑林](https://kexue.fm) 发表于
April 15th, 2023

synthesizer应该事先固定了最大的$n$。

[回复评论](https://kexue.fm/archives/7430/comment-page-2?replyTo=21409#respond-post-7430)

hazdzz

October 20th, 2023

注意力矩陣如果由幾個稀疏矩陣相乘而得到，既可降低時間複雜度又可滿秩。

[回复评论](https://kexue.fm/archives/7430/comment-page-2?replyTo=22910#respond-post-7430)

[苏剑林](https://kexue.fm) 发表于
October 21st, 2023

一般的稀疏矩阵乘法并不是那么容易降低复杂度的～

[回复评论](https://kexue.fm/archives/7430/comment-page-2?replyTo=22924#respond-post-7430)

Allen7575

January 21st, 2024

這些替代方案的問題是，沒辦法像原始的 attention 一樣把 weight 可視化出來。很多 attention 的應用不只是看任務的準確度，而是要利用 attention weight 取出有用的資訊。比方說我可以用bert finetune 下游任務後，取出 attention weight 來找出對下游任務最重要的關鍵字，這就可以當成是一種自動摘要或可解釋性的方案。

當 attention 取出來的東西跟我們認知差異太大時，可以判斷該結果可能有問題，就算答案是對的，但推論過程不一定是對的。例如他可能attention到某個符號，而這符號只會出現在某個來源的文章，剛好該來源大部分都是某類文章，結果就錯誤歸因了，而非真的從語意中學到該分類。這或許可以解釋上面結果為何遷移能力較弱，因為 synthesizer 可能只學習到資料中某些表面的訊號，並沒有深層理解字義。要深層理解字義，必然要給出字跟意義之間的正確歸因，然而現實世界的資料雜訊太多，錯誤歸因的情況難以避免，更高的準確率有可能只是擬合到了錯誤的歸因。然而 attention 是一個鼓勵正確歸因的模型，因為錯置的 attention weight 不容易泛化到不同樣本，只有正確歸因的 attention weight 能容易泛化到其它樣本。

因此我認為，從可解釋性的角度， attention 相較於上面的 synthesizer 或其它種方法，還是一個比較容易理解的模型。

[回复评论](https://kexue.fm/archives/7430/comment-page-2?replyTo=23537#respond-post-7430)

Allen7575 发表于
January 22nd, 2024

應該說，attention的token對token機制，鼓勵了正確歸因。因為 query中的某個token，可以歸因到key中的某個token。但在上面的 synthesizer 方法中，省略了q, k 的對應，等於只用一個q來計算attention，這樣就有可能造成錯誤歸因。想像q裡面一個token，他只靠自己產生attention，跟與k交互產生attention，若k不同於q時(例如cross attention)，就有機會消除掉錯誤歸因。因為q中的同一個token，可能對應到不同k的某個token，這樣就消除了只依賴於q自己所可能產生的錯誤歸因。

而在bert的情況下，兩個句子用sep分開，所以其實不用cross attention，只要看句子1的q對句子2的k所產生的attention，再加上next sentence prediction，把句子2換成另外一個句子，然後讓模型辨識出差異，這樣就構成了一個q對應到兩個不同k，這時候q中所學到的特徵就必需不能是虛假關聯，否則他就無法正確分辨兩者的差異。因此就消除了虛假關聯，從而提升了泛化能力。

我認為泛化能力，也可理解為消除虛假關聯的能力。比方說某個字經常同某個字一起出現，但他們兩個可能根本沒甚麼關係，但是在統計上，某批資料中這兩個字共同出現的機率就很高，行長虛假關聯。然而泛化要求的是這兩個字如果各別出現在別的領域的資料中，能夠按照他原來的意義被理解。而在別的領域中，你就不能得到這兩個字會經常一起出現的統計現象了。這時，next sentence prediction 的作用就有點像模擬來自兩個不同domain的資料的情況，起到了消除某些虛假關聯的作用。

因此我認為 BERT的 next sentence prediction可能是它可以容易微調泛化到不同領域的重要關鍵。

[回复评论](https://kexue.fm/archives/7430/comment-page-2?replyTo=23538#respond-post-7430)

[苏剑林](https://kexue.fm) 发表于
January 22nd, 2024

怎么说呢，从现在的LLM角度来看，追求模型内部的可解释性几乎毫无意义，一切都只是scale up（model size、data size）

不过确实，我以前发现，Linear Attention训练出来的BERT，在微调迁移时效果远远不如标准Attention的BERT（但预训练的效果相当），所以说明架构本身应该会影响迁移能力，甚至Pre Norm/Post Norm的区别也会明显微调效果。但又回到LLM时代，微调已经基本不重要了，大家都在追求直接能用的通用AI（当然，在这个目标完全实现之前，微调还有一定的用武之地），所以过于纠结模型架构上的事情意义不大了。

[回复评论](https://kexue.fm/archives/7430/comment-page-2?replyTo=23558#respond-post-7430)

Allen7575 发表于
January 22nd, 2024

謝謝你的回答！其實我目前在讀博士班，我的研究方向並不追求使用LLM的直接應用(那大部分已經屬於工程上的技巧，無法用來發paper)，反倒是要嘗試一些NLP目前還未解決的問題，例如可解釋性、可遷移性...等。所以不能說有了LLM其他都不重要了，不然我們做NLP的都不要做研究了...

[回复评论](https://kexue.fm/archives/7430/comment-page-2?replyTo=23563#respond-post-7430)

[苏剑林](https://kexue.fm) 发表于
January 23rd, 2024

我能理解学术上的需要，但上面我主要想说的是，其实模型对人类来说的一些可解释性，可能无法代表模型真正在拟合数据时的能力，最经典之一就是注意力机制，人们想象它应该像注意力那样描述了token与token之间的相对重要性，并且也以“注意力”命名，但实际训练出来的Attention矩阵是很难解释的（或者说至少从人类的角度是很难理解）。

另一方面，神经网络本身是高度过参数化的。大致意思是说，假设一个任务理论上可以用x参数量就能完成，但实际要scale到10x参数量的实际效果才比较好，多出来的9x参数量实际上比较冗余，但又不得不加（这可能跟梯度下降有关），而加了之后可能就会把原本能解释的部分给掩盖了。

[回复评论](https://kexue.fm/archives/7430/comment-page-2?replyTo=23568#respond-post-7430)

Allen7575 发表于
January 23rd, 2024

不過還是有一些可視化的應用，最經典的例如影像上的熱力圖，有些人嘗試把 stable diffution 中 cross attention weight 拿出來，發現輸入的文字可以直觀的對應到圖上的部份，例如輸入"帽子"，attention weight 就會特別關注到影像上的帽子部分。

我認為你說的人類難以理解，是因為模型是從 low level 開始訓練的，自然很難得到甚麼有意義的解釋。token跟token之間本來就難以理解。但是文字跟影像之間就存在 high level 的關係，因為人是看圖的整塊區域，不是看像素，而文字的意義是一個詞或一個句子來理解的，不是看一個token。所以我們可以從生成的圖像跟輸入的文字中找到某種高層的對應關係，而非一個像素對一個token的關係。

回到文字上，token對token 這種方式，可能無法反映出人類可理解的意義。真要說，應該是句子對句子比較有意義。只是目前的建模方式，也不用斷句了，透過模型的強大能力，直接從token level 開始，層數深一點，就能夠表示句子；而生成的方式，是一個token一個token生成的，並非像人類一樣，是先有概念，由概念構成句子，再輸出成文字。人類可理解的，是屬於概念的層次，而非文字之間怎麼組成。我們不用懂中文語法也能講中文，因為大腦的底層已經幫我們做了這件事了，所以我們在想事情時，是以概念來進行思考的，而非一個字一個字思考，這跟目前的模型有很大的差別。

因此我認為，如果是以句子或子句為單位來建模，它的可解釋性會比較清楚。至於底下詞跟詞之間怎麼關聯的，無法解釋也沒有關係。就像在量子尺度很多事情無法用常理解釋，但是當這些效應加總起來，到了巨觀尺度時，就又變得可解釋了。你不必懂量子力學，也能用可解釋的方式算出行星軌道，就是這個道理。

所以我認為可解釋性是存在的，只是目前建模的層次不是人類可理解的層次。就好像模型給你的是量子之間的關係，然而你能理解的是巨觀的現象，中間存在一個gap，但是語言又不像物理，可以有一些理論來銜接巨觀與量子尺度，所以造成了人類很難理解模型到底在做甚麼。而我們的任務應該是找到人類可理解的層次，讓模型在這個層次上輸出可解釋的訊號。

[回复评论](https://kexue.fm/archives/7430/comment-page-2?replyTo=23571#respond-post-7430)

[苏剑林](https://kexue.fm) 发表于
January 24th, 2024

1、不是人类可理解的解释性还叫可解释吗？（我不知道）；

2、如果句子足够多，那么是不是也没有“比较清楚”的可解释性了？

3、视觉上的Attention确实能做一些归因的事情，但相比于直接分析Attention矩阵，类似“积分梯度（ [https://kexue.fm/archives/7533](https://kexue.fm/archives/7533) ）”之类的模型无关的方法其实更加主流，也可以用到NLP中。

[回复评论](https://kexue.fm/archives/7430/comment-page-2?replyTo=23582#respond-post-7430)

Allen7575 发表于
January 24th, 2024

1\. 我的意思是，人類可理解的範圍是有限的，聚焦在高階的概念。但是模型是從低階開始建模的，如 bit-level 或 token-level，低階的運作方式可能完全不同於高階的方式，追求低階的可解釋性可能本來就是沒有意義的。就像你一開始提到費曼說的：「如果有人說他懂量子力學，那他就是不懂。」，因為量子力學是作用在原子以下的尺度，其運作方式完全不同於巨觀物理。因此追求理解量子力學可能是沒有意義的。但是由量子力學集體產生的巨觀現象，卻是人類可以理解的。我想強調的是"尺度"的問題。

lljbash 发表于
February 14th, 2025

模型并不是一个字一个字思考，而是每次只讲出来一个字。

[回复评论](https://kexue.fm/archives/7430/comment-page-2?replyTo=26610#respond-post-7430)

[苏剑林](https://kexue.fm) 发表于
February 15th, 2025

模型内部也未必一个字一个字地思考，它只是一个字一个字地读。

1. [«](https://kexue.fm/archives/7430/comment-page-1#comments)
2. [1](https://kexue.fm/archives/7430/comment-page-1#comments)
3. [2](https://kexue.fm/archives/7430/comment-page-2#comments)

[取消回复](https://kexue.fm/archives/7430#respond-post-7430)

你的大名

电子邮箱

个人网站（选填）

1\. 可以使用LaTeX代码，点击“预览效果”可查看效果；2. 可以通过点击评论楼层编号来引用该楼层；3. 网站可能会有点卡，如非确认评论失败，请 **不要重复点击提交**。

### 内容速览

[写在开头](https://kexue.fm/archives/7430#%E5%86%99%E5%9C%A8%E5%BC%80%E5%A4%B4)
[自注意力](https://kexue.fm/archives/7430#%E8%87%AA%E6%B3%A8%E6%84%8F%E5%8A%9B)
[天马行空](https://kexue.fm/archives/7430#%E5%A4%A9%E9%A9%AC%E8%A1%8C%E7%A9%BA)
[Dense形式](https://kexue.fm/archives/7430#Dense%E5%BD%A2%E5%BC%8F)
[Random形式](https://kexue.fm/archives/7430#Random%E5%BD%A2%E5%BC%8F)
[低秩分解](https://kexue.fm/archives/7430#%E4%BD%8E%E7%A7%A9%E5%88%86%E8%A7%A3)
[混合模式](https://kexue.fm/archives/7430#%E6%B7%B7%E5%90%88%E6%A8%A1%E5%BC%8F)
[结果分析](https://kexue.fm/archives/7430#%E7%BB%93%E6%9E%9C%E5%88%86%E6%9E%90)
[机器翻译](https://kexue.fm/archives/7430#%E6%9C%BA%E5%99%A8%E7%BF%BB%E8%AF%91)
[摘要对话](https://kexue.fm/archives/7430#%E6%91%98%E8%A6%81%E5%AF%B9%E8%AF%9D)
[预训练+微调](https://kexue.fm/archives/7430#%E9%A2%84%E8%AE%AD%E7%BB%83+%E5%BE%AE%E8%B0%83)
[文末小结](https://kexue.fm/archives/7430#%E6%96%87%E6%9C%AB%E5%B0%8F%E7%BB%93)

### 智能搜索

支持整句搜索！网站自动使用 [结巴分词](https://github.com/fxsjy/jieba) 进行分词，并结合ngrams排序算法给出合理的搜索结果。

### 热门标签

[生成模型](https://kexue.fm/tag/%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B/) [attention](https://kexue.fm/tag/attention/) [优化](https://kexue.fm/tag/%E4%BC%98%E5%8C%96/) [语言模型](https://kexue.fm/tag/%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/) [模型](https://kexue.fm/tag/%E6%A8%A1%E5%9E%8B/) [网站](https://kexue.fm/tag/%E7%BD%91%E7%AB%99/) [概率](https://kexue.fm/tag/%E6%A6%82%E7%8E%87/) [梯度](https://kexue.fm/tag/%E6%A2%AF%E5%BA%A6/) [转载](https://kexue.fm/tag/%E8%BD%AC%E8%BD%BD/) [微分方程](https://kexue.fm/tag/%E5%BE%AE%E5%88%86%E6%96%B9%E7%A8%8B/) [矩阵](https://kexue.fm/tag/%E7%9F%A9%E9%98%B5/) [天象](https://kexue.fm/tag/%E5%A4%A9%E8%B1%A1/) [分析](https://kexue.fm/tag/%E5%88%86%E6%9E%90/) [深度学习](https://kexue.fm/tag/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/) [积分](https://kexue.fm/tag/%E7%A7%AF%E5%88%86/) [python](https://kexue.fm/tag/python/) [优化器](https://kexue.fm/tag/%E4%BC%98%E5%8C%96%E5%99%A8/) [力学](https://kexue.fm/tag/%E5%8A%9B%E5%AD%A6/) [无监督](https://kexue.fm/tag/%E6%97%A0%E7%9B%91%E7%9D%A3/) [扩散](https://kexue.fm/tag/%E6%89%A9%E6%95%A3/) [几何](https://kexue.fm/tag/%E5%87%A0%E4%BD%95/) [节日](https://kexue.fm/tag/%E8%8A%82%E6%97%A5/) [生活](https://kexue.fm/tag/%E7%94%9F%E6%B4%BB/) [文本生成](https://kexue.fm/tag/%E6%96%87%E6%9C%AC%E7%94%9F%E6%88%90/) [数论](https://kexue.fm/tag/%E6%95%B0%E8%AE%BA/)

### 随机文章

- [大词表语言模型在续写任务上的一个问题及对策](https://kexue.fm/archives/9762)
- [捉弄计划的失败——单摆周期](https://kexue.fm/archives/674)
- [三角函数幂的积分](https://kexue.fm/archives/1422)
- [2010年诺贝尔文学奖落户秘鲁](https://kexue.fm/archives/981)
- [费曼路径积分思想的发展(一)](https://kexue.fm/archives/1844)
- [关于“平衡态公理”的更正与思考](https://kexue.fm/archives/1902)
- [天体力学巨匠——拉普拉斯](https://kexue.fm/archives/1779)
- [有理直角三角形的面积能否为整数？](https://kexue.fm/archives/1463)
- [本站域名Spaces.Ac.Cn的PR为2了](https://kexue.fm/archives/241)
- [从费马大定理谈起（四）：唯一分解整环](https://kexue.fm/archives/2819)

### 最近评论

- [Chenguang Wang](https://kexue.fm/archives/10958/comment-page-1#comment-27821): 苏神好，我有个疑问，既然瞬时速度与时间t无关，那么对于样本$x\_0$和$x\_1$之间任意时刻的...
- [陈少龙](https://kexue.fm/archives/10907/comment-page-2#comment-27813): 苏神，您好！看了您的分享受益匪浅。但是我有个担心和疑问，您这里的每个实验表里不同对比组的los...
- [PengchengMa](https://kexue.fm/archives/10996/comment-page-1#comment-27811): 牛啊
- [xczh](https://kexue.fm/archives/10958/comment-page-1#comment-27810): 已使用mean flow policy，一步推理效果确实惊人，性能跟多步推理的diffusio...
- [Cosine](https://kexue.fm/archives/10945/comment-page-1#comment-27809): 是不是因为shared experts每次都激活，而routed experts是依概率被选中...
- [rpsun](https://kexue.fm/archives/10699/comment-page-1#comment-27808): 这样似乎与传统的经验正交函数之类的有相似之处。把样本的平均值减掉之后做正交分解。那么如果单纯地...
- [贵阳机场接机](https://kexue.fm/archives/1490/comment-page-1#comment-27807): 怎么不更新啦
- [czvzb](https://kexue.fm/archives/10958/comment-page-1#comment-27806): 具身智能模型目前主流也是在使用扩散和流匹配这类方法来预测动作。
苏神推荐你看这几篇文章：
1....
- [Shawn\_yang](https://kexue.fm/archives/10945/comment-page-1#comment-27802): 苏神，关于您所说的：“推理阶段可以事先预估Routed Expert的实际分布，只要细致地进行...
- [OceanYU](https://kexue.fm/archives/9164/comment-page-4#comment-27801): 您好，关于由式（7）推导出高斯分布，我这里有一点问题，式（7）只能保证关于x\_t-1是二次函数...

### 友情链接

- [Cool Papers](https://papers.cool)
- [数学研发](https://bbs.emath.ac.cn)
- [Seatop](http://www.seatop.com.cn/)
- [Xiaoxia](https://xiaoxia.org/)
- [积分表-网络版](https://kexue.fm/sci/integral/index.html)
- [丝路博傲](http://blog.dvxj.com/)
- [ph4ntasy 饭特稀](http://www.ph4ntasy.com/)
- [数学之家](http://www.2math.cn/)
- [有趣天文奇观](http://interesting-sky.china-vo.org/)
- [TwistedW](http://www.twistedwg.com/)
- [godweiyang](https://godweiyang.com/)
- [AI柠檬](https://blog.ailemon.net/)
- [王登科-DK博客](https://greatdk.com)
- [ESON](https://blog.eson.org/)
- [枫之羽](https://fzhiy.net/)
- [Mathor's blog](https://wmathor.com/)
- [coding-zuo](https://coding-zuo.github.io/)
- [博科园](https://www.bokeyuan.net/)
- [孔皮皮的博客](https://www.kppkkp.top/)
- [运鹏的博客](https://yunpengtai.top/)
- [jiming.site](https://jiming.site/)
- [OmegaXYZ](https://www.omegaxyz.com/)
- [Blog by Eacls](https://www.eacls.top/)
- [EAI猩球](https://www.robotech.ink/)
- [文举的博客](https://liwenju0.com/)
- [用代码打点酱油](https://bruceyuan.com/)
- [申请链接](https://kexue.fm/links.html)

本站采用创作共用版权协议，要求署名、非商业用途和保持一致。转载本站内容必须也遵循“ [署名-非商业用途-保持一致](http://creativecommons.org/licenses/by-nc-nd/2.5/cn/)”的创作共用协议。
© 2009-2025 Scientific Spaces. All rights reserved. Theme by [laogui](http://www.laogui.com). Powered by [Typecho](http://typecho.org). 备案号: [粤ICP备09093259号-1/2](https://beian.miit.gov.cn/)。