![MobileSideBar](https://kexue.fm/usr/themes/geekg/images/slide-button.png)

## SEARCH

## MENU

- [打赏](https://kexue.fm/reward.html)
- [公式](https://kexue.fm/latex.html)
- [天象](https://kexue.fm/ac.html)
- [链接](https://kexue.fm/links.html)
- [时光](https://kexue.fm/me.html)
- [博览](https://kexue.fm/science.html)
- [归档](https://kexue.fm/content.html)

## CATEGORIES

- [千奇百怪](https://kexue.fm/category/Everything)
- [天文探索](https://kexue.fm/category/Astronomy)
- [数学研究](https://kexue.fm/category/Mathematics)
- [物理化学](https://kexue.fm/category/Phy-chem)
- [信息时代](https://kexue.fm/category/Big-Data)
- [生物自然](https://kexue.fm/category/Biology)
- [图片摄影](https://kexue.fm/category/Photograph)
- [问题百科](https://kexue.fm/category/Questions)
- [生活/情感](https://kexue.fm/category/Life-Feeling)
- [资源共享](https://kexue.fm/category/Resources)

## NEWPOSTS

- [通向概率分布之路：盘点Softma...](https://kexue.fm/archives/10145)
- [重温SSM（二）：HiPPO的一些...](https://kexue.fm/archives/10137)
- [Transformer升级之路：1...](https://kexue.fm/archives/10122)
- [重温SSM（一）：线性系统和HiP...](https://kexue.fm/archives/10114)
- [缓存与效果的极限拉扯：从MHA、M...](https://kexue.fm/archives/10091)
- [Cool Papers更新：简单搭...](https://kexue.fm/archives/10088)
- [以蒸馏的名义：“从去噪自编码器到生...](https://kexue.fm/archives/10085)
- [生成扩散模型漫谈（二十四）：少走捷...](https://kexue.fm/archives/10077)
- [生成扩散模型漫谈（二十三）：信噪比...](https://kexue.fm/archives/10055)
- [生成扩散模型漫谈（二十二）：信噪比...](https://kexue.fm/archives/10047)

## COMMENTS

- [Wen Fei: 您好，苏神，我有一点不明白。l2 norm对所有token和b...](https://kexue.fm/archives/9859/comment-page-1#comment-24558)
- [Wen Fei: 现在能讲一下 质量的来源了嘛？ 在相对论里，质量和能量是一样的...](https://kexue.fm/archives/2036/comment-page-1#comment-24557)
- [bill: 有人使用VQGAN的训练方法对比过VQ和FSQ吗？这应该是FS...](https://kexue.fm/archives/9826/comment-page-2#comment-24556)
- [NirVa: 为啥不做利用cookie保存star？](https://kexue.fm/archives/9978/comment-page-1#comment-24555)
- [lcz: 苏神，为什么“找一个在整个实数域上都单调递增的函数，而且增长速...](https://kexue.fm/archives/3290/comment-page-2#comment-24554)
- [周名远: Kiro: 很高兴你通过实践验证了SiD的稳定性。SDXL的d...](https://kexue.fm/archives/10085/comment-page-1#comment-24553)
- [苏剑林: 刚刷到这篇paper，它是每个像素都视为一个token，这种做...](https://kexue.fm/archives/9984/comment-page-2#comment-24552)
- [苏剑林: 谢谢，已更正。](https://kexue.fm/archives/10114/comment-page-1#comment-24551)
- [苏剑林: 感谢提醒。Softmax Bottleneck有所耳闻，但我个...](https://kexue.fm/archives/10145/comment-page-1#comment-24550)
- [苏剑林: Chrome和Safari测试正常，暂时无法测试所有浏览器，抱歉。](https://kexue.fm/archives/9164/comment-page-3#comment-24549)

## USERLOGIN

- [登录](https://kexue.fm/admin/login.php)

[科学空间\|Scientific Spaces](https://kexue.fm)

- [登录](https://kexue.fm/admin/login.php)
- [打赏](https://kexue.fm/reward.html)
- [公式](https://kexue.fm/latex.html)
- [天象](https://kexue.fm/ac.html)
- [链接](https://kexue.fm/links.html)
- [时光](https://kexue.fm/me.html)
- [博览](https://kexue.fm/science.html)
- [归档](https://kexue.fm/content.html)

渴望成为一个小飞侠

- [![](https://kexue.fm/usr/themes/geekg/images/rss.png)\
\
欢迎订阅](https://kexue.fm/feed)
- [![](https://kexue.fm/usr/themes/geekg/images/mail.png)\
\
个性邮箱](https://kexue.fm/archives/119)
- [![](https://kexue.fm/usr/themes/geekg/images/Saturn.png)\
\
天象信息](https://kexue.fm/ac.html)
- [![](https://kexue.fm/usr/themes/geekg/images/iss.png)\
\
观测ISS](https://kexue.fm/archives/41)
- [![](https://kexue.fm/usr/themes/geekg/images/pi.png)\
\
LaTeX](https://kexue.fm/latex.html)
- [![](https://kexue.fm/usr/themes/geekg/images/mlogo.png)\
\
关于博主](https://kexue.fm/me.html)

欢迎访问“科学空间”，这里将与您共同探讨自然科学，回味人生百态；也期待大家的分享～

- [**千奇百怪** Everything](https://kexue.fm/category/Everything)
- [**天文探索** Astronomy](https://kexue.fm/category/Astronomy)
- [**数学研究** Mathematics](https://kexue.fm/category/Mathematics)
- [**物理化学** Phy-chem](https://kexue.fm/category/Phy-chem)
- [**信息时代** Big-Data](https://kexue.fm/category/Big-Data)
- [**生物自然** Biology](https://kexue.fm/category/Biology)
- [**图片摄影** Photograph](https://kexue.fm/category/Photograph)
- [**问题百科** Questions](https://kexue.fm/category/Questions)
- [**生活/情感** Life-Feeling](https://kexue.fm/category/Life-Feeling)
- [**资源共享** Resources](https://kexue.fm/category/Resources)

- [**千奇百怪**](https://kexue.fm/category/Everything)
- [**天文探索**](https://kexue.fm/category/Astronomy)
- [**数学研究**](https://kexue.fm/category/Mathematics)
- [**物理化学**](https://kexue.fm/category/Phy-chem)
- [**信息时代**](https://kexue.fm/category/Big-Data)
- [**生物自然**](https://kexue.fm/category/Biology)
- [**图片摄影**](https://kexue.fm/category/Photograph)
- [**问题百科**](https://kexue.fm/category/Questions)
- [**生活/情感**](https://kexue.fm/category/Life-Feeling)
- [**资源共享**](https://kexue.fm/category/Resources)

[首页](https://kexue.fm) [信息时代](https://kexue.fm/category/Big-Data) 让研究人员绞尽脑汁的Transformer位置编码

3Feb

# [让研究人员绞尽脑汁的Transformer位置编码](https://kexue.fm/archives/8130)

By 苏剑林 \|
2021-02-03 \|
149575位读者\|

不同于RNN、CNN等模型，对于Transformer模型来说，位置编码的加入是必不可少的，因为纯粹的Attention模块是无法捕捉输入顺序的，即无法区分不同位置的Token。为此我们大体有两个选择：1、想办法将位置信息融入到输入中，这构成了绝对位置编码的一般做法；2、想办法微调一下Attention结构，使得它有能力分辨不同位置的Token，这构成了相对位置编码的一般做法。

虽然说起来主要就是绝对位置编码和相对位置编码两大类，但每一类其实又能衍生出各种各样的变种，为此研究人员可算是煞费苦心、绞尽脑汁了，此外还有一些不按套路出牌的位置编码。本文就让我们来欣赏一下研究人员为了更好地表达位置信息所构建出来的“八仙过海，各显神通”般的编码方案。

## 绝对位置编码 [\#](https://kexue.fm/archives/8130\#%E7%BB%9D%E5%AF%B9%E4%BD%8D%E7%BD%AE%E7%BC%96%E7%A0%81)

形式上来看，绝对位置编码是相对简单的一种方案，但即便如此，也不妨碍各路研究人员的奇思妙想，也有不少的变种。一般来说，绝对位置编码会加到输入中：在输入的第$k$个向量$\\boldsymbol{x}\_k$中加入位置向量$\\boldsymbol{p}\_k$变为$\\boldsymbol{x}\_k + \\boldsymbol{p}\_k$，其中$\\boldsymbol{p}\_k$只依赖于位置编号$k$。

### 训练式 [\#](https://kexue.fm/archives/8130\#%E8%AE%AD%E7%BB%83%E5%BC%8F)

很显然，绝对位置编码的一个最朴素方案是不特意去设计什么，而是直接将位置编码当作可训练参数，比如最大长度为512，编码维度为768，那么就初始化一个$512\\times 768$的矩阵作为位置向量，让它随着训练过程更新。现在的BERT、GPT等模型所用的就是这种位置编码，事实上它还可以追溯得更早，比如2017年Facebook的 [《Convolutional Sequence to Sequence Learning》](https://papers.cool/arxiv/1705.03122) 就已经用到了它。

对于这种训练式的绝对位置编码，一般的认为它的缺点是没有外推性，即如果预训练最大长度为512的话，那么最多就只能处理长度为512的句子，再长就处理不了了。当然，也可以将超过512的位置向量随机初始化，然后继续微调。但笔者最近的研究表明，通过层次分解的方式，可以使得绝对位置编码能外推到足够长的范围，同时保持还不错的效果，细节请参考笔者之前的博文 [《层次分解位置编码，让BERT可以处理超长文本》](https://kexue.fm/archives/7947)。因此，其实外推性也不是绝对位置编码的明显缺点。

### 三角式 [\#](https://kexue.fm/archives/8130\#%E4%B8%89%E8%A7%92%E5%BC%8F)

三角函数式位置编码，一般也称为Sinusoidal位置编码，是Google的论文 [《Attention is All You Need》](https://papers.cool/arxiv/1706.03762) 所提出来的一个显式解：

\\begin{equation}\\left\\{\\begin{aligned}&\\boldsymbol{p}\_{k,2i}=\\sin\\Big(k/10000^{2i/d}\\Big)\\\

&\\boldsymbol{p}\_{k, 2i+1}=\\cos\\Big(k/10000^{2i/d}\\Big)

\\end{aligned}\\right.\\end{equation}

其中$\\boldsymbol{p}\_{k,2i},\\boldsymbol{p}\_{k,2i+1}$分别是位置$k$的编码向量的第$2i,2i+1$个分量，$d$是位置向量的维度。

很明显，三角函数式位置编码的特点是有显式的生成规律，因此可以期望于它有一定的外推性。另外一个使用它的理由是：由于$\\sin(\\alpha+\\beta)=\\sin\\alpha\\cos\\beta+\\cos\\alpha\\sin\\beta$以及$\\cos(\\alpha+\\beta)=\\cos\\alpha\\cos\\beta-\\sin\\alpha\\sin\\beta$，这表明位置$\\alpha+\\beta$的向量可以表示成位置$\\alpha$和位置$\\beta$的向量组合，这提供了表达相对位置信息的可能性。但很奇怪的是，现在我们很少能看到直接使用这种形式的绝对位置编码的工作，原因不详。

### 递归式 [\#](https://kexue.fm/archives/8130\#%E9%80%92%E5%BD%92%E5%BC%8F)

原则上来说，RNN模型不需要位置编码，它在结构上就自带了学习到位置信息的可能性（因为递归就意味着我们可以训练一个“数数”模型），因此，如果在输入后面先接一层RNN，然后再接Transformer，那么理论上就不需要加位置编码了。同理，我们也可以用RNN模型来学习一种绝对位置编码，比如从一个向量$\\boldsymbol{p}\_0$出发，通过递归格式$\\boldsymbol{p}\_{k+1}=f(\\boldsymbol{p}\_k)$来得到各个位置的编码向量。

ICML 2020的论文 [《Learning to Encode Position for Transformer with Continuous Dynamical Model》](https://papers.cool/arxiv/2003.09229) 把这个思想推到了极致，它提出了用微分方程（ODE）$d\\boldsymbol{p}\_t/dt=\\boldsymbol{h}(\\boldsymbol{p}\_t,t)$的方式来建模位置编码，该方案称之为FLOATER。显然，FLOATER也属于递归模型，函数$\\boldsymbol{h}(\\boldsymbol{p}\_t,t)$可以通过神经网络来建模，因此这种微分方程也称为神经微分方程，关于它的工作最近也逐渐多了起来。

理论上来说，基于递归模型的位置编码也具有比较好的外推性，同时它也比三角函数式的位置编码有更好的灵活性（比如容易证明三角函数式的位置编码就是FLOATER的某个特解）。但是很明显，递归形式的位置编码牺牲了一定的并行性，可能会带速度瓶颈。

### 相乘式 [\#](https://kexue.fm/archives/8130\#%E7%9B%B8%E4%B9%98%E5%BC%8F)

刚才我们说到，输入$\\boldsymbol{x}\_k$与绝对位置编码$\\boldsymbol{p}\_k$的组合方式一般是$\\boldsymbol{x}\_k + \\boldsymbol{p}\_k$，那有没有“不一般”的组合方式呢？比如$\\boldsymbol{x}\_k \\otimes \\boldsymbol{p}\_k$（逐位相乘）？我们平时在搭建模型的时候，对于融合两个向量有多种方式，相加、相乘甚至拼接都是可以考虑的，怎么大家在做绝对位置编码的时候，都默认只考虑相加了？

很抱歉，笔者也不知道答案。可能大家默认选择相加是因为向量的相加具有比较鲜明的几何意义，但是对于深度学习模型来说，这种几何意义其实没有什么实际的价值。最近笔者看到的一个实验显示，似乎将“加”换成“乘”，也就是$\\boldsymbol{x}\_k \\otimes \\boldsymbol{p}\_k$的方式，似乎比$\\boldsymbol{x}\_k + \\boldsymbol{p}\_k$能取得更好的结果。具体效果笔者也没有完整对比过，只是提供这么一种可能性。关于实验来源，可以参考 [《中文语言模型研究：(1) 乘性位置编码》](https://zhuanlan.zhihu.com/p/183234823)。

## 相对位置编码 [\#](https://kexue.fm/archives/8130\#%E7%9B%B8%E5%AF%B9%E4%BD%8D%E7%BD%AE%E7%BC%96%E7%A0%81)

相对位置并没有完整建模每个输入的位置信息，而是在算Attention的时候考虑当前位置与被Attention的位置的相对距离，由于自然语言一般更依赖于相对位置，所以相对位置编码通常也有着优秀的表现。对于相对位置编码来说，它的灵活性更大，更加体现出了研究人员的“天马行空”。

### 经典式 [\#](https://kexue.fm/archives/8130\#%E7%BB%8F%E5%85%B8%E5%BC%8F)

相对位置编码起源于Google的论文 [《Self-Attention with Relative Position Representations》](https://papers.cool/arxiv/1803.02155)，华为开源的NEZHA模型也用到了这种位置编码，后面各种相对位置编码变体基本也是依葫芦画瓢的简单修改。

一般认为，相对位置编码是由绝对位置编码启发而来，考虑一般的带绝对位置编码的Attention：

\\begin{equation}\\left\\{\\begin{aligned}

\\boldsymbol{q}\_i =&\\, (\\boldsymbol{x}\_i + \\boldsymbol{p}\_i)\\boldsymbol{W}\_Q \\\

\\boldsymbol{k}\_j =&\\, (\\boldsymbol{x}\_j + \\boldsymbol{p}\_j)\\boldsymbol{W}\_K \\\

\\boldsymbol{v}\_j =&\\, (\\boldsymbol{x}\_j + \\boldsymbol{p}\_j)\\boldsymbol{W}\_V \\\

a\_{i,j} =&\\, softmax\\left(\\boldsymbol{q}\_i \\boldsymbol{k}\_j^{\\top}\\right)\\\

\\boldsymbol{o}\_i =&\\, \\sum\_j a\_{i,j}\\boldsymbol{v}\_j

\\end{aligned}\\right.\\end{equation}

其中$softmax$对$j$那一维归一化，这里的向量都是指行向量。我们初步展开$\\boldsymbol{q}\_i \\boldsymbol{k}\_j^{\\top}$：

\\begin{equation}

\\boldsymbol{q}\_i \\boldsymbol{k}\_j^{\\top} = \\left(\\boldsymbol{x}\_i + \\boldsymbol{p}\_i\\right)\\boldsymbol{W}\_Q \\boldsymbol{W}\_K^{\\top}\\left(\\boldsymbol{x}\_j + \\boldsymbol{p}\_j\\right)^{\\top} = \\left(\\boldsymbol{x}\_i \\boldsymbol{W}\_Q + \\boldsymbol{p}\_i \\boldsymbol{W}\_Q\\right)\\left(\\boldsymbol{W}\_K^{\\top}\\boldsymbol{x}\_j^{\\top} + \\boldsymbol{W}\_K^{\\top}\\boldsymbol{p}\_j^{\\top}\\right)

\\end{equation}

为了引入相对位置信息，Google把第一项位置去掉，第二项$\\boldsymbol{p}\_j \\boldsymbol{W}\_K$改为二元位置向量$\\boldsymbol{R}\_{i,j}^{K}$，变成

\\begin{equation}

a\_{i,j} = softmax\\left(\\boldsymbol{x}\_i \\boldsymbol{W}\_Q\\left(\\boldsymbol{x}\_j\\boldsymbol{W}\_K + \\color{green}{\\boldsymbol{R}\_{i,j}^K}\\right)^{\\top}\\right)

\\end{equation}

以及$\\boldsymbol{o}\_i =\\sum\\limits\_j a\_{i,j}\\boldsymbol{v}\_j = \\sum\\limits\_j a\_{i,j}(\\boldsymbol{x}\_j\\boldsymbol{W}\_V + \\boldsymbol{p}\_j\\boldsymbol{W}\_V)$中的$\\boldsymbol{p}\_j \\boldsymbol{W}\_V$换成$\\boldsymbol{R}\_{i,j}^{V}$：

\\begin{equation}\\boldsymbol{o}\_i = \\sum\_j a\_{i,j}\\left(\\boldsymbol{x}\_j\\boldsymbol{W}\_V + \\color{green}{\\boldsymbol{R}\_{i,j}^{V}}\\right)

\\end{equation}

所谓相对位置，是将本来依赖于二元坐标$(i,j)$的向量$\\boldsymbol{R}\_{i,j}^{K},\\boldsymbol{R}\_{i,j}^{V}$，改为只依赖于相对距离$i-j$，并且通常来说会进行截断，以适应不同任意的距离

\\begin{equation}\\begin{aligned}

\\boldsymbol{R}\_{i,j}^{K} = \\boldsymbol{p}\_K\\left\[\\text{clip}(i-j, p\_{\\min}, p\_{\\max})\\right\]\\\

\\boldsymbol{R}\_{i,j}^{V} = \\boldsymbol{p}\_V\\left\[\\text{clip}(i-j, p\_{\\min}, p\_{\\max})\\right\]

\\end{aligned}\\label{eq:rp-clip}\\end{equation}

这样一来，只需要有限个位置编码，就可以表达出任意长度的相对位置（因为进行了截断），不管$\\boldsymbol{p}\_K,\\boldsymbol{p}\_V$是选择可训练式的还是三角函数式的，都可以达到处理任意长度文本的需求。

### XLNET式 [\#](https://kexue.fm/archives/8130\#XLNET%E5%BC%8F)

XLNET式位置编码其实源自Transformer-XL的论文 [《Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context》](https://papers.cool/arxiv/1901.02860)，只不过因为使用了Transformer-XL架构的 [XLNET](https://papers.cool/arxiv/1906.08237) 模型并在一定程度上超过了BERT后，Transformer-XL才算广为人知，因此这种位置编码通常也被冠以XLNET之名。

XLNET式位置编码源于对上述$\\boldsymbol{q}\_i \\boldsymbol{k}\_j^{\\top}$的完全展开：

\\begin{equation}

\\boldsymbol{q}\_i \\boldsymbol{k}\_j^{\\top} = \\boldsymbol{x}\_i \\boldsymbol{W}\_Q \\boldsymbol{W}\_K^{\\top}\\boldsymbol{x}\_j^{\\top} + \\boldsymbol{x}\_i \\boldsymbol{W}\_Q \\boldsymbol{W}\_K^{\\top}\\boldsymbol{p}\_j^{\\top} + \\boldsymbol{p}\_i \\boldsymbol{W}\_Q \\boldsymbol{W}\_K^{\\top}\\boldsymbol{x}\_j^{\\top} + \\boldsymbol{p}\_i \\boldsymbol{W}\_Q \\boldsymbol{W}\_K^{\\top}\\boldsymbol{p}\_j^{\\top}\\label{eq:qk-exp}

\\end{equation}

Transformer-XL的做法很简单，直接将$\\boldsymbol{p}\_j$替换为相对位置向量$\\boldsymbol{R}\_{i-j}$，至于两个$\\boldsymbol{p}\_i$，则干脆替换为两个可训练的向量$\\boldsymbol{u},\\boldsymbol{v}$：

\\begin{equation}\\boldsymbol{x}\_i \\boldsymbol{W}\_Q \\boldsymbol{W}\_K^{\\top}\\boldsymbol{x}\_j^{\\top} + \\boldsymbol{x}\_i \\boldsymbol{W}\_Q \\boldsymbol{W}\_K^{\\top}\\color{green}{\\boldsymbol{R}\_{i-j}^{\\top}} + \\color{red}{\\boldsymbol{u}}\\boldsymbol{W}\_Q \\boldsymbol{W}\_K^{\\top}\\boldsymbol{x}\_j^{\\top} + \\color{red}{\\boldsymbol{v}} \\boldsymbol{W}\_Q \\boldsymbol{W}\_K^{\\top}\\color{green}{\\boldsymbol{R}\_{i-j}^{\\top}}

\\end{equation}

该编码方式中的$\\boldsymbol{R}\_{i-j}$没有像式$\\eqref{eq:rp-clip}$那样进行截断，而是直接用了Sinusoidal式的生成方案，由于$\\boldsymbol{R}\_{i-j}$的编码空间与$\\boldsymbol{x}\_j$不一定相同，所以$\\boldsymbol{R}\_{i-j}$前面的$\\boldsymbol{W}\_K^{\\top}$换了另一个独立的矩阵$\\boldsymbol{W}\_{K,R}^{\\top}$，还有$\\color{red}{\\boldsymbol{u}}\\boldsymbol{W}\_Q$ 、$\\color{red}{\\boldsymbol{v}} \\boldsymbol{W}\_Q$可以直接合并为单个$\\color{red}{\\boldsymbol{u}}$ 、$\\color{red}{\\boldsymbol{v}}$，所以最终使用的式子是

\\begin{equation}\\boldsymbol{x}\_i \\boldsymbol{W}\_Q \\boldsymbol{W}\_K^{\\top}\\boldsymbol{x}\_j^{\\top} + \\boldsymbol{x}\_i \\boldsymbol{W}\_Q \\boldsymbol{W}\_{K,R}^{\\top}\\color{green}{\\boldsymbol{R}\_{i-j}^{\\top}} + \\color{red}{\\boldsymbol{u}}\\boldsymbol{W}\_K^{\\top}\\boldsymbol{x}\_j^{\\top} + \\color{red}{\\boldsymbol{v}} \\boldsymbol{W}\_{K,R}^{\\top}\\color{green}{\\boldsymbol{R}\_{i-j}^{\\top}}

\\end{equation}

此外，$\\boldsymbol{v}\_j$上的位置偏置就直接去掉了，即直接令$\\boldsymbol{o}\_i = \\sum\\limits\_j a\_{i,j}\\boldsymbol{x}\_j\\boldsymbol{W}\_V$。似乎从这个工作开始，后面的相对位置编码都只加到Attention矩阵上去，而不加到$\\boldsymbol{v}\_j$上去了。

### T5式 [\#](https://kexue.fm/archives/8130\#T5%E5%BC%8F)

T5模型出自文章 [《Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer》](https://papers.cool/arxiv/1910.10683)，里边用到了一种更简单的相对位置编码。思路依然源自展开式$\\eqref{eq:qk-exp}$，如果非要分析每一项的含义，那么可以分别理解为“输入-输入”、“输入-位置”、“位置-输入”、“位置-位置”四项注意力的组合。如果我们认为输入信息与位置信息应该是独立（解耦）的，那么它们就不应该有过多的交互，所以“输入-位置”、“位置-输入”两项Attention可以删掉，而$\\boldsymbol{p}\_i \\boldsymbol{W}\_Q \\boldsymbol{W}\_K^{\\top}\\boldsymbol{p}\_j^{\\top}$实际上只是一个只依赖于$(i,j)$的标量，我们可以直接将它作为参数训练出来，即简化为

\\begin{equation}\\boldsymbol{x}\_i \\boldsymbol{W}\_Q \\boldsymbol{W}\_K^{\\top}\\boldsymbol{x}\_j^{\\top} + \\color{green}{\\boldsymbol{\\beta}\_{i,j}}\\end{equation}

说白了，它仅仅是在Attention矩阵的基础上加一个可训练的偏置项而已，而跟XLNET式一样，在$\\boldsymbol{v}\_j$上的位置偏置则直接被去掉了。包含同样的思想的还有微软在ICLR 2021的论文 [《Rethinking Positional Encoding in Language Pre-training》](https://papers.cool/arxiv/2006.15595) 中提出的TUPE位置编码。

比较“别致”的是，不同于常规位置编码对将$\\boldsymbol{\\beta}\_{i,j}$视为$i-j$的函数并进行截断的做法，T5对相对位置进行了一个“分桶”处理，即相对位置是$i-j$的位置实际上对应的是$f(i-j)$位置，映射关系如下：

\\begin{array}{c\|c\|c\|c\|c\|c\|c\|c\|c\|c\|c\|c\|c\|c\|c\|c\|c}

\\hline

i - j & 0 & 1 & 2 & 3 & 4 & 5 & 6 & 7 & 8 & 9 & 10 & 11 & 12 & 13 & 14 & 15\\\

\\hline

f(i-j) & 0 & 1 & 2 & 3 & 4 & 5 & 6 & 7 & 8 & 8 & 8 & 8 & 9 & 9 & 9 & 9 \\\

\\hline

i - j & 16 & 17 & 18 & 19 & 20 & 21 & 22 & 23 & 24 & 25 & 26 & 27 & 28 & 29 & 30 & \\cdots\\\

\\hline

f(i-j) & 10 & 10 & 10 & 10 & 10 & 10 & 10 & 11 & 11 & 11 & 11 & 11 & 11 & 11 & 11 & \\cdots \\\

\\hline\\end{array}

具体的映射代码，读者自行看源码就好。这个设计的思路其实也很直观，就是比较邻近的位置（0～7），我们需要比较得精细一些，所以给它们都分配一个独立的位置编码，至于稍远的位置（比如8～11），我们不用区分得太清楚，所以它们可以共用一个位置编码，距离越远，共用的范围就可以越大，直到达到指定范围再clip。

### DeBERTa式 [\#](https://kexue.fm/archives/8130\#DeBERTa%E5%BC%8F)

DeBERTa也是微软搞的，去年6月就发出来了，论文为 [《DeBERTa: Decoding-enhanced BERT with Disentangled Attention》](https://papers.cool/arxiv/2006.03654)，最近又小小地火了一把，一是因为它正式中了ICLR 2021，二则是它登上 [SuperGLUE](https://super.gluebenchmark.com/) 的榜首，成绩稍微超过了T5。

其实DeBERTa的主要改进也是在位置编码上，同样还是从展开式$\\eqref{eq:qk-exp}$出发，T5是干脆去掉了第2、3项，只保留第4项并替换为相对位置编码，而DeBERTa则刚刚相反，它扔掉了第4项，保留第2、3项并且替换为相对位置编码（果然，科研就是枚举所有的排列组合看哪个最优）：

\\begin{equation}

\\boldsymbol{q}\_i \\boldsymbol{k}\_j^{\\top} = \\boldsymbol{x}\_i \\boldsymbol{W}\_Q \\boldsymbol{W}\_K^{\\top}\\boldsymbol{x}\_j^{\\top} + \\boldsymbol{x}\_i \\boldsymbol{W}\_Q \\boldsymbol{W}\_K^{\\top}\\color{green}{\\boldsymbol{R}\_{i,j}^{\\top}} + \\color{green}{\\boldsymbol{R}\_{j,i}} \\boldsymbol{W}\_Q \\boldsymbol{W}\_K^{\\top}\\boldsymbol{x}\_j^{\\top}

\\end{equation}

至于$\\boldsymbol{R}\_{i,j}$的设计也是像式$\\eqref{eq:rp-clip}$那样进行截断的，没有特别的地方。

不过，DeBERTa比较有意思的地方，是提供了使用相对位置和绝对位置编码的一个新视角，它指出NLP的大多数任务可能都只需要相对位置信息，但确实有些场景下绝对位置信息更有帮助，于是它将整个模型分为两部分来理解。以Base版的MLM预训练模型为例，它一共有13层，前11层只是用相对位置编码，这部分称为Encoder，后面2层加入绝对位置信息，这部分它称之为Decoder，还弄了个简称EMD（Enhanced Mask Decoder）；至于下游任务的微调截断，则是使用前11层的Encoder加上1层的Decoder来进行。

SuperGLUE上的成绩肯定了DeBERTa的价值，但是它论文的各种命名真的是让人觉得极度不适，比如它自称的“Encoder”、“Decoder”就很容易让人误解这是一个Seq2Seq模型，比如EMD这个简称也跟Earth Mover's Distance重名。虽然有时候重名是不可避免的，但它重的名都是ML界大家都比较熟悉的对象，相当容易引起误解，真不知道作者是怎么想的...

## 其他位置编码 [\#](https://kexue.fm/archives/8130\#%E5%85%B6%E4%BB%96%E4%BD%8D%E7%BD%AE%E7%BC%96%E7%A0%81)

绝对位置编码和相对位置编码虽然花样百出，但仍然算是经典范围内，从上述介绍中我们依然可以体会到满满的套路感。除此之外，还有一些并不按照常规套路出牌，它们同样也表达了位置编码。

### CNN式 [\#](https://kexue.fm/archives/8130\#CNN%E5%BC%8F)

尽管经典的将CNN用于NLP的工作 [《Convolutional Sequence to Sequence Learning》](https://papers.cool/arxiv/1705.03122) 往里边加入了位置编码，但我们知道一般的CNN模型尤其是图像中的CNN模型，都是没有另外加位置编码的，那CNN模型究竟是怎么捕捉位置信息的呢？

如果让笔者来回答，那么答案可能是卷积核的各项异性导致了它能分辨出不同方向的相对位置。不过ICLR 2020的论文 [《How Much Position Information Do Convolutional Neural Networks Encode?》](https://papers.cool/arxiv/2001.08248) 给出了一个可能让人比较意外的答案：CNN模型的位置信息，是Zero Padding泄漏的！

我们知道，为了使得卷积编码过程中的feature保持一定的大小，我们通常会对输入padding一定的0，而这篇论文显示该操作导致模型有能力识别位置信息。也就是说，卷积核的各向异性固然重要，但是最根本的是zero padding的存在，那么可以想象，实际上提取的是当前位置与padding的边界的相对距离。

不过，这个能力依赖于CNN的局部性，像Attention这种全局的无先验结构并不适用，如果只关心Transformer位置编码方案的读者，这就权当是扩展一下视野吧。

### 复数式 [\#](https://kexue.fm/archives/8130\#%E5%A4%8D%E6%95%B0%E5%BC%8F)

复数式位置编码可谓是最特立独行的一种位置编码方案了，它来自ICLR 2020的论文 [《Encoding word order in complex embeddings》](https://papers.cool/arxiv/1912.12333)。论文的主要思想是结合复数的性质以及一些基本原理，推导出了它的位置编码形式（Complex Order）为：

\\begin{equation}\\left\[r\_{j, 1} e^{\\text{i}\\left(\\omega\_{j, 1} k+\\theta\_{j, 1}\\right)}, \\ldots, r\_{j, 2} e^{\\text{i}\\left(\\omega\_{j, 2} k+\\theta\_{j, 2}\\right)}, \\cdots, r\_{j, d} e^{\\text{i}\\left(\\omega\_{j, d} k+\\theta\_{j, d}\\right)}\\right\]\\label{eq:complex}\\end{equation}

这里的$\\text{i}$是虚数单位，$j$代表某个词，$k$代表该词所在的位置，而

\\begin{equation}\\begin{aligned}

\\boldsymbol{r}\_j =&\\, \[r\_{j, 1},r\_{j, 2},\\cdots,r\_{j, d}\]\\\

\\boldsymbol{\\omega}\_j =&\\, \[\\omega\_{j, 1},\\omega\_{j, 2},\\cdots,\\omega\_{j, d}\]\\\

\\boldsymbol{\\theta}\_j =&\\, \[\\theta\_{j, 1},\\theta\_{j, 2},\\cdots,\\theta\_{j, d}\]\\\

\\end{aligned}\\end{equation}

代表词$j$的三组词向量。你没看错，它确实假设每个词有三组跟位置无关的词向量了（当然可以按照某种形式进行参数共享，使得它退化为两组甚至一组），然后跟位置$k$相关的词向量就按照上述公式运算。

你以为引入多组词向量就是它最特立独行的地方了？并不是！我们看到式$\\eqref{eq:complex}$还是复数形式，你猜它接下来怎么着？将它实数化？非也，它是将它直接用于复数模型！也就是说，它走的是一条复数模型路线，不仅仅输入的Embedding层是复数的，里边的每一层Transformer都是复数的，它还实现和对比了复数版的Fasttext、LSTM、CNN等模型！这篇文章的一作是Benyou Wang，可以搜到他的相关工作基本上都是围绕着复数模型展开的，可谓复数模型的铁杆粉了～

### 融合式 [\#](https://kexue.fm/archives/8130\#%E8%9E%8D%E5%90%88%E5%BC%8F)

无偶独有，利用复数的形式，笔者其实也构思了一种比较巧的位置编码，它可以将绝对位置编码与相对位置编码融于一体，分享在此，有兴趣的读者欢迎一起交流研究。

简单起见，我们先假设$\\boldsymbol{q}\_m,\\boldsymbol{k}\_n$是所在位置分别为$m,n$的二维行向量，既然是二维，那么我们可以将它当作复数来运算。我们知道，Attention关键之处在于向量的内积，用复数表示为

\\begin{equation}\\langle \\boldsymbol{q}\_m, \\boldsymbol{k}\_n\\rangle = \\text{Re}\\left\[\\boldsymbol{q}\_m \\boldsymbol{k}\_n^\*\\right\]\\end{equation}

其中$^\*$是共轭复数，右端的乘法是普通的复数乘法，$\\text{Re}\[\]$表示取结果的实部。上式也就是说：

> 两个二维向量的内积，等于把它们当复数看时，一个复数与另一个复数的共轭的乘积实部。

如果我们将$\\boldsymbol{q}\_m,\\boldsymbol{k}\_n$分别乘以$e^{\\text{i}m\\theta},e^{\\text{i}n\\theta}$变成$\\boldsymbol{q}\_m e^{\\text{i}m\\theta}, \\boldsymbol{k}\_n e^{\\text{i}n\\theta}$，那么就相当于给它们配上了绝对位置编码了（因为显式地依赖绝对位置$m,n$），然后放到内积里边，我们有

\\begin{equation}\\langle \\boldsymbol{q}\_m e^{\\text{i}m\\theta}, \\boldsymbol{k}\_n e^{\\text{i}n\\theta}\\rangle = \\text{Re}\\left\[\\left(\\boldsymbol{q}\_m e^{\\text{i}m\\theta}\\right) \\left(\\boldsymbol{k}\_n e^{\\text{i}n\\theta}\\right)^\*\\right\] = \\text{Re}\\left\[\\boldsymbol{q}\_m \\boldsymbol{k}\_n^\* e^{\\text{i}(m-n)\\theta}\\right\]\\end{equation}

相当有意思的是，内积只依赖于相对位置$m-n$！这就巧妙地将绝对位置与相对位置融合在一起了。

注意，我们没有像Complex Order那么“疯狂”，上述运算本质上还是在实数范畴内的，只不过我们是借助复数来完成了某些推导而已。由上述结果可知，对于位置为$n$的二维实数向量$\[x,y\]$，我们当它复数来运算，乘以$e^{\\text{i}n\\theta}$，得到恒等式

\\begin{equation}(x + y\\text{i})e^{\\text{i}n\\theta} = (x \\cos n\\theta - y\\sin n\\theta) + \\text{i} (x \\sin n\\theta + y\\cos n\\theta)\\end{equation}

这也就是意味着，通过

\\begin{equation}\\begin{pmatrix}x \\\ y\\end{pmatrix}

\\to

\\begin{pmatrix}x \\cos n\\theta - y\\sin n\\theta \\\ x \\sin n\\theta + y\\cos n\\theta

\\end{pmatrix} = \\begin{pmatrix}x \\\ y

\\end{pmatrix}\\cos n\\theta + \\begin{pmatrix}-y \\\ x

\\end{pmatrix}\\sin n\\theta\\end{equation}

来赋予$\[x, y\]$绝对位置信息，那么在Attention运算的时候也等价于相对位置编码。如果是多于二维的向量，可以考虑每两维为一组进行同样的运算，每一组的$\\theta$可以不一样。

这样一来，我们得到了一种融绝对位置与相对位置于一体的位置编码方案，从形式上看它有点像乘性的绝对位置编码，通过在$\\boldsymbol{q},\\boldsymbol{k}$中施行该位置编码，那么效果就等价于相对位置编码，而如果还需要显式的绝对位置信息，则可以同时在$\\boldsymbol{v}$上也施行这种位置编码。总的来说，我们通过绝对位置的操作，可以达到绝对位置的效果，也能达到相对位置的效果，初步实验显示它是可以work的，但还没有充分验证，欢迎大家尝试交流。

## 文章内容小结 [\#](https://kexue.fm/archives/8130\#%E6%96%87%E7%AB%A0%E5%86%85%E5%AE%B9%E5%B0%8F%E7%BB%93)

本文汇总了一些位置编码的工作，大体分为绝对式、相对式、非套路式三种，从中我们可以看到各种神奇的操作。最后，笔者分享了自己构思的一种融合绝对位置与相对位置的编码方案，供有兴趣的读者参考。

_**转载到请包括本文地址：** [https://kexue.fm/archives/8130](https://kexue.fm/archives/8130)_

_**更详细的转载事宜请参考：**_ [《科学空间FAQ》](https://kexue.fm/archives/6508#%E6%96%87%E7%AB%A0%E5%A6%82%E4%BD%95%E8%BD%AC%E8%BD%BD/%E5%BC%95%E7%94%A8)

**如果您还有什么疑惑或建议，欢迎在下方评论区继续讨论。**

**如果您觉得本文还不错，欢迎 [分享](https://kexue.fm/archives/8130#share)/ [打赏](https://kexue.fm/archives/8130#pay) 本文。打赏并非要从中获得收益，而是希望知道科学空间获得了多少读者的真心关注。当然，如果你无视它，也不会影响你的阅读。再次表示欢迎和感谢！**

打赏

![科学空间](https://kexue.fm/usr/themes/geekg/payment/wx.png)

微信打赏

![科学空间](https://kexue.fm/usr/themes/geekg/payment/zfb.png)

支付宝打赏

因为网站后台对打赏并无记录，因此欢迎在打赏时候备注留言。

你还可以 [**点击这里**](http://mail.qq.com/cgi-bin/qm_share?t=qm_mailme&email=tN7d1drY3drrx8H0xcWa19vZ) 或在下方评论区留言来告知你的建议或需求。

**如果您需要引用本文，请参考：**

苏剑林. (Feb. 03, 2021). 《让研究人员绞尽脑汁的Transformer位置编码 》\[Blog post\]. Retrieved from [https://kexue.fm/archives/8130](https://kexue.fm/archives/8130)

@online{kexuefm-8130,

        title={让研究人员绞尽脑汁的Transformer位置编码},

        author={苏剑林},

        year={2021},

        month={Feb},

        url={\\url{https://kexue.fm/archives/8130}},

}

分类： [信息时代](https://kexue.fm/category/Big-Data)    标签： [复数](https://kexue.fm/tag/%E5%A4%8D%E6%95%B0/), [attention](https://kexue.fm/tag/attention/), [位置编码](https://kexue.fm/tag/%E4%BD%8D%E7%BD%AE%E7%BC%96%E7%A0%81/)[68 评论](https://kexue.fm/archives/8130#comments)

< [Seq2Seq重复解码现象的理论分析尝试](https://kexue.fm/archives/8128) \| [一个二值化词向量模型，是怎么跟果蝇搭上关系的？](https://kexue.fm/archives/8159) >

### 你也许还对下面的内容感兴趣

- [Transformer升级之路：18、RoPE的底数选择原则](https://kexue.fm/archives/10122)
- [缓存与效果的极限拉扯：从MHA、MQA、GQA到MLA](https://kexue.fm/archives/10091)
- [Transformer升级之路：17、多模态位置编码的简单思考](https://kexue.fm/archives/10040)
- [时空之章：将Attention视为平方复杂度的RNN](https://kexue.fm/archives/10017)
- [“闭门造车”之多模态模型方案浅谈](https://kexue.fm/archives/9984)
- [Transformer升级之路：16、“复盘”长度外推技术](https://kexue.fm/archives/9948)
- [注意力机制真的可以“集中注意力”吗？](https://kexue.fm/archives/9889)
- [我在Performer中发现了Transformer-VQ的踪迹](https://kexue.fm/archives/9862)
- [Transformer升级之路：15、Key归一化助力长度外推](https://kexue.fm/archives/9859)
- [VQ一下Key，Transformer的复杂度就变成线性了](https://kexue.fm/archives/9844)

[发表你的看法](https://kexue.fm/archives/8130#comment_form)

1. [«](https://kexue.fm/archives/8130/comment-page-5#comments)
2. [1](https://kexue.fm/archives/8130/comment-page-1#comments)
3. ...
4. [3](https://kexue.fm/archives/8130/comment-page-3#comments)
5. [4](https://kexue.fm/archives/8130/comment-page-4#comments)
6. [5](https://kexue.fm/archives/8130/comment-page-5#comments)
7. [6](https://kexue.fm/archives/8130/comment-page-6#comments)

Tamer

January 30th, 2024

666666

[回复评论](https://kexue.fm/archives/8130/comment-page-6?replyTo=23629#respond-post-8130)

Minliang

April 19th, 2024

请问位置编码能直接用二进制码吗？比如用长度为3的向量表示位置2就是\[0,1,0\]

[回复评论](https://kexue.fm/archives/8130/comment-page-6?replyTo=24180#respond-post-8130)

[苏剑林](https://kexue.fm) 发表于
April 25th, 2024

问题不大，再加个线性投影到指定维度就行，绝对位置编码基本上也是类似。

[回复评论](https://kexue.fm/archives/8130/comment-page-6?replyTo=24197#respond-post-8130)

1. [«](https://kexue.fm/archives/8130/comment-page-5#comments)
2. [1](https://kexue.fm/archives/8130/comment-page-1#comments)
3. ...
4. [3](https://kexue.fm/archives/8130/comment-page-3#comments)
5. [4](https://kexue.fm/archives/8130/comment-page-4#comments)
6. [5](https://kexue.fm/archives/8130/comment-page-5#comments)
7. [6](https://kexue.fm/archives/8130/comment-page-6#comments)

[取消回复](https://kexue.fm/archives/8130#respond-post-8130)

你的大名

电子邮箱

个人网站（选填）

1\. 可以在评论中使用LaTeX代码，点击“预览效果”可即时查看效果，点击 [这里](https://kexue.fm/content.html) 可以查看更多内容；

2\. 可以通过点击评论楼层编号来引用该楼层；

3\. **提交评论之前，建议复制一下评论内容，避免提交失败导致辛苦打的字没了。**

### 内容速览

[绝对位置编码](https://kexue.fm/archives/8130#%E7%BB%9D%E5%AF%B9%E4%BD%8D%E7%BD%AE%E7%BC%96%E7%A0%81)
[训练式](https://kexue.fm/archives/8130#%E8%AE%AD%E7%BB%83%E5%BC%8F)
[三角式](https://kexue.fm/archives/8130#%E4%B8%89%E8%A7%92%E5%BC%8F)
[递归式](https://kexue.fm/archives/8130#%E9%80%92%E5%BD%92%E5%BC%8F)
[相乘式](https://kexue.fm/archives/8130#%E7%9B%B8%E4%B9%98%E5%BC%8F)
[相对位置编码](https://kexue.fm/archives/8130#%E7%9B%B8%E5%AF%B9%E4%BD%8D%E7%BD%AE%E7%BC%96%E7%A0%81)
[经典式](https://kexue.fm/archives/8130#%E7%BB%8F%E5%85%B8%E5%BC%8F)
[XLNET式](https://kexue.fm/archives/8130#XLNET%E5%BC%8F)
[T5式](https://kexue.fm/archives/8130#T5%E5%BC%8F)
[DeBERTa式](https://kexue.fm/archives/8130#DeBERTa%E5%BC%8F)
[其他位置编码](https://kexue.fm/archives/8130#%E5%85%B6%E4%BB%96%E4%BD%8D%E7%BD%AE%E7%BC%96%E7%A0%81)
[CNN式](https://kexue.fm/archives/8130#CNN%E5%BC%8F)
[复数式](https://kexue.fm/archives/8130#%E5%A4%8D%E6%95%B0%E5%BC%8F)
[融合式](https://kexue.fm/archives/8130#%E8%9E%8D%E5%90%88%E5%BC%8F)
[文章内容小结](https://kexue.fm/archives/8130#%E6%96%87%E7%AB%A0%E5%86%85%E5%AE%B9%E5%B0%8F%E7%BB%93)

### 智能搜索

支持整句搜索！网站自动使用 [结巴分词](https://github.com/fxsjy/jieba) 进行分词，并结合ngrams排序算法给出合理的搜索结果。

### 热门标签

[生成模型](https://kexue.fm/tag/%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B/) [attention](https://kexue.fm/tag/attention/) [模型](https://kexue.fm/tag/%E6%A8%A1%E5%9E%8B/) [语言模型](https://kexue.fm/tag/%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/) [优化](https://kexue.fm/tag/%E4%BC%98%E5%8C%96/) [概率](https://kexue.fm/tag/%E6%A6%82%E7%8E%87/) [网站](https://kexue.fm/tag/%E7%BD%91%E7%AB%99/) [转载](https://kexue.fm/tag/%E8%BD%AC%E8%BD%BD/) [微分方程](https://kexue.fm/tag/%E5%BE%AE%E5%88%86%E6%96%B9%E7%A8%8B/) [天象](https://kexue.fm/tag/%E5%A4%A9%E8%B1%A1/) [深度学习](https://kexue.fm/tag/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/) [积分](https://kexue.fm/tag/%E7%A7%AF%E5%88%86/) [分析](https://kexue.fm/tag/%E5%88%86%E6%9E%90/) [python](https://kexue.fm/tag/python/) [梯度](https://kexue.fm/tag/%E6%A2%AF%E5%BA%A6/) [无监督](https://kexue.fm/tag/%E6%97%A0%E7%9B%91%E7%9D%A3/) [力学](https://kexue.fm/tag/%E5%8A%9B%E5%AD%A6/) [节日](https://kexue.fm/tag/%E8%8A%82%E6%97%A5/) [几何](https://kexue.fm/tag/%E5%87%A0%E4%BD%95/) [文本生成](https://kexue.fm/tag/%E6%96%87%E6%9C%AC%E7%94%9F%E6%88%90/) [数论](https://kexue.fm/tag/%E6%95%B0%E8%AE%BA/) [矩阵](https://kexue.fm/tag/%E7%9F%A9%E9%98%B5/) [GAN](https://kexue.fm/tag/GAN/) [生活](https://kexue.fm/tag/%E7%94%9F%E6%B4%BB/) [扩散](https://kexue.fm/tag/%E6%89%A9%E6%95%A3/)

### 随机文章

- [BERT可以上几年级了？Seq2Seq“硬刚”小学数学应用题](https://kexue.fm/archives/7809)
- [语言模型输出端共享Embedding的重新探索](https://kexue.fm/archives/9698)
- [桃花开放了](https://kexue.fm/archives/1548)
- [不成功的尝试：将多标签交叉熵推广到“n个m分类”上去](https://kexue.fm/archives/9158)
- [又折腾网络了......](https://kexue.fm/archives/1715)
- [月底回家看彗星C/2012 S1 (ISON)](https://kexue.fm/archives/2111)
- [【中文分词系列】 7\. 深度学习分词？只需一个词典！](https://kexue.fm/archives/4245)
- [生成扩散模型漫谈（二十一）：中值定理加速ODE采样](https://kexue.fm/archives/9881)
- [【理解黎曼几何】7\. 高斯-博内公式](https://kexue.fm/archives/4033)
- [关于“平衡态公理”的更正与思考](https://kexue.fm/archives/1902)

### 最近评论

- [Wen Fei](https://kexue.fm/archives/9859/comment-page-1#comment-24558): 您好，苏神，我有一点不明白。l2 norm对所有token和batch求平均？那推理过程呢，t...
- [Wen Fei](https://kexue.fm/archives/2036/comment-page-1#comment-24557): 现在能讲一下 质量的来源了嘛？ 在相对论里，质量和能量是一样的东西，应该是不同坐标系下的投影。...
- [bill](https://kexue.fm/archives/9826/comment-page-2#comment-24556): 有人使用VQGAN的训练方法对比过VQ和FSQ吗？这应该是FSQ论文的实现方式。我训练出来无论...
- [NirVa](https://kexue.fm/archives/9978/comment-page-1#comment-24555): 为啥不做利用cookie保存star？
- [lcz](https://kexue.fm/archives/3290/comment-page-2#comment-24554): 苏神，为什么“找一个在整个实数域上都单调递增的函数，而且增长速度要快于线性增长，然后求和，最后...
- [周名远](https://kexue.fm/archives/10085/comment-page-1#comment-24553): Kiro: 很高兴你通过实践验证了SiD的稳定性。SDXL的distill目前实验还没具体开展...
- [苏剑林](https://kexue.fm/archives/9984/comment-page-2#comment-24552): 刚刷到这篇paper，它是每个像素都视为一个token，这种做法远比我说的激进，而且它自己越承...
- [苏剑林](https://kexue.fm/archives/10114/comment-page-1#comment-24551): 谢谢，已更正。
- [苏剑林](https://kexue.fm/archives/10145/comment-page-1#comment-24550): 感谢提醒。Softmax Bottleneck有所耳闻，但我个人觉得它本质上是Logits的低...
- [苏剑林](https://kexue.fm/archives/9164/comment-page-3#comment-24549): Chrome和Safari测试正常，暂时无法测试所有浏览器，抱歉。

### 友情链接

- [Cool Papers](https://papers.cool)
- [数学研发](https://bbs.emath.ac.cn)
- [Seatop](http://www.seatop.com.cn/)
- [Xiaoxia](https://xiaoxia.org/)
- [积分表-网络版](https://kexue.fm/sci/integral/index.html)
- [丝路博傲](http://blog.dvxj.com/)
- [ph4ntasy 饭特稀](http://www.ph4ntasy.com/)
- [数学之家](http://www.2math.cn/)
- [有趣天文奇观](http://interesting-sky.china-vo.org/)
- [bsky](https://bsky.spaces.ac.cn/)
- [TwistedW](http://www.twistedwg.com/)
- [godweiyang](https://godweiyang.com/)
- [AI柠檬](https://blog.ailemon.net/)
- [王登科-DK博客](https://greatdk.com)
- [ESON](https://blog.eson.org/)
- [枫之羽](https://fzhiy.net/)
- [Mathor's blog](https://wmathor.com/)
- [孙云增的博客](https://sunyunzeng.com/)
- [coding-zuo](https://coding-zuo.github.io/)
- [博科园](https://www.bokeyuan.net/)
- [孔皮皮的博客](https://www.kppkkp.top/)
- [运鹏的博客](https://yunpengtai.top/)
- [jiming.site](https://jiming.site/)
- [OmegaXYZ](https://www.omegaxyz.com/)
- [Blog by Eacls](https://www.eacls.top/)
- [申请链接](https://kexue.fm/links.html)

[![署名-非商业用途-保持一致](https://kexue.fm/usr/themes/geekg/images/cc.gif)](http://creativecommons.org/licenses/by-nc-nd/2.5/cn/) 本站采用创作共用版权协议，要求署名、非商业用途和保持一致。转载本站内容必须也遵循“ [署名-非商业用途-保持一致](http://creativecommons.org/licenses/by-nc-nd/2.5/cn/)”的创作共用协议。

© 2009-2024 Scientific Spaces. All rights reserved. Theme by [laogui](http://www.laogui.com). Powered by [Typecho](http://typecho.org). 备案号: [粤ICP备09093259号-1/2](https://beian.miit.gov.cn/)。