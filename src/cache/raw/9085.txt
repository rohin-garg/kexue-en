## SEARCH

## MENU

- [打赏](https://kexue.fm/reward.html)
- [公式](https://kexue.fm/latex.html)
- [天象](https://kexue.fm/ac.html)
- [链接](https://kexue.fm/links.html)
- [时光](https://kexue.fm/me.html)
- [博览](https://kexue.fm/science.html)
- [归档](https://kexue.fm/content.html)

## CATEGORIES

- [千奇百怪](https://kexue.fm/category/Everything)
- [天文探索](https://kexue.fm/category/Astronomy)
- [数学研究](https://kexue.fm/category/Mathematics)
- [物理化学](https://kexue.fm/category/Phy-chem)
- [信息时代](https://kexue.fm/category/Big-Data)
- [生物自然](https://kexue.fm/category/Biology)
- [图片摄影](https://kexue.fm/category/Photograph)
- [问题百科](https://kexue.fm/category/Questions)
- [生活/情感](https://kexue.fm/category/Life-Feeling)
- [资源共享](https://kexue.fm/category/Resources)

## NEWPOSTS

- [AdamW的Weight RMS的...](https://kexue.fm/archives/11404)
- [n个正态随机数的最大值的渐近估计](https://kexue.fm/archives/11390)
- [流形上的最速下降：5\. 对偶梯度下降](https://kexue.fm/archives/11388)
- [低精度Attention可能存在有...](https://kexue.fm/archives/11371)
- [MuP之上：1. 好模型的三个特征](https://kexue.fm/archives/11340)
- [随机矩阵的谱范数的快速估计](https://kexue.fm/archives/11335)
- [DiVeQ：一种非常简洁的VQ训练方案](https://kexue.fm/archives/11328)
- [为什么线性注意力要加Short C...](https://kexue.fm/archives/11320)
- [AdamW的Weight RMS的...](https://kexue.fm/archives/11307)
- [重新思考学习率与Batch Siz...](https://kexue.fm/archives/11301)

## COMMENTS

- [danyao12: 好的，感谢！](https://kexue.fm/archives/11371/comment-page-1#comment-28845)
- [ljj: 我理解了，谢谢～](https://kexue.fm/archives/10542/comment-page-1#comment-28844)
- [Andrea: 苏神好！想问下 快速估计 这里对theta\_t的要求只是维数d...](https://kexue.fm/archives/11307/comment-page-1#comment-28843)
- [苏剑林: 你的推导没错，完整结果是多一个$\\Vert\\boldsymbo...](https://kexue.fm/archives/10592/comment-page-2#comment-28842)
- [苏剑林: 不出售，抱歉。](https://kexue.fm/archives/11390/comment-page-1#comment-28841)
- [苏剑林: 不需要，对齐update\_rms就行](https://kexue.fm/archives/10739/comment-page-2#comment-28840)
- [苏剑林: \[comment=28814\]pang\[/comment\]这些...](https://kexue.fm/archives/10862/comment-page-1#comment-28839)
- [苏剑林: 是的，采样的是$z$，而$x$是给定的，直接代入就行。](https://kexue.fm/archives/8791/comment-page-1#comment-28838)
- [苏剑林: 都可以，不过事实上现在linear已经不流行除分母了，参考：h...](https://kexue.fm/archives/8265/comment-page-8#comment-28837)
- [苏剑林: 理论上是的，Stochastic Rounding后它就不会因...](https://kexue.fm/archives/11371/comment-page-1#comment-28836)

## USERLOGIN

- [登录](https://kexue.fm/admin/login.php)

[科学空间\|Scientific Spaces](https://kexue.fm)

- [登录](https://kexue.fm/admin/login.php)
- [打赏](https://kexue.fm/reward.html)
- [公式](https://kexue.fm/latex.html)
- [天象](https://kexue.fm/ac.html)
- [链接](https://kexue.fm/links.html)
- [时光](https://kexue.fm/me.html)
- [博览](https://kexue.fm/science.html)
- [归档](https://kexue.fm/content.html)

渴望成为一个小飞侠

- [欢迎订阅](https://kexue.fm/feed)
- [个性邮箱](https://kexue.fm/archives/119)
- [天象信息](https://kexue.fm/ac.html)
- [观测ISS](https://kexue.fm/archives/41)
- [LaTeX](https://kexue.fm/latex.html)
- [关于博主](https://kexue.fm/me.html)

欢迎访问“科学空间”，这里将与您共同探讨自然科学，回味人生百态；也期待大家的分享～

- [**千奇百怪** Everything](https://kexue.fm/category/Everything)
- [**天文探索** Astronomy](https://kexue.fm/category/Astronomy)
- [**数学研究** Mathematics](https://kexue.fm/category/Mathematics)
- [**物理化学** Phy-chem](https://kexue.fm/category/Phy-chem)
- [**信息时代** Big-Data](https://kexue.fm/category/Big-Data)
- [**生物自然** Biology](https://kexue.fm/category/Biology)
- [**图片摄影** Photograph](https://kexue.fm/category/Photograph)
- [**问题百科** Questions](https://kexue.fm/category/Questions)
- [**生活/情感** Life-Feeling](https://kexue.fm/category/Life-Feeling)
- [**资源共享** Resources](https://kexue.fm/category/Resources)

- [**千奇百怪**](https://kexue.fm/category/Everything)
- [**天文探索**](https://kexue.fm/category/Astronomy)
- [**数学研究**](https://kexue.fm/category/Mathematics)
- [**物理化学**](https://kexue.fm/category/Phy-chem)
- [**信息时代**](https://kexue.fm/category/Big-Data)
- [**生物自然**](https://kexue.fm/category/Biology)
- [**图片摄影**](https://kexue.fm/category/Photograph)
- [**问题百科**](https://kexue.fm/category/Questions)
- [**生活/情感**](https://kexue.fm/category/Life-Feeling)
- [**资源共享**](https://kexue.fm/category/Resources)

[首页](https://kexue.fm) [数学研究](https://kexue.fm/category/Mathematics) 从重参数的角度看离散概率分布的构建

25May

# [从重参数的角度看离散概率分布的构建](https://kexue.fm/archives/9085)

By 苏剑林 \|
2022-05-25 \|
23442位读者\|

一般来说，神经网络的输出都是无约束的，也就是值域为$\\mathbb{R}$，而为了得到有约束的输出，通常是采用加激活函数的方式。例如，如果我们想要输出一个概率分布来代表每个类别的概率，那么通常在最后加上Softmax作为激活函数。那么一个紧接着的疑问就是：除了Softmax，还有什么别的操作能生成一个概率分布吗？

在 [《漫谈重参数：从正态分布到Gumbel Softmax》](https://kexue.fm/archives/6705) 中，我们介绍了Softmax的重参数操作，本文将这个过程反过来，即先定义重参数操作，然后去反推对应的概率分布，从而得到一个理解概率分布构建的新视角。

## 问题定义 [\#](https://kexue.fm/kexue.fm\#%E9%97%AE%E9%A2%98%E5%AE%9A%E4%B9%89)

假设模型的输出向量为$\\boldsymbol{\\mu}=\[\\mu\_1,\\cdots,\\mu\_n\]\\in\\mathbb{R}^n$，不失一般性，这里假设$\\mu\_i$两两不等。我们希望通过某个变换$\\mathcal{T}$将$\\boldsymbol{\\mu}$转换为$n$元概率分布$\\boldsymbol{p}=\[p\_1,\\cdots,p\_n\]$，并保持一定的性质。比如，最基本的要求是：
\\begin{equation}{\\color{red}1.}\\,p\_i\\geq 0 \\qquad {\\color{red}2.}\\,\\sum\_i p\_i = 1 \\qquad {\\color{red}3.}\\,p\_i \\geq p\_j \\Leftrightarrow \\mu\_i \\geq \\mu\_j\\end{equation}
当然，这些要求都很平凡，只要$f$是$\\mathbb{R}\\mapsto\\mathbb{R}^+$的单调函数（对于Softmax有$f(x)=e^x$），那么变换
\\begin{equation}p\_i = \\frac{f(\\mu\_i)}{\\sum\\limits\_j f(\\mu\_j)}\\end{equation}
都可以满足上述要求。接下来我们增加一个不那么平凡的条件：
\\begin{equation}{\\color{red}4.}\\, \\mathcal{T}(\\boldsymbol{\\mu}) = \\mathcal{T}(\\boldsymbol{\\mu} + c\\boldsymbol{1})\\quad (\\forall c \\in \\mathbb{R})\\end{equation}
其中$\\boldsymbol{1}$代表全1向量，$c$则是任意常数。也就是说，$\\boldsymbol{\\mu}$的每个分量都加上同一常数后，变换的结果保持不变，提出这个条件是因为每个分量都加上一个常数后，$\\mathop{\\text{argmax}}$的结果不会改变，而$\\mathcal{T}$最好能尽量保持跟它一样的性质。容易检验Softmax是满足这个条件的，然而除了Softmax外，我们似乎很难想到别的变换了。

## 噪声扰动 [\#](https://kexue.fm/kexue.fm\#%E5%99%AA%E5%A3%B0%E6%89%B0%E5%8A%A8)

非常有意思的是，我们可以借助重参数（Reparameterization）的逆过程来构造这样的变换！假设$\\boldsymbol{\\varepsilon}=\[\\varepsilon\_1,\\cdots,\\varepsilon\_n\]$是从分布$p(\\varepsilon)$独立重复采样$n$次得到的向量，由于$\\boldsymbol{\\varepsilon}$是随机的，那么$\\mathop{\\text{argmax}}(\\boldsymbol{\\mu}+\\boldsymbol{\\varepsilon})$通常也是随机的，那么我们可以通过
\\begin{equation}p\_i = P\[\\mathop{\\text{argmax}}(\\boldsymbol{\\mu}+\\boldsymbol{\\varepsilon})=i\]\\end{equation}
来定义变换$\\mathcal{T}$。由于$\\boldsymbol{\\varepsilon}$是独立同分布的，且整个定义只跟$\\mathop{\\text{argmax}}(\\boldsymbol{\\mu}+\\boldsymbol{\\varepsilon})$有关，也就是只涉及到每个分量的相对大小，因此所定义的变换必然是满足前述4个条件的。

我们也可以通过直接算出$p\_i$的形式来判断它满足的性质。具体来说，$\\mathop{\\text{argmax}}(\\boldsymbol{\\mu}+\\boldsymbol{\\varepsilon})=i$意味着
\\begin{equation}\\mu\_i + \\varepsilon\_i > \\mu\_j + \\varepsilon\_j\\quad (\\forall j\\neq i)\\end{equation}
也就是$\\mu\_i - \\mu\_j + \\varepsilon\_i > \\varepsilon\_j$，显然$\\mu\_i$越大该式成立的可能性越大，也即$\\mu\_i$越大对应的$p\_i$越大，这便是条件$3$。具体来说，固定$\\varepsilon\_i$的情况下，满足该条件的概率是
\\begin{equation}\\int\_{-\\infty}^{\\mu\_i - \\mu\_j + \\varepsilon\_i} p(\\varepsilon\_j)d\\varepsilon\_j = \\Phi(\\mu\_i - \\mu\_j + \\varepsilon\_i)\\end{equation}
这里$\\Phi$是$p(\\varepsilon)$的累积分布函数（Cumulative Distribution Function）。由于各个$\\varepsilon\_j$都是独立同分布的，因此我们可以将概率直接连乘起来：
\\begin{equation}\\prod\_{j\\neq i} \\Phi(\\mu\_i - \\mu\_j + \\varepsilon\_i)\\end{equation}
这是固定$\\varepsilon\_i$的情况下，$\\mathop{\\text{argmax}}(\\boldsymbol{\\mu}+\\boldsymbol{\\varepsilon})=i$的概率。最后我们只需要对$\\varepsilon\_i$求平均，就可以得到$p\_i$：
\\begin{equation}p\_i = \\int\_{-\\infty}^{\\infty} p(\\varepsilon\_i)\\left\[\\prod\_{j\\neq i} \\Phi(\\mu\_i - \\mu\_j + \\varepsilon\_i)\\right\]d\\varepsilon\_i \\label{eq:pi}\\end{equation}
从$p\_i$的表达式可以看到它只依赖于相对值$\\mu\_i - \\mu\_j$，因此显然它满足定义中的条件$4$。

## 温故知新 [\#](https://kexue.fm/kexue.fm\#%E6%B8%A9%E6%95%85%E7%9F%A5%E6%96%B0)

对照 [《漫谈重参数：从正态分布到Gumbel Softmax》](https://kexue.fm/archives/6705) 中关于Gumbel Max的介绍，我们可以发现上述推导跟重参数正好相反，它是先定义了重参数的方法，然后在反向推导出对应的概率分布。

现在我们可以来重新检验一下之前的结果，即当噪声分布取 [Gumbel分布](https://en.wikipedia.org/wiki/Gumbel_distribution) 时，式$\\eqref{eq:pi}$是否能得到常规的Softmax操作。Gumbel噪声是$u\\sim U\[0,1\]$通过$\\varepsilon = -\\log(-\\log u)$变换而来，由于$u$的分布正好是$U\[0,1\]$，所以解出来$u=e^{-e^{-\\varepsilon}}$正好就是Gumbel分布的累积分布函数，即$\\Phi(\\varepsilon)=e^{-e^{-\\varepsilon}}$，而$p(\\varepsilon)$就是$\\Phi(\\varepsilon)$的导数，即$p(\\varepsilon)=\\Phi'(\\varepsilon)=e^{-\\varepsilon-e^{-\\varepsilon}}$。

将上述结果代入式$\\eqref{eq:pi}$得
\\begin{equation}\\begin{aligned}
p\_i =&\\, \\int\_{-\\infty}^{\\infty} e^{-\\varepsilon\_i-e^{-\\varepsilon\_i}} e^{-\\sum\\limits\_{j\\neq i}e^{-\\varepsilon\_i + \\mu\_j - \\mu\_i}} d\\varepsilon\_i \\\
=&\\, \\int\_{-\\infty}^0 e^{-e^{-\\varepsilon\_i}\\left(1+\\sum\\limits\_{j\\neq i}e^{\\mu\_j - \\mu\_i}\\right)} d(-e^{-\\varepsilon\_i}) \\\
=&\\, \\int\_{-\\infty}^0 e^{t\\left(1+\\sum\\limits\_{j\\neq i}e^{\\mu\_j - \\mu\_i}\\right)} dt\\\
=&\\, \\frac{1}{1+\\sum\\limits\_{j\\neq i}e^{\\mu\_j - \\mu\_i}} = \\frac{e^{\\mu\_i}}{\\sum\\limits\_j e^{\\mu\_j }}
\\end{aligned}\\end{equation}
这正好是Softmax。于是我们再次验证了Gumbel Max与Softmax的对应关系。

## 数值计算 [\#](https://kexue.fm/kexue.fm\#%E6%95%B0%E5%80%BC%E8%AE%A1%E7%AE%97)

能像Gumbel分布那样解出诸如Softmax的解析解是极其稀罕的，至少笔者目前还找不到第二例。因此，大多数情况下，我们只能用数值计算方法近似估算式$\\eqref{eq:pi}$。由于$p(\\varepsilon)=\\Phi'(\\varepsilon)$，所以我们可以直接凑微分得：
\\begin{equation}p\_i = \\int\_0^1 \\left\[\\prod\_{j\\neq i} \\Phi(\\mu\_i - \\mu\_j + \\varepsilon\_i)\\right\]d\\Phi(\\varepsilon\_i)\\end{equation}
记$t=\\Phi(\\varepsilon\_i)$，那么
\\begin{equation}\\begin{aligned}
p\_i =&\\, \\int\_0^1 \\left\[\\prod\_{j\\neq i} \\Phi(\\mu\_i - \\mu\_j + \\Phi^{-1}(t))\\right\]dt \\\
\\approx&\\, \\frac{1}{K}\\sum\_{k=1}^K\\prod\_{j\\neq i} \\Phi\\left(\\mu\_i - \\mu\_j + \\Phi^{-1}\\left(\\frac{k}{K+1}\\right)\\right)
\\end{aligned}\\end{equation}
其中$\\Phi^{-1}$是$\\Phi$的逆函数，在概率中也叫分位函数（Quantile Function、Percent Point Function等）。

从上式可以看到，只要我们知道$\\Phi$的解析式，就可以对$p\_i$进行近似计算。注意我们不需要知道$\\Phi^{-1}$的解析式，因为采样点$\\Phi^{-1}\\left(\\frac{k}{K+1}\\right)$的结果我们可以用其他数值方法提前计算好。

以标准正态分布为例，$\\Phi(x)=\\frac{1}{2} \\left(1+\\text{erf}\\left(\\frac{x}{\\sqrt{2}}\\right)\\right)$，而主流的深度学习框架基本上都自带了$\\text{erf}$函数，所以$\\Phi(x)$的计算是没有问题的；至于$\\Phi^{-1}\\left(\\frac{k}{K+1}\\right)$我们可以通过 `scipy.stats.norm.ppf` 来事先计算好。所以当$\\boldsymbol{\\varepsilon}$采样自标准正态分布时，$p\_i$的计算在主流深度学习框架中都是没问题的。

## 文章小结 [\#](https://kexue.fm/kexue.fm\#%E6%96%87%E7%AB%A0%E5%B0%8F%E7%BB%93)

本文从重参数角度对Softmax进行推广，得到了一类具备相似性质的概率归一化方法。

_**转载到请包括本文地址：** [https://kexue.fm/archives/9085](https://kexue.fm/archives/9085)_

_**更详细的转载事宜请参考：**_ [《科学空间FAQ》](https://kexue.fm/archives/6508#%E6%96%87%E7%AB%A0%E5%A6%82%E4%BD%95%E8%BD%AC%E8%BD%BD/%E5%BC%95%E7%94%A8)

**如果您还有什么疑惑或建议，欢迎在下方评论区继续讨论。**

**如果您觉得本文还不错，欢迎 [分享](https://kexue.fm/kexue.fm#share)/ [打赏](https://kexue.fm/kexue.fm#pay) 本文。打赏并非要从中获得收益，而是希望知道科学空间获得了多少读者的真心关注。当然，如果你无视它，也不会影响你的阅读。再次表示欢迎和感谢！**

打赏

微信打赏

支付宝打赏

因为网站后台对打赏并无记录，因此欢迎在打赏时候备注留言。你还可以 [**点击这里**](http://mail.qq.com/cgi-bin/qm_share?t=qm_mailme&email=tN7d1drY3drrx8H0xcWa19vZ) 或在下方评论区留言来告知你的建议或需求。

**如果您需要引用本文，请参考：**

苏剑林. (May. 25, 2022). 《从重参数的角度看离散概率分布的构建 》\[Blog post\]. Retrieved from [https://kexue.fm/archives/9085](https://kexue.fm/archives/9085)

@online{kexuefm-9085,
        title={从重参数的角度看离散概率分布的构建},
        author={苏剑林},
        year={2022},
        month={May},
        url={\\url{https://kexue.fm/archives/9085}},
}

分类： [数学研究](https://kexue.fm/category/Mathematics)    标签： [概率](https://kexue.fm/tag/%E6%A6%82%E7%8E%87/), [重参数](https://kexue.fm/tag/%E9%87%8D%E5%8F%82%E6%95%B0/)[2 评论](https://kexue.fm/archives/9085#comments)

< [当BERT-whitening引入超参数：总有一款适合你](https://kexue.fm/archives/9079) \| [如何训练你的准确率？](https://kexue.fm/archives/9098) >

### 你也许还对下面的内容感兴趣

- [n个正态随机数的最大值的渐近估计](https://kexue.fm/archives/11390)
- [一道概率不等式：盯着它到显然成立为止！](https://kexue.fm/archives/10902)
- [Softmax后传：寻找Top-K的光滑近似](https://kexue.fm/archives/10373)
- [通向最优分布之路：概率空间的最小化](https://kexue.fm/archives/10289)
- [通向概率分布之路：盘点Softmax及其替代品](https://kexue.fm/archives/10145)
- [用傅里叶级数拟合一维概率密度函数](https://kexue.fm/archives/10007)
- [随机分词再探：从Viterbi Sampling到完美采样算法](https://kexue.fm/archives/9811)
- [EMO：基于最优传输思想设计的分类损失函数](https://kexue.fm/archives/9797)
- [随机分词浅探：从Viterbi Decoding到Viterbi Sampling](https://kexue.fm/archives/9768)
- [大词表语言模型在续写任务上的一个问题及对策](https://kexue.fm/archives/9762)

[发表你的看法](https://kexue.fm/kexue.fm#comment_form)

壁橱里的小男孩

June 22nd, 2022

很有意思，但是我有一个疑惑:条件3和条件4对于神经网络中构造离散概率分布是必要的吗？似乎任意合理的构造的离散概率分布在经过训练后都可以收敛到正确值。

[回复评论](https://kexue.fm/archives/9085/comment-page-1?replyTo=19339#respond-post-9085)

[苏剑林](https://kexue.fm) 发表于
June 24th, 2022

不是完全必要的，主要是看理论需求吧。在这两点条件之下，$\\boldsymbol{\\mu}$和$\\boldsymbol{p}$保持了尽可能多的共性，方便理论研究。

[回复评论](https://kexue.fm/archives/9085/comment-page-1?replyTo=19348#respond-post-9085)

[取消回复](https://kexue.fm/archives/9085#respond-post-9085)

你的大名

电子邮箱

个人网站（选填）

1\. 可以使用LaTeX代码，点击“预览效果”可查看效果；2. 可以通过点击评论楼层编号来引用该楼层；3. 网站可能会有点卡，如非确认评论失败，请 **不要重复点击提交**。

### 内容速览

[问题定义](https://kexue.fm/kexue.fm#%E9%97%AE%E9%A2%98%E5%AE%9A%E4%B9%89)
[噪声扰动](https://kexue.fm/kexue.fm#%E5%99%AA%E5%A3%B0%E6%89%B0%E5%8A%A8)
[温故知新](https://kexue.fm/kexue.fm#%E6%B8%A9%E6%95%85%E7%9F%A5%E6%96%B0)
[数值计算](https://kexue.fm/kexue.fm#%E6%95%B0%E5%80%BC%E8%AE%A1%E7%AE%97)
[文章小结](https://kexue.fm/kexue.fm#%E6%96%87%E7%AB%A0%E5%B0%8F%E7%BB%93)

### 智能搜索

支持整句搜索！网站自动使用 [结巴分词](https://github.com/fxsjy/jieba) 进行分词，并结合ngrams排序算法给出合理的搜索结果。

### 热门标签

[生成模型](https://kexue.fm/tag/%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B/) [attention](https://kexue.fm/tag/attention/) [优化](https://kexue.fm/tag/%E4%BC%98%E5%8C%96/) [语言模型](https://kexue.fm/tag/%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/) [模型](https://kexue.fm/tag/%E6%A8%A1%E5%9E%8B/) [网站](https://kexue.fm/tag/%E7%BD%91%E7%AB%99/) [概率](https://kexue.fm/tag/%E6%A6%82%E7%8E%87/) [梯度](https://kexue.fm/tag/%E6%A2%AF%E5%BA%A6/) [矩阵](https://kexue.fm/tag/%E7%9F%A9%E9%98%B5/) [转载](https://kexue.fm/tag/%E8%BD%AC%E8%BD%BD/) [优化器](https://kexue.fm/tag/%E4%BC%98%E5%8C%96%E5%99%A8/) [微分方程](https://kexue.fm/tag/%E5%BE%AE%E5%88%86%E6%96%B9%E7%A8%8B/) [分析](https://kexue.fm/tag/%E5%88%86%E6%9E%90/) [天象](https://kexue.fm/tag/%E5%A4%A9%E8%B1%A1/) [深度学习](https://kexue.fm/tag/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/) [积分](https://kexue.fm/tag/%E7%A7%AF%E5%88%86/) [python](https://kexue.fm/tag/python/) [力学](https://kexue.fm/tag/%E5%8A%9B%E5%AD%A6/) [无监督](https://kexue.fm/tag/%E6%97%A0%E7%9B%91%E7%9D%A3/) [扩散](https://kexue.fm/tag/%E6%89%A9%E6%95%A3/) [几何](https://kexue.fm/tag/%E5%87%A0%E4%BD%95/) [节日](https://kexue.fm/tag/%E8%8A%82%E6%97%A5/) [生活](https://kexue.fm/tag/%E7%94%9F%E6%B4%BB/) [文本生成](https://kexue.fm/tag/%E6%96%87%E6%9C%AC%E7%94%9F%E6%88%90/) [数论](https://kexue.fm/tag/%E6%95%B0%E8%AE%BA/)

### 随机文章

- [OCR技术浅探：5. 文本切割](https://kexue.fm/archives/3823)
- [Nyströmformer：基于矩阵分解的线性化Attention方案](https://kexue.fm/archives/8180)
- [中国探索火星，还等两年！](https://kexue.fm/archives/138)
- [“未解之谜”：为何不讲中点矩形法则？](https://kexue.fm/archives/1668)
- [无监督语义相似度哪家强？我们做了个比较全面的评测](https://kexue.fm/archives/8321)
- [从对称角度看代数方程](https://kexue.fm/archives/1336)
- [分享一个slide：花式自然语言处理](https://kexue.fm/archives/4823)
- [威力巨大的“有向线段”](https://kexue.fm/archives/689)
- [\[更正\]一道经典不等式的美妙证明](https://kexue.fm/archives/1420)
- [从DCGAN到SELF-MOD：GAN的模型架构发展一览](https://kexue.fm/archives/6549)

### 最近评论

- [danyao12](https://kexue.fm/archives/11371/comment-page-1#comment-28845): 好的，感谢！
- [ljj](https://kexue.fm/archives/10542/comment-page-1#comment-28844): 我理解了，谢谢～
- [Andrea](https://kexue.fm/archives/11307/comment-page-1#comment-28843): 苏神好！想问下 快速估计 这里对theta\_t的要求只是维数d要足够大，而不一定需要是全部的参...
- [苏剑林](https://kexue.fm/archives/10592/comment-page-2#comment-28842): 你的推导没错，完整结果是多一个$\\Vert\\boldsymbol{G}\_t\\Vert^{\\da...
- [苏剑林](https://kexue.fm/archives/11390/comment-page-1#comment-28841): 不出售，抱歉。
- [苏剑林](https://kexue.fm/archives/10739/comment-page-2#comment-28840): 不需要，对齐update\_rms就行
- [苏剑林](https://kexue.fm/archives/10862/comment-page-1#comment-28839): \[comment=28814\]pang\[/comment\]这些BERT时代的常见操作了，之前写...
- [苏剑林](https://kexue.fm/archives/8791/comment-page-1#comment-28838): 是的，采样的是$z$，而$x$是给定的，直接代入就行。
- [苏剑林](https://kexue.fm/archives/8265/comment-page-8#comment-28837): 都可以，不过事实上现在linear已经不流行除分母了，参考：https://kexue.fm/...
- [苏剑林](https://kexue.fm/archives/11371/comment-page-1#comment-28836): 理论上是的，Stochastic Rounding后它就不会因为微小的、实际上几乎可以忽略的小...

### 友情链接

- [Cool Papers](https://papers.cool)
- [数学研发](https://bbs.emath.ac.cn)
- [Seatop](http://www.seatop.com.cn/)
- [Xiaoxia](https://xiaoxia.org/)
- [积分表-网络版](https://kexue.fm/sci/integral/index.html)
- [丝路博傲](http://blog.dvxj.com/)
- [数学之家](http://www.2math.cn/)
- [有趣天文奇观](http://interesting-sky.china-vo.org/)
- [TwistedW](http://www.twistedwg.com/)
- [godweiyang](https://godweiyang.com/)
- [AI柠檬](https://blog.ailemon.net/)
- [王登科-DK博客](https://greatdk.com)
- [ESON](https://blog.eson.org/)
- [枫之羽](https://fzhiy.net/)
- [Mathor's blog](https://wmathor.com/)
- [coding-zuo](https://coding-zuo.github.io/)
- [博科园](https://www.bokeyuan.net/)
- [孔皮皮的博客](https://www.kppkkp.top/)
- [运鹏的博客](https://yunpengtai.top/)
- [jiming.site](https://jiming.site/)
- [OmegaXYZ](https://www.omegaxyz.com/)
- [EAI猩球](https://www.robotech.ink/)
- [文举的博客](https://liwenju0.com/)
- [申请链接](https://kexue.fm/links.html)

本站采用创作共用版权协议，要求署名、非商业用途和保持一致。转载本站内容必须也遵循“ [署名-非商业用途-保持一致](http://creativecommons.org/licenses/by-nc-nd/2.5/cn/)”的创作共用协议。
© 2009-2025 Scientific Spaces. All rights reserved. Theme by [laogui](http://www.laogui.com). Powered by [Typecho](http://typecho.org). 备案号: [粤ICP备09093259号-1/2](https://beian.miit.gov.cn/)。