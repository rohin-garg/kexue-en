## SEARCH

## MENU

- [打赏](https://kexue.fm/reward.html)
- [公式](https://kexue.fm/latex.html)
- [天象](https://kexue.fm/ac.html)
- [链接](https://kexue.fm/links.html)
- [时光](https://kexue.fm/me.html)
- [博览](https://kexue.fm/science.html)
- [归档](https://kexue.fm/content.html)

## CATEGORIES

- [千奇百怪](https://kexue.fm/category/Everything)
- [天文探索](https://kexue.fm/category/Astronomy)
- [数学研究](https://kexue.fm/category/Mathematics)
- [物理化学](https://kexue.fm/category/Phy-chem)
- [信息时代](https://kexue.fm/category/Big-Data)
- [生物自然](https://kexue.fm/category/Biology)
- [图片摄影](https://kexue.fm/category/Photograph)
- [问题百科](https://kexue.fm/category/Questions)
- [生活/情感](https://kexue.fm/category/Life-Feeling)
- [资源共享](https://kexue.fm/category/Resources)

## NEWPOSTS

- [重新思考学习率与Batch Siz...](https://kexue.fm/archives/11285)
- [重新思考学习率与Batch Siz...](https://kexue.fm/archives/11280)
- [为什么Adam的Update RM...](https://kexue.fm/archives/11267)
- [重新思考学习率与Batch Siz...](https://kexue.fm/archives/11260)
- [Cool Papers更新：简单适...](https://kexue.fm/archives/11250)
- [流形上的最速下降：4\. Muon ...](https://kexue.fm/archives/11241)
- [ReLU/GeLU/Swish的一...](https://kexue.fm/archives/11233)
- [流形上的最速下降：3\. Muon ...](https://kexue.fm/archives/11221)
- [流形上的最速下降：2\. Muon ...](https://kexue.fm/archives/11215)
- [基于树莓派Zero2W搭建一个随身旁路由](https://kexue.fm/archives/11206)

## COMMENTS

- [zyp: 您好，我在反向过程中有个疑问，您在\
μ(xt)=1αt(xt−...](https://kexue.fm/archives/9119/comment-page-13#comment-28572)
- [歪门正道: 喔，我看前面的文章设定有n >> d，如果deltanet中分...](https://kexue.fm/archives/11033/comment-page-2#comment-28571)
- [歪门正道: 你好苏老师，没有理解单位下三角矩阵求逆，复杂度是n^2那一步。...](https://kexue.fm/archives/11033/comment-page-2#comment-28570)
- [doggy: 苏神好，请问能提供调 kimi 的 prompt 吗？我在浏览...](https://kexue.fm/archives/9907/comment-page-4#comment-28569)
- [markchin: 苏佬发下大图吗？看了《银河铁道之夜》后深受触动，1459294...](https://kexue.fm/archives/443/comment-page-1#comment-28567)
- [kkkfm: 苏老师您好，我想请问一下您提到的使用Muon可以用更大的lea...](https://kexue.fm/archives/10592/comment-page-2#comment-28565)
- [flyingDog: 将K,V视作语料对(k1,v1),(k2,v2),⋯,(kt,...](https://kexue.fm/archives/11033/comment-page-2#comment-28564)
- [简明: 请教一下各位，（6）是有限项之和从而支撑抽样定理，但（6）的推...](https://kexue.fm/archives/3266/comment-page-1#comment-28563)
- [胡韬: 特征值的模长小于一？](https://kexue.fm/archives/11158/comment-page-1#comment-28561)
- [guanchanghao: 苏老师，最开始的物理中热传导方程的解为什么是左边那幅图呢？？没...](https://kexue.fm/archives/9359/comment-page-1#comment-28560)

## USERLOGIN

- [登录](https://kexue.fm/admin/login.php)

[科学空间\|Scientific Spaces](https://kexue.fm)

- [登录](https://kexue.fm/admin/login.php)
- [打赏](https://kexue.fm/reward.html)
- [公式](https://kexue.fm/latex.html)
- [天象](https://kexue.fm/ac.html)
- [链接](https://kexue.fm/links.html)
- [时光](https://kexue.fm/me.html)
- [博览](https://kexue.fm/science.html)
- [归档](https://kexue.fm/content.html)

渴望成为一个小飞侠

- [欢迎订阅](https://kexue.fm/feed)
- [个性邮箱](https://kexue.fm/archives/119)
- [天象信息](https://kexue.fm/ac.html)
- [观测ISS](https://kexue.fm/archives/41)
- [LaTeX](https://kexue.fm/latex.html)
- [关于博主](https://kexue.fm/me.html)

欢迎访问“科学空间”，这里将与您共同探讨自然科学，回味人生百态；也期待大家的分享～

- [**千奇百怪** Everything](https://kexue.fm/category/Everything)
- [**天文探索** Astronomy](https://kexue.fm/category/Astronomy)
- [**数学研究** Mathematics](https://kexue.fm/category/Mathematics)
- [**物理化学** Phy-chem](https://kexue.fm/category/Phy-chem)
- [**信息时代** Big-Data](https://kexue.fm/category/Big-Data)
- [**生物自然** Biology](https://kexue.fm/category/Biology)
- [**图片摄影** Photograph](https://kexue.fm/category/Photograph)
- [**问题百科** Questions](https://kexue.fm/category/Questions)
- [**生活/情感** Life-Feeling](https://kexue.fm/category/Life-Feeling)
- [**资源共享** Resources](https://kexue.fm/category/Resources)

- [**千奇百怪**](https://kexue.fm/category/Everything)
- [**天文探索**](https://kexue.fm/category/Astronomy)
- [**数学研究**](https://kexue.fm/category/Mathematics)
- [**物理化学**](https://kexue.fm/category/Phy-chem)
- [**信息时代**](https://kexue.fm/category/Big-Data)
- [**生物自然**](https://kexue.fm/category/Biology)
- [**图片摄影**](https://kexue.fm/category/Photograph)
- [**问题百科**](https://kexue.fm/category/Questions)
- [**生活/情感**](https://kexue.fm/category/Life-Feeling)
- [**资源共享**](https://kexue.fm/category/Resources)

[首页](https://kexue.fm) [信息时代](https://kexue.fm/category/Big-Data) 能量视角下的GAN模型（三）：生成模型=能量模型

10May

# [能量视角下的GAN模型（三）：生成模型=能量模型](https://kexue.fm/archives/6612)

By 苏剑林 \|
2019-05-10 \|
67160位读者\|

本文的模型在ImageNet(128x128)上的条件生成效果

今天要介绍的结果还是跟能量模型相关，来自论文 [《Implicit Generation and Generalization in Energy-Based Models》](https://papers.cool/arxiv/1903.08689)。当然，它已经跟GAN没有什么关系了，但是跟 [本系列第二篇](https://kexue.fm/archives/6331) 所介绍的能量模型关系较大，所以还是把它放到这个系列好了。

我当初留意到这篇论文，是因为机器之心的报导 [《MIT本科学神重启基于能量的生成模型，新框架堪比GAN》](https://mp.weixin.qq.com/s/c1zzP9_Ds1oCkFitLZLrCg)，但是说实在的，这篇文章没什么意思，说句不中听的，就是炒冷饭系列，媒体的标题也算中肯，是“重启”。这篇文章就是指出能量模型实际上就是某个特定的Langevin方程的静态解，然后就用这个Langevin方程来实现采样，有了采样过程也就可以完成能量模型的训练，这些理论都是现成的，所以这个过程我在学习随机微分方程的时候都想过，我相信很多人也都想过。因此，我觉得作者的贡献就是把这个直白的想法通过一系列炼丹技巧实现了。

但不管怎样，能训练出来也是一件很不错的事情，另外对于之前没了解过相关内容的读者来说，这确实也算是一个不错的能量模型案例，所以我论文的整体思路整理一下，让读者能够更全面地理解能量模型。

## 能量分布 [\#](https://kexue.fm/kexue.fm\#%E8%83%BD%E9%87%8F%E5%88%86%E5%B8%83)

跟 [《能量视角下的GAN模型（二）：GAN＝“分析”＋“采样”》](https://kexue.fm/archives/6331) 一样，假设我们有一批数据$x\_1,x\_2,\\dots,x\_n\\sim p(x)$，我们希望用一个概率模型去拟合它，我们选取的模型为
\\begin{equation}q\_{\\theta}(x) = \\frac{e^{-U\_{\\theta}(x)}}{Z\_{\\theta}}\\end{equation}
其中$U\_{\\theta}$是带参数$\\theta$的未定函数，我们称为“能量函数”，而$Z\_{\\theta}$是归一化因子（配分函数）
\\begin{equation}Z\_{\\theta} = \\int e^{-U\_{\\theta}(x)}dx\\label{eq:z}\\end{equation}
这样的分布可以称为“能量分布”，在物理中也被称为“玻尔兹曼分布”。

为了求出参数$\\theta$，我们先定义对数似然函数：
\\begin{equation}\\mathbb{E}\_{x\\sim p(x)} \\big\[\\log q\_{\\theta}(x)\\big\]\\end{equation}
我们希望它越大越好，也就是希望
\\begin{equation}L\_{\\theta}=\\mathbb{E}\_{x\\sim p(x)} \\big\[-\\log q\_{\\theta}(x)\\big\]\\end{equation}
越小越好，为此，我们对$L\_{\\theta}$使用梯度下降。我们有（具体推导参考第二篇）
\\begin{equation}\\nabla\_{\\theta}\\log q\_{\\theta}(x)=-\\nabla\_{\\theta} U\_{\\theta}(x)+\\mathbb{E}\_{x\\sim q\_{\\theta}(x)}\\big\[\\nabla\_{\\theta} U\_{\\theta}(x)\\big\]\\end{equation}
所以
\\begin{equation}\\nabla\_{\\theta} L\_{\\theta} = \\mathbb{E}\_{x\\sim p(x)}\\big\[\\nabla\_{\\theta} U\_{\\theta}(x)\\big\] - \\mathbb{E}\_{x\\sim q\_{\\theta}(x)}\\big\[\\nabla\_{\\theta} U\_{\\theta}(x)\\big\]\\label{eq:q-grad}\\end{equation}
这意味着梯度下降的更新公式是
\\begin{equation}\\theta \\leftarrow \\theta - \\varepsilon \\Big(\\mathbb{E}\_{x\\sim p(x)}\\big\[\\nabla\_{\\theta} U\_{\\theta}(x)\\big\] - \\mathbb{E}\_{x\\sim q\_{\\theta}(x)}\\big\[\\nabla\_{\\theta} U\_{\\theta}(x)\\big\]\\Big)\\end{equation}

## Langevin方程 [\#](https://kexue.fm/kexue.fm\#Langevin%E6%96%B9%E7%A8%8B)

在式$\\eqref{eq:q-grad}$中，$\\mathbb{E}\_{x\\sim p(x)}\\big\[\\nabla\_{\\theta} U\_{\\theta}(x)\\big\]$是容易估算的，直接抽样一批真实数据来计算就行了；但是$\\mathbb{E}\_{x\\sim q\_{\\theta}(x)}\\big\[\\nabla\_{\\theta} U\_{\\theta}(x)\\big\]$却很困难，因为我们不知道怎么实现从$q\_{\\theta}(x)$中采样。

[《能量视角下的GAN模型（二）：GAN＝“分析”＋“采样”》](https://kexue.fm/archives/6331) 中的思路是定义另外一个容易采样的分布$q\_{\\varphi}(x)$，然后改为从$q\_{\\varphi}(x)$中采样，同时去缩小$q\_{\\varphi}(x)$和$q\_{\\theta}(x)$的差异，使得$q\_{\\varphi}(x)$确实可以成为$q\_{\\theta}(x)$的一个良好近似。但这篇论文不一样，它直接从能量模型对应的Langevin方程采样。

其实思路很简单，在上一篇文章已经已经提到过，对于Langevin方程：
\\begin{equation}x\_{t+1} = x\_t - \\frac{1}{2}\\varepsilon \\nabla\_x U(x\_t) + \\sqrt{\\varepsilon}\\alpha,\\quad \\alpha \\sim \\mathcal{N}(\\alpha;0,1)\\label{eq:sde}\\end{equation}
当$\\varepsilon\\to 0$且$t\\to\\infty$时，序列$\\{x\_t\\}$所服从的分布就是$q\_{\\theta}(x)$，换句话说，$q\_{\\theta}(x)$是该Langevin方程的静态分布，再换句话说，那就是给定$U\_{\\theta}(x)$后（$q\_{\\theta}(x)$也确定了），式$\\eqref{eq:sde}$的递归过程就可以帮我们得到一批从$q\_{\\theta}(x)$采样的样本来。

嗯嗯，有了这个采样过程，那就完事了呀，首先$\\mathbb{E}\_{x\\sim q\_{\\theta}(x)}\\big\[\\nabla\_{\\theta} U\_{\\theta}(x)\\big\]$可以估算了，因此能量模型可以完成训练了；训练完成之后，还是由式$\\eqref{eq:sde}$帮助我们从中采样出一批新样本了，这样就完成生成过程了。

## 模型细节 [\#](https://kexue.fm/kexue.fm\#%E6%A8%A1%E5%9E%8B%E7%BB%86%E8%8A%82)

当然，理论是这样子，实际操作肯定有很多细节，而且少不了炼丹。我当初也就只是思考到这一步，觉得里边的边角问题太多，难以解决，就没有继续做下去了。但作者坚持下去了，终究是跑通了，这一点我是很佩服的。

首先是作者往模型$U\_{\\theta}(x)$加入了 [谱归一化](https://kexue.fm/archives/6051)，而$U\_{\\theta}(x)$本身就相当于GAN中的判别器地位，所以加入谱归一化是可以理解的。其次，在训练的过程中，用的能量函数不是$U\_{\\theta}(x)$，而是加上一个小的L2正则：$U\_{\\theta}(x) + \\lambda U\_{\\theta}^2(x)$，其中$\\lambda$是一个小的正常数，作者的意思是这样会使得整个loss更光滑，训练起来更稳定（使用时还是$U\_{\\theta}(x)$）。

然后，回到采样问题，采样是通过式$\\eqref{eq:sde}$进行的，它是一个迭代过程，既然是迭代就需要初始值。然而如果直接从随机分布（比如均匀分布）中采样随机向量作为初始值，作者提到会出现模式单一的问题，即迭代出来的图片形式比较单一，导致采样不充分，所以作者维护了一个Buffer，它把历史的的采样结果缓存起来，作为下一次采样的候选初始值。

总的来说，模型的更新过程如下：

> 假定数据样本分布为$p(x)$，选定迭代步长$\\varepsilon$（参考值为1/200）、迭代步数$K$（参考值20～50）和batch size $N$，Buffer记为$\\mathcal{B}$，初始化是空集。
>
> 循环执行，直到收敛：
>
> 循环执行，得到一批真假样本：
>       1、从$p(x)$中采样一个真样本$x\_r$，加入到当前批；
>       2、以95%的概率从$\\mathcal{B}$（或者以5%的概率从均匀分布）选取一个样本作为初始值$x\_{f,0}$；
>       3、以$x\_{f,0}$为初始值，迭代式$\\eqref{eq:sde}$共$K$步，得到$x\_{f,K}$；
>       4、将$x\_{f,K}$作为假样本$x\_f$，加入到当前批，同时加入到$\\mathcal{B}$。
>
> 有了真假样本后，执行一步优化器，优化目标为：
> $\\frac{1}{N}\\sum\\limits\_{x\_r, x\_f} \\Big\\{U\_{\\theta}(x\_r) - U\_{\\theta}(x\_f) + \\lambda \\big\[U\_{\\theta}^2(x\_r) - U\_{\\theta}^2(x\_f)\\big\]\\Big\\}$

而训练完成后的采样，同样需要维护Buffer，并且作者为了保证多样性，他将模型分别训练几次，得到若干个不同权重的统一模型，然后同时从这若干个模型中采样，并且共享、共同维护一个Buffer。其他细节问题大家直接看原论文即可，因为不打算复现，所以就不考究了。

作者实现： [https://github.com/openai/ebm\_code\_release](https://github.com/openai/ebm_code_release)

## 个人总结 [\#](https://kexue.fm/kexue.fm\#%E4%B8%AA%E4%BA%BA%E6%80%BB%E7%BB%93)

总的来说，我认为这是一篇中规中矩、差强人意的论文。首先思路和理论都是成熟的，能量模型与Langevin方程的关系前人早已得出，不算什么创新；但是能够攻克其中的细节难题，真正把这个思路落实下去，也不是一件容易的事情，体现了作者在生成模型领域深厚的（炼丹）功底。从能量模型的角度看，也可以说是为训练复杂的能量模型提供了一个可行的方案。

至于效果上，可以说它媲美GAN，也可以说比不上GAN。作者主要在Cifar10和ImageNet上做实验，这两个数据集当然很难，可以说一般的GAN都生成不好，从效果图来看，确实可以PK大多数GAN了，在Cifar10上明显完胜Glow。说它比不上，则是感觉它太有技巧性了，不够优雅，比如Langevin方程的所导致的采样思路，我感觉就没有什么底，维护一个Buffer的做法，虽然实践效果还可以，但显然工程味道太浓了....

在cifar10的无条件生成上的效果图

_**转载到请包括本文地址：** [https://kexue.fm/archives/6612](https://kexue.fm/archives/6612)_

_**更详细的转载事宜请参考：**_ [《科学空间FAQ》](https://kexue.fm/archives/6508#%E6%96%87%E7%AB%A0%E5%A6%82%E4%BD%95%E8%BD%AC%E8%BD%BD/%E5%BC%95%E7%94%A8)

**如果您还有什么疑惑或建议，欢迎在下方评论区继续讨论。**

**如果您觉得本文还不错，欢迎 [分享](https://kexue.fm/kexue.fm#share)/ [打赏](https://kexue.fm/kexue.fm#pay) 本文。打赏并非要从中获得收益，而是希望知道科学空间获得了多少读者的真心关注。当然，如果你无视它，也不会影响你的阅读。再次表示欢迎和感谢！**

打赏

微信打赏

支付宝打赏

因为网站后台对打赏并无记录，因此欢迎在打赏时候备注留言。你还可以 [**点击这里**](http://mail.qq.com/cgi-bin/qm_share?t=qm_mailme&email=tN7d1drY3drrx8H0xcWa19vZ) 或在下方评论区留言来告知你的建议或需求。

**如果您需要引用本文，请参考：**

苏剑林. (May. 10, 2019). 《能量视角下的GAN模型（三）：生成模型=能量模型 》\[Blog post\]. Retrieved from [https://kexue.fm/archives/6612](https://kexue.fm/archives/6612)

@online{kexuefm-6612,
        title={能量视角下的GAN模型（三）：生成模型=能量模型},
        author={苏剑林},
        year={2019},
        month={May},
        url={\\url{https://kexue.fm/archives/6612}},
}

分类： [信息时代](https://kexue.fm/category/Big-Data)    标签： [概率](https://kexue.fm/tag/%E6%A6%82%E7%8E%87/), [能量](https://kexue.fm/tag/%E8%83%BD%E9%87%8F/), [优化](https://kexue.fm/tag/%E4%BC%98%E5%8C%96/), [生成模型](https://kexue.fm/tag/%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B/)[9 评论](https://kexue.fm/archives/6612#comments)

< [从动力学角度看优化算法（四）：GAN的第三个阶段](https://kexue.fm/archives/6583) \| [函数光滑化杂谈：不可导函数的可导逼近](https://kexue.fm/archives/6620) >

### 你也许还对下面的内容感兴趣

- [QK-Clip：让Muon在Scaleup之路上更进一步](https://kexue.fm/archives/11126)
- [Transformer升级之路：21、MLA好在哪里?（下）](https://kexue.fm/archives/11111)
- [线性注意力简史：从模仿、创新到反哺](https://kexue.fm/archives/11033)
- [生成扩散模型漫谈（三十）：从瞬时速度到平均速度](https://kexue.fm/archives/10958)
- [MoE环游记：5、均匀分布的反思](https://kexue.fm/archives/10945)
- [Transformer升级之路：20、MLA好在哪里?（上）](https://kexue.fm/archives/10907)
- [一道概率不等式：盯着它到显然成立为止！](https://kexue.fm/archives/10902)
- [MoE环游记：4、难处应当多投入](https://kexue.fm/archives/10815)
- [生成扩散模型漫谈（二十九）：用DDPM来离散编码](https://kexue.fm/archives/10711)
- [细水长flow之TARFLOW：流模型满血归来？](https://kexue.fm/archives/10667)

[发表你的看法](https://kexue.fm/kexue.fm#comment_form)

[白杨](http://baiyangscut)

May 13th, 2019

厉害！！！

[回复评论](https://kexue.fm/archives/6612/comment-page-1?replyTo=11147#respond-post-6612)

willianmshun

August 18th, 2019

其实传统上最大似然求ebm参数时，大家之所以不愿意用一般的mcmc或者langevin dynamics/mala 来采样估计z是因为太慢了。。。尤其是复杂后验游走的步数太长。。所以这篇文章在中之前估计会碰到不少钉子。。毕竟这可是这个领域根深蒂固的观点（逃

[回复评论](https://kexue.fm/archives/6612/comment-page-1?replyTo=11838#respond-post-6612)

[苏剑林](https://kexue.fm) 发表于
August 29th, 2019

确实如此～

[回复评论](https://kexue.fm/archives/6612/comment-page-1?replyTo=11883#respond-post-6612)

[shouldsee](http://www.catsmile.info)

June 30th, 2022

又过了三年了!

[回复评论](https://kexue.fm/archives/6612/comment-page-1?replyTo=19376#respond-post-6612)

[shouldsee](http://www.catsmile.info)

June 30th, 2022

感谢分享,这篇好像少加了个GAN关键字哦

[回复评论](https://kexue.fm/archives/6612/comment-page-1?replyTo=19377#respond-post-6612)

我就是来试试

October 19th, 2022

为啥真假样本数目1:1就可以了呀？按理来说给定一个真样本，我们需要S个假样本对partition function的梯度做一个比较好的Monte Carlo估计？

[回复评论](https://kexue.fm/archives/6612/comment-page-1?replyTo=20136#respond-post-6612)

[苏剑林](https://kexue.fm) 发表于
October 21st, 2022

从梯度公式$(7)$来说，对真假样本分布是平权的，两者应该采样同样多的数目才比较公平。

[回复评论](https://kexue.fm/archives/6612/comment-page-1?replyTo=20151#respond-post-6612)

Jason YU

November 1st, 2022

你好，请问这篇讲的是能量视角下的GAN模型（二）：GAN＝“分析”＋“采样”（二）说中的

“当然，直接从能量函数和式(25)中采样x可能不大现实，因为x维度（常见的情景下，x代表图片）过大，可控性难以保证。另一方面，式(25)最后一项是高斯噪声，所以只要ε≠0，那么结果必然是有噪声的，图片真实性也难以保证“吗

[回复评论](https://kexue.fm/archives/6612/comment-page-1?replyTo=20233#respond-post-6612)

[苏剑林](https://kexue.fm) 发表于
November 3rd, 2022

不大理解你想表达什么。

[回复评论](https://kexue.fm/archives/6612/comment-page-1?replyTo=20255#respond-post-6612)

[取消回复](https://kexue.fm/archives/6612#respond-post-6612)

你的大名

电子邮箱

个人网站（选填）

1\. 可以使用LaTeX代码，点击“预览效果”可查看效果；2. 可以通过点击评论楼层编号来引用该楼层；3. 网站可能会有点卡，如非确认评论失败，请 **不要重复点击提交**。

### 内容速览

[能量分布](https://kexue.fm/kexue.fm#%E8%83%BD%E9%87%8F%E5%88%86%E5%B8%83)
[Langevin方程](https://kexue.fm/kexue.fm#Langevin%E6%96%B9%E7%A8%8B)
[模型细节](https://kexue.fm/kexue.fm#%E6%A8%A1%E5%9E%8B%E7%BB%86%E8%8A%82)
[个人总结](https://kexue.fm/kexue.fm#%E4%B8%AA%E4%BA%BA%E6%80%BB%E7%BB%93)

### 智能搜索

支持整句搜索！网站自动使用 [结巴分词](https://github.com/fxsjy/jieba) 进行分词，并结合ngrams排序算法给出合理的搜索结果。

### 热门标签

[生成模型](https://kexue.fm/tag/%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B/) [attention](https://kexue.fm/tag/attention/) [优化](https://kexue.fm/tag/%E4%BC%98%E5%8C%96/) [语言模型](https://kexue.fm/tag/%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/) [模型](https://kexue.fm/tag/%E6%A8%A1%E5%9E%8B/) [网站](https://kexue.fm/tag/%E7%BD%91%E7%AB%99/) [概率](https://kexue.fm/tag/%E6%A6%82%E7%8E%87/) [梯度](https://kexue.fm/tag/%E6%A2%AF%E5%BA%A6/) [转载](https://kexue.fm/tag/%E8%BD%AC%E8%BD%BD/) [矩阵](https://kexue.fm/tag/%E7%9F%A9%E9%98%B5/) [微分方程](https://kexue.fm/tag/%E5%BE%AE%E5%88%86%E6%96%B9%E7%A8%8B/) [优化器](https://kexue.fm/tag/%E4%BC%98%E5%8C%96%E5%99%A8/) [分析](https://kexue.fm/tag/%E5%88%86%E6%9E%90/) [天象](https://kexue.fm/tag/%E5%A4%A9%E8%B1%A1/) [深度学习](https://kexue.fm/tag/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/) [积分](https://kexue.fm/tag/%E7%A7%AF%E5%88%86/) [python](https://kexue.fm/tag/python/) [力学](https://kexue.fm/tag/%E5%8A%9B%E5%AD%A6/) [无监督](https://kexue.fm/tag/%E6%97%A0%E7%9B%91%E7%9D%A3/) [扩散](https://kexue.fm/tag/%E6%89%A9%E6%95%A3/) [几何](https://kexue.fm/tag/%E5%87%A0%E4%BD%95/) [节日](https://kexue.fm/tag/%E8%8A%82%E6%97%A5/) [生活](https://kexue.fm/tag/%E7%94%9F%E6%B4%BB/) [文本生成](https://kexue.fm/tag/%E6%96%87%E6%9C%AC%E7%94%9F%E6%88%90/) [数论](https://kexue.fm/tag/%E6%95%B0%E8%AE%BA/)

### 随机文章

- [最小熵原理（三）：“飞象过河”之句模版和语言结构](https://kexue.fm/archives/5577)
- [中秋节快乐！](https://kexue.fm/archives/1725)
- [\[欧拉数学\]伯努利级数及相关级数的总结](https://kexue.fm/archives/3680)
- [VQ的又一技巧：给编码表加一个线性变换](https://kexue.fm/archives/10519)
- [基于双向LSTM和迁移学习的seq2seq核心实体识别](https://kexue.fm/archives/3942)
- [基于GRU和AM-Softmax的句子相似度模型](https://kexue.fm/archives/5743)
- [力学系统及其对偶性（三）](https://kexue.fm/archives/2177)
- [基于DGCNN和概率图的轻量级信息抽取模型](https://kexue.fm/archives/6671)
- [均值不等式的两个巧妙证明](https://kexue.fm/archives/1716)
- [美国科学家用3000幅照片拼接夜空全景](https://kexue.fm/archives/244)

### 最近评论

- [zyp](https://kexue.fm/archives/9119/comment-page-13#comment-28572): 您好，我在反向过程中有个疑问，您在
μ(xt)=1αt(xt−βtϵθ(xt,t))（8）
x...
- [歪门正道](https://kexue.fm/archives/11033/comment-page-2#comment-28571): 喔，我看前面的文章设定有n >> d，如果deltanet中分chunk选取chunksize...
- [歪门正道](https://kexue.fm/archives/11033/comment-page-2#comment-28570): 你好苏老师，没有理解单位下三角矩阵求逆，复杂度是n^2那一步。(I+B)U = V 这里的U和...
- [doggy](https://kexue.fm/archives/9907/comment-page-4#comment-28569): 苏神好，请问能提供调 kimi 的 prompt 吗？我在浏览一篇文章（Unsupervise...
- [markchin](https://kexue.fm/archives/443/comment-page-1#comment-28567): 苏佬发下大图吗？看了《银河铁道之夜》后深受触动，1459294449@qq.com,万分感谢。
- [kkkfm](https://kexue.fm/archives/10592/comment-page-2#comment-28565): 苏老师您好，我想请问一下您提到的使用Muon可以用更大的learning rate，是不是说明...
- [flyingDog](https://kexue.fm/archives/11033/comment-page-2#comment-28564): 将K,V视作语料对(k1,v1),(k2,v2),⋯,(kt,vt)，根据这些语料训练得到一个...
- [简明](https://kexue.fm/archives/3266/comment-page-1#comment-28563): 请教一下各位，（6）是有限项之和从而支撑抽样定理，但（6）的推导是基于频域是离散级数（4），这...
- [胡韬](https://kexue.fm/archives/11158/comment-page-1#comment-28561): 特征值的模长小于一？
- [guanchanghao](https://kexue.fm/archives/9359/comment-page-1#comment-28560): 苏老师，最开始的物理中热传导方程的解为什么是左边那幅图呢？？没有看懂那幅图的含义

### 友情链接

- [Cool Papers](https://papers.cool)
- [数学研发](https://bbs.emath.ac.cn)
- [Seatop](http://www.seatop.com.cn/)
- [Xiaoxia](https://xiaoxia.org/)
- [积分表-网络版](https://kexue.fm/sci/integral/index.html)
- [丝路博傲](http://blog.dvxj.com/)
- [数学之家](http://www.2math.cn/)
- [有趣天文奇观](http://interesting-sky.china-vo.org/)
- [TwistedW](http://www.twistedwg.com/)
- [godweiyang](https://godweiyang.com/)
- [AI柠檬](https://blog.ailemon.net/)
- [王登科-DK博客](https://greatdk.com)
- [ESON](https://blog.eson.org/)
- [枫之羽](https://fzhiy.net/)
- [Mathor's blog](https://wmathor.com/)
- [coding-zuo](https://coding-zuo.github.io/)
- [博科园](https://www.bokeyuan.net/)
- [孔皮皮的博客](https://www.kppkkp.top/)
- [运鹏的博客](https://yunpengtai.top/)
- [jiming.site](https://jiming.site/)
- [OmegaXYZ](https://www.omegaxyz.com/)
- [EAI猩球](https://www.robotech.ink/)
- [文举的博客](https://liwenju0.com/)
- [用代码打点酱油](https://bruceyuan.com/)
- [Zhang's blog](https://armcvai.cn/)
- [申请链接](https://kexue.fm/links.html)

本站采用创作共用版权协议，要求署名、非商业用途和保持一致。转载本站内容必须也遵循“ [署名-非商业用途-保持一致](http://creativecommons.org/licenses/by-nc-nd/2.5/cn/)”的创作共用协议。
© 2009-2025 Scientific Spaces. All rights reserved. Theme by [laogui](http://www.laogui.com). Powered by [Typecho](http://typecho.org). 备案号: [粤ICP备09093259号-1/2](https://beian.miit.gov.cn/)。