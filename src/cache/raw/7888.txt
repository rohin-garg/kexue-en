![MobileSideBar](https://kexue.fm/usr/themes/geekg/images/slide-button.png)

## SEARCH

## MENU

- [打赏](https://kexue.fm/reward.html)
- [公式](https://kexue.fm/latex.html)
- [天象](https://kexue.fm/ac.html)
- [链接](https://kexue.fm/links.html)
- [时光](https://kexue.fm/me.html)
- [博览](https://kexue.fm/science.html)
- [归档](https://kexue.fm/content.html)

## CATEGORIES

- [千奇百怪](https://kexue.fm/category/Everything)
- [天文探索](https://kexue.fm/category/Astronomy)
- [数学研究](https://kexue.fm/category/Mathematics)
- [物理化学](https://kexue.fm/category/Phy-chem)
- [信息时代](https://kexue.fm/category/Big-Data)
- [生物自然](https://kexue.fm/category/Biology)
- [图片摄影](https://kexue.fm/category/Photograph)
- [问题百科](https://kexue.fm/category/Questions)
- [生活/情感](https://kexue.fm/category/Life-Feeling)
- [资源共享](https://kexue.fm/category/Resources)

## NEWPOSTS

- [重温SSM（二）：HiPPO的一些...](https://kexue.fm/archives/10137)
- [Transformer升级之路：1...](https://kexue.fm/archives/10122)
- [重温SSM（一）：线性系统和HiP...](https://kexue.fm/archives/10114)
- [缓存与效果的极限拉扯：从MHA、M...](https://kexue.fm/archives/10091)
- [Cool Papers更新：简单搭...](https://kexue.fm/archives/10088)
- [以蒸馏的名义：“从去噪自编码器到生...](https://kexue.fm/archives/10085)
- [生成扩散模型漫谈（二十四）：少走捷...](https://kexue.fm/archives/10077)
- [生成扩散模型漫谈（二十三）：信噪比...](https://kexue.fm/archives/10055)
- [生成扩散模型漫谈（二十二）：信噪比...](https://kexue.fm/archives/10047)
- [Transformer升级之路：1...](https://kexue.fm/archives/10040)

## COMMENTS

- [leo1231: 您好，公式（10）的标号没有显示\
公式（13）的标号只显示了部...](https://kexue.fm/archives/9164/comment-page-3#comment-24538)
- [night: (4)式中n在transformer中的物理含义是什么？还是说...](https://kexue.fm/archives/9812/comment-page-1#comment-24537)
- [rough yang: 理解了，谢谢。想当然了。\
以为$\\overline{β}\_{t...](https://kexue.fm/archives/9119/comment-page-10#comment-24536)
- [Ziyao: 可以用kimi抽取吗？](https://kexue.fm/archives/9978/comment-page-1#comment-24535)
- [JRain: 苏神您好，我有一个问题。在DDPM的采样过程中，是从$x\_T$...](https://kexue.fm/archives/9164/comment-page-3#comment-24534)
- [想过好日子的瓜娃子: 谢谢苏神，我明白了您的意思，我觉得我个人还是觉得原论文里面的思...](https://kexue.fm/archives/10114/comment-page-1#comment-24533)
- [苏剑林: $z$本身已经有来自Autoencoder的梯度来，所以它关于...](https://kexue.fm/archives/6760/comment-page-7#comment-24532)
- [苏剑林: 1、MLA跟LoRA没什么关系，不建议从LoRA角度理解MLA...](https://kexue.fm/archives/10091/comment-page-2#comment-24531)
- [苏剑林: 一般来说，$n$越大，$g\_n(t)$描述的信号越高频，也就是...](https://kexue.fm/archives/10114/comment-page-1#comment-24530)
- [苏剑林: 正交基的范围是$\[0,1\]$，函数的定义域也是$\[0,1\]$，...](https://kexue.fm/archives/10114/comment-page-1#comment-24529)

## USERLOGIN

- [登录](https://kexue.fm/admin/login.php)

[科学空间\|Scientific Spaces](https://kexue.fm)

- [登录](https://kexue.fm/admin/login.php)
- [打赏](https://kexue.fm/reward.html)
- [公式](https://kexue.fm/latex.html)
- [天象](https://kexue.fm/ac.html)
- [链接](https://kexue.fm/links.html)
- [时光](https://kexue.fm/me.html)
- [博览](https://kexue.fm/science.html)
- [归档](https://kexue.fm/content.html)

渴望成为一个小飞侠

- [![](https://kexue.fm/usr/themes/geekg/images/rss.png)\
\
欢迎订阅](https://kexue.fm/feed)
- [![](https://kexue.fm/usr/themes/geekg/images/mail.png)\
\
个性邮箱](https://kexue.fm/archives/119)
- [![](https://kexue.fm/usr/themes/geekg/images/Saturn.png)\
\
天象信息](https://kexue.fm/ac.html)
- [![](https://kexue.fm/usr/themes/geekg/images/iss.png)\
\
观测ISS](https://kexue.fm/archives/41)
- [![](https://kexue.fm/usr/themes/geekg/images/pi.png)\
\
LaTeX](https://kexue.fm/latex.html)
- [![](https://kexue.fm/usr/themes/geekg/images/mlogo.png)\
\
关于博主](https://kexue.fm/me.html)

欢迎访问“科学空间”，这里将与您共同探讨自然科学，回味人生百态；也期待大家的分享～

- [**千奇百怪** Everything](https://kexue.fm/category/Everything)
- [**天文探索** Astronomy](https://kexue.fm/category/Astronomy)
- [**数学研究** Mathematics](https://kexue.fm/category/Mathematics)
- [**物理化学** Phy-chem](https://kexue.fm/category/Phy-chem)
- [**信息时代** Big-Data](https://kexue.fm/category/Big-Data)
- [**生物自然** Biology](https://kexue.fm/category/Biology)
- [**图片摄影** Photograph](https://kexue.fm/category/Photograph)
- [**问题百科** Questions](https://kexue.fm/category/Questions)
- [**生活/情感** Life-Feeling](https://kexue.fm/category/Life-Feeling)
- [**资源共享** Resources](https://kexue.fm/category/Resources)

- [**千奇百怪**](https://kexue.fm/category/Everything)
- [**天文探索**](https://kexue.fm/category/Astronomy)
- [**数学研究**](https://kexue.fm/category/Mathematics)
- [**物理化学**](https://kexue.fm/category/Phy-chem)
- [**信息时代**](https://kexue.fm/category/Big-Data)
- [**生物自然**](https://kexue.fm/category/Biology)
- [**图片摄影**](https://kexue.fm/category/Photograph)
- [**问题百科**](https://kexue.fm/category/Questions)
- [**生活/情感**](https://kexue.fm/category/Life-Feeling)
- [**资源共享**](https://kexue.fm/category/Resources)

[首页](https://kexue.fm) [数学研究](https://kexue.fm/category/Mathematics) [信息时代](https://kexue.fm/category/Big-Data) 也来谈谈RNN的梯度消失/爆炸问题

13Nov

# [也来谈谈RNN的梯度消失/爆炸问题](https://kexue.fm/archives/7888)

By 苏剑林 \|
2020-11-13 \|
75981位读者\|

尽管Transformer类的模型已经攻占了NLP的多数领域，但诸如LSTM、GRU之类的RNN模型依然在某些场景下有它的独特价值，所以RNN依然是值得我们好好学习的模型。而对于RNN梯度的相关分析，则是一个从优化角度思考分析模型的优秀例子，值得大家仔细琢磨理解。君不见，诸如“LSTM为什么能解决梯度消失/爆炸”等问题依然是目前流行的面试题之一...

[![经典的LSTM](https://kexue.fm/usr/uploads/2020/11/2786261413.png)](https://kexue.fm/usr/uploads/2020/11/2786261413.png)

经典的LSTM

关于此类问题，已有不少网友做出过回答，然而笔者查找了一些文章（包括知乎上的部分回答、专栏以及经典的英文博客），发现没有找到比较好的答案：有些推导记号本身就混乱不堪，有些论述过程没有突出重点，整体而言感觉不够清晰自洽。为此，笔者也尝试给出自己的理解，供大家参考。

## RNN及其梯度 [\#](https://kexue.fm/archives/7888\#RNN%E5%8F%8A%E5%85%B6%E6%A2%AF%E5%BA%A6)

RNN的统一定义为

\\begin{equation}h\_t = f\\left(x\_t, h\_{t-1};\\theta\\right)\\end{equation}

其中$h\_t$是每一步的输出，它由当前输入$x\_t$和前一时刻输出$h\_{t-1}$共同决定，而$\\theta$则是可训练参数。在做最基本的分析时，我们可以假设$h\_t,x\_t,\\theta$都是一维的，这可以让我们获得最直观的理解，并且其结果对高维情形仍有参考价值。之所以要考虑梯度，是因为我们目前主流的优化器还是梯度下降及其变种，因此要求我们定义的模型有一个比较合理的梯度。我们可以求得：

\\begin{equation}\\frac{d h\_t}{d\\theta} = \\frac{\\partial h\_t}{\\partial h\_{t-1}}\\frac{d h\_{t-1}}{d\\theta} + \\frac{\\partial h\_t}{\\partial \\theta}\\end{equation}

可以看到，其实RNN的梯度也是一个RNN，当前时刻梯度$\\frac{d h\_t}{d\\theta}$是前一时刻梯度$\\frac{d h\_{t-1}}{d\\theta}$与当前运算梯度$\\frac{\\partial h\_t}{\\partial \\theta}$的函数。同时，从上式我们就可以看出，其实梯度消失或者梯度爆炸现象几乎是必然存在的：当$\\left\|\\frac{\\partial h\_t}{\\partial h\_{t-1}}\\right\| < 1$时，意味着历史的梯度信息是衰减的，因此步数多了梯度必然消失（好比$\\lim\\limits\_{n\\to\\infty} 0.9^n \\to 0$）；当$\\left\|\\frac{\\partial h\_t}{\\partial h\_{t-1}}\\right\| > 1$，因为这历史的梯度信息逐步增强，因此步数多了梯度必然爆炸（好比$\\lim\\limits\_{n\\to\\infty} 1.1^n \\to \\infty$）。总不可能一直$\\left\|\\frac{\\partial h\_t}{\\partial h\_{t-1}}\\right\| = 1$吧？当然，也有可能有些时刻大于1，有些时刻小于1，最终稳定在1附近，但这样概率很小，需要很精巧地设计模型才行。

所以步数多了，梯度消失或爆炸几乎都是不可避免的，我们只能对于有限的步数去缓解这个问题。

## 消失还是爆炸？ [\#](https://kexue.fm/archives/7888\#%E6%B6%88%E5%A4%B1%E8%BF%98%E6%98%AF%E7%88%86%E7%82%B8%EF%BC%9F)

说到这里，我们还没说清楚一个问题：什么是RNN的梯度消失/爆炸？梯度爆炸好理解，就是梯度数值发散，甚至慢慢就NaN了；那梯度消失就是梯度变成零吗？并不是，我们刚刚说梯度消失是$\\left\|\\frac{\\partial h\_t}{\\partial h\_{t-1}}\\right\|$一直小于1，历史梯度不断衰减，但不意味着总的梯度就为0了，具体来说，一直迭代下去，我们有

\\begin{equation}\\begin{aligned}\\frac{d h\_t}{d\\theta} =& \\frac{\\partial h\_t}{\\partial h\_{t-1}}\\frac{d h\_{t-1}}{d\\theta} + \\frac{\\partial h\_t}{\\partial \\theta}\\\

=& \\frac{\\partial h\_t}{\\partial \\theta}+\\frac{\\partial h\_t}{\\partial h\_{t-1}}\\frac{\\partial h\_{t-1}}{\\partial \\theta}+\\frac{\\partial h\_t}{\\partial h\_{t-1}}\\frac{\\partial h\_{t-1}}{\\partial h\_{t-2}}\\frac{\\partial h\_{t-2}}{\\partial \\theta}+\\dots\\\

\\end{aligned}\\end{equation}

显然，其实只要$\\frac{\\partial h\_t}{\\partial \\theta}$不为0，那么总梯度为0的概率其实是很小的；但是一直迭代下去的话，那么$\\frac{\\partial h\_1}{\\partial \\theta}$这一项前面的稀疏就是$t-1$项的连乘$\\frac{\\partial h\_t}{\\partial h\_{t-1}}\\frac{\\partial h\_{t-1}}{\\partial h\_{t-2}}\\cdots\\frac{\\partial h\_2}{\\partial h\_1}$，如果它们的绝对值都小于1，那么结果就会趋于0，这样一来，$\\frac{d h\_t}{d\\theta}$几乎就没有包含最初的梯度$\\frac{\\partial h\_1}{\\partial \\theta}$的信息了，这才是RNN中梯度消失的含义：距离当前时间步越长，那么其反馈的梯度信号越不显著，最后可能完全没有起作用，这就意味着RNN对长距离语义的捕捉能力失效了。

说白了，你优化过程都跟长距离的反馈没关系，怎么能保证学习出来的模型能有效捕捉长距离呢？

## 几个数学公式 [\#](https://kexue.fm/archives/7888\#%E5%87%A0%E4%B8%AA%E6%95%B0%E5%AD%A6%E5%85%AC%E5%BC%8F)

上面的文字都是一般性的分析，接下来我们具体RNN具体分析。不过在此之前，我们需要回顾几条数学公式，后面的推导中我们将多次运用到这几条公式：

$$\\begin{equation}\\begin{aligned}

&\\tanh x = 2\\sigma(2x) - 1\\\

&\\sigma(x) = \\frac{1}{2}\\left(\\tanh \\frac{x}{2} + 1\\right)\\\

&(\\tanh x)' = 1 - \\tanh^2 x\\\

&\\sigma'(x) = \\sigma(x)\\left(1 - \\sigma(x)\\right)

\\end{aligned}\\end{equation}$$

其中$\\sigma(x) = 1/(1+e^{-x})$是sigmoid函数。这几条公式其实就是说了这么一件事：$\\tanh x$和$\\sigma(x)$基本上是等价的，它们的导数均可以用它们自身来表示。

## 简单RNN分析 [\#](https://kexue.fm/archives/7888\#%E7%AE%80%E5%8D%95RNN%E5%88%86%E6%9E%90)

首先登场的是比较原始的简单RNN（有时候我们确实直接称它为SimpleRNN），它的公式为：

\\begin{equation}h\_t = \\tanh \\left(Wx\_t + Uh\_{t-1} + b\\right)\\end{equation}

其中$W,U,b$是待优化参数。看到这里很自然就能提出第一个疑问：为什么激活函数用$\\tanh$而不是更流行的$\\text{relu}$？这是个好问题，我们很快就会回答它。

从上面的讨论中我们已经知道，梯度消失还是爆炸主要取决于$\\left\|\\frac{\\partial h\_t}{\\partial h\_{t-1}}\\right\|$，所以我们计算

\\begin{equation}\\frac{\\partial h\_t}{\\partial h\_{t-1}} = \\left(1-h\_t^2\\right)U\\label{eq:rnn-g}\\end{equation}

由于我们无法确定$U$的范围，因此$\\left\|\\frac{\\partial h\_t}{\\partial h\_{t-1}}\\right\|$可能小于1也可能大于1，梯度消失/爆炸的风险是存在的。但有意思的是，如果$\|U\|$很大，那么相应地$h\_t$就会很接近1或-1，这样$\\left(1-h\_t^2\\right)U$反而会小，事实上可以严格证明：如果固定$h\_{t-1}\\neq 0$，那么$\\left(1-h\_t^2\\right)U$作为$U$的函数是有界的，也就是说不管$U$等于什么，它都不超过一个固定的常数。

这样一来，我们就能回答为什么激活函数要用$\\tanh$了，因为激活函数用$\\tanh$后，对应的梯度$\\frac{\\partial h\_t}{\\partial h\_{t-1}}$是有界的，虽然这个界未必是1，但一个有界的量不超过1的概率总高于无界的量，因此梯度爆炸的风险更低。相比之下，如果用$\\text{relu}$激活的话，它在正半轴的导数恒为1，此时$\\frac{\\partial h\_t}{\\partial h\_{t-1}}=U$是无界的，梯度爆炸风险更高。

所以，RNN用$\\text{tanh}$而不是$\\text{relu}$的主要目的就是缓解梯度爆炸风险。当然，这个缓解是相对的，用了$\\tanh$依然有爆炸的可能性。事实上，处理梯度爆炸的最根本方法是参数裁剪或梯度裁剪，换句话说我人为地把$U$给裁剪到$\[-1,1\]$内，那不就可以保证梯度不爆了吗？当然，又有读者会问，既然裁剪可以解决问题，那么是不是可以用$\\text{relu}$了？确实是这样子，配合良好的初始化方法和参数/梯度裁剪方案，$\\text{relu}$版的RNN也可以训练好，但是我们还是愿意用$\\tanh$，这还是因为它对应的$\\frac{\\partial h\_t}{\\partial h\_{t-1}}$有界，要裁剪也不用裁剪得太厉害，模型的拟合能力可能会更好。

## LSTM的结果 [\#](https://kexue.fm/archives/7888\#LSTM%E7%9A%84%E7%BB%93%E6%9E%9C)

当然，裁剪的方式虽然也能work，但终究是无奈之举，况且裁剪也只能解决梯度爆炸问题，解决不了梯度消失，如果能从模型设计上解决这个问题，那么自然是最好的。传说中的LSTM就是这样的一种设计，真相是否如此？我们马上来分析一下。

LSTM的更新公式比较复杂，它是：

\\begin{equation}\\begin{aligned} f\_{t} & = \\sigma \\left( W\_{f} x\_{t} + U\_{f} h\_{t - 1} + b\_{f} \\right) \\\

i\_{t} & = \\sigma \\left( W\_{i} x\_{t} + U\_{i} h\_{t - 1} + b\_{i} \\right) \\\

o\_{t} & = \\sigma \\left( W\_{o} x\_{t} + U\_{o} h\_{t - 1} + b\_{o} \\right) \\\

\\hat{c}\_t & = \\tanh \\left( W\_{c} x\_{t} + U\_{c} h\_{t - 1} + b\_{c} \\right)\\\

c\_{t} & = f\_{t} \\circ c\_{t - 1} + i\_{t} \\circ \\hat{c}\_t \\\

h\_{t} & = o\_{t} \\circ \\tanh \\left( c\_{t} \\right)\\end{aligned}\\end{equation}

我们可以像上面一样计算$\\frac{\\partial h\_t}{\\partial h\_{t-1}}$，但从$h\_{t} = o\_{t} \\circ \\tanh \\left( c\_{t} \\right)$可以看出分析$c\_{t}$就等价于分析$h\_{t}$，而计算$\\frac{\\partial c\_t}{\\partial c\_{t-1}}$显得更加简单一些，因此我们往这个方向走。

同样地，我们先只关心1维的情形，这时候根据求导公式，我们有

\\begin{equation}\\frac{\\partial c\_t}{\\partial c\_{t-1}}=f\_t + c\_{t-1}\\frac{\\partial f\_t}{\\partial c\_{t-1}}+ \\hat{c}\_{t}\\frac{\\partial i\_t}{\\partial c\_{t-1}}+ i\_{t}\\frac{\\partial \\hat{c}\_t}{\\partial c\_{t-1}}\\end{equation}

右端第一项$f\_t$，也就是我们所说的“遗忘门”，从下面的论述我们可以知道一般情况下其余三项都是次要项，因此$f\_t$是“主项”，由于$f\_t$在0～1之间，因此就意味着梯度爆炸的风险将会很小，至于会不会梯度消失，取决于$f\_t$是否接近于1。但非常碰巧的是，这里有个相当自洽的结论：如果我们的任务比较依赖于历史信息，那么$f\_t$就会接近于1，这时候历史的梯度信息也正好不容易消失；如果$f\_t$很接近于0，那么就说明我们的任务不依赖于历史信息，这时候就算梯度消失也无妨了。

所以，现在的关键就是看“其余三项都是次要项”这个结论能否成立。后面的三项都是“一项乘以另一项的偏导”的形式，而且求偏导的项都是$\\sigma$或$\\tanh$激活，前面在回顾数学公式的时候说了$\\sigma$和$\\tanh$基本上是等价的，因此后面三项是类似的，分析了其中一项就相当于分析了其余两项。以第二项为例，代入$h\_{t-1} = o\_{t-1} \\tanh \\left( c\_{t-1} \\right)$，可以算得

\\begin{equation}c\_{t-1}\\frac{\\partial f\_t}{\\partial c\_{t-1}}=f\_t \\left(1 - f\_t\\right) o\_{t-1} \\left(1-\\tanh^2 c\_{t-1}\\right)c\_{t-1}U\_f\\end{equation}

注意到$f\_t,1 - f\_t,o\_{t-1},$都是在0～1之间，也可以证明$\\left\|\\left(1-\\tanh^2 c\_{t-1}\\right)c\_{t-1}\\right\| < 0.45$，因此它也在-1～1之间。所以$c\_{t-1}\\frac{\\partial f\_t}{\\partial c\_{t-1}}$就相当于1个$U\_f$乘上4个门，结果会变得更加小，所以只要初始化不是很糟糕，那么它都会被压缩得相当小，因此占不到主导作用。跟简单RNN的梯度$\\eqref{eq:rnn-g}$相比，它多出了3个门，所以这个变化说白点就是：1个门我压不垮你，多来几个门还不行么？

剩下两项的结论也是类似的：

\\begin{equation}\\begin{aligned}

\\hat{c}\_{t}\\frac{\\partial i\_t}{\\partial c\_{t-1}}=&\\,i\_t \\left(1 - i\_t\\right) o\_{t-1} \\left(1-\\tanh^2 c\_{t-1}\\right)\\hat{c}\_{t}U\_i\\\

i\_{t}\\frac{\\partial \\hat{c}\_t}{\\partial c\_{t-1}}=&\\,\\left(1 - \\hat{c}\_t^2\\right) o\_{t-1} \\left(1-\\tanh^2 c\_{t-1}\\right)i\_{t}U\_c

\\end{aligned}\\end{equation}

所以，后面三项的梯度带有更多的“门”，一般而言乘起来后会被压缩的更厉害，因此占主导的项还是$f\_t$，$f\_t$在0～1之间这个特性决定了它梯度爆炸的风险很小，同时$f\_t$表明了模型对历史信息的依赖性，也正好是历史梯度的保留程度，两者相互自洽，所以LSTM也能较好地缓解梯度消失问题。因此，LSTM同时较好地缓解了梯度消失/爆炸问题，现在我们训练LSTM时，多数情况下只需要直接调用Adam等自适应学习率优化器，不需要人为对梯度做什么调整了。

当然，这些结果都是“概论”，你非要构造一个会梯度消失/爆炸的LSTM来，那也是能构造出来的。此外，就算LSTM能缓解这两个问题，也是在一定步数内，如果你的序列很长，比如几千上万步，那么该消失的还会消失。毕竟单靠一个向量，也缓存不了那么多信息啊～

## 顺便看看GRU [\#](https://kexue.fm/archives/7888\#%E9%A1%BA%E4%BE%BF%E7%9C%8B%E7%9C%8BGRU)

在文章结束之前，我们顺便对LSTM的强力竞争对手GRU也做一个分析。GRU的运算过程为：

\\begin{equation}\\begin{aligned} z\_{t} & = \\sigma \\left( W\_{z} x\_{t} + U\_{z} h\_{t - 1} + b\_{z} \\right) \\\

r\_{t} & = \\sigma \\left( W\_{r} x\_{t} + U\_{r} h\_{t - 1} + b\_{r} \\right) \\\

\\hat{h}\_t & = \\tanh \\left( W\_{h} x\_{t} + U\_{h} (r\_t \\circ h\_{t - 1}) + b\_{c} \\right)\\\

h\_{t} & = \\left(1 - z\_{t}\\right) \\circ h\_{t - 1} + z\_{t} \\circ \\hat{h}\_t \\end{aligned}\\end{equation}

还有个更极端的版本是将$r\_t,z\_t$合成一个：

\\begin{equation}\\begin{aligned}

r\_{t} & = \\sigma \\left( W\_{r} x\_{t} + U\_{r} h\_{t - 1} + b\_{r} \\right) \\\

\\hat{h}\_t & = \\tanh \\left( W\_{h} x\_{t} + U\_{h} (r\_t \\circ h\_{t - 1}) + b\_{c} \\right)\\\

h\_{t} & = \\left(1 - r\_{t}\\right) \\circ h\_{t - 1} + r\_{t} \\circ \\hat{h}\_t \\end{aligned}\\end{equation}

不管是哪一个，我们发现它在算$\\hat{h}\_t$的时候，$h\_{t-1}$都是先乘个$r\_t$变成$r\_t \\circ h\_{t - 1}$，不知道读者是否困惑过这一点？直接用$h\_{t-1}$不是更简洁更符合直觉吗？

首先我们观察到，而$h\_0$一般全零初始化，$\\hat{h}\_t$则因为$\\tanh$激活，因此结果必然在-1～1之间，所以作为$h\_{t-1}$与$\\hat{h}\_t$的加权平均的$h\_{t}$也一直保持在-1～1之间，因此$h\_t$本身就有类似门的作用。这跟LSTM的$c\_t$不一样，理论上$c\_t$是有可能发散的。了解到这一点后，我们再去求导：

\\begin{equation}\\begin{aligned}

\\frac{\\partial h\_t}{\\partial h\_{t-1}} =& 1 - z\_t - z\_t (1-z\_t) h\_{t-1} U\_z + z\_t (1-z\_t) \\hat{h}\_{t} U\_z \\\

& \+ \\left(1-\\hat{h}\_{t}^2\\right)r\_t\\left(1 + (1 - r\_t)h\_{t-1}U\_r\\right) z\_t U\_h

\\end{aligned}\\end{equation}

其实结果跟LSTM的类似，主导项应该是$1-z\_t$，但剩下的项比LSTM对应的项少了1个门，因此它们的量级可能更大，相对于LSTM的梯度其实更不稳定，特别是$r\_t \\circ h\_{t - 1}$这步操作，虽然给最后一项引入了多一个门$r\_t$，但也同时引入了多一项$1 + (1 - r\_t)h\_{t - 1}U\_r$，是好是歹很难说。总体相对而言，感觉GRU应该会更不稳定，比LSTM更依赖于好的初始化方式。

针对上述分析结果，个人认为如果沿用GRU的思想，又需要简化LSTM并且保持LSTM对梯度的友好性，更好的做法是把$r\_t \\circ h\_{t - 1}$放到最后：

\\begin{equation}\\begin{aligned} z\_{t} & = \\sigma \\left( W\_{z} x\_{t} + U\_{z} h\_{t - 1} + b\_{z} \\right) \\\

r\_{t} & = \\sigma \\left( W\_{r} x\_{t} + U\_{r} h\_{t - 1} + b\_{r} \\right) \\\

\\hat{c}\_t & = \\tanh \\left( W\_{h} x\_{t} + U\_{h} h\_{t - 1} + b\_{c} \\right)\\\

c\_{t} & = \\left(1 - z\_{t}\\right) \\circ c\_{t - 1} + z\_{t} \\circ \\hat{c}\_t \\\

h\_t & = r\_t \\circ c\_t\\end{aligned}\\end{equation}

当然，这样需要多缓存一个变量，带来额外的显存消耗了。

## 文章总结概述 [\#](https://kexue.fm/archives/7888\#%E6%96%87%E7%AB%A0%E6%80%BB%E7%BB%93%E6%A6%82%E8%BF%B0)

本文讨论了RNN的梯度消失/爆炸问题，主要是从梯度函数的有界性、门控数目的多少来较为明确地讨论RNN、LSTM、GRU等模型的梯度流情况，以确定其中梯度消失/爆炸风险的大小。本文属于闭门造车之作，如有错漏，请读者海涵并斧正。

_**转载到请包括本文地址：** [https://kexue.fm/archives/7888](https://kexue.fm/archives/7888)_

_**更详细的转载事宜请参考：**_ [《科学空间FAQ》](https://kexue.fm/archives/6508#%E6%96%87%E7%AB%A0%E5%A6%82%E4%BD%95%E8%BD%AC%E8%BD%BD/%E5%BC%95%E7%94%A8)

**如果您还有什么疑惑或建议，欢迎在下方评论区继续讨论。**

**如果您觉得本文还不错，欢迎 [分享](https://kexue.fm/archives/7888#share)/ [打赏](https://kexue.fm/archives/7888#pay) 本文。打赏并非要从中获得收益，而是希望知道科学空间获得了多少读者的真心关注。当然，如果你无视它，也不会影响你的阅读。再次表示欢迎和感谢！**

打赏

![科学空间](https://kexue.fm/usr/themes/geekg/payment/wx.png)

微信打赏

![科学空间](https://kexue.fm/usr/themes/geekg/payment/zfb.png)

支付宝打赏

因为网站后台对打赏并无记录，因此欢迎在打赏时候备注留言。

你还可以 [**点击这里**](http://mail.qq.com/cgi-bin/qm_share?t=qm_mailme&email=tN7d1drY3drrx8H0xcWa19vZ) 或在下方评论区留言来告知你的建议或需求。

**如果您需要引用本文，请参考：**

苏剑林. (Nov. 13, 2020). 《也来谈谈RNN的梯度消失/爆炸问题 》\[Blog post\]. Retrieved from [https://kexue.fm/archives/7888](https://kexue.fm/archives/7888)

@online{kexuefm-7888,

        title={也来谈谈RNN的梯度消失/爆炸问题},

        author={苏剑林},

        year={2020},

        month={Nov},

        url={\\url{https://kexue.fm/archives/7888}},

}

分类： [数学研究](https://kexue.fm/category/Mathematics), [信息时代](https://kexue.fm/category/Big-Data)    标签： [模型](https://kexue.fm/tag/%E6%A8%A1%E5%9E%8B/), [优化](https://kexue.fm/tag/%E4%BC%98%E5%8C%96/), [深度学习](https://kexue.fm/tag/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/), [梯度](https://kexue.fm/tag/%E6%A2%AF%E5%BA%A6/)[32 评论](https://kexue.fm/archives/7888#comments)

< [当GPT遇上中国象棋：写过文章解过题，要不再来下盘棋？](https://kexue.fm/archives/7877) \| [跟风玩玩目前最大的中文GPT2模型（bert4keras）](https://kexue.fm/archives/7912) >

### 你也许还对下面的内容感兴趣

- [重温SSM（二）：HiPPO的一些遗留问题](https://kexue.fm/archives/10137)
- [缓存与效果的极限拉扯：从MHA、MQA、GQA到MLA](https://kexue.fm/archives/10091)
- [以蒸馏的名义：“从去噪自编码器到生成模型”重现江湖](https://kexue.fm/archives/10085)
- [配置不同的学习率，LoRA还能再涨一点？](https://kexue.fm/archives/10001)
- [旁门左道之如何让Python的重试代码更加优雅](https://kexue.fm/archives/9938)
- [VQ一下Key，Transformer的复杂度就变成线性了](https://kexue.fm/archives/9844)
- [简单得令人尴尬的FSQ：“四舍五入”超越了VQ-VAE](https://kexue.fm/archives/9826)
- [从梯度最大化看Attention的Scale操作](https://kexue.fm/archives/9812)
- [随机分词再探：从Viterbi Sampling到完美采样算法](https://kexue.fm/archives/9811)
- [EMO：基于最优传输思想设计的分类损失函数](https://kexue.fm/archives/9797)

[发表你的看法](https://kexue.fm/archives/7888#comment_form)

1. [«](https://kexue.fm/archives/7888/comment-page-1#comments)
2. [1](https://kexue.fm/archives/7888/comment-page-1#comments)
3. [2](https://kexue.fm/archives/7888/comment-page-2#comments)

[利用Tensorflow构建RNN并对序列数据进行建模 R11; 源码巴士](https://code84.com/421880.html)

October 3rd, 2022

\[...\]相关博文连接–苏建林\[...\]

[回复评论](https://kexue.fm/archives/7888/comment-page-2?replyTo=19980#respond-post-7888)

boyeteystog

August 12th, 2023

个人感觉Lstm缩小梯度爆炸可能性这一点可以再细说一下。因为权重更新过程中梯度流不仅仅有ct到ct-1的路径，还有ht到ht-1，ht到ct， ct到ht-1的路径的各种组合，单单分析一个ct到ct-1的路径并不能说明其他路径不会梯度爆炸（毕竟只要有一条路径爆了那整个梯度就都爆了）。另外还看到一个说法，在初始化lstm各权重时，会把遗忘门的偏置bf初始化的比较大以保证训练伊始的各个遗忘门都比较接近于1，从而使过去的历史信息更好的传递以训练lstm，我认为这也可以提一下，不然光说lstm 通过诸多ft连乘来缓解梯度消失还是感觉非常的玄学，因为毕竟ft最后变成啥样并非我们可以直接控制的嘛

[回复评论](https://kexue.fm/archives/7888/comment-page-2?replyTo=22490#respond-post-7888)

[苏剑林](https://kexue.fm) 发表于
August 15th, 2023

嗯，细节还有挺多的，但事实上LSTM也无法绝对保证梯度消失。此外，还有一个case，就是在单向语言模型场景，每个token都需要算loss，那么基本上也不会有梯度消失问题了。

[回复评论](https://kexue.fm/archives/7888/comment-page-2?replyTo=22506#respond-post-7888)

1. [«](https://kexue.fm/archives/7888/comment-page-1#comments)
2. [1](https://kexue.fm/archives/7888/comment-page-1#comments)
3. [2](https://kexue.fm/archives/7888/comment-page-2#comments)

[取消回复](https://kexue.fm/archives/7888#respond-post-7888)

你的大名

电子邮箱

个人网站（选填）

1\. 可以在评论中使用LaTeX代码，点击“预览效果”可即时查看效果，点击 [这里](https://kexue.fm/content.html) 可以查看更多内容；

2\. 可以通过点击评论楼层编号来引用该楼层；

3\. **提交评论之前，建议复制一下评论内容，避免提交失败导致辛苦打的字没了。**

### 内容速览

[RNN及其梯度](https://kexue.fm/archives/7888#RNN%E5%8F%8A%E5%85%B6%E6%A2%AF%E5%BA%A6)
[消失还是爆炸？](https://kexue.fm/archives/7888#%E6%B6%88%E5%A4%B1%E8%BF%98%E6%98%AF%E7%88%86%E7%82%B8%EF%BC%9F)
[几个数学公式](https://kexue.fm/archives/7888#%E5%87%A0%E4%B8%AA%E6%95%B0%E5%AD%A6%E5%85%AC%E5%BC%8F)
[简单RNN分析](https://kexue.fm/archives/7888#%E7%AE%80%E5%8D%95RNN%E5%88%86%E6%9E%90)
[LSTM的结果](https://kexue.fm/archives/7888#LSTM%E7%9A%84%E7%BB%93%E6%9E%9C)
[顺便看看GRU](https://kexue.fm/archives/7888#%E9%A1%BA%E4%BE%BF%E7%9C%8B%E7%9C%8BGRU)
[文章总结概述](https://kexue.fm/archives/7888#%E6%96%87%E7%AB%A0%E6%80%BB%E7%BB%93%E6%A6%82%E8%BF%B0)

### 智能搜索

支持整句搜索！网站自动使用 [结巴分词](https://github.com/fxsjy/jieba) 进行分词，并结合ngrams排序算法给出合理的搜索结果。

### 热门标签

[生成模型](https://kexue.fm/tag/%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B/) [attention](https://kexue.fm/tag/attention/) [模型](https://kexue.fm/tag/%E6%A8%A1%E5%9E%8B/) [语言模型](https://kexue.fm/tag/%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/) [优化](https://kexue.fm/tag/%E4%BC%98%E5%8C%96/) [网站](https://kexue.fm/tag/%E7%BD%91%E7%AB%99/) [概率](https://kexue.fm/tag/%E6%A6%82%E7%8E%87/) [转载](https://kexue.fm/tag/%E8%BD%AC%E8%BD%BD/) [微分方程](https://kexue.fm/tag/%E5%BE%AE%E5%88%86%E6%96%B9%E7%A8%8B/) [天象](https://kexue.fm/tag/%E5%A4%A9%E8%B1%A1/) [深度学习](https://kexue.fm/tag/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/) [积分](https://kexue.fm/tag/%E7%A7%AF%E5%88%86/) [python](https://kexue.fm/tag/python/) [分析](https://kexue.fm/tag/%E5%88%86%E6%9E%90/) [无监督](https://kexue.fm/tag/%E6%97%A0%E7%9B%91%E7%9D%A3/) [梯度](https://kexue.fm/tag/%E6%A2%AF%E5%BA%A6/) [力学](https://kexue.fm/tag/%E5%8A%9B%E5%AD%A6/) [节日](https://kexue.fm/tag/%E8%8A%82%E6%97%A5/) [几何](https://kexue.fm/tag/%E5%87%A0%E4%BD%95/) [文本生成](https://kexue.fm/tag/%E6%96%87%E6%9C%AC%E7%94%9F%E6%88%90/) [数论](https://kexue.fm/tag/%E6%95%B0%E8%AE%BA/) [矩阵](https://kexue.fm/tag/%E7%9F%A9%E9%98%B5/) [GAN](https://kexue.fm/tag/GAN/) [生活](https://kexue.fm/tag/%E7%94%9F%E6%B4%BB/) [扩散](https://kexue.fm/tag/%E6%89%A9%E6%95%A3/)

### 随机文章

- [高考倒计时15天...](https://kexue.fm/archives/1610)
- [《方程与宇宙》：三体问题和它的初积分(六)](https://kexue.fm/archives/1181)
- [获取并处理中文维基百科语料](https://kexue.fm/archives/4176)
- [高三学生写的数学情书（佩服）](https://kexue.fm/archives/116)
- [【NASA每日一图】火星上的奇形黑色陨石](https://kexue.fm/archives/73)
- [\[问题解答\]运煤车的最大路程（更正）](https://kexue.fm/archives/2587)
- [数独的自动推理](https://kexue.fm/archives/2527)
- [当酸溶液遇到了更多的水时...](https://kexue.fm/archives/1042)
- [意犹未尽——继续光学曲线](https://kexue.fm/archives/1058)
- [巧断梯度：单个loss实现GAN模型](https://kexue.fm/archives/6387)

### 最近评论

- [leo1231](https://kexue.fm/archives/9164/comment-page-3#comment-24538): 您好，公式（10）的标号没有显示
公式（13）的标号只显示了部分
用的是edge浏览器
- [night](https://kexue.fm/archives/9812/comment-page-1#comment-24537): (4)式中n在transformer中的物理含义是什么？还是说n只是单纯地在推导中引入了，表示...
- [rough yang](https://kexue.fm/archives/9119/comment-page-10#comment-24536): 理解了，谢谢。想当然了。
以为$\\overline{β}\_{t}={β}\_{1}{β}\_{2}...
- [Ziyao](https://kexue.fm/archives/9978/comment-page-1#comment-24535): 可以用kimi抽取吗？
- [JRain](https://kexue.fm/archives/9164/comment-page-3#comment-24534): 苏神您好，我有一个问题。在DDPM的采样过程中，是从$x\_T$开始一步步采样到最终的$x\_0$...
- [想过好日子的瓜娃子](https://kexue.fm/archives/10114/comment-page-1#comment-24533): 谢谢苏神，我明白了您的意思，我觉得我个人还是觉得原论文里面的思路好理解一点。但是我还有一个疑惑...
- [苏剑林](https://kexue.fm/archives/6760/comment-page-7#comment-24532): $z$本身已经有来自Autoencoder的梯度来，所以它关于编码表的梯度可以适当降低，而$z...
- [苏剑林](https://kexue.fm/archives/10091/comment-page-2#comment-24531): 1、MLA跟LoRA没什么关系，不建议从LoRA角度理解MLA，MLA纯粹是GQA的一般化，所...
- [苏剑林](https://kexue.fm/archives/10114/comment-page-1#comment-24530): 一般来说，$n$越大，$g\_n(t)$描述的信号越高频，也就是说越大的$n$描述的是越震荡的高...
- [苏剑林](https://kexue.fm/archives/10114/comment-page-1#comment-24529): 正交基的范围是$\[0,1\]$，函数的定义域也是$\[0,1\]$，你先认真看一下推导逻辑。通过变量...

### 友情链接

- [Cool Papers](https://papers.cool)
- [数学研发](https://bbs.emath.ac.cn)
- [Seatop](http://www.seatop.com.cn/)
- [Xiaoxia](https://xiaoxia.org/)
- [积分表-网络版](https://kexue.fm/sci/integral/index.html)
- [丝路博傲](http://blog.dvxj.com/)
- [ph4ntasy 饭特稀](http://www.ph4ntasy.com/)
- [数学之家](http://www.2math.cn/)
- [有趣天文奇观](http://interesting-sky.china-vo.org/)
- [bsky](https://bsky.spaces.ac.cn/)
- [TwistedW](http://www.twistedwg.com/)
- [godweiyang](https://godweiyang.com/)
- [AI柠檬](https://blog.ailemon.net/)
- [王登科-DK博客](https://greatdk.com)
- [ESON](https://blog.eson.org/)
- [枫之羽](https://fzhiy.net/)
- [Mathor's blog](https://wmathor.com/)
- [孙云增的博客](https://sunyunzeng.com/)
- [coding-zuo](https://coding-zuo.github.io/)
- [博科园](https://www.bokeyuan.net/)
- [孔皮皮的博客](https://www.kppkkp.top/)
- [运鹏的博客](https://yunpengtai.top/)
- [jiming.site](https://jiming.site/)
- [OmegaXYZ](https://www.omegaxyz.com/)
- [Blog by Eacls](https://www.eacls.top/)
- [申请链接](https://kexue.fm/links.html)

[![署名-非商业用途-保持一致](https://kexue.fm/usr/themes/geekg/images/cc.gif)](http://creativecommons.org/licenses/by-nc-nd/2.5/cn/) 本站采用创作共用版权协议，要求署名、非商业用途和保持一致。转载本站内容必须也遵循“ [署名-非商业用途-保持一致](http://creativecommons.org/licenses/by-nc-nd/2.5/cn/)”的创作共用协议。

© 2009-2024 Scientific Spaces. All rights reserved. Theme by [laogui](http://www.laogui.com). Powered by [Typecho](http://typecho.org). 备案号: [粤ICP备09093259号-1/2](https://beian.miit.gov.cn/)。