## SEARCH

## MENU

- [打赏](https://kexue.fm/reward.html)
- [公式](https://kexue.fm/latex.html)
- [天象](https://kexue.fm/ac.html)
- [链接](https://kexue.fm/links.html)
- [时光](https://kexue.fm/me.html)
- [博览](https://kexue.fm/science.html)
- [归档](https://kexue.fm/content.html)

## CATEGORIES

- [千奇百怪](https://kexue.fm/category/Everything)
- [天文探索](https://kexue.fm/category/Astronomy)
- [数学研究](https://kexue.fm/category/Mathematics)
- [物理化学](https://kexue.fm/category/Phy-chem)
- [信息时代](https://kexue.fm/category/Big-Data)
- [生物自然](https://kexue.fm/category/Biology)
- [图片摄影](https://kexue.fm/category/Photograph)
- [问题百科](https://kexue.fm/category/Questions)
- [生活/情感](https://kexue.fm/category/Life-Feeling)
- [资源共享](https://kexue.fm/category/Resources)

## NEWPOSTS

- [线性注意力简史：从模仿、创新到反哺](https://kexue.fm/archives/11033)
- [msign的导数](https://kexue.fm/archives/11025)
- [通过msign来计算mclip（奇...](https://kexue.fm/archives/11006)
- [msign算子的Newton-Sc...](https://kexue.fm/archives/10996)
- [等值振荡定理：最优多项式逼近的充要条件](https://kexue.fm/archives/10972)
- [生成扩散模型漫谈（三十）：从瞬时速...](https://kexue.fm/archives/10958)
- [MoE环游记：5、均匀分布的反思](https://kexue.fm/archives/10945)
- [msign算子的Newton-Sc...](https://kexue.fm/archives/10922)
- [Transformer升级之路：2...](https://kexue.fm/archives/10907)
- [一道概率不等式：盯着它到显然成立为止！](https://kexue.fm/archives/10902)

## COMMENTS

- [石子131: 也许可以尝试把热水管的回水管的开关阀做成用户的手动阀，在热水管...](https://kexue.fm/archives/9405/comment-page-2#comment-27955)
- [Kuo: 我的理解，这是一个迭代过程，注意下标K是指condition还...](https://kexue.fm/archives/10795/comment-page-1#comment-27954)
- [musicfish1973: 好的设计都是相似的,haha](https://kexue.fm/archives/8009/comment-page-1#comment-27953)
- [忍者猫: 这优化器的作者真的应该给你打钱](https://kexue.fm/archives/10592/comment-page-2#comment-27952)
- [Chaofa Yuan: 写得太好了](https://kexue.fm/archives/11033/comment-page-1#comment-27951)
- [Skyler Lin: respect苏神！](https://kexue.fm/archives/11033/comment-page-1#comment-27949)
- [宋佳铭: 对，个人感觉mean flow就是continuous tim...](https://kexue.fm/archives/10958/comment-page-1#comment-27947)
- [宋佳铭: 的确，对sg这个事情我感觉如果是用‘归纳’法做是不太能避免的，...](https://kexue.fm/archives/10958/comment-page-1#comment-27946)
- [MoFHeka: 苏老师您好，请问一下这套结论在稀疏参数上应该如何应用？比如大规...](https://kexue.fm/archives/10542/comment-page-1#comment-27945)
- [苏剑林: Temp LoRA倒是有印象，其实思想是一样的，如果我单独开一...](https://kexue.fm/archives/11033/comment-page-1#comment-27944)

## USERLOGIN

- [登录](https://kexue.fm/admin/login.php)

[科学空间\|Scientific Spaces](https://kexue.fm)

- [登录](https://kexue.fm/admin/login.php)
- [打赏](https://kexue.fm/reward.html)
- [公式](https://kexue.fm/latex.html)
- [天象](https://kexue.fm/ac.html)
- [链接](https://kexue.fm/links.html)
- [时光](https://kexue.fm/me.html)
- [博览](https://kexue.fm/science.html)
- [归档](https://kexue.fm/content.html)

渴望成为一个小飞侠

- [欢迎订阅](https://kexue.fm/feed)
- [个性邮箱](https://kexue.fm/archives/119)
- [天象信息](https://kexue.fm/ac.html)
- [观测ISS](https://kexue.fm/archives/41)
- [LaTeX](https://kexue.fm/latex.html)
- [关于博主](https://kexue.fm/me.html)

欢迎访问“科学空间”，这里将与您共同探讨自然科学，回味人生百态；也期待大家的分享～

- [**千奇百怪** Everything](https://kexue.fm/category/Everything)
- [**天文探索** Astronomy](https://kexue.fm/category/Astronomy)
- [**数学研究** Mathematics](https://kexue.fm/category/Mathematics)
- [**物理化学** Phy-chem](https://kexue.fm/category/Phy-chem)
- [**信息时代** Big-Data](https://kexue.fm/category/Big-Data)
- [**生物自然** Biology](https://kexue.fm/category/Biology)
- [**图片摄影** Photograph](https://kexue.fm/category/Photograph)
- [**问题百科** Questions](https://kexue.fm/category/Questions)
- [**生活/情感** Life-Feeling](https://kexue.fm/category/Life-Feeling)
- [**资源共享** Resources](https://kexue.fm/category/Resources)

- [**千奇百怪**](https://kexue.fm/category/Everything)
- [**天文探索**](https://kexue.fm/category/Astronomy)
- [**数学研究**](https://kexue.fm/category/Mathematics)
- [**物理化学**](https://kexue.fm/category/Phy-chem)
- [**信息时代**](https://kexue.fm/category/Big-Data)
- [**生物自然**](https://kexue.fm/category/Biology)
- [**图片摄影**](https://kexue.fm/category/Photograph)
- [**问题百科**](https://kexue.fm/category/Questions)
- [**生活/情感**](https://kexue.fm/category/Life-Feeling)
- [**资源共享**](https://kexue.fm/category/Resources)

[首页](https://kexue.fm) [信息时代](https://kexue.fm/category/Big-Data) Seq2Seq+前缀树：检索任务新范式（以KgCLUE为例）

17Dec

# [Seq2Seq+前缀树：检索任务新范式（以KgCLUE为例）](https://kexue.fm/archives/8802)

By 苏剑林 \|
2021-12-17 \|
80627位读者\|

两年前，在 [《万能的seq2seq：基于seq2seq的阅读理解问答》](https://kexue.fm/archives/7115) 和 [《“非自回归”也不差：基于MLM的阅读理解问答》](https://kexue.fm/archives/7148) 中，我们在尝试过分别利用“Seq2Seq+前缀树”和“MLM+前缀树”的方式做抽取式阅读理解任务，并获得了不错的结果。而在去年的ICLR2021上，Facebook的论文 [《Autoregressive Entity Retrieval》](https://papers.cool/arxiv/2010.00904) 同样利用“Seq2Seq+前缀树”的组合，在实体链接和文档检索上做到了效果与效率的“双赢”。

事实上，“Seq2Seq+前缀树”的组合理论上可以用到任意检索型任务中，堪称是检索任务的“新范式”。本文将再次回顾“Seq2Seq+前缀树”的思路，并用它来实现最近推出的 [KgCLUE知识图谱问答榜单](https://github.com/CLUEbenchmark/KgCLUE) 的一个baseline。

## 检索任务 [\#](https://kexue.fm/archives/8802\#%E6%A3%80%E7%B4%A2%E4%BB%BB%E5%8A%A1)

说到检索任务，想必大家都不陌生，除了常规的相似问检索外，NLP中还有很多任务都可以视为检索，比如抽取式阅读理解、实体链接甚至基于知识图谱的问答，等等。

以相似问检索为例，常规的检索系统的流程为：

> 1、训练句子编码模型，这通常包含复杂的负样本构建流程，负样本质量直接影响到最终效果；
>
> 2、将每个句子编码为向量，存到诸如Faiss的向量检索库中，这一步通常需要消耗相当大的空间；
>
> 3、将查询句子编码为向量，然后进行检索，返回Topk结果及其相似度。

如果现在告诉你，有一个做检索的新方案，可以不用花心思挑选负样本，也不用消耗极大内存的向量检索库，最终效果和速度也不差于旧方案，那你会不会迫不及待想要尝试一下？没错，这就是本文的主角“Seq2Seq+前缀树”了。

## Seq2Seq [\#](https://kexue.fm/archives/8802\#Seq2Seq)

首先，让我们抛开各种繁琐的细枝末节，想一想检索任务究竟在做什么。对于相似问检索来说，我们输入一个问句，希望输出数据库中与之最相近的句子；对于实体链接来说，我们输入一个包含该实体的句子，希望输出知识库中能正确诠释该实体的实体名或者编号；等等。

抛开某些约束规则不说，可以发现这些任务本质上都可以抽象为：

> 输入一个句子，输出另一个句子。

这不正是Seq2Seq最擅长的工作了嘛！所以用Seq2Seq做不是一件很自然的事情？当然，有读者会说我要输出的是数据库里边已有的一个句子呀，Seq2Seq要是“溢出”（解码出了数据库不存在的句子）了怎么办？这时该前缀树上场了，我们可以利用前缀树约束解码过程，保证生成的结果在数据库中，这一点我们等会细说。

没有了“溢出”的顾虑，我们将会发现Seq2Seq的做法真的是集诸多优点于一身，比如：

> 1、训练Seq2Seq模型只需要输入和目标，这也就是说我们只需要正样本，免除了构建负样本的烦恼，或者说它是将所有其他句子都当成负样本了；
>
> 2、Seq2Seq是直接解码出目标句子，省去了句向量的储存和检索，也就免除了Faiss等工具的使用了；
>
> 3、Seq2Seq包含了目标句子与输入句子之间Token级别的交互，理论上比基于内积的向量检索能做到更精细的对比，从而有更好的检索效果。

## 前缀解码 [\#](https://kexue.fm/archives/8802\#%E5%89%8D%E7%BC%80%E8%A7%A3%E7%A0%81)

现在我们来细说一下利用前缀树来约束解码的过程，看一下它是如何保证输出结果在数据库中的。我们假设数据库有如下几句话：

> 明月几时有
> 明天会更好
> 明天下雨
> 明天下午开会
> 明天下午放假
> 明年见
> 今夕是何年
> 今天去哪里玩

那么我们将用如下的前缀树来存储这些句子：

前缀树示意图：本质上是序列的一种压缩表示

其实就是从左往右地把相同位置的相同token聚合起来，树上的每一条完整路径（以\[BOS\]开头、\[EOS\]结尾）都代表数据库中的一个句子。之所以叫“前缀树”，是因为利用这种树状结构，我们可以很快地查找到以某个前缀开头的字/句有哪些。比如从上图中我们可以看到，第一个字只可能是“明”或“今”，“明”后面只能接“月”、“天”或“年”，“明天”后面只能接“会”或“下”，等等。

有了前缀树，我们约束Seq2Seq解码就不困难了，比如第一个字只能是“明”或“今”，那么在预测第一个字的时候，我们可以把模型预测其他字的概率都置零，这样模型只可能从这两个字中二选一；如果已经确定了第一个字，比如“明”，那么我们在预测第二个字的时候，同样可以将“月”、“天”或“年”以外的字的概率都置零，这样模型只可能从这三个字中选一个，结果必然是“明月”、“明天”、“明年”之一；依此类推，通过将不在前缀树上的候选token置零，保证解码过程只走前缀树的分支，而且必须走到最后，这样解码出来的结果必然是数据库中已有的句子。

相比常规的向量检索方案，“Seq2Seq+前缀树”的方案用前缀树代替了要储存的检索向量，而前缀树本质上是原始句子的一种“压缩表示”，所以不难想象前缀树所需要的储存空间要比稠密的检索向量要少得多。在Python中，实现前缀树比较简单的方案就是利用字典结构来实现嵌套，具体例子可以参考后面的KgCLUE的代码。

## KgCLUE [\#](https://kexue.fm/archives/8802\#KgCLUE)

实践是检验真理的唯一标准，现在我们就利用“Seq2Seq+前缀树”的方案实现一个KgCLUE的baseline，以检验它的有效性。

### 任务简介 [\#](https://kexue.fm/archives/8802\#%E4%BB%BB%E5%8A%A1%E7%AE%80%E4%BB%8B)

[KgCLUE](https://github.com/CLUEbenchmark/KgCLUE) 是CLUE组织最近推出的一个中文知识图谱问答的评测任务，数据比较规范，适合科研实验测试。具体来说，它以约2000万个三元组为知识库，每个三元组是$(S, P, O)$的格式（Subject-Predicate-Object），如下图：

KgCLUE知识库截图

然后它还提供了一批标注语料供训练，每个样本是简单的问题加答案的形式，其实问题本质上都可以抽象为“$S$的$P$是什么”，而答案则是对应的三原则$(S,P,O)$，如下图：

KgCLUE标注语料截图

原则上来说，只要确定了$(S,P)$，就可以从知识库里边抽取出对应的$O$，所以我们主要的任务，就是要从问题中解析出正确的$S$和$P$出来。

### 一般思路 [\#](https://kexue.fm/archives/8802\#%E4%B8%80%E8%88%AC%E6%80%9D%E8%B7%AF)

完成这个任务比较朴素的方法是分两步进行：第一步训练一个标注模型负责从问题中抽取$S$，然后从知识库中找出该$S$对应的所有三元组，将$S$分别每一个$P$组合起来，逐一与问题算相似度，所以第二步也就是一个相似度模型。

但要注意的是，知识库里边可能存在很多同名实体，比如“牛郎织女”，可能指神话故事，也可能指一本书或者一首歌等，为了对它们进行区分，知识库里边还有一个“义项（Meaning）”的概念，用来注明该名词的具体指代，在KgCLUE的知识库中，义项是通过括号的方式补充在Subject后面，比如“牛郎织女（中国著名民间故事）”、“牛郎织女（2015年东方出版社出版的图书）”等，但如果直接从问题抽取的话，通常只能抽取到义项以外的部分，即“牛郎织女”，所以我们实际处理时，通常会将它们分隔开，即将每条知识视为一个四元组$(S, M, P, O)$而不是三元组。

根据给定问题来确定Subject究竟属于哪个Meaning，这就是“实体链接”问题，是知识图谱问答的必要步骤。但如果局限在当前的KgCLUE任务中，由于语料本身不大，对于分两步的模型来说，我们可以认为实体链接这一步整合到了相似度模型中，而不单独进行实体链接任务。

### 本文方案 [\#](https://kexue.fm/archives/8802\#%E6%9C%AC%E6%96%87%E6%96%B9%E6%A1%88)

如果用“Seq2Seq+前缀树”来做的话，那么模型训练方面将会变得非常简单。

具体来说，我们就只需要一个Seq2Seq模型，然后将问题当作Seq2Seq的输入，将$(S,P,M)$用\[SEP\]连接起来作为目标，进行正常的Seq2Seq训练就行了。这里的一个技巧是按照$(S,P,M)$的顺序进行拼接，要比按照$(S,M,P)$的顺序的最终效果要好很多（8～10个百分点），这是因为问题来预测$S,P$要比预测$M$都更容易，我们要先把容易预测的预测出来，以减少候选答案的数量。

本文baseline模型示意图

在参考代码中，我们所用的Seq2Seq模型是 [《SimBERTv2来了！融合检索和生成的RoFormer-Sim模型》](https://kexue.fm/archives/8454)、 [《用开源的人工标注数据来增强RoFormer-Sim》](https://kexue.fm/archives/8541) 中介绍的RoFormer-Sim-FT模型，它是利用 [UniLM](https://kexue.fm/archives/6933) 模型预训练过的相似问生成模型，经过对比，用RoFormer-Sim-FT相比直接用RoFormer，效果至少提升2个百分点。这也说明相似问生成是该方案的一种有效的预训练方式。

### 错例分析 [\#](https://kexue.fm/archives/8802\#%E9%94%99%E4%BE%8B%E5%88%86%E6%9E%90)

解码的时候，我们先把所有的$(S,P,M)$建立成前缀树，然后按照前缀树进行解码，就保证了解码结果必然落到知识库的某个三元组中，从而能够合理地输出结果。详细的前缀解码步骤前面已经介绍过了，这里就不再细说。

然而，观察bad case的时候，发现模型有可能会出现一些非常“简单”的bad case，比如“海浦东香格里拉大酒店离火车站有多远？”，正确的$(S,P)$应该是“(上海浦东香格里拉大酒店, 火车站距离)”，但模型却生成了“(上海浦东香格里拉大酒店, 酒店星级)”；有时候问“XXX讨厌什么”，结果模型却生成了“(XXX, 喜欢)”；有时候问“XXX主要讲什么课程”，正确的答案应该是“(XXX, 主讲课程)”，结果模型却生成了“(XXX, 主要成就)”。也就是说，模型似乎会在一些字面上看起来非常简单的问题上犯错（生成错误的$P$）。

经过思考，笔者认为这种bad case本质上是Seq2Seq本身的固有缺点所导致的，主要包含两方面：1、训练时的Exposure Bias问题；2、解码时Beam Search的贪心问题。首先，由于Seq2Seq在训练时是已知上一真实标签的，这会弱化训练难度，导致模型的“全局观”不够；其次，解码哪怕用了Beam Search，本质上也是贪心的，很难做到综合后几个token来预测当前token。比如刚才的“XXX主要讲什么课程”一例，模型生成$P$的时候，首先就很贪心地生成“主要”两个字，然后按照前缀树的约束，“主要”后面只能接“成就”了（因为“主讲课程”前两个字是“主讲”），所以就出来了“主要成就”。

理论上，应用一些缓解Seq2Seq的Exposure Bias问题的策略，比如 [《Seq2Seq中Exposure Bias现象的浅析与对策》](https://kexue.fm/archives/7259)、 [《TeaForN：让Teacher Forcing更有“远见”一些》](https://kexue.fm/archives/7818) 等，应该是对此问题有帮助的。这些方法比较多，复杂度也有所不一样，就留给读者自行尝试了。

### “前瞻”策略 [\#](https://kexue.fm/archives/8802\#%E2%80%9C%E5%89%8D%E7%9E%BB%E2%80%9D%E7%AD%96%E7%95%A5)

这里笔者提出另一个能够缓解此问题的“前瞻”策略。

在“Seq2Seq+前缀树”方案中，我们的Seq2Seq不是真的要去生成任意文本，而是在前缀树的约束下做本质上是检索的操作，所以生成了$S$之后，我们可以知道后续容许的$P$，并且一般来说后续容许的$P$不会太多，所以我们可以直接逐一枚举容许的$P$，通过$P$对问题的覆盖程度，来调整当前token的预测结果。

具体的步骤是，假设当前问题$Q$已经解码出$S$，以及$P$的前$t-1$个字符$P\_{< t}$，现在要预测$P$的第$t$个字$P\_t$，那么我们根据$S,P\_{< t}$检索出所有可能的$P^{(1)},P^{(2)},\\cdots,P^{(n)}$，每个$P^{(k)}$可以表示为$\[P\_{< t},P\_t^{(k)},P\_{> t}^{(k)}\]$的格式；然后我们用一个覆盖度函数，这里直接用最长公共子序列长度$\\text{LCS}$，来算出每个候选的$P$所能带来的覆盖度增益：
\\begin{equation}\\Delta^{(k)} = \\text{LCS}(P^{(k)},Q) - \\text{LCS}(P\_{< t},Q)\\end{equation}
该增益视为将$P\_t$预测为$P\_t^{(k)}$的“潜在收益”，如果多个$P\_t^{(k)}$对应同一个字，那么就取最大者。

这样，对于$P\_t$的所有候选值$k$，我们都计算出来了一个“潜在收益”$\\Delta^{(k)}$，我们可以调整Seq2Seq的预测概率，来强化一下$\\Delta^{(k)}$的token。笔者用的强化规则为：
\\begin{equation}p\_k \\leftarrow p\_k^{1/(\\Delta^{(k)} + 1)}\\end{equation}
也就是如果潜在收益是$\\Delta^{(k)}$，那么对应的概率就开$\\Delta^{(k)}+1$次方，然后重新归一化，由于概率是小于1的，所以开方起到了放大的作用。该策略带来的收益，大概是4个百分点的提升。其他强化规则也可以尝试，这部分主观性比较强，就不一一列举了。

### 效果分析 [\#](https://kexue.fm/archives/8802\#%E6%95%88%E6%9E%9C%E5%88%86%E6%9E%90)

本文的代码分享在：

> **Github： [https://github.com/bojone/KgCLUE-bert4keras](https://github.com/bojone/KgCLUE-bert4keras)**

最终效果是：
$$\\begin{array}{c\|ccc}
\\hline
& \\text{F1} & \\text{EM} & \\text{平均} \\\
\\hline
\\text{valid} & 89.20 & 91.04 & 90.12\\\
\\text{test} & 90.25 & 92.48 & 91.37\\\
\\text{线上} & 86.03 & 88.45 & 87.24\\\
\\hline
\\end{array}$$

目前排行榜上排名第二：

当前KgCLUE榜单截图

调试历程大概是：

> 1、开始用RoFormer+UniLM按照$(S,M,P)$顺序预测，验证集EM大约是70多；
>
> 2、然后改为$(S,P,M)$的顺序之后，验证集能做到82了；
>
> 3、接着预训练模型换用RoFormer-Sim-FT后，提升到84～85；
>
> 4、最后加上“前瞻”策略，就做到当前的89了。

## 一些缺点 [\#](https://kexue.fm/archives/8802\#%E4%B8%80%E4%BA%9B%E7%BC%BA%E7%82%B9)

至此，我们已经比较详细地介绍了“Seq2Seq+前缀树”方案，并且以KgCLUE为例分享了一个baseline给大家。总的来说，“Seq2Seq+前缀树”有它的明显优点，并且能在检索任务上取得有竞争力的效果，但其中依然有一些问题值得思考。

最典型的问题，就是Seq2Seq本身的固有问题，这我们在前面已经提过了。虽然“前瞻”策略已经能给我们带来不错的提升，但相应的问题依然没有完全解决，如何更自然地解决该问题，或者设计更自然的解码规则，依然是值得思考，没有标准答案。此外，前面提到的解决Exposure Bias问题的通用策略，笔者也还没有尝试，不确定具体效果如何。

还有一个问题，就是“Seq2Seq+前缀树”的方案，是靠模型把结果“生成”的，而一旦遇到生僻字被识别为\[UNK\]，那么大概率就会失败了，尤其是如果S的第一个字就是\[UNK\]，那么几乎都会失败，所以如何更好地解决\[UNK\]问题，是一个值得研究的问题。当然，也可以尝试传统的Copy机制，这就看个人的审美了。

最后，“Seq2Seq+前缀树”或许可以在评测指标上取得不错的效果，但它对于工程来说，有一个不大有好的特点，就是修正bad case会变得比较困难，因为传统方法修正bad case，你只需要不断加样本就行了，而“Seq2Seq+前缀树”则需要你修改解码过程，这通常困难得多。

## 文章总结 [\#](https://kexue.fm/archives/8802\#%E6%96%87%E7%AB%A0%E6%80%BB%E7%BB%93)

本文介绍了检索模型的一种新方案——“Seq2Seq+前缀树”，并以KgCLUE为例给出了一个具体的baseline。“Seq2Seq+前缀树”的方案有着训练简单、空间占用少等优点，也有一些不足之处，总的来说算得上是一种简明的有竞争力的方案。

_**转载到请包括本文地址：** [https://kexue.fm/archives/8802](https://kexue.fm/archives/8802)_

_**更详细的转载事宜请参考：**_ [《科学空间FAQ》](https://kexue.fm/archives/6508#%E6%96%87%E7%AB%A0%E5%A6%82%E4%BD%95%E8%BD%AC%E8%BD%BD/%E5%BC%95%E7%94%A8)

**如果您还有什么疑惑或建议，欢迎在下方评论区继续讨论。**

**如果您觉得本文还不错，欢迎 [分享](https://kexue.fm/archives/8802#share)/ [打赏](https://kexue.fm/archives/8802#pay) 本文。打赏并非要从中获得收益，而是希望知道科学空间获得了多少读者的真心关注。当然，如果你无视它，也不会影响你的阅读。再次表示欢迎和感谢！**

打赏

微信打赏

支付宝打赏

因为网站后台对打赏并无记录，因此欢迎在打赏时候备注留言。你还可以 [**点击这里**](http://mail.qq.com/cgi-bin/qm_share?t=qm_mailme&email=tN7d1drY3drrx8H0xcWa19vZ) 或在下方评论区留言来告知你的建议或需求。

**如果您需要引用本文，请参考：**

苏剑林. (Dec. 17, 2021). 《Seq2Seq+前缀树：检索任务新范式（以KgCLUE为例） 》\[Blog post\]. Retrieved from [https://kexue.fm/archives/8802](https://kexue.fm/archives/8802)

@online{kexuefm-8802,
        title={Seq2Seq+前缀树：检索任务新范式（以KgCLUE为例）},
        author={苏剑林},
        year={2021},
        month={Dec},
        url={\\url{https://kexue.fm/archives/8802}},
}

分类： [信息时代](https://kexue.fm/category/Big-Data)    标签： [代码](https://kexue.fm/tag/%E4%BB%A3%E7%A0%81/), [语义](https://kexue.fm/tag/%E8%AF%AD%E4%B9%89/), [keras](https://kexue.fm/tag/keras/), [相似度](https://kexue.fm/tag/%E7%9B%B8%E4%BC%BC%E5%BA%A6/)[17 评论](https://kexue.fm/archives/8802#comments)

< [输入梯度惩罚与参数梯度惩罚的一个不等式](https://kexue.fm/archives/8796) \| [从熵不变性看Attention的Scale操作](https://kexue.fm/archives/8823) >

### 你也许还对下面的内容感兴趣

- [旁门左道之如何让Python的重试代码更加优雅](https://kexue.fm/archives/9938)
- [局部余弦相似度大，全局余弦相似度一定也大吗？](https://kexue.fm/archives/9931)
- [从局部到全局：语义相似度的测地线距离](https://kexue.fm/archives/9368)
- [CoSENT（三）：作为交互式相似度的损失函数](https://kexue.fm/archives/9341)
- [利用CUR分解加速交互式相似度模型的检索](https://kexue.fm/archives/9336)
- [当BERT-whitening引入超参数：总有一款适合你](https://kexue.fm/archives/9079)
- [CoSENT（二）：特征式匹配与交互式匹配有多大差距？](https://kexue.fm/archives/8860)
- [CoSENT（一）：比Sentence-BERT更有效的句向量方案](https://kexue.fm/archives/8847)
- [bert4keras在手，baseline我有：CLUE基准代码](https://kexue.fm/archives/8739)
- [从三角不等式到Margin Softmax](https://kexue.fm/archives/8656)

[发表你的看法](https://kexue.fm/archives/8802#comment_form)

1. [«](https://kexue.fm/archives/8802/comment-page-1#comments)
2. [1](https://kexue.fm/archives/8802/comment-page-1#comments)
3. [2](https://kexue.fm/archives/8802/comment-page-2#comments)

xunfeng

January 10th, 2022

感觉用于实际工程还有很多问题，比如calibration，比如知识的更新（删除过期知识、替换原有知识、增加新知识）…

[回复评论](https://kexue.fm/archives/8802/comment-page-2?replyTo=18215#respond-post-8802)

1. [«](https://kexue.fm/archives/8802/comment-page-1#comments)
2. [1](https://kexue.fm/archives/8802/comment-page-1#comments)
3. [2](https://kexue.fm/archives/8802/comment-page-2#comments)

[取消回复](https://kexue.fm/archives/8802#respond-post-8802)

你的大名

电子邮箱

个人网站（选填）

1\. 可以使用LaTeX代码，点击“预览效果”可查看效果；2. 可以通过点击评论楼层编号来引用该楼层；3. 网站可能会有点卡，如非确认评论失败，请 **不要重复点击提交**。

### 内容速览

[检索任务](https://kexue.fm/archives/8802#%E6%A3%80%E7%B4%A2%E4%BB%BB%E5%8A%A1)
[Seq2Seq](https://kexue.fm/archives/8802#Seq2Seq)
[前缀解码](https://kexue.fm/archives/8802#%E5%89%8D%E7%BC%80%E8%A7%A3%E7%A0%81)
[KgCLUE](https://kexue.fm/archives/8802#KgCLUE)
[任务简介](https://kexue.fm/archives/8802#%E4%BB%BB%E5%8A%A1%E7%AE%80%E4%BB%8B)
[一般思路](https://kexue.fm/archives/8802#%E4%B8%80%E8%88%AC%E6%80%9D%E8%B7%AF)
[本文方案](https://kexue.fm/archives/8802#%E6%9C%AC%E6%96%87%E6%96%B9%E6%A1%88)
[错例分析](https://kexue.fm/archives/8802#%E9%94%99%E4%BE%8B%E5%88%86%E6%9E%90)
[“前瞻”策略](https://kexue.fm/archives/8802#%E2%80%9C%E5%89%8D%E7%9E%BB%E2%80%9D%E7%AD%96%E7%95%A5)
[效果分析](https://kexue.fm/archives/8802#%E6%95%88%E6%9E%9C%E5%88%86%E6%9E%90)
[一些缺点](https://kexue.fm/archives/8802#%E4%B8%80%E4%BA%9B%E7%BC%BA%E7%82%B9)
[文章总结](https://kexue.fm/archives/8802#%E6%96%87%E7%AB%A0%E6%80%BB%E7%BB%93)

### 智能搜索

支持整句搜索！网站自动使用 [结巴分词](https://github.com/fxsjy/jieba) 进行分词，并结合ngrams排序算法给出合理的搜索结果。

### 热门标签

[生成模型](https://kexue.fm/tag/%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B/) [attention](https://kexue.fm/tag/attention/) [优化](https://kexue.fm/tag/%E4%BC%98%E5%8C%96/) [语言模型](https://kexue.fm/tag/%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/) [模型](https://kexue.fm/tag/%E6%A8%A1%E5%9E%8B/) [网站](https://kexue.fm/tag/%E7%BD%91%E7%AB%99/) [概率](https://kexue.fm/tag/%E6%A6%82%E7%8E%87/) [梯度](https://kexue.fm/tag/%E6%A2%AF%E5%BA%A6/) [转载](https://kexue.fm/tag/%E8%BD%AC%E8%BD%BD/) [微分方程](https://kexue.fm/tag/%E5%BE%AE%E5%88%86%E6%96%B9%E7%A8%8B/) [矩阵](https://kexue.fm/tag/%E7%9F%A9%E9%98%B5/) [天象](https://kexue.fm/tag/%E5%A4%A9%E8%B1%A1/) [分析](https://kexue.fm/tag/%E5%88%86%E6%9E%90/) [深度学习](https://kexue.fm/tag/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/) [积分](https://kexue.fm/tag/%E7%A7%AF%E5%88%86/) [python](https://kexue.fm/tag/python/) [优化器](https://kexue.fm/tag/%E4%BC%98%E5%8C%96%E5%99%A8/) [力学](https://kexue.fm/tag/%E5%8A%9B%E5%AD%A6/) [无监督](https://kexue.fm/tag/%E6%97%A0%E7%9B%91%E7%9D%A3/) [扩散](https://kexue.fm/tag/%E6%89%A9%E6%95%A3/) [几何](https://kexue.fm/tag/%E5%87%A0%E4%BD%95/) [节日](https://kexue.fm/tag/%E8%8A%82%E6%97%A5/) [生活](https://kexue.fm/tag/%E7%94%9F%E6%B4%BB/) [文本生成](https://kexue.fm/tag/%E6%96%87%E6%9C%AC%E7%94%9F%E6%88%90/) [数论](https://kexue.fm/tag/%E6%95%B0%E8%AE%BA/)

### 随机文章

- [\[公告\]评论功能说明](https://kexue.fm/archives/1908)
- [2015诺贝尔医学奖：中国人在内](https://kexue.fm/archives/3473)
- [一道级数求和证明题(非数学归纳法)](https://kexue.fm/archives/49)
- [一道从小学到高中都可能考到的题目](https://kexue.fm/archives/132)
- [简述无偏估计和有偏估计](https://kexue.fm/archives/6747)
- [科学空间：2011年4月重要天象](https://kexue.fm/archives/1299)
- [百科翻译：盐酸的历史（氯化氢，HCl）](https://kexue.fm/archives/9)
- [MoE环游记：4、难处应当多投入](https://kexue.fm/archives/10815)
- [SimBERTv2来了！融合检索和生成的RoFormer-Sim模型](https://kexue.fm/archives/8454)
- [写在2013年即将逝去之际](https://kexue.fm/archives/2236)

### 最近评论

- [石子131](https://kexue.fm/archives/9405/comment-page-2#comment-27955): 也许可以尝试把热水管的回水管的开关阀做成用户的手动阀，在热水管临近回水管、手动阀靠近热水管侧加...
- [Kuo](https://kexue.fm/archives/10795/comment-page-1#comment-27954): 我的理解，这是一个迭代过程，注意下标K是指condition还是desideratum
- [musicfish1973](https://kexue.fm/archives/8009/comment-page-1#comment-27953): 好的设计都是相似的,haha
- [忍者猫](https://kexue.fm/archives/10592/comment-page-2#comment-27952): 这优化器的作者真的应该给你打钱
- [Chaofa Yuan](https://kexue.fm/archives/11033/comment-page-1#comment-27951): 写得太好了
- [Skyler Lin](https://kexue.fm/archives/11033/comment-page-1#comment-27949): respect苏神！
- [宋佳铭](https://kexue.fm/archives/10958/comment-page-1#comment-27947): 对，个人感觉mean flow就是continuous time CTM
- [宋佳铭](https://kexue.fm/archives/10958/comment-page-1#comment-27946): 的确，对sg这个事情我感觉如果是用‘归纳’法做是不太能避免的，因为毕竟是用步长短的模型去约束步...
- [MoFHeka](https://kexue.fm/archives/10542/comment-page-1#comment-27945): 苏老师您好，请问一下这套结论在稀疏参数上应该如何应用？比如大规模稀疏Embedding，每个B...
- [苏剑林](https://kexue.fm/archives/11033/comment-page-1#comment-27944): Temp LoRA倒是有印象，其实思想是一样的，如果我单独开一篇文章介绍TTT的话，应该会提到...

### 友情链接

- [Cool Papers](https://papers.cool)
- [数学研发](https://bbs.emath.ac.cn)
- [Seatop](http://www.seatop.com.cn/)
- [Xiaoxia](https://xiaoxia.org/)
- [积分表-网络版](https://kexue.fm/sci/integral/index.html)
- [丝路博傲](http://blog.dvxj.com/)
- [ph4ntasy 饭特稀](http://www.ph4ntasy.com/)
- [数学之家](http://www.2math.cn/)
- [有趣天文奇观](http://interesting-sky.china-vo.org/)
- [TwistedW](http://www.twistedwg.com/)
- [godweiyang](https://godweiyang.com/)
- [AI柠檬](https://blog.ailemon.net/)
- [王登科-DK博客](https://greatdk.com)
- [ESON](https://blog.eson.org/)
- [枫之羽](https://fzhiy.net/)
- [Mathor's blog](https://wmathor.com/)
- [coding-zuo](https://coding-zuo.github.io/)
- [博科园](https://www.bokeyuan.net/)
- [孔皮皮的博客](https://www.kppkkp.top/)
- [运鹏的博客](https://yunpengtai.top/)
- [jiming.site](https://jiming.site/)
- [OmegaXYZ](https://www.omegaxyz.com/)
- [Blog by Eacls](https://www.eacls.top/)
- [EAI猩球](https://www.robotech.ink/)
- [文举的博客](https://liwenju0.com/)
- [用代码打点酱油](https://bruceyuan.com/)
- [申请链接](https://kexue.fm/links.html)

本站采用创作共用版权协议，要求署名、非商业用途和保持一致。转载本站内容必须也遵循“ [署名-非商业用途-保持一致](http://creativecommons.org/licenses/by-nc-nd/2.5/cn/)”的创作共用协议。
© 2009-2025 Scientific Spaces. All rights reserved. Theme by [laogui](http://www.laogui.com). Powered by [Typecho](http://typecho.org). 备案号: [粤ICP备09093259号-1/2](https://beian.miit.gov.cn/)。