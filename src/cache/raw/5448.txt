## SEARCH

## MENU

- [打赏](https://kexue.fm/reward.html)
- [公式](https://kexue.fm/latex.html)
- [天象](https://kexue.fm/ac.html)
- [链接](https://kexue.fm/links.html)
- [时光](https://kexue.fm/me.html)
- [博览](https://kexue.fm/science.html)
- [归档](https://kexue.fm/content.html)

## CATEGORIES

- [千奇百怪](https://kexue.fm/category/Everything)
- [天文探索](https://kexue.fm/category/Astronomy)
- [数学研究](https://kexue.fm/category/Mathematics)
- [物理化学](https://kexue.fm/category/Phy-chem)
- [信息时代](https://kexue.fm/category/Big-Data)
- [生物自然](https://kexue.fm/category/Biology)
- [图片摄影](https://kexue.fm/category/Photograph)
- [问题百科](https://kexue.fm/category/Questions)
- [生活/情感](https://kexue.fm/category/Life-Feeling)
- [资源共享](https://kexue.fm/category/Resources)

## NEWPOSTS

- [“对角+低秩”三角阵的高效求逆方法](https://kexue.fm/archives/11072)
- [通过msign来计算奇异值裁剪mc...](https://kexue.fm/archives/11059)
- [矩阵符号函数mcsgn能计算什么？](https://kexue.fm/archives/11056)
- [线性注意力简史：从模仿、创新到反哺](https://kexue.fm/archives/11033)
- [msign的导数](https://kexue.fm/archives/11025)
- [通过msign来计算奇异值裁剪mc...](https://kexue.fm/archives/11006)
- [msign算子的Newton-Sc...](https://kexue.fm/archives/10996)
- [等值振荡定理：最优多项式逼近的充要条件](https://kexue.fm/archives/10972)
- [生成扩散模型漫谈（三十）：从瞬时速...](https://kexue.fm/archives/10958)
- [MoE环游记：5、均匀分布的反思](https://kexue.fm/archives/10945)

## COMMENTS

- [Kuo: 在 $PaTH$ 论文章节 \`UT Transform for...](https://kexue.fm/archives/11033/comment-page-1#comment-28053)
- [Fanhao: 假定Hessian阵正定，那不是意味着$L(\\theta)$是...](https://kexue.fm/archives/10542/comment-page-1#comment-28052)
- [曲笑一: 对于第一个疑问，我看到分布式的版本已经开源。我在想如果将每个梯...](https://kexue.fm/archives/10739/comment-page-2#comment-28051)
- [曲笑一: 苏老师您好，阅读了您关于Muon系列的博客，受益匪浅。在此有两...](https://kexue.fm/archives/10739/comment-page-2#comment-28050)
- [tll1945tll1937: 老师，您好，向您请教一个问题：会不会因为LoRA中用到的梯度的...](https://kexue.fm/archives/10266/comment-page-1#comment-28049)
- [香蕉大王: 还是刚刚flow matching的例子$\\frac{d x\_...](https://kexue.fm/archives/9280/comment-page-2#comment-28048)
- [香蕉大王: 谢谢老师回复。明白老师您说的了。我再想请问一个小小的问题：既然...](https://kexue.fm/archives/9280/comment-page-2#comment-28047)
- [NoAmateur: 2025年第一次了解到苏前辈，进入科学空间后怀着好奇翻阅苏前辈...](https://kexue.fm/archives/12/comment-page-7#comment-28046)
- [liukoulong: $T^{3}$](https://kexue.fm/archives/11033/comment-page-1#comment-28043)
- [nihaowhut: 请问ϵ为什么要限制到2^-7, 2^-6, .. 2^-1, ...](https://kexue.fm/archives/10617/comment-page-1#comment-28042)

## USERLOGIN

- [登录](https://kexue.fm/admin/login.php)

[科学空间\|Scientific Spaces](https://kexue.fm)

- [登录](https://kexue.fm/admin/login.php)
- [打赏](https://kexue.fm/reward.html)
- [公式](https://kexue.fm/latex.html)
- [天象](https://kexue.fm/ac.html)
- [链接](https://kexue.fm/links.html)
- [时光](https://kexue.fm/me.html)
- [博览](https://kexue.fm/science.html)
- [归档](https://kexue.fm/content.html)

渴望成为一个小飞侠

- [欢迎订阅](https://kexue.fm/feed)
- [个性邮箱](https://kexue.fm/archives/119)
- [天象信息](https://kexue.fm/ac.html)
- [观测ISS](https://kexue.fm/archives/41)
- [LaTeX](https://kexue.fm/latex.html)
- [关于博主](https://kexue.fm/me.html)

欢迎访问“科学空间”，这里将与您共同探讨自然科学，回味人生百态；也期待大家的分享～

- [**千奇百怪** Everything](https://kexue.fm/category/Everything)
- [**天文探索** Astronomy](https://kexue.fm/category/Astronomy)
- [**数学研究** Mathematics](https://kexue.fm/category/Mathematics)
- [**物理化学** Phy-chem](https://kexue.fm/category/Phy-chem)
- [**信息时代** Big-Data](https://kexue.fm/category/Big-Data)
- [**生物自然** Biology](https://kexue.fm/category/Biology)
- [**图片摄影** Photograph](https://kexue.fm/category/Photograph)
- [**问题百科** Questions](https://kexue.fm/category/Questions)
- [**生活/情感** Life-Feeling](https://kexue.fm/category/Life-Feeling)
- [**资源共享** Resources](https://kexue.fm/category/Resources)

- [**千奇百怪**](https://kexue.fm/category/Everything)
- [**天文探索**](https://kexue.fm/category/Astronomy)
- [**数学研究**](https://kexue.fm/category/Mathematics)
- [**物理化学**](https://kexue.fm/category/Phy-chem)
- [**信息时代**](https://kexue.fm/category/Big-Data)
- [**生物自然**](https://kexue.fm/category/Biology)
- [**图片摄影**](https://kexue.fm/category/Photograph)
- [**问题百科**](https://kexue.fm/category/Questions)
- [**生活/情感**](https://kexue.fm/category/Life-Feeling)
- [**资源共享**](https://kexue.fm/category/Resources)

[首页](https://kexue.fm) [信息时代](https://kexue.fm/category/Big-Data) 最小熵原理（一）：无监督学习的原理

18Apr

# [最小熵原理（一）：无监督学习的原理](https://kexue.fm/archives/5448)

By 苏剑林 \|
2018-04-18 \|
101700位读者\|

## 话在开头 [\#](https://kexue.fm/archives/5448\#%E8%AF%9D%E5%9C%A8%E5%BC%80%E5%A4%B4)

> 在深度学习等端到端方案已经逐步席卷NLP的今天，你是否还愿意去思考自然语言背后的基本原理？我们常说“文本挖掘”，你真的感受到了“挖掘”的味道了吗？

### 无意中的邂逅 [\#](https://kexue.fm/archives/5448\#%E6%97%A0%E6%84%8F%E4%B8%AD%E7%9A%84%E9%82%82%E9%80%85)

前段时间看了一篇关于 [无监督句法分析的文章](http://www.52nlp.cn/%E4%B8%80%E7%A7%8D%E5%9F%BA%E4%BA%8E%E7%94%9F%E8%AF%AD%E6%96%99%E7%9A%84%E6%97%A0%E7%9B%91%E7%9D%A3%E7%9A%84%E8%AF%AD%E6%B3%95%E8%A7%84%E5%88%99%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95)，继而从它的参考文献中发现了论文 [《Redundancy Reduction as a Strategy for Unsupervised Learning》](https://kexue.fm/usr/uploads/2018/04/3543773848.pdf)，这篇论文介绍了如何从去掉空格的英文文章中将英文单词复原。对应到中文，这不就是词库构建吗？于是饶有兴致地细读了一番，发现论文思路清晰、理论完整、结果漂亮，让人赏心悦目。

尽管现在看来，这篇论文的价值不是很大，甚至其结果可能已经被很多人学习过了，但是要注意：这是一篇1993年的论文！在PC机还没有流行的年代，就做出了如此前瞻性的研究。虽然如今深度学习流行，NLP任务越做越复杂，这确实是一大进步，但是我们对NLP原理的真正了解，还不一定超过几十年前的前辈们多少。

这篇论文是通过“去冗余”（Redundancy Reduction）来实现无监督地构建词库的，从信息论的角度来看，“去冗余”就是信息熵的最小化。无监督句法分析那篇文章也指出“信息熵最小化是无监督的NLP的唯一可行的方案”。我进而学习了一些相关资料，并且结合自己的理解思考了一番，发现这个评论确实是耐人寻味。我觉得，不仅仅是NLP，信息熵最小化很可能是所有无监督学习的根本。

### 何为最小熵原理？ [\#](https://kexue.fm/archives/5448\#%E4%BD%95%E4%B8%BA%E6%9C%80%E5%B0%8F%E7%86%B5%E5%8E%9F%E7%90%86%EF%BC%9F)

读者或许已经听说过 [最大熵原理](https://kexue.fm/archives/3552#%E6%9C%80%E5%A4%A7%E7%86%B5%E5%8E%9F%E7%90%86) 和 [最大熵模型](https://kexue.fm/archives/3567#%E6%9C%80%E5%A4%A7%E7%86%B5%E6%A8%A1%E5%9E%8B)，现在这个最小熵原理又是什么？它跟最大熵不矛盾吗？

我们知道，熵是不确定性的度量，最大熵原理的意思就是说我们在对结果进行推测时，要承认我们的无知，所以要最大化不确定性，以得到最客观的结果。而对于最小熵原理，我们有两个理解角度：

> 1、直观的理解：文明的演化过程总是一个探索和发现的过程，经过我们的努力，越多越多的东西从不确定变成了确定，熵逐渐地趋于最小化。因此，要从一堆原始数据中发现隐含的规律（把文明重演出来），就要在这个规律是否有助于降低总体的信息熵，因为这代表了文明演化的方向，这就是“最小熵原理”。
>
> 2、稍严谨的理解：“知识”有一个固有信息熵，代表它的本质信息量。但在我们彻底理解它之前，总会有未知的因素，这使得我们在表达它的时候带有冗余，所以按照我们当前的理解去估算信息熵，得到的事实上是固有信息熵的上界，而信息熵最小化意味着我们要想办法降低这个上界，也就意味着减少了未知，逼近固有信息熵。

于是，我沿着“最小熵原理”这条路，重新整理了前人的工作，并做了一些新的拓展，最终就有了这些文字。读者将会慢慢看到，最小熵原理能用一种极具解释性和启发性的方法来导出丰富的结果来。

## 语言的信息 [\#](https://kexue.fm/archives/5448\#%E8%AF%AD%E8%A8%80%E7%9A%84%E4%BF%A1%E6%81%AF)

让我们从考究语言的信息熵开始，慢慢进入这个最小熵原理的世界～

### 信息熵=学习成本 [\#](https://kexue.fm/archives/5448\#%E4%BF%A1%E6%81%AF%E7%86%B5=%E5%AD%A6%E4%B9%A0%E6%88%90%E6%9C%AC)

从 [《“熵”不起：从熵、最大熵原理到最大熵模型（一）》](https://kexue.fm/archives/3534/) 我们可以知道，一个对象的信息熵是正比于它的概率的负对数的，也就是
$$I(c)\\sim -\\log p\_c\\tag{1.1}$$
如果认为中文的基本单位是字，那么中文就是字的组合，$p\_c$就意味着对应字的概率，而$-\\log p\_c$就是该字的信息量。我们可以通过大量的语料估算每个汉字的平均信息：
$$\\mathcal{H}\_c = -\\sum\_{c\\in\\text{汉字}} p\_c\\log p\_c\\tag{1.2}$$
如果$\\log$是以2为底的话，那么根据网上流传的数据，这个值约是9.65比特（我自己统计了一些文章，得到的数值约为9.5，两者相当）。类似地，英文中每个字母的平均信息约是4.03比特。

这个数字意味着什么呢？一般可以认为，我们接收或记忆信息的速度是固定的，那么这个信息量的大小事实上也就相当于我们接收这个信息所需要的时间（或者所花费的精力，等等），从而也可以说这个数字意味着我们学习这个东西的难度（记忆负荷）。比如，假设我们每秒只能接收1比特的信息，那么按字来记忆一篇800字文章，就需要$9.65\\times 800$秒的时间。

### 中英文孰优孰劣？ [\#](https://kexue.fm/archives/5448\#%E4%B8%AD%E8%8B%B1%E6%96%87%E5%AD%B0%E4%BC%98%E5%AD%B0%E5%8A%A3%EF%BC%9F)

既然中文单字信息熵是9.65，而英文字母信息熵才4.03，这是不是意味着英文是一种更高效的表达呢？

显然不能这么武断， **难道背英语作文一定比背诵汉语作文要容易么？**

比如800字的中文作文翻译成英文的话，也许就有500词了，平均每个英文4个字母，那么信息量就是$4.03\\times 500\\times 4 \\approx 9.65\\times 800$，可见它们是相当的。换句话说，比较不同文字单元的信息量是没有意义的，有意义的是信息总量，也就是说描述同样的意思时谁更简练。

当两句话的意思一样时，这个“意思”的固有信息量是不变的，但用不同语言表达时，就不可避免引入“冗余”，所以不同语言表达起来的信息量不一样，这个信息量其实就相当于记忆负荷，越累赘的语言信息量越大，记忆负荷越大。就好比教同样的课程，有的老师教得清晰明了，学生轻松地懂了，有的老师教得哆里哆嗦，学生也学得很痛苦。同样是一节课程，本质上它们的知识量是一样的，而教得差的老师，表明授课过程中带来了过多的无关信息量，这就是“冗余”，所以我们要想办法“去冗余”。

上述中英文的估计结果相当，表明中英文都经过长期的优化，双方都大致达到了一个比较优化的状态，并没有谁明显优于谁的说法。

## 套路之路 [\#](https://kexue.fm/archives/5448\#%E5%A5%97%E8%B7%AF%E4%B9%8B%E8%B7%AF)

注意，上面的估算中，我们强调了“按字来记忆”，也许我们太熟悉中文了，没意识到这意味着什么，事实上这代表了一种很机械的记忆方式，而我们事实上不是这样做的。

### 念经也有学问 [\#](https://kexue.fm/archives/5448\#%E5%BF%B5%E7%BB%8F%E4%B9%9F%E6%9C%89%E5%AD%A6%E9%97%AE)

回顾我们小时候背诵古诗、文言文的情景，刚开始我们是完全不理解、囫囵吞枣地背诵，也就是每个字都认识、串起来就不知道什么含义的那种，这就是所谓的“按字来阅读”了。很显然，这样的记忆难度是很大的。后来，我们也慢慢去揣摩古文的成文规律了，逐渐能理解一些古诗或文言文的含义了，背诵难度就会降下来。到了高中，我们还会学习到文言文中“宾语前置”、“定语后置”之类的语法规律，这对我们记忆和理解文言文都是很有帮助的。

重点来了！

从古文的例子我们就能够感受到，像念经一样逐字背诵是很困难的，组词理解后就容易些，如果能找到一些语法规律，就更加容易记忆和理解了。但是我们接收（记忆）信息的速度还是固定的，这也就意味着分词、语法这些步骤，降低了语言的信息量，从而降低了我们的学习成本！

再细想一下，其实不单单是语言，我们学习任何东西都是这样的，如果只有少数的内容要学习，那么我们强行记住就行了，但学习的东西比较多时，我们就试图找出其中的“套路”，比如中国象棋中就分开局、中局、残局，每种局面都有很多“定式”，用来降低初学者的学习难度，而且也是在复杂局面中应变的基础；再好比我们有《孙子兵法》、《三十六计》，这些都是“套路大全”。通过挖掘“套路”来减轻逐一记忆的负担，“套路”的出现就是一个减少信息量的过程。

说到底，念经念多了，也能发现经文的套路了。

### 以不变应万变 [\#](https://kexue.fm/archives/5448\#%E4%BB%A5%E4%B8%8D%E5%8F%98%E5%BA%94%E4%B8%87%E5%8F%98)

一言以蔽之，我们接收信息的速度是固定的，所以加快我们的学习进度的唯一方法就是降低学习目标的冗余信息量，所谓“去芜存菁”，这就是NLP中的最小熵原理了，也就是一开始所提到的“去冗余”，我们可以理解为是“省去没必要的学习成本”。

事实上，一个高效的学习过程必定能体现出这个思想，同样地，教师也是根据这个思想来设计他们的教学计划的。在教学的时候，教师更倾向于讲授那些“通解通法”（哪怕步骤多一点），也不会选择每一题都讲一种独特的、巧妙的解法；在准备高考时，我们会努力摸索各种出题套路、解题套路。这些都是通过挖掘“套路”来降低信息熵、从而降低学习成本的过程，“套路”就是“去冗余”的方法。

“套路”即“定式”，有了足够多的套路，我们才能以不变应万变。所谓“万变不离其宗”，这个“宗”，也就是套路了吧。当“套路”过多时，我们又会进一步寻找“套套路”——即套路的套路，来减轻我们记忆套路的负担，这是一个层层递进的过程。看来，将个体现象上升为套路，正是人类的智能的体现呢～

好了，空话不宜多说，接下来我们就正式走上修炼套路的旅途。

_**转载到请包括本文地址：** [https://kexue.fm/archives/5448](https://kexue.fm/archives/5448)_

_**更详细的转载事宜请参考：**_ [《科学空间FAQ》](https://kexue.fm/archives/6508#%E6%96%87%E7%AB%A0%E5%A6%82%E4%BD%95%E8%BD%AC%E8%BD%BD/%E5%BC%95%E7%94%A8)

**如果您还有什么疑惑或建议，欢迎在下方评论区继续讨论。**

**如果您觉得本文还不错，欢迎 [分享](https://kexue.fm/archives/5448#share)/ [打赏](https://kexue.fm/archives/5448#pay) 本文。打赏并非要从中获得收益，而是希望知道科学空间获得了多少读者的真心关注。当然，如果你无视它，也不会影响你的阅读。再次表示欢迎和感谢！**

打赏

微信打赏

支付宝打赏

因为网站后台对打赏并无记录，因此欢迎在打赏时候备注留言。你还可以 [**点击这里**](http://mail.qq.com/cgi-bin/qm_share?t=qm_mailme&email=tN7d1drY3drrx8H0xcWa19vZ) 或在下方评论区留言来告知你的建议或需求。

**如果您需要引用本文，请参考：**

苏剑林. (Apr. 18, 2018). 《最小熵原理（一）：无监督学习的原理 》\[Blog post\]. Retrieved from [https://kexue.fm/archives/5448](https://kexue.fm/archives/5448)

@online{kexuefm-5448,
        title={最小熵原理（一）：无监督学习的原理},
        author={苏剑林},
        year={2018},
        month={Apr},
        url={\\url{https://kexue.fm/archives/5448}},
}

分类： [信息时代](https://kexue.fm/category/Big-Data)    标签： [熵](https://kexue.fm/tag/%E7%86%B5/), [无监督](https://kexue.fm/tag/%E6%97%A0%E7%9B%91%E7%9D%A3/), [NLP](https://kexue.fm/tag/NLP/), [最小熵](https://kexue.fm/tag/%E6%9C%80%E5%B0%8F%E7%86%B5/)[14 评论](https://kexue.fm/archives/5448#comments)

< [基于CNN的阅读理解式问答模型：DGCNN](https://kexue.fm/archives/5409) \| [最小熵原理（二）：“当机立断”之词库构建](https://kexue.fm/archives/5476) >

### 你也许还对下面的内容感兴趣

- [矩阵的有效秩（Effective Rank）](https://kexue.fm/archives/10847)
- [生成扩散模型漫谈（二十三）：信噪比与大图生成（下）](https://kexue.fm/archives/10055)
- [注意力机制真的可以“集中注意力”吗？](https://kexue.fm/archives/9889)
- [BytePiece：更纯粹、更高压缩率的Tokenizer](https://kexue.fm/archives/9752)
- [如何度量数据的稀疏程度？](https://kexue.fm/archives/9595)
- [注意力和Softmax的两点有趣发现：鲁棒性和信息量](https://kexue.fm/archives/9593)
- [从JL引理看熵不变性Attention](https://kexue.fm/archives/9588)
- [用热传导方程来指导自监督学习](https://kexue.fm/archives/9359)
- [熵不变性Softmax的一个快速推导](https://kexue.fm/archives/9034)
- [听说Attention与Softmax更配哦～](https://kexue.fm/archives/9019)

[发表你的看法](https://kexue.fm/archives/5448#comment_form)

sundllyq

April 18th, 2018

期待！！！

[回复评论](https://kexue.fm/archives/5448/comment-page-1?replyTo=9034#respond-post-5448)

sandove

April 20th, 2018

期待大神后续的见解！

[回复评论](https://kexue.fm/archives/5448/comment-page-1?replyTo=9054#respond-post-5448)

aluminumbox

January 2nd, 2019

个人理解最大熵就是训练过程中希望模型无偏好，能拟合到客观结果。最小熵则是在推理阶段，使用认为最有可能的推论。望指正。

[回复评论](https://kexue.fm/archives/5448/comment-page-1?replyTo=10472#respond-post-5448)

[苏剑林](https://kexue.fm) 发表于
January 2nd, 2019

最大熵公平地对待各种可能性，是“承认自己的无知”；最小熵是认为自然界（也可以理解为其他人）在很多情况下都已经达到最优状态，是“接受他人的聪明”。

这跟下棋的道理一样，“最大熵”意味着我们无法断定对手下一步究竟走什么，“最小熵”意味着我们必须假设，对手下一步会走对他那一方最有利的那一步。

[回复评论](https://kexue.fm/archives/5448/comment-page-1?replyTo=10474#respond-post-5448)

白云 发表于
February 28th, 2023

哇，这么一解释豁然开朗啊！厉害！

[回复评论](https://kexue.fm/archives/5448/comment-page-1?replyTo=21022#respond-post-5448)

8f

May 26th, 2019

有点意思，特意注册一个账号和苏先生交流一下。
1，从信息论的角度来看，“去冗余”就是信息熵的最小化。这个提法欠妥。
冗余并没有增加熵，只是从信宿的角度，改变了理解信息的难易，信源的entropy或者也就是你说的固有的entropy是没有变的。冗余可能增加或者减少信宿接收处理信息的难易。

2，所谓学习，就是指从信宿的角度如何学习。“所以加快我们的学习进度的唯一方法就是降低学习目标的冗余信息量”，唯一方法应该是重构数据的表现形式，去除冗余或者增加冗余，只要有助于信宿“接受”，“理解”都是好的方法。人脑有人脑的学习模式（如抽象，因果，结构化），机器有机器的学习模式（目前主要是统计学习方法）。无关冗余，而在信宿prefer的“数据表示”也。

[回复评论](https://kexue.fm/archives/5448/comment-page-1?replyTo=11234#respond-post-5448)

felixdae

November 12th, 2019

联想起侯世达的geb

[回复评论](https://kexue.fm/archives/5448/comment-page-1?replyTo=12348#respond-post-5448)

[数据挖掘：数据清洗——数据噪声处理 \- 算法网](https://itpcb.com/a/1320952)

October 3rd, 2021

\[...\]数据挖掘：数据清洗——数据噪声处理 \- 算法网 算法网首页精品教程数据结构时间复杂度空间复杂度树二叉查找树满二叉树完全二叉树平衡二叉树红黑树B树图队列散列表链表算法常用算法排序算法贪心算法递归算法动态规划分治算法回溯法分支限界法拓扑排序字符串相关算法数组相关算法链表相关算法树相关算法二叉树相关算法LeetCodeOnline Judge剑指offer编程语言javajava并发Java多线程\[...\]

[回复评论](https://kexue.fm/archives/5448/comment-page-1?replyTo=17474#respond-post-5448)

宇航员

October 9th, 2021

苏神讲故事也是一把好手啊！

[回复评论](https://kexue.fm/archives/5448/comment-page-1?replyTo=17519#respond-post-5448)

sxj

November 18th, 2023

已经看了知乎，但是对式2.9和2.10的推导没看懂，能将详细过程说下吗？谢谢

[回复评论](https://kexue.fm/archives/5448/comment-page-1?replyTo=23095#respond-post-5448)

[苏剑林](https://kexue.fm) 发表于
November 20th, 2023

第二篇文章的问题为啥问到第一篇文章来...

我看了一下，看上去那处不算十分难理解，也许你可以尝试多读几遍。

[回复评论](https://kexue.fm/archives/5448/comment-page-1?replyTo=23124#respond-post-5448)

龙行

November 20th, 2023

假设存在语义信息，本文的意思应该是保持语义信息量不变，降低数据信息量，提高训练效率。怎么降低，就是分词，分句等预处理。但是这样能提高网络表现吗？直观感觉会降低网络表现。

[回复评论](https://kexue.fm/archives/5448/comment-page-1?replyTo=23098#respond-post-5448)

[苏剑林](https://kexue.fm) 发表于
November 20th, 2023

高压缩率Tokenizer是目前LLM的基础组件之一。

[回复评论](https://kexue.fm/archives/5448/comment-page-1?replyTo=23126#respond-post-5448)

龙行 发表于
November 21st, 2023

感谢苏神，这里我理解错了。降低数据信息量，不等于降低数据量。

[回复评论](https://kexue.fm/archives/5448/comment-page-1?replyTo=23138#respond-post-5448)

[取消回复](https://kexue.fm/archives/5448#respond-post-5448)

你的大名

电子邮箱

个人网站（选填）

1\. 可以使用LaTeX代码，点击“预览效果”可查看效果；2. 可以通过点击评论楼层编号来引用该楼层；3. 网站可能会有点卡，如非确认评论失败，请 **不要重复点击提交**。

### 内容速览

[话在开头](https://kexue.fm/archives/5448#%E8%AF%9D%E5%9C%A8%E5%BC%80%E5%A4%B4)
[无意中的邂逅](https://kexue.fm/archives/5448#%E6%97%A0%E6%84%8F%E4%B8%AD%E7%9A%84%E9%82%82%E9%80%85)
[何为最小熵原理？](https://kexue.fm/archives/5448#%E4%BD%95%E4%B8%BA%E6%9C%80%E5%B0%8F%E7%86%B5%E5%8E%9F%E7%90%86%EF%BC%9F)
[语言的信息](https://kexue.fm/archives/5448#%E8%AF%AD%E8%A8%80%E7%9A%84%E4%BF%A1%E6%81%AF)
[信息熵=学习成本](https://kexue.fm/archives/5448#%E4%BF%A1%E6%81%AF%E7%86%B5=%E5%AD%A6%E4%B9%A0%E6%88%90%E6%9C%AC)
[中英文孰优孰劣？](https://kexue.fm/archives/5448#%E4%B8%AD%E8%8B%B1%E6%96%87%E5%AD%B0%E4%BC%98%E5%AD%B0%E5%8A%A3%EF%BC%9F)
[套路之路](https://kexue.fm/archives/5448#%E5%A5%97%E8%B7%AF%E4%B9%8B%E8%B7%AF)
[念经也有学问](https://kexue.fm/archives/5448#%E5%BF%B5%E7%BB%8F%E4%B9%9F%E6%9C%89%E5%AD%A6%E9%97%AE)
[以不变应万变](https://kexue.fm/archives/5448#%E4%BB%A5%E4%B8%8D%E5%8F%98%E5%BA%94%E4%B8%87%E5%8F%98)

### 智能搜索

支持整句搜索！网站自动使用 [结巴分词](https://github.com/fxsjy/jieba) 进行分词，并结合ngrams排序算法给出合理的搜索结果。

### 热门标签

[生成模型](https://kexue.fm/tag/%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B/) [attention](https://kexue.fm/tag/attention/) [优化](https://kexue.fm/tag/%E4%BC%98%E5%8C%96/) [语言模型](https://kexue.fm/tag/%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/) [模型](https://kexue.fm/tag/%E6%A8%A1%E5%9E%8B/) [网站](https://kexue.fm/tag/%E7%BD%91%E7%AB%99/) [概率](https://kexue.fm/tag/%E6%A6%82%E7%8E%87/) [梯度](https://kexue.fm/tag/%E6%A2%AF%E5%BA%A6/) [转载](https://kexue.fm/tag/%E8%BD%AC%E8%BD%BD/) [矩阵](https://kexue.fm/tag/%E7%9F%A9%E9%98%B5/) [微分方程](https://kexue.fm/tag/%E5%BE%AE%E5%88%86%E6%96%B9%E7%A8%8B/) [天象](https://kexue.fm/tag/%E5%A4%A9%E8%B1%A1/) [分析](https://kexue.fm/tag/%E5%88%86%E6%9E%90/) [深度学习](https://kexue.fm/tag/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/) [积分](https://kexue.fm/tag/%E7%A7%AF%E5%88%86/) [python](https://kexue.fm/tag/python/) [优化器](https://kexue.fm/tag/%E4%BC%98%E5%8C%96%E5%99%A8/) [力学](https://kexue.fm/tag/%E5%8A%9B%E5%AD%A6/) [无监督](https://kexue.fm/tag/%E6%97%A0%E7%9B%91%E7%9D%A3/) [扩散](https://kexue.fm/tag/%E6%89%A9%E6%95%A3/) [几何](https://kexue.fm/tag/%E5%87%A0%E4%BD%95/) [节日](https://kexue.fm/tag/%E8%8A%82%E6%97%A5/) [生活](https://kexue.fm/tag/%E7%94%9F%E6%B4%BB/) [文本生成](https://kexue.fm/tag/%E6%96%87%E6%9C%AC%E7%94%9F%E6%88%90/) [数论](https://kexue.fm/tag/%E6%95%B0%E8%AE%BA/)

### 随机文章

- [2012年快乐！](https://kexue.fm/archives/1523)
- [殊途同归的策略梯度与零阶优化](https://kexue.fm/archives/7737)
- [2012年全年天象大观](https://kexue.fm/archives/1490)
- [倒立单摆之分离频率](https://kexue.fm/archives/2471)
- [奥赛版《春天里》](https://kexue.fm/archives/1418)
- [科学空间：2010年12月重要天象](https://kexue.fm/archives/1085)
- [这样的世界之最你见过没有？](https://kexue.fm/archives/36)
- [用狄拉克函数来构造非光滑函数的光滑近似](https://kexue.fm/archives/8718)
- [又是一年农耕时...](https://kexue.fm/archives/825)
- [开始学习数学软件Scilab](https://kexue.fm/archives/1720)

### 最近评论

- [Kuo](https://kexue.fm/archives/11033/comment-page-1#comment-28053): 在 $PaTH$ 论文章节 \`UT Transform for Products of Hou...
- [Fanhao](https://kexue.fm/archives/10542/comment-page-1#comment-28052): 假定Hessian阵正定，那不是意味着$L(\\theta)$是$\\theta$的凸函数吗？这一...
- [曲笑一](https://kexue.fm/archives/10739/comment-page-2#comment-28051): 对于第一个疑问，我看到分布式的版本已经开源。我在想如果将每个梯度矩阵G拆分为N\*N,再利用mu...
- [曲笑一](https://kexue.fm/archives/10739/comment-page-2#comment-28050): 苏老师您好，阅读了您关于Muon系列的博客，受益匪浅。在此有两个疑问想请教您：第一个问题是，M...
- [tll1945tll1937](https://kexue.fm/archives/10266/comment-page-1#comment-28049): 老师，您好，向您请教一个问题：会不会因为LoRA中用到的梯度的维度仅仅是全参数微调中梯度的维度...
- [香蕉大王](https://kexue.fm/archives/9280/comment-page-2#comment-28048): 还是刚刚flow matching的例子$\\frac{d x\_t}{dt} = v\_\\thet...
- [香蕉大王](https://kexue.fm/archives/9280/comment-page-2#comment-28047): 谢谢老师回复。明白老师您说的了。我再想请问一个小小的问题：既然ODE未必是唯一的，有没有人尝试...
- [NoAmateur](https://kexue.fm/archives/12/comment-page-7#comment-28046): 2025年第一次了解到苏前辈，进入科学空间后怀着好奇翻阅苏前辈的点滴，让我备受震撼，苏前辈对科...
- [liukoulong](https://kexue.fm/archives/11033/comment-page-1#comment-28043): $T^{3}$
- [nihaowhut](https://kexue.fm/archives/10617/comment-page-1#comment-28042): 请问ϵ为什么要限制到2^-7, 2^-6, .. 2^-1, 2^0这8个数，感觉没有必要

### 友情链接

- [Cool Papers](https://papers.cool)
- [数学研发](https://bbs.emath.ac.cn)
- [Seatop](http://www.seatop.com.cn/)
- [Xiaoxia](https://xiaoxia.org/)
- [积分表-网络版](https://kexue.fm/sci/integral/index.html)
- [丝路博傲](http://blog.dvxj.com/)
- [ph4ntasy 饭特稀](http://www.ph4ntasy.com/)
- [数学之家](http://www.2math.cn/)
- [有趣天文奇观](http://interesting-sky.china-vo.org/)
- [TwistedW](http://www.twistedwg.com/)
- [godweiyang](https://godweiyang.com/)
- [AI柠檬](https://blog.ailemon.net/)
- [王登科-DK博客](https://greatdk.com)
- [ESON](https://blog.eson.org/)
- [枫之羽](https://fzhiy.net/)
- [Mathor's blog](https://wmathor.com/)
- [coding-zuo](https://coding-zuo.github.io/)
- [博科园](https://www.bokeyuan.net/)
- [孔皮皮的博客](https://www.kppkkp.top/)
- [运鹏的博客](https://yunpengtai.top/)
- [jiming.site](https://jiming.site/)
- [OmegaXYZ](https://www.omegaxyz.com/)
- [Blog by Eacls](https://www.eacls.top/)
- [EAI猩球](https://www.robotech.ink/)
- [文举的博客](https://liwenju0.com/)
- [用代码打点酱油](https://bruceyuan.com/)
- [申请链接](https://kexue.fm/links.html)

本站采用创作共用版权协议，要求署名、非商业用途和保持一致。转载本站内容必须也遵循“ [署名-非商业用途-保持一致](http://creativecommons.org/licenses/by-nc-nd/2.5/cn/)”的创作共用协议。
© 2009-2025 Scientific Spaces. All rights reserved. Theme by [laogui](http://www.laogui.com). Powered by [Typecho](http://typecho.org). 备案号: [粤ICP备09093259号-1/2](https://beian.miit.gov.cn/)。