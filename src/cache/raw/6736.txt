## SEARCH

## MENU

- [打赏](https://kexue.fm/reward.html)
- [公式](https://kexue.fm/latex.html)
- [天象](https://kexue.fm/ac.html)
- [链接](https://kexue.fm/links.html)
- [时光](https://kexue.fm/me.html)
- [博览](https://kexue.fm/science.html)
- [归档](https://kexue.fm/content.html)

## CATEGORIES

- [千奇百怪](https://kexue.fm/category/Everything)
- [天文探索](https://kexue.fm/category/Astronomy)
- [数学研究](https://kexue.fm/category/Mathematics)
- [物理化学](https://kexue.fm/category/Phy-chem)
- [信息时代](https://kexue.fm/category/Big-Data)
- [生物自然](https://kexue.fm/category/Biology)
- [图片摄影](https://kexue.fm/category/Photograph)
- [问题百科](https://kexue.fm/category/Questions)
- [生活/情感](https://kexue.fm/category/Life-Feeling)
- [资源共享](https://kexue.fm/category/Resources)

## NEWPOSTS

- [通过msign来计算mclip（奇...](https://kexue.fm/archives/11006)
- [msign算子的Newton-Sc...](https://kexue.fm/archives/10996)
- [等值振荡定理：最优多项式逼近的充要条件](https://kexue.fm/archives/10972)
- [生成扩散模型漫谈（三十）：从瞬时速...](https://kexue.fm/archives/10958)
- [MoE环游记：5、均匀分布的反思](https://kexue.fm/archives/10945)
- [msign算子的Newton-Sc...](https://kexue.fm/archives/10922)
- [Transformer升级之路：2...](https://kexue.fm/archives/10907)
- [一道概率不等式：盯着它到显然成立为止！](https://kexue.fm/archives/10902)
- [SVD的导数](https://kexue.fm/archives/10878)
- [智能家居之手搓一套能接入米家的零冷水装置](https://kexue.fm/archives/10869)

## COMMENTS

- [苏剑林: 刚入门那会的文章，不用深究了。](https://kexue.fm/archives/481/comment-page-1#comment-27835)
- [苏剑林: 目前各方面的实测效果看来不会，我觉得本质上就是因为partia...](https://kexue.fm/archives/10122/comment-page-1#comment-27834)
- [苏剑林: 首先，瞬时速度为什么跟$t$无关？其次，现在reflow和me...](https://kexue.fm/archives/10958/comment-page-1#comment-27833)
- [苏剑林: 对于每一步数据都严格对齐来说，0.01的loss差距不小了，因...](https://kexue.fm/archives/10907/comment-page-2#comment-27832)
- [苏剑林: \[comment=27808\]rpsun\[/comment\]\
...](https://kexue.fm/archives/10699/comment-page-1#comment-27831)
- [苏剑林: 自己都没怎么关注天象了，惭愧](https://kexue.fm/archives/1490/comment-page-1#comment-27830)
- [苏剑林: 原来如此。其实只要预测空间是连续空间，并且任务本质是一对多的输...](https://kexue.fm/archives/10958/comment-page-1#comment-27829)
- [苏剑林: 你可以拿一批语料去eval，看各个expert分别激活了多少次呀。](https://kexue.fm/archives/10945/comment-page-1#comment-27828)
- [苏剑林: 首先这个分布肯定是存在的（贝叶斯公式无关分布），然后它的概率密...](https://kexue.fm/archives/9164/comment-page-4#comment-27827)
- [苏剑林: 这两天看了下FoPE，感觉它的分析有点道理，但它实现的代码跟论...](https://kexue.fm/archives/10907/comment-page-2#comment-27826)

## USERLOGIN

- [登录](https://kexue.fm/admin/login.php)

[科学空间\|Scientific Spaces](https://kexue.fm)

- [登录](https://kexue.fm/admin/login.php)
- [打赏](https://kexue.fm/reward.html)
- [公式](https://kexue.fm/latex.html)
- [天象](https://kexue.fm/ac.html)
- [链接](https://kexue.fm/links.html)
- [时光](https://kexue.fm/me.html)
- [博览](https://kexue.fm/science.html)
- [归档](https://kexue.fm/content.html)

渴望成为一个小飞侠

- [欢迎订阅](https://kexue.fm/feed)
- [个性邮箱](https://kexue.fm/archives/119)
- [天象信息](https://kexue.fm/ac.html)
- [观测ISS](https://kexue.fm/archives/41)
- [LaTeX](https://kexue.fm/latex.html)
- [关于博主](https://kexue.fm/me.html)

欢迎访问“科学空间”，这里将与您共同探讨自然科学，回味人生百态；也期待大家的分享～

- [**千奇百怪** Everything](https://kexue.fm/category/Everything)
- [**天文探索** Astronomy](https://kexue.fm/category/Astronomy)
- [**数学研究** Mathematics](https://kexue.fm/category/Mathematics)
- [**物理化学** Phy-chem](https://kexue.fm/category/Phy-chem)
- [**信息时代** Big-Data](https://kexue.fm/category/Big-Data)
- [**生物自然** Biology](https://kexue.fm/category/Biology)
- [**图片摄影** Photograph](https://kexue.fm/category/Photograph)
- [**问题百科** Questions](https://kexue.fm/category/Questions)
- [**生活/情感** Life-Feeling](https://kexue.fm/category/Life-Feeling)
- [**资源共享** Resources](https://kexue.fm/category/Resources)

- [**千奇百怪**](https://kexue.fm/category/Everything)
- [**天文探索**](https://kexue.fm/category/Astronomy)
- [**数学研究**](https://kexue.fm/category/Mathematics)
- [**物理化学**](https://kexue.fm/category/Phy-chem)
- [**信息时代**](https://kexue.fm/category/Big-Data)
- [**生物自然**](https://kexue.fm/category/Biology)
- [**图片摄影**](https://kexue.fm/category/Photograph)
- [**问题百科**](https://kexue.fm/category/Questions)
- [**生活/情感**](https://kexue.fm/category/Life-Feeling)
- [**资源共享**](https://kexue.fm/category/Resources)

[首页](https://kexue.fm) [信息时代](https://kexue.fm/category/Big-Data) 当Bert遇上Keras：这可能是Bert最简单的打开姿势

18Jun

# [当Bert遇上Keras：这可能是Bert最简单的打开姿势](https://kexue.fm/archives/6736)

By 苏剑林 \|
2019-06-18 \|
498053位读者\|

[Bert](https://papers.cool/arxiv/1810.04805) 是什么，估计也不用笔者来诸多介绍了。虽然笔者不是很喜欢Bert，但不得不说，Bert确实在NLP界引起了一阵轩然大波。现在不管是中文还是英文，关于Bert的科普和解读已经满天飞了，隐隐已经超过了当年Word2Vec刚出来的势头了。有意思的是，Bert是Google搞出来的，当年的word2vec也是Google搞出来的，不管你用哪个，都是在跟着Google大佬的屁股跑啊～

Bert刚出来不久，就有读者建议我写个解读，但我终究还是没有写。一来，Bert的解读已经不少了，二来其实Bert也就是基于Attention的搞出来的大规模语料预训练的模型，本身在技术上不算什么创新，而关于 [Google的Attention](https://kexue.fm/archives/4765) 我已经写过解读了，所以就提不起劲来写了。

Bert的预训练和微调（图片来自Bert的原论文）

总的来说，我个人对Bert一直也没啥兴趣，直到上个月末在做信息抽取比赛时，才首次尝试了Bert。因为后来想到，即使不感兴趣，终究也是得学会它，毕竟用不用是一回事，会不会又是另一回事。再加上在Keras中使用（fine tune）Bert，似乎还没有什么文章介绍，所以就分享一下自己的使用经验。

## 当Bert遇上Keras [\#](https://kexue.fm/archives/6736\#%E5%BD%93Bert%E9%81%87%E4%B8%8AKeras)

很幸运的是，已经有大佬封装好了Keras版的Bert，可以直接调用官方发布的预训练权重，对于已经有一定Keras基础的读者来说，这可能是最简单的调用Bert的方式了。所谓“站在巨人的肩膀上”，就是形容我们这些Keras爱好者此刻的心情了。

### keras-bert [\#](https://kexue.fm/archives/6736\#keras-bert)

个人认为，目前在Keras下对Bert最好的封装是：

> **keras-bert：** [https://github.com/CyberZHG/keras-bert](https://github.com/CyberZHG/keras-bert)

本文也是以此为基础的。

顺便一提的是，除了keras-bert之外，CyberZHG大佬还封装了很多有价值的keras模块，比如keras-gpt-2（你可以用像用bert一样用gpt2模型了）、keras-lr-multiplier（分层设置学习率）、keras-ordered-neurons（就是前不久介绍的 [ON-LSTM](https://kexue.fm/archives/6621)）等等，汇总可以看 [这里](https://github.com/CyberZHG/summary)。看来也是一位Keras铁杆粉丝啊～致敬大佬。

事实上，有了keras-bert之后，再加上一点点keras基础知识，而且keras-bert所给的demo已经足够完善，调用、微调Bert都已经变成了意见没有什么技术含量的事情了。所以后面笔者只是给出几个中文的例子，来让读者上手keras-bert的基本用法。

### Tokenizer [\#](https://kexue.fm/archives/6736\#Tokenizer)

正式讲例子之前，还有必要先讲一下Tokenizer相关内容。我们导入Bert的Tokenizer并重构一下它：

```
from keras_bert import load_trained_model_from_checkpoint, Tokenizer
import codecs

config_path = '../bert/chinese_L-12_H-768_A-12/bert_config.json'
checkpoint_path = '../bert/chinese_L-12_H-768_A-12/bert_model.ckpt'
dict_path = '../bert/chinese_L-12_H-768_A-12/vocab.txt'

token_dict = {}
with codecs.open(dict_path, 'r', 'utf8') as reader:
 for line in reader:
 token = line.strip()
 token_dict[token] = len(token_dict)

class OurTokenizer(Tokenizer):
 def _tokenize(self, text):
 R = []
 for c in text:
 if c in self._token_dict:
 R.append(c)
 elif self._is_space(c):
 R.append('[unused1]') # space类用未经训练的[unused1]表示
 else:
 R.append('[UNK]') # 剩余的字符是[UNK]
 return R

tokenizer = OurTokenizer(token_dict)
tokenizer.tokenize(u'今天天气不错')
# 输出是 ['[CLS]', u'今', u'天', u'天', u'气', u'不', u'错', '[SEP]']

```

这里简单解释一下 `Tokenizer` 的输出结果。首先，默认情况下，分词后句子首位会分别加上\[CLS\]和\[SEP\]标记，其中\[CLS\]位置对应的输出向量是能代表整句的句向量（反正Bert是这样设计的），而\[SEP\]则是句间的分隔符，其余部分则是单字输出（对于中文来说）。

本来 `Tokenizer` 有自己的 `_tokenize` 方法，我这里重写了这个方法，是要保证tokenize之后的结果，跟原来的字符串长度等长（如果算上两个标记，那么就是等长再加2）。 `Tokenizer` 自带的 `_tokenize` 会自动去掉空格，然后有些字符会粘在一块输出，导致tokenize之后的列表不等于原来字符串的长度了，这样如果做序列标注的任务会很麻烦。而为了避免这种麻烦，还是自己重写一遍好了～主要就是用\[unused1\]来表示空格类字符，而其余的不在列表的字符用\[UNK\]表示，其中\[unused\*\]这些标记是未经训练的（随即初始化），是Bert预留出来用来增量添加词汇的标记，所以我们可以用它们来指代任何新字符。

## 三个例子 [\#](https://kexue.fm/archives/6736\#%E4%B8%89%E4%B8%AA%E4%BE%8B%E5%AD%90)

这里包含keras-bert的三个例子，分别是 **文本分类**、 **关系抽取** 和 **主体抽取**，都是在官方发布的预训练权重基础上进行微调来做的。

> **Bert官方Github：** [https://github.com/google-research/bert](https://github.com/google-research/bert)
>
> **官方的中文预训练权重：** [chinese\_L-12\_H-768\_A-12.zip](https://storage.googleapis.com/bert_models/2018_11_03/chinese_L-12_H-768_A-12.zip)
>
> **例子所在Github：** [https://github.com/bojone/bert\_in\_keras/](https://github.com/bojone/bert_in_keras/)

根据官方介绍，这份权重是用中文维基百科为语料进行训练的。

（2019年6月20日更新：哈工大讯飞联合实验室发布了一版新权重，也可以用keras\_bert加载，详情请看 [这里](https://github.com/ymcui/Chinese-BERT-wwm)。）

### 文本分类 [\#](https://kexue.fm/archives/6736\#%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB)

作为第一个例子，我们做一个最基本的文本分类任务，熟悉做这个基本任务之后，剩下的各种任务都会变得相当简单了。这次我们以之前已经讨论过多次的 [文本感情分类任务](https://kexue.fm/archives/3863) 为例，所用的 [标注数据](https://kexue.fm/archives/3414) 也是以前所整理的。

让我们来看看模型部分全貌（完整代码见 [这里](https://github.com/bojone/bert_in_keras/blob/master/sentiment.py)）：

```
# 注意，尽管可以设置seq_len=None，但是仍要保证序列长度不超过512
bert_model = load_trained_model_from_checkpoint(config_path, checkpoint_path, seq_len=None)

for l in bert_model.layers:
 l.trainable = True

x1_in = Input(shape=(None,))
x2_in = Input(shape=(None,))

x = bert_model([x1_in, x2_in])
x = Lambda(lambda x: x[:, 0])(x) # 取出[CLS]对应的向量用来做分类
p = Dense(1, activation='sigmoid')(x)

model = Model([x1_in, x2_in], p)
model.compile(
 loss='binary_crossentropy',
 optimizer=Adam(1e-5), # 用足够小的学习率
 metrics=['accuracy']
)
model.summary()
```

在Keras中调用Bert来做情感分类任务就这样写完了～写完了～～

是不是感觉还没有尽兴，模型代码就结束了？Keras调用Bert就这么简短。事实上，真正调用Bert的也就只有 `load_trained_model_from_checkpoint` 那一行代码，剩下的只是普通的Keras操作（再次感谢CyberZHG大佬）。所以，如果你已经入门了Keras，那么调用Bert是无往不利啊。

如此简单的调用，能达到什么精度？经过5个epoch的fine tune后，验证集的最好准确率是95.5%+！之前我们在 [《文本情感分类（三）：分词 OR 不分词》](https://kexue.fm/archives/3863) 中死调烂调，也就只有90%上下的准确率；而用了Bert之后，寥寥几行，就提升了5个百分点多的准确率！也难怪Bert能在NLP界掀起一阵热潮...

> 在这里，用笔者的个人经历先回答读者可能关心的两个问题。
>
> 第一个问题应该是大家都很关心的，那就是“要多少显存才够？”。事实上，这没有一个标准答案，显存的使用取决于三个因素：句子长度、batch size、模型复杂度。像上面的情感分析例子，在笔者的GTX1060 6G显存上也能跑起来，只需要将batch size调到24即可。所以，如果你的显存不够大，将句子的maxlen和batch size都调小一点试试。当然，如果你的任务太复杂，再小的maxlen和batch size也可能OOM，那就只有升级显卡了。
>
> 第二个问题是“有什么原则来指导Bert后面应该要接哪些层？”。答案是：用尽可能少的层来完成你的任务。比如上述情感分析只是一个二分类任务，你就取出第一个向量然后加个Dense(1)就好了，不要想着多加几层Dense，更加不要想着接个LSTM再接Dense；如果你要做序列标注（比如NER），那你就接个Dense+CRF就好，也不要多加其他东西。总之，额外加的东西尽可能少。一是因为Bert本身就足够复杂，它有足够能力应对你要做的很多任务；二来你自己加的层都是随机初始化的，加太多会对Bert的预训练权重造成剧烈扰动，容易降低效果甚至造成模型不收敛～

### 关系抽取 [\#](https://kexue.fm/archives/6736\#%E5%85%B3%E7%B3%BB%E6%8A%BD%E5%8F%96)

假如读者已经有了一定的Keras基础，那么经过第一个例子的学习，其实我们应该已经完全掌握了Bert的fine tune了，因为实在是简单到没有什么好讲了。所以，后面两个例子主要是提供一些参考模式，让读者能体会到如何“用尽可能少的层来完成你的任务”。

在第二个例子中，我们介绍基于Bert实现的一个极简的关系抽取模型，其标注原理跟 [《基于DGCNN和概率图的轻量级信息抽取模型》](https://kexue.fm/archives/6671) 介绍的一样，但是得益于Bert强大的编码能力，我们所写的部分可以大大简化。在笔者所给出的一种参考实现中，模型部分如下（完整模型见 [这里](https://github.com/bojone/bert_in_keras/blob/master/relation_extract.py)）：

```
t = bert_model([t1, t2])
ps1 = Dense(1, activation='sigmoid')(t)
ps2 = Dense(1, activation='sigmoid')(t)

subject_model = Model([t1_in, t2_in], [ps1, ps2]) # 预测subject的模型

k1v = Lambda(seq_gather)([t, k1])
k2v = Lambda(seq_gather)([t, k2])
kv = Average()([k1v, k2v])
t = Add()([t, kv])
po1 = Dense(num_classes, activation='sigmoid')(t)
po2 = Dense(num_classes, activation='sigmoid')(t)

object_model = Model([t1_in, t2_in, k1_in, k2_in], [po1, po2]) # 输入text和subject，预测object及其关系

train_model = Model([t1_in, t2_in, s1_in, s2_in, k1_in, k2_in, o1_in, o2_in],
 [ps1, ps2, po1, po2])
```

如果读者已经读过 [《基于DGCNN和概率图的轻量级信息抽取模型》](https://kexue.fm/archives/6671) 一文，了解到不用Bert时的模型架构，那么就会理解到上述实现是多么的简介明了。

可以看到，我们引入了Bert作为编码器，然后得到了编码序列$t$，然后直接接两个Dense(1)，这就完成了subject的标注模型；接着，我们把传入的s的首尾对应的编码向量拿出来，直接加到编码向量序列$t$中去，然后再接两个Dense(num\_classes)，就完成object的标注模型（同时标注出了关系）。

这样简单的设计，最终F1能到多少？答案是：线下dev能接近82%，线上我提交过一次，结果是85%+（都是单模型）！相比之下， [《基于DGCNN和概率图的轻量级信息抽取模型》](https://kexue.fm/archives/6671) 中的模型，需要接CNN，需要搞全局特征，需要将s传入到LSTM进行编码，还需要相对位置向量，各种拍脑袋的模块融合在一起，单模型也只比它好一点点（大约82.5%）。要知道，这个基于Bert的简单模型我只写了一个小时就写出来了，而各种技巧和模型融合在一起的DGCNN模型，我前前后后调试了差不多两个月！Bert的强悍之处可见一斑。

（注：这个模型的fine tune最好有8G以上的显存。另外，因为我在比赛即将结束的前几天才接触的Bert，才把这个基于Bert的模型写出来，没有花心思好好调试，所以最终的提交结果并没有包含Bert。）

> 用Bert做关系抽取的这个例子，跟前面情感分析的简单例子，有一个明显的差别是学习率的变化。
>
> 情感分析的例子中，只是用了恒定的学习率（$10^{-5}$）训练了几个epoch，效果就还不错了。在关系抽取这个例子中，第一个epoch的学习率慢慢从$0$增加到$5\\times 10^{-5}$（这样称为warmup），第二个epoch再从$5\\times 10^{-5}$降到$10^{-5}$，总的来说就是先增后减，Bert本身也是用类似的学习率曲线来训练的，这样的训练方式比较稳定，不容易崩溃，而且效果也比较好。

### 事件主体抽取 [\#](https://kexue.fm/archives/6736\#%E4%BA%8B%E4%BB%B6%E4%B8%BB%E4%BD%93%E6%8A%BD%E5%8F%96)

最后一个例子来自 [CCKS 2019 面向金融领域的事件主体抽取](https://biendata.com/competition/ccks_2019_4/)，这个比赛目前还在进行，不过我也已经没有什么动力和兴趣做下去了，所以放出我现在的模型（准确率为89%+）供大家参考，祝继续参赛的选手取得更好的成绩。

简单介绍一下这个比赛的数据，大概是这样的

> 输入：“公司A产品出现添加剂，其下属子公司B和公司C遭到了调查”， “产品出现问题”
>
> 输出： “公司A”

也就是说，这是个双输入、单输出的模型，输入是一个query和一个事件类型，输出一个实体（有且只有一个，并且是query的一个片段）。其实这个任务可以看成是 [SQUAD 1.0](https://rajpurkar.github.io/SQuAD-explorer/explore/1.1/dev/) 的简化版，根据这个输出特性，输出应该用指针结构比较好（两个softmax分别预测首尾）。剩下的问题是：双输入怎么搞？

前面两个例子虽然复杂度不同，但它们都是单一输入的，双输入怎么办呢？当然，这里的实体类型只有有限个，直接Embedding也行，只不过我使用一种更能体现Bert的简单粗暴和强悍的方案：直接用连接符将两个输入连接成一个句子，然后就变成单输入了！比如上述示例样本处理成：

> 输入：“\_\_\_产品出现问题\_\_\_公司A产品出现添加剂，其下属子公司B和公司C遭到了调查”
>
> 输出： “公司A”

然后就变成了普通的单输入抽取问题了。说到这个，这个模型的代码也就没有什么好说的了，就简单几行（完整代码请看 [这里](https://github.com/bojone/bert_in_keras/blob/master/subject_extract.py)）：

```
x = bert_model([x1, x2])
ps1 = Dense(1, use_bias=False)(x)
ps1 = Lambda(lambda x: x[0][..., 0] - (1 - x[1][..., 0]) * 1e10)([ps1, x_mask])
ps2 = Dense(1, use_bias=False)(x)
ps2 = Lambda(lambda x: x[0][..., 0] - (1 - x[1][..., 0]) * 1e10)([ps2, x_mask])

model = Model([x1_in, x2_in], [ps1, ps2])
```

另外加上一些解码的trick，还有模型融合，提交上去，就可以做到89%+了。在看看目前排行榜，发现最好的结果也就是90%多一点点，所以估计大家都差不多是这样做的了...（这个代码重复实验时波动比较大，大家可以多跑几次，取最优结果。）

> 这个例子主要告诉我们，用Bert实现自己的任务时，最好能整理成单输入的模式，这样一来比较简单，二来也更加高效。
>
> 比如做句子相似度模型，输入两个句子，输出一个相似度，有两个可以想到的做法，第一种是两个句子分别过同一个Bert，然后取出各自的\[CLS\]特征来做分类；第二种就是像上面一样，用个记号把两个句子连接在一起，变成一个句子，然后过一个Bert，然后将输出特征做分类，后者显然会更快一些，而且能够做到特征之间更全面的交互。

## 文章小结 [\#](https://kexue.fm/archives/6736\#%E6%96%87%E7%AB%A0%E5%B0%8F%E7%BB%93)

本文介绍了Keras下Bert的基本调用方法，其中主要是提供三个参考例子，供大家逐步熟悉Bert的fine tune步骤和原理。其中有不少是笔者自己闭门造车的经验之谈，如果有所偏颇，还望读者指正。

事实上有了CyberZHG大佬实现的keras-bert，在Keras下使用Bert也就是小菜一碟，大家折腾个半天，也就上手了。最后祝大家用得痛快～

_**转载到请包括本文地址：** [https://kexue.fm/archives/6736](https://kexue.fm/archives/6736)_

_**更详细的转载事宜请参考：**_ [《科学空间FAQ》](https://kexue.fm/archives/6508#%E6%96%87%E7%AB%A0%E5%A6%82%E4%BD%95%E8%BD%AC%E8%BD%BD/%E5%BC%95%E7%94%A8)

**如果您还有什么疑惑或建议，欢迎在下方评论区继续讨论。**

**如果您觉得本文还不错，欢迎 [分享](https://kexue.fm/archives/6736#share)/ [打赏](https://kexue.fm/archives/6736#pay) 本文。打赏并非要从中获得收益，而是希望知道科学空间获得了多少读者的真心关注。当然，如果你无视它，也不会影响你的阅读。再次表示欢迎和感谢！**

打赏

微信打赏

支付宝打赏

因为网站后台对打赏并无记录，因此欢迎在打赏时候备注留言。你还可以 [**点击这里**](http://mail.qq.com/cgi-bin/qm_share?t=qm_mailme&email=tN7d1drY3drrx8H0xcWa19vZ) 或在下方评论区留言来告知你的建议或需求。

**如果您需要引用本文，请参考：**

苏剑林. (Jun. 18, 2019). 《当Bert遇上Keras：这可能是Bert最简单的打开姿势 》\[Blog post\]. Retrieved from [https://kexue.fm/archives/6736](https://kexue.fm/archives/6736)

@online{kexuefm-6736,
        title={当Bert遇上Keras：这可能是Bert最简单的打开姿势},
        author={苏剑林},
        year={2019},
        month={Jun},
        url={\\url{https://kexue.fm/archives/6736}},
}

分类： [信息时代](https://kexue.fm/category/Big-Data)    标签： [语言模型](https://kexue.fm/tag/%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/), [比赛](https://kexue.fm/tag/%E6%AF%94%E8%B5%9B/), [信息抽取](https://kexue.fm/tag/%E4%BF%A1%E6%81%AF%E6%8A%BD%E5%8F%96/), [attention](https://kexue.fm/tag/attention/)[168 评论](https://kexue.fm/archives/6736#comments)

< [漫谈重参数：从正态分布到Gumbel Softmax](https://kexue.fm/archives/6705) \| [简述无偏估计和有偏估计](https://kexue.fm/archives/6747) >

### 你也许还对下面的内容感兴趣

- [Transformer升级之路：20、MLA究竟好在哪里？](https://kexue.fm/archives/10907)
- [Transformer升级之路：19、第二类旋转位置编码](https://kexue.fm/archives/10862)
- [细水长flow之TARFLOW：流模型满血归来？](https://kexue.fm/archives/10667)
- [“闭门造车”之多模态思路浅谈（三）：位置编码](https://kexue.fm/archives/10352)
- [Decoder-only的LLM为什么需要位置编码？](https://kexue.fm/archives/10347)
- [Monarch矩阵：计算高效的稀疏型矩阵分解](https://kexue.fm/archives/10249)
- [Transformer升级之路：18、RoPE的底数选择原则](https://kexue.fm/archives/10122)
- [缓存与效果的极限拉扯：从MHA、MQA、GQA到MLA](https://kexue.fm/archives/10091)
- [Transformer升级之路：17、多模态位置编码的简单思考](https://kexue.fm/archives/10040)
- [时空之章：将Attention视为平方复杂度的RNN](https://kexue.fm/archives/10017)

[发表你的看法](https://kexue.fm/archives/6736#comment_form)

1. [«](https://kexue.fm/archives/6736/comment-page-5#comments)
2. [1](https://kexue.fm/archives/6736/comment-page-1#comments)
3. ...
4. [3](https://kexue.fm/archives/6736/comment-page-3#comments)
5. [4](https://kexue.fm/archives/6736/comment-page-4#comments)
6. [5](https://kexue.fm/archives/6736/comment-page-5#comments)
7. [6](https://kexue.fm/archives/6736/comment-page-6#comments)

yiyi

March 10th, 2021

请教苏神，事件主体抽取中，到这里model = Model(\[x1\_in, x2\_in\], \[ps1, ps2\])是输入文本的token\_idx,segment\_ids预测主体的起始位置，后面train\_model = Model(\[x1\_in, x2\_in, s1\_in, s2\_in\], \[ps1, ps2\])作用是什么呢？

[回复评论](https://kexue.fm/archives/6736/comment-page-6?replyTo=15731#respond-post-6736)

[苏剑林](https://kexue.fm) 发表于
March 10th, 2021

作用是建立train\_model

[回复评论](https://kexue.fm/archives/6736/comment-page-6?replyTo=15733#respond-post-6736)

yiyi 发表于
March 10th, 2021

专门为了训练模型的？请问博主这种方式是属于keras哪种特性？我好学习一下

[回复评论](https://kexue.fm/archives/6736/comment-page-6?replyTo=15736#respond-post-6736)

[苏剑林](https://kexue.fm) 发表于
March 10th, 2021

是的。模型重用，权重共享。

[回复评论](https://kexue.fm/archives/6736/comment-page-6?replyTo=15737#respond-post-6736)

lvyang

April 9th, 2021

苏老师我有个问题想请教一下 Bert的Embedding layer参与训练吗？看代码我觉得是参与训练的。但是如果Embedding层的参数一直在更新的话，也就意味着词对应的词向量也一直在变。那对于模型来说是不是很难去捕捉这些词的语义。期待您的回答谢谢！

[回复评论](https://kexue.fm/archives/6736/comment-page-6?replyTo=16049#respond-post-6736)

[苏剑林](https://kexue.fm) 发表于
April 9th, 2021

更新。不要想那么多，模型的输入是one hot，剩下的都是梯度下降参数罢了，该怎么优化，模型自己决定，不用我们着急。

[回复评论](https://kexue.fm/archives/6736/comment-page-6?replyTo=16053#respond-post-6736)

lvyang 发表于
April 9th, 2021

好的谢谢老师

[回复评论](https://kexue.fm/archives/6736/comment-page-6?replyTo=16057#respond-post-6736)

H.W

April 22nd, 2021

[https://spaces.ac.cn/archives/6736/comment-page-6#%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB](https://spaces.ac.cn/archives/6736/comment-page-6#%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB)
这个代码例子中，x = bert\_model(\[x1\_in, x2\_in\]) 这一行， x1\_in 表示输入序列，
x2\_in 表示seg\_type\_id是吗？（0，0，0，....0单句;\[0,0,0,1,1,1,...1\]双句) 但是这样naming 变量是不是容易歧义？如果默认是（ seg\_type=\[1,1,1,...1,\]）模型和输入\[0,0,0...\]表现会有差别吗？
另外一个问题是transformer类的tokenizer工具在encode输入序列后输出的dict有三个{\[input\_ids\],\[token\_type\_ids\],\[att\_mask\]}, 但是bert4keras 和 keras\_bert都没有\[mask\]这个，请问是目前还不支持使用mask吗？

[回复评论](https://kexue.fm/archives/6736/comment-page-6?replyTo=16176#respond-post-6736)

[苏剑林](https://kexue.fm) 发表于
April 22nd, 2021

1、这是我最早使用bert的代码，有不规范之处很正常；

2、bert4keras自动mask掉padding部分，自动mask才是正常的、智能的，如果没有什么特别需求，还需要人工传入mask矩阵，那才是不科学的。

[回复评论](https://kexue.fm/archives/6736/comment-page-6?replyTo=16183#respond-post-6736)

Virgil

August 4th, 2021

苏神，你好！请教一下Bert4keras中如何取出隐藏层全部12层或者部分隐藏层的输出结果？

[回复评论](https://kexue.fm/archives/6736/comment-page-6?replyTo=17054#respond-post-6736)

[苏剑林](https://kexue.fm) 发表于
August 5th, 2021

这是个keras问题，跟bert4keras没关系，所以你需要的是学好keras，bert4keras原则上不提供keras教学。

==================

大概的思路如下：

outputs = \[\]
for layer in model.layers:
if xxxx:
outputs.append(layer.output)

new\_model = Model(model.inputs, outputs)

[回复评论](https://kexue.fm/archives/6736/comment-page-6?replyTo=17059#respond-post-6736)

Virgil 发表于
August 5th, 2021

拜教了 感谢感谢！

[回复评论](https://kexue.fm/archives/6736/comment-page-6?replyTo=17061#respond-post-6736)

z飞龙在天

September 16th, 2021

苏神你好，我是看了您的才玩起了bert，当前按照您的思路，我使用bert对词进行编码，然后接了一个双向（16序列）GRU单元实现对文本的编码，然后做分类，但是我看您在做情感分析的时候6g的显存都可以跑起来，
我这边现在的情况是，真正跑起来纯使用CPU会占用112g左右的内存，纯使用GPU的时候，我32g的GPU根本不够用，运行起来就OOM killed，您知道这是什么情况吗？期待您的回复，谢谢

[回复评论](https://kexue.fm/archives/6736/comment-page-6?replyTo=17364#respond-post-6736)

[苏剑林](https://kexue.fm) 发表于
September 17th, 2021

GRU我没接过，理论上接了也不会有太大变化。你这问题我猜不到什么原因，理论上BERT base的简单finetune，6g显存是足够的。

[回复评论](https://kexue.fm/archives/6736/comment-page-6?replyTo=17374#respond-post-6736)

z飞龙在天 发表于
September 18th, 2021

我对比了下，主要是这些矩阵转换耗费了大量的内存，我也非常疑惑，因为我的输入是包含文本句子级维度和句子中单词级维度，然后bert的输入就必须要求我把句子维度先消解，然后编码之后，又必须再把句子维度恢复，这样就多出了两个相反的矩阵操作，通过实验发现确实是这两个矩阵操作耗费大量内存，请教下苏神我该怎么处理呢，因为想了很多办法，这个转换又是必须的
\# 消解句子维度，转换成bert需要的输入形式
input\_1 = tf.reshape(inputs\[0\], (-1, maxlen\_word))
input\_2 = tf.reshape(inputs\[1\], (-1, maxlen\_word))
x = bert\_model(\[input\_1, input\_2\])
x = Lambda(lambda x: x\[:, 0\])(x)
\# 恢复句子级维度，以便后续使用GRU单元编码
x = tf.reshape(x, shape=(-1, maxlen\_sentence, 768))

[回复评论](https://kexue.fm/archives/6736/comment-page-6?replyTo=17382#respond-post-6736)

z飞龙在天 发表于
September 18th, 2021

配置是：maxlen\_word——256，maxlen\_sentence——8，训练一个batch\_size才取值16，内存基本就要占用90g左右了，这里就很困惑，我一个batch\_size才取16，理论上即使矩阵变换也不会占用很大内存的吧，然后我降低batch\_size或者降低maxlen\_sentence取值，内存消耗确实会大幅减少，实在是不太懂了苏神

[回复评论](https://kexue.fm/archives/6736/comment-page-6?replyTo=17383#respond-post-6736)

[苏剑林](https://kexue.fm) 发表于
September 18th, 2021

又word又sentence的，你这等价于总的maxlen是256\*8=2048了。你倒是看看我的参考脚本用的maxlen是多少...你这maxlen相当于我在1060上用的20倍了，用你90G显存算少了。

[回复评论](https://kexue.fm/archives/6736/comment-page-6?replyTo=17392#respond-post-6736)

z飞龙在天 发表于
September 20th, 2021

哦，我好像明白了，我虽然是一次16个样本，但是消解完句子之后变成了128个句子，这样等于批次变成128 \* 256，确实庞大了，感谢苏神，想的有点简单了。
还能再请教一个问题吗，就是如果这样的设计，那我如果使用keras的multi\_gpu\_model这个功能，加入我有四块GPU，是不是一个batch\_size就会平均分配给四个GPU，这样每一块每次就变成4个样本，4 \* 8 = 32个句子，这样应该是不是就能玩起来了

[回复评论](https://kexue.fm/archives/6736/comment-page-6?replyTo=17401#respond-post-6736)

[苏剑林](https://kexue.fm) 发表于
September 22nd, 2021

大概是这样算吧。

z飞龙在天 发表于
September 27th, 2021

苏神，我又碰到一个问题，我做一个分类，将下面的层设置为True和False效果差距较大，设置为False一两个epoch就能达到一个还可以的效果，但是设置为True的时候，100个epoch都不收敛，验证集和训练集指标一直很低，每一个eopch没有什么明显的变化，这个合理正常吗？
for l in bert\_model.layers:
l.trainable = True

[回复评论](https://kexue.fm/archives/6736/comment-page-6?replyTo=17445#respond-post-6736)

[苏剑林](https://kexue.fm) 发表于
September 28th, 2021

不正常，但我也看不懂什么原因。

z飞龙在天

September 29th, 2021

[@苏剑林\|comment-17454](https://kexue.fm/archives/6736/comment-page-6#comment-17454)

这几天试了下，感觉是bert参数全部放开，学习率和我后面的网络相互制衡的原因，后面网络复杂了，学习率低一开始就比较低，收敛也慢，现在在尝试仅仅开放bert高层的参数训练，就可以玩起来了

[回复评论](https://kexue.fm/archives/6736/comment-page-6?replyTo=17463#respond-post-6736)

[苏剑林](https://kexue.fm) 发表于
October 4th, 2021

这个应该是了，BERT后面不宜接太多新的层，不然要专门分层设置学习率才好

[回复评论](https://kexue.fm/archives/6736/comment-page-6?replyTo=17479#respond-post-6736)

ironsun

December 16th, 2021

您好大佬，有个疑惑，请问只用一个模型能同时做文本分类和NER吗，比如输入的是\[cls\]+原文，然后用这同一个输入，cls的encoding用来接个分类层，而\[cls\]+原文这部分的每个字符的encoding接用于NER的网络，两个任务的loss合成一个loss一起训练，不知道可不可以

[回复评论](https://kexue.fm/archives/6736/comment-page-6?replyTo=18031#respond-post-6736)

[苏剑林](https://kexue.fm) 发表于
December 17th, 2021

理论上肯定可以～实际效果实验了才知道。

[回复评论](https://kexue.fm/archives/6736/comment-page-6?replyTo=18039#respond-post-6736)

kern

April 1st, 2022

model = Model(\[x1\_in, x2\_in\], \[ps1, ps2\])
train\_model = Model(\[x1\_in, x2\_in, s1\_in, s2\_in\], \[ps1, ps2\])
您好，想请教一下，这里如不用参数共享，直接用model = Model(\[x1\_in, x2\_in\], \[ps1, ps2\])，利用这个模型进行训练，即双输入双输出，预测实体边界，这样是否合适呢，请您指教

[回复评论](https://kexue.fm/archives/6736/comment-page-6?replyTo=18843#respond-post-6736)

[苏剑林](https://kexue.fm) 发表于
April 1st, 2022

在我的设计中，loss要用到s1\_in, s2\_in，所以要新建一个训练专用的模型。如果你的设计没用到s1\_in, s2\_in，那就可以像你说的那样做。

[回复评论](https://kexue.fm/archives/6736/comment-page-6?replyTo=18847#respond-post-6736)

1. [«](https://kexue.fm/archives/6736/comment-page-5#comments)
2. [1](https://kexue.fm/archives/6736/comment-page-1#comments)
3. ...
4. [3](https://kexue.fm/archives/6736/comment-page-3#comments)
5. [4](https://kexue.fm/archives/6736/comment-page-4#comments)
6. [5](https://kexue.fm/archives/6736/comment-page-5#comments)
7. [6](https://kexue.fm/archives/6736/comment-page-6#comments)

[取消回复](https://kexue.fm/archives/6736#respond-post-6736)

你的大名

电子邮箱

个人网站（选填）

1\. 可以使用LaTeX代码，点击“预览效果”可查看效果；2. 可以通过点击评论楼层编号来引用该楼层；3. 网站可能会有点卡，如非确认评论失败，请 **不要重复点击提交**。

### 内容速览

[当Bert遇上Keras](https://kexue.fm/archives/6736#%E5%BD%93Bert%E9%81%87%E4%B8%8AKeras)
[keras-bert](https://kexue.fm/archives/6736#keras-bert)
[Tokenizer](https://kexue.fm/archives/6736#Tokenizer)
[三个例子](https://kexue.fm/archives/6736#%E4%B8%89%E4%B8%AA%E4%BE%8B%E5%AD%90)
[文本分类](https://kexue.fm/archives/6736#%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB)
[关系抽取](https://kexue.fm/archives/6736#%E5%85%B3%E7%B3%BB%E6%8A%BD%E5%8F%96)
[事件主体抽取](https://kexue.fm/archives/6736#%E4%BA%8B%E4%BB%B6%E4%B8%BB%E4%BD%93%E6%8A%BD%E5%8F%96)
[文章小结](https://kexue.fm/archives/6736#%E6%96%87%E7%AB%A0%E5%B0%8F%E7%BB%93)

### 智能搜索

支持整句搜索！网站自动使用 [结巴分词](https://github.com/fxsjy/jieba) 进行分词，并结合ngrams排序算法给出合理的搜索结果。

### 热门标签

[生成模型](https://kexue.fm/tag/%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B/) [attention](https://kexue.fm/tag/attention/) [优化](https://kexue.fm/tag/%E4%BC%98%E5%8C%96/) [语言模型](https://kexue.fm/tag/%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/) [模型](https://kexue.fm/tag/%E6%A8%A1%E5%9E%8B/) [网站](https://kexue.fm/tag/%E7%BD%91%E7%AB%99/) [概率](https://kexue.fm/tag/%E6%A6%82%E7%8E%87/) [梯度](https://kexue.fm/tag/%E6%A2%AF%E5%BA%A6/) [转载](https://kexue.fm/tag/%E8%BD%AC%E8%BD%BD/) [微分方程](https://kexue.fm/tag/%E5%BE%AE%E5%88%86%E6%96%B9%E7%A8%8B/) [矩阵](https://kexue.fm/tag/%E7%9F%A9%E9%98%B5/) [天象](https://kexue.fm/tag/%E5%A4%A9%E8%B1%A1/) [分析](https://kexue.fm/tag/%E5%88%86%E6%9E%90/) [深度学习](https://kexue.fm/tag/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/) [积分](https://kexue.fm/tag/%E7%A7%AF%E5%88%86/) [python](https://kexue.fm/tag/python/) [优化器](https://kexue.fm/tag/%E4%BC%98%E5%8C%96%E5%99%A8/) [力学](https://kexue.fm/tag/%E5%8A%9B%E5%AD%A6/) [无监督](https://kexue.fm/tag/%E6%97%A0%E7%9B%91%E7%9D%A3/) [扩散](https://kexue.fm/tag/%E6%89%A9%E6%95%A3/) [几何](https://kexue.fm/tag/%E5%87%A0%E4%BD%95/) [节日](https://kexue.fm/tag/%E8%8A%82%E6%97%A5/) [生活](https://kexue.fm/tag/%E7%94%9F%E6%B4%BB/) [文本生成](https://kexue.fm/tag/%E6%96%87%E6%9C%AC%E7%94%9F%E6%88%90/) [数论](https://kexue.fm/tag/%E6%95%B0%E8%AE%BA/)

### 随机文章

- [三月二十七日](https://kexue.fm/archives/2516)
- [Cantor-Bernstein 定理（给出双射！）](https://kexue.fm/archives/2951)
- [一个神秘而糟糕的图形](https://kexue.fm/archives/853)
- [高阶muP：更简明但更高明的谱条件缩放](https://kexue.fm/archives/10795)
- [美华裔教授破百年物理定律 获国际同行喝彩(图)](https://kexue.fm/archives/52)
- [【备忘】维基百科与DNSCrypt](https://kexue.fm/archives/3327)
- [《教材如何写》:BoJone的粗浅看法](https://kexue.fm/archives/1329)
- [2019年全年天象](https://kexue.fm/archives/6257)
- [《积分公式大全》电子书](https://kexue.fm/archives/354)
- [重新拥抱国家天文台！](https://kexue.fm/archives/146)

### 最近评论

- [苏剑林](https://kexue.fm/archives/481/comment-page-1#comment-27835): 刚入门那会的文章，不用深究了。
- [苏剑林](https://kexue.fm/archives/10122/comment-page-1#comment-27834): 目前各方面的实测效果看来不会，我觉得本质上就是因为partial rope的实测效果优于rop...
- [苏剑林](https://kexue.fm/archives/10958/comment-page-1#comment-27833): 首先，瞬时速度为什么跟$t$无关？其次，现在reflow和meanflow的第一、第二目标，不...
- [苏剑林](https://kexue.fm/archives/10907/comment-page-2#comment-27832): 对于每一步数据都严格对齐来说，0.01的loss差距不小了，因为它代表了每一个step的los...
- [苏剑林](https://kexue.fm/archives/10699/comment-page-1#comment-27831): \[comment=27808\]rpsun\[/comment\]
有人这样做了：https://a...
- [苏剑林](https://kexue.fm/archives/1490/comment-page-1#comment-27830): 自己都没怎么关注天象了，惭愧
- [苏剑林](https://kexue.fm/archives/10958/comment-page-1#comment-27829): 原来如此。其实只要预测空间是连续空间，并且任务本质是一对多的输出，那么都有可能关联到Diffu...
- [苏剑林](https://kexue.fm/archives/10945/comment-page-1#comment-27828): 你可以拿一批语料去eval，看各个expert分别激活了多少次呀。
- [苏剑林](https://kexue.fm/archives/9164/comment-page-4#comment-27827): 首先这个分布肯定是存在的（贝叶斯公式无关分布），然后它的概率密度对数是二次函数形式，概率密度的...
- [苏剑林](https://kexue.fm/archives/10907/comment-page-2#comment-27826): 这两天看了下FoPE，感觉它的分析有点道理，但它实现的代码跟论文其实是不一样的。看论文的描述，...

### 友情链接

- [Cool Papers](https://papers.cool)
- [数学研发](https://bbs.emath.ac.cn)
- [Seatop](http://www.seatop.com.cn/)
- [Xiaoxia](https://xiaoxia.org/)
- [积分表-网络版](https://kexue.fm/sci/integral/index.html)
- [丝路博傲](http://blog.dvxj.com/)
- [ph4ntasy 饭特稀](http://www.ph4ntasy.com/)
- [数学之家](http://www.2math.cn/)
- [有趣天文奇观](http://interesting-sky.china-vo.org/)
- [TwistedW](http://www.twistedwg.com/)
- [godweiyang](https://godweiyang.com/)
- [AI柠檬](https://blog.ailemon.net/)
- [王登科-DK博客](https://greatdk.com)
- [ESON](https://blog.eson.org/)
- [枫之羽](https://fzhiy.net/)
- [Mathor's blog](https://wmathor.com/)
- [coding-zuo](https://coding-zuo.github.io/)
- [博科园](https://www.bokeyuan.net/)
- [孔皮皮的博客](https://www.kppkkp.top/)
- [运鹏的博客](https://yunpengtai.top/)
- [jiming.site](https://jiming.site/)
- [OmegaXYZ](https://www.omegaxyz.com/)
- [Blog by Eacls](https://www.eacls.top/)
- [EAI猩球](https://www.robotech.ink/)
- [文举的博客](https://liwenju0.com/)
- [用代码打点酱油](https://bruceyuan.com/)
- [申请链接](https://kexue.fm/links.html)

本站采用创作共用版权协议，要求署名、非商业用途和保持一致。转载本站内容必须也遵循“ [署名-非商业用途-保持一致](http://creativecommons.org/licenses/by-nc-nd/2.5/cn/)”的创作共用协议。
© 2009-2025 Scientific Spaces. All rights reserved. Theme by [laogui](http://www.laogui.com). Powered by [Typecho](http://typecho.org). 备案号: [粤ICP备09093259号-1/2](https://beian.miit.gov.cn/)。