Processing math: 0%

![MobileSideBar](https://kexue.fm/usr/themes/geekg/images/slide-button.png)

## SEARCH

## MENU

- [打赏](https://kexue.fm/reward.html)
- [公式](https://kexue.fm/latex.html)
- [天象](https://kexue.fm/ac.html)
- [链接](https://kexue.fm/links.html)
- [时光](https://kexue.fm/me.html)
- [博览](https://kexue.fm/science.html)
- [归档](https://kexue.fm/content.html)

## CATEGORIES

- [千奇百怪](https://kexue.fm/category/Everything)
- [天文探索](https://kexue.fm/category/Astronomy)
- [数学研究](https://kexue.fm/category/Mathematics)
- [物理化学](https://kexue.fm/category/Phy-chem)
- [信息时代](https://kexue.fm/category/Big-Data)
- [生物自然](https://kexue.fm/category/Biology)
- [图片摄影](https://kexue.fm/category/Photograph)
- [问题百科](https://kexue.fm/category/Questions)
- [生活/情感](https://kexue.fm/category/Life-Feeling)
- [资源共享](https://kexue.fm/category/Resources)

## NEWPOSTS

- [让炼丹更科学一些（五）：基于梯度精...](https://kexue.fm/archives/11530)
- [让炼丹更科学一些（四）：新恒等式，...](https://kexue.fm/archives/11494)
- [为什么DeltaNet要加L2 N...](https://kexue.fm/archives/11486)
- [让炼丹更科学一些（三）：SGD的终...](https://kexue.fm/archives/11480)
- [让炼丹更科学一些（二）：将结论推广...](https://kexue.fm/archives/11469)
- [滑动平均视角下的权重衰减和学习率](https://kexue.fm/archives/11459)
- [生成扩散模型漫谈（三十一）：预测数...](https://kexue.fm/archives/11428)
- [Muon优化器指南：快速上手与关键细节](https://kexue.fm/archives/11416)
- [AdamW的Weight RMS的...](https://kexue.fm/archives/11404)
- [n个正态随机数的最大值的渐近估计](https://kexue.fm/archives/11390)

## COMMENTS

- [Bin: 今天偶然从某个论坛看到有人推荐您的博客，定睛一看竟然是华师同院...](https://kexue.fm/archives/1990/comment-page-2#comment-29105)
- [Rapture D: 我有一个问题，为什么不考虑亥姆霍兹定理和斯托克斯公式。](https://kexue.fm/archives/11530/comment-page-1#comment-29104)
- [mofheka: 苏神是还在用jax是么？最近在做基于Google Pathwa...](https://kexue.fm/archives/11390/comment-page-1#comment-29103)
- [长琴: 看懂这篇博客也不是一件容易的事情。](https://kexue.fm/archives/11530/comment-page-1#comment-29102)
- [AlexLi: 苏老师，请教一下(7)式中将 \\mu(x\_t) 传给 $p...](https://kexue.fm/archives/9257/comment-page-4#comment-29101)
- [tyler\_zxc: "Performer的思想是将标准的Attention线性化，...](https://kexue.fm/archives/7921/comment-page-2#comment-29100)
- [我: 似乎并非mHC提出矩阵的思想？之前hyper connecti...](https://kexue.fm/archives/11494/comment-page-1#comment-29099)
- [winter: 苏神您好，假如对于比较均匀的attention weightP...](https://kexue.fm/archives/10847/comment-page-1#comment-29098)
- [苏剑林: KL散度、JS散度、W距离啥的，都行啊，看你喜欢哪个](https://kexue.fm/archives/8512/comment-page-2#comment-29097)
- [苏剑林: 没有绝对公平的对比方法，主要看你关心什么。比如，如果只关心推理...](https://kexue.fm/archives/9119/comment-page-14#comment-29096)

## USERLOGIN

- [登录](https://kexue.fm/admin/login.php)

[科学空间\|Scientific Spaces](https://kexue.fm/)

- [登录](https://kexue.fm/admin/login.php)
- [打赏](https://kexue.fm/reward.html)
- [公式](https://kexue.fm/latex.html)
- [天象](https://kexue.fm/ac.html)
- [链接](https://kexue.fm/links.html)
- [时光](https://kexue.fm/me.html)
- [博览](https://kexue.fm/science.html)
- [归档](https://kexue.fm/content.html)

渴望成为一个小飞侠

- [![](https://kexue.fm/usr/themes/geekg/images/rss.png)\\
\\
欢迎订阅](https://kexue.fm/feed)
- [![](https://kexue.fm/usr/themes/geekg/images/mail.png)\\
\\
个性邮箱](https://kexue.fm/archives/119)
- [![](https://kexue.fm/usr/themes/geekg/images/Saturn.png)\\
\\
天象信息](https://kexue.fm/ac.html)
- [![](https://kexue.fm/usr/themes/geekg/images/iss.png)\\
\\
观测ISS](https://kexue.fm/archives/41)
- [![](https://kexue.fm/usr/themes/geekg/images/pi.png)\\
\\
LaTeX](https://kexue.fm/latex.html)
- [![](https://kexue.fm/usr/themes/geekg/images/mlogo.png)\\
\\
关于博主](https://kexue.fm/me.html)

欢迎访问“科学空间”，这里将与您共同探讨自然科学，回味人生百态；也期待大家的分享～

- [**千奇百怪** Everything](https://kexue.fm/category/Everything)
- [**天文探索** Astronomy](https://kexue.fm/category/Astronomy)
- [**数学研究** Mathematics](https://kexue.fm/category/Mathematics)
- [**物理化学** Phy-chem](https://kexue.fm/category/Phy-chem)
- [**信息时代** Big-Data](https://kexue.fm/category/Big-Data)
- [**生物自然** Biology](https://kexue.fm/category/Biology)
- [**图片摄影** Photograph](https://kexue.fm/category/Photograph)
- [**问题百科** Questions](https://kexue.fm/category/Questions)
- [**生活/情感** Life-Feeling](https://kexue.fm/category/Life-Feeling)
- [**资源共享** Resources](https://kexue.fm/category/Resources)

- [**千奇百怪**](https://kexue.fm/category/Everything)
- [**天文探索**](https://kexue.fm/category/Astronomy)
- [**数学研究**](https://kexue.fm/category/Mathematics)
- [**物理化学**](https://kexue.fm/category/Phy-chem)
- [**信息时代**](https://kexue.fm/category/Big-Data)
- [**生物自然**](https://kexue.fm/category/Biology)
- [**图片摄影**](https://kexue.fm/category/Photograph)
- [**问题百科**](https://kexue.fm/category/Questions)
- [**生活/情感**](https://kexue.fm/category/Life-Feeling)
- [**资源共享**](https://kexue.fm/category/Resources)

[首页](https://kexue.fm/) [信息时代](https://kexue.fm/category/Big-Data) 生成扩散模型漫谈（十九）：作为扩散ODE的GAN

24Jun

# [生成扩散模型漫谈（十九）：作为扩散ODE的GAN](https://kexue.fm/archives/9662)

By 苏剑林 \|
2023-06-24 \|
53899位读者 \|

在文章 [《生成扩散模型漫谈（十六）：W距离 ≤ 得分匹配》](https://kexue.fm/archives/9467) 中，我们推导了Wasserstein距离与扩散模型得分匹配损失之间的一个不等式，表明扩散模型的优化目标与WGAN的优化目标在某种程度上具有相似性。而在本文，我们将探讨 [《MonoFlow: Rethinking Divergence GANs via the Perspective of Wasserstein Gradient Flows》](https://papers.cool/arxiv/2302.01075) 中的研究成果，它进一步展示了GAN与扩散模型之间的联系：GAN实际上可以被视为在另一个时间维度上的扩散ODE！

这些发现表明，尽管GAN和扩散模型表面上是两种截然不同的生成式模型，但它们实际上存在许多相似之处，并在许多方面可以相互借鉴和参考。

## 思路简介 [\#](https://kexue.fm/archives/9662\#%E6%80%9D%E8%B7%AF%E7%AE%80%E4%BB%8B)

我们知道，GAN所训练的生成器是从噪声\\boldsymbol{z}到真实样本的一个直接的确定性变换\\boldsymbol{g}\_{\\boldsymbol{\\theta}}(\\boldsymbol{z})，而扩散模型的显著特点是“渐进式生成”，它的生成过程对应于从一系列渐变的分布p\_0(\\boldsymbol{x}\_0),p\_1(\\boldsymbol{x}\_1),\\cdots,p\_T(\\boldsymbol{x}\_T)中采样（注：在前面十几篇文章中，\\boldsymbol{x}\_T是噪声，\\boldsymbol{x}\_0是目标样本，采样过程是\\boldsymbol{x}\_T\\to \\boldsymbol{x}\_0，但为了便于下面的表述，这里反过来改为\\boldsymbol{x}\_0\\to \\boldsymbol{x}\_T）。看上去确实找不到多少相同之处，那怎么才能将两者联系起来呢？

很明显，如果想要从扩散模型的视角理解GAN，那么就要想办法构造出一系列渐变的分布出来。生成器\\boldsymbol{g}\_{\\boldsymbol{\\theta}}(\\boldsymbol{z})本身就是一个一步到位的变换，不存在渐变，然而我们知道模型的优化是渐变的，可否用参数\\boldsymbol{\\theta}的历史轨迹\\boldsymbol{\\theta}\_t来构建这一系列渐变分布呢？具体来说，假设生成器初始化为\\boldsymbol{\\theta}\_0，经过T步对抗训练后得到最优参数\\boldsymbol{\\theta}\_T，训练过程的中间参数为\\boldsymbol{\\theta}\_1,\\boldsymbol{\\theta}\_2,\\cdots,\\boldsymbol{\\theta}\_{T-1}，那么我们定义\\boldsymbol{x}\_t = \\boldsymbol{g}\_{\\boldsymbol{\\theta}\_t}(\\boldsymbol{z})，不就定义了一系列渐变的\\boldsymbol{x}\_0,\\boldsymbol{x}\_1,\\cdots,\\boldsymbol{x}\_T，从而也就定义了渐变的分布p\_0(\\boldsymbol{x}\_0),p\_1(\\boldsymbol{x}\_1),\\cdots,p\_T(\\boldsymbol{x}\_T)了？

如果这个思路可行的话，那么GAN就可以诠释为梯度下降的（虚拟）时间维度上的扩散模型！下面我们就沿着这个思路进行探索。

## 梯度之流 [\#](https://kexue.fm/archives/9662\#%E6%A2%AF%E5%BA%A6%E4%B9%8B%E6%B5%81)

首先，我们需要重温上一篇文章 [《梯度流：探索通向最小值之路》](https://kexue.fm/archives/9660) 关于Wasserstein梯度流的结果：它指出方程

\\begin{equation}\\frac{\\partial q\_t(\\boldsymbol{x})}{\\partial t} = - \\nabla\_{\\boldsymbol{x}}\\cdot\\big(q\_t(\\boldsymbol{x})\\nabla\_{\\boldsymbol{x}}\\log r\_t(\\boldsymbol{x})\\big)\\label{eq:w-flow}\\end{equation}

在最小化p(\\boldsymbol{x})和q\_t(\\boldsymbol{x})的KL散度，即\\lim\\limits\_{t\\to\\infty} q\_t(\\boldsymbol{x}) = p(\\boldsymbol{x})，这里r\_t(\\boldsymbol{x})=\\frac{p(\\boldsymbol{x})}{q\_t(\\boldsymbol{x})}。如果p(\\boldsymbol{x})代表真实样本的分布，那么如果能实现从q\_t(\\boldsymbol{x})采样的话，那么逐渐推到t\\to\\infty时，就可以实现从p(\\boldsymbol{x})采样了。根据 [《测试函数法推导连续性方程和Fokker-Planck方程》](https://kexue.fm/archives/9461)，从q\_t(\\boldsymbol{x})采样可以通过下述ODE实现：

\\begin{equation}\\frac{d\\boldsymbol{x}}{dt} = \\nabla\_{\\boldsymbol{x}}\\log r\_t(\\boldsymbol{x})\\label{eq:ode-core}\\end{equation}

然而，上式中的r\_t(\\boldsymbol{x})是未知的，所以我们还无法通过上式进行采样，需要先想办法估算r\_t(\\boldsymbol{x})。

## 判别估计 [\#](https://kexue.fm/archives/9662\#%E5%88%A4%E5%88%AB%E4%BC%B0%E8%AE%A1)

这时候登场的是GAN的判别器。以最早的Vanilla GAN为例，它的训练目标是

\\begin{equation}\\max\_D\\, \\mathbb{E}\_{\\boldsymbol{x}\\sim p(\\boldsymbol{x})}\[\\log \\sigma(D(\\boldsymbol{x}))\] + \\mathbb{E}\_{\\boldsymbol{x}\\sim q(\\boldsymbol{x})}\[\\log (1 - \\sigma(D(\\boldsymbol{x})))\]\\label{eq:gan-d}\\end{equation}

这里的D是判别器，\\sigma(t)=1/(1+e^{-t})是sigmoid函数，p(\\boldsymbol{x})是真样本的分布，q(\\boldsymbol{x})是假样本的分布。可以证明（不清楚的读者可以参考 [《RSGAN：对抗模型中的“图灵测试”思想》](https://kexue.fm/archives/6110) 中的“补充证明”一节），上式中判别器D的理论最优解是

\\begin{equation}D(\\boldsymbol{x}) = \\log \\frac{p(\\boldsymbol{x})}{q(\\boldsymbol{x})}\\end{equation}

更一般化的f-GAN（参考 [《f-GAN简介：GAN模型的生产车间》](https://kexue.fm/archives/6016)、 [《Designing GANs：又一个GAN生产车间》](https://kexue.fm/archives/7210)）结果会稍有不同，但可以证明的是它们判别器的理论最优解都是\\frac{p(\\boldsymbol{x})}{q(\\boldsymbol{x})}的函数。也就是说，只要我们可以实现从p(\\boldsymbol{x})和q\_t(\\boldsymbol{x})中采样，那么通过GAN的判别器训练\\eqref{eq:gan-d}就可以估算出r\_t(\\boldsymbol{x})=\\frac{p(\\boldsymbol{x})}{q\_t(\\boldsymbol{x})}出来。

## 向前一步 [\#](https://kexue.fm/archives/9662\#%E5%90%91%E5%89%8D%E4%B8%80%E6%AD%A5)

这时候可能有读者疑惑：这不就进入“鸡生蛋、蛋生鸡”的循环论证了吗？我们估算r\_t(\\boldsymbol{x})不就是为了利用式\\eqref{eq:ode-core}实现从q\_t(\\boldsymbol{x})中采样吗？现在你又假设能从q\_t(\\boldsymbol{x})采样才来估算r\_t(\\boldsymbol{x})？不着急，经典的一笔就要来了。

假设我们有生成器\\boldsymbol{g}\_{\\boldsymbol{\\theta}\_t}(\\boldsymbol{z})，它的采样生成结果就等于从q\_t(\\boldsymbol{x})采样的结果，即

\\begin{equation}\\big\\{\\boldsymbol{g}\_{\\boldsymbol{\\theta}\_t}(\\boldsymbol{z})\\big\|\\,\\boldsymbol{z}\\sim \\mathcal{N}(\\boldsymbol{0},\\boldsymbol{I})\\big\\}\\quad = \\quad\\big\\{\\boldsymbol{x}\_t\\big\|\\,\\boldsymbol{x}\_t\\sim q\_t(\\boldsymbol{x})\\big\\}\\end{equation}

那么现在我们就可以利用它和式\\eqref{eq:gan-d}来估算r\_t(\\boldsymbol{x})。注意这只是t时刻的r\_t(\\boldsymbol{x})，其他时刻的r\_t(\\boldsymbol{x})我们并不知道，所以无法直接通过式\\eqref{eq:ode-core}完成最终的采样过程，但是我们可以往前推一小步：

\\begin{equation}\\boldsymbol{x}\_{t+1} = \\boldsymbol{x}\_t + \\epsilon \\nabla\_{\\boldsymbol{x}\_t}\\log r\_t(\\boldsymbol{x}\_t) = \\boldsymbol{x}\_t + \\epsilon \\nabla\_{\\boldsymbol{x}\_t} D(\\boldsymbol{x}\_t)\\label{eq:forward}\\end{equation}

这里的\\epsilon是一个很小的正数，代表步长。那么，现在我们就有了下一步采样的结果，我们希望它继续能等价于下一步的生成器的采样结果，即

\\begin{equation}\\begin{aligned}
\\big\\{\\boldsymbol{g}\_{\\boldsymbol{\\theta}\_{t+1}}(\\boldsymbol{z})\\big\|\\,\\boldsymbol{z}\\sim \\mathcal{N}(\\boldsymbol{0},\\boldsymbol{I})\\big\\}\\quad =& \\quad\\big\\{\\boldsymbol{x}\_{t+1}\\big\|\\,\\boldsymbol{x}\_{t+1}\\sim q\_{t+1}(\\boldsymbol{x})\\big\\} \\\\[5pt\]
\\quad =& \\quad\\big\\{\\boldsymbol{x}\_{t+1}\\big\|\\,\\boldsymbol{x}\_t + \\epsilon \\nabla\_{\\boldsymbol{x}\_t} D(\\boldsymbol{x}\_t),\\boldsymbol{x}\_t\\sim q\_t(\\boldsymbol{x})\\big\\}
\\end{aligned}\\end{equation}

换句话说，我们想要 **将扩散模型中样本的运动转化为生成器参数的运动**！为了达到这个目标，我们通过如下损失去求\\boldsymbol{\\theta}\_{t+1}：

\\begin{equation}\\boldsymbol{\\theta}\_{t+1} = \\mathop{\\text{argmin}}\_{\\boldsymbol{\\theta}}\\mathbb{E}\_{\\boldsymbol{z}\\sim \\mathcal{N}(\\boldsymbol{0},\\boldsymbol{I})}\\Big\[\\big\\Vert \\boldsymbol{g}\_{\\boldsymbol{\\theta}}(\\boldsymbol{z}) - \\boldsymbol{g}\_{\\boldsymbol{\\theta}\_t}(\\boldsymbol{z}) - \\epsilon \\nabla\_{\\boldsymbol{g}}D(\\boldsymbol{g}\_{\\boldsymbol{\\theta}\_t}(\\boldsymbol{z}))\\big\\Vert^2\\Big\]\\label{eq:gan-g0}\\end{equation}

也就是说，拿\\boldsymbol{x}\_t = \\boldsymbol{g}\_{\\boldsymbol{\\theta}\_t}(\\boldsymbol{z})往前迭代一步得到\\boldsymbol{x}\_{t+1}，然后希望新的\\boldsymbol{g}\_{\\boldsymbol{\\theta}\_{t+1}}(\\boldsymbol{z})能尽量等于\\boldsymbol{x}\_{t+1}。完成这一轮后，再用\\boldsymbol{\\theta}\_{t+1}替代原本的\\boldsymbol{\\theta}\_t开始新一轮的迭代，也就是式\\eqref{eq:gan-d}和式\\eqref{eq:gan-g0}交替执行，是不是就有GAN的味道了？

## 点睛之笔 [\#](https://kexue.fm/archives/9662\#%E7%82%B9%E7%9D%9B%E4%B9%8B%E7%AC%94)

如果这还不够，我们还可以继续完善一下，将它变得跟GAN更加一致。注意到式\\eqref{eq:gan-g0}的被期望函数的梯度是：

\\begin{equation}\\begin{aligned}
&\\,\\nabla\_{\\boldsymbol{\\theta}}\\Vert \\boldsymbol{g}\_{\\boldsymbol{\\theta}}(\\boldsymbol{z}) - \\boldsymbol{g}\_{\\boldsymbol{\\theta}\_t}(\\boldsymbol{z}) - \\epsilon \\nabla\_{\\boldsymbol{g}}D(\\boldsymbol{g}\_{\\boldsymbol{\\theta}\_t}(\\boldsymbol{z}))\\Vert^2 \\\
=&\\,2\\big\\langle\\boldsymbol{g}\_{\\boldsymbol{\\theta}}(\\boldsymbol{z}) - \\boldsymbol{g}\_{\\boldsymbol{\\theta}\_t}(\\boldsymbol{z}) - \\epsilon \\nabla\_{\\boldsymbol{g}}D(\\boldsymbol{g}\_{\\boldsymbol{\\theta}\_t}(\\boldsymbol{z})), \\nabla\_{\\boldsymbol{\\theta}}\\boldsymbol{g}\_{\\boldsymbol{\\theta}}(\\boldsymbol{z}) \\big\\rangle \\\
\\end{aligned}\\end{equation}

代入当前值\\boldsymbol{\\theta}=\\boldsymbol{\\theta}\_t，那么结果是

\\begin{equation}-2\\epsilon\\big\\langle \\nabla\_{\\boldsymbol{g}}D(\\boldsymbol{g}\_{\\boldsymbol{\\theta}\_t}(\\boldsymbol{z})), \\nabla\_{\\boldsymbol{\\theta}\_t}\\boldsymbol{g}\_{\\boldsymbol{\\theta}\_t}(\\boldsymbol{z}) \\big\\rangle = -2\\epsilon\\nabla\_{\\boldsymbol{\\theta}\_t}D(\\boldsymbol{g}\_{\\boldsymbol{\\theta}\_t}(\\boldsymbol{z}))\\end{equation}

也就是说，如果用基于梯度的优化器只优化一步的话，那么以式\\eqref{eq:gan-g0}为损失函数，跟以下式为损失函数，结果是等价的（因为梯度只差一个常数倍）：

\\begin{equation}\\boldsymbol{\\theta}\_{t+1} = \\mathop{\\text{argmin}}\_{\\boldsymbol{\\theta}}\\mathbb{E}\_{\\boldsymbol{z}\\sim \\mathcal{N}(\\boldsymbol{0},\\boldsymbol{I})}\[-D(\\boldsymbol{g}\_{\\boldsymbol{\\theta}}(\\boldsymbol{z}))\]\\label{eq:gan-g}\\end{equation}

这便是常见的生成器损失之一。式\\eqref{eq:gan-d}和式\\eqref{eq:gan-g}交替训练，就是一个常见的GAN变体。

特别地，原论文还证明了生成器的损失函数可以一般化为

\\begin{equation}\\boldsymbol{\\theta}\_{t+1} = \\mathop{\\text{argmin}}\_{\\boldsymbol{\\theta}}\\mathbb{E}\_{\\boldsymbol{z}\\sim \\mathcal{N}(\\boldsymbol{0},\\boldsymbol{I})}\[-h(D(\\boldsymbol{g}\_{\\boldsymbol{\\theta}}(\\boldsymbol{z})))\]\\end{equation}

其中h(\\cdot)是任意单调递增函数，它也对应于Wasserstein梯度流\\eqref{eq:w-flow}中的\\log r\_t(\\boldsymbol{x})可以换成h(\\log r\_t(\\boldsymbol{x}))，这应该也是MonoFlow一词的来源（Monotonically increasing function + Wasserstein flow）。这个证明过程就不展开了，大家自行看原论文就好。

## 意义思考 [\#](https://kexue.fm/archives/9662\#%E6%84%8F%E4%B9%89%E6%80%9D%E8%80%83)

总的来说，将GAN理解为扩散模型的思路是

\\require{AMScd}\\begin{CD}
\\cdots @>\\quad>> \\boldsymbol{g}\_{\\boldsymbol{\\theta}\_t}(\\boldsymbol{z}) @> 式\\eqref{eq:gan-d} >> r\_t(\\boldsymbol{x}) @> 式\\eqref{eq:forward}>> \\boldsymbol{x}\_{t+1} @> 式\\eqref{eq:gan-g0}>> \\boldsymbol{g}\_{\\boldsymbol{\\theta}\_{t+1}}(\\boldsymbol{z})@>\\quad>>\\cdots
\\end{CD}

其中，核心的式子是\\eqref{eq:forward}，它源于Wasserstein梯度流的式\\eqref{eq:w-flow}和式\\eqref{eq:ode-core}，这部分我们在上一篇文章 [《梯度流：探索通向最小值之路》](https://kexue.fm/archives/9660) 讨论过了。

可能有读者想问：这个视角看上去并没有得到比GAN更多的东西，为什么要费这番大功夫去重新理解GAN呢？首先，在笔者看来，从扩散模型角度理解GAN，或者说将扩散模型和GAN统一起来，它本身就是一件很有趣、很好玩的事情，并不一定需要发挥什么实际作用，有趣、好玩就是它最大的意义。

其次，如同作者在原论文所说，已有的GAN的推导过程跟它实际的训练过程是不一致的，而本文所讨论的扩散视角，则是跟训练过程是一致的。也就是说，以训练过程为标准的话，GAN已有的推导过程是错的，本文的扩散视角才是对的。怎么理解这一点呢？以前面提到的GAN为例，判别器和生成器的目标分别是：

\\begin{gather}\\max\_D\\, \\mathbb{E}\_{\\boldsymbol{x}\\sim p(\\boldsymbol{x})}\[\\log \\sigma(D(\\boldsymbol{x}))\] + \\mathbb{E}\_{\\boldsymbol{x}\\sim q(\\boldsymbol{x})}\[\\log (1 - \\sigma(D(\\boldsymbol{x})))\] \\\
\\min\_q\\mathbb{E}\_{\\boldsymbol{x}\\sim q(\\boldsymbol{x})}\[-D(\\boldsymbol{x})\]
\\end{gather}

通常的证明方式是，证明D的最优解是\\log\\frac{p(\\boldsymbol{x})}{q(\\boldsymbol{x})}，然后代入生成器的损失函数，发现它在最小化q(\\boldsymbol{x}),p(\\boldsymbol{x})的KL散度，所以最优解是q(\\boldsymbol{x})=p(\\boldsymbol{x})。但是，这样的证明对应的训练方式应该是先针对任意的q(\\boldsymbol{x})，将\\max\\limits\_D这一步都求解出来（求出的D应该是q(x)的函数，或者说应该是生成器参数\\boldsymbol{\\theta}的函数），然后再去执行\\min\\limits\_q这一步，而不是实际上用的交替训练。而基于扩散模型的理解，它在设计上就是交替的，所以它跟训练过程更加一致。

总的来说，从扩散模型的角度来理解GAN，不单单是理解GAN的一种新视角，而且还是一种更贴近训练过程的视角。比如，我们可以解释为什么GAN的生成器不能训练太多步，因为只有单步优化时，式\\eqref{eq:gan-g}和式\\eqref{eq:gan-g0}才等价，如果GAN要进行更多步的优化，那么应该使用式\\eqref{eq:gan-g0}为损失函数。事实上，式\\eqref{eq:gan-g0}就相当于笔者之前在 [《用变分推断统一理解生成模型（VAE、GAN、AAE、ALI）》](https://kexue.fm/archives/5716) 所提出的KL\\left(q(x)\\Vert q^{o}(x)\\right)项，它保证了生成器的“传承”而不仅仅是“创新”。

## 文章小结 [\#](https://kexue.fm/archives/9662\#%E6%96%87%E7%AB%A0%E5%B0%8F%E7%BB%93)

本文介绍了MonoFlow，它展示了我们可以将GAN理解为在另一个时间维度上的扩散ODE，从而建立了一种基于扩散模型理解GAN的新视角。特别地，这是一种比GAN的常规推导更加贴近训练过程的视角。

_**转载到请包括本文地址：** [https://kexue.fm/archives/9662](https://kexue.fm/archives/9662 "生成扩散模型漫谈（十九）：作为扩散ODE的GAN")_

_**更详细的转载事宜请参考：**_ [《科学空间FAQ》](https://kexue.fm/archives/6508#%E6%96%87%E7%AB%A0%E5%A6%82%E4%BD%95%E8%BD%AC%E8%BD%BD/%E5%BC%95%E7%94%A8 "《科学空间FAQ》")

**如果您还有什么疑惑或建议，欢迎在下方评论区继续讨论。**

**如果您觉得本文还不错，欢迎 [分享](https://kexue.fm/archives/9662#share)/ [打赏](https://kexue.fm/archives/9662#pay) 本文。打赏并非要从中获得收益，而是希望知道科学空间获得了多少读者的真心关注。当然，如果你无视它，也不会影响你的阅读。再次表示欢迎和感谢！**

打赏

![科学空间](https://kexue.fm/usr/themes/geekg/payment/wx.png)

微信打赏

![科学空间](https://kexue.fm/usr/themes/geekg/payment/zfb.png)

支付宝打赏

因为网站后台对打赏并无记录，因此欢迎在打赏时候备注留言。

你还可以 [**点击这里**](http://mail.qq.com/cgi-bin/qm_share?t=qm_mailme&email=tN7d1drY3drrx8H0xcWa19vZ) 或在下方评论区留言来告知你的建议或需求。

**如果您需要引用本文，请参考：**

苏剑林. (Jun. 24, 2023). 《生成扩散模型漫谈（十九）：作为扩散ODE的GAN 》\[Blog post\]. Retrieved from [https://kexue.fm/archives/9662](https://kexue.fm/archives/9662)

@online{kexuefm-9662,

         title={生成扩散模型漫谈（十九）：作为扩散ODE的GAN},

         author={苏剑林},

         year={2023},

         month={Jun},

         url={\\url{https://kexue.fm/archives/9662}},

}


分类： [信息时代](https://kexue.fm/category/Big-Data)    标签： [优化](https://kexue.fm/tag/%E4%BC%98%E5%8C%96/), [GAN](https://kexue.fm/tag/GAN/), [扩散](https://kexue.fm/tag/%E6%89%A9%E6%95%A3/)[14 评论](https://kexue.fm/archives/9662#comments)

< [梯度流：探索通向最小值之路](https://kexue.fm/archives/9660 "梯度流：探索通向最小值之路") \| [生成扩散模型漫谈（二十）：从ReFlow到WGAN-GP](https://kexue.fm/archives/9668 "生成扩散模型漫谈（二十）：从ReFlow到WGAN-GP") >

### 你也许还对下面的内容感兴趣

- [生成扩散模型漫谈（三十一）：预测数据而非噪声](https://kexue.fm/archives/11428 "生成扩散模型漫谈（三十一）：预测数据而非噪声")
- [Muon优化器指南：快速上手与关键细节](https://kexue.fm/archives/11416 "Muon优化器指南：快速上手与关键细节")
- [低精度Attention可能存在有偏的舍入误差](https://kexue.fm/archives/11371 "低精度Attention可能存在有偏的舍入误差")
- [MuP之上：1. 好模型的三个特征](https://kexue.fm/archives/11340 "MuP之上：1. 好模型的三个特征")
- [QK-Clip：让Muon在Scaleup之路上更进一步](https://kexue.fm/archives/11126 "QK-Clip：让Muon在Scaleup之路上更进一步")
- [Transformer升级之路：21、MLA好在哪里?（下）](https://kexue.fm/archives/11111 "Transformer升级之路：21、MLA好在哪里?（下）")
- [生成扩散模型漫谈（三十）：从瞬时速度到平均速度](https://kexue.fm/archives/10958 "生成扩散模型漫谈（三十）：从瞬时速度到平均速度")
- [MoE环游记：5、均匀分布的反思](https://kexue.fm/archives/10945 "MoE环游记：5、均匀分布的反思")
- [Transformer升级之路：20、MLA好在哪里?（上）](https://kexue.fm/archives/10907 "Transformer升级之路：20、MLA好在哪里?（上）")
- [MoE环游记：4、难处应当多投入](https://kexue.fm/archives/10815 "MoE环游记：4、难处应当多投入")

[发表你的看法](https://kexue.fm/archives/9662#comment_form)

[生成扩散模型漫谈（十九）：作为扩散ODE的GAN R11; AI 資訊](https://news.aitime.space/2023/06/6772/)

June 24th, 2023

\[...\]​Read More \[...\]

[回复评论](https://kexue.fm/archives/9662/comment-page-1?replyTo=22037#respond-post-9662)

o\_glay

June 25th, 2023

我想了想，既然都从轨迹的角度来看待了，能不能用卡尔曼滤波来加速扩散模型的收敛呢，于是我在Reflow的基础上把目标改成v(x\_1, t\_1)=v\_{ema}(x\_2, t\_2)+t\_2 \\cdot (v - v\_{ema}(x\_2, t\_2))，没想到还真能大幅加速模型收敛的速度，这里就相当于用了最简单的g-h 滤波器，那么理论上GAN也可以这样。

[回复评论](https://kexue.fm/archives/9662/comment-page-1?replyTo=22054#respond-post-9662)

ZYZ 发表于
June 27th, 2023

大佬最近我也在看cm，有没有兴趣留个联系方式交流一下

[回复评论](https://kexue.fm/archives/9662/comment-page-1?replyTo=22064#respond-post-9662)

o\_glay 发表于
June 28th, 2023

可以加个Discord：amun\_alpha

[回复评论](https://kexue.fm/archives/9662/comment-page-1?replyTo=22085#respond-post-9662)

[苏剑林](https://kexue.fm/) 发表于
June 27th, 2023

恭喜！惭愧，这个概念已经超出我的理解范围了（捂脸）～

[回复评论](https://kexue.fm/archives/9662/comment-page-1?replyTo=22069#respond-post-9662)

yantijin 发表于
June 27th, 2023

请问大佬说的是RectifiedFlow里面提出的那个Reflow模型吗？这里ema的权重是怎么确定的呢？

[回复评论](https://kexue.fm/archives/9662/comment-page-1?replyTo=22081#respond-post-9662)

o\_glay 发表于
June 28th, 2023

ema就只是模型的滑动平均啊，我这里用的0.95的平滑系数

[回复评论](https://kexue.fm/archives/9662/comment-page-1?replyTo=22086#respond-post-9662)

ReyLock 发表于
October 23rd, 2023

想问下o\_glay大佬，为啥用卡尔曼滤波能加速扩散模型的收敛呢？and为啥既然都从轨迹的角度来看待了就可以用卡尔曼滤波嘞？

[回复评论](https://kexue.fm/archives/9662/comment-page-1?replyTo=22942#respond-post-9662)

nlp爱好者

June 27th, 2023

看了作者的词向量相关的文章，很感兴趣。我也有一个问题，希望作者赐教。

词向量和句向量，除了可以进行相似度以及距离等计算。

我这里关于词和其它词以及(句子、以及多个词和句子的组合)之间的结构化关系，词向量等怎样表达呢？

[回复评论](https://kexue.fm/archives/9662/comment-page-1?replyTo=22077#respond-post-9662)

[苏剑林](https://kexue.fm/) 发表于
June 28th, 2023

一般情况下，词向量没有这种功能。你可能需要参考ON-LSTM： [https://kexue.fm/archives/6621](https://kexue.fm/archives/6621)

[回复评论](https://kexue.fm/archives/9662/comment-page-1?replyTo=22094#respond-post-9662)

yantijin

June 27th, 2023

awesome!!

[回复评论](https://kexue.fm/archives/9662/comment-page-1?replyTo=22080#respond-post-9662)

[生成扩散模型漫谈（二十）：从ReFlow到WGAN-GP R11; AI 資訊](https://news.aitime.space/2023/06/7890/)

June 28th, 2023

\[...\]上一篇文章《生成扩散模型漫谈（十九）：作为扩散ODE的GAN》中，我们介绍了如何将GAN理解为在另一个时间维度上的扩散ODE，简而言之，GAN实际上就是将扩散模型中样本的运动转化为生成器参数的运动！然而，该文章的推导过程依赖于Wasserstein梯度流等相对复杂和独立的内容，没法很好地跟扩散系列前面的文章连接起来，技术上显得有些“断层”。\[...\]

[回复评论](https://kexue.fm/archives/9662/comment-page-1?replyTo=22090#respond-post-9662)

ZYZ

August 28th, 2023

公式(1)右边的q好像缺了下标t。

还有就是 “鸡生蛋、蛋生鸡”的循环论证 这里的操作是不是能量模型里面挺常见的？

[回复评论](https://kexue.fm/archives/9662/comment-page-1?replyTo=22580#respond-post-9662)

[苏剑林](https://kexue.fm/) 发表于
August 28th, 2023

谢谢指出，已经修正。从能量模型的角度来看， 能量模型的似然函数的梯度确实呈现出“鸡生蛋、蛋生鸡”形式，确实算是常见了。

[回复评论](https://kexue.fm/archives/9662/comment-page-1?replyTo=22589#respond-post-9662)

[取消回复](https://kexue.fm/archives/9662#respond-post-9662)

你的大名

电子邮箱

个人网站（选填）

1\. 可以使用LaTeX代码，点击“预览效果”可查看效果；

2\. 可以通过点击评论楼层编号来引用该楼层；

3\. 网站可能会有点卡，如非确认评论失败，请 **不要重复点击提交**。

### 内容速览

[思路简介](https://kexue.fm/archives/9662#%E6%80%9D%E8%B7%AF%E7%AE%80%E4%BB%8B)
[梯度之流](https://kexue.fm/archives/9662#%E6%A2%AF%E5%BA%A6%E4%B9%8B%E6%B5%81)
[判别估计](https://kexue.fm/archives/9662#%E5%88%A4%E5%88%AB%E4%BC%B0%E8%AE%A1)
[向前一步](https://kexue.fm/archives/9662#%E5%90%91%E5%89%8D%E4%B8%80%E6%AD%A5)
[点睛之笔](https://kexue.fm/archives/9662#%E7%82%B9%E7%9D%9B%E4%B9%8B%E7%AC%94)
[意义思考](https://kexue.fm/archives/9662#%E6%84%8F%E4%B9%89%E6%80%9D%E8%80%83)
[文章小结](https://kexue.fm/archives/9662#%E6%96%87%E7%AB%A0%E5%B0%8F%E7%BB%93)

### 智能搜索

支持整句搜索！网站自动使用 [结巴分词](https://github.com/fxsjy/jieba) 进行分词，并结合ngrams排序算法给出合理的搜索结果。

### 热门标签

[生成模型](https://kexue.fm/tag/%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B/) [attention](https://kexue.fm/tag/attention/) [优化](https://kexue.fm/tag/%E4%BC%98%E5%8C%96/) [语言模型](https://kexue.fm/tag/%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/) [模型](https://kexue.fm/tag/%E6%A8%A1%E5%9E%8B/) [梯度](https://kexue.fm/tag/%E6%A2%AF%E5%BA%A6/) [网站](https://kexue.fm/tag/%E7%BD%91%E7%AB%99/) [概率](https://kexue.fm/tag/%E6%A6%82%E7%8E%87/) [矩阵](https://kexue.fm/tag/%E7%9F%A9%E9%98%B5/) [优化器](https://kexue.fm/tag/%E4%BC%98%E5%8C%96%E5%99%A8/) [转载](https://kexue.fm/tag/%E8%BD%AC%E8%BD%BD/) [微分方程](https://kexue.fm/tag/%E5%BE%AE%E5%88%86%E6%96%B9%E7%A8%8B/) [分析](https://kexue.fm/tag/%E5%88%86%E6%9E%90/) [天象](https://kexue.fm/tag/%E5%A4%A9%E8%B1%A1/) [深度学习](https://kexue.fm/tag/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/) [积分](https://kexue.fm/tag/%E7%A7%AF%E5%88%86/) [python](https://kexue.fm/tag/python/) [扩散](https://kexue.fm/tag/%E6%89%A9%E6%95%A3/) [力学](https://kexue.fm/tag/%E5%8A%9B%E5%AD%A6/) [无监督](https://kexue.fm/tag/%E6%97%A0%E7%9B%91%E7%9D%A3/) [几何](https://kexue.fm/tag/%E5%87%A0%E4%BD%95/) [节日](https://kexue.fm/tag/%E8%8A%82%E6%97%A5/) [生活](https://kexue.fm/tag/%E7%94%9F%E6%B4%BB/) [文本生成](https://kexue.fm/tag/%E6%96%87%E6%9C%AC%E7%94%9F%E6%88%90/) [数论](https://kexue.fm/tag/%E6%95%B0%E8%AE%BA/)

### 随机文章

- [最小熵原理（二）：“当机立断”之词库构建](https://kexue.fm/archives/5476)
- [积分估计的极值原理——变分原理的初级版本](https://kexue.fm/archives/3630)
- [2010年广东省高中学生化学竞赛试题和答案](https://kexue.fm/archives/1331)
- [让炼丹更科学一些（一）：SGD的平均损失收敛](https://kexue.fm/archives/9902)
- [一篇费曼的介绍](https://kexue.fm/archives/1686)
- [ChildTuning：试试把Dropout加到梯度上去？](https://kexue.fm/archives/8764)
- [必须要GPT3吗？不，BERT的MLM模型也能小样本学习](https://kexue.fm/archives/7764)
- [层次分解位置编码，让BERT可以处理超长文本](https://kexue.fm/archives/7947)
- [新的一个月：8月了](https://kexue.fm/archives/46)
- [两百万素数之和与“电脑病”](https://kexue.fm/archives/2991)

### 最近评论

- [Bin](https://kexue.fm/archives/1990/comment-page-2#comment-29105): 今天偶然从某个论坛看到有人推荐您的博客，定睛一看竟然是华师同院的往届师兄！看到这篇2013年的...
- [Rapture D](https://kexue.fm/archives/11530/comment-page-1#comment-29104): 我有一个问题，为什么不考虑亥姆霍兹定理和斯托克斯公式。
- [mofheka](https://kexue.fm/archives/11390/comment-page-1#comment-29103): 苏神是还在用jax是么？最近在做基于Google Pathway的理念做一个动态版的MPMD框...
- [长琴](https://kexue.fm/archives/11530/comment-page-1#comment-29102): 看懂这篇博客也不是一件容易的事情。
- [AlexLi](https://kexue.fm/archives/9257/comment-page-4#comment-29101): 苏老师，请教一下(7)式中将 \\mu(x\_t) 传给 p\_o 进行推理的操作。 $x\_...
- [tyler\_zxc](https://kexue.fm/archives/7921/comment-page-2#comment-29100): "Performer的思想是将标准的Attention线性化，所以为什么不干脆直接训练一个线性...
- [我](https://kexue.fm/archives/11494/comment-page-1#comment-29099): 似乎并非mHC提出矩阵的思想？之前hyper connection就是了
- [winter](https://kexue.fm/archives/10847/comment-page-1#comment-29098): 苏神您好，假如对于比较均匀的attention weightP，往往呈现long tail分布...
- [苏剑林](https://kexue.fm/archives/8512/comment-page-2#comment-29097): KL散度、JS散度、W距离啥的，都行啊，看你喜欢哪个
- [苏剑林](https://kexue.fm/archives/9119/comment-page-14#comment-29096): 没有绝对公平的对比方法，主要看你关心什么。比如，如果只关心推理成本和推理效果，那么有的方法可以...

### 友情链接

- [Cool Papers](https://papers.cool/)
- [数学研发](https://bbs.emath.ac.cn/)
- [Seatop](http://www.seatop.com.cn/)
- [Xiaoxia](https://xiaoxia.org/)
- [积分表-网络版](https://kexue.fm/sci/integral/index.html)
- [丝路博傲](http://blog.dvxj.com/)
- [数学之家](http://www.2math.cn/)
- [有趣天文奇观](http://interesting-sky.china-vo.org/)
- [TwistedW](http://www.twistedwg.com/)
- [godweiyang](https://godweiyang.com/)
- [AI柠檬](https://blog.ailemon.net/)
- [王登科-DK博客](https://greatdk.com/)
- [ESON](https://blog.eson.org/)
- [枫之羽](https://fzhiy.net/)
- [coding-zuo](https://coding-zuo.github.io/)
- [博科园](https://www.bokeyuan.net/)
- [孔皮皮的博客](https://www.kppkkp.top/)
- [运鹏的博客](https://yunpengtai.top/)
- [jiming.site](https://jiming.site/)
- [OmegaXYZ](https://www.omegaxyz.com/)
- [EAI猩球](https://www.robotech.ink/)
- [文举的博客](https://liwenju0.com/)
- [申请链接](https://kexue.fm/links.html)

[![署名-非商业用途-保持一致](https://kexue.fm/usr/themes/geekg/images/cc.gif)](http://creativecommons.org/licenses/by-nc-nd/2.5/cn/) 本站采用创作共用版权协议，要求署名、非商业用途和保持一致。转载本站内容必须也遵循“ [署名-非商业用途-保持一致](http://creativecommons.org/licenses/by-nc-nd/2.5/cn/)”的创作共用协议。



© 2009-2026 Scientific Spaces. All rights reserved. Theme by [laogui](http://www.laogui.com/). Powered by [Typecho](http://typecho.org/). 备案号: [粤ICP备09093259号-1/2](https://beian.miit.gov.cn/ "粤ICP备09093259号")。