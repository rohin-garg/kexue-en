## SEARCH

## MENU

- [打赏](https://kexue.fm/reward.html)
- [公式](https://kexue.fm/latex.html)
- [天象](https://kexue.fm/ac.html)
- [链接](https://kexue.fm/links.html)
- [时光](https://kexue.fm/me.html)
- [博览](https://kexue.fm/science.html)
- [归档](https://kexue.fm/content.html)

## CATEGORIES

- [千奇百怪](https://kexue.fm/category/Everything)
- [天文探索](https://kexue.fm/category/Astronomy)
- [数学研究](https://kexue.fm/category/Mathematics)
- [物理化学](https://kexue.fm/category/Phy-chem)
- [信息时代](https://kexue.fm/category/Big-Data)
- [生物自然](https://kexue.fm/category/Biology)
- [图片摄影](https://kexue.fm/category/Photograph)
- [问题百科](https://kexue.fm/category/Questions)
- [生活/情感](https://kexue.fm/category/Life-Feeling)
- [资源共享](https://kexue.fm/category/Resources)

## NEWPOSTS

- [Muon优化器指南：快速上手与关键细节](https://kexue.fm/archives/11416)
- [AdamW的Weight RMS的...](https://kexue.fm/archives/11404)
- [n个正态随机数的最大值的渐近估计](https://kexue.fm/archives/11390)
- [流形上的最速下降：5\. 对偶梯度下降](https://kexue.fm/archives/11388)
- [低精度Attention可能存在有...](https://kexue.fm/archives/11371)
- [MuP之上：1. 好模型的三个特征](https://kexue.fm/archives/11340)
- [随机矩阵的谱范数的快速估计](https://kexue.fm/archives/11335)
- [DiVeQ：一种非常简洁的VQ训练方案](https://kexue.fm/archives/11328)
- [为什么线性注意力要加Short C...](https://kexue.fm/archives/11320)
- [AdamW的Weight RMS的...](https://kexue.fm/archives/11307)

## COMMENTS

- [lcyedd: 如果n个独立进程耗时服从正态分布，整个系统的总耗时就可以这样估...](https://kexue.fm/archives/11390/comment-page-1#comment-28856)
- [苏剑林: 总结一下几点经验，供参考。1、用Muon的理由可以很多，比如追...](https://kexue.fm/archives/11416/comment-page-1#comment-28855)
- [苏剑林: 我自己没特别关注这种物理景观特别明显的研究，或者说这类研究真正...](https://kexue.fm/archives/9305/comment-page-1#comment-28854)
- [苏剑林: 你可以理解为给这个loss多乘以了一项标量权重，它不改变这个l...](https://kexue.fm/archives/9209/comment-page-7#comment-28853)
- [苏剑林: 如果是$t\\to\\infty$，那么理论上是这样子；但如果$t...](https://kexue.fm/archives/11307/comment-page-1#comment-28852)
- [BaoLi: 返回去又看了博主相关论文。如果我理解没错的话，考虑两种情形：1...](https://kexue.fm/archives/11416/comment-page-1#comment-28851)
- [z: 可以省显存，加速收敛，提高训练效率啊](https://kexue.fm/archives/11416/comment-page-1#comment-28850)
- [BaoLi: 感谢大佬分享，我有几个问题：1，看了一遍以后，感觉整个流程比起...](https://kexue.fm/archives/11416/comment-page-1#comment-28849)
- [Dhuzi: 苏神，能多发点这种物理和机器学习结合的研究内容吗？或者您觉得机...](https://kexue.fm/archives/9305/comment-page-1#comment-28848)
- [小白一枚: 苏老师，（14）到（15）你给的解释是直接删去分母，确实对（1...](https://kexue.fm/archives/9209/comment-page-7#comment-28847)

## USERLOGIN

- [登录](https://kexue.fm/admin/login.php)

[科学空间\|Scientific Spaces](https://kexue.fm)

- [登录](https://kexue.fm/admin/login.php)
- [打赏](https://kexue.fm/reward.html)
- [公式](https://kexue.fm/latex.html)
- [天象](https://kexue.fm/ac.html)
- [链接](https://kexue.fm/links.html)
- [时光](https://kexue.fm/me.html)
- [博览](https://kexue.fm/science.html)
- [归档](https://kexue.fm/content.html)

渴望成为一个小飞侠

- [欢迎订阅](https://kexue.fm/feed)
- [个性邮箱](https://kexue.fm/archives/119)
- [天象信息](https://kexue.fm/ac.html)
- [观测ISS](https://kexue.fm/archives/41)
- [LaTeX](https://kexue.fm/latex.html)
- [关于博主](https://kexue.fm/me.html)

欢迎访问“科学空间”，这里将与您共同探讨自然科学，回味人生百态；也期待大家的分享～

- [**千奇百怪** Everything](https://kexue.fm/category/Everything)
- [**天文探索** Astronomy](https://kexue.fm/category/Astronomy)
- [**数学研究** Mathematics](https://kexue.fm/category/Mathematics)
- [**物理化学** Phy-chem](https://kexue.fm/category/Phy-chem)
- [**信息时代** Big-Data](https://kexue.fm/category/Big-Data)
- [**生物自然** Biology](https://kexue.fm/category/Biology)
- [**图片摄影** Photograph](https://kexue.fm/category/Photograph)
- [**问题百科** Questions](https://kexue.fm/category/Questions)
- [**生活/情感** Life-Feeling](https://kexue.fm/category/Life-Feeling)
- [**资源共享** Resources](https://kexue.fm/category/Resources)

- [**千奇百怪**](https://kexue.fm/category/Everything)
- [**天文探索**](https://kexue.fm/category/Astronomy)
- [**数学研究**](https://kexue.fm/category/Mathematics)
- [**物理化学**](https://kexue.fm/category/Phy-chem)
- [**信息时代**](https://kexue.fm/category/Big-Data)
- [**生物自然**](https://kexue.fm/category/Biology)
- [**图片摄影**](https://kexue.fm/category/Photograph)
- [**问题百科**](https://kexue.fm/category/Questions)
- [**生活/情感**](https://kexue.fm/category/Life-Feeling)
- [**资源共享**](https://kexue.fm/category/Resources)

[首页](https://kexue.fm) [数学研究](https://kexue.fm/category/Mathematics) [信息时代](https://kexue.fm/category/Big-Data) Muon续集：为什么我们选择尝试Muon？

27Feb

# [Muon续集：为什么我们选择尝试Muon？](https://kexue.fm/archives/10739)

By 苏剑林 \|
2025-02-27 \|
83115位读者\|

本文解读一下我们最新的技术报告 [《Muon is Scalable for LLM Training》](https://papers.cool/arxiv/2502.16982)，里边分享了我们之前在 [《Muon优化器赏析：从向量到矩阵的本质跨越》](https://kexue.fm/archives/10592) 介绍过的Muon优化器的一次较大规模的实践，并开源了相应的模型（我们称之为“ [Moonlight](https://github.com/MoonshotAI/Moonlight)”，目前是一个3B/16B的MoE模型）。我们发现了一个比较惊人的结论：在我们的实验设置下，Muon相比Adam能够达到将近2倍的训练效率。

Muon的Scaling Law及Moonlight的MMLU表现

优化器的工作说多不多，但说少也不少，为什么我们会选择Muon来作为新的尝试方向呢？已经调好超参的Adam优化器，怎么快速切换到Muon上进行尝试呢？模型Scale上去之后，Muon与Adam的性能效果差异如何？接下来将分享我们的思考过程。

## 优化原理 [\#](https://kexue.fm/kexue.fm\#%E4%BC%98%E5%8C%96%E5%8E%9F%E7%90%86)

关于优化器，其实笔者之前在 [《Muon优化器赏析：从向量到矩阵的本质跨越》](https://kexue.fm/archives/10592) 时就有过浅评，多数优化器改进实际上都只是一些小补丁，不能说毫无价值，但终究没能给人一种深刻和惊艳的感觉。

我们需要从更贴近本质的原理出发，来思考什么才是好的优化器。直观来想，理想的优化器应当有两个特性： **稳** 和 **快**。具体来说，理想的优化器每一步的更新应该满足两点：1、对模型扰动尽可能小；2、对Loss贡献尽可能大。说更直接点，就是我们不希望大改模型（稳），但希望大降Loss（快），典型的“既要...又要...”。

怎么将这两个特性转化为数学语言呢？ **稳** 我们可以理解为对更新量的一个约束，而 **快** 则可以理解为寻找让损失函数下降最快的更新量，所以这可以转化为一个约束优化问题。用回前文的记号，对于矩阵参数$\\boldsymbol{W}\\in\\mathbb{R}^{n\\times m}$，其梯度为$\\boldsymbol{G}\\in\\mathbb{R}^{n\\times m}$，当参数由$\\boldsymbol{W}$变成$\\boldsymbol{W}+\\Delta\\boldsymbol{W}$时，损失函数的变化量为
\\begin{equation}\\text{Tr}(\\boldsymbol{G}^{\\top}\\Delta\\boldsymbol{W})\\end{equation}
那么在 **稳** 的前提下寻找 **快** 的更新量，那么就可以表示为
\\begin{equation}\\mathop{\\text{argmin}}\_{\\Delta\\boldsymbol{W}}\\text{Tr}(\\boldsymbol{G}^{\\top}\\Delta\\boldsymbol{W})\\quad\\text{s.t.}\\quad \\rho(\\Delta\\boldsymbol{W})\\leq \\eta\\label{eq:least-action}\\end{equation}
这里$\\rho(\\Delta\\boldsymbol{W})\\geq 0$是 **稳** 的某个指标，越小表示越稳，而$\\eta$是某个小于1的常数，表示我们对 **稳** 的要求，后面我们会看到它实际上就是优化器的学习率。如果读者不介意，我们可以模仿理论物理的概念，称上述原理为优化器的“ **最小作用量原理（Least Action Principle）**”。

## 矩阵范数 [\#](https://kexue.fm/kexue.fm\#%E7%9F%A9%E9%98%B5%E8%8C%83%E6%95%B0)

式$\\eqref{eq:least-action}$唯一不确定的就是 **稳** 的度量$\\rho(\\Delta\\boldsymbol{W})$，选定$\\rho(\\Delta\\boldsymbol{W})$后就可以把$\\Delta\\boldsymbol{W}$明确求解出来（至少理论上没问题）。某种程度上来说，我们可以认为不同优化器的本质差异就是它们对 **稳** 的定义不一样。

很多读者在刚学习SGD时想必都看到过类似“梯度反方向是函数值局部下降最快的方向”的说法，放到这里的框架来看，它其实就是把 **稳** 的度量选择为矩阵的$F$范数$\\Vert\\Delta\\boldsymbol{W}\\Vert\_F$，也就是说，“下降最快的方向”并不是一成不变的，选定度量后才能把它确定下来，换一个范数就不一定是梯度反方向了。

接下来的问题自然是什么范数才能最恰当地度量 **稳**？如果我们加了强约束，那么稳则稳矣，但优化器举步维艰，只能收敛到次优解；相反如果减弱约束，那么优化器放飞自我，那么训练进程将会极度不可控。所以，最理想的情况就能找到 **稳** 的最精准指标。考虑到神经网络以矩阵乘法为主，我们以$\\boldsymbol{y}=\\boldsymbol{x}\\boldsymbol{W}$为例，有
\\begin{equation}\\Vert\\Delta \\boldsymbol{y}\\Vert = \\Vert\\boldsymbol{x}(\\boldsymbol{W} + \\Delta\\boldsymbol{W}) - \\boldsymbol{x}\\boldsymbol{W}\\Vert = \\Vert\\boldsymbol{x} \\Delta\\boldsymbol{W}\\Vert\\leq \\rho(\\Delta\\boldsymbol{W}) \\Vert\\boldsymbol{x}\\Vert\\end{equation}
上式的意思是，当参数由$\\boldsymbol{W}$变成$\\boldsymbol{W}+\\Delta\\boldsymbol{W}$时，模型输出变化量为$\\Delta\\boldsymbol{y}$，我们寄望于这个变化量的模长能够被$\\Vert\\boldsymbol{x}\\Vert$以及$\\Delta\\boldsymbol{W}$相关的一个函数$\\rho(\\Delta\\boldsymbol{W})$控制，我们就用这个函数作为 **稳** 的指标。从线性代数我们知道，$\\rho(\\Delta\\boldsymbol{W})$的最准确值就是$\\Delta\\boldsymbol{W}$的谱范数$\\Vert\\Delta\\boldsymbol{W}\\Vert\_2$，代入式$\\eqref{eq:least-action}$得到
\\begin{equation}\\mathop{\\text{argmin}}\_{\\Delta\\boldsymbol{W}}\\text{Tr}(\\boldsymbol{G}^{\\top}\\Delta\\boldsymbol{W})\\quad\\text{s.t.}\\quad \\Vert\\Delta\\boldsymbol{W}\\Vert\_2\\leq \\eta\\end{equation}
这个优化问题求解出来就是$\\beta=0$的Muon：
\\begin{equation}\\Delta\\boldsymbol{W} = -\\eta\\, \\text{msign}(\\boldsymbol{G}) = -\\eta\\,\\boldsymbol{U}\_{\[:,:r\]}\\boldsymbol{V}\_{\[:,:r\]}^{\\top}, \\quad \\boldsymbol{U},\\boldsymbol{\\Sigma},\\boldsymbol{V}^{\\top} = \\mathop{\\text{SVD}}(\\boldsymbol{G})\\end{equation}
当$\\beta > 0$时，$\\boldsymbol{G}$换成动量$\\boldsymbol{M}$，$\\boldsymbol{M}$可以看作是对梯度更平滑的估计，所以依然可以理解为上式的结论，因此我们可以得出“Muon就是谱范数下的最速下降”的说法，至于Newton-schulz迭代之类的，则是计算上的近似，这里就不细说。详细推导我们在 [《Muon优化器赏析：从向量到矩阵的本质跨越》](https://kexue.fm/archives/10592) 已经给出，也不再重复。

## 权重衰减 [\#](https://kexue.fm/kexue.fm\#%E6%9D%83%E9%87%8D%E8%A1%B0%E5%87%8F)

至此，我们可以回答第一个问题：为什么选择尝试Muon？因为跟SGD一样，Muon给出的同样是下降最快的方向，但它的谱范数约束比SGD的$F$范数更为精准，所以有更佳的潜力。另一方面，从“为不同的参数选择最恰当的约束”角度来改进优化器，看起来也比各种补丁式修改更为本质。

当然，潜力不意味着实力，在更大尺寸的模型上验证Muon存在一些“陷阱”。首先登场的是Weight Decay问题，尽管我们在 [《Muon优化器赏析：从向量到矩阵的本质跨越》](https://kexue.fm/archives/10592) 介绍Muon的时候带上了Weight Decay，但实际上作者提出Muon时是没有的，而我们一开始也按照官方版本来实现，结果发现Muon前期收敛是快，但很快就被Adam追上了，而且各种“内科”还有崩溃的苗头。

我们很快意识到这可能是Weight Decay的问题，于是补上Weight Decay：
\\begin{equation}\\Delta\\boldsymbol{W} = -\\eta\\, \[\\text{msign}(\\boldsymbol{M})+ \\lambda \\boldsymbol{W}\]\\end{equation}
继续实验，果不其然，这时候Muon一直保持领先于Adam，如论文Figure 2所示：

有无Weight Decay的效果比较

Weight Decay起到什么作用呢？事后分析来看，可能比较关键的地方是能够让参数范数保持有界：
\\begin{equation}\\begin{aligned}
\\Vert\\boldsymbol{W}\_t\\Vert =&\\, \\Vert\\boldsymbol{W}\_{t-1} - \\eta\_t (\\boldsymbol{\\Phi}\_t + \\lambda \\boldsymbol{W}\_{t-1})\\Vert \\\\[5pt\]
=&\\, \\Vert(1 - \\eta\_t \\lambda)\\boldsymbol{W}\_{t-1} - \\eta\_t \\lambda (\\boldsymbol{\\Phi}\_t/\\lambda)\\Vert \\\\[5pt\]
\\leq &\\,(1 - \\eta\_t \\lambda)\\Vert\\boldsymbol{W}\_{t-1}\\Vert + \\eta\_t \\lambda \\Vert\\boldsymbol{\\Phi}\_t/\\lambda\\Vert \\\\[5pt\]
\\leq &\\,\\max(\\Vert\\boldsymbol{W}\_{t-1}\\Vert,\\Vert\\boldsymbol{\\Phi}\_t/\\lambda\\Vert) \\\\[5pt\]
\\end{aligned}\\end{equation}
这里的$\\Vert\\cdot\\Vert$是任意一种矩阵范数，即上述不等式对于任意矩阵范数都是成立的，$\\boldsymbol{\\Phi}\_t$是优化器给出的更新向量，对Muon来说是$\\text{msign}(\\boldsymbol{M})$，当我们取谱范数时，有$\\Vert\\text{msign}(\\boldsymbol{M})\\Vert\_2 = 1$，所以对于Muon来说有
\\begin{equation}
\\Vert\\boldsymbol{W}\_t\\Vert\_2 \\leq \\max(\\Vert\\boldsymbol{W}\_{t-1}\\Vert\_2,1/\\lambda)\\leq\\cdots \\leq \\max(\\Vert\\boldsymbol{W}\_0\\Vert\_2,1/\\lambda)\\end{equation}
这保证了模型“内科”的健康，因为$\\Vert\\boldsymbol{x}\\boldsymbol{W}\\Vert\\leq \\Vert\\boldsymbol{x}\\Vert\\Vert\\boldsymbol{W}\\Vert\_2$，$\\Vert\\boldsymbol{W}\\Vert\_2$被控制住了，意味着$\\Vert\\boldsymbol{x}\\boldsymbol{W}\\Vert$也被控制住了，就不会有爆炸的风险，这对于Attention Logits爆炸等问题也尤其重要。当然这个上界在多数情况下还是相当宽松的，实际中参数的谱范数多数会明显小于这个上界，这个不等关系只是简单显示Weight Decay有控制范数的性质存在。

## RMS对齐 [\#](https://kexue.fm/kexue.fm\#RMS%E5%AF%B9%E9%BD%90)

当我们决定去尝试新的优化器时，一个比较头疼的问题是如何快速找到接近最优的超参数，比如Muon至少有学习率$\\eta\_t$和衰减率$\\lambda$两个超参数。网格搜索自然是可以，但比较费时费力，这里我们提出Update RMS对齐的超参迁移思路，可以将Adam调好的超参数用到其他优化器上。

首先，对于一个矩阵$\\boldsymbol{W}\\in\\mathbb{R}^{n\\times m}$，它的RMS（Root Mean Square）定义为
\\begin{equation}\\text{RMS}(\\boldsymbol{W}) = \\frac{\\Vert \\boldsymbol{W}\\Vert\_F}{\\sqrt{nm}} = \\sqrt{\\frac{1}{nm}\\sum\_{i=1}^n\\sum\_{j=1}^m W\_{i,j}^2}\\end{equation}
简单来说，RMS度量了矩阵每个元素的平均大小。我们观察到Adam更新量的RMS是比较稳定的，通常在0.2～0.4之间，这也是为什么 [理论分析](https://kexue.fm/archives/10542) 常用SignSGD作为Adam近似。基于此，我们建议通过RMS Norm将新优化器的Update RMS对齐到0.2：
\\begin{gather}
\\boldsymbol{W}\_t =\\boldsymbol{W}\_{t-1} - \\eta\_t (\\boldsymbol{\\Phi}\_t + \\lambda \\boldsymbol{W}\_{t-1}) \\\\[6pt\]
\\downarrow \\notag\\\\[6pt\]
\\boldsymbol{W}\_t = \\boldsymbol{W}\_{t-1} - \\eta\_t (0.2\\, \\boldsymbol{\\Phi}\_t/\\text{RMS}(\\boldsymbol{\\Phi}\_t) + \\lambda \\boldsymbol{W}\_{t-1})
\\end{gather}
这样一来，我们就可以复用Adam的$\\eta\_t$和$\\lambda$，以达到每步对参数的更新幅度大致相同的效果。实践表明，通过这个简单策略从Adam迁移到Muon，就能训出明显优于Adam的效果，接近进一步对Muon超参进行精搜索的结果。特别地，Muon的$\\text{RMS}(\\boldsymbol{\\Phi}\_t)=\\text{RMS}(\\boldsymbol{U}\_{\[:,:r\]}\\boldsymbol{V}\_{\[:,:r\]}^{\\top})$还可以解析地算出来：
\\begin{equation}nm\\,\\text{RMS}(\\boldsymbol{\\Phi}\_t)^2 = \\sum\_{i=1}^n\\sum\_{j=1}^m \\sum\_{k=1}^r U\_{i,k}^2V\_{k,j}^2 = \\sum\_{k=1}^r\\left(\\sum\_{i=1}^n U\_{i,k}^2\\right)\\left(\\sum\_{j=1}^m V\_{k,j}^2\\right) = \\sum\_{k=1}^r 1 = r\\end{equation}
即$\\text{RMS}(\\boldsymbol{\\Phi}\_t) = \\sqrt{r/nm}$，实践中一个矩阵严格低秩的概率比较小，因此可以认为$r = \\min(n,m)$，从而有$\\text{RMS}(\\boldsymbol{\\Phi}\_t) = \\sqrt{1/\\max(n,m)}$。所以我们最终没有用RMS Norm而是用等价的解析版本：
\\begin{equation}\\boldsymbol{W}\_t = \\boldsymbol{W}\_{t-1} - \\eta\_t (0.2\\, \\boldsymbol{\\Phi}\_t\\,\\sqrt{\\max(n,m)} + \\lambda \\boldsymbol{W}\_{t-1})\\end{equation}
最后的这个式子，表明了在Muon中不适宜所有参数使用同一个学习率。比如Moonlight是一个MoE模型，有不少矩阵参数的形状都偏离方阵，$\\max(n,m)$跨度比较大，如果用单一学习率，必然会导致某些参数学习过快/过慢的不同步问题，从而影响最终效果。

## 实验分析 [\#](https://kexue.fm/kexue.fm\#%E5%AE%9E%E9%AA%8C%E5%88%86%E6%9E%90)

我们在2.4B/16B这个尺寸的MoE上，对Adam和Muon做了比较充分的对比，发现Muon在收敛速度和最终效果上都有明显的优势。详细的比较结果建议大家去看原论文，这里仅截取部分分享。

> **Github: [https://github.com/MoonshotAI/Moonlight](https://github.com/MoonshotAI/Moonlight)**

首先是一个相对客观的对照表格，包括我们自己控制变量训练的Muon和Adam的对比，以及与外界（DeepSeek）用Adam训练的同样架构的模型对比（为了便于对比，Moonlight的架构跟DSV3-Small完全一致），显示出Muon的独特优势：

Muon（Moonlight） vs Adam（Moonlight-A 和DSV3-small）的比较

Muon训练出来的模型有什么不同呢？既然前面我们说Muon是谱范数下的最速下降，谱范数是最大的奇异值，所以我们想到了监控和分析奇异值。果然，我们发现了一些有趣的信号，Muon训出来的参数，奇异值分布相对更均匀一些，我们使用奇异值熵来定量描述这个现象：
\\begin{equation}H(\\boldsymbol{\\sigma}) = -\\frac{1}{\\log n}\\sum\_{i=1}^n \\frac{\\sigma\_i^2}{\\sum\_{j=1}^n\\sigma\_j^2}\\log \\frac{\\sigma\_i^2}{\\sum\_{j=1}^n\\sigma\_j^2}\\end{equation}
这里$\\boldsymbol{\\sigma}=(\\sigma\_1,\\sigma\_2,\\cdots,\\sigma\_n)$是某个参数的全体奇异值。Muon训出来的参数熵更大，即奇异值分布更均匀，意味着这个参数越不容易压缩，这说明Muon更充分发挥了参数的潜能：

Muon训练出来的权重奇异值熵更高

还有一个有趣的发现是当我们将Muon用于微调（SFT）时，可能会因为预训练没用Muon而得到次优解。具体来说，如果预训练和微调都用Muon，那么表现是最好的，但如果是另外三种组合（Adam+Muon、Muon+Adam、Adam+Adam），其效果优劣没有呈现于明显的规律。

预训练/微调分别用Muon/Adam的组合测试

在开源模型上用Muon/Adam微调的尝试

这个现象表明存在一些特殊的初始化对Muon不利，当然反过来也可能存在一些初始化对Muon更有利，更底层的原理我们还在探索中。

## 拓展思考 [\#](https://kexue.fm/kexue.fm\#%E6%8B%93%E5%B1%95%E6%80%9D%E8%80%83)

总的来说，在我们的实验里，Muon的表现跟Adam相比显得非常有竞争力。作为一个形式上跟Adam差异较大的新优化器，Muon的这个表现其实不单单是“可圈可点”了，还表明它可能捕捉到了一些本质的特性。

此前，社区流传着一个观点：Adam之所以表现好，是因为主流的模型架构改进都在“过拟合”Adam。这个观点最早应该出自 [《Neural Networks (Maybe) Evolved to Make Adam The Best Optimizer》](https://parameterfree.com/2020/12/06/neural-network-maybe-evolved-to-make-adam-the-best-optimizer/)，看上去有点荒谬，但实际上意蕴深长。试想一下，当我们尝试改进模型后，就会拿Adam训一遍看效果，效果好就保留，否则放弃。可这个效果好，究竟是因为它本质更佳，还是因为它更匹配Adam了呢？

这就有点耐人寻味了。当然不说全部，肯定至少有一部份工作，是因为它跟Adam更配而表现出更好的效果，所以久而久之，模型架构就会朝着一个有利于Adam的方向演进。在这个背景下，一个跟Adam显著不同的优化器还能“出圈”，就尤其值得关注和思考了。注意笔者和所在公司都不属Muon提出者，所以这番言论纯属“肺腑之言”，并不存在自卖自夸的意思。

接下来Muon还有什么工作可做呢？其实应该还有不少。比如上面提到的“Adam预训练+Muon微调”效果不佳问题，进一步分析还是有必要和有价值的，毕竟现在大家开源的模型权重基本都是Adam训的，如果Muon微调不行，必然也影响它的普及。当然了，我们还可以借着这个契机进一步深化对Muon理解（面向Bug学习）。

还有一个推广的思考，就是Muon基于谱范数，谱范数是最大奇异值，事实上基于奇异值我们还可以构造一系列范数，如 [Schatten范数](https://en.wikipedia.org/wiki/Schatten_norm)，将它推广到这种更广义的范数然后进行调参，理论上还有机会取得更好的效果。此外，Moonlight发布后，还有一些读者问到Muon下 [µP（maximal update parametrization）](https://papers.cool/arxiv/2203.03466) 如何设计，这也是一个亟待解决的问题。

## 文章小结 [\#](https://kexue.fm/kexue.fm\#%E6%96%87%E7%AB%A0%E5%B0%8F%E7%BB%93)

本文介绍了我们在Muon优化器上的一次较大规模实践（Moonlight），并分享了我们对Muon优化器的最新思考。

_**转载到请包括本文地址：** [https://kexue.fm/archives/10739](https://kexue.fm/archives/10739)_

_**更详细的转载事宜请参考：**_ [《科学空间FAQ》](https://kexue.fm/archives/6508#%E6%96%87%E7%AB%A0%E5%A6%82%E4%BD%95%E8%BD%AC%E8%BD%BD/%E5%BC%95%E7%94%A8)

**如果您还有什么疑惑或建议，欢迎在下方评论区继续讨论。**

**如果您觉得本文还不错，欢迎 [分享](https://kexue.fm/kexue.fm#share)/ [打赏](https://kexue.fm/kexue.fm#pay) 本文。打赏并非要从中获得收益，而是希望知道科学空间获得了多少读者的真心关注。当然，如果你无视它，也不会影响你的阅读。再次表示欢迎和感谢！**

打赏

微信打赏

支付宝打赏

因为网站后台对打赏并无记录，因此欢迎在打赏时候备注留言。你还可以 [**点击这里**](http://mail.qq.com/cgi-bin/qm_share?t=qm_mailme&email=tN7d1drY3drrx8H0xcWa19vZ) 或在下方评论区留言来告知你的建议或需求。

**如果您需要引用本文，请参考：**

苏剑林. (Feb. 27, 2025). 《Muon续集：为什么我们选择尝试Muon？ 》\[Blog post\]. Retrieved from [https://kexue.fm/archives/10739](https://kexue.fm/archives/10739)

@online{kexuefm-10739,
        title={Muon续集：为什么我们选择尝试Muon？},
        author={苏剑林},
        year={2025},
        month={Feb},
        url={\\url{https://kexue.fm/archives/10739}},
}

分类： [数学研究](https://kexue.fm/category/Mathematics), [信息时代](https://kexue.fm/category/Big-Data)    标签： [矩阵](https://kexue.fm/tag/%E7%9F%A9%E9%98%B5/), [梯度](https://kexue.fm/tag/%E6%A2%AF%E5%BA%A6/), [优化器](https://kexue.fm/tag/%E4%BC%98%E5%8C%96%E5%99%A8/), [谱范数](https://kexue.fm/tag/%E8%B0%B1%E8%8C%83%E6%95%B0/), [muon](https://kexue.fm/tag/muon/)[53 评论](https://kexue.fm/archives/10739#comments)

< [MoE环游记：2、不患寡而患不均](https://kexue.fm/archives/10735) \| [MoE环游记：3、换个思路来分配](https://kexue.fm/archives/10757) >

### 你也许还对下面的内容感兴趣

- [Muon优化器指南：快速上手与关键细节](https://kexue.fm/archives/11416)
- [AdamW的Weight RMS的渐近估计（下）](https://kexue.fm/archives/11404)
- [流形上的最速下降：5\. 对偶梯度下降](https://kexue.fm/archives/11388)
- [MuP之上：1. 好模型的三个特征](https://kexue.fm/archives/11340)
- [随机矩阵的谱范数的快速估计](https://kexue.fm/archives/11335)
- [DiVeQ：一种非常简洁的VQ训练方案](https://kexue.fm/archives/11328)
- [AdamW的Weight RMS的渐近估计（上）](https://kexue.fm/archives/11307)
- [重新思考学习率与Batch Size（四）：EMA](https://kexue.fm/archives/11301)
- [重新思考学习率与Batch Size（三）：Muon](https://kexue.fm/archives/11285)
- [重新思考学习率与Batch Size（二）：平均场](https://kexue.fm/archives/11280)

[发表你的看法](https://kexue.fm/kexue.fm#comment_form)

1. [«](https://kexue.fm/archives/10739/comment-page-1#comments)
2. [1](https://kexue.fm/archives/10739/comment-page-1#comments)
3. [2](https://kexue.fm/archives/10739/comment-page-2#comments)

bdy

March 23rd, 2025

苏神好，想问一下Muons是不是没法对对称矩阵进行优化呢，因为$UV^{\\top}=I$。这样看起来好像对称矩阵只有“长度”信息，即特征值；但是并没有“方向”信息，即$UV^{\\top}$。

[回复评论](https://kexue.fm/archives/10739/comment-page-2?replyTo=27211#respond-post-10739)

[苏剑林](https://kexue.fm) 发表于
March 23rd, 2025

这个对称矩阵怎么定义呢？是一个矩阵初始化为对称矩阵？那它的梯度一定也是对称矩阵吗？似乎不是。

[回复评论](https://kexue.fm/archives/10739/comment-page-2?replyTo=27234#respond-post-10739)

xwl

April 13th, 2025

苏神好，想问一下基于目前的实践证据和趋势，将AdamW迁移到Muon还存在什么（可能的）阻力和坏处？

例如ImageNet上训练DiT/SiT等生成模型，往往需要800+epochs的训练量，体现出生成模型较判别模型在training sample efficiency上的明显差距，看起来和用更少的tokens训练LLM是相似的场景和问题。

那么假设训练DiT/SiT的常见已有实践是 AdamW(lr=2e-4, bs=1024, betas=(0.9, 0.95), weightdecay=0) 这种量级的超参数，那把它迁移到Muon上可能存在什么问题？谢谢！

[回复评论](https://kexue.fm/archives/10739/comment-page-2?replyTo=27356#respond-post-10739)

[苏剑林](https://kexue.fm) 发表于
April 13th, 2025

主要瓶颈可能是小规模训练Muon的运行速度可能会比Adam明显慢一点？超参数的话，利用我们的对齐update rms技巧，即本文的$(13)$，那基本可以照搬Adam的超参数，这是我们的主要贡献之一。

[回复评论](https://kexue.fm/archives/10739/comment-page-2?replyTo=27376#respond-post-10739)

pki 发表于
November 15th, 2025

苏老师您好，weightdecay=0 也不需要重新调整吗

[回复评论](https://kexue.fm/archives/10739/comment-page-2?replyTo=28815#respond-post-10739)

[苏剑林](https://kexue.fm) 发表于
November 18th, 2025

不需要，对齐update\_rms就行

[回复评论](https://kexue.fm/archives/10739/comment-page-2?replyTo=28840#respond-post-10739)

[Truenobility303](http://truenobility303.github.io)

June 27th, 2025

苏神好，关于RMS对齐这里有些疑问。1. 如果从 A Spectral Condition for Feature Learning 一文的角度（我也读了您介绍的博客），应该对齐的是spectral norm而非F norm, 这也符合Muon的初衷。但是RMS对齐是从F norm对齐导出的，而我认为如果对齐spectral norm会导致不同的学习率设定。 2. 假设m=n=d, 这里根据RMS对齐原则直接复用Adam的学习率有lr\_Muon = sqrt{d} lr\_Adam ，但是这与利用muP导出的结果并不一致，如果按照muP的原则应该是lr\_Muon = d lr\_Adam (您在自己的博客里也推出了这个）。请问上述两点应该如何理解呢？

[回复评论](https://kexue.fm/archives/10739/comment-page-2?replyTo=28016#respond-post-10739)

[苏剑林](https://kexue.fm) 发表于
July 2nd, 2025

你说的没有问题，对齐update rms是一种快速将Adam超参搬过来给Muon用，争取快速复现出不差于Adam结果的方案，它并不一定是最优的，也不符合muP。

从实验结果来看，也表明并非不符合muP就不能用了，只不过对于这种格式的Muon，需要另外探索超参数迁移规律，而不是直接套用muP的形式。

当然，我们也在尝试直接从Spectral Condition的形式出发来重新搜索Muon的超参数，这部分如果有新结果我们会及时共享。

[回复评论](https://kexue.fm/archives/10739/comment-page-2?replyTo=28033#respond-post-10739)

[Truenobility303](http://truenobility303.github.io) 发表于
July 8th, 2025

谢谢苏神的详细解答！

[回复评论](https://kexue.fm/archives/10739/comment-page-2?replyTo=28059#respond-post-10739)

[曲笑一](https://xiaoyi-qu.github.io/)

July 4th, 2025

苏老师您好，阅读了您关于Muon系列的博客，受益匪浅。在此有两个疑问想请教您：第一个问题是，Muon在进行分布式训练时是否会遇到困难？因为常见的Adam是按元素计算的，而Muon是按矩阵操作的，似乎在模型并行或数据并行时可能存在额外的通信或同步开销。第二个问题是，像归一化层的参数（例如BatchNorm或LayerNorm中的缩放与偏移）不是矩阵形式，那这些参数是否不适合使用Muon进行训练呢？

[回复评论](https://kexue.fm/archives/10739/comment-page-2?replyTo=28050#respond-post-10739)

曲笑一 发表于
July 4th, 2025

对于第一个疑问，我看到分布式的版本已经开源。我在想如果将每个梯度矩阵G拆分为N\*N,再利用muon这一套流程，得到一个个小的分块矩阵，之后进行分布式训练是否会更高效一些？对于第二个疑问，是不是可以对这类参数做reshape的操作将其转化成方块矩阵，这样就可以全部使用muon进行训练

[回复评论](https://kexue.fm/archives/10739/comment-page-2?replyTo=28051#respond-post-10739)

[苏剑林](https://kexue.fm) 发表于
July 11th, 2025

分布式训练肯定会有额外通信和计算成本，但相比获取gradient的时间，optimizer更新的时间其实是比较短的，而且模型越大、卡越多，optimizer的时间占比就越少，在优化实现之后，Muon相比Adam的时间增加几乎是无感的。

对于非矩阵参数，第一篇文章 [https://kexue.fm/archives/10592](https://kexue.fm/archives/10592) 专门讨论过，可以参考一下。

[回复评论](https://kexue.fm/archives/10739/comment-page-2?replyTo=28072#respond-post-10739)

ChengQinyuan

July 13th, 2025

苏神好，参数矩阵的奇异值分布比较均匀是否意味着本征维度可能较大，进而导致LoRA等依赖低秩特性的PEFT算法效果不显著呢

[回复评论](https://kexue.fm/archives/10739/comment-page-2?replyTo=28100#respond-post-10739)

[苏剑林](https://kexue.fm) 发表于
July 14th, 2025

我猜测是这样。这甚至也可以用来解释“为什么Muon训练的模型，用Adam去SFT效果不大行”，因为Muon训出来的权重有效秩更高，而Adam的Update通常是Low-rank的，这样一升一降之下，优化效率就变差了。

类似地，Adam训练的模型，Muon去SFT效果也不一定最优，因为Adam训练出来的权重，已经进入了有效秩低的局部最优点，而Muon则会将它强行拔高，可能也会影响优化效率。

[回复评论](https://kexue.fm/archives/10739/comment-page-2?replyTo=28109#respond-post-10739)

CTR-RTC

July 20th, 2025

请问关于Adam的Update RMS稳定性有相关的数理分析吗?

[回复评论](https://kexue.fm/archives/10739/comment-page-2?replyTo=28200#respond-post-10739)

[苏剑林](https://kexue.fm) 发表于
July 20th, 2025

Adam一般认为跟SignSGD是同阶的，在理论分析中经常用SignSGD作为Adam的近似（比如 [https://kexue.fm/archives/10542#%E8%87%AA%E9%80%82%E5%BA%94%E7%89%88](https://kexue.fm/archives/10542#%E8%87%AA%E9%80%82%E5%BA%94%E7%89%88) ），而SignSGD的Update RMS恒为1

[回复评论](https://kexue.fm/archives/10739/comment-page-2?replyTo=28206#respond-post-10739)

JueningJin

August 11th, 2025

请问在优化MoE中router的weight时是用Muon还是AdamW呢？Muon的作者提到说embedding matrix要用AdamW，原因是embedding的输入是one hot的。那么sparse MoE的router的输出也是sparse的，是否意味着对应的router的weight也需要用AdamW呢

[回复评论](https://kexue.fm/archives/10739/comment-page-2?replyTo=28355#respond-post-10739)

[苏剑林](https://kexue.fm) 发表于
August 12th, 2025

router还是用muon。严格来讲sparse形式结论确实会跟muon有少许不同，但因为router通常是n选k(k > 1)而不是n选1，所以muon还是一个比较好的近似，因此用muon就行。后面有可能，我们可能会专门讨论这个问题。

[回复评论](https://kexue.fm/archives/10739/comment-page-2?replyTo=28371#respond-post-10739)

JueningJin 发表于
August 14th, 2025

感谢苏神回复！期待能有进一步的精彩分析

[回复评论](https://kexue.fm/archives/10739/comment-page-2?replyTo=28387#respond-post-10739)

Gusto

September 5th, 2025

按照个人理解将weight decay理解为损失函数中的惩罚项的话，为什么weight decay使用了与adamw一致的 ΔW=−η\[msign(M)+λW\] 而不是对于 λW 也一并进行 msign 操作的 ΔW=−η\[msign(M+λW)\] ？

[回复评论](https://kexue.fm/archives/10739/comment-page-2?replyTo=28523#respond-post-10739)

[苏剑林](https://kexue.fm) 发表于
September 12th, 2025

AdamW之所以单独突出一个W，就是强调要把weight decay单独分离出来，而不要作为L2正则项。

[回复评论](https://kexue.fm/archives/10739/comment-page-2?replyTo=28545#respond-post-10739)

1. [«](https://kexue.fm/archives/10739/comment-page-1#comments)
2. [1](https://kexue.fm/archives/10739/comment-page-1#comments)
3. [2](https://kexue.fm/archives/10739/comment-page-2#comments)

[取消回复](https://kexue.fm/archives/10739#respond-post-10739)

你的大名

电子邮箱

个人网站（选填）

1\. 可以使用LaTeX代码，点击“预览效果”可查看效果；2. 可以通过点击评论楼层编号来引用该楼层；3. 网站可能会有点卡，如非确认评论失败，请 **不要重复点击提交**。

### 内容速览

[优化原理](https://kexue.fm/kexue.fm#%E4%BC%98%E5%8C%96%E5%8E%9F%E7%90%86)
[矩阵范数](https://kexue.fm/kexue.fm#%E7%9F%A9%E9%98%B5%E8%8C%83%E6%95%B0)
[权重衰减](https://kexue.fm/kexue.fm#%E6%9D%83%E9%87%8D%E8%A1%B0%E5%87%8F)
[RMS对齐](https://kexue.fm/kexue.fm#RMS%E5%AF%B9%E9%BD%90)
[实验分析](https://kexue.fm/kexue.fm#%E5%AE%9E%E9%AA%8C%E5%88%86%E6%9E%90)
[拓展思考](https://kexue.fm/kexue.fm#%E6%8B%93%E5%B1%95%E6%80%9D%E8%80%83)
[文章小结](https://kexue.fm/kexue.fm#%E6%96%87%E7%AB%A0%E5%B0%8F%E7%BB%93)

### 智能搜索

支持整句搜索！网站自动使用 [结巴分词](https://github.com/fxsjy/jieba) 进行分词，并结合ngrams排序算法给出合理的搜索结果。

### 热门标签

[生成模型](https://kexue.fm/tag/%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B/) [attention](https://kexue.fm/tag/attention/) [优化](https://kexue.fm/tag/%E4%BC%98%E5%8C%96/) [语言模型](https://kexue.fm/tag/%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/) [模型](https://kexue.fm/tag/%E6%A8%A1%E5%9E%8B/) [网站](https://kexue.fm/tag/%E7%BD%91%E7%AB%99/) [概率](https://kexue.fm/tag/%E6%A6%82%E7%8E%87/) [梯度](https://kexue.fm/tag/%E6%A2%AF%E5%BA%A6/) [矩阵](https://kexue.fm/tag/%E7%9F%A9%E9%98%B5/) [转载](https://kexue.fm/tag/%E8%BD%AC%E8%BD%BD/) [优化器](https://kexue.fm/tag/%E4%BC%98%E5%8C%96%E5%99%A8/) [微分方程](https://kexue.fm/tag/%E5%BE%AE%E5%88%86%E6%96%B9%E7%A8%8B/) [分析](https://kexue.fm/tag/%E5%88%86%E6%9E%90/) [天象](https://kexue.fm/tag/%E5%A4%A9%E8%B1%A1/) [深度学习](https://kexue.fm/tag/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/) [积分](https://kexue.fm/tag/%E7%A7%AF%E5%88%86/) [python](https://kexue.fm/tag/python/) [力学](https://kexue.fm/tag/%E5%8A%9B%E5%AD%A6/) [无监督](https://kexue.fm/tag/%E6%97%A0%E7%9B%91%E7%9D%A3/) [扩散](https://kexue.fm/tag/%E6%89%A9%E6%95%A3/) [几何](https://kexue.fm/tag/%E5%87%A0%E4%BD%95/) [节日](https://kexue.fm/tag/%E8%8A%82%E6%97%A5/) [生活](https://kexue.fm/tag/%E7%94%9F%E6%B4%BB/) [文本生成](https://kexue.fm/tag/%E6%96%87%E6%9C%AC%E7%94%9F%E6%88%90/) [数论](https://kexue.fm/tag/%E6%95%B0%E8%AE%BA/)

### 随机文章

- [从费马大定理谈起（八）：艾森斯坦整数](https://kexue.fm/archives/2900)
- [最小熵原理（三）：“飞象过河”之句模版和语言结构](https://kexue.fm/archives/5577)
- [矩阵描述三维空间旋转](https://kexue.fm/archives/2224)
- [第114号化学元素再次被实验确认](https://kexue.fm/archives/692)
- [能量视角下的GAN模型（三）：生成模型=能量模型](https://kexue.fm/archives/6612)
- [为节约而生：从标准Attention到稀疏Attention](https://kexue.fm/archives/6853)
- [斯皮策太空望远镜发现两颗行星大碰撞(图)](https://kexue.fm/archives/69)
- [logsumexp运算的几个不等式](https://kexue.fm/archives/9070)
- [\[备份\]全国大学生数学建模竞赛论文LaTex模板](https://kexue.fm/archives/2935)
- [从语言模型到Seq2Seq：Transformer如戏，全靠Mask](https://kexue.fm/archives/6933)

### 最近评论

- [lcyedd](https://kexue.fm/archives/11390/comment-page-1#comment-28856): 如果n个独立进程耗时服从正态分布，整个系统的总耗时就可以这样估。当然现实中一般服从长尾分布，不...
- [苏剑林](https://kexue.fm/archives/11416/comment-page-1#comment-28855): 总结一下几点经验，供参考。1、用Muon的理由可以很多，比如追求效果的，或者单纯尝鲜的，或者单...
- [苏剑林](https://kexue.fm/archives/9305/comment-page-1#comment-28854): 我自己没特别关注这种物理景观特别明显的研究，或者说这类研究真正明显work的似乎不多。
- [苏剑林](https://kexue.fm/archives/9209/comment-page-7#comment-28853): 你可以理解为给这个loss多乘以了一项标量权重，它不改变这个loss的理论最优解（万能拟合能力...
- [苏剑林](https://kexue.fm/archives/11307/comment-page-1#comment-28852): 如果是$t\\to\\infty$，那么理论上是这样子；但如果$t$并非那么大，或者说$\\lamb...
- [BaoLi](https://kexue.fm/archives/11416/comment-page-1#comment-28851): 返回去又看了博主相关论文。如果我理解没错的话，考虑两种情形：1，使用者有较为充裕多卡资源进行优...
- [z](https://kexue.fm/archives/11416/comment-page-1#comment-28850): 可以省显存，加速收敛，提高训练效率啊
- [BaoLi](https://kexue.fm/archives/11416/comment-page-1#comment-28849): 感谢大佬分享，我有几个问题：1，看了一遍以后，感觉整个流程比起Adam还是有些麻烦，但是拉到最...
- [Dhuzi](https://kexue.fm/archives/9305/comment-page-1#comment-28848): 苏神，能多发点这种物理和机器学习结合的研究内容吗？或者您觉得机器学习在物理研究当中作用大吗？现...
- [小白一枚](https://kexue.fm/archives/9209/comment-page-7#comment-28847): 苏老师，（14）到（15）你给的解释是直接删去分母，确实对（14）式子不影响，但是损失函数（1...

### 友情链接

- [Cool Papers](https://papers.cool)
- [数学研发](https://bbs.emath.ac.cn)
- [Seatop](http://www.seatop.com.cn/)
- [Xiaoxia](https://xiaoxia.org/)
- [积分表-网络版](https://kexue.fm/sci/integral/index.html)
- [丝路博傲](http://blog.dvxj.com/)
- [数学之家](http://www.2math.cn/)
- [有趣天文奇观](http://interesting-sky.china-vo.org/)
- [TwistedW](http://www.twistedwg.com/)
- [godweiyang](https://godweiyang.com/)
- [AI柠檬](https://blog.ailemon.net/)
- [王登科-DK博客](https://greatdk.com)
- [ESON](https://blog.eson.org/)
- [枫之羽](https://fzhiy.net/)
- [Mathor's blog](https://wmathor.com/)
- [coding-zuo](https://coding-zuo.github.io/)
- [博科园](https://www.bokeyuan.net/)
- [孔皮皮的博客](https://www.kppkkp.top/)
- [运鹏的博客](https://yunpengtai.top/)
- [jiming.site](https://jiming.site/)
- [OmegaXYZ](https://www.omegaxyz.com/)
- [EAI猩球](https://www.robotech.ink/)
- [文举的博客](https://liwenju0.com/)
- [申请链接](https://kexue.fm/links.html)

本站采用创作共用版权协议，要求署名、非商业用途和保持一致。转载本站内容必须也遵循“ [署名-非商业用途-保持一致](http://creativecommons.org/licenses/by-nc-nd/2.5/cn/)”的创作共用协议。
© 2009-2025 Scientific Spaces. All rights reserved. Theme by [laogui](http://www.laogui.com). Powered by [Typecho](http://typecho.org). 备案号: [粤ICP备09093259号-1/2](https://beian.miit.gov.cn/)。