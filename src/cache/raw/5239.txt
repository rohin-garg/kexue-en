![MobileSideBar](https://kexue.fm/usr/themes/geekg/images/slide-button.png)

## SEARCH

## MENU

- [打赏](https://kexue.fm/reward.html)
- [公式](https://kexue.fm/latex.html)
- [天象](https://kexue.fm/ac.html)
- [链接](https://kexue.fm/links.html)
- [时光](https://kexue.fm/me.html)
- [博览](https://kexue.fm/science.html)
- [归档](https://kexue.fm/content.html)

## CATEGORIES

- [千奇百怪](https://kexue.fm/category/Everything)
- [天文探索](https://kexue.fm/category/Astronomy)
- [数学研究](https://kexue.fm/category/Mathematics)
- [物理化学](https://kexue.fm/category/Phy-chem)
- [信息时代](https://kexue.fm/category/Big-Data)
- [生物自然](https://kexue.fm/category/Biology)
- [图片摄影](https://kexue.fm/category/Photograph)
- [问题百科](https://kexue.fm/category/Questions)
- [生活/情感](https://kexue.fm/category/Life-Feeling)
- [资源共享](https://kexue.fm/category/Resources)

## NEWPOSTS

- [SVD的导数](https://kexue.fm/archives/10878)
- [智能家居之手搓一套能接入米家的零冷水装置](https://kexue.fm/archives/10869)
- [Transformer升级之路：1...](https://kexue.fm/archives/10862)
- [矩阵的有效秩（Effective ...](https://kexue.fm/archives/10847)
- [通过梯度近似寻找Normaliza...](https://kexue.fm/archives/10831)
- [MoE环游记：4、难处应当多投入](https://kexue.fm/archives/10815)
- [高阶muP：更简明但更高明的谱条件缩放](https://kexue.fm/archives/10795)
- [初探muP：超参数的跨模型尺度迁移规律](https://kexue.fm/archives/10770)
- [MoE环游记：3、换个思路来分配](https://kexue.fm/archives/10757)
- [Muon续集：为什么我们选择尝试M...](https://kexue.fm/archives/10739)

## COMMENTS

- [苏剑林: 1、明白了，我将$q\_{\\phi}(z\|x)$看成$q\_{\\p...](https://kexue.fm/archives/5239/comment-page-3#comment-27496)
- [Suahi: 谢谢苏老师的回复！1\. 首先回复您为什么ELBO不带KL，并不...](https://kexue.fm/archives/5239/comment-page-3#comment-27493)
- [eular: 是的，当$k$比较大时会出现这种情况。](https://kexue.fm/archives/10373/comment-page-1#comment-27492)
- [苏剑林: 肯定是$\\mathbb{E}\_{x \\sim p\_{data}...](https://kexue.fm/archives/5239/comment-page-3#comment-27491)
- [苏剑林: 你的意思是$\\lambda(\\boldsymbol{x}) <...](https://kexue.fm/archives/10373/comment-page-1#comment-27490)
- [苏剑林: 好问题，下一篇文章可能会讨论这个问题](https://kexue.fm/archives/10735/comment-page-1#comment-27489)
- [苏剑林: 不大熟悉，但都diffusion forcing了，还有必要CM吗](https://kexue.fm/archives/10633/comment-page-1#comment-27488)
- [苏剑林: 简单看了一下，好像没什么新东西呀？还是我看漏了什么？](https://kexue.fm/archives/10711/comment-page-2#comment-27487)
- [苏剑林: 感谢指出，已修正。](https://kexue.fm/archives/8601/comment-page-1#comment-27486)
- [苏剑林: 扩散桥确实时不时刷到，但没理解这一套东西有什么特别价值或者应用...](https://kexue.fm/archives/9209/comment-page-7#comment-27485)

## USERLOGIN

- [登录](https://kexue.fm/admin/login.php)

[科学空间\|Scientific Spaces](https://kexue.fm)

- [登录](https://kexue.fm/admin/login.php)
- [打赏](https://kexue.fm/reward.html)
- [公式](https://kexue.fm/latex.html)
- [天象](https://kexue.fm/ac.html)
- [链接](https://kexue.fm/links.html)
- [时光](https://kexue.fm/me.html)
- [博览](https://kexue.fm/science.html)
- [归档](https://kexue.fm/content.html)

渴望成为一个小飞侠

- [![](https://kexue.fm/usr/themes/geekg/images/rss.png)\
\
欢迎订阅](https://kexue.fm/feed)
- [![](https://kexue.fm/usr/themes/geekg/images/mail.png)\
\
个性邮箱](https://kexue.fm/archives/119)
- [![](https://kexue.fm/usr/themes/geekg/images/Saturn.png)\
\
天象信息](https://kexue.fm/ac.html)
- [![](https://kexue.fm/usr/themes/geekg/images/iss.png)\
\
观测ISS](https://kexue.fm/archives/41)
- [![](https://kexue.fm/usr/themes/geekg/images/pi.png)\
\
LaTeX](https://kexue.fm/latex.html)
- [![](https://kexue.fm/usr/themes/geekg/images/mlogo.png)\
\
关于博主](https://kexue.fm/me.html)

欢迎访问“科学空间”，这里将与您共同探讨自然科学，回味人生百态；也期待大家的分享～

- [**千奇百怪** Everything](https://kexue.fm/category/Everything)
- [**天文探索** Astronomy](https://kexue.fm/category/Astronomy)
- [**数学研究** Mathematics](https://kexue.fm/category/Mathematics)
- [**物理化学** Phy-chem](https://kexue.fm/category/Phy-chem)
- [**信息时代** Big-Data](https://kexue.fm/category/Big-Data)
- [**生物自然** Biology](https://kexue.fm/category/Biology)
- [**图片摄影** Photograph](https://kexue.fm/category/Photograph)
- [**问题百科** Questions](https://kexue.fm/category/Questions)
- [**生活/情感** Life-Feeling](https://kexue.fm/category/Life-Feeling)
- [**资源共享** Resources](https://kexue.fm/category/Resources)

- [**千奇百怪**](https://kexue.fm/category/Everything)
- [**天文探索**](https://kexue.fm/category/Astronomy)
- [**数学研究**](https://kexue.fm/category/Mathematics)
- [**物理化学**](https://kexue.fm/category/Phy-chem)
- [**信息时代**](https://kexue.fm/category/Big-Data)
- [**生物自然**](https://kexue.fm/category/Biology)
- [**图片摄影**](https://kexue.fm/category/Photograph)
- [**问题百科**](https://kexue.fm/category/Questions)
- [**生活/情感**](https://kexue.fm/category/Life-Feeling)
- [**资源共享**](https://kexue.fm/category/Resources)

[首页](https://kexue.fm) [数学研究](https://kexue.fm/category/Mathematics) 从最大似然到EM算法：一致的理解方式

15Mar

# [从最大似然到EM算法：一致的理解方式](https://kexue.fm/archives/5239)

By 苏剑林 \|
2018-03-15 \|
169533位读者\|

最近在思考NLP的无监督学习和概率图相关的一些内容，于是重新把一些参数估计方法理了一遍。在深度学习中，参数估计是最基本的步骤之一了，也就是我们所说的模型训练过程。为了训练模型就得有个损失函数，而如果没有系统学习过概率论的读者，能想到的最自然的损失函数估计是平均平方误差，它也就是对应于我们所说的欧式距离。而理论上来讲，概率模型的最佳搭配应该是“交叉熵”函数，它来源于概率论中的最大似然函数。

## 最大似然 [\#](https://kexue.fm/archives/5239\#%E6%9C%80%E5%A4%A7%E4%BC%BC%E7%84%B6)

### 合理的存在 [\#](https://kexue.fm/archives/5239\#%E5%90%88%E7%90%86%E7%9A%84%E5%AD%98%E5%9C%A8)

何为最大似然？哲学上有句话叫做“存在就是合理的”，最大似然的意思是“存在就是最合理的”。具体来说，如果事件$X$的概率分布为$p(X)$，如果一次观测中具体观测到的值分别为$X\_1,X\_2,\\dots,X\_n$，并假设它们是相互独立，那么

$$\\mathcal{P} = \\prod\_{i=1}^n p(X\_i)\\tag{1}$$

是最大的。如果$p(X)$是一个带有参数$\\theta$的概率分布式$p\_{\\theta}(X)$，那么我们应当想办法选择$\\theta$，使得$\\mathcal{L}$最大化，即

$$\\theta = \\mathop{\\text{argmax}}\_{\\theta} \\mathcal{P}(\\theta) = \\mathop{\\text{argmax}}\_{\\theta}\\prod\_{i=1}^n p\_{\\theta}(X\_i)\\tag{2}$$

对概率取对数，就得到等价形式

$$\\theta = \\mathop{\\text{argmax}}\_{\\theta}\\sum\_{i=1}^n \\log p\_{\\theta}(X\_i)\\tag{3}$$

如果右端再除以$n$，我们就得到更精炼的表达形式

$$\\theta = \\mathop{\\text{argmax}}\_{\\theta} \\mathcal{L}(\\theta) = \\mathop{\\text{argmax}}\_{\\theta} \\mathbb{E}\\big\[\\log p\_{\\theta}(X\_i)\\big\]\\tag{4}$$

其中我们将$-\\mathcal{L}(\\theta)$就称为交叉熵。

### 理论形式 [\#](https://kexue.fm/archives/5239\#%E7%90%86%E8%AE%BA%E5%BD%A2%E5%BC%8F)

理论上，根据已有的数据，我们可以得到每个$X$的统计频率$\\tilde{p}(X)$，那么可以得到上式的等价形式

$$\\theta = \\mathop{\\text{argmax}}\_{\\theta} \\mathcal{L}(\\theta) = \\mathop{\\text{argmax}}\_{\\theta}\\sum\_X \\tilde{p}(X)\\log p\_{\\theta}(X)\\tag{5}$$

但实际上我们几乎都不可能得到$\\tilde{p}(X)$（尤其是对于连续分布），我们能直接算的是关于它的数学期望，也就是$(4)$式，因为求期望只需要把每个样本的值算出来，然后求和并除以$n$就行了。所以$(5)$式只有理论价值，它能方便后面的推导。

要注意的是，上面的描述是非常一般的，其中$X$可以是任意对象，它也有可能是连续的实数，这时候就要把求和换成积分，把$p(X)$变成概率密度函数。当然，这并没有什么本质困难。

### 更广泛的KL散度 [\#](https://kexue.fm/archives/5239\#%E6%9B%B4%E5%B9%BF%E6%B3%9B%E7%9A%84KL%E6%95%A3%E5%BA%A6)

从KL散度出发也可以导出最大似然的形式来。假如两个分布$\\tilde{p}(X)$和$p(X)$，我们用KL散度来衡量它们的距离：

$$\\begin{aligned}KL\\Big(\\tilde{p}(X)\\Big\\Vert p(X)\\Big) =& \\sum\_X \\tilde{p}(X) \\ln \\frac{\\tilde{p}(X)}{p(X)}\\\

=&\\mathbb{E}\\left\[\\ln \\frac{\\tilde{p}(X)}{p(X)}\\right\]\\end{aligned}\\tag{6}$$

当两个分布相同时，KL散度为0，当两个分布不同时，KL散度大于0，假设读者已经知道这些性质。

接着假设$X$的样本已经给出来了，这就意味着$\\tilde{p}(X)$可以视为已知了，这时候：

$$\\begin{aligned}\\theta =& \\mathop{\\text{argmin}}\_{\\theta} KL\\Big(\\tilde{p}(X)\\Big\\Vert p\_{\\theta}(X)\\Big)\\\

=& \\mathop{\\text{argmax}}\_{\\theta}\\sum\_X \\tilde{p}(X)\\log p\_{\\theta}(X)\\\

=& \\mathop{\\text{argmax}}\_{\\theta}\\mathbb{E}\\big\[\\log p\_{\\theta}(X\_i)\\big\]\\end{aligned}\\tag{7}$$

这就重新导出了$(4)$和$(5)$。事实上KL散度要比简单的最大似然含义更为丰富，因为最大似然相当于假设了$\\tilde{p}(X)$是已知的（已知$X$的样本），这并不总是能实现的（比如EM算法的场景），很多时候我们只知道$X$的部分信息，这时候就要回归到KL散度中来。

**注：如果读者不能很好地理解采样计算，请阅读《变分自编码器（二）：从贝叶斯观点出发》中的 [《数值计算vs采样计算》](https://kexue.fm/archives/5343#%E6%95%B0%E5%80%BC%E8%AE%A1%E7%AE%97vs%E9%87%87%E6%A0%B7%E8%AE%A1%E7%AE%97) 一节。**

## 有监督模型 [\#](https://kexue.fm/archives/5239\#%E6%9C%89%E7%9B%91%E7%9D%A3%E6%A8%A1%E5%9E%8B)

现在我们来观察有监督学习中是如何应用上述内容的。假设输入为$X$，标签为$Y$，那么$(X,Y)$就构成了一个事件，于是我们根据$(4)$就有

$$\\theta = \\mathop{\\text{argmax}}\_{\\theta} \\mathbb{E}\_{X,Y}\\big\[\\log p\_{\\theta}(X,Y)\\big\]\\tag{8}$$

这里已经注明了是对$X,Y$整体求数学期望，然而该式却是不够实用的。

### 分类问题 [\#](https://kexue.fm/archives/5239\#%E5%88%86%E7%B1%BB%E9%97%AE%E9%A2%98)

以分类问题为例，我们通常建模的是$p(Y\|X)$而不是$p(X,Y)$，也就是我们要根据输入确定输出的分布，而不是它们的联合分布。所以我们还是要从$(5)$式出发，利用$p(X,Y)=p(X) p(Y\|X)$，先得到

$$\\theta = \\mathop{\\text{argmax}}\_{\\theta} \\sum\_{X,Y} \\tilde{p}(X,Y)\\log \\big\[p\_{\\theta}(X)p\_{\\theta}(Y\|X)\\big\]\\tag{9}$$

因为我们只对$p(Y\|X)$建模，因此$p\_{\\theta}(X)$我们认为就是$\\tilde{p}(X)$，那么这相当于让优化目标多了一个常数项，因此$(9)$等价于

$$\\theta = \\mathop{\\text{argmax}}\_{\\theta} \\sum\_{X,Y} \\tilde{p}(X, Y)\\log p\_{\\theta}(Y\|X)\\tag{10}$$

然后，我们还有$\\tilde{p}(X,Y)=\\tilde{p}(X)\\tilde{p}(Y\|X)$，于是$(8)$式还可以再变化成

$$\\theta = \\mathop{\\text{argmax}}\_{\\theta} \\sum\_X \\tilde{p}(X) \\sum\_Y \\tilde{p}(Y\|X)\\log p\_{\\theta}(Y\|X)\\tag{11}$$

最后别忘了，我们是处理有监督学习中的分类问题，一般而言在训练数据中对于确定的输入$X$就只有一个类别，所以$\\tilde{p}(Y\_t\|X)=1$，其余为0，$Y\_t$就是$X$的目标标签，所以

$$\\theta = \\mathop{\\text{argmax}}\_{\\theta} \\sum\_X \\tilde{p}(X) \\log p\_{\\theta}(Y\_t\|X)\\tag{12}$$

这就是最常见的分类问题的最大似然函数了：

$$\\theta = \\mathop{\\text{argmax}}\_{\\theta} \\mathbb{E}\_{X}\\big\[\\log p\_{\\theta}(Y\_t\|X)\\big\]\\tag{13}$$

### 变变变 [\#](https://kexue.fm/archives/5239\#%E5%8F%98%E5%8F%98%E5%8F%98)

事实上，上述的内容只是一些恒等变换，应该说没有特别重要的价值，而它的结果（也就是分类问题的交叉熵损失）也早就被我们用得滚瓜烂熟了。因此，这一节仅仅是展示了如何将最大似然函数从最原始的形式出发，最终落实到一个具体的问题中，让读者熟悉一下这种逐步推进的变换过程。

## 隐变量 [\#](https://kexue.fm/archives/5239\#%E9%9A%90%E5%8F%98%E9%87%8F)

现在就是展示它的价值的时候了，我们要将**用它来给出一个EM算法的直接推导**（本博客还提供了另外一个理解角度，参考 [《梯度下降和EM算法：系出同源，一脉相承》](https://kexue.fm/archives/4277)）。对于EM算法，一般将它分为M步和E步，应当说，M步是比较好理解的，难就难在E步的那个$Q$函数为什么要这样构造。很多教程并没有给出这个$Q$函数的解释，有一些教程给出了基于詹森不等式的理解，但我认为这些做法都没有很好凸显出EM算法的精髓。

一般来说，EM算法用于存在隐变量的概率问题优化。什么是隐变量？很简单，还是以刚才的分类问题为例，分类问题要建模的是$p(Y\|X)$，当然也等价于$p(X,Y)$，我们说过要用最大似然函数为目标，得到$(8)$式

$$\\theta = \\mathop{\\text{argmax}}\_{\\theta} \\mathbb{E}\_{X,Y}\\big\[\\log p\_{\\theta}(X,Y)\\big\]\\tag{8}$$

如果给出$(X,Y)$的标签数据对，那就是一个普通的有监督学习问题了，然而如果只给出$X$不给出$Y$呢？这时候$Y$就称为隐变量，它存在，但我们看不见，所以“隐”。

### GMM模型 [\#](https://kexue.fm/archives/5239\#GMM%E6%A8%A1%E5%9E%8B)

等等，没有标签数据你也想做分类问题？当然有可能，GMM模型不就是这样的一个模型了吗？在GMM中假设了

$$p\_{\\theta}(X,Y) = p\_{\\theta}(Y) p\_{\\theta}(X\|Y)\\tag{14}$$

注意，是$p\_{\\theta}(Y) p\_{\\theta}(X\|Y)$而不是$p\_{\\theta}(X) p\_{\\theta}(Y\|X)$，两者区别在于我们难以直接估计$p(X)$，也比较难直接猜测$p(Y\|X)$的形式。而$p(Y)$和$p(X\|Y)$就相对容易了，因为我们通常假设$Y$的意义是类别，所以$p(Y)$只是一个有限向量，而$p(X\|Y)$表示每个类内的对象的分布，既然这些对象都属于同一个类，同一个类应该都长得差不多吧，所以GMM假设它为正态分布，这时候做的假设就有依据了，不然将所有数据混合在一起，谁知道假设什么分布好呢？

这种情况下，我们完整的数据应该是$(X,Y)$，但我们并没有这种成对的样本$(X\_1,Y\_1),\\dots,(X\_n,Y\_n)$（不然就退化为有监督学习了），我们只知道$X$的样本$X\_1,\\dots,X\_n$，这就对应了我们在KL散度这一节描述的情形了。

### pLSA模型 [\#](https://kexue.fm/archives/5239\#pLSA%E6%A8%A1%E5%9E%8B)

当然，并不只有无监督学习才有隐变量，有监督学习也可以有，比如我们可以设

$$p(Y\|X)=\\sum\_{Z}p\_{\\theta}(Y\|Z)p\_{\\theta}(Z\|X)\\tag{15}$$

这时候多出了一个变量$Z$，就算给出$(X,Y)$这样的标签数据对，但$Z$仍然是没有数据的，是我们假想的一个变量，它也就是隐变量，pLSA就是这样的一个问题。也就是说，这时候完整的数据对应该是$(X,Y,Z)$的形式，但我们只知道$(X\_1,Y\_1),\\dots,(X\_n,Y\_n)$这样的部分样本。

### 贝叶斯学派 [\#](https://kexue.fm/archives/5239\#%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%AD%A6%E6%B4%BE)

可能有读者“异想天开”：那么参数$\\theta$是不是也可以看作一个隐变量呢？恭喜你，如果你有这层领悟，那你已经进入贝叶斯学派的思维范畴了。贝叶斯学派认为，一切都是随机的，一切都服从某个概率分布，参数$\\theta$也不例外。不过很遗憾，贝叶斯学派的概率理论很艰深，我们这里还没法派上用场。（其实更重要的是，笔者也还不懂～～）

## EM算法 [\#](https://kexue.fm/archives/5239\#EM%E7%AE%97%E6%B3%95)

好了，不再废话了，还是正式进入对EM算法的讨论吧。

### 联合KL散度 [\#](https://kexue.fm/archives/5239\#%E8%81%94%E5%90%88KL%E6%95%A3%E5%BA%A6)

我们先来看一下，对于含有隐变量的问题求解，一般教程的处理方案是这样的：由于隐变量不可观测，因此一般改用边缘分布（也就是显变量的分布）的最大似然为目标函数，即

$$\\theta = \\mathop{\\text{argmax}}\_{\\theta}\\sum\_X \\tilde{p}(X)\\log\\sum\_Z p\_{\\theta}(X\|Z)p\_{\\theta}(Z)\\tag{16}$$

为最大化的目标。

这种做法不是不行，而是这样一来为了得到EM算法就需要引入比较多的数学知识，而且严格证明还需要比较冗长的推导。事实上可以从KL散度出发，通过分析联合概率分布的KL散度来极大简化EM算法的推导。而如果采用边缘分布最大似然的做法，我们就无法直观地理解那个$Q$函数的来源了。

以GMM为例，首先我们来算$\\tilde{p}(X,Y)$和$p\_{\\theta}(X,Y)$的KL散度：

$$\\begin{aligned} &KL\\Big(\\tilde{p}(X,Y)\\Big\\Vert p\_{\\theta}(X,Y)\\Big)\\\

=& \\sum\_{X,Y}\\tilde{p}(X,Y)\\log \\frac{\\tilde{p}(X,Y)}{p\_{\\theta}(X,Y)}\\\

=& \\sum\_{X}\\tilde{p}(X)\\sum\_{Y} \\tilde{p}(Y\|X)\\log \\frac{\\tilde{p}(Y\|X)\\tilde{p}(X)}{p\_{\\theta}(X\|Y)p\_{\\theta}(Y)}\\\

=& \\mathbb{E}\_X\\left\[\\sum\_{Y} \\tilde{p}(Y\|X)\\log \\frac{\\tilde{p}(Y\|X)\\tilde{p}(X)}{p\_{\\theta}(X\|Y)p\_{\\theta}(Y)}\\right\]\\\

=& \\mathbb{E}\_X\\left\[\\sum\_{Y} \\tilde{p}(Y\|X)\\log \\frac{\\tilde{p}(Y\|X)}{p\_{\\theta}(X\|Y)p\_{\\theta}(Y)}+\\sum\_{Y} \\tilde{p}(Y\|X)\\log \\tilde{p}(X)\\right\]\\\

=& \\mathbb{E}\_X\\left\[\\sum\_{Y} \\tilde{p}(Y\|X)\\log \\frac{\\tilde{p}(Y\|X)}{p\_{\\theta}(X\|Y)p\_{\\theta}(Y)}\\right\]+\\mathbb{E}\\left\[\\log \\tilde{p}(X)\\right\]\\\

=& \\mathbb{E}\_X\\left\[\\sum\_{Y} \\tilde{p}(Y\|X)\\log \\frac{\\tilde{p}(Y\|X)}{p\_{\\theta}(X\|Y)p\_{\\theta}(Y)}\\right\]+\\text{常数}

\\end{aligned}\\tag{17}$$

这个过程虽然比较长，但并没有什么迂回的变换，是比较容易接受的。

### EM大佬来了 [\#](https://kexue.fm/archives/5239\#EM%E5%A4%A7%E4%BD%AC%E6%9D%A5%E4%BA%86)

再次回顾$(17)$式的来源，我们希望找到一组分布的参数$\\theta$，使得$KL\\Big(\\tilde{p}(X,Y)\\Big\\Vert p\_{\\theta}(X,Y)\\Big)$越小越好，$p\_{\\theta}(X,Y)$我们已经给出为$p\_{\\theta}(X\|Y)p\_{\\theta}(Y)$的形式，只有参数$\\theta$是未知的。但是在$(17)$式中，$\\tilde{p}(Y\|X)$也是未知的，包括形式。

这时候，大佬就发话了：先当它已知的吧，这时候$\\tilde{p}(Y\|X)$可以视为常数，那么我们就可以算参数$\\theta$了：

$$\\begin{aligned}\\theta^{(r)} =& \\mathop{\\text{argmin}}\_{\\theta} \\mathbb{E}\_X\\left\[\\sum\_{Y} \\tilde{p}^{(r-1)}(Y\|X)\\log \\frac{\\tilde{p}^{(r-1)}(Y\|X)}{p\_{\\theta}(X\|Y)p\_{\\theta}(Y)}\\right\]\\\

=& \\mathop{\\text{argmax}}\_{\\theta} \\mathbb{E}\_X\\left\[\\sum\_{Y}\\tilde{p}^{(r-1)}(Y\|X)\\log p\_{\\theta}(Y) p\_{\\theta}(X\|Y)\\right\]\\end{aligned}\\tag{18}$$

然后这时候算出了新的$\\theta^{(r)}$，我们把$p\_{\\theta}(X\|Y)$当成已知的，来求$\\tilde{p}(Y\|X)$，

$$\\tilde{p}^{(r)}(Y\|X) = \\mathop{\\text{argmin}}\_{\\tilde{p}(Y\|X)} \\mathbb{E}\_X\\left\[\\sum\_{Y} \\tilde{p}(Y\|X)\\log \\frac{\\tilde{p}(Y\|X)}{p\_{\\theta^{(r)}}(X\|Y)p\_{\\theta^{(r)}}(Y)}\\right\]\\tag{19}$$

事实上$(19)$式是可以直接写出解析解的，答案是：

$$\\tilde{p}^{(r)}(Y\|X)=\\frac{p\_{\\theta^{(r)}}(Y)p\_{\\theta^{(r)}}(X\|Y)}{\\sum\\limits\_Y p\_{\\theta^{(r)}}(Y)p\_{\\theta^{(r)}}(X\|Y)}\\tag{20}$$

> **补充推导：** $(19)$式方括号内的部分，可以改写为
>
> $$\\begin{aligned}&\\sum\_{Y} \\tilde{p}(Y\|X)\\log \\frac{\\tilde{p}(Y\|X)}{p\_{\\theta^{(r)}}(X,Y)}\\\
>
> =&\\sum\_{Y} \\tilde{p}(Y\|X)\\log \\frac{\\tilde{p}(Y\|X)}{p\_{\\theta^{(r)}}(Y\|X)} - \\sum\_{Y} \\tilde{p}(Y\|X)\\log p\_{\\theta^{(r)}}(X)\\\
>
> =& KL\\Big(\\tilde{p}(Y\|X)\\Big\\Vert p\_{\\theta^{(r)}}(Y\|X)\\Big) - \\text{常数}
>
> \\end{aligned}$$
>
> 所以最小化$(19)$式也就相当于最小化$KL\\Big(\\tilde{p}(Y\|X)\\Big\\Vert p\_{\\theta^{(r)}}(Y\|X)\\Big)$，根据KL散度的性质，显然最优解就是两个分布完全一致，即
>
> $$\\tilde{p}(Y\|X) = p\_{\\theta^{(r)}}(Y\|X) = \\frac{p\_{\\theta^{(r)}}(Y)p\_{\\theta^{(r)}}(X\|Y)}{\\sum\\limits\_Y p\_{\\theta^{(r)}}(Y)p\_{\\theta^{(r)}}(X\|Y)}$$
>
> 这就得到了$(20)$式。

因为我们没法一步到位求$(17)$的最小值，所以现在就将它交替地训练：先固定一部分，最大化另外一部分，然后交换过来。**EM算法就是对复杂目标函数的交替训练方法**！

联合$(18)$式和$(20)$式，就构成了整个求解算法。现在来看看$(18)$式， **有个E（求期望），又有个M（$\\mathop{\\text{argmax}}$），就叫它EM算法吧，那个被E的式子，我们就叫它$Q$函数好了**。于是EM大佬就这样出来了，$Q$函数也出来了，就这么任性...

> 当然，EM算法中的E的本意是将$\\sum\\limits\_{Y}\\tilde{p}^{(r-1)}(Y\|X)\\log p\_{\\theta}(Y) p\_{\\theta}(X\|Y)$看成是对隐变量$Y$求期望，这里我们就随意一点的，结论没错就行～

是不是感觉很突然？感觉啥也没做，EM算法就这么两句话说清楚了？还包括了推导？

### 究竟在做啥 [\#](https://kexue.fm/archives/5239\#%E7%A9%B6%E7%AB%9F%E5%9C%A8%E5%81%9A%E5%95%A5)

对于pLSA或者其他含有隐变量的模型的EM算法，也可以类似地推导。对比目前我能找到的EM算法的推导，我相信上面的过程已经是相当简洁了。尽管前面很多铺垫，但其实都是基础知识而已。

那这是如何实现的呢？回顾整个过程，其实我们也没做什么，只是纯粹地使用KL散度作为联合分布的差异性度量，然后对KL散度交替最小化罢了～这样子得到的推导，比从边缘分布的最大似然出发，居然直接快捷了很多，也是个惊喜。

## 一致的理解 [\#](https://kexue.fm/archives/5239\#%E4%B8%80%E8%87%B4%E7%9A%84%E7%90%86%E8%A7%A3)

本文是作者对最大似然原理的一翻思考，整体思路是从最大似然的原理和形式出发，来诱导出有监督/无监督学习的一些结果，希望能用一个统一的思想将各种相关内容都串起来。最后发现结果也挺让人满意的，尤其是EM算法部分，以后只需要记住一切的根本都是（联合）分布的最大似然或KL散度，再也不用死记EM算法中的Q函数形式了。

当然，文章有些观点都是“我认为”的，因此可能有不当之处，请读者甄别。不过可以保证的是结果跟现有的都是一样的。欢迎读者继续交流～

_**转载到请包括本文地址：** [https://kexue.fm/archives/5239](https://kexue.fm/archives/5239)_

_**更详细的转载事宜请参考：**_ [《科学空间FAQ》](https://kexue.fm/archives/6508#%E6%96%87%E7%AB%A0%E5%A6%82%E4%BD%95%E8%BD%AC%E8%BD%BD/%E5%BC%95%E7%94%A8)

**如果您还有什么疑惑或建议，欢迎在下方评论区继续讨论。**

**如果您觉得本文还不错，欢迎 [分享](https://kexue.fm/archives/5239#share)/ [打赏](https://kexue.fm/archives/5239#pay) 本文。打赏并非要从中获得收益，而是希望知道科学空间获得了多少读者的真心关注。当然，如果你无视它，也不会影响你的阅读。再次表示欢迎和感谢！**

打赏

![科学空间](https://kexue.fm/usr/themes/geekg/payment/wx.png)

微信打赏

![科学空间](https://kexue.fm/usr/themes/geekg/payment/zfb.png)

支付宝打赏

因为网站后台对打赏并无记录，因此欢迎在打赏时候备注留言。

你还可以 [**点击这里**](http://mail.qq.com/cgi-bin/qm_share?t=qm_mailme&email=tN7d1drY3drrx8H0xcWa19vZ) 或在下方评论区留言来告知你的建议或需求。

**如果您需要引用本文，请参考：**

苏剑林. (Mar. 15, 2018). 《从最大似然到EM算法：一致的理解方式 》\[Blog post\]. Retrieved from [https://kexue.fm/archives/5239](https://kexue.fm/archives/5239)

@online{kexuefm-5239,

        title={从最大似然到EM算法：一致的理解方式},

        author={苏剑林},

        year={2018},

        month={Mar},

        url={\\url{https://kexue.fm/archives/5239}},

}

分类： [数学研究](https://kexue.fm/category/Mathematics)    标签： [概率](https://kexue.fm/tag/%E6%A6%82%E7%8E%87/), [优化](https://kexue.fm/tag/%E4%BC%98%E5%8C%96/), [聚类](https://kexue.fm/tag/%E8%81%9A%E7%B1%BB/)[50 评论](https://kexue.fm/archives/5239#comments)

< [三味Capsule：矩阵Capsule与EM路由](https://kexue.fm/archives/5155) \| [变分自编码器（一）：原来是这么一回事](https://kexue.fm/archives/5253) >

### 你也许还对下面的内容感兴趣

- [MoE环游记：4、难处应当多投入](https://kexue.fm/archives/10815)
- [为什么梯度裁剪的默认模长是1？](https://kexue.fm/archives/10657)
- [从谱范数梯度到新式权重衰减的思考](https://kexue.fm/archives/10648)
- [从Hessian近似看自适应学习率优化器](https://kexue.fm/archives/10588)
- [Softmax后传：寻找Top-K的光滑近似](https://kexue.fm/archives/10373)
- [通向最优分布之路：概率空间的最小化](https://kexue.fm/archives/10289)
- [通向概率分布之路：盘点Softmax及其替代品](https://kexue.fm/archives/10145)
- [缓存与效果的极限拉扯：从MHA、MQA、GQA到MLA](https://kexue.fm/archives/10091)
- [用傅里叶级数拟合一维概率密度函数](https://kexue.fm/archives/10007)
- [旁门左道之如何让Python的重试代码更加优雅](https://kexue.fm/archives/9938)

[发表你的看法](https://kexue.fm/archives/5239#comment_form)

1. [«](https://kexue.fm/archives/5239/comment-page-2#comments)
2. [1](https://kexue.fm/archives/5239/comment-page-1#comments)
3. [2](https://kexue.fm/archives/5239/comment-page-2#comments)
4. [3](https://kexue.fm/archives/5239/comment-page-3#comments)

古今

April 21st, 2025

感谢苏老师，最近温习了以下EM算法，自己在看VAE和DM的时候就感觉这些本质上都是一类算法

[回复评论](https://kexue.fm/archives/5239/comment-page-3?replyTo=27458#respond-post-5239)

[苏剑林](https://kexue.fm) 发表于
April 27th, 2025

点赞

[回复评论](https://kexue.fm/archives/5239/comment-page-3?replyTo=27479#respond-post-5239)

Suahi

April 27th, 2025

苏老师您好，我最近在写一篇基于\*\*最大似然估计\*\*求VAE损失函数的博客，推导背景如下：

现有一个神经网络参数化的$p\_{\\theta}$，和真实数据分布$p\_{data}$，本质目的为了使$p\_{\\theta} \\rightarrow p\_{data}$，为此我们计算二者KL散度公式：

$$

\\begin{aligned}

D\_{KL}(p\_{data}\|\|p\_{\\theta}) &= \\int p\_{data}(x)log\\frac{p\_{data}(x)}{p\_{\\theta}(x)}dx \\\

&= \\int p\_{data}(x)logp\_{data}(x)dx - \\int p\_{data}(x)logp\_{\\theta}(x)dx \\\

&=\\mathbb{E}\_{x \\sim p\_{data}(x)}\[logp\_{data}(x)\]-\\mathbb{E}\_{x \\sim p\_{data}(x)}\[logp\_{\\theta}(x)\]

\\end{aligned}

$$

这要求$\\mathbb{E}\_{x \\sim p\_{data}(x)}\[logp\_{\\theta}(x)\]$尽可能的大，在大数定理前提下，最大似然估计（MLE）的公式是和上述推导具有一致的形式。

$$

\\frac{1}{N}\\sum\_{i=1}^Nlogp\_{\\theta}(x\_i) \\rightarrow \\mathbb{E}\_{x \\sim p\_{data}(x)}\[log(p\_{\\theta}(x))\]

$$

但是我在开头就卡了壳，我的问题随之而来：

$\\text{1. 最大似然估计和ELBO的计算具有联系吗？}$

网上部分资料从$logp\_{\\theta}(x)$开始推导VAE的ELBO公式，我觉得既不满足MLE公式推导中$logp\_{\\theta}(x\_i)$的离散形式，也不满足$\\mathbb{E}\_{x \\sim p\_{data}(x)}\[log(p\_{\\theta}(x))\]$的连续形式。可能是因为我数学功底太差，没有想清楚二者到底是否有联系，只是形式上的一致吗？如果有联系为什么推导过程可以省去$\\mathbb{E}\_{x \\sim p\_{data}(x)}$？

$\\text{2. VAE的ELBO计算到底该从$logp\_{\\theta}(x)$出发还是从$logp(x)$出发？}$

关于这个问题，可能不适合在此专栏下提出，但是我考虑到和MLE可能有一定关联性还是斗胆提问

$$

\\begin{aligned}

logp(x)&=logp(x)\\int\_zq\_{\\phi}(z\|x)dz \\\

&=\\int\_z q\_{\\phi}(z\|x)logp(x)dz \\\

&=\\mathbb{E}\_{z \\sim q\_{\\phi}(z\|x)}\[logp(x)\] \\\

&=\\mathbb{E}\_{z \\sim q\_{\\phi}(z\|x)}\[log\\frac{p(x,z)}{p(z\|x)}\] \\\

&=\\mathbb{E}\_{z \\sim q\_{\\phi}(z\|x)}\[log\\frac{p(x,z)}{p(z\|x)}\\frac{q\_{\\phi}(z\|x)}{q\_{\\phi}(z\|x)}\] \\\

&=\\mathbb{E}\_{z \\sim q\_{\\phi}(z\|x)}\[log\\frac{p(x,z)}{q\_{\\phi}(z\|x)}\]+\\mathbb{E}\_{z \\sim q\_{\\phi}(z\|x)}\[\\frac{q\_{\\phi}(z\|x)}{p(z\|x)}\] \\\

&=\\mathbb{E}\_{z \\sim q\_{\\phi}(z\|x)}\[log\\frac{p(x,z)}{q\_{\\phi}(z\|x)}\]+D\_{KL}(q\_{\\phi}(z\|x)\|\|p(z\|x))\\\

&\\geq \\mathbb{E}\_{z \\sim q\_{\\phi}(z\|x)}\[log\\frac{p(x,z)}{q\_{\\phi}(z\|x)}\] = ELBO

\\end{aligned}

$$

但也有一部分资料推导显示

$$

\\begin{aligned}

logp\_{\\theta}(x)&=logp\_{\\theta}(x)\\int\_zq\_{\\phi}(z\|x)dz \\\

&=\\int\_z q\_{\\phi}(z\|x)logp\_{\\theta}(x)dz \\\

&=\\mathbb{E}\_{z \\sim q\_{\\phi}(z\|x)}\[logp\_{\\theta}(x)\] \\\

&=\\mathbb{E}\_{z \\sim q\_{\\phi}(z\|x)}\[log\\frac{p\_{\\theta}(x,z)}{p\_{\\theta}(z\|x)}\] \\\

&=\\mathbb{E}\_{z \\sim q\_{\\phi}(z\|x)}\[log\\frac{p\_{\\theta}(x,z)}{p\_{\\theta}(z\|x)}\\frac{q\_{\\phi}(z\|x)}{q\_{\\phi}(z\|x)}\] \\\

&=\\mathbb{E}\_{z \\sim q\_{\\phi}(z\|x)}\[log\\frac{p\_{\\theta}(x,z)}{q\_{\\phi}(z\|x)}\]+\\mathbb{E}\_{z \\sim q\_{\\phi}(z\|x)}\[\\frac{q\_{\\phi}(z\|x)}{p\_{\\theta}(z\|x)}\] \\\

&=\\mathbb{E}\_{z \\sim q\_{\\phi}(z\|x)}\[log\\frac{p\_{\\theta}(x,z)}{q\_{\\phi}(z\|x)}\]+D\_{KL}(q\_{\\phi}(z\|x)\|\|p\_{\\theta}(z\|x))\\\

&\\geq \\mathbb{E}\_{z \\sim q\_{\\phi}(z\|x)}\[log\\frac{p\_{\\theta}(x,z)}{q\_{\\phi}(z\|x)}\] = ELBO

\\end{aligned}

$$

对于前者我理解是$logp(x)$是一个定值，所以ELBO设计有天然的意义，最大化ELBO的同时还能最小化$D\_{KL}(q\_{\\phi}(z\|x)\|\|p(z\|x))$，规避了由于不知道真实分布$p(z\|x)$进而导致两个分布之间无法计算距离（无法评价Encoder）的问题；对于后者，$logp\_{\\theta}(x)$已经是一个参数化的分布，该值并非定值，那么最大化ELBO还有意义吗？$D\_{KL}(q\_{\\phi}(z\|x)\|\|p\_{\\theta}(z\|x))$一项是两个参数化分布之间的距离，其中意义在于什么呢？因此特向苏神请教，这两种推导方法之间的联系是什么？后者更像是MLE的形式但是我目前暂时理解不了后者推导的意义。

数学水平不高，希望苏神指教，感谢！！

[回复评论](https://kexue.fm/archives/5239/comment-page-3?replyTo=27484#respond-post-5239)

[苏剑林](https://kexue.fm) 发表于
April 27th, 2025

肯定是$\\mathbb{E}\_{x \\sim p\_{data}(x)}\[\\log(p\_{\\theta}(x))\]$为基础进行推导啊，有些教程省略了$\\mathbb{E}\_{x \\sim p\_{data}(x)}$，那是简单起见，推导的是单个样本的损失函数。因为实际训练肯定要多个样本求平均的，这自动包含了$\\mathbb{E}\_{x \\sim p\_{data}(x)}$。

至于$\\log p(x)$，你最后推导出来的$\\mathbb{E}\_{z \\sim q\_{\\phi}(z\|x)}\[log\\frac{p(x,z)}{q\_{\\phi}(z\|x)}\] = ELBO$我没看懂：一来按照你的定义，$p(x,z)$是真实的联合分布，这是不知道解析式的，怎么训练？二来这个目标也不是VAE的训练目标形式啊（KL散度哪里去了？）当然你的$\\log p\_{\\theta}(x)$也有同样问题。

也可能是我不了解ELBO那一套。总之，“反对 + 打倒”一切来自旧式变分推断的乱七八糟的恶心ELBO！

其实我的推导很简单：

$$KL\\Big(p\_{data}(x) p\_{\\theta}(z\|x)\\Big\\Vert q\_{\\phi}(x\|z)q(z)\\Big)\\geq KL\\Big(p\_{data}(x) \\Big\\Vert p\_{\\phi}(x)\\Big)$$

这里

$$p\_{\\phi}(x) = \\int q\_{\\phi}(x\|z)q(z) dz$$

所以用$KL\\Big(p\_{data}(x) p\_{\\theta}(z\|x)\\Big\\Vert q\_{\\phi}(x\|z)q(z)\\Big)$作为训练目标。

[回复评论](https://kexue.fm/archives/5239/comment-page-3?replyTo=27491#respond-post-5239)

Suahi 发表于
April 27th, 2025

谢谢苏老师的回复！

1\. 首先回复您为什么ELBO不带KL，并不是最终损失函数形式，需要做如下变形：

$$

\\begin{aligned}

ELBO &= \\mathbb{E}\_{z \\sim q\_{\\phi}(z\|x)}\[log\\frac{p\_{\\theta}(x,z)}{q\_{\\phi}(z\|x)}\] \\\

&= \\mathbb{E}\_{z \\sim q\_{\\phi}(z\|x)}\[log\\frac{p\_{\\theta}(x\|z)p(z)}{q\_{\\phi}(z\|x)}\] \\\

&=\\mathbb{E}\_{z \\sim q\_{\\phi}(z\|x)}\[logp\_{\\theta}(x\|z)\]-\\mathbb{E}\_{z \\sim q\_{\\phi}(z\|x)}\[log\\frac{q\_{\\phi}(z\|x)}{p(z)}\] \\\

&=\\mathbb{E}\_{z \\sim q\_{\\phi}(z\|x)}\[logp\_{\\theta}(x\|z)\]-D\_{KL}(q\_{\\phi}(z\|x)\|\|p(z))

\\end{aligned}

$$

2\. 其次，关于您对“打倒基于旧式变分推断的ELBO”，我有一些不同看法：

VAE的参数调优策略与EM算法有同工之妙。

①固定$\\phi$参数，优化$\\theta$参数。在ELBO中$\\mathbb{E}\_{z \\sim q\_{\\phi}(z\|x)}\[logp\_{\\theta}(x\|z)\]$表现为提高解码器生成图像精度。但是此时$\\theta$的变动导致后验分布$p\_{\\theta}(z\|x)$也发生了变化，进而导致$D\_{KL}(q\_{\\phi}(z\|x)\|\|p\_{\\theta}(z\|x))$值发生了变化，最终影响$ELBO$值；

②固定$\\theta$参数，优化$\\phi$参数。因此$q\_{\\phi}(z\|x)$会通过调整$\\mu\_{\\phi}$和$\\sigma\_{\\phi}$去最大化ELBO值，体现为最小化$D\_{KL}(q\_{\\phi}(z\|x)\|\|p(z))=\\frac{1}{2}(1+log\\sigma\_{\\phi}^2-\\mu\_{\\phi}^2-\\sigma\_{\\phi}^2)$。同时，$\\mu\_{\\phi}$和$\\sigma\_{\\phi}$值的变动会重采样生成不同的$z$，这也使得$\\theta$参数得以不断更新；

二者实现动态平衡的效果。

按照变分推断的形式

$$

logp\_{\\theta}(x) = \\mathbb{E}\_{z \\sim q\_{\\phi}(z\|x)}\[log\\frac{p\_{\\theta}(x,z)}{q\_{\\phi}(z\|x)}\]+D\_{KL}(q\_{\\phi}(z\|x)\|\|p\_{\\theta}(z\|x))

$$

较为直观的解释了参数$\\phi$和$\\theta$之间的互相影响关系。

[回复评论](https://kexue.fm/archives/5239/comment-page-3?replyTo=27493#respond-post-5239)

[苏剑林](https://kexue.fm) 发表于
April 27th, 2025

1、明白了，我将$q\_{\\phi}(z\|x)$看成$q\_{\\phi}(x\|z)$了（ELBO厌恶感+1）；

2、“VAE与EM算法有同工之妙”跟“ELBO推导恶心”并不矛盾。

ELBO基于最大似然，或者说基于最小化$\\mathbb{E}\_{x \\sim p\_{data}(x)}\[-\\log(p\_{\\theta}(x))\]$，这玩意既不直观，也不好用，是概率统计最应该抛弃的概念之一。直观的做法应该是换成$KL$散度，比如$KL\\Big(p\_{data}(x) \\Big\\Vert p\_{\\phi}(x)\\Big)$，这是类似距离的存在，越小越好，等于零则相等。

而且$KL$散度是可以双向优化的，它不排斥$p\_{data}(x)$也有参数的情形（最大似然如果$p\_{data}(x)$有参数就挂了），比如VAE的优化目标，就是定义两个联合分布$p\_{data}(x) p\_{\\theta}(z\|x)$和$q\_{\\phi}(x\|z)q(z)$，然后优化它们的联合KL散度$KL\\Big(p\_{data}(x) p\_{\\theta}(z\|x)\\Big\\Vert q\_{\\phi}(x\|z)q(z)\\Big)$，多简单。

如果非要从ELBO角度来理解，我们也有不等式

$$KL\\Big(p\_{data}(x) p\_{\\theta}(z\|x)\\Big\\Vert q\_{\\phi}(x\|z)q(z)\\Big)\\geq KL\\Big(p\_{data}(x) \\Big\\Vert p\_{\\phi}(x)\\Big)$$

即联合分布的KL散度，是边缘分布的KL散度的上界，简单直观又好记，这不比原始的ELBO一通乱七八糟的形式舒服多了。

联合分布KL散度的视角，还可以推出 [https://kexue.fm/archives/5887](https://kexue.fm/archives/5887) 类似的聚类算法，很难想象用旧式ELBO的处理方式推导起来会有多麻烦。

总之，坚决打倒旧式ELBO。

至于你说的动态平衡理解，我在 [https://kexue.fm/archives/5253](https://kexue.fm/archives/5253) 也写过差不多的内容，但拆分开理解仅仅是一种不算很紧要的理解方式，它跟实际训练不吻合，并且理解结果也容易引起误解（拆分两项后，总有人觉得每一项都能优化到零）。

[回复评论](https://kexue.fm/archives/5239/comment-page-3?replyTo=27496#respond-post-5239)

1. [«](https://kexue.fm/archives/5239/comment-page-2#comments)
2. [1](https://kexue.fm/archives/5239/comment-page-1#comments)
3. [2](https://kexue.fm/archives/5239/comment-page-2#comments)
4. [3](https://kexue.fm/archives/5239/comment-page-3#comments)

[取消回复](https://kexue.fm/archives/5239#respond-post-5239)

你的大名

电子邮箱

个人网站（选填）

1\. 可以使用LaTeX代码，点击“预览效果”可查看效果；

2\. 可以通过点击评论楼层编号来引用该楼层；

3\. 网站可能会有点卡，如非确认评论失败，请不要重复点击提交。

### 内容速览

[最大似然](https://kexue.fm/archives/5239#%E6%9C%80%E5%A4%A7%E4%BC%BC%E7%84%B6)
[合理的存在](https://kexue.fm/archives/5239#%E5%90%88%E7%90%86%E7%9A%84%E5%AD%98%E5%9C%A8)
[理论形式](https://kexue.fm/archives/5239#%E7%90%86%E8%AE%BA%E5%BD%A2%E5%BC%8F)
[更广泛的KL散度](https://kexue.fm/archives/5239#%E6%9B%B4%E5%B9%BF%E6%B3%9B%E7%9A%84KL%E6%95%A3%E5%BA%A6)
[有监督模型](https://kexue.fm/archives/5239#%E6%9C%89%E7%9B%91%E7%9D%A3%E6%A8%A1%E5%9E%8B)
[分类问题](https://kexue.fm/archives/5239#%E5%88%86%E7%B1%BB%E9%97%AE%E9%A2%98)
[变变变](https://kexue.fm/archives/5239#%E5%8F%98%E5%8F%98%E5%8F%98)
[隐变量](https://kexue.fm/archives/5239#%E9%9A%90%E5%8F%98%E9%87%8F)
[GMM模型](https://kexue.fm/archives/5239#GMM%E6%A8%A1%E5%9E%8B)
[pLSA模型](https://kexue.fm/archives/5239#pLSA%E6%A8%A1%E5%9E%8B)
[贝叶斯学派](https://kexue.fm/archives/5239#%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%AD%A6%E6%B4%BE)
[EM算法](https://kexue.fm/archives/5239#EM%E7%AE%97%E6%B3%95)
[联合KL散度](https://kexue.fm/archives/5239#%E8%81%94%E5%90%88KL%E6%95%A3%E5%BA%A6)
[EM大佬来了](https://kexue.fm/archives/5239#EM%E5%A4%A7%E4%BD%AC%E6%9D%A5%E4%BA%86)
[究竟在做啥](https://kexue.fm/archives/5239#%E7%A9%B6%E7%AB%9F%E5%9C%A8%E5%81%9A%E5%95%A5)
[一致的理解](https://kexue.fm/archives/5239#%E4%B8%80%E8%87%B4%E7%9A%84%E7%90%86%E8%A7%A3)

### 智能搜索

支持整句搜索！网站自动使用 [结巴分词](https://github.com/fxsjy/jieba) 进行分词，并结合ngrams排序算法给出合理的搜索结果。

### 热门标签

[生成模型](https://kexue.fm/tag/%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B/) [attention](https://kexue.fm/tag/attention/) [模型](https://kexue.fm/tag/%E6%A8%A1%E5%9E%8B/) [语言模型](https://kexue.fm/tag/%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/) [优化](https://kexue.fm/tag/%E4%BC%98%E5%8C%96/) [网站](https://kexue.fm/tag/%E7%BD%91%E7%AB%99/) [概率](https://kexue.fm/tag/%E6%A6%82%E7%8E%87/) [梯度](https://kexue.fm/tag/%E6%A2%AF%E5%BA%A6/) [转载](https://kexue.fm/tag/%E8%BD%AC%E8%BD%BD/) [微分方程](https://kexue.fm/tag/%E5%BE%AE%E5%88%86%E6%96%B9%E7%A8%8B/) [天象](https://kexue.fm/tag/%E5%A4%A9%E8%B1%A1/) [矩阵](https://kexue.fm/tag/%E7%9F%A9%E9%98%B5/) [深度学习](https://kexue.fm/tag/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/) [分析](https://kexue.fm/tag/%E5%88%86%E6%9E%90/) [积分](https://kexue.fm/tag/%E7%A7%AF%E5%88%86/) [python](https://kexue.fm/tag/python/) [力学](https://kexue.fm/tag/%E5%8A%9B%E5%AD%A6/) [无监督](https://kexue.fm/tag/%E6%97%A0%E7%9B%91%E7%9D%A3/) [几何](https://kexue.fm/tag/%E5%87%A0%E4%BD%95/) [优化器](https://kexue.fm/tag/%E4%BC%98%E5%8C%96%E5%99%A8/) [扩散](https://kexue.fm/tag/%E6%89%A9%E6%95%A3/) [节日](https://kexue.fm/tag/%E8%8A%82%E6%97%A5/) [生活](https://kexue.fm/tag/%E7%94%9F%E6%B4%BB/) [文本生成](https://kexue.fm/tag/%E6%96%87%E6%9C%AC%E7%94%9F%E6%88%90/) [数论](https://kexue.fm/tag/%E6%95%B0%E8%AE%BA/)

### 随机文章

- [以自然数幂为系数的幂级数](https://kexue.fm/archives/986)
- [太空中的巨影——日食间的月球影子](https://kexue.fm/archives/32)
- [《科学》：我们发现了磁单极子](https://kexue.fm/archives/141)
- [最受尊崇的3位诺贝尔奖得主](https://kexue.fm/archives/154)
- [今日晒书](https://kexue.fm/archives/1423)
- [《巨眼问苍穹》：讲述望远镜的400年](https://kexue.fm/archives/356)
- [【NASA每日一图】月夜流星](https://kexue.fm/archives/76)
- [暑假结束了，上学去~](https://kexue.fm/archives/2056)
- [智能家居之小爱同学控制极米投影仪的简单方案](https://kexue.fm/archives/9365)
- [什么时候多进程的加速比可以大于1？](https://kexue.fm/archives/7031)

### 最近评论

- [苏剑林](https://kexue.fm/archives/5239/comment-page-3#comment-27496): 1、明白了，我将$q\_{\\phi}(z\|x)$看成$q\_{\\phi}(x\|z)$了（ELBO厌...
- [Suahi](https://kexue.fm/archives/5239/comment-page-3#comment-27493): 谢谢苏老师的回复！1\. 首先回复您为什么ELBO不带KL，并不是最终损失函数形式，需要做如下变...
- [eular](https://kexue.fm/archives/10373/comment-page-1#comment-27492): 是的，当$k$比较大时会出现这种情况。
- [苏剑林](https://kexue.fm/archives/5239/comment-page-3#comment-27491): 肯定是$\\mathbb{E}\_{x \\sim p\_{data}(x)}\[\\log(p\_{\\th...
- [苏剑林](https://kexue.fm/archives/10373/comment-page-1#comment-27490): 你的意思是$\\lambda(\\boldsymbol{x}) < x\_{\\min}$，一般情况下...
- [苏剑林](https://kexue.fm/archives/10735/comment-page-1#comment-27489): 好问题，下一篇文章可能会讨论这个问题
- [苏剑林](https://kexue.fm/archives/10633/comment-page-1#comment-27488): 不大熟悉，但都diffusion forcing了，还有必要CM吗
- [苏剑林](https://kexue.fm/archives/10711/comment-page-2#comment-27487): 简单看了一下，好像没什么新东西呀？还是我看漏了什么？
- [苏剑林](https://kexue.fm/archives/8601/comment-page-1#comment-27486): 感谢指出，已修正。
- [苏剑林](https://kexue.fm/archives/9209/comment-page-7#comment-27485): 扩散桥确实时不时刷到，但没理解这一套东西有什么特别价值或者应用，所以也就没去学习。如果您了解它...

### 友情链接

- [Cool Papers](https://papers.cool)
- [数学研发](https://bbs.emath.ac.cn)
- [Seatop](http://www.seatop.com.cn/)
- [Xiaoxia](https://xiaoxia.org/)
- [积分表-网络版](https://kexue.fm/sci/integral/index.html)
- [丝路博傲](http://blog.dvxj.com/)
- [ph4ntasy 饭特稀](http://www.ph4ntasy.com/)
- [数学之家](http://www.2math.cn/)
- [有趣天文奇观](http://interesting-sky.china-vo.org/)
- [TwistedW](http://www.twistedwg.com/)
- [godweiyang](https://godweiyang.com/)
- [AI柠檬](https://blog.ailemon.net/)
- [王登科-DK博客](https://greatdk.com)
- [ESON](https://blog.eson.org/)
- [枫之羽](https://fzhiy.net/)
- [Mathor's blog](https://wmathor.com/)
- [coding-zuo](https://coding-zuo.github.io/)
- [博科园](https://www.bokeyuan.net/)
- [孔皮皮的博客](https://www.kppkkp.top/)
- [运鹏的博客](https://yunpengtai.top/)
- [jiming.site](https://jiming.site/)
- [OmegaXYZ](https://www.omegaxyz.com/)
- [Blog by Eacls](https://www.eacls.top/)
- [EAI猩球](https://www.robotech.ink/)
- [文举的博客](https://liwenju0.com/)
- [申请链接](https://kexue.fm/links.html)

[![署名-非商业用途-保持一致](https://kexue.fm/usr/themes/geekg/images/cc.gif)](http://creativecommons.org/licenses/by-nc-nd/2.5/cn/) 本站采用创作共用版权协议，要求署名、非商业用途和保持一致。转载本站内容必须也遵循“ [署名-非商业用途-保持一致](http://creativecommons.org/licenses/by-nc-nd/2.5/cn/)”的创作共用协议。

© 2009-2025 Scientific Spaces. All rights reserved. Theme by [laogui](http://www.laogui.com). Powered by [Typecho](http://typecho.org). 备案号: [粤ICP备09093259号-1/2](https://beian.miit.gov.cn/)。