## SEARCH

## MENU

- [打赏](https://kexue.fm/reward.html)
- [公式](https://kexue.fm/latex.html)
- [天象](https://kexue.fm/ac.html)
- [链接](https://kexue.fm/links.html)
- [时光](https://kexue.fm/me.html)
- [博览](https://kexue.fm/science.html)
- [归档](https://kexue.fm/content.html)

## CATEGORIES

- [千奇百怪](https://kexue.fm/category/Everything)
- [天文探索](https://kexue.fm/category/Astronomy)
- [数学研究](https://kexue.fm/category/Mathematics)
- [物理化学](https://kexue.fm/category/Phy-chem)
- [信息时代](https://kexue.fm/category/Big-Data)
- [生物自然](https://kexue.fm/category/Biology)
- [图片摄影](https://kexue.fm/category/Photograph)
- [问题百科](https://kexue.fm/category/Questions)
- [生活/情感](https://kexue.fm/category/Life-Feeling)
- [资源共享](https://kexue.fm/category/Resources)

## NEWPOSTS

- [msign算子的Newton-Sc...](https://kexue.fm/archives/10996)
- [从无穷范数求导到等值振荡定理](https://kexue.fm/archives/10972)
- [生成扩散模型漫谈（三十）：从瞬时速...](https://kexue.fm/archives/10958)
- [MoE环游记：5、均匀分布的反思](https://kexue.fm/archives/10945)
- [msign算子的Newton-Sc...](https://kexue.fm/archives/10922)
- [Transformer升级之路：2...](https://kexue.fm/archives/10907)
- [一道概率不等式：盯着它到显然成立为止！](https://kexue.fm/archives/10902)
- [SVD的导数](https://kexue.fm/archives/10878)
- [智能家居之手搓一套能接入米家的零冷水装置](https://kexue.fm/archives/10869)
- [Transformer升级之路：1...](https://kexue.fm/archives/10862)

## COMMENTS

- [PengchengMa: 牛啊](https://kexue.fm/archives/10996/comment-page-1#comment-27811)
- [xczh: 已使用mean flow policy，一步推理效果确实惊人，...](https://kexue.fm/archives/10958/comment-page-1#comment-27810)
- [Cosine: 是不是因为shared experts每次都激活，而route...](https://kexue.fm/archives/10945/comment-page-1#comment-27809)
- [rpsun: 这样似乎与传统的经验正交函数之类的有相似之处。把样本的平均值减...](https://kexue.fm/archives/10699/comment-page-1#comment-27808)
- [贵阳机场接机: 怎么不更新啦](https://kexue.fm/archives/1490/comment-page-1#comment-27807)
- [czvzb: 具身智能模型目前主流也是在使用扩散和流匹配这类方法来预测动作。...](https://kexue.fm/archives/10958/comment-page-1#comment-27806)
- [Shawn\_yang: 苏神，关于您所说的：“推理阶段可以事先预估Routed Exp...](https://kexue.fm/archives/10945/comment-page-1#comment-27802)
- [OceanYU: 您好，关于由式（7）推导出高斯分布，我这里有一点问题，式（7）...](https://kexue.fm/archives/9164/comment-page-4#comment-27801)
- [jorjiang: 训练和prefill这个compute-bound阶段不做矩阵...](https://kexue.fm/archives/10907/comment-page-2#comment-27800)
- [amy: 苏老师，您有关注傅里叶旋转位置编码这篇工作吗，想知道您对这篇工...](https://kexue.fm/archives/10907/comment-page-2#comment-27799)

## USERLOGIN

- [登录](https://kexue.fm/admin/login.php)

[科学空间\|Scientific Spaces](https://kexue.fm)

- [登录](https://kexue.fm/admin/login.php)
- [打赏](https://kexue.fm/reward.html)
- [公式](https://kexue.fm/latex.html)
- [天象](https://kexue.fm/ac.html)
- [链接](https://kexue.fm/links.html)
- [时光](https://kexue.fm/me.html)
- [博览](https://kexue.fm/science.html)
- [归档](https://kexue.fm/content.html)

渴望成为一个小飞侠

- [欢迎订阅](https://kexue.fm/feed)
- [个性邮箱](https://kexue.fm/archives/119)
- [天象信息](https://kexue.fm/ac.html)
- [观测ISS](https://kexue.fm/archives/41)
- [LaTeX](https://kexue.fm/latex.html)
- [关于博主](https://kexue.fm/me.html)

欢迎访问“科学空间”，这里将与您共同探讨自然科学，回味人生百态；也期待大家的分享～

- [**千奇百怪** Everything](https://kexue.fm/category/Everything)
- [**天文探索** Astronomy](https://kexue.fm/category/Astronomy)
- [**数学研究** Mathematics](https://kexue.fm/category/Mathematics)
- [**物理化学** Phy-chem](https://kexue.fm/category/Phy-chem)
- [**信息时代** Big-Data](https://kexue.fm/category/Big-Data)
- [**生物自然** Biology](https://kexue.fm/category/Biology)
- [**图片摄影** Photograph](https://kexue.fm/category/Photograph)
- [**问题百科** Questions](https://kexue.fm/category/Questions)
- [**生活/情感** Life-Feeling](https://kexue.fm/category/Life-Feeling)
- [**资源共享** Resources](https://kexue.fm/category/Resources)

- [**千奇百怪**](https://kexue.fm/category/Everything)
- [**天文探索**](https://kexue.fm/category/Astronomy)
- [**数学研究**](https://kexue.fm/category/Mathematics)
- [**物理化学**](https://kexue.fm/category/Phy-chem)
- [**信息时代**](https://kexue.fm/category/Big-Data)
- [**生物自然**](https://kexue.fm/category/Biology)
- [**图片摄影**](https://kexue.fm/category/Photograph)
- [**问题百科**](https://kexue.fm/category/Questions)
- [**生活/情感**](https://kexue.fm/category/Life-Feeling)
- [**资源共享**](https://kexue.fm/category/Resources)

[首页](https://kexue.fm) [数学研究](https://kexue.fm/category/Mathematics) 通向最优分布之路：概率空间的最小化

6Aug

# [通向最优分布之路：概率空间的最小化](https://kexue.fm/archives/10289)

By 苏剑林 \|
2024-08-06 \|
27129位读者\|

当要求函数的最小值时，我们通常会先求导函数然后寻找其零点，比较幸运的情况下，这些零点之一正好是原函数的最小值点。如果是向量函数，则将导数改为梯度并求其零点。当梯度零点不易求得时，我们可以使用梯度下降来逐渐逼近最小值点。

以上这些都是无约束优化的基础结果，相信不少读者都有所了解。然而，本文的主题是概率空间中的优化，即目标函数的输入是一个概率分布，这类目标的优化更为复杂，因为它的搜索空间不再是无约束的，如果我们依旧去求解梯度零点或者执行梯度下降，所得结果未必能保证是一个概率分布。因此，我们需要寻找一种新的分析和计算方法，以确保优化结果能够符合概率分布的特性。

对此，笔者一直以来也感到颇为头疼，所以近来决定”痛定思痛“，针对概率分布的优化问题系统学习了一番，最后将学习所得整理在此，供大家参考。

## 梯度下降 [\#](https://kexue.fm/archives/10289\#%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D)

我们先来重温一下无约束优化的相关内容。首先，假设我们的目标是
\\begin{equation}\\boldsymbol{x}\_\* = \\mathop{\\text{argmin}}\_{\\boldsymbol{x}\\in\\mathbb{R}^n} F(\\boldsymbol{x})\\end{equation}
高中生都知道，要求函数的最值，往往先求导在让它等于零来找极值点，这对很多人来说已经成为“常识”。但这里不妨考考各位读者，有多少人能证明这个结论？换句话说，函数的最值为什么会跟“导数等于零”扯上关系呢？

### 搜索视角 [\#](https://kexue.fm/archives/10289\#%E6%90%9C%E7%B4%A2%E8%A7%86%E8%A7%92)

我们可以从搜索的视角来探究这个问题。假设我们当前所知的$\\boldsymbol{x}$为$\\boldsymbol{x}\_t$，我们怎么判断$\\boldsymbol{x}\_t$是不是最小值点呢？这个问题可以反过来思考：如果我们能找到$\\boldsymbol{x}\_{t+\\eta}$，使得$F(\\boldsymbol{x}\_{t+\\eta}) < F(\\boldsymbol{x}\_t)$，那么$\\boldsymbol{x}\_t$自然就不可能是最小值点了。为此，我们可以搜索如下格式的$\\boldsymbol{x}\_{t+\\eta}$：
\\begin{equation}\\boldsymbol{x}\_{t+\\eta} = \\boldsymbol{x}\_t + \\eta \\boldsymbol{u}\_t,\\quad 0 < \\eta \\ll 1\\end{equation}
当$F(\\boldsymbol{x})$足够光滑、$\\eta$足够小时，我们认为一阶近似的精度是够用的，于是可以利用一阶近似：
\\begin{equation}F(\\boldsymbol{x}\_{t+\\eta}) = F(\\boldsymbol{x}\_t + \\eta \\boldsymbol{u}\_t) \\approx F(\\boldsymbol{x}\_t) + \\eta \\boldsymbol{u}\_t \\cdot \\nabla\_{\\boldsymbol{x}\_t}F(\\boldsymbol{x}\_t)\\end{equation}
只要$\\nabla\_{\\boldsymbol{x}\_t}F(\\boldsymbol{x}\_t)\\neq 0$，我们我们就可以选取$\\boldsymbol{u}\_t = -\\nabla\_{\\boldsymbol{x}\_t}F(\\boldsymbol{x}\_t)$，使得
\\begin{equation}F(\\boldsymbol{x}\_{t+\\eta}) \\approx F(\\boldsymbol{x}\_t) - \\eta \\Vert\\nabla\_{\\boldsymbol{x}\_t}F(\\boldsymbol{x}\_t)\\Vert^2 < F(\\boldsymbol{x}\_t)\\end{equation}
这意味着，对于足够光滑的函数，它的最小值只能在满足$\\nabla\_{\\boldsymbol{x}\_t}F(\\boldsymbol{x}\_t) = 0$的点或者无穷远处取到，这也就是为什么求最值的第一步通常是“导数等于零”。如果$\\nabla\_{\\boldsymbol{x}\_t}F(\\boldsymbol{x}\_t) \\neq 0$，我们总可选择足够小的$\\eta$，通过
\\begin{equation}\\boldsymbol{x}\_{t+\\eta} = \\boldsymbol{x}\_t-\\eta\\nabla\_{\\boldsymbol{x}\_t}F(\\boldsymbol{x}\_t)\\label{eq:gd}\\end{equation}
来得到让$f$更小的点，这便是梯度下降。如果让$\\eta\\to 0$，我们可以得到ODE：
\\begin{equation}\\frac{d\\boldsymbol{x}\_t}{dt} = -\\nabla\_{\\boldsymbol{x}\_t}F(\\boldsymbol{x}\_t)\\end{equation}
这就是 [《梯度流：探索通向最小值之路》](https://kexue.fm/archives/9660) 介绍过的“梯度流”，它可以视为我们用梯度下降搜索最小值点的轨迹。

### 投影下降 [\#](https://kexue.fm/archives/10289\#%E6%8A%95%E5%BD%B1%E4%B8%8B%E9%99%8D)

刚才我们说的都是无约束优化，现在我们来简单讨论梯度下降在约束优化中的一个简单推广。假设我们面临的问题是：
\\begin{equation}\\boldsymbol{x}\_\* = \\mathop{\\text{argmin}}\_{\\boldsymbol{x}\\in\\mathbb{X}} F(\\boldsymbol{x})\\label{eq:c-loss}\\end{equation}
其中$\\mathbb{X}$是$\\mathbb{R}^n$的一个子集，如果是理论分析，通常要给$\\mathbb{X}$加上“有界凸集”的要求，但如果是简单了解的话，我们就可以先不管这些细节了。

如果此时我们仍用梯度下降$\\eqref{eq:gd}$，那么最大的问题就是无法保证$\\boldsymbol{x}\_{t+\\eta}\\in\\mathbb{X}$，但其实我们可以多加一步投影运算
\\begin{equation}\\Pi\_{\\mathbb{X}} (\\boldsymbol{y}) = \\mathop{\\text{argmin}}\_{\\boldsymbol{x}\\in\\mathbb{X}}\\Vert\\boldsymbol{x}-\\boldsymbol{y}\\Vert\\label{eq:project}\\end{equation}
从而形成“投影梯度下降”：
\\begin{equation}\\boldsymbol{x}\_{t+\\eta} = \\Pi\_{\\mathbb{X}}(\\boldsymbol{x}\_t-\\eta\\nabla\_{\\boldsymbol{x}\_t}F(\\boldsymbol{x}\_t))\\label{eq:pgd}\\end{equation}
说白了，投影梯度下降就是先梯度下降，然后在$\\mathbb{X}$中找到跟梯度下降结果最相近的点作为输出，这样就保证了输出结果一定在$\\mathbb{X}$内。在 [《让炼丹更科学一些（一）：SGD的平均损失收敛》](https://kexue.fm/archives/9902#%E5%9F%9F%E5%86%85%E6%8A%95%E5%BD%B1) 中我们证明了，在一定假设下，投影梯度下降可以找到约束优化问题$\\eqref{eq:c-loss}$的最优解。

从结果来看，投影梯度下降将约束优化$\\eqref{eq:c-loss}$转化为了“梯度下降+投影”两步，而投影$\\eqref{eq:project}$本身也是一个约束优化问题，虽然优化目标已经固定了，但仍属于未解决的问题，需要具体$\\mathbb{X}$具体分析，因此还需要进一步探索下去。

## 离散分布 [\#](https://kexue.fm/archives/10289\#%E7%A6%BB%E6%95%A3%E5%88%86%E5%B8%83)

本文聚焦于概率空间中的优化，即搜索对象必须是一个概率分布，这一节我们先关注离散分布，搜索空间我们记为$\\Delta^{n-1}$，它是全体$n$元离散型概率分布的集合，即
\\begin{equation}\\Delta^{n-1} = \\left\\{\\boldsymbol{p}=(p\_1,p\_2,\\cdots,p\_n)\\left\|\\, p\_1,p\_2,\\cdots,p\_n\\geq 0,\\sum\_{i=1}^n p\_i = 1\\right.\\right\\}\\end{equation}
我们的优化目标则是
\\begin{equation}\\boldsymbol{p}\_\* = \\mathop{\\text{argmin}}\_{\\boldsymbol{p}\\in\\Delta^{n-1}} F(\\boldsymbol{p})\\label{eq:p-loss}\\end{equation}

### 拉氏乘子 [\#](https://kexue.fm/archives/10289\#%E6%8B%89%E6%B0%8F%E4%B9%98%E5%AD%90)

对于等式或不等式约束下的优化问题，标准方法通常是“ [拉格朗日乘子法](https://en.wikipedia.org/wiki/Lagrange_multiplier)”，它将约束优化问题$\\eqref{eq:p-loss}$转化为一个弱约束的$\\text{min-max}$问题：
\\begin{equation}\\min\_{\\boldsymbol{p}\\in\\Delta^{n-1}} F(\\boldsymbol{p}) = \\min\_{\\boldsymbol{p}\\in\\mathbb{R}^n} \\max\_{\\mu\_i \\geq 0,\\lambda\\in\\mathbb{R}}F(\\boldsymbol{p}) - \\sum\_{i=1}^n \\mu\_i p\_i + \\lambda\\left(\\sum\_{i=1}^n p\_i - 1\\right)\\label{eq:min-max}\\end{equation}
注意在这个$\\text{min-max}$优化中，我们去掉了$\\boldsymbol{p}\\in\\Delta^{n-1}$这个约束，只是在$\\max$这一步有一个比较简单的$\\mu\_i \\geq 0$的约束。怎么证明右边的优化问题等价于左边呢？其实并不难，分三步来理解：

> 1、我们先要理解右端的$\\text{min-max}$含义：$\\min$在左，$\\max$在右，这意味着我们最终是要求一个尽可能小的结果，但这个目标函数是先要对某些变量取$\\max$；
>
> 2、当$p\_i < 0$是，那么$\\max$这一步必然有$\\mu\_i\\to\\infty$，此时结果目标函数值是$\\infty$，而如果$p\_i \\geq 0$，那么$\\max$这一步就必然有$\\mu\_i p\_i =0$，此时目标函数值是有限的，显然后者更小一点，因此当右端取最优值时$p\_i\\geq 0$成立，同理我们也可以证明$\\sum\_{i=1}^n p\_i = 1$成立；
>
> 3、由第2步的分析可知，当右端取最优值时，必然满足$\\boldsymbol{p}\\in\\Delta^{n-1}$，且多出来的项为零，那么就等价于左边的优化问题。

接下来要用到一个“ [Minimax定理](https://en.wikipedia.org/wiki/Minimax_theorem)”：

> 如果$\\mathbb{X},\\mathbb{Y}$是两个凸集，$\\boldsymbol{x}\\in\\mathbb{X},\\boldsymbol{y}\\in\\mathbb{Y}$，并且$f(\\boldsymbol{x},\\boldsymbol{y})$关于$\\boldsymbol{x}$是凸函数的（对于任意固定$\\boldsymbol{y}$），关于$\\boldsymbol{y}$是凹函数的（对于任意固定$\\boldsymbol{x}$），那么成立
> \\begin{equation}\\min\_{\\boldsymbol{x}\\in\\mathbb{X}}\\max\_{\\boldsymbol{y}\\in\\mathbb{Y}} f(\\boldsymbol{x},\\boldsymbol{y}) = \\max\_{\\boldsymbol{y}\\in\\mathbb{Y}}\\min\_{\\boldsymbol{x}\\in\\mathbb{X}} f(\\boldsymbol{x},\\boldsymbol{y})\\end{equation}

Minimax定理提供了$\\min,\\max$可交换的一个充分条件，这里边出现了一个新名词“凸集”，指的是集合内任意两点的加权平均，结果依然在集合内，即
\\begin{equation}(1-\\lambda)\\boldsymbol{x}\_1 + \\lambda \\boldsymbol{x}\_2\\in \\mathbb{X},\\qquad\\forall \\boldsymbol{x}\_1,\\boldsymbol{x}\_2\\in \\mathbb{X},\\quad\\forall \\lambda\\in \[0, 1\]\\end{equation}
由此可见凸集的条件并不是太苛刻，$\\mathbb{R}^n,\\Delta^{n-1}$都是凸集，还有全体非负数也是凸集，等等。

对于式$\\eqref{eq:min-max}$右端的目标函数，它关于$\\mu\_i,\\lambda$是一次函数，因此符合关于$\\mu\_i,\\lambda$是凹函数的条件，并且除开$F(\\boldsymbol{p})$外的项关于$\\boldsymbol{p}$也是一次的，所以整个目标函数关于$\\boldsymbol{p}$的凸性，等价于$F(\\boldsymbol{p})$关于$\\boldsymbol{p}$的凸性，即如果$F(\\boldsymbol{p})$是关于$\\boldsymbol{p}$的凸函数，那么式$\\eqref{eq:min-max}$的$\\min,\\max$就可以交换：
\\begin{equation}\\small\\min\_{\\boldsymbol{p}\\in\\mathbb{R}^n} \\max\_{\\mu\_i \\geq 0,\\lambda\\in\\mathbb{R}}F(\\boldsymbol{p}) - \\sum\_{i=1}^n \\mu\_i p\_i + \\lambda\\left(\\sum\_{i=1}^n p\_i - 1\\right) = \\max\_{\\mu\_i \\geq 0,\\lambda\\in\\mathbb{R}} \\min\_{\\boldsymbol{p}\\in\\mathbb{R}^n}
F(\\boldsymbol{p}) - \\sum\_{i=1}^n \\mu\_i p\_i + \\lambda\\left(\\sum\_{i=1}^n p\_i - 1\\right)\\end{equation}
这样我们就可以先对$\\boldsymbol{p}$求$\\min$了，这是一个无约束最小化问题，可以通过求解梯度等于零的方程组来完成，结果将带有参数$\\lambda$和$\\mu\_i$，最后通过$p\_i \\geq 0$、$\\mu\_i p\_i = 0$和$\\sum\_{i=1}^n p\_i = 1$来确定这些参数。

### 凸集搜索 [\#](https://kexue.fm/archives/10289\#%E5%87%B8%E9%9B%86%E6%90%9C%E7%B4%A2)

然而，尽管拉格朗日乘子法被视为求解约束优化问题的标准方法，但它并不算直观，而且它只能通过解方程来求得精确解，并不能导出类似梯度下降的迭代逼近算法，因此我们不能满足于拉格朗日乘子法。

从搜索的视角看，求解概率空间中的优化问题的关键，是保证在搜索过程中试探点都在集合$\\Delta^{n-1}$内。换句话说，假设当前概率分布为$\\boldsymbol{p}\_t\\in \\Delta^{n-1}$，我们怎么构造下一个试探点$\\boldsymbol{p}\_{t+\\eta}$呢？它有两个要求，一是$\\boldsymbol{p}\_{t+\\eta}\\in \\Delta^{n-1}$，二是可以通过控制$\\eta$的大小来控制它跟$\\boldsymbol{p}\_t$的接近程度。这时候$\\Delta^{n-1}$的“凸集”性质就派上用场了，利用这一性质我们可以将$\\boldsymbol{p}\_{t+\\eta}$定为
\\begin{equation}\\boldsymbol{p}\_{t+\\eta} = (1-\\eta)\\boldsymbol{p}\_t + \\eta \\boldsymbol{q}\_t,\\quad \\boldsymbol{q}\_t\\in \\Delta^{n-1}\\end{equation}
那么有
\\begin{equation}F(\\boldsymbol{p}\_{t+\\eta}) = F((1-\\eta)\\boldsymbol{p}\_t + \\eta \\boldsymbol{q}\_t) \\approx F(\\boldsymbol{p}\_t) + \\eta(\\boldsymbol{q}\_t - \\boldsymbol{p}\_t)\\cdot\\nabla\_{\\boldsymbol{p}\_t} F(\\boldsymbol{p}\_t)\\end{equation}
假设一阶近似的精度足够，那么要获得下降最快的方向，就相当于求解
\\begin{equation}\\mathop{\\text{argmin}}\_{\\boldsymbol{q}\_t\\in\\Delta^{n-1}}\\,\\boldsymbol{q}\_t\\cdot\\nabla\_{\\boldsymbol{p}\_t} F(\\boldsymbol{p}\_t) \\end{equation}
这个目标函数倒是很简单，答案是
\\begin{equation}\\boldsymbol{q}\_t = \\text{onehot}(\\text{argmin}(\\nabla\_{\\boldsymbol{p}\_t} F(\\boldsymbol{p}\_t)))\\end{equation}
这里$\\nabla\_{\\boldsymbol{p}\_t} F(\\boldsymbol{p}\_t)$是一个向量，对一个向量做$\\text{argmin}$指的是找出最小分量的位置。所以，上式也就是说$\\boldsymbol{q}\_t$是一个one hot分布，其中$1$所在的位置，就是梯度$\\nabla\_{\\boldsymbol{p}\_t} F(\\boldsymbol{p}\_t)$最小的分量所在的位置。

由此可见，概率空间的梯度下降形式是
\\begin{equation}\\boldsymbol{p}\_{t+\\eta} = (1 - \\eta)\\boldsymbol{p}\_t + \\eta\\, \\text{onehot}(\\text{argmin}(\\nabla\_{\\boldsymbol{p}\_t} F(\\boldsymbol{p}\_t)))\\end{equation}
以及$\\boldsymbol{p}\_t$是$F(\\boldsymbol{p}\_t)$极小值点的条件是：
\\begin{equation}\\boldsymbol{p}\_t\\cdot\\nabla\_{\\boldsymbol{p}\_t} F(\\boldsymbol{p}\_t) = (\\nabla\_{\\boldsymbol{p}\_t} F(\\boldsymbol{p}\_t))\_{\\min}\\label{eq:p-min}\\end{equation}
这里对向量的$\\min$是指返回最小的那个分量。

### 一个例子 [\#](https://kexue.fm/archives/10289\#%E4%B8%80%E4%B8%AA%E4%BE%8B%E5%AD%90)

以 [《通向概率分布之路：盘点Softmax及其替代品》](https://kexue.fm/archives/10145) 介绍的Sparsemax为例，它的原始定义为
\\begin{equation}Sparsemax(\\boldsymbol{x}) = \\mathop{\\text{argmin}}\\limits\_{\\boldsymbol{p}\\in\\Delta^{n-1}}\\Vert \\boldsymbol{p} - \\boldsymbol{x}\\Vert^2\\end{equation}
其中$\\boldsymbol{x}\\in\\mathbb{R}^n$。不难发现，从前面投影梯度下降的角度来看，Sparsemax正好是从$\\mathbb{R}^n$到$\\Delta^{n-1}$的“投影”操作。

我们记$F(\\boldsymbol{p})=\\Vert \\boldsymbol{p} - \\boldsymbol{x}\\Vert^2$，它对$\\boldsymbol{p}$的梯度是$2(\\boldsymbol{p} - \\boldsymbol{x})$，所以根据式$\\eqref{eq:p-min}$，极小值点满足的方程就是
\\begin{equation}\\boldsymbol{p}\\cdot(\\boldsymbol{p}-\\boldsymbol{x}) = (\\boldsymbol{p}-\\boldsymbol{x})\_{\\min}\\end{equation}
我们约定$x\_i = x\_j\\Leftrightarrow p\_i = p\_j$，这里没有加粗的下标如$p\_i$表示向量$\\boldsymbol{p}$的第$i$个分量（即是一个标量），前一节加粗的下标如$\\boldsymbol{p}\_t$表示$\\boldsymbol{p}$的第$t$次迭代结果（即还是一个向量），请读者细心区分。

在该约定下，由上式可以得到
\\begin{equation}p\_i > 0 \\quad \\Leftrightarrow \\quad p\_i-x\_i = (\\boldsymbol{p}-\\boldsymbol{x})\_{\\min}\\end{equation}
因为$\\boldsymbol{p}$可以由$\\boldsymbol{x}$确定，所以$(\\boldsymbol{p}-\\boldsymbol{x})\_{\\min}$是$\\boldsymbol{x}$的函数，我们记为$-\\lambda(\\boldsymbol{x})$，那么$p\_i = x\_i - \\lambda(\\boldsymbol{x})$，但这只是对于$p\_i > 0$成立，对于$p\_i=0$，我们有$p\_i-x\_i > (\\boldsymbol{p}-\\boldsymbol{x})\_{\\min}$，即$x\_i - \\lambda(\\boldsymbol{x}) < 0$。基于这两点，我们可以统一记
\\begin{equation}p\_i = \\text{relu}(x\_i - \\lambda(\\boldsymbol{x}))\\end{equation}
其中$\\lambda(\\boldsymbol{x})$由$\\boldsymbol{p}$的各分量之和为1来确定，其他细节内容请参考 [《通向概率分布之路：盘点Softmax及其替代品》](https://kexue.fm/archives/10145)。

## 连续分布 [\#](https://kexue.fm/archives/10289\#%E8%BF%9E%E7%BB%AD%E5%88%86%E5%B8%83)

说完离散分布，接下来我们就转到连续分布了。看上去连续型分布只是离散型分布的极限版本，结果似乎不应该有太大差别，但事实上它们之间的特性有着本质不同，以至于我们需要为连续分布构建全新的方法论。

### 目标泛函 [\#](https://kexue.fm/archives/10289\#%E7%9B%AE%E6%A0%87%E6%B3%9B%E5%87%BD)

首先我们来说说目标函数。我们知道，描述连续型分布的方式是概率密度函数，所以此时目标函数的输入是一个概率密度函数，而此时的目标函数其实也不是普通的函数了，我们通常称之为“泛函”——从一整个函数到一个标量的映射。换言之，我们需要寻找一个概率密度函数，使得某个目标泛函最小化。

尽管很多人觉得“泛函分析心犯寒”，但实际上我们大部份人都接触过泛函，因为满足“输入函数，输出标量”的映射太多了，比如定积分
\\begin{equation}\\mathcal{I}\[f\]\\triangleq \\int\_a^b f(x) dx\\end{equation}
就是一个函数到标量的映射，所以它也是泛函。事实上，我们在实际应用中会遇到的泛函，基本上都是由定积分构建出来的，比如概率分布的KL散度：
\\begin{equation}\\mathcal{KL}\[p\\Vert q\] = \\int p(\\boldsymbol{x})\\log \\frac{p(\\boldsymbol{x})}{q(\\boldsymbol{x})}d\\boldsymbol{x}\\end{equation}
其中积分默认在全空间（整个$\\mathbb{R}^n$）进行。更一般的泛函的被积函数里边可能还包含导数项，如理论物理中的最小作用量：
\\begin{equation}\\mathcal{A}\[x\] = \\int\_{t\_a}^{t\_b} L(x(t),x'(t),t)dt\\end{equation}

而接下来我们要最小化的目标泛函，则可以一般地写成
\\begin{equation}\\mathcal{F}\[p\] = \\int F(p(\\boldsymbol{x}))d\\boldsymbol{x}\\end{equation}
方便起见，我们还可以定义泛函导数
\\begin{equation}\\frac{\\delta\\mathcal{F}\[p\]}{\\delta p}(\\boldsymbol{x}) = \\frac{\\partial F(p\_t(\\boldsymbol{x}))}{\\partial p\_t(\\boldsymbol{x})}\\end{equation}

### 紧支撑集 [\#](https://kexue.fm/archives/10289\#%E7%B4%A7%E6%94%AF%E6%92%91%E9%9B%86)

此外，我们还需要一个连续型概率空间的记号，它的基本定义是
\\begin{equation}\\mathbb{P} = \\left\\{p(\\boldsymbol{x}) \\,\\Bigg\|\\, p(\\boldsymbol{x})\\geq 0(\\forall\\boldsymbol{x}\\in\\mathbb{R}^n),\\int p(\\boldsymbol{x})d\\boldsymbol{x} = 1\\right\\}\\end{equation}
不难证明，如果概率密度函数$p(\\boldsymbol{x})$的极限$\\lim\_{\\Vert\\boldsymbol{x}\\Vert\\to\\infty} p(\\boldsymbol{x})$存在，那么必然有$\\lim\_{\\Vert\\boldsymbol{x}\\Vert\\to\\infty} p(\\boldsymbol{x}) = 0$，这也是后面的证明中要用到的一个性质。

然而，可以举例证明的是，并非所有概率密度函数在无穷远处都存在极限。为了避免理论上的困难，我们通常在理论证明时假设$p(\\boldsymbol{x})$的支撑集是紧集。这里边又有两个概念： **支撑集（Support）** 和 **紧集（Compact Set）**，支撑集指的是让$p(\\boldsymbol{x}) > 0$的全体$\\boldsymbol{x}$的集合，即
\\begin{equation}\\text{supp}(p) = \\{\\boldsymbol{x} \| p(\\boldsymbol{x}) > 0\\}\\end{equation}
紧集的一般定义比较复杂，不过在$\\mathbb{R}^n$中，紧集等价于有界闭集。所以说白了，$p(\\boldsymbol{x})$的支撑集是紧集的假设，直接作用是让$p(\\boldsymbol{x})$具有“存在常数$C$，使得$\\forall \|\\boldsymbol{x}\| > C$都有$p(\\boldsymbol{x}) = 0$”的性质，简化了$p(\\boldsymbol{x})$在无穷远处的性态，从根本上避免了$\\lim\_{\\Vert\\boldsymbol{x}\\Vert\\to\\infty} p(\\boldsymbol{x}) = 0$的讨论。

从理论上来看，这个假设是非常强的，它甚至排除了像正态分布这样的简单分布（正态分布的支撑集是$\\mathbb{R}^n$）。不过，从实践上来说，这个假设并不算离谱，因为我们说了如果极限$\\lim\_{\\Vert\\boldsymbol{x}\\Vert\\to\\infty} p(\\boldsymbol{x})$存在就必然为零，因此在超出一定范围后它就跟等于零没有太大区别了。极限不存在的例子确实有，但一般都需要比较刻意构造，对于我们实际能遇到的数据，基本上都满足极限存在的条件。

### 旧路不通 [\#](https://kexue.fm/archives/10289\#%E6%97%A7%E8%B7%AF%E4%B8%8D%E9%80%9A)

直觉上，连续分布的优化应该是照搬离散分布的思路，即设$\\boldsymbol{p}\_{t+\\eta}(\\boldsymbol{x}) = (1 - \\eta)\\boldsymbol{p}\_t(\\boldsymbol{x}) + \\eta \\boldsymbol{q}\_t(\\boldsymbol{x})$，因为跟离散分布一样，连续分布的概率密度函数集$\\mathbb{P}$同样是一个凸集。现在我们将它代入目标泛函
\\begin{equation}\\begin{aligned}
\\mathcal{F}\[p\_{t+\\eta}\] =&\\, \\int F(p\_{t+\\eta}(\\boldsymbol{x}))d\\boldsymbol{x} \\\
=&\\, \\int F((1 - \\eta)\\boldsymbol{p}\_t(\\boldsymbol{x}) + \\eta \\boldsymbol{q}\_t(\\boldsymbol{x}))d\\boldsymbol{x} \\\
\\approx&\\,\\int \\left\[F(p\_t(\\boldsymbol{x})) + \\eta\\frac{\\partial F(p\_t(\\boldsymbol{x}))}{\\partial p\_t(\\boldsymbol{x})}\\Big(q\_t(\\boldsymbol{x}) - p\_t(\\boldsymbol{x})\\Big)\\right\]d\\boldsymbol{x}
\\end{aligned}\\end{equation}
假设一阶近似够用，那么问题转化为
\\begin{equation}\\mathop{\\text{argmin}}\_{q\_t\\in \\mathbb{P}}\\int\\frac{\\partial F(p\_t(\\boldsymbol{x}))}{\\partial p\_t(\\boldsymbol{x})}q\_t(\\boldsymbol{x})d\\boldsymbol{x}\\end{equation}
这个问题倒也是不难解，答案跟离散分布的one hot类似
\\begin{equation}q\_t(\\boldsymbol{x}) = \\delta\\left(\\boldsymbol{x} - \\mathop{\\text{argmin}}\_{\\boldsymbol{x}'} \\frac{\\partial F(p\_t(\\boldsymbol{x}'))}{\\partial p\_t(\\boldsymbol{x}')}\\right)\\end{equation}
这里的$\\delta(\\cdot)$是 [狄拉克delta函数](https://kexue.fm/archives/1870)，表示单点分布的概率密度。

看上去很顺利，然而实际上此路并不通。首先，狄拉克delta函数并不是常规意义下的函数，它是广义函数（也是泛函的一种）；其次，如果我们用普通函数的视角去看的话，狄拉克delta函数在某点处具有无穷大的值，而既然是无穷大的值，那么推导过程中的“一阶近似够用”的假设就不可能成立了。

### 变量代换 [\#](https://kexue.fm/archives/10289\#%E5%8F%98%E9%87%8F%E4%BB%A3%E6%8D%A2)

我们可以考虑继续对上一节的推导做一些修补，比如加上$q\_t(\\boldsymbol{x}) \\leq C$的限制，以获得有意义的结果，然而这种缝缝补补的做法终究显得不够优雅。可是如果不利用凸集的性质，又该如何构建下一步的试探分布$\\boldsymbol{p}\_{t+\\eta}(\\boldsymbol{x})$呢？

这时候就要充分发挥概率密度函数的特性了——我们可以通过变量代换，来将一个概率密度函数变换为另一个概率密度函数，这是连续型分布的独有性质。具体来说，如果$p(\\boldsymbol{x})$是一个概率密度函数，$\\boldsymbol{y}=\\boldsymbol{T}(\\boldsymbol{x})$是一个可逆变换，那么$p(\\boldsymbol{T}(\\boldsymbol{x}))\\left\|\\frac{\\partial \\boldsymbol{T}(\\boldsymbol{x})}{\\partial\\boldsymbol{x}}\\right\|$同样是一个概率密度函数，其中$\|\\cdot\|$表示矩阵的行列式绝对值。

基于这个特性，我们将下一步要试探的概率分布定义为
\\begin{equation}\\begin{aligned}
p\_{t+\\eta}(\\boldsymbol{x}) =&\\, p\_t(\\boldsymbol{x} + \\eta\\boldsymbol{\\mu}\_t(\\boldsymbol{x}))\\left\|\\boldsymbol{I} + \\eta\\frac{\\partial \\boldsymbol{\\mu}\_t(\\boldsymbol{x})}{\\partial\\boldsymbol{x}}\\right\| \\\
\\approx &\\, \\Big\[p\_t(\\boldsymbol{x}) + \\eta\\boldsymbol{\\mu}\_t(\\boldsymbol{x})\\cdot\\nabla\_{\\boldsymbol{x}} p\_t(\\boldsymbol{x})\\Big\]\\left\[1 + \\eta\\,\\text{Tr}\\frac{\\partial \\boldsymbol{\\mu}\_t(\\boldsymbol{x})}{\\partial\\boldsymbol{x}}\\right\] \\\\[3pt\]
\\approx &\\, p\_t(\\boldsymbol{x}) + \\eta\\boldsymbol{\\mu}\_t(\\boldsymbol{x})\\cdot\\nabla\_{\\boldsymbol{x}} p\_t(\\boldsymbol{x}) + \\eta\\, p\_t(\\boldsymbol{x})\\nabla\_{\\boldsymbol{x}}\\cdot\\boldsymbol{\\mu}\_t(\\boldsymbol{x}) \\\\[5pt\]
= &\\, p\_t(\\boldsymbol{x}) + \\eta\\nabla\_{\\boldsymbol{x}}\\cdot\\big\[p\_t(\\boldsymbol{x})\\boldsymbol{\\mu}\_t(\\boldsymbol{x})\\big\] \\\
\\end{aligned}\\end{equation}
同样的结果我们在 [《生成扩散模型漫谈（十二）：“硬刚”扩散ODE》](https://kexue.fm/archives/9280) 已经推导过，其中行列式的近似展开，可以参考 [《行列式的导数》](https://kexue.fm/archives/2383) 一文。

### 积分变换 [\#](https://kexue.fm/archives/10289\#%E7%A7%AF%E5%88%86%E5%8F%98%E6%8D%A2)

利用这个新的$p\_{t+\\eta}(\\boldsymbol{x})$，我们可以得到
\\begin{equation}\\begin{aligned}
\\mathcal{F}\[p\_{t+\\eta}\] =&\\, \\int F(p\_{t+\\eta}(\\boldsymbol{x}))d\\boldsymbol{x} \\\
\\approx&\\, \\int F\\Big(p\_t(\\boldsymbol{x}) + \\eta\\nabla\_{\\boldsymbol{x}}\\cdot\\big\[p\_t(\\boldsymbol{x})\\boldsymbol{\\mu}\_t(\\boldsymbol{x})\\big\]\\Big)d\\boldsymbol{x} \\\
\\approx&\\, \\int \\left\[F(p\_t(\\boldsymbol{x})) + \\eta\\frac{\\partial F(p\_t(\\boldsymbol{x}))}{\\partial p\_t(\\boldsymbol{x})}\\nabla\_{\\boldsymbol{x}}\\cdot\\big\[p\_t(\\boldsymbol{x})\\boldsymbol{\\mu}\_t(\\boldsymbol{x})\\big\]\\right\]d\\boldsymbol{x} \\\
=&\\, \\mathcal{F}\[p\_t\] + \\eta\\int \\frac{\\partial F(p\_t(\\boldsymbol{x}))}{\\partial p\_t(\\boldsymbol{x})}\\nabla\_{\\boldsymbol{x}}\\cdot\\big\[p\_t(\\boldsymbol{x})\\boldsymbol{\\mu}\_t(\\boldsymbol{x})\\big\] d\\boldsymbol{x} \\\
\\end{aligned}\\label{eq:px-approx}\\end{equation}
接下来需要像 [《测试函数法推导连续性方程和Fokker-Planck方程》](https://kexue.fm/archives/9461) 一样，推导一个概率密度相关的积分恒等式。首先我们有
\\begin{equation}\\begin{aligned}
&\\,\\int \\nabla\_{\\boldsymbol{x}}\\cdot\\left\[\\frac{\\partial F(p\_t(\\boldsymbol{x}))}{\\partial p\_t(\\boldsymbol{x})} p\_t(\\boldsymbol{x})\\boldsymbol{\\mu}\_t(\\boldsymbol{x})\\right\] d\\boldsymbol{x} \\\\[5pt\]
=&\\, \\int \\frac{\\partial F(p\_t(\\boldsymbol{x}))}{\\partial p\_t(\\boldsymbol{x})}\\nabla\_{\\boldsymbol{x}}\\cdot\\big\[p\_t(\\boldsymbol{x})\\boldsymbol{\\mu}\_t(\\boldsymbol{x})\\big\] d\\boldsymbol{x} + \\int \\left(\\nabla\_{\\boldsymbol{x}}\\frac{\\partial F(p\_t(\\boldsymbol{x}))}{\\partial p\_t(\\boldsymbol{x})}\\right)\\cdot\\big\[p\_t(\\boldsymbol{x})\\boldsymbol{\\mu}\_t(\\boldsymbol{x})\\big\] d\\boldsymbol{x}
\\end{aligned}\\end{equation}
根据 [散度定理](https://en.wikipedia.org/wiki/Divergence_theorem)，我们有
\\begin{equation}\\int\_{\\Omega} \\nabla\_{\\boldsymbol{x}}\\cdot\\left\[\\frac{\\partial F(p\_t(\\boldsymbol{x}))}{\\partial p\_t(\\boldsymbol{x})} p\_t(\\boldsymbol{x})\\boldsymbol{\\mu}\_t(\\boldsymbol{x})\\right\] d\\boldsymbol{x} = \\int\_{\\partial\\Omega} \\left\[\\frac{\\partial F(p\_t(\\boldsymbol{x}))}{\\partial p\_t(\\boldsymbol{x})} p\_t(\\boldsymbol{x})\\boldsymbol{\\mu}\_t(\\boldsymbol{x})\\right\]\\cdot \\hat{\\boldsymbol{n}} dS\\end{equation}
其中$\\Omega$是积分区域，在这里是整个$\\mathbb{R}^n$，$\\partial\\Omega$是区域边界，$\\mathbb{R}^n$的边界自然是无穷远处，$\\hat{\\boldsymbol{n}}$是边界的外向单位法向量，$dS$是面积微元。在紧支撑集的假设下，无穷远处$p\_t(\\boldsymbol{x})=0$，所以上式右端实际上就是零的积分，结果是零。因此我们有
\\begin{equation}\\int \\frac{\\partial F(p\_t(\\boldsymbol{x}))}{\\partial p\_t(\\boldsymbol{x})}\\nabla\_{\\boldsymbol{x}}\\cdot\\big\[p\_t(\\boldsymbol{x})\\boldsymbol{\\mu}\_t(\\boldsymbol{x})\\big\] d\\boldsymbol{x} = - \\int \\left(\\nabla\_{\\boldsymbol{x}}\\frac{\\partial F(p\_t(\\boldsymbol{x}))}{\\partial p\_t(\\boldsymbol{x})}\\right)\\cdot\\big\[p\_t(\\boldsymbol{x})\\boldsymbol{\\mu}\_t(\\boldsymbol{x})\\big\] d\\boldsymbol{x}\\end{equation}
代入式$\\eqref{eq:px-approx}$得到
\\begin{equation}\\mathcal{F}\[p\_{t+\\eta}\] \\approx \\mathcal{F}\[p\_t\] - \\eta\\int \\left(p\_t(\\boldsymbol{x})\\nabla\_{\\boldsymbol{x}}\\frac{\\partial F(p\_t(\\boldsymbol{x}))}{\\partial p\_t(\\boldsymbol{x})}\\right)\\cdot \\boldsymbol{\\mu}\_t(\\boldsymbol{x}) d\\boldsymbol{x} \\label{eq:px-approx-2}\\end{equation}

### 梯度之流 [\#](https://kexue.fm/archives/10289\#%E6%A2%AF%E5%BA%A6%E4%B9%8B%E6%B5%81)

根据式$\\eqref{eq:px-approx-2}$，让$\\mathcal{F}\[p\_{t+\\eta}\] \\leq \\mathcal{F}\[p\_t\]$的一个简单选择是
\\begin{equation}\\boldsymbol{\\mu}\_t(\\boldsymbol{x}) = \\nabla\_{\\boldsymbol{x}}\\frac{\\partial F(p\_t(\\boldsymbol{x}))}{\\partial p\_t(\\boldsymbol{x})}\\end{equation}
相应的迭代格式是
\\begin{equation}p\_{t+\\eta}(\\boldsymbol{x}) \\approx p\_t(\\boldsymbol{x}) + \\eta\\nabla\_{\\boldsymbol{x}}\\cdot\\left\[p\_t(\\boldsymbol{x})\\nabla\_{\\boldsymbol{x}}\\frac{\\partial F(p\_t(\\boldsymbol{x}))}{\\partial p\_t(\\boldsymbol{x})}\\right\] \\end{equation}
我们还可以取$\\eta\\to 0$的极限得
\\begin{equation}\\frac{\\partial}{\\partial t}p\_t(\\boldsymbol{x}) = \\nabla\_{\\boldsymbol{x}}\\cdot\\left\[p\_t(\\boldsymbol{x})\\nabla\_{\\boldsymbol{x}}\\frac{\\partial F(p\_t(\\boldsymbol{x}))}{\\partial p\_t(\\boldsymbol{x})}\\right\] \\end{equation}
或者简写成
\\begin{equation}\\frac{\\partial p\_t}{\\partial t} = \\nabla\\cdot\\left\[p\_t\\nabla\\frac{\\delta \\mathcal{F}\[p\_t\]}{\\delta p\_t}\\right\] \\end{equation}
这就是 [《梯度流：探索通向最小值之路》](https://kexue.fm/archives/9660) 介绍过的Wasserstein梯度流，但这里我们没有引入Wasserstein距离的概念也得到了相同的结果。

由于$p\_{t+\\eta}(\\boldsymbol{x})$是$p\_t(\\boldsymbol{x})$通过变换$\\boldsymbol{x}\\to \\boldsymbol{x} + \\eta \\boldsymbol{\\mu}\_t(\\boldsymbol{x})$得到的，所以我们还可以写出$\\boldsymbol{x}$的运动轨迹ODE：
\\begin{equation}\\boldsymbol{x}\_t = \\boldsymbol{x}\_{t+\\eta} + \\eta \\boldsymbol{\\mu}\_t(\\boldsymbol{x}\_{t+\\eta})\\quad\\Rightarrow\\quad \\frac{d\\boldsymbol{x}\_t}{dt} = -\\boldsymbol{\\mu}\_t(\\boldsymbol{x}\_t) = -\\nabla\_{\\boldsymbol{x}}\\frac{\\partial F(p\_t(\\boldsymbol{x}))}{\\partial p\_t(\\boldsymbol{x})}\\end{equation}
这个ODE的意义是，从分布$p\_0(\\boldsymbol{x})$的采样结果$\\boldsymbol{x}\_0$出发，按照此ODE运动到$\\boldsymbol{x}\_t$时，$\\boldsymbol{x}\_t$所服从的分布正是$p\_t(\\boldsymbol{x})$。

## 文章小结 [\#](https://kexue.fm/archives/10289\#%E6%96%87%E7%AB%A0%E5%B0%8F%E7%BB%93)

本文系统整理了概率空间中目标函数的最小化方法，包括取到极小值的必要条件、类似梯度下降的迭代法等内容，相关结果在最优化、生成模型（尤其是扩散模型）等场景中时有用到。

_**转载到请包括本文地址：** [https://kexue.fm/archives/10289](https://kexue.fm/archives/10289)_

_**更详细的转载事宜请参考：**_ [《科学空间FAQ》](https://kexue.fm/archives/6508#%E6%96%87%E7%AB%A0%E5%A6%82%E4%BD%95%E8%BD%AC%E8%BD%BD/%E5%BC%95%E7%94%A8)

**如果您还有什么疑惑或建议，欢迎在下方评论区继续讨论。**

**如果您觉得本文还不错，欢迎 [分享](https://kexue.fm/archives/10289#share)/ [打赏](https://kexue.fm/archives/10289#pay) 本文。打赏并非要从中获得收益，而是希望知道科学空间获得了多少读者的真心关注。当然，如果你无视它，也不会影响你的阅读。再次表示欢迎和感谢！**

打赏

微信打赏

支付宝打赏

因为网站后台对打赏并无记录，因此欢迎在打赏时候备注留言。你还可以 [**点击这里**](http://mail.qq.com/cgi-bin/qm_share?t=qm_mailme&email=tN7d1drY3drrx8H0xcWa19vZ) 或在下方评论区留言来告知你的建议或需求。

**如果您需要引用本文，请参考：**

苏剑林. (Aug. 06, 2024). 《通向最优分布之路：概率空间的最小化 》\[Blog post\]. Retrieved from [https://kexue.fm/archives/10289](https://kexue.fm/archives/10289)

@online{kexuefm-10289,
        title={通向最优分布之路：概率空间的最小化},
        author={苏剑林},
        year={2024},
        month={Aug},
        url={\\url{https://kexue.fm/archives/10289}},
}

分类： [数学研究](https://kexue.fm/category/Mathematics)    标签： [概率](https://kexue.fm/tag/%E6%A6%82%E7%8E%87/), [优化](https://kexue.fm/tag/%E4%BC%98%E5%8C%96/), [梯度](https://kexue.fm/tag/%E6%A2%AF%E5%BA%A6/), [扩散](https://kexue.fm/tag/%E6%89%A9%E6%95%A3/)[6 评论](https://kexue.fm/archives/10289#comments)

< [对齐全量微调！这是我看过最精彩的LoRA改进（二）](https://kexue.fm/archives/10266) \| [“Cool Papers + 站内搜索”的一些新尝试](https://kexue.fm/archives/10311) >

### 你也许还对下面的内容感兴趣

- [生成扩散模型漫谈（三十）：从瞬时速度到平均速度](https://kexue.fm/archives/10958)
- [MoE环游记：5、均匀分布的反思](https://kexue.fm/archives/10945)
- [Transformer升级之路：20、MLA究竟好在哪里？](https://kexue.fm/archives/10907)
- [一道概率不等式：盯着它到显然成立为止！](https://kexue.fm/archives/10902)
- [SVD的导数](https://kexue.fm/archives/10878)
- [通过梯度近似寻找Normalization的替代品](https://kexue.fm/archives/10831)
- [MoE环游记：4、难处应当多投入](https://kexue.fm/archives/10815)
- [高阶muP：更简明但更高明的谱条件缩放](https://kexue.fm/archives/10795)
- [初探muP：超参数的跨模型尺度迁移规律](https://kexue.fm/archives/10770)
- [MoE环游记：3、换个思路来分配](https://kexue.fm/archives/10757)

[发表你的看法](https://kexue.fm/archives/10289#comment_form)

bluseking

September 19th, 2024

苏老师，文中的这句话

“这时候就要充分发挥概率密度函数的特性了——我们可以通过变量代换，来将一个概率密度函数变换为另一个概率密度函数，这是连续型分布的独有性质。具体来说，如果$p(\\boldsymbol{x})$是一个概率密度函数，$\\boldsymbol{y}=\\boldsymbol{T}(\\boldsymbol{x})$是一个可逆变换，那么$p(\\boldsymbol{T}(\\boldsymbol{x}))\\left\|\\frac{\\partial \\boldsymbol{T}(\\boldsymbol{x})}{\\partial\\boldsymbol{x}}\\right\|$同样是一个概率密度函数”

这里是不是写错了，如果是概率密度函数的变换，应该是$p(\\boldsymbol{x})\\left\|\\frac{\\partial(\\boldsymbol{x})}{\\partial\\boldsymbol{y}}\\right\|$，这里应该要用到逆变换，
你写的这个应该是积分中的变量替换的公式，下述的公式36是不是要改一下，不知道我的理解是不是正确

[回复评论](https://kexue.fm/archives/10289/comment-page-1?replyTo=25243#respond-post-10289)

bluseking 发表于
September 19th, 2024

苏老师，您是对的，我看错了

[回复评论](https://kexue.fm/archives/10289/comment-page-1?replyTo=25249#respond-post-10289)

bluseking

September 20th, 2024

苏老师，$p\_{t+\\eta}(\\boldsymbol{x}) =\\, p\_t(\\boldsymbol{x} + \\eta\\boldsymbol{\\mu}(\\boldsymbol{x}))\\left\|\\boldsymbol{I} + \\eta\\frac{\\partial \\boldsymbol{\\mu}(\\boldsymbol{x})}{\\partial\\boldsymbol{x}}\\right\|$，这里是不是应该换成 $p\_{t}(\\boldsymbol{x}) =\\, p\_{t+\\eta}(\\boldsymbol{x} + \\eta\\boldsymbol{\\mu}(\\boldsymbol{x}))\\left\|\\boldsymbol{I} + \\eta\\frac{\\partial \\boldsymbol{\\mu}(\\boldsymbol{x})}{\\partial\\boldsymbol{x}}\\right\|$,因为状态是从$\\boldsymbol{x}\\to \\boldsymbol{x} + \\eta \\boldsymbol{\\mu}(\\boldsymbol{x})$,即是 $\\boldsymbol{x}\_{t+\\eta} = \\boldsymbol{x}\_t + \\eta \\boldsymbol{\\mu}(\\boldsymbol{x}\_t)$?

[回复评论](https://kexue.fm/archives/10289/comment-page-1?replyTo=25250#respond-post-10289)

[苏剑林](https://kexue.fm) 发表于
September 20th, 2024

呃，我这里其实没把它当作变量代换用，而是将它当作一种构造新分布的方法。不过你指出的也对，但真正要改的是最后的运动方程，右端要多加个负号（目前已经加上）

[回复评论](https://kexue.fm/archives/10289/comment-page-1?replyTo=25262#respond-post-10289)

huleeee

October 22nd, 2024

苏神你好，拜读了你的这篇博客，对于你那个n 元离散型概率分布的定义（10）式感觉困惑，这里如果是n元的，那么每一个元的独立性不会让∑i=1npi=1成立的，这里是否应该是1元的离散型概率分布，只是离散成了n个值来表示。

[回复评论](https://kexue.fm/archives/10289/comment-page-1?replyTo=25546#respond-post-10289)

[苏剑林](https://kexue.fm) 发表于
October 25th, 2024

你这样理解为，这里说的$n$元离散分布就是$(10)$式所定义的东西，而不是你头脑里想的另外一个东西。

[回复评论](https://kexue.fm/archives/10289/comment-page-1?replyTo=25558#respond-post-10289)

[取消回复](https://kexue.fm/archives/10289#respond-post-10289)

你的大名

电子邮箱

个人网站（选填）

1\. 可以使用LaTeX代码，点击“预览效果”可查看效果；2. 可以通过点击评论楼层编号来引用该楼层；3. 网站可能会有点卡，如非确认评论失败，请不要重复点击提交。

### 内容速览

[梯度下降](https://kexue.fm/archives/10289#%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D)
[搜索视角](https://kexue.fm/archives/10289#%E6%90%9C%E7%B4%A2%E8%A7%86%E8%A7%92)
[投影下降](https://kexue.fm/archives/10289#%E6%8A%95%E5%BD%B1%E4%B8%8B%E9%99%8D)
[离散分布](https://kexue.fm/archives/10289#%E7%A6%BB%E6%95%A3%E5%88%86%E5%B8%83)
[拉氏乘子](https://kexue.fm/archives/10289#%E6%8B%89%E6%B0%8F%E4%B9%98%E5%AD%90)
[凸集搜索](https://kexue.fm/archives/10289#%E5%87%B8%E9%9B%86%E6%90%9C%E7%B4%A2)
[一个例子](https://kexue.fm/archives/10289#%E4%B8%80%E4%B8%AA%E4%BE%8B%E5%AD%90)
[连续分布](https://kexue.fm/archives/10289#%E8%BF%9E%E7%BB%AD%E5%88%86%E5%B8%83)
[目标泛函](https://kexue.fm/archives/10289#%E7%9B%AE%E6%A0%87%E6%B3%9B%E5%87%BD)
[紧支撑集](https://kexue.fm/archives/10289#%E7%B4%A7%E6%94%AF%E6%92%91%E9%9B%86)
[旧路不通](https://kexue.fm/archives/10289#%E6%97%A7%E8%B7%AF%E4%B8%8D%E9%80%9A)
[变量代换](https://kexue.fm/archives/10289#%E5%8F%98%E9%87%8F%E4%BB%A3%E6%8D%A2)
[积分变换](https://kexue.fm/archives/10289#%E7%A7%AF%E5%88%86%E5%8F%98%E6%8D%A2)
[梯度之流](https://kexue.fm/archives/10289#%E6%A2%AF%E5%BA%A6%E4%B9%8B%E6%B5%81)
[文章小结](https://kexue.fm/archives/10289#%E6%96%87%E7%AB%A0%E5%B0%8F%E7%BB%93)

### 智能搜索

支持整句搜索！网站自动使用 [结巴分词](https://github.com/fxsjy/jieba) 进行分词，并结合ngrams排序算法给出合理的搜索结果。

### 热门标签

[生成模型](https://kexue.fm/tag/%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B/) [attention](https://kexue.fm/tag/attention/) [优化](https://kexue.fm/tag/%E4%BC%98%E5%8C%96/) [语言模型](https://kexue.fm/tag/%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/) [模型](https://kexue.fm/tag/%E6%A8%A1%E5%9E%8B/) [网站](https://kexue.fm/tag/%E7%BD%91%E7%AB%99/) [概率](https://kexue.fm/tag/%E6%A6%82%E7%8E%87/) [梯度](https://kexue.fm/tag/%E6%A2%AF%E5%BA%A6/) [转载](https://kexue.fm/tag/%E8%BD%AC%E8%BD%BD/) [微分方程](https://kexue.fm/tag/%E5%BE%AE%E5%88%86%E6%96%B9%E7%A8%8B/) [天象](https://kexue.fm/tag/%E5%A4%A9%E8%B1%A1/) [矩阵](https://kexue.fm/tag/%E7%9F%A9%E9%98%B5/) [分析](https://kexue.fm/tag/%E5%88%86%E6%9E%90/) [深度学习](https://kexue.fm/tag/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/) [积分](https://kexue.fm/tag/%E7%A7%AF%E5%88%86/) [python](https://kexue.fm/tag/python/) [优化器](https://kexue.fm/tag/%E4%BC%98%E5%8C%96%E5%99%A8/) [力学](https://kexue.fm/tag/%E5%8A%9B%E5%AD%A6/) [无监督](https://kexue.fm/tag/%E6%97%A0%E7%9B%91%E7%9D%A3/) [扩散](https://kexue.fm/tag/%E6%89%A9%E6%95%A3/) [几何](https://kexue.fm/tag/%E5%87%A0%E4%BD%95/) [节日](https://kexue.fm/tag/%E8%8A%82%E6%97%A5/) [生活](https://kexue.fm/tag/%E7%94%9F%E6%B4%BB/) [文本生成](https://kexue.fm/tag/%E6%96%87%E6%9C%AC%E7%94%9F%E6%88%90/) [数论](https://kexue.fm/tag/%E6%95%B0%E8%AE%BA/)

### 随机文章

- [训练集、验证集和测试集的意义](https://kexue.fm/archives/4638)
- [为节约而生：从标准Attention到稀疏Attention](https://kexue.fm/archives/6853)
- [引力透镜——用经典力学推导光的偏转公式](https://kexue.fm/archives/1592)
- [眼见未必为实——“视超光速”现象的产生](https://kexue.fm/archives/671)
- [ChildTuning：试试把Dropout加到梯度上去？](https://kexue.fm/archives/8764)
- [第一学期结束了](https://kexue.fm/archives/352)
- [在bert4keras中使用混合精度和XLA加速训练](https://kexue.fm/archives/9059)
- [简单的迅雷VIP账号获取器（Python）](https://kexue.fm/archives/3594)
- [无监督语义相似度哪家强？我们做了个比较全面的评测](https://kexue.fm/archives/8321)
- [椭圆内的一根定长弦（化圆法）](https://kexue.fm/archives/1654)

### 最近评论

- [PengchengMa](https://kexue.fm/archives/10996/comment-page-1#comment-27811): 牛啊
- [xczh](https://kexue.fm/archives/10958/comment-page-1#comment-27810): 已使用mean flow policy，一步推理效果确实惊人，性能跟多步推理的diffusio...
- [Cosine](https://kexue.fm/archives/10945/comment-page-1#comment-27809): 是不是因为shared experts每次都激活，而routed experts是依概率被选中...
- [rpsun](https://kexue.fm/archives/10699/comment-page-1#comment-27808): 这样似乎与传统的经验正交函数之类的有相似之处。把样本的平均值减掉之后做正交分解。那么如果单纯地...
- [贵阳机场接机](https://kexue.fm/archives/1490/comment-page-1#comment-27807): 怎么不更新啦
- [czvzb](https://kexue.fm/archives/10958/comment-page-1#comment-27806): 具身智能模型目前主流也是在使用扩散和流匹配这类方法来预测动作。
苏神推荐你看这几篇文章：
1....
- [Shawn\_yang](https://kexue.fm/archives/10945/comment-page-1#comment-27802): 苏神，关于您所说的：“推理阶段可以事先预估Routed Expert的实际分布，只要细致地进行...
- [OceanYU](https://kexue.fm/archives/9164/comment-page-4#comment-27801): 您好，关于由式（7）推导出高斯分布，我这里有一点问题，式（7）只能保证关于x\_t-1是二次函数...
- [jorjiang](https://kexue.fm/archives/10907/comment-page-2#comment-27800): 训练和prefill这个compute-bound阶段不做矩阵吸收，这个用我这个解释更好理解了...
- [amy](https://kexue.fm/archives/10907/comment-page-2#comment-27799): 苏老师，您有关注傅里叶旋转位置编码这篇工作吗，想知道您对这篇工作的看法是什么，这篇工作可以wo...

### 友情链接

- [Cool Papers](https://papers.cool)
- [数学研发](https://bbs.emath.ac.cn)
- [Seatop](http://www.seatop.com.cn/)
- [Xiaoxia](https://xiaoxia.org/)
- [积分表-网络版](https://kexue.fm/sci/integral/index.html)
- [丝路博傲](http://blog.dvxj.com/)
- [ph4ntasy 饭特稀](http://www.ph4ntasy.com/)
- [数学之家](http://www.2math.cn/)
- [有趣天文奇观](http://interesting-sky.china-vo.org/)
- [TwistedW](http://www.twistedwg.com/)
- [godweiyang](https://godweiyang.com/)
- [AI柠檬](https://blog.ailemon.net/)
- [王登科-DK博客](https://greatdk.com)
- [ESON](https://blog.eson.org/)
- [枫之羽](https://fzhiy.net/)
- [Mathor's blog](https://wmathor.com/)
- [coding-zuo](https://coding-zuo.github.io/)
- [博科园](https://www.bokeyuan.net/)
- [孔皮皮的博客](https://www.kppkkp.top/)
- [运鹏的博客](https://yunpengtai.top/)
- [jiming.site](https://jiming.site/)
- [OmegaXYZ](https://www.omegaxyz.com/)
- [Blog by Eacls](https://www.eacls.top/)
- [EAI猩球](https://www.robotech.ink/)
- [文举的博客](https://liwenju0.com/)
- [用代码打点酱油](https://bruceyuan.com/)
- [申请链接](https://kexue.fm/links.html)

本站采用创作共用版权协议，要求署名、非商业用途和保持一致。转载本站内容必须也遵循“ [署名-非商业用途-保持一致](http://creativecommons.org/licenses/by-nc-nd/2.5/cn/)”的创作共用协议。
© 2009-2025 Scientific Spaces. All rights reserved. Theme by [laogui](http://www.laogui.com). Powered by [Typecho](http://typecho.org). 备案号: [粤ICP备09093259号-1/2](https://beian.miit.gov.cn/)。