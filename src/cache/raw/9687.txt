## SEARCH

## MENU

- [打赏](https://kexue.fm/reward.html)
- [公式](https://kexue.fm/latex.html)
- [天象](https://kexue.fm/ac.html)
- [链接](https://kexue.fm/links.html)
- [时光](https://kexue.fm/me.html)
- [博览](https://kexue.fm/science.html)
- [归档](https://kexue.fm/content.html)

## CATEGORIES

- [千奇百怪](https://kexue.fm/category/Everything)
- [天文探索](https://kexue.fm/category/Astronomy)
- [数学研究](https://kexue.fm/category/Mathematics)
- [物理化学](https://kexue.fm/category/Phy-chem)
- [信息时代](https://kexue.fm/category/Big-Data)
- [生物自然](https://kexue.fm/category/Biology)
- [图片摄影](https://kexue.fm/category/Photograph)
- [问题百科](https://kexue.fm/category/Questions)
- [生活/情感](https://kexue.fm/category/Life-Feeling)
- [资源共享](https://kexue.fm/category/Resources)

## NEWPOSTS

- [从无穷范数求导到等值振荡定理](https://kexue.fm/archives/10972)
- [生成扩散模型漫谈（三十）：从瞬时速...](https://kexue.fm/archives/10958)
- [MoE环游记：5、均匀分布的反思](https://kexue.fm/archives/10945)
- [msign算子的Newton-Sc...](https://kexue.fm/archives/10922)
- [Transformer升级之路：2...](https://kexue.fm/archives/10907)
- [一道概率不等式：盯着它到显然成立为止！](https://kexue.fm/archives/10902)
- [SVD的导数](https://kexue.fm/archives/10878)
- [智能家居之手搓一套能接入米家的零冷水装置](https://kexue.fm/archives/10869)
- [Transformer升级之路：1...](https://kexue.fm/archives/10862)
- [矩阵的有效秩（Effective ...](https://kexue.fm/archives/10847)

## COMMENTS

- [OceanYU: 您好，关于由式（7）推导出高斯分布，我这里有一点问题，式（7）...](https://kexue.fm/archives/9164/comment-page-4#comment-27801)
- [jorjiang: 训练和prefill这个compute-bound阶段不做矩阵...](https://kexue.fm/archives/10907/comment-page-2#comment-27800)
- [amy: 苏老师，您有关注傅里叶旋转位置编码这篇工作吗，想知道您对这篇工...](https://kexue.fm/archives/10907/comment-page-2#comment-27799)
- [jiurizz: 在2\*shared experts + 160\*routed ...](https://kexue.fm/archives/10945/comment-page-1#comment-27798)
- [开水: 感谢苏老师回复，论文appendix里面写了实验是load b...](https://kexue.fm/archives/10945/comment-page-1#comment-27797)
- [苏剑林: 欢迎作者，这篇文章确实在收藏夹了，结果还没来得及看，抱歉哈，马...](https://kexue.fm/archives/10958/comment-page-1#comment-27796)
- [苏剑林: “将存储kv cache改为存储降维后的Embedding X...](https://kexue.fm/archives/10907/comment-page-2#comment-27795)
- [苏剑林: 呃，是这样的，Moonlight-16B-A3B实际上对应的是...](https://kexue.fm/archives/10945/comment-page-1#comment-27794)
- [Jiaming Song: 推销一下我们前一段时间的工作，这个其实已经达到了你说的三个标准...](https://kexue.fm/archives/10958/comment-page-1#comment-27793)
- [ZhouTimeMachine: 感谢您的回复！这么一看，我才注意到能从 $p(x\_0)$ 变换...](https://kexue.fm/archives/9497/comment-page-2#comment-27792)

## USERLOGIN

- [登录](https://kexue.fm/admin/login.php)

[科学空间\|Scientific Spaces](https://kexue.fm)

- [登录](https://kexue.fm/admin/login.php)
- [打赏](https://kexue.fm/reward.html)
- [公式](https://kexue.fm/latex.html)
- [天象](https://kexue.fm/ac.html)
- [链接](https://kexue.fm/links.html)
- [时光](https://kexue.fm/me.html)
- [博览](https://kexue.fm/science.html)
- [归档](https://kexue.fm/content.html)

渴望成为一个小飞侠

- [欢迎订阅](https://kexue.fm/feed)
- [个性邮箱](https://kexue.fm/archives/119)
- [天象信息](https://kexue.fm/ac.html)
- [观测ISS](https://kexue.fm/archives/41)
- [LaTeX](https://kexue.fm/latex.html)
- [关于博主](https://kexue.fm/me.html)

欢迎访问“科学空间”，这里将与您共同探讨自然科学，回味人生百态；也期待大家的分享～

- [**千奇百怪** Everything](https://kexue.fm/category/Everything)
- [**天文探索** Astronomy](https://kexue.fm/category/Astronomy)
- [**数学研究** Mathematics](https://kexue.fm/category/Mathematics)
- [**物理化学** Phy-chem](https://kexue.fm/category/Phy-chem)
- [**信息时代** Big-Data](https://kexue.fm/category/Big-Data)
- [**生物自然** Biology](https://kexue.fm/category/Biology)
- [**图片摄影** Photograph](https://kexue.fm/category/Photograph)
- [**问题百科** Questions](https://kexue.fm/category/Questions)
- [**生活/情感** Life-Feeling](https://kexue.fm/category/Life-Feeling)
- [**资源共享** Resources](https://kexue.fm/category/Resources)

- [**千奇百怪**](https://kexue.fm/category/Everything)
- [**天文探索**](https://kexue.fm/category/Astronomy)
- [**数学研究**](https://kexue.fm/category/Mathematics)
- [**物理化学**](https://kexue.fm/category/Phy-chem)
- [**信息时代**](https://kexue.fm/category/Big-Data)
- [**生物自然**](https://kexue.fm/category/Biology)
- [**图片摄影**](https://kexue.fm/category/Photograph)
- [**问题百科**](https://kexue.fm/category/Questions)
- [**生活/情感**](https://kexue.fm/category/Life-Feeling)
- [**资源共享**](https://kexue.fm/category/Resources)

[首页](https://kexue.fm) [信息时代](https://kexue.fm/category/Big-Data) 当生成模型肆虐：互联网将有“疯牛病”之忧？

14Jul

# [当生成模型肆虐：互联网将有“疯牛病”之忧？](https://kexue.fm/archives/9687)

By 苏剑林 \|
2023-07-14 \|
68833位读者\|

众所周知，不管是文本还是视觉领域，各种生成模型正在以无法阻挡的势头“肆虐”互联网。虽然大家都明白，实现真正的通用人工智能（AGI）还有很长的路要走，但这并不妨碍人们越来越频繁地利用生成模型来创作和分享内容。君不见，很多网络文章已经配上了Stable Diffusion模型生成的插图；君不见，很多新闻风格已经越来越显现出ChatGPT的影子。看似无害的这种趋势，正悄然引发了一个问题：我们是否应该对互联网上充斥的生成模型数据保持警惕？

近期发表的论文 [《Self-Consuming Generative Models Go MAD》](https://papers.cool/arxiv/2307.01850) 揭示了一种令人担忧的可能性，那就是生成模型正在互联网上的无节制扩张，可能会导致一场数字版的“疯牛病”疫情。本文一起学习这篇论文，探讨其可能带来的影响。

## “食自己” [\#](https://kexue.fm/archives/9687\#%E2%80%9C%E9%A3%9F%E8%87%AA%E5%B7%B1%E2%80%9D)

一方面，人们使用生成模型的频率越来越高，将会导致互联网上由生成模型创作的内容越来越多；另一方面，生成模型也在更新迭代，其所用的数据也是从互联网爬取的，可以想像，后面的训练集中由生成模型创作的部分占比将会越来越高。换句话说，后面的每一代模型迭代时可能都没有足够多的新鲜数据，纯粹是用自己生产的数据来训，用广东话说就是“食自己”，这将导致模型的质量或者多样性越来越差，原论文称之为“模型自噬紊乱（Model Autophagy Disorder，MAD）”。

无独有偶，生物学上也曾出现了类似的例子。牛是草食动物，然而，一些畜牧业者为了增强其营养供应，将其他牛的残骸（包括大脑）粉碎并混入饲料中。这在当时看起来是一个机智的做法，但未曾想到最后导致了“ [疯牛症](https://zh.wikipedia.org/zh-cn/%E7%89%9B%E8%85%A6%E6%B5%B7%E7%B6%BF%E7%8B%80%E7%97%85%E8%AE%8A)”的出现和大规模传播。这一事例说明，长期的“食自己”可能会导致有害因素累积在生物体内，一旦达到一定程度，甚至可能触发灾难性的疾病。

因此，我们同样需要反思生成模型的“肆虐”是否会在互联网上引发另一场“疯牛症”——这不仅可能导致信息的同质化，使得各种内容开始变得千篇一律，缺乏原创性和多样性，还有可能引发一系列无法预见的问题。

## 降多样性 [\#](https://kexue.fm/archives/9687\#%E9%99%8D%E5%A4%9A%E6%A0%B7%E6%80%A7)

可能有读者会产生疑问：生成模型不就是对真实数据分布的模拟吗？即便连续地使用生成模型的数据进行迭代训练，应该只是在重复呈现真实的数据分布，怎么会导致多样性的丧失呢？

这其中的原因是多方面的。首先，训练生成模型的数据往往并非直接取自真实分布，而是经过人为的加工处理，比如去噪、规范化和对齐。经过加工后，训练集就已经丧失了部分多样性。例如，我们之所以能观察到很多新闻报道或知乎回答都有一股ChatGPT的味道，并非是因为内容本身，而是因为它们的格式与ChatGPT的相似性，这就说明ChatGPT的训练数据和输出结果的风格都比较明显且局限。再比如，为了降低图像生成模型的训练难度，我们通常需要对图像进行对齐处理，如在训练人脸生成模型时，常常需要将所有人脸的眼睛对齐到同一位置，这些操作也导致了多样性的丧失。

此外，还有一个很关键的因素是，由于生成模型本身或者训练技巧等限制，每个生成模型都无法做到完美，此时我们通常会主动地引入一些牺牲多样性来提高生成质量的技巧。比如，对于GAN、 [Flow](https://kexue.fm/archives/5807) 等生成模型，我们会选择降低采样噪声的方差，以获得质量更高的生成结果，这就是所谓的截断技巧或退火技巧。另外，如 [《生成扩散模型漫谈（九）：条件控制生成结果》](https://kexue.fm/archives/9257) 所述，在扩散模型中我们通常引入条件信息以控制输出结果，不管是Classifier-Guidance还是Classifier-Free方案，额外条件的引入也会限制生成结果的多样性。总而言之，在生成模型不尽完美时，我们在平衡质量与多样性的过程中，就主动地放弃了部分多样性。

## 正态分布 [\#](https://kexue.fm/archives/9687\#%E6%AD%A3%E6%80%81%E5%88%86%E5%B8%83)

为了更深刻地认识到这种现象，我们接下来将探讨一些具体的例子。作为开始，我们首先考虑的是正态分布，因为它足够简单，所以求解和分析都更加清晰。但后面我们可以观察到，结果已经足够有代表性了。

假设真实分布是多元正态分布$\\mathcal{N}(\\boldsymbol{\\mu}\_0,\\boldsymbol{\\Sigma}\_0)$，我们用来建模的分布也是正态分布$\\mathcal{N}(\\boldsymbol{\\mu},\\boldsymbol{\\Sigma})$，那么训练模型的过程，就是从训练集里边估计均值向量$\\boldsymbol{\\mu}$和协方差矩阵$\\boldsymbol{\\Sigma}$。接下来我们假设每一代生成模型训练时，都只用到上一代生成模型创作的数据，这是比较极端的假设，但不可否认当生成模型进一步普及时，这个假设越来越接近成立。

在这些假设下，我们从$t-1$代生成模型$\\mathcal{N}(\\boldsymbol{\\mu}\_{t-1},\\boldsymbol{\\Sigma}\_{t-1})$中采样$n$个样本$\\boldsymbol{x}\_{t-1}^{(1)},\\boldsymbol{x}\_{t-1}^{(2)},\\cdots,\\boldsymbol{x}\_{t-1}^{(n)}$，来训练第$t$代的生成模型：
\\begin{equation}\\boldsymbol{\\mu}\_t = \\frac{1}{n}\\sum\_{i=1}^n \\boldsymbol{x}\_{t-1}^{(i)},\\quad \\boldsymbol{\\Sigma}\_t=\\frac{1}{n-1} \\sum\_{i=1}^n \\big(\\boldsymbol{x}\_{t-1}^{(i)} - \\boldsymbol{\\mu}\_t\\big)\\big(\\boldsymbol{x}\_{t-1}^{(i)} - \\boldsymbol{\\mu}\_t\\big)^{\\top}\\end{equation}
注意，如果加上截断技巧，那么第$t$代的生成模型就是$\\mathcal{N}(\\boldsymbol{\\mu}\_t,\\lambda\\boldsymbol{\\Sigma}\_t)$，其中$\\lambda\\in(0,1)$。于是可以想象，每一代的方差（多样性）都将以$\\lambda$的比率衰减下去，最后变成零（完全丧失多样性）。如果不使用截断技巧（即$\\lambda=1$）是不是就没事了？并不是。根据定义$\\boldsymbol{\\mu}\_t = \\frac{1}{n}\\sum\\limits\_{i=1}^n \\boldsymbol{x}\_{t-1}^{(i)}$，由于$\\boldsymbol{x}\_{t-1}^{(i)}$都是随机采样得到的，所以$\\boldsymbol{\\mu}\_t$也是一个随机变量，根据正态分布的叠加性，它实际上服从
\\begin{equation}\\boldsymbol{\\mu}\_t \\sim \\mathcal{N}\\left(\\boldsymbol{\\mu}\_{t-1},\\frac{1}{n}\\boldsymbol{\\Sigma}\_{t-1}\\right)\\quad\\Rightarrow\\quad\\boldsymbol{\\mu}\_t \\sim \\mathcal{N}\\left(\\boldsymbol{\\mu}\_0,\\frac{t}{n}\\boldsymbol{\\Sigma}\_0\\right)\\end{equation}
可以预见，当$t$足够大时，$\\boldsymbol{\\mu}\_t$本身就会明显偏离$\\boldsymbol{\\mu}\_0$，这对应的是质量的崩溃，而不单单是多样性的降低。

总的来说，截断技巧的引入，会大大加速多样性的丧失速度，而即便没有截断技巧，在长期有限样本的迭代训练中，生成分布也有可能明显偏离原始的真实分布。注意，正态分布这个例子所做的假设已经比一般的生成模型要弱得多，至少它的拟合能力是保证足够的，但这依然不可避免多样性衰减或者质量崩溃，而对于真实世界的数据和能力有限的生成模型来说，理论上只会更加糟糕。

## 生成模型 [\#](https://kexue.fm/archives/9687\#%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B)

对于实际的生成模型，理论分析难以进行，所以只能通过实验来探索结果了。原论文做了非常丰富的实验，结果基本上跟正态分布的结论一致，即如果加入截断技巧的话，多样性将会迅速丧失，即使没有截断技巧，经过反复迭代后的模型依然会不可避免地出现一些偏离。

这是带有截断技巧的一个例子：

带截断技巧，第1代生成结果

带截断技巧，第5代生成结果

这是不带截断技巧的一个例子：

不带截断技巧，第1代生成结果

不带截断技巧，第7代生成结果

当然，“每一轮的迭代只用上一轮的模型生成的数据”这个假设比较极端，原论文还分析了每一轮都包含一定数量的真实数据的情况，这个情况有包含两个子情况：1、真实数据的采样结果一开始就恒定不变；2、每次迭代都能采样到新鲜的真实数据。第1种方式比较容易实现，但原论文显示它只能减缓退化的速度，无法从根本上解决这个问题；第2种方式虽然可以解决退化问题，但在实际背景下，我们却很难有效筛选出真实数据和模型生成的数据。

## 文章小结 [\#](https://kexue.fm/archives/9687\#%E6%96%87%E7%AB%A0%E5%B0%8F%E7%BB%93)

本文探讨了当各种生成模型大规模“肆虐”互联网时可能出现的后果，在生成模型反复用自己生成的数据进行更新迭代时，可能导致信息严重同质化、丧失多样性的问题，类似于曾经因“牛吃牛”而出现的“疯牛病”。

_**转载到请包括本文地址：** [https://kexue.fm/archives/9687](https://kexue.fm/archives/9687)_

_**更详细的转载事宜请参考：**_ [《科学空间FAQ》](https://kexue.fm/archives/6508#%E6%96%87%E7%AB%A0%E5%A6%82%E4%BD%95%E8%BD%AC%E8%BD%BD/%E5%BC%95%E7%94%A8)

**如果您还有什么疑惑或建议，欢迎在下方评论区继续讨论。**

**如果您觉得本文还不错，欢迎 [分享](https://kexue.fm/archives/9687#share)/ [打赏](https://kexue.fm/archives/9687#pay) 本文。打赏并非要从中获得收益，而是希望知道科学空间获得了多少读者的真心关注。当然，如果你无视它，也不会影响你的阅读。再次表示欢迎和感谢！**

打赏

微信打赏

支付宝打赏

因为网站后台对打赏并无记录，因此欢迎在打赏时候备注留言。你还可以 [**点击这里**](http://mail.qq.com/cgi-bin/qm_share?t=qm_mailme&email=tN7d1drY3drrx8H0xcWa19vZ) 或在下方评论区留言来告知你的建议或需求。

**如果您需要引用本文，请参考：**

苏剑林. (Jul. 14, 2023). 《当生成模型肆虐：互联网将有“疯牛病”之忧？ 》\[Blog post\]. Retrieved from [https://kexue.fm/archives/9687](https://kexue.fm/archives/9687)

@online{kexuefm-9687,
        title={当生成模型肆虐：互联网将有“疯牛病”之忧？},
        author={苏剑林},
        year={2023},
        month={Jul},
        url={\\url{https://kexue.fm/archives/9687}},
}

分类： [信息时代](https://kexue.fm/category/Big-Data)    标签： [生成模型](https://kexue.fm/tag/%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B/)[35 评论](https://kexue.fm/archives/9687#comments)

< [Transformer升级之路：10、RoPE是一种β进制编码](https://kexue.fm/archives/9675) \| [语言模型输出端共享Embedding的重新探索](https://kexue.fm/archives/9698) >

### 你也许还对下面的内容感兴趣

- [生成扩散模型漫谈（三十）：从瞬时速度到平均速度](https://kexue.fm/archives/10958)
- [Transformer升级之路：20、MLA究竟好在哪里？](https://kexue.fm/archives/10907)
- [生成扩散模型漫谈（二十九）：用DDPM来离散编码](https://kexue.fm/archives/10711)
- [细水长flow之TARFLOW：流模型满血归来？](https://kexue.fm/archives/10667)
- [生成扩散模型漫谈（二十八）：分步理解一致性模型](https://kexue.fm/archives/10633)
- [生成扩散模型漫谈（二十七）：将步长作为条件输入](https://kexue.fm/archives/10617)
- [生成扩散模型漫谈（二十六）：基于恒等式的蒸馏（下）](https://kexue.fm/archives/10567)
- [VQ的又一技巧：给编码表加一个线性变换](https://kexue.fm/archives/10519)
- [VQ的旋转技巧：梯度直通估计的一般推广](https://kexue.fm/archives/10489)
- [“闭门造车”之多模态思路浅谈（二）：自回归](https://kexue.fm/archives/10197)

[发表你的看法](https://kexue.fm/archives/9687#comment_form)

1. [«](https://kexue.fm/archives/9687/comment-page-1#comments)
2. [1](https://kexue.fm/archives/9687/comment-page-1#comments)
3. [2](https://kexue.fm/archives/9687/comment-page-2#comments)

李鸿伟

July 21st, 2023

公式(2)的右边那个$\\Sigma\_{t-1}$？

[回复评论](https://kexue.fm/archives/9687/comment-page-2?replyTo=22314#respond-post-9687)

[苏剑林](https://kexue.fm) 发表于
July 22nd, 2023

已经修改，应该是$\\boldsymbol{\\Sigma}\_0$。

[回复评论](https://kexue.fm/archives/9687/comment-page-2?replyTo=22326#respond-post-9687)

jianzhe

July 21st, 2023

苏神 问个哲学问题 为啥地球物种的繁衍不会产生上述问题，几千年来保持着多样性？

[回复评论](https://kexue.fm/archives/9687/comment-page-2?replyTo=22315#respond-post-9687)

[苏剑林](https://kexue.fm) 发表于
July 22nd, 2023

参考 [@yanyan\|comment-22239](https://kexue.fm/archives/9687/comment-page-1#comment-22239) 的讨论

[回复评论](https://kexue.fm/archives/9687/comment-page-2?replyTo=22327#respond-post-9687)

one

July 24th, 2023

如果训练生成模型时加入判别机制，比如用上每个数据的reward，是不是不会有这个问题了？

[回复评论](https://kexue.fm/archives/9687/comment-page-2?replyTo=22342#respond-post-9687)

[苏剑林](https://kexue.fm) 发表于
July 27th, 2023

现在主要是问题是训练数据的污染，“训练生成模型时加入判别机制”似乎不能解决这个问题。

[回复评论](https://kexue.fm/archives/9687/comment-page-2?replyTo=22354#respond-post-9687)

flyingdoubleG

October 17th, 2023

μt∼N(μ0,t/nΣ0)(2)想请教下这一步是怎么推出来的，为啥方差是t/nΣ0

[回复评论](https://kexue.fm/archives/9687/comment-page-2?replyTo=22904#respond-post-9687)

[苏剑林](https://kexue.fm) 发表于
October 21st, 2023

正态分布方差的可加性

[回复评论](https://kexue.fm/archives/9687/comment-page-2?replyTo=22920#respond-post-9687)

skbl5694 发表于
December 24th, 2023

请问苏神，为什么不是根据∑1=1/n\*∑0，∑2=1/n\*∑1这种迭代的形式，而是相加呢？ 不同代的均值的分布为什么是相加的形式呢？

[回复评论](https://kexue.fm/archives/9687/comment-page-2?replyTo=23334#respond-post-9687)

[苏剑林](https://kexue.fm) 发表于
December 26th, 2023

不大理解你要表达什么，下式
$$\\boldsymbol{\\mu}\_t \\sim \\mathcal{N}\\left(\\boldsymbol{\\mu}\_{t-1},\\frac{1}{n}\\boldsymbol{\\Sigma}\_{t-1}\\right)\\quad\\Rightarrow\\quad\\boldsymbol{\\mu}\_t \\sim \\mathcal{N}\\left(\\boldsymbol{\\mu}\_0,\\frac{t}{n}\\boldsymbol{\\Sigma}\_0\\right)$$
是严格成立的数学推理呀，有什么问题？

[回复评论](https://kexue.fm/archives/9687/comment-page-2?replyTo=23353#respond-post-9687)

1. [«](https://kexue.fm/archives/9687/comment-page-1#comments)
2. [1](https://kexue.fm/archives/9687/comment-page-1#comments)
3. [2](https://kexue.fm/archives/9687/comment-page-2#comments)

[取消回复](https://kexue.fm/archives/9687#respond-post-9687)

你的大名

电子邮箱

个人网站（选填）

1\. 可以使用LaTeX代码，点击“预览效果”可查看效果；2. 可以通过点击评论楼层编号来引用该楼层；3. 网站可能会有点卡，如非确认评论失败，请不要重复点击提交。

### 内容速览

[“食自己”](https://kexue.fm/archives/9687#%E2%80%9C%E9%A3%9F%E8%87%AA%E5%B7%B1%E2%80%9D)
[降多样性](https://kexue.fm/archives/9687#%E9%99%8D%E5%A4%9A%E6%A0%B7%E6%80%A7)
[正态分布](https://kexue.fm/archives/9687#%E6%AD%A3%E6%80%81%E5%88%86%E5%B8%83)
[生成模型](https://kexue.fm/archives/9687#%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B)
[文章小结](https://kexue.fm/archives/9687#%E6%96%87%E7%AB%A0%E5%B0%8F%E7%BB%93)

### 智能搜索

支持整句搜索！网站自动使用 [结巴分词](https://github.com/fxsjy/jieba) 进行分词，并结合ngrams排序算法给出合理的搜索结果。

### 热门标签

[生成模型](https://kexue.fm/tag/%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B/) [attention](https://kexue.fm/tag/attention/) [优化](https://kexue.fm/tag/%E4%BC%98%E5%8C%96/) [语言模型](https://kexue.fm/tag/%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/) [模型](https://kexue.fm/tag/%E6%A8%A1%E5%9E%8B/) [网站](https://kexue.fm/tag/%E7%BD%91%E7%AB%99/) [概率](https://kexue.fm/tag/%E6%A6%82%E7%8E%87/) [梯度](https://kexue.fm/tag/%E6%A2%AF%E5%BA%A6/) [转载](https://kexue.fm/tag/%E8%BD%AC%E8%BD%BD/) [微分方程](https://kexue.fm/tag/%E5%BE%AE%E5%88%86%E6%96%B9%E7%A8%8B/) [天象](https://kexue.fm/tag/%E5%A4%A9%E8%B1%A1/) [矩阵](https://kexue.fm/tag/%E7%9F%A9%E9%98%B5/) [分析](https://kexue.fm/tag/%E5%88%86%E6%9E%90/) [深度学习](https://kexue.fm/tag/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/) [积分](https://kexue.fm/tag/%E7%A7%AF%E5%88%86/) [python](https://kexue.fm/tag/python/) [力学](https://kexue.fm/tag/%E5%8A%9B%E5%AD%A6/) [无监督](https://kexue.fm/tag/%E6%97%A0%E7%9B%91%E7%9D%A3/) [优化器](https://kexue.fm/tag/%E4%BC%98%E5%8C%96%E5%99%A8/) [扩散](https://kexue.fm/tag/%E6%89%A9%E6%95%A3/) [几何](https://kexue.fm/tag/%E5%87%A0%E4%BD%95/) [节日](https://kexue.fm/tag/%E8%8A%82%E6%97%A5/) [生活](https://kexue.fm/tag/%E7%94%9F%E6%B4%BB/) [文本生成](https://kexue.fm/tag/%E6%96%87%E6%9C%AC%E7%94%9F%E6%88%90/) [数论](https://kexue.fm/tag/%E6%95%B0%E8%AE%BA/)

### 随机文章

- [无穷级数求和的积分审敛法](https://kexue.fm/archives/68)
- [今天你食了吗？（广东云浮观测日偏食之旅）](https://kexue.fm/archives/30)
- [我的自主招生成绩公布了](https://kexue.fm/archives/1556)
- [从一个单位向量变换到另一个单位向量的正交矩阵](https://kexue.fm/archives/8453)
- [从无穷范数求导到等值振荡定理](https://kexue.fm/archives/10972)
- [生成扩散模型漫谈（三十）：从瞬时速度到平均速度](https://kexue.fm/archives/10958)
- [简单的迅雷VIP账号获取器（Python）](https://kexue.fm/archives/3594)
- [生成扩散模型漫谈（二十五）：基于恒等式的蒸馏（上）](https://kexue.fm/archives/10085)
- [BytePiece：更纯粹、更高压缩率的Tokenizer](https://kexue.fm/archives/9752)
- [高斯型积分的微扰展开（一）](https://kexue.fm/archives/3217)

### 最近评论

- [OceanYU](https://kexue.fm/archives/9164/comment-page-4#comment-27801): 您好，关于由式（7）推导出高斯分布，我这里有一点问题，式（7）只能保证关于x\_t-1是二次函数...
- [jorjiang](https://kexue.fm/archives/10907/comment-page-2#comment-27800): 训练和prefill这个compute-bound阶段不做矩阵吸收，这个用我这个解释更好理解了...
- [amy](https://kexue.fm/archives/10907/comment-page-2#comment-27799): 苏老师，您有关注傅里叶旋转位置编码这篇工作吗，想知道您对这篇工作的看法是什么，这篇工作可以wo...
- [jiurizz](https://kexue.fm/archives/10945/comment-page-1#comment-27798): 在2\*shared experts + 160\*routed expert + top6的配置...
- [开水](https://kexue.fm/archives/10945/comment-page-1#comment-27797): 感谢苏老师回复，论文appendix里面写了实验是load balance loss，正交lo...
- [苏剑林](https://kexue.fm/archives/10958/comment-page-1#comment-27796): 欢迎作者，这篇文章确实在收藏夹了，结果还没来得及看，抱歉哈，马上学习。
- [苏剑林](https://kexue.fm/archives/10907/comment-page-2#comment-27795): “将存储kv cache改为存储降维后的Embedding X”这句话没错，但是按照我的理解，...
- [苏剑林](https://kexue.fm/archives/10945/comment-page-1#comment-27794): 呃，是这样的，Moonlight-16B-A3B实际上对应的是 scaling\_factor(...
- [Jiaming Song](https://kexue.fm/archives/10958/comment-page-1#comment-27793): 推销一下我们前一段时间的工作，这个其实已经达到了你说的三个标准，并且不需要stop\_grad或...
- [ZhouTimeMachine](https://kexue.fm/archives/9497/comment-page-2#comment-27792): 感谢您的回复！这么一看，我才注意到能从 $p(x\_0)$ 变换到 $p(x\_1)$ 实际上是每...

### 友情链接

- [Cool Papers](https://papers.cool)
- [数学研发](https://bbs.emath.ac.cn)
- [Seatop](http://www.seatop.com.cn/)
- [Xiaoxia](https://xiaoxia.org/)
- [积分表-网络版](https://kexue.fm/sci/integral/index.html)
- [丝路博傲](http://blog.dvxj.com/)
- [ph4ntasy 饭特稀](http://www.ph4ntasy.com/)
- [数学之家](http://www.2math.cn/)
- [有趣天文奇观](http://interesting-sky.china-vo.org/)
- [TwistedW](http://www.twistedwg.com/)
- [godweiyang](https://godweiyang.com/)
- [AI柠檬](https://blog.ailemon.net/)
- [王登科-DK博客](https://greatdk.com)
- [ESON](https://blog.eson.org/)
- [枫之羽](https://fzhiy.net/)
- [Mathor's blog](https://wmathor.com/)
- [coding-zuo](https://coding-zuo.github.io/)
- [博科园](https://www.bokeyuan.net/)
- [孔皮皮的博客](https://www.kppkkp.top/)
- [运鹏的博客](https://yunpengtai.top/)
- [jiming.site](https://jiming.site/)
- [OmegaXYZ](https://www.omegaxyz.com/)
- [Blog by Eacls](https://www.eacls.top/)
- [EAI猩球](https://www.robotech.ink/)
- [文举的博客](https://liwenju0.com/)
- [用代码打点酱油](https://bruceyuan.com/)
- [申请链接](https://kexue.fm/links.html)

本站采用创作共用版权协议，要求署名、非商业用途和保持一致。转载本站内容必须也遵循“ [署名-非商业用途-保持一致](http://creativecommons.org/licenses/by-nc-nd/2.5/cn/)”的创作共用协议。
© 2009-2025 Scientific Spaces. All rights reserved. Theme by [laogui](http://www.laogui.com). Powered by [Typecho](http://typecho.org). 备案号: [粤ICP备09093259号-1/2](https://beian.miit.gov.cn/)。