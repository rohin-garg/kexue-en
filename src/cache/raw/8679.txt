## SEARCH

## MENU

- [打赏](https://kexue.fm/reward.html)
- [公式](https://kexue.fm/latex.html)
- [天象](https://kexue.fm/ac.html)
- [链接](https://kexue.fm/links.html)
- [时光](https://kexue.fm/me.html)
- [博览](https://kexue.fm/science.html)
- [归档](https://kexue.fm/content.html)

## CATEGORIES

- [千奇百怪](https://kexue.fm/category/Everything)
- [天文探索](https://kexue.fm/category/Astronomy)
- [数学研究](https://kexue.fm/category/Mathematics)
- [物理化学](https://kexue.fm/category/Phy-chem)
- [信息时代](https://kexue.fm/category/Big-Data)
- [生物自然](https://kexue.fm/category/Biology)
- [图片摄影](https://kexue.fm/category/Photograph)
- [问题百科](https://kexue.fm/category/Questions)
- [生活/情感](https://kexue.fm/category/Life-Feeling)
- [资源共享](https://kexue.fm/category/Resources)

## NEWPOSTS

- [流形上的最速下降：5\. 对偶梯度下降](https://kexue.fm/archives/11388)
- [低精度Attention可能存在有...](https://kexue.fm/archives/11371)
- [MuP之上：1. 好模型的三个特征](https://kexue.fm/archives/11340)
- [随机矩阵的谱范数的快速估计](https://kexue.fm/archives/11335)
- [DiVeQ：一种非常简洁的VQ训练方案](https://kexue.fm/archives/11328)
- [为什么线性注意力要加Short C...](https://kexue.fm/archives/11320)
- [AdamW的Weight RMS的...](https://kexue.fm/archives/11307)
- [重新思考学习率与Batch Siz...](https://kexue.fm/archives/11301)
- [重新思考学习率与Batch Siz...](https://kexue.fm/archives/11285)
- [重新思考学习率与Batch Siz...](https://kexue.fm/archives/11280)

## COMMENTS

- [Tim: saw this paper, seems quite rel...](https://kexue.fm/archives/11388/comment-page-1#comment-28780)
- [wade: 最早的线性Attention对应的损失函数是$ - \\nu ^...](https://kexue.fm/archives/11033/comment-page-2#comment-28779)
- [House Li: 感觉上online update好像没有缓解这个问题，而是促使...](https://kexue.fm/archives/11033/comment-page-2#comment-28778)
- [ZHANG Ruiqi: 谢谢苏神的教程和回复，我同意你的观点。数学上 VQ-VAE 不...](https://kexue.fm/archives/6760/comment-page-8#comment-28777)
- [苏剑林: 很详细，我认为这个理解没有问题，但我依然不认同VQ-VAE的命...](https://kexue.fm/archives/6760/comment-page-8#comment-28776)
- [苏剑林: 激活函数是$f(x)$，$\\phi(x)$是构造$f(x)$的...](https://kexue.fm/archives/11233/comment-page-1#comment-28775)
- [苏剑林: 这里的等价性只是一个具体例子，如果你认为一个image tok...](https://kexue.fm/archives/10352/comment-page-2#comment-28774)
- [苏剑林: 好像是。你可以去X上@一下Jeremy Bernstein～](https://kexue.fm/archives/11388/comment-page-1#comment-28773)
- [苏剑林: 1\. 我们暂时认为MLA相比GQA有优势；\
2\. Kimi L...](https://kexue.fm/archives/11111/comment-page-2#comment-28772)
- [苏剑林: 我自己测过，效果尚可。](https://kexue.fm/archives/10862/comment-page-1#comment-28771)

## USERLOGIN

- [登录](https://kexue.fm/admin/login.php)

[科学空间\|Scientific Spaces](https://kexue.fm)

- [登录](https://kexue.fm/admin/login.php)
- [打赏](https://kexue.fm/reward.html)
- [公式](https://kexue.fm/latex.html)
- [天象](https://kexue.fm/ac.html)
- [链接](https://kexue.fm/links.html)
- [时光](https://kexue.fm/me.html)
- [博览](https://kexue.fm/science.html)
- [归档](https://kexue.fm/content.html)

渴望成为一个小飞侠

- [欢迎订阅](https://kexue.fm/feed)
- [个性邮箱](https://kexue.fm/archives/119)
- [天象信息](https://kexue.fm/ac.html)
- [观测ISS](https://kexue.fm/archives/41)
- [LaTeX](https://kexue.fm/latex.html)
- [关于博主](https://kexue.fm/me.html)

欢迎访问“科学空间”，这里将与您共同探讨自然科学，回味人生百态；也期待大家的分享～

- [**千奇百怪** Everything](https://kexue.fm/category/Everything)
- [**天文探索** Astronomy](https://kexue.fm/category/Astronomy)
- [**数学研究** Mathematics](https://kexue.fm/category/Mathematics)
- [**物理化学** Phy-chem](https://kexue.fm/category/Phy-chem)
- [**信息时代** Big-Data](https://kexue.fm/category/Big-Data)
- [**生物自然** Biology](https://kexue.fm/category/Biology)
- [**图片摄影** Photograph](https://kexue.fm/category/Photograph)
- [**问题百科** Questions](https://kexue.fm/category/Questions)
- [**生活/情感** Life-Feeling](https://kexue.fm/category/Life-Feeling)
- [**资源共享** Resources](https://kexue.fm/category/Resources)

- [**千奇百怪**](https://kexue.fm/category/Everything)
- [**天文探索**](https://kexue.fm/category/Astronomy)
- [**数学研究**](https://kexue.fm/category/Mathematics)
- [**物理化学**](https://kexue.fm/category/Phy-chem)
- [**信息时代**](https://kexue.fm/category/Big-Data)
- [**生物自然**](https://kexue.fm/category/Biology)
- [**图片摄影**](https://kexue.fm/category/Photograph)
- [**问题百科**](https://kexue.fm/category/Questions)
- [**生活/情感**](https://kexue.fm/category/Life-Feeling)
- [**资源共享**](https://kexue.fm/category/Resources)

[首页](https://kexue.fm) [数学研究](https://kexue.fm/category/Mathematics) [信息时代](https://kexue.fm/category/Big-Data) 让人惊叹的Johnson-Lindenstrauss引理：理论篇

17Sep

# [让人惊叹的Johnson-Lindenstrauss引理：理论篇](https://kexue.fm/archives/8679)

By 苏剑林 \|
2021-09-17 \|
118351位读者\|

今天我们来学习Johnson-Lindenstrauss引理，由于名字比较长，下面都简称“JL引理”。

个人认为，JL引理是每一个计算机科学的同学都必须了解的神奇结论之一，它是一个关于降维的著名的结果，它也是高维空间中众多反直觉的“维度灾难”现象的经典例子之一。可以说，JL引理是机器学习中各种降维、Hash等技术的理论基础，此外，在现代机器学习中，JL引理也为我们理解、调试模型维度等相关参数提供了重要的理论支撑。

## 对数的维度 [\#](https://kexue.fm/kexue.fm\#%E5%AF%B9%E6%95%B0%E7%9A%84%E7%BB%B4%E5%BA%A6)

JL引理，可以非常通俗地表达为：

> **通俗版JL引理**： 塞下$N$个向量，只需要$\\mathcal{O}(\\log N)$维空间。

具体来说，JL引理说的是，不管这$N$个向量原来是多少维的，我们都可以将它们降到$\\mathcal{O}(\\log N)$，并将相对距离的误差控制在一定范围内。可以想象，这是一个非常强、非常反直觉、非常实用的结论，比如我们要做向量检索，原本的向量维度可能非常大，这样全量检索一次的成本也非常大，而JL引理告诉我们，可以将它们变换到$\\mathcal{O}(\\log N)$维，并且检索效果近似不变，这简直就是“天上掉馅饼”的好事！

可能读者会有疑问：这么强的结论，那么对应的降维方法会不会特别复杂？答案是刚刚相反，降维过程仅仅用到随机线性投影！甚至有评价说，JL引理是一个“证明比理解更容易的结论”，也就是说，从数学上证明它还真不算特别困难，但如何直观地理解这个反直觉的结论，反而是不那么容易的。

无独有偶，我们之前其实就介绍过两个反直觉的结果：在文章 [《n维空间下两个随机向量的夹角分布》](https://kexue.fm/archives/7076) 中，我们就介绍过“高维空间中任意两个向量几乎都是垂直的”，这显然与二维、三维空间的结果差距甚远；在文章 [《从几何视角来理解模型参数的初始化策略》](https://kexue.fm/archives/7180) 中，这个结果进一步升级为“从$\\mathcal{N}(0,1/n)$采样出来的$n\\times n$矩阵几乎是一个正交矩阵”，这更与我们一直理解的“正交性是非常苛刻的（要求转置等于逆）”有严重出入。

但事实上，这两个结论不仅对，而且还跟JL引理直接相关。可以说，JL引理可以看成是它们的细化和应用。所以，我们需要先用更定量的语言来刻画这两个结论，比如“几乎垂直”，那垂直的概率究竟有多少，比如“近似正交”，那误差究竟有多大。

## 概率不等式 [\#](https://kexue.fm/kexue.fm\#%E6%A6%82%E7%8E%87%E4%B8%8D%E7%AD%89%E5%BC%8F)

为此，我们需要一些概率知识，其中最主要是“马尔可夫不等式”：

> **马尔可夫不等式**：如果$x$是非负随机变量，$a > 0$，那么
> \\begin{equation}P(x\\geq a)\\leq \\frac{\\mathbb{E}\[x\]}{a}\\end{equation}

注意该不等式并没有对$x$所服从的分布有其他特别的限制，只要求随机变量的取值空间是非负的（或者等价地，负的$x$的概率恒为0），证明其实非常简单：
\\begin{equation}\\mathbb{E}\[x\]=\\int\_0^{\\infty}x p(x) \\geq \\int\_a^{\\infty}x p(x) \\geq \\int\_a^{\\infty} a p(x) = a P(x\\geq a)\\end{equation}

马尔可夫不等式要求随机变量是非负的，但我们平时要处理的随机变量不一定是非负的，所以通常需要变换一下才能用。比如$x - \\mathbb{E}\[x\]$不是非负的，但$\|x - \\mathbb{E}\[x\]\|$是非负的，于是利用马尔可夫不等式有：
\\begin{equation}P(\|x - \\mathbb{E}\[x\]\|\\geq a) = P((x - \\mathbb{E}\[x\])^2\\geq a^2) \\leq \\frac{\\mathbb{E}\[(x - \\mathbb{E}\[x\])^2\]}{a^2}=\\frac{\\mathbb{V}ar\[x\]}{a^2}\\end{equation}
这就是“切比雪夫不等式”。

另外一个经典技巧称为“Cramér-Chernoff方法”，也是我们后面主要利用到的方法，它通过指数函数将随机变量变成非负的：对于任意$\\lambda > 0$，我们有
\\begin{equation}x \\geq a \\quad\\Leftrightarrow\\quad \\lambda x \\geq \\lambda a \\quad\\Leftrightarrow\\quad e^{\\lambda x} \\geq e^{\\lambda a}\\end{equation}
所以利用马尔可夫不等式有
\\begin{equation}P(x \\geq a) = P(e^{\\lambda x} \\geq e^{\\lambda a})\\leq e^{-\\lambda a}\\mathbb{E}\[e^{\\lambda x}\]\\end{equation}
最左端是跟$\\lambda$无关的，但是最右端有一个$\\lambda$，而这不等式是对于任意$\\lambda > 0$都成立的。所以理论上，我们可以找到使得最右端最小的$\\lambda$，以获得最高的估计精度：
\\begin{equation}P(x \\geq a) \\leq \\min\_{\\lambda > 0} e^{-\\lambda a}\\mathbb{E}\[e^{\\lambda x}\]\\end{equation}

## 引理的引理 [\#](https://kexue.fm/kexue.fm\#%E5%BC%95%E7%90%86%E7%9A%84%E5%BC%95%E7%90%86)

现在，我们可以引入如下结果，它是JL引理的引理，甚至可以说，它是本文一切结论的理论基础：

> **单位模引理**： 设$u\\in\\mathbb{R}^{n}$是独立重复采样自$\\mathcal{N}(0,1/n)$的向量，$\\varepsilon \\in (0, 1)$是给定常数，那么我们有
> \\begin{equation}P(\|\\Vert u\\Vert^2 - 1\| \\geq \\varepsilon) \\leq 2\\exp\\left(-\\frac{\\varepsilon^2 n}{8}\\right)\\end{equation}

该引理告诉我们，当$n$足够大的时候，$u$的模长明显偏离1的概率是非常小的（给定$\\varepsilon$后，将以$n$的指数形式递减至0），所以从$\\mathcal{N}(0,1/n)$采样出来的$n$维向量将会非常接近单位向量。

它的证明正是用到“Cramér-Chernoff方法”：首先$\|\\Vert u\\Vert^2 - 1\| \\geq \\varepsilon$意味着$\\Vert u\\Vert^2 - 1 \\geq \\varepsilon$或$1 - \\Vert u\\Vert^2\\geq \\varepsilon$，我们需要分别进行推导，不失一般性，先推导$\\Vert u\\Vert^2 - 1 \\geq \\varepsilon$的概率，根据Cramér-Chernoff方法，有
\\begin{equation}P(\\Vert u\\Vert^2 - 1 \\geq \\varepsilon) \\leq \\min\_{\\lambda > 0} e^{-\\lambda \\varepsilon}\\mathbb{E}\\big\[e^{\\lambda (\\Vert u\\Vert^2 - 1)}\\big\] = \\min\_{\\lambda > 0} e^{-\\lambda (\\varepsilon + 1)}\\mathbb{E}\\big\[e^{\\lambda \\Vert u\\Vert^2}\\big\]\\end{equation}
将$u$写成分量形式$(u\_1, u\_2, \\cdots, u\_n)$，其中每个分量都是独立的，分布均为$\\mathcal{N}(0,1/n)$，那么我们有
\\begin{equation}\\mathbb{E}\\big\[e^{\\lambda \\Vert u\\Vert^2}\\big\] = \\mathbb{E}\\big\[e^{\\lambda\\sum\\limits\_i u\_i^2}\\big\] = \\mathbb{E}\\big\[\\prod\_i e^{\\lambda u\_i^2}\\big\]=\\prod\_i \\mathbb{E}\\big\[ e^{\\lambda u\_i^2}\\big\]\\end{equation}
而$\\mathbb{E}\\big\[ e^{\\lambda u\_i^2}\\big\]=\\int\_{-\\infty}^{\\infty} \\frac{1}{\\sqrt{2\\pi}}e^{-u\_i^2/2}e^{\\lambda u\_i^2/n} du\_i=\\sqrt{n/(n-2\\lambda)}$，所以
\\begin{equation}P(\\Vert u\\Vert^2 - 1 \\geq \\varepsilon) \\leq \\min\_{\\lambda > 0} e^{-\\lambda (\\varepsilon + 1)}\\left(\\frac{n}{n-2\\lambda}\\right)^{n/2}\\end{equation}
右端的极小值在$\\lambda = \\frac{n\\varepsilon}{2(1+\\varepsilon)}$取到，推导过程就留给读者了，然后代入得到
\\begin{equation}P(\\Vert u\\Vert^2 - 1 \\geq \\varepsilon) \\leq e^{n(\\log(1+\\varepsilon) - \\varepsilon)/2}\\leq e^{-n\\varepsilon^2/8}\\end{equation}
其中$\\log(1+\\varepsilon) - \\varepsilon \\leq -\\varepsilon^2/4$的证明也留给读者了。类似地，我们可以对$1 - \\Vert u\\Vert^2\\geq \\varepsilon$的概率进行推导，结果为：
\\begin{equation}P(1 - \\Vert u\\Vert^2\\geq \\varepsilon) \\leq e^{n(\\log(1-\\varepsilon) + \\varepsilon)/2}\\leq e^{-n\\varepsilon^2/8}\\end{equation}
其中可证$\\log(1-\\varepsilon) + \\varepsilon \\leq \\log(1+\\varepsilon) - \\varepsilon$，所以上式沿用了$\\log(1+\\varepsilon) - \\varepsilon$的不等关系。现在两式相加，我们得到$P(\|\\Vert u\\Vert^2 - 1\| \\geq \\varepsilon)\\leq 2e^{-n\\varepsilon^2/8}$。证毕。

从“单位模引理”出发，我们可以证明“正交性引理”：

> **正交性引理**： 设$u,v\\in\\mathbb{R}^{n}$是独立重复采样自$\\mathcal{N}(0,1/n)$的两个向量，$\\varepsilon \\in (0, 1)$是给定常数，那么我们有
> \\begin{equation}P(\|\\langle u, v\\rangle\| \\geq \\varepsilon) \\leq 4\\exp\\left(-\\frac{\\varepsilon^2 n}{8}\\right)\\end{equation}

该引理告诉我们，当$n$足够大的时候，$u,v$的内积明显偏离0的概率是非常小的（给定$\\varepsilon$后，将以$n$的指数形式递减至0），所以从$\\mathcal{N}(0,1/n)$采样出来的两个$n$维向量将会非常接近正交。而结合“单位模引理”，我们就得到“从$\\mathcal{N}(0,1/n)$采样出来的$n\\times n$矩阵几乎是一个正交矩阵”的结论了。

有了“单位模引理”铺垫，它的证明不算难。我们知道如果$u,v\\sim \\mathcal{N}(0,1/n)$，那么$\\frac{u\\pm v}{\\sqrt{2}}\\sim \\mathcal{N}(0,1/n)$，所以根据“单位模引理”的证明，我们有
\\begin{equation}P\\left(\\left\\Vert \\frac{u+v}{\\sqrt{2}}\\right\\Vert^2 - 1 \\geq \\varepsilon\\right) \\leq e^{-n\\varepsilon^2/8},\\quad P\\left(1-\\left\\Vert \\frac{u-v}{\\sqrt{2}}\\right\\Vert^2 \\geq \\varepsilon\\right) \\leq e^{-n\\varepsilon^2/8}\\end{equation}
注意$\\left\\Vert \\frac{u+v}{\\sqrt{2}}\\right\\Vert^2 - 1 \\geq \\varepsilon$和$1-\\left\\Vert \\frac{u-v}{\\sqrt{2}}\\right\\Vert^2 \\geq \\varepsilon$两式相加后可以得出$\\langle u, v\\rangle\\geq \\varepsilon$，所以
\\begin{equation}P(\\langle u, v\\rangle\\geq \\varepsilon) \\leq P\\left(\\left\\Vert \\frac{u+v}{\\sqrt{2}}\\right\\Vert^2 - 1 \\geq \\varepsilon\\right) + P\\left(1-\\left\\Vert \\frac{u-v}{\\sqrt{2}}\\right\\Vert^2 \\geq \\varepsilon\\right) \\leq 2e^{-n\\varepsilon^2/8}\\end{equation}
同理可证$P(-\\langle u, v\\rangle\\geq \\varepsilon) \\leq 2e^{-n\\varepsilon^2/8}$，两者结合就得到“正交性引理”。

## 证明的过程 [\#](https://kexue.fm/kexue.fm\#%E8%AF%81%E6%98%8E%E7%9A%84%E8%BF%87%E7%A8%8B)

现在我们就可以着手证明JL引理了，下面是它的数学表述：

> **数学版JL引理**： 给定$N$个向量$v\_1,v\_2,\\cdots,v\_N\\in\\mathbb{R}^m$和$n > \\frac{24\\log N}{\\varepsilon^2}$，而随机矩阵$A\\in\\mathbb{R}^{n\\times m}$独立重复采样自$\\mathcal{N}(0,1/n)$，$\\varepsilon \\in (0, 1)$是给定常数，那么至少有$\\frac{N-1}{N}$的概率，使得对于所有的$i\\neq j$，都成立
> \\begin{equation}(1-\\varepsilon)\\Vert v\_i - v\_j\\Vert^2 \\leq \\Vert Av\_i - A v\_j\\Vert^2 \\leq (1+\\varepsilon)\\Vert v\_i - v\_j\\Vert^2\\label{eq:bound}\\end{equation}

引理告诉我们，不管原来的向量维数$m$是多少，只需要$n > \\frac{24\\log N}{\\varepsilon^2}$的维度，我们就可以容纳下$N$个向量，使得它们相对距离的偏离都不超过$\\varepsilon$。而且JL引理还告诉我们降维方法：只需要从$\\mathcal{N}(0,1/n)$随机采样一个$n\\times m$的矩阵$A$，然后变换$v\\to Av$就有$\\frac{N-1}{N}$的可能性达到目的。真可谓是简单实用了～

证明过程也是“单位模引理”的直接应用。首先，如果$u\\in\\mathbb{R}^m$是给定的单位向量，而$A\\in\\mathbb{R}^{n\\times m}$独立重复采样自$\\mathcal{N}(0,1/n)$，那么$Au$的每个分量都独立地服从$\\mathcal{N}(0,1/n)$。证明也并不难，根据定义每个分量$(Au)\_i = \\sum\\limits\_j A\_{i,j}u\_j$，由于$A\_{i,j}$相互独立，所以$(Au)\_i$显然相互独立，并且由于$A\_{i,j}\\sim\\mathcal{N}(0,1/n)$，正态随机变量和的分布依然是正态分布，所以$(Au)\_i$服从正态分布，其均值为$\\sum\\limits\_j u\_j\\times 0=0$，其方差则为$\\sum\\limits\_j u\_j^2\\times \\frac{1}{n} = \\frac{1}{n}$。

所以，说白了，$Au$相当于从$\\mathcal{N}(0,1/n)$独立重复采样出来的$n$维向量。现在代入$u=\\frac{v\_i - v\_j}{\\Vert v\_i - v\_j\\Vert}$，利用“单位模引理”，得到
\\begin{equation}P\\left(\\left\|\\left\\Vert \\frac{A(v\_i - v\_j)}{\\Vert v\_i - v\_j\\Vert}\\right\\Vert^2 - 1\\right\| \\geq \\varepsilon\\right) \\leq 2\\exp\\left(-\\frac{\\varepsilon^2 n}{8}\\right)\\end{equation}
此结果对于任意$i\\neq j$都成立，那么遍历所有的$i\\neq j$的组合，我们得到至少有一项$\\geq \\varepsilon$的概率不超过
\\begin{equation}P\\left(\\exists (i,j):\\,\\left\|\\left\\Vert \\frac{A(v\_i - v\_j)}{\\Vert v\_i - v\_j\\Vert}\\right\\Vert^2 - 1\\right\| \\geq \\varepsilon\\right) \\leq 2 {N\\choose 2} \\exp\\left(-\\frac{\\varepsilon^2 n}{8}\\right)\\end{equation}
或者反过来说，对于任意$i\\neq j$，都成立$\\left\|\\left\\Vert \\frac{A(v\_i - v\_j)}{\\Vert v\_i - v\_j\\Vert}\\right\\Vert^2 - 1\\right\| \\leq \\varepsilon$（等价于$\\eqref{eq:bound}$）的概率不小于
\\begin{equation}1 - 2 {N\\choose 2} \\exp\\left(-\\frac{\\varepsilon^2 n}{8}\\right) = 1 - N(N-1)\\exp\\left(-\\frac{\\varepsilon^2 n}{8}\\right)\\end{equation}
代入$n > \\frac{24\\log N}{\\varepsilon^2}$，可以得到
\\begin{equation}1 - N(N-1)\\exp\\left(-\\frac{\\varepsilon^2 n}{8}\\right)\\geq 1 - N(N-1)N^{-3}\\geq 1-N^{-1}\\end{equation}
至此，证明已经完成。

上面的JL引理中保持的是欧氏距离近似不变，很多时候我们检索用的是内积（比如余弦相似度）而不是欧氏距离。对此，我们有

> **内积版JL引理**： 给定$N$个单位向量$v\_1,v\_2,\\cdots,v\_N\\in\\mathbb{R}^m$和$n > \\frac{24\\log N}{\\varepsilon^2}$，而随机矩阵$A\\in\\mathbb{R}^{n\\times m}$独立重复采样自$\\mathcal{N}(0,1/n)$，$\\varepsilon \\in (0, 1)$是给定常数，那么至少有$\\frac{N-2}{N}$的概率，使得对于所有的$i\\neq j$，都成立
> \\begin{equation}\\left\|\\langle Av\_i, Av\_j\\rangle - \\langle v\_i, v\_j\\rangle\\right\|\\leq\\varepsilon\\end{equation}

证明很简单，模仿“正交性引理”的证明即可。根据JL引理的证明，我们可以得到在相同的条件下，至少有$\\frac{N-2}{N}$的概率同时满足对于任意$i\\neq j$有
\\begin{equation}\\begin{aligned}
(1-\\varepsilon)\\Vert v\_i - v\_j\\Vert^2 \\leq \\Vert Av\_i - A v\_j\\Vert^2 \\leq (1+\\varepsilon)\\Vert v\_i - v\_j\\Vert^2 \\\
(1-\\varepsilon)\\Vert v\_i + v\_j\\Vert^2 \\leq \\Vert Av\_i + A v\_j\\Vert^2 \\leq (1+\\varepsilon)\\Vert v\_i + v\_j\\Vert^2
\\end{aligned}\\end{equation}
将第一乘上$-1$得到$-(1+\\varepsilon)\\Vert v\_i - v\_j\\Vert^2 \\leq -\\Vert Av\_i - A v\_j\\Vert^2 \\leq -(1-\\varepsilon)\\Vert v\_i - v\_j\\Vert^2$，然后加到第二式得到
\\begin{equation}4\\langle v\_i, v\_j\\rangle-2\\varepsilon(\\Vert v\_i\\Vert^2 + \\Vert v\_j\\Vert)\\leq 4\\langle Av\_i, Av\_j\\rangle \\leq 4\\langle v\_i, v\_j\\rangle + 2\\varepsilon(\\Vert v\_i\\Vert^2 + \\Vert v\_j\\Vert)\\end{equation}
注意到$v\_i,v\_j$是单位向量，所以上式等价于$\\left\|\\langle Av\_i, Av\_j\\rangle - \\langle v\_i, v\_j\\rangle\\right\|\\leq\\varepsilon$。

## 极度的充分 [\#](https://kexue.fm/kexue.fm\#%E6%9E%81%E5%BA%A6%E7%9A%84%E5%85%85%E5%88%86)

动手去推过一次JL引理证明的同学应该能感觉到，JL引理的结论中之所以能够出现$\\log N$，本质上是因为“单位模引理”中的概率项$2\\exp\\left(-\\frac{\\varepsilon^2 n}{8}\\right)$是指数衰减的，而我们可以放宽这个衰减速度，让其变成多项式衰减，从而出现了$\\log N$。

总的来说，JL引理告诉我们，以误差$\\varepsilon$塞下$N$个向量，只需要$\\mathcal{O}\\left(\\frac{\\log N}{\\varepsilon^2}\\right)$维的空间，至于$\\frac{\\log N}{\\varepsilon^2}$前面的常数是多少，其实不大重要。因为事实上JL引理是一个非常充分的条件，实际情况中条件往往更加宽松。比如，在JL引理的证明中如果我们将条件改为$n > \\frac{16\\log N}{\\varepsilon^2}$，那么式$\\eqref{eq:bound}$成立的概率就不小于
\\begin{equation}1 - N(N-1)\\exp\\left(-\\frac{\\varepsilon^2 n}{8}\\right)\\geq 1 - N(N-1)N^{-2}=1/N\\end{equation}
注意$1/N$虽然小，但终究是大于0的，所以此时依然是存在$A$使得$\\eqref{eq:bound}$成立，只不过寻找$A$的成本更大罢了（每次命中的概率只有$1/N$），而如果我们只关心存在性，那么这也够了。

而且，JL引理只考虑了在随机线性投影下的降维，就已经得到$n > \\frac{16\\log N}{\\varepsilon^2}$了，如果是其他更精细的降维，比如基于SVD的降维，是有可能得到更好的结果的（前面的系数更小）；如果非线性的降维方法也考虑进去，那么结果又能变得更优了。所以说，不需要太关心$\\frac{\\log N}{\\varepsilon^2}$前面的常数是多少，我们只需要知道$\\mathcal{O}\\left(\\frac{\\log N}{\\varepsilon^2}\\right)$的量级，如果真要用到它，通常还需要根据实际情况确定前面的常数，而不是调用理论结果。

## 且待下回续 [\#](https://kexue.fm/kexue.fm\#%E4%B8%94%E5%BE%85%E4%B8%8B%E5%9B%9E%E7%BB%AD)

在这篇文章中，我们介绍了Johnson–Lindenstrauss引理（JL引理），它是关于降维的一个重要而奇妙的结论，是高维空间的不同寻常之处的重要体现之一。它告诉我们“只需要$\\mathcal{O}(\\log N)$维空间就可以塞下$N$个向量”，使得原本高维空间中的检索问题可以降低到$\\mathcal{O}(\\log N)$维空间中。

本文主要讨论了JL引理的相关理论证明细节，下一篇文章我们则尝试应用它来理解一些机器学习问题，敬请期待～

_**转载到请包括本文地址：** [https://kexue.fm/archives/8679](https://kexue.fm/archives/8679)_

_**更详细的转载事宜请参考：**_ [《科学空间FAQ》](https://kexue.fm/archives/6508#%E6%96%87%E7%AB%A0%E5%A6%82%E4%BD%95%E8%BD%AC%E8%BD%BD/%E5%BC%95%E7%94%A8)

**如果您还有什么疑惑或建议，欢迎在下方评论区继续讨论。**

**如果您觉得本文还不错，欢迎 [分享](https://kexue.fm/kexue.fm#share)/ [打赏](https://kexue.fm/kexue.fm#pay) 本文。打赏并非要从中获得收益，而是希望知道科学空间获得了多少读者的真心关注。当然，如果你无视它，也不会影响你的阅读。再次表示欢迎和感谢！**

打赏

微信打赏

支付宝打赏

因为网站后台对打赏并无记录，因此欢迎在打赏时候备注留言。你还可以 [**点击这里**](http://mail.qq.com/cgi-bin/qm_share?t=qm_mailme&email=tN7d1drY3drrx8H0xcWa19vZ) 或在下方评论区留言来告知你的建议或需求。

**如果您需要引用本文，请参考：**

苏剑林. (Sep. 17, 2021). 《让人惊叹的Johnson-Lindenstrauss引理：理论篇 》\[Blog post\]. Retrieved from [https://kexue.fm/archives/8679](https://kexue.fm/archives/8679)

@online{kexuefm-8679,
        title={让人惊叹的Johnson-Lindenstrauss引理：理论篇},
        author={苏剑林},
        year={2021},
        month={Sep},
        url={\\url{https://kexue.fm/archives/8679}},
}

分类： [数学研究](https://kexue.fm/category/Mathematics), [信息时代](https://kexue.fm/category/Big-Data)    标签： [模型](https://kexue.fm/tag/%E6%A8%A1%E5%9E%8B/), [分析](https://kexue.fm/tag/%E5%88%86%E6%9E%90/), [维度](https://kexue.fm/tag/%E7%BB%B4%E5%BA%A6/), [机器学习](https://kexue.fm/tag/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/)[41 评论](https://kexue.fm/archives/8679#comments)

< [曾被嫌弃的预训练任务NSP，做出了优秀的Zero Shot效果](https://kexue.fm/archives/8671) \| [让人惊叹的Johnson-Lindenstrauss引理：应用篇](https://kexue.fm/archives/8706) >

### 你也许还对下面的内容感兴趣

- [低精度Attention可能存在有偏的舍入误差](https://kexue.fm/archives/11371)
- [为什么Adam的Update RMS是0.2？](https://kexue.fm/archives/11267)
- [ReLU/GeLU/Swish的一个恒等式](https://kexue.fm/archives/11233)
- [等值振荡定理：最优多项式逼近的充要条件](https://kexue.fm/archives/10972)
- [SVD的导数](https://kexue.fm/archives/10878)
- [通过梯度近似寻找Normalization的替代品](https://kexue.fm/archives/10831)
- [MoE环游记：1、从几何意义出发](https://kexue.fm/archives/10699)
- [通向概率分布之路：盘点Softmax及其替代品](https://kexue.fm/archives/10145)
- [用傅里叶级数拟合一维概率密度函数](https://kexue.fm/archives/10007)
- [基于量子化假设推导模型的尺度定律（Scaling Law）](https://kexue.fm/archives/9607)

[发表你的看法](https://kexue.fm/kexue.fm#comment_form)

1. [«](https://kexue.fm/archives/8679/comment-page-1#comments)
2. [1](https://kexue.fm/archives/8679/comment-page-1#comments)
3. [2](https://kexue.fm/archives/8679/comment-page-2#comments)

studentK

August 30th, 2022

苏神你好，最近正好在学习信息论，感觉JL引理的结论形式与信息熵有点像，如果假定N个向量是均匀分布的话，其最优编码的长度也正好是logN，这可能是数据压缩的下确界？

[回复评论](https://kexue.fm/archives/8679/comment-page-2?replyTo=19694#respond-post-8679)

[苏剑林](https://kexue.fm) 发表于
September 1st, 2022

确实跟信息熵有关，参考我之前的推导结果： [https://kexue.fm/archives/7695](https://kexue.fm/archives/7695)

[回复评论](https://kexue.fm/archives/8679/comment-page-2?replyTo=19710#respond-post-8679)

Li Ming

December 24th, 2022

苏教授你好，本文中“A从N~(1,1/n)采样”，是否应为“A从N~(1,1/√n)采样”？个人浅见，请您参考。

[回复评论](https://kexue.fm/archives/8679/comment-page-2?replyTo=20632#respond-post-8679)

[苏剑林](https://kexue.fm) 发表于
December 28th, 2022

否。正态分布的一般记号是$\\mathcal{N}(\\mu,\\sigma^2)$，而不是$\\mathcal{N}(\\mu,\\sigma)$

[回复评论](https://kexue.fm/archives/8679/comment-page-2?replyTo=20657#respond-post-8679)

[句向量表示\_Johngo学长](https://www.johngo689.com/556354/)

June 1st, 2023

\[...\]维度选择公式：n>8.33logNJL理论：塞下N个向量，只需要

[回复评论](https://kexue.fm/archives/8679/comment-page-2?replyTo=21851#respond-post-8679)

ZZ

June 20th, 2023

写的不错，JL引理的证明还可以结合 $\\chi^2$ R.V. 和 Bernstein-type bound 证明。https://doi.org/10.1017/9781108627771.002这本书里面有写。
另外苏神可以讲讲VC dimension（Vapnik-Chervonenkis）吗？

[回复评论](https://kexue.fm/archives/8679/comment-page-2?replyTo=22026#respond-post-8679)

[苏剑林](https://kexue.fm) 发表于
June 25th, 2023

谢谢肯定和推荐，不过VC dimension我也不大了解，抱歉

[回复评论](https://kexue.fm/archives/8679/comment-page-2?replyTo=22047#respond-post-8679)

hehongyu

June 20th, 2023

这个定理和现在模型量化之间有没有关系呢

[回复评论](https://kexue.fm/archives/8679/comment-page-2?replyTo=22027#respond-post-8679)

[苏剑林](https://kexue.fm) 发表于
June 25th, 2023

挺好的问题，我估计应该会有千丝万缕的联系吧（哭笑）

[回复评论](https://kexue.fm/archives/8679/comment-page-2?replyTo=22048#respond-post-8679)

水博

July 10th, 2024

请问下，如果是随机稀疏投影支持JL引理么，怎么证明的

[回复评论](https://kexue.fm/archives/8679/comment-page-2?replyTo=24771#respond-post-8679)

[苏剑林](https://kexue.fm) 发表于
July 12th, 2024

随机稀疏投影的投影矩阵采样自三元分布，看上去本文的所有证明过程依然可以类似地进行，所以类似的结论是存在的。

或者从直观上来讲，正态分布$\\mathcal{N}(0,\\sigma^2)$可以视为任意均值为0、方差为$\\sigma^2$的分布的一阶近似，所以在正态分布$\\mathcal{N}(0,\\sigma^2)$上成立的结果，往往在一定范围内都可以推广到任意均值为0、方差为$\\sigma^2$的分布。

[回复评论](https://kexue.fm/archives/8679/comment-page-2?replyTo=24792#respond-post-8679)

forher

October 6th, 2024

你好，我不是很明白（9）到（10）之间的那个积分公式，感觉不太对，不知道那个积分项怎么出来的，我觉得应该是

$\\mathbb{E}\[e^{\\lambda u\_i^2}\] = \\int\_{-\\infty}^{\\infty} e^{\\lambda u\_i^2} \\sqrt{\\frac{n}{2\\pi}} e^{-\\frac{nu\_i^2}{2}} du\_i$

[回复评论](https://kexue.fm/archives/8679/comment-page-2?replyTo=25392#respond-post-8679)

[苏剑林](https://kexue.fm) 发表于
October 11th, 2024

你这个也没错，我这个是利用了从$\\mathcal{N}(0,1/n)$中采样一个$x$，等价于从$\\mathcal{N}(0,1)$中采样一个$y$，然后让$x=y/\\sqrt{n}$，这是简单的重参数技巧（或者说积分换元）

[回复评论](https://kexue.fm/archives/8679/comment-page-2?replyTo=25466#respond-post-8679)

Yaoxin 发表于
February 27th, 2025

可能是我太菜了跟不上苏神的步伐，看到这个评论我才恍然大悟，但我觉得这个地方确实不应该用 \\( u\_i \\)，显得没有重参数的痕迹。不过感谢苏神！

[回复评论](https://kexue.fm/archives/8679/comment-page-2?replyTo=26791#respond-post-8679)

asjdlkjlzkjcl

December 20th, 2024

求教一下（11）这里的λ是怎么取的，直接求导求最值吗，我求导算出来不是这个点，是不是我算错了

[回复评论](https://kexue.fm/archives/8679/comment-page-2?replyTo=26041#respond-post-8679)

[苏剑林](https://kexue.fm) 发表于
December 20th, 2024

是求导，结果不对的话你再认真检查一下你的求导过程～

[回复评论](https://kexue.fm/archives/8679/comment-page-2?replyTo=26052#respond-post-8679)

Astrologos

February 14th, 2025

感觉和Toy Models of Superposition讲的有关

[回复评论](https://kexue.fm/archives/8679/comment-page-2?replyTo=26608#respond-post-8679)

[苏剑林](https://kexue.fm) 发表于
February 15th, 2025

搜了一下这篇paper，感觉差得比较远啊

[回复评论](https://kexue.fm/archives/8679/comment-page-2?replyTo=26639#respond-post-8679)

1. [«](https://kexue.fm/archives/8679/comment-page-1#comments)
2. [1](https://kexue.fm/archives/8679/comment-page-1#comments)
3. [2](https://kexue.fm/archives/8679/comment-page-2#comments)

[取消回复](https://kexue.fm/archives/8679#respond-post-8679)

你的大名

电子邮箱

个人网站（选填）

1\. 可以使用LaTeX代码，点击“预览效果”可查看效果；2. 可以通过点击评论楼层编号来引用该楼层；3. 网站可能会有点卡，如非确认评论失败，请 **不要重复点击提交**。

### 内容速览

[对数的维度](https://kexue.fm/kexue.fm#%E5%AF%B9%E6%95%B0%E7%9A%84%E7%BB%B4%E5%BA%A6)
[概率不等式](https://kexue.fm/kexue.fm#%E6%A6%82%E7%8E%87%E4%B8%8D%E7%AD%89%E5%BC%8F)
[引理的引理](https://kexue.fm/kexue.fm#%E5%BC%95%E7%90%86%E7%9A%84%E5%BC%95%E7%90%86)
[证明的过程](https://kexue.fm/kexue.fm#%E8%AF%81%E6%98%8E%E7%9A%84%E8%BF%87%E7%A8%8B)
[极度的充分](https://kexue.fm/kexue.fm#%E6%9E%81%E5%BA%A6%E7%9A%84%E5%85%85%E5%88%86)
[且待下回续](https://kexue.fm/kexue.fm#%E4%B8%94%E5%BE%85%E4%B8%8B%E5%9B%9E%E7%BB%AD)

### 智能搜索

支持整句搜索！网站自动使用 [结巴分词](https://github.com/fxsjy/jieba) 进行分词，并结合ngrams排序算法给出合理的搜索结果。

### 热门标签

[生成模型](https://kexue.fm/tag/%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B/) [attention](https://kexue.fm/tag/attention/) [优化](https://kexue.fm/tag/%E4%BC%98%E5%8C%96/) [语言模型](https://kexue.fm/tag/%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/) [模型](https://kexue.fm/tag/%E6%A8%A1%E5%9E%8B/) [网站](https://kexue.fm/tag/%E7%BD%91%E7%AB%99/) [概率](https://kexue.fm/tag/%E6%A6%82%E7%8E%87/) [梯度](https://kexue.fm/tag/%E6%A2%AF%E5%BA%A6/) [矩阵](https://kexue.fm/tag/%E7%9F%A9%E9%98%B5/) [转载](https://kexue.fm/tag/%E8%BD%AC%E8%BD%BD/) [优化器](https://kexue.fm/tag/%E4%BC%98%E5%8C%96%E5%99%A8/) [微分方程](https://kexue.fm/tag/%E5%BE%AE%E5%88%86%E6%96%B9%E7%A8%8B/) [分析](https://kexue.fm/tag/%E5%88%86%E6%9E%90/) [天象](https://kexue.fm/tag/%E5%A4%A9%E8%B1%A1/) [深度学习](https://kexue.fm/tag/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/) [积分](https://kexue.fm/tag/%E7%A7%AF%E5%88%86/) [python](https://kexue.fm/tag/python/) [力学](https://kexue.fm/tag/%E5%8A%9B%E5%AD%A6/) [无监督](https://kexue.fm/tag/%E6%97%A0%E7%9B%91%E7%9D%A3/) [扩散](https://kexue.fm/tag/%E6%89%A9%E6%95%A3/) [几何](https://kexue.fm/tag/%E5%87%A0%E4%BD%95/) [节日](https://kexue.fm/tag/%E8%8A%82%E6%97%A5/) [生活](https://kexue.fm/tag/%E7%94%9F%E6%B4%BB/) [文本生成](https://kexue.fm/tag/%E6%96%87%E6%9C%AC%E7%94%9F%E6%88%90/) [数论](https://kexue.fm/tag/%E6%95%B0%E8%AE%BA/)

### 随机文章

- [logsumexp运算的几个不等式](https://kexue.fm/archives/9070)
- [不成功的尝试：将多标签交叉熵推广到“n个m分类”上去](https://kexue.fm/archives/9158)
- [生成扩散模型漫谈（二十九）：用DDPM来离散编码](https://kexue.fm/archives/10711)
- [看完了刘亦菲版《倩女幽魂》](https://kexue.fm/archives/1333)
- [线圈感抗和电容容抗的计算](https://kexue.fm/archives/1276)
- [Transformer升级之路：1、Sinusoidal位置编码追根溯源](https://kexue.fm/archives/8231)
- [《环球科学》：高温超导 “铁”的飞跃](https://kexue.fm/archives/135)
- [三百年之谜——费马大定理(历史+证明)](https://kexue.fm/archives/38)
- [采样定理：有限个点构建出整个函数](https://kexue.fm/archives/3266)
- [2012诺贝尔奖...](https://kexue.fm/archives/1735)

### 最近评论

- [Tim](https://kexue.fm/archives/11388/comment-page-1#comment-28780): saw this paper, seems quite relevant: https://a...
- [wade](https://kexue.fm/archives/11033/comment-page-2#comment-28779): 最早的线性Attention对应的损失函数是$ - \\nu ^{T} \\left ( S k ...
- [House Li](https://kexue.fm/archives/11033/comment-page-2#comment-28778): 感觉上online update好像没有缓解这个问题，而是促使状态矩阵成一个"global a...
- [ZHANG Ruiqi](https://kexue.fm/archives/6760/comment-page-8#comment-28777): 谢谢苏神的教程和回复，我同意你的观点。数学上 VQ-VAE 不像 VAE 这么优美，感觉像计算...
- [苏剑林](https://kexue.fm/archives/6760/comment-page-8#comment-28776): 很详细，我认为这个理解没有问题，但我依然不认同VQ-VAE的命名。常规的VAE，是在训练Aut...
- [苏剑林](https://kexue.fm/archives/11233/comment-page-1#comment-28775): 激活函数是$f(x)$，$\\phi(x)$是构造$f(x)$的一部份。
- [苏剑林](https://kexue.fm/archives/10352/comment-page-2#comment-28774): 这里的等价性只是一个具体例子，如果你认为一个image token等价于0.5个text to...
- [苏剑林](https://kexue.fm/archives/11388/comment-page-1#comment-28773): 好像是。你可以去X上@一下Jeremy Bernstein～
- [苏剑林](https://kexue.fm/archives/11111/comment-page-2#comment-28772): 1\. 我们暂时认为MLA相比GQA有优势；
2\. Kimi Linear其实还是个toy，用来...
- [苏剑林](https://kexue.fm/archives/10862/comment-page-1#comment-28771): 我自己测过，效果尚可。

### 友情链接

- [Cool Papers](https://papers.cool)
- [数学研发](https://bbs.emath.ac.cn)
- [Seatop](http://www.seatop.com.cn/)
- [Xiaoxia](https://xiaoxia.org/)
- [积分表-网络版](https://kexue.fm/sci/integral/index.html)
- [丝路博傲](http://blog.dvxj.com/)
- [数学之家](http://www.2math.cn/)
- [有趣天文奇观](http://interesting-sky.china-vo.org/)
- [TwistedW](http://www.twistedwg.com/)
- [godweiyang](https://godweiyang.com/)
- [AI柠檬](https://blog.ailemon.net/)
- [王登科-DK博客](https://greatdk.com)
- [ESON](https://blog.eson.org/)
- [枫之羽](https://fzhiy.net/)
- [Mathor's blog](https://wmathor.com/)
- [coding-zuo](https://coding-zuo.github.io/)
- [博科园](https://www.bokeyuan.net/)
- [孔皮皮的博客](https://www.kppkkp.top/)
- [运鹏的博客](https://yunpengtai.top/)
- [jiming.site](https://jiming.site/)
- [OmegaXYZ](https://www.omegaxyz.com/)
- [EAI猩球](https://www.robotech.ink/)
- [文举的博客](https://liwenju0.com/)
- [申请链接](https://kexue.fm/links.html)

本站采用创作共用版权协议，要求署名、非商业用途和保持一致。转载本站内容必须也遵循“ [署名-非商业用途-保持一致](http://creativecommons.org/licenses/by-nc-nd/2.5/cn/)”的创作共用协议。
© 2009-2025 Scientific Spaces. All rights reserved. Theme by [laogui](http://www.laogui.com). Powered by [Typecho](http://typecho.org). 备案号: [粤ICP备09093259号-1/2](https://beian.miit.gov.cn/)。