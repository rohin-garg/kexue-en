## SEARCH

## MENU

- [打赏](https://kexue.fm/reward.html)
- [公式](https://kexue.fm/latex.html)
- [天象](https://kexue.fm/ac.html)
- [链接](https://kexue.fm/links.html)
- [时光](https://kexue.fm/me.html)
- [博览](https://kexue.fm/science.html)
- [归档](https://kexue.fm/content.html)

## CATEGORIES

- [千奇百怪](https://kexue.fm/category/Everything)
- [天文探索](https://kexue.fm/category/Astronomy)
- [数学研究](https://kexue.fm/category/Mathematics)
- [物理化学](https://kexue.fm/category/Phy-chem)
- [信息时代](https://kexue.fm/category/Big-Data)
- [生物自然](https://kexue.fm/category/Biology)
- [图片摄影](https://kexue.fm/category/Photograph)
- [问题百科](https://kexue.fm/category/Questions)
- [生活/情感](https://kexue.fm/category/Life-Feeling)
- [资源共享](https://kexue.fm/category/Resources)

## NEWPOSTS

- [通过msign来计算奇异值裁剪mc...](https://kexue.fm/archives/11059)
- [矩阵符号函数mcsgn能计算什么？](https://kexue.fm/archives/11056)
- [线性注意力简史：从模仿、创新到反哺](https://kexue.fm/archives/11033)
- [msign的导数](https://kexue.fm/archives/11025)
- [通过msign来计算奇异值裁剪mc...](https://kexue.fm/archives/11006)
- [msign算子的Newton-Sc...](https://kexue.fm/archives/10996)
- [等值振荡定理：最优多项式逼近的充要条件](https://kexue.fm/archives/10972)
- [生成扩散模型漫谈（三十）：从瞬时速...](https://kexue.fm/archives/10958)
- [MoE环游记：5、均匀分布的反思](https://kexue.fm/archives/10945)
- [msign算子的Newton-Sc...](https://kexue.fm/archives/10922)

## COMMENTS

- [abcm: 公式(14)为什么是直接带入，我的理解是0到T积分再换元\
\\b...](https://kexue.fm/archives/10114/comment-page-3#comment-28017)
- [Truenobility303: 苏神好，关于RMS对齐这里有些疑问。1. 如果从 A Spec...](https://kexue.fm/archives/10739/comment-page-2#comment-28016)
- [wolfzdf: 您好，请问cool paper中收集的会议论文，有保存整理文章...](https://kexue.fm/archives/9907/comment-page-4#comment-27987)
- [无敌大铁锤: 嗯嗯笔误了，就是去掉Causal的mask，变成一个纯Self...](https://kexue.fm/archives/11033/comment-page-1#comment-27986)
- [HikaruNight: 苏老师，想请教一下如果把四元数放在三维RoPE里面是不是可行的](https://kexue.fm/archives/8397/comment-page-3#comment-27985)
- [Leco: 请问LoRA的A,B矩阵初始化时，一个高斯随机一个全零还是只能...](https://kexue.fm/archives/9590/comment-page-2#comment-27984)
- [苏剑林: 如果你把你这里提到的数学都学通透了，数学基础基本上可以胜任95...](https://kexue.fm/archives/9119/comment-page-13#comment-27983)
- [苏剑林: 我跑过这个项目，效果是能复现的。“在 CIFAR-10 上效果...](https://kexue.fm/archives/10958/comment-page-2#comment-27982)
- [Henry Zha: 苏神你好，我是一名管理科学与工程专业的博士生，研究方向是结合人...](https://kexue.fm/archives/9119/comment-page-13#comment-27981)
- [SunlightZero: 我根据 https://github.com/haidog-y...](https://kexue.fm/archives/10958/comment-page-2#comment-27980)

## USERLOGIN

- [登录](https://kexue.fm/admin/login.php)

[科学空间\|Scientific Spaces](https://kexue.fm)

- [登录](https://kexue.fm/admin/login.php)
- [打赏](https://kexue.fm/reward.html)
- [公式](https://kexue.fm/latex.html)
- [天象](https://kexue.fm/ac.html)
- [链接](https://kexue.fm/links.html)
- [时光](https://kexue.fm/me.html)
- [博览](https://kexue.fm/science.html)
- [归档](https://kexue.fm/content.html)

渴望成为一个小飞侠

- [欢迎订阅](https://kexue.fm/feed)
- [个性邮箱](https://kexue.fm/archives/119)
- [天象信息](https://kexue.fm/ac.html)
- [观测ISS](https://kexue.fm/archives/41)
- [LaTeX](https://kexue.fm/latex.html)
- [关于博主](https://kexue.fm/me.html)

欢迎访问“科学空间”，这里将与您共同探讨自然科学，回味人生百态；也期待大家的分享～

- [**千奇百怪** Everything](https://kexue.fm/category/Everything)
- [**天文探索** Astronomy](https://kexue.fm/category/Astronomy)
- [**数学研究** Mathematics](https://kexue.fm/category/Mathematics)
- [**物理化学** Phy-chem](https://kexue.fm/category/Phy-chem)
- [**信息时代** Big-Data](https://kexue.fm/category/Big-Data)
- [**生物自然** Biology](https://kexue.fm/category/Biology)
- [**图片摄影** Photograph](https://kexue.fm/category/Photograph)
- [**问题百科** Questions](https://kexue.fm/category/Questions)
- [**生活/情感** Life-Feeling](https://kexue.fm/category/Life-Feeling)
- [**资源共享** Resources](https://kexue.fm/category/Resources)

- [**千奇百怪**](https://kexue.fm/category/Everything)
- [**天文探索**](https://kexue.fm/category/Astronomy)
- [**数学研究**](https://kexue.fm/category/Mathematics)
- [**物理化学**](https://kexue.fm/category/Phy-chem)
- [**信息时代**](https://kexue.fm/category/Big-Data)
- [**生物自然**](https://kexue.fm/category/Biology)
- [**图片摄影**](https://kexue.fm/category/Photograph)
- [**问题百科**](https://kexue.fm/category/Questions)
- [**生活/情感**](https://kexue.fm/category/Life-Feeling)
- [**资源共享**](https://kexue.fm/category/Resources)

[首页](https://kexue.fm) [信息时代](https://kexue.fm/category/Big-Data) WGAN-div：一个默默无闻的WGAN填坑者

7Nov

# [WGAN-div：一个默默无闻的WGAN填坑者](https://kexue.fm/archives/6139)

By 苏剑林 \|
2018-11-07 \|
179347位读者\|

**今天我们来谈一下Wasserstein散度，简称“W散度”。注意，这跟Wasserstein距离（Wasserstein distance，简称“W距离”，又叫Wasserstein度量、Wasserstein metric）是不同的两个东西。**

本文源于论文 [《Wasserstein Divergence for GANs》](https://papers.cool/arxiv/1712.01026)，论文中提出了称为WGAN-div的GAN训练方案。这是一篇我很是欣赏却默默无闻的paper，我只是找文献时偶然碰到了它。不管英文还是中文界，它似乎都没有流行起来，但是我感觉它是一个相当漂亮的结果。

如果读者需要入门一下WGAN的相关知识，不妨请阅读拙作 [《互怼的艺术：从零直达WGAN-GP》](https://kexue.fm/archives/4439)。

## WGAN [\#](https://kexue.fm/archives/6139\#WGAN)

我们知道原始的GAN（SGAN）会有可能存在梯度消失的问题，因此WGAN横空出世了。

### W距离 [\#](https://kexue.fm/archives/6139\#W%E8%B7%9D%E7%A6%BB)

WGAN引入了最优传输里边的W距离来度量两个分布的距离：
\\begin{equation}W\_c\[\\tilde{p}(x), q(x)\] = \\inf\_{\\gamma\\in \\Pi(\\tilde{p}(x), q(x))} \\mathbb{E}\_{(x,y)\\sim \\gamma}\[c(x,y)\] \\end{equation}
这里的$\\tilde{p}(x)$是真实样本的分布，$q(x)$是伪造分布，$c(x,y)$是传输成本，论文中用的是$c(x,y)=\\Vert x-y\\Vert$；而$\\gamma\\in \\Pi(\\tilde{p}(x), q(x))$的意思是说：$\\gamma$是任意关于$x, y$的二元分布，其边缘分布则为$\\tilde{p}(x)$和$q(y)$。直观来看，$\\gamma$描述了一个运输方案，而$c(x,y)$则是运输成本，$W\_c\[\\tilde{p}(x), q(x)\]$就是说要找到成本最低的那个运输方案所对应的成本作为分布度量。

### 对偶问题 [\#](https://kexue.fm/archives/6139\#%E5%AF%B9%E5%81%B6%E9%97%AE%E9%A2%98)

W距离确实是一个很好的度量，但显然不好算。当$c(x,y)=\\Vert x-y\\Vert$时，我们可以将其转化为对偶问题：
\\begin{equation}W(\\tilde{p}(x), q(x)) = \\sup\_{\\Vert T\\Vert\_L\\leq 1} \\mathbb{E}\_{x\\sim \\tilde{p}(x)}\[T(x)\] - \\mathbb{E}\_{x\\sim q(x)}\[T(x)\]\\label{eq:wgan-d}\\end{equation}
其中$T(x)$是一个标量函数，$\\Vert T\\Vert\_L$则是Lipschitz范数：
\\begin{equation}\\Vert T\\Vert\_L = \\max\_{x\\neq y} \\frac{\|T(x)-T(y)\|}{\\Vert x - y\\Vert}\\end{equation}
说白了，$T(x)$要满足：
\\begin{equation}\|T(x)-T(y)\| \\leq \\Vert x - y\\Vert\\end{equation}

### 生成模型 [\#](https://kexue.fm/archives/6139\#%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B)

这样一来，生成模型的训练，可以作为W距离下的一个最小-最大问题：
\\begin{equation}\\mathop{\\text{argmin}}\_{G}\\mathop{\\text{argmax}}\_{T,\\Vert T\\Vert\_L\\leq 1} \\mathbb{E}\_{x\\sim \\tilde{p}(x)}\[T(x)\] - \\mathbb{E}\_{x\\sim q(z)}\[T(G(z))\]\\end{equation}
第一个$\\text{argmax}$试图获得W距离的近似表达式，而第二个$\\text{argmin}$则试图最小化W距离。

然而，$T$不是任意的，需要满足$\\Vert T\\Vert\_L\\leq 1$，这称为Lipschitz约束（L约束），该怎么施加这个约束呢？因此，一方面，WGAN开创了GAN的一个新流派，使得GAN的理论上了一个新高度，另一方面，WGAN也挖了一个关于L约束的大坑，这个坑也引得不少研究者前仆后继地...（跳坑？）

## L约束 [\#](https://kexue.fm/archives/6139\#L%E7%BA%A6%E6%9D%9F)

目前，往模型中加入L约束，有三种主要的方案。

### 权重裁剪 [\#](https://kexue.fm/archives/6139\#%E6%9D%83%E9%87%8D%E8%A3%81%E5%89%AA)

这是WGAN最原始的论文所提出的一种方案：在每一步的判别器的梯度下降后，将判别器的参数的绝对值裁剪到不超过某个固定常数。

这是一种非常朴素的做法，现在基本上已经不用了。其思想就是：L约束本质上就是要网络的波动程度不能超过一个线性函数，而激活函数通常都满足这个条件，所以只需要考虑网络权重，最简单的一种方案就是直接限制权重范围，这样就不会抖动太剧烈了。

### 梯度惩罚 [\#](https://kexue.fm/archives/6139\#%E6%A2%AF%E5%BA%A6%E6%83%A9%E7%BD%9A)

这种思路非常直接，即$\\Vert T\\Vert\_L\\leq 1$可以由$\\Vert \\nabla T\\Vert \\leq 1$来保证，所以干脆把判别器的梯度作为一个惩罚项加入到判别器的loss中：
\\begin{equation}T=\\mathop{\\text{argmin}}\_{T} -\\mathbb{E}\_{x\\sim \\tilde{p}(x)}\[T(x)\] + \\mathbb{E}\_{x\\sim q(x)}\[T(x)\] + \\lambda \\mathbb{E}\_{x\\sim r(x)}\\Big\[\\big(\\Vert \\nabla T\\Vert - 1\\big)^2\\Big\]\\end{equation}
但问题是我们要求$\\Vert T\\Vert\_L\\leq 1$是在每一处都成立，所以$r(x)$应该是全空间的均匀分布才行，显然这很难做到。所以作者采用了一个非常机智（也有点流氓）的做法：在真假样本之间随机插值来惩罚，这样保证真假样本之间的过渡区域满足L约束。

这种方案就是WGAN-GP。显然，它比权重裁剪要高明一些，而且通常都work得很好。但是这种方案是一种经验方案，没有更完备的理论支撑。

### 谱归一化 [\#](https://kexue.fm/archives/6139\#%E8%B0%B1%E5%BD%92%E4%B8%80%E5%8C%96)

另一种实现L约束的方案就是谱归一化（SN），可以参考我之前写考 [《深度学习中的Lipschitz约束：泛化与生成模型》](https://kexue.fm/archives/6051)。

本质上来说，谱归一化和权重裁剪都是同一类方案，只是谱归一化的理论更完备，结果更加松弛。而且还有一点不同的是：权重裁剪是一种“事后”的处理方案，也就是每次梯度下降后才直接裁剪参数，这种处理方案本身就可能导致优化上的不稳定；谱归一化是一种“事前”的处理方案，它直接将每一层的权重都谱归一化后才进行运算，谱归一化作为了模型的一部分，更加合理一些。

尽管谱归一化更加高明，但是它跟权重裁剪一样存在一个问题：把判别器限制在了一小簇函数之间。也就是说，加了谱归一化的$T$，只是所有满足L约束的函数的一小部分。因为谱归一化事实上要求网络的每一层都满足L约束，但这个条件太死了，也许这一层可以不满足L约束，下一层则满足更强的L约束，两者抵消，整体就满足L约束，但谱归一化不能适应这种情况。

## WGAN-div [\#](https://kexue.fm/archives/6139\#WGAN-div)

在这种情况下， [《Wasserstein Divergence for GANs》](https://papers.cool/arxiv/1712.01026) 引入了W散度，它声称：现在我们可以去掉L约束了，并且还保留了W距离的好性质。

### 论文回顾 [\#](https://kexue.fm/archives/6139\#%E8%AE%BA%E6%96%87%E5%9B%9E%E9%A1%BE)

有这样的好事？我们来看看W散度是什么。一上来，作者先回顾了一些经典的GAN的训练方案，然后随手扔出一篇文献，叫做 [《Partial differential equations and monge-kantorovich mass transfer》](https://math.berkeley.edu/~evans/Monge-Kantorovich.survey.pdf)，里边提供了一个方案（下面的出场顺序跟论文有所不同），能直接将$T$训练出来，目标是（跟原文的写法有些不一样）
\\begin{equation}T^\* = \\mathop{\\text{argmax}}\_{T} \\mathbb{E}\_{x\\sim \\tilde{p}(x)}\[T(x)\] - \\mathbb{E}\_{x\\sim q(x)}\[T(x)\] - \\frac{1}{2}\\mathbb{E}\_{x\\sim r(x)}\[\\Vert \\nabla T\\Vert^2\]\\label{eq:wgan-div-d1}\\end{equation}
这里的$r(x)$是一个非常宽松的分布，我们后面再细谈。整个loss的意思是：你只要按照这个公式将$T$训练出来，它就是$\\eqref{eq:wgan-d}$式中的$T$的最优解，也就是说，接下来只要把它代进$\\eqref{eq:wgan-d}$式，就得到了W距离，最小化它就可以得到生成器了。
\\begin{equation}\\mathop{\\text{argmin}}\_{G}\\mathbb{E}\_{x\\sim \\tilde{p}(x)}\[T^\*(x)\] - \\mathbb{E}\_{x\\sim q(z)}\[T^\*(G(z))\]\\label{eq:wgan-div-g1}\\end{equation}

### 一些注解 [\#](https://kexue.fm/archives/6139\#%E4%B8%80%E4%BA%9B%E6%B3%A8%E8%A7%A3)

首先，我为什么说作者“随手”跑出一篇论文呢？因为作者确实是随手啊...

作者直接说“According to \[19\]”，然后就给出了后面的结果，\[19\]就是这篇论文，是一篇最优传输和偏微分方程的论文，59页...我翻来翻去，才发现作者引用的应该是36页和40页的结果（不过翻到了也没能进一步看懂，放弃了），也不提供多一点参考资料，尴尬～～还有后面的一些引理，作者也说“直接去看\[19\]的discussion吧”.....

然后，读者更多的疑问是：这玩意跟梯度惩罚方案有什么差别，加个负号变成最小化不都是差不多吗？做实验时也许没有多大差别，但是理论上的差别是很大的，因为WGAN-GP的梯度惩罚只能算是一种经验方案，而$\\eqref{eq:wgan-div-d1}$式是有理论保证的。后面我们会继续讲完它。

### W散度 [\#](https://kexue.fm/archives/6139\#W%E6%95%A3%E5%BA%A6)

式$\\eqref{eq:wgan-div-d1}$是一个理论结果，而不管怎样深度学习还是一门理论和工程结合的学科，所以作者一般化地考虑了下面的目标
\\begin{equation}W\_{k,p}\[\\tilde{p}(x), q(x)\] = \\max\_{T} \\mathbb{E}\_{x\\sim \\tilde{p}(x)}\[T(x)\] - \\mathbb{E}\_{x\\sim q(x)}\[T(x)\] - k\\mathbb{E}\_{x\\sim r(x)}\[\\Vert \\nabla T\\Vert^p\]\\label{eq:wdiv}\\end{equation}
其中$k > 0, p > 1$。基于此，作者证明了$W\_{k,p}$有非常好的性质：

> 1、$W\_{k,p}$是个对称的散度。散度的意思是：$\\mathcal{D}\[P,Q\]\\geq 0$且$\\mathcal{D}\[P,Q\]=0\\Leftrightarrow P=Q$，它跟“距离”的差别是它不一定满足三角不等式，也有叫做“半度量”、“半距离”的。$W\_{k,p}$是一个散度，这已经非常棒了，因为我们大多数GAN都只是在优化某个散度而已。散度意味着当我们最小化它时，我们真正是在缩小两个分布的距离。
>
> 2、$W\_{k,p}$的最优解跟W距离有一定的联系。$\\eqref{eq:wgan-div-d1}$式就是一个特殊的$W\_{1/2,2}$。这说明当我们最大化$W\_{k,p}$得到$T$之后，可以去掉梯度项，通过最小化$\\eqref{eq:wgan-div-g1}$来训练生成器。这也表明以$W\_{k,p}$为目标，性质跟W距离类似，不会有梯度消失的问题。
>
> 3、这是我觉得最逗的一点，作者证明了
> \\begin{equation}\\max\_{T} \\mathbb{E}\_{x\\sim \\tilde{p}(x)}\[T(x)\] - \\mathbb{E}\_{x\\sim q(x)}\[T(x)\] - k\\mathbb{E}\_{x\\sim r(x)}\[(\\Vert \\nabla T\\Vert - n)^p\]\\end{equation}
> 不总是一个散度。当$n=1,p=2$时这就是WGAN-GP的梯度惩罚，作者说它不是一个散度，明摆着要跟WGAN-GP对着干，哈哈哈～～不是散度意味着WGAN-GP在训练判别器的时候，并非总是会在拉大两个分布的距离（鉴别者在偷懒，没有好好提升自己的鉴别技能），从而使得训练生成器时回传的梯度不准。

### WGAN-div [\#](https://kexue.fm/archives/6139\#WGAN-div)

好了，说了这么久，终于可以引入WGAN-div了，其实就是基于$\\eqref{eq:wdiv}$的WGAN的训练模式了：
\\begin{equation}\\begin{aligned}T =& \\mathop{\\text{argmax}}\_{T} \\mathbb{E}\_{x\\sim \\tilde{p}(x)}\[T(x)\] - \\mathbb{E}\_{x\\sim q(x)}\[T(x)\] - k\\mathbb{E}\_{x\\sim r(x)}\[\\Vert \\nabla T\\Vert^p\]\\\
G =& \\mathop{\\text{argmin}}\_{G} \\mathbb{E}\_{x\\sim \\tilde{p}(x)}\[T(x)\] - \\mathbb{E}\_{x\\sim q(z)}\[T(G(z))\]\\end{aligned}\\end{equation}
前者是为了通过W散度$W\_{k,p}$找出W距离中最优的$T$，后者就是为了最小化W距离。所以，W散度的角色，就是一个为W距离的默默无闻的填坑者呀，再结合这篇论文本身的鲜有反响，我觉得这种感觉更加强烈了。

## 实验 [\#](https://kexue.fm/archives/6139\#%E5%AE%9E%E9%AA%8C)

### k,p的选择 [\#](https://kexue.fm/archives/6139\#k,p%E7%9A%84%E9%80%89%E6%8B%A9)

作者通过做了一批搜索实验，发现$k=2,p=6$时效果最好（用FID为指标）。这进一步与WGAN-GP的做法有出入：范数的二次幂并非是最好的选择。

不同的k,p对FID的影响（FID越小越好）

### r(x)的选择 [\#](https://kexue.fm/archives/6139\#r(x)%E7%9A%84%E9%80%89%E6%8B%A9)

前面我们就说过，W散度中对$r(x)$的要求非常宽松，论文也做了一组对比实验，对比了常见的做法：

> 1、真假样本随机插值；
>
> 2、真样本之间随机插值、假样本之间随机插值；
>
> 3、真假样本混合后，随机选两个样本插值；
>
> 4、直接选原始的真假样本混合；
>
> 5、直接只选原始的假样本；
>
> 6、直接只选原始的真样本。

结果发现，在WGAN-div之下这几种做法表现都差不多（用FID为指标），但是对于WGAN-GP，这几种做法差别比较大，而且WGAN-GP中最好的结果比WGAN-div中最差的结果还要差。这时候WGAN-GP就被彻底虐倒了...

不同的采样方式所导致的不同模型的FID不同差异（FID越小越好）

这里边的差别不难解释，WGAN-GP是凭经验加上梯度惩罚，并且“真假样本随机插值”只是它无法做到全空间采样的一个折衷做法，但是W散度和WGAN-div，从理论的开始，就没对$r(x)$有什么严格的限制。其实，原始W散度的构造（这个需要看参考论文）基本上只要求$r(x)$是一个样本空间跟$\\tilde{p}(x)、q(x)$一样的分布，非常弱的要求，而我们一般选择为$\\tilde{p}(x)、q(x)$两者共同衍生出来的分布，相对来说收敛快一点。

### 参考代码 [\#](https://kexue.fm/archives/6139\#%E5%8F%82%E8%80%83%E4%BB%A3%E7%A0%81)

自然是用Keras写的～人生苦短，我用Keras
[https://github.com/bojone/gan/blob/master/keras/wgan\_div\_celeba.py](https://github.com/bojone/gan/blob/master/keras/wgan_div_celeba.py)

随机样本（自己的实验结果）：

WGAN-div的部分样本（2w iter）

当然，原论文的实验结果也表明WGAN-div是很优秀的：

WGAN-div与不同的模型在不同的数据集效果比较（指标为FID，越小越好）

## 结语 [\#](https://kexue.fm/archives/6139\#%E7%BB%93%E8%AF%AD)

不知道业界是怎么看这篇WGAN-div的，也许是觉得跟WGAN-GP没什么不同，就觉得没有什么意思了。不过我是很佩服这些从理论上推导并且改进原始结果的大牛及其成果。虽然看起来像是随手甩了一篇论文然后说“你看着办吧”的感觉，但这种将理论和实践结合起来的结果仍然是很有美感的。

本来我对WGAN-GP是多少有些芥蒂的，总觉得它太丑，不想用。但是WGAN-div出现了，在我心中已经替代了WGAN-GP，并且它不再丑了～

_**转载到请包括本文地址：** [https://kexue.fm/archives/6139](https://kexue.fm/archives/6139)_

_**更详细的转载事宜请参考：**_ [《科学空间FAQ》](https://kexue.fm/archives/6508#%E6%96%87%E7%AB%A0%E5%A6%82%E4%BD%95%E8%BD%AC%E8%BD%BD/%E5%BC%95%E7%94%A8)

**如果您还有什么疑惑或建议，欢迎在下方评论区继续讨论。**

**如果您觉得本文还不错，欢迎 [分享](https://kexue.fm/archives/6139#share)/ [打赏](https://kexue.fm/archives/6139#pay) 本文。打赏并非要从中获得收益，而是希望知道科学空间获得了多少读者的真心关注。当然，如果你无视它，也不会影响你的阅读。再次表示欢迎和感谢！**

打赏

微信打赏

支付宝打赏

因为网站后台对打赏并无记录，因此欢迎在打赏时候备注留言。你还可以 [**点击这里**](http://mail.qq.com/cgi-bin/qm_share?t=qm_mailme&email=tN7d1drY3drrx8H0xcWa19vZ) 或在下方评论区留言来告知你的建议或需求。

**如果您需要引用本文，请参考：**

苏剑林. (Nov. 07, 2018). 《WGAN-div：一个默默无闻的WGAN填坑者 》\[Blog post\]. Retrieved from [https://kexue.fm/archives/6139](https://kexue.fm/archives/6139)

@online{kexuefm-6139,
        title={WGAN-div：一个默默无闻的WGAN填坑者},
        author={苏剑林},
        year={2018},
        month={Nov},
        url={\\url{https://kexue.fm/archives/6139}},
}

分类： [信息时代](https://kexue.fm/category/Big-Data)    标签： [GAN](https://kexue.fm/tag/GAN/), [生成模型](https://kexue.fm/tag/%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B/), [度量](https://kexue.fm/tag/%E5%BA%A6%E9%87%8F/)[50 评论](https://kexue.fm/archives/6139#comments)

< [缅怀金庸 \| 愿你登上10930小行星继续翱翔](https://kexue.fm/archives/6131) \| [又一道川菜！媲美“开水白菜”的瓜燕穗肚](https://kexue.fm/archives/6158) >

### 你也许还对下面的内容感兴趣

- [线性注意力简史：从模仿、创新到反哺](https://kexue.fm/archives/11033)
- [生成扩散模型漫谈（三十）：从瞬时速度到平均速度](https://kexue.fm/archives/10958)
- [Transformer升级之路：20、MLA好在哪里?（上）](https://kexue.fm/archives/10907)
- [生成扩散模型漫谈（二十九）：用DDPM来离散编码](https://kexue.fm/archives/10711)
- [细水长flow之TARFLOW：流模型满血归来？](https://kexue.fm/archives/10667)
- [生成扩散模型漫谈（二十八）：分步理解一致性模型](https://kexue.fm/archives/10633)
- [生成扩散模型漫谈（二十七）：将步长作为条件输入](https://kexue.fm/archives/10617)
- [生成扩散模型漫谈（二十六）：基于恒等式的蒸馏（下）](https://kexue.fm/archives/10567)
- [VQ的又一技巧：给编码表加一个线性变换](https://kexue.fm/archives/10519)
- [VQ的旋转技巧：梯度直通估计的一般推广](https://kexue.fm/archives/10489)

[发表你的看法](https://kexue.fm/archives/6139#comment_form)

1. [«](https://kexue.fm/archives/6139/comment-page-1#comments)
2. [1](https://kexue.fm/archives/6139/comment-page-1#comments)
3. [2](https://kexue.fm/archives/6139/comment-page-2#comments)

膜拜大神

December 13th, 2019

或者说为什么不能加在生成器里

[回复评论](https://kexue.fm/archives/6139/comment-page-2?replyTo=12614#respond-post-6139)

[苏剑林](https://kexue.fm) 发表于
December 13th, 2019

你需要找一下wgan以及wgan-gp相关论文，来看一下为什么我们需要gp

[回复评论](https://kexue.fm/archives/6139/comment-page-2?replyTo=12619#respond-post-6139)

李梓强 发表于
April 15th, 2020

最近有文章加在生成器了，不过没有理论证明。DIVERSITY-SENSITIVE CONDITIONAL
GENERATIVE ADVERSARIAL NETWORKS
我觉得并不是很优美

[回复评论](https://kexue.fm/archives/6139/comment-page-2?replyTo=13186#respond-post-6139)

李梓强 发表于
April 15th, 2020

[@李梓强\|comment-13186](https://kexue.fm/archives/6139/comment-page-2#comment-13186)
也不能算是用在生成器了吧，这篇文章是加了正则项，为了使隐空间的改变在图像空间产生尽可能大的变化，和wgan中的L约束正好相反，但实现用的和gp是一样的，都是做差求了梯度。

[回复评论](https://kexue.fm/archives/6139/comment-page-2?replyTo=13187#respond-post-6139)

两点疑惑

February 19th, 2020

作者您好，拜读了您写的三篇文章，受益匪浅，但有几点存疑，期待您的解答。
1.根据我自己的理解，WGAN使用的W距离实际上是使用一个二重性转换取的近似值，而L约束是为了使这个近似计算成立，换言之，使用W距离时就必须要满足L连续，这样的理解是否正确？
2.WGAN-div不需要满足L连续条件，这是因为WGAN使用的是W散度。而不管我怎么看，W散度和W距离都像是双胞胎兄弟，不同之处就在于W散度多了一个梯度项，也正是前面两项为W散度提供了W距离的优势，那为什么W散度不需要L条件？是否可以这样理解：W散度中的梯度项在某种角度上使其满足了L约束，从本质上来说，WGAN-div是更一般形式的WGAN-GP？

[回复评论](https://kexue.fm/archives/6139/comment-page-2?replyTo=12892#respond-post-6139)

[苏剑林](https://kexue.fm) 发表于
February 21st, 2020

1、WGAN的重点是对偶变换，所谓对偶变换其实就是将某个约束下的极值问题转换为另一个约束下的另一个极值问题。原来W距离的定义下，约束条件是
$$\\begin{equation}\\int \\gamma(\\boldsymbol{x},\\boldsymbol{y}) d\\boldsymbol{y}=p(\\boldsymbol{x}),\\quad\\int \\gamma(\\boldsymbol{x},\\boldsymbol{y}) d\\boldsymbol{x}=q(\\boldsymbol{y}),\\quad \\gamma(\\boldsymbol{x},\\boldsymbol{y})\\geq 0\\end{equation}$$
转换之后，约束条件就是L约束，具体内容请参考： [https://kexue.fm/archives/6280](https://kexue.fm/archives/6280)

2、“W散度和W距离都像是双胞胎兄弟”这一点我无法直接看出来，“WGAN-div是更一般形式的WGAN-GP”这个理解没有问题。但是你要注意，虽然WGAN-GP算是WGAN的一种实现，但是严格来讲，WGAN-GP都不算WGAN（通过惩罚项加的约束都不是严格的约束，而理论上的WGAN需要严格的约束）。

[回复评论](https://kexue.fm/archives/6139/comment-page-2?replyTo=12895#respond-post-6139)

bob

June 7th, 2021

大佬，如果将W-div公式中附加的梯度项的分布选择为真实样本，那么优化生成器的过程是否可以理解为最小化W散度呢？

[回复评论](https://kexue.fm/archives/6139/comment-page-2?replyTo=16588#respond-post-6139)

[苏剑林](https://kexue.fm) 发表于
June 7th, 2021

可以吧，具体疑惑是啥？

[回复评论](https://kexue.fm/archives/6139/comment-page-2?replyTo=16590#respond-post-6139)

bob 发表于
June 7th, 2021

那这样判别器计算出W散度，生成器最小化W散度，整个模型是以W散度为优化目标。如果这么看，W散度和W距离的关系也就不那么重要了。这么理解对不对，麻烦大佬解答。

[回复评论](https://kexue.fm/archives/6139/comment-page-2?replyTo=16595#respond-post-6139)

[苏剑林](https://kexue.fm) 发表于
June 8th, 2021

本身就不重要，而且这篇文章 [https://kexue.fm/archives/8244](https://kexue.fm/archives/8244) 还说了，WGAN的成功跟W距离没啥关系。

[回复评论](https://kexue.fm/archives/6139/comment-page-2?replyTo=16602#respond-post-6139)

一点疑惑

June 7th, 2021

苏神，论文论述W distance与W divergence关系的时候用的是k=1/2 p=2。后面将其推广到Wgan-div的目标函数时，似乎没有给出证明吧，这是否有些不严谨呢

[回复评论](https://kexue.fm/archives/6139/comment-page-2?replyTo=16596#respond-post-6139)

[苏剑林](https://kexue.fm) 发表于
June 8th, 2021

论证过程是对于任意$k,p$的。不过你也可以当作纯粹的调参就是了...

[回复评论](https://kexue.fm/archives/6139/comment-page-2?replyTo=16603#respond-post-6139)

Xxytz\_

April 17th, 2022

大佬，我想问个关于BN的问题。WGAN-div和gp都是额外加了梯度惩罚，wgan-gp的论文有说“要对每个样本独立地施加梯度惩罚，所以在判别器的模型架构中不能使用BN算法，因为它会引入同一个批次中不同样本的相互依赖关系”，但我看你编的wgan-div中的残差块中又有BN层讷？这是为什么讷?

[回复评论](https://kexue.fm/archives/6139/comment-page-2?replyTo=18980#respond-post-6139)

[苏剑林](https://kexue.fm) 发表于
April 19th, 2022

这是个好问题。事实上最开始成功的DCGAN架构就是用了BN，而且将BN换成别的效果会明显变差。所以只能说GAN这个训练本身就是一件很玄学的事情，很多细节真的禁不住深究...

[回复评论](https://kexue.fm/archives/6139/comment-page-2?replyTo=18989#respond-post-6139)

苏神死忠粉111

May 16th, 2022

苏神你好，我拜读过您很多有关GAN的文章，觉得您对WGAN,WGAN-GP的理解都很好，而且几年前有很多研究也从实验上证明了应该让梯度收敛于0而不是1 (Which Training Methods for GANs do actually Converge?). 我是做计算生物的，但是我最近用一个基于WGAN的研究发现总是收敛于1的情况好于收敛于0的情况，而且我用最新的gradient normalization也不会帮助提升效果。我认为这不是bug，但也想不明白，因此请问您感觉这其中的缘由是什么，望不吝赐教。我认为是我们定义的问题不好。如果您有兴趣的话，我们可以等paper pub之后发给您。

[回复评论](https://kexue.fm/archives/6139/comment-page-2?replyTo=19116#respond-post-6139)

[苏剑林](https://kexue.fm) 发表于
May 16th, 2022

当梯度惩罚的中心为0时，理论支撑是连同梯度惩罚一起视为一个新的概率散度，有可能是这种新的散度并不适应你当前的数据。

[回复评论](https://kexue.fm/archives/6139/comment-page-2?replyTo=19125#respond-post-6139)

苏神死忠粉111 发表于
May 16th, 2022

谢谢苏神，那我觉得这个gradient center其实应该是一个超参数，虽然GP带来的其实不是一个散度，但是并不是所有的数据都是0 center divergence最适用的。

[回复评论](https://kexue.fm/archives/6139/comment-page-2?replyTo=19132#respond-post-6139)

[苏剑林](https://kexue.fm) 发表于
May 18th, 2022

是的。而且GAN的成功涉及到很多因素，与SGD结合的动力学分析也很重要，但也很难分析清楚。

[回复评论](https://kexue.fm/archives/6139/comment-page-2?replyTo=19140#respond-post-6139)

1. [«](https://kexue.fm/archives/6139/comment-page-1#comments)
2. [1](https://kexue.fm/archives/6139/comment-page-1#comments)
3. [2](https://kexue.fm/archives/6139/comment-page-2#comments)

[取消回复](https://kexue.fm/archives/6139#respond-post-6139)

你的大名

电子邮箱

个人网站（选填）

1\. 可以使用LaTeX代码，点击“预览效果”可查看效果；2. 可以通过点击评论楼层编号来引用该楼层；3. 网站可能会有点卡，如非确认评论失败，请 **不要重复点击提交**。

### 内容速览

[WGAN](https://kexue.fm/archives/6139#WGAN)
[W距离](https://kexue.fm/archives/6139#W%E8%B7%9D%E7%A6%BB)
[对偶问题](https://kexue.fm/archives/6139#%E5%AF%B9%E5%81%B6%E9%97%AE%E9%A2%98)
[生成模型](https://kexue.fm/archives/6139#%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B)
[L约束](https://kexue.fm/archives/6139#L%E7%BA%A6%E6%9D%9F)
[权重裁剪](https://kexue.fm/archives/6139#%E6%9D%83%E9%87%8D%E8%A3%81%E5%89%AA)
[梯度惩罚](https://kexue.fm/archives/6139#%E6%A2%AF%E5%BA%A6%E6%83%A9%E7%BD%9A)
[谱归一化](https://kexue.fm/archives/6139#%E8%B0%B1%E5%BD%92%E4%B8%80%E5%8C%96)
[WGAN-div](https://kexue.fm/archives/6139#WGAN-div)
[论文回顾](https://kexue.fm/archives/6139#%E8%AE%BA%E6%96%87%E5%9B%9E%E9%A1%BE)
[一些注解](https://kexue.fm/archives/6139#%E4%B8%80%E4%BA%9B%E6%B3%A8%E8%A7%A3)
[W散度](https://kexue.fm/archives/6139#W%E6%95%A3%E5%BA%A6)
[WGAN-div](https://kexue.fm/archives/6139#WGAN-div)
[实验](https://kexue.fm/archives/6139#%E5%AE%9E%E9%AA%8C)
[k,p的选择](https://kexue.fm/archives/6139#k,p%E7%9A%84%E9%80%89%E6%8B%A9)
[r(x)的选择](https://kexue.fm/archives/6139#r(x)%E7%9A%84%E9%80%89%E6%8B%A9)
[参考代码](https://kexue.fm/archives/6139#%E5%8F%82%E8%80%83%E4%BB%A3%E7%A0%81)
[结语](https://kexue.fm/archives/6139#%E7%BB%93%E8%AF%AD)

### 智能搜索

支持整句搜索！网站自动使用 [结巴分词](https://github.com/fxsjy/jieba) 进行分词，并结合ngrams排序算法给出合理的搜索结果。

### 热门标签

[生成模型](https://kexue.fm/tag/%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B/) [attention](https://kexue.fm/tag/attention/) [优化](https://kexue.fm/tag/%E4%BC%98%E5%8C%96/) [语言模型](https://kexue.fm/tag/%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/) [模型](https://kexue.fm/tag/%E6%A8%A1%E5%9E%8B/) [网站](https://kexue.fm/tag/%E7%BD%91%E7%AB%99/) [概率](https://kexue.fm/tag/%E6%A6%82%E7%8E%87/) [梯度](https://kexue.fm/tag/%E6%A2%AF%E5%BA%A6/) [转载](https://kexue.fm/tag/%E8%BD%AC%E8%BD%BD/) [微分方程](https://kexue.fm/tag/%E5%BE%AE%E5%88%86%E6%96%B9%E7%A8%8B/) [矩阵](https://kexue.fm/tag/%E7%9F%A9%E9%98%B5/) [天象](https://kexue.fm/tag/%E5%A4%A9%E8%B1%A1/) [分析](https://kexue.fm/tag/%E5%88%86%E6%9E%90/) [深度学习](https://kexue.fm/tag/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/) [积分](https://kexue.fm/tag/%E7%A7%AF%E5%88%86/) [python](https://kexue.fm/tag/python/) [优化器](https://kexue.fm/tag/%E4%BC%98%E5%8C%96%E5%99%A8/) [力学](https://kexue.fm/tag/%E5%8A%9B%E5%AD%A6/) [无监督](https://kexue.fm/tag/%E6%97%A0%E7%9B%91%E7%9D%A3/) [扩散](https://kexue.fm/tag/%E6%89%A9%E6%95%A3/) [几何](https://kexue.fm/tag/%E5%87%A0%E4%BD%95/) [节日](https://kexue.fm/tag/%E8%8A%82%E6%97%A5/) [生活](https://kexue.fm/tag/%E7%94%9F%E6%B4%BB/) [文本生成](https://kexue.fm/tag/%E6%96%87%E6%9C%AC%E7%94%9F%E6%88%90/) [数论](https://kexue.fm/tag/%E6%95%B0%E8%AE%BA/)

### 随机文章

- [生成扩散模型漫谈（六）：一般框架之ODE篇](https://kexue.fm/archives/9228)
- [美国科学家用3000幅照片拼接夜空全景](https://kexue.fm/archives/244)
- [关于“平衡态公理”的更正与思考](https://kexue.fm/archives/1902)
- [fashion-mnist的gan玩具](https://kexue.fm/archives/4540)
- [生活\|我家的几只小鸡](https://kexue.fm/archives/177)
- [变分法的一个技巧及其“误用”](https://kexue.fm/archives/2040)
- [正十七边形的尺规作图存在之证明](https://kexue.fm/archives/133)
- [Welcome New Server for CosmoStation](https://kexue.fm/archives/735)
- [军训中的数学——握手奇数次的人数](https://kexue.fm/archives/1713)
- [蘑菇的最优形状模型](https://kexue.fm/archives/1339)

### 最近评论

- [abcm](https://kexue.fm/archives/10114/comment-page-3#comment-28017): 公式(14)为什么是直接带入，我的理解是0到T积分再换元
\\begin{align}
\\int...
- [Truenobility303](https://kexue.fm/archives/10739/comment-page-2#comment-28016): 苏神好，关于RMS对齐这里有些疑问。1. 如果从 A Spectral Condition f...
- [wolfzdf](https://kexue.fm/archives/9907/comment-page-4#comment-27987): 您好，请问cool paper中收集的会议论文，有保存整理文章（通讯）作者的邮箱吗？
现在计算...
- [无敌大铁锤](https://kexue.fm/archives/11033/comment-page-1#comment-27986): 嗯嗯笔误了，就是去掉Causal的mask，变成一个纯Self-Attention的形式,然后...
- [HikaruNight](https://kexue.fm/archives/8397/comment-page-3#comment-27985): 苏老师，想请教一下如果把四元数放在三维RoPE里面是不是可行的
- [Leco](https://kexue.fm/archives/9590/comment-page-2#comment-27984): 请问LoRA的A,B矩阵初始化时，一个高斯随机一个全零还是只能A高斯，B全零呢？
- [苏剑林](https://kexue.fm/archives/9119/comment-page-13#comment-27983): 如果你把你这里提到的数学都学通透了，数学基础基本上可以胜任95%以上的场景了吧？至于“直觉”这...
- [苏剑林](https://kexue.fm/archives/10958/comment-page-2#comment-27982): 我跑过这个项目，效果是能复现的。“在 CIFAR-10 上效果非常差，生成的图片都是模糊的”是...
- [Henry Zha](https://kexue.fm/archives/9119/comment-page-13#comment-27981): 苏神你好，我是一名管理科学与工程专业的博士生，研究方向是结合人工智能模型建模用户行为之类的管理...
- [SunlightZero](https://kexue.fm/archives/10958/comment-page-2#comment-27980): 我根据 https://github.com/haidog-yaqub/MeanFlow 尝试...

### 友情链接

- [Cool Papers](https://papers.cool)
- [数学研发](https://bbs.emath.ac.cn)
- [Seatop](http://www.seatop.com.cn/)
- [Xiaoxia](https://xiaoxia.org/)
- [积分表-网络版](https://kexue.fm/sci/integral/index.html)
- [丝路博傲](http://blog.dvxj.com/)
- [ph4ntasy 饭特稀](http://www.ph4ntasy.com/)
- [数学之家](http://www.2math.cn/)
- [有趣天文奇观](http://interesting-sky.china-vo.org/)
- [TwistedW](http://www.twistedwg.com/)
- [godweiyang](https://godweiyang.com/)
- [AI柠檬](https://blog.ailemon.net/)
- [王登科-DK博客](https://greatdk.com)
- [ESON](https://blog.eson.org/)
- [枫之羽](https://fzhiy.net/)
- [Mathor's blog](https://wmathor.com/)
- [coding-zuo](https://coding-zuo.github.io/)
- [博科园](https://www.bokeyuan.net/)
- [孔皮皮的博客](https://www.kppkkp.top/)
- [运鹏的博客](https://yunpengtai.top/)
- [jiming.site](https://jiming.site/)
- [OmegaXYZ](https://www.omegaxyz.com/)
- [Blog by Eacls](https://www.eacls.top/)
- [EAI猩球](https://www.robotech.ink/)
- [文举的博客](https://liwenju0.com/)
- [用代码打点酱油](https://bruceyuan.com/)
- [申请链接](https://kexue.fm/links.html)

本站采用创作共用版权协议，要求署名、非商业用途和保持一致。转载本站内容必须也遵循“ [署名-非商业用途-保持一致](http://creativecommons.org/licenses/by-nc-nd/2.5/cn/)”的创作共用协议。
© 2009-2025 Scientific Spaces. All rights reserved. Theme by [laogui](http://www.laogui.com). Powered by [Typecho](http://typecho.org). 备案号: [粤ICP备09093259号-1/2](https://beian.miit.gov.cn/)。