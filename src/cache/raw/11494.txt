![MobileSideBar](https://kexue.fm/usr/themes/geekg/images/slide-button.png)

## SEARCH

## MENU

- [打赏](https://kexue.fm/reward.html)
- [公式](https://kexue.fm/latex.html)
- [天象](https://kexue.fm/ac.html)
- [链接](https://kexue.fm/links.html)
- [时光](https://kexue.fm/me.html)
- [博览](https://kexue.fm/science.html)
- [归档](https://kexue.fm/content.html)

## CATEGORIES

- [千奇百怪](https://kexue.fm/category/Everything)
- [天文探索](https://kexue.fm/category/Astronomy)
- [数学研究](https://kexue.fm/category/Mathematics)
- [物理化学](https://kexue.fm/category/Phy-chem)
- [信息时代](https://kexue.fm/category/Big-Data)
- [生物自然](https://kexue.fm/category/Biology)
- [图片摄影](https://kexue.fm/category/Photograph)
- [问题百科](https://kexue.fm/category/Questions)
- [生活/情感](https://kexue.fm/category/Life-Feeling)
- [资源共享](https://kexue.fm/category/Resources)

## NEWPOSTS

- [让炼丹更科学一些（五）：基于梯度精...](https://kexue.fm/archives/11530)
- [让炼丹更科学一些（四）：新恒等式，...](https://kexue.fm/archives/11494)
- [为什么DeltaNet要加L2 N...](https://kexue.fm/archives/11486)
- [让炼丹更科学一些（三）：SGD的终...](https://kexue.fm/archives/11480)
- [让炼丹更科学一些（二）：将结论推广...](https://kexue.fm/archives/11469)
- [滑动平均视角下的权重衰减和学习率](https://kexue.fm/archives/11459)
- [生成扩散模型漫谈（三十一）：预测数...](https://kexue.fm/archives/11428)
- [Muon优化器指南：快速上手与关键细节](https://kexue.fm/archives/11416)
- [AdamW的Weight RMS的...](https://kexue.fm/archives/11404)
- [n个正态随机数的最大值的渐近估计](https://kexue.fm/archives/11390)

## COMMENTS

- [rynn: 现在推荐中不止有用vq做量化生成id进行自回归生成，也有用vq...](https://kexue.fm/archives/11328/comment-page-1#comment-29106)
- [Bin: 今天偶然从某个论坛看到有人推荐您的博客，定睛一看竟然是华师同院...](https://kexue.fm/archives/1990/comment-page-2#comment-29105)
- [Rapture D: 我有一个问题，为什么不考虑亥姆霍兹定理和斯托克斯公式。](https://kexue.fm/archives/11530/comment-page-1#comment-29104)
- [mofheka: 苏神是还在用jax是么？最近在做基于Google Pathwa...](https://kexue.fm/archives/11390/comment-page-1#comment-29103)
- [长琴: 看懂这篇博客也不是一件容易的事情。](https://kexue.fm/archives/11530/comment-page-1#comment-29102)
- [AlexLi: 苏老师，请教一下(7)式中将 μ(xt)μ(xt) 传给 $p...](https://kexue.fm/archives/9257/comment-page-4#comment-29101)
- [tyler\_zxc: "Performer的思想是将标准的Attention线性化，...](https://kexue.fm/archives/7921/comment-page-2#comment-29100)
- [我: 似乎并非mHC提出矩阵的思想？之前hyper connecti...](https://kexue.fm/archives/11494/comment-page-1#comment-29099)
- [winter: 苏神您好，假如对于比较均匀的attention weightP...](https://kexue.fm/archives/10847/comment-page-1#comment-29098)
- [苏剑林: KL散度、JS散度、W距离啥的，都行啊，看你喜欢哪个](https://kexue.fm/archives/8512/comment-page-2#comment-29097)

## USERLOGIN

- [登录](https://kexue.fm/admin/login.php)

[科学空间\|Scientific Spaces](https://kexue.fm/)

- [登录](https://kexue.fm/admin/login.php)
- [打赏](https://kexue.fm/reward.html)
- [公式](https://kexue.fm/latex.html)
- [天象](https://kexue.fm/ac.html)
- [链接](https://kexue.fm/links.html)
- [时光](https://kexue.fm/me.html)
- [博览](https://kexue.fm/science.html)
- [归档](https://kexue.fm/content.html)

渴望成为一个小飞侠

- [![](https://kexue.fm/usr/themes/geekg/images/rss.png)\\
\\
欢迎订阅](https://kexue.fm/feed)
- [![](https://kexue.fm/usr/themes/geekg/images/mail.png)\\
\\
个性邮箱](https://kexue.fm/archives/119)
- [![](https://kexue.fm/usr/themes/geekg/images/Saturn.png)\\
\\
天象信息](https://kexue.fm/ac.html)
- [![](https://kexue.fm/usr/themes/geekg/images/iss.png)\\
\\
观测ISS](https://kexue.fm/archives/41)
- [![](https://kexue.fm/usr/themes/geekg/images/pi.png)\\
\\
LaTeX](https://kexue.fm/latex.html)
- [![](https://kexue.fm/usr/themes/geekg/images/mlogo.png)\\
\\
关于博主](https://kexue.fm/me.html)

欢迎访问“科学空间”，这里将与您共同探讨自然科学，回味人生百态；也期待大家的分享～

- [**千奇百怪** Everything](https://kexue.fm/category/Everything)
- [**天文探索** Astronomy](https://kexue.fm/category/Astronomy)
- [**数学研究** Mathematics](https://kexue.fm/category/Mathematics)
- [**物理化学** Phy-chem](https://kexue.fm/category/Phy-chem)
- [**信息时代** Big-Data](https://kexue.fm/category/Big-Data)
- [**生物自然** Biology](https://kexue.fm/category/Biology)
- [**图片摄影** Photograph](https://kexue.fm/category/Photograph)
- [**问题百科** Questions](https://kexue.fm/category/Questions)
- [**生活/情感** Life-Feeling](https://kexue.fm/category/Life-Feeling)
- [**资源共享** Resources](https://kexue.fm/category/Resources)

- [**千奇百怪**](https://kexue.fm/category/Everything)
- [**天文探索**](https://kexue.fm/category/Astronomy)
- [**数学研究**](https://kexue.fm/category/Mathematics)
- [**物理化学**](https://kexue.fm/category/Phy-chem)
- [**信息时代**](https://kexue.fm/category/Big-Data)
- [**生物自然**](https://kexue.fm/category/Biology)
- [**图片摄影**](https://kexue.fm/category/Photograph)
- [**问题百科**](https://kexue.fm/category/Questions)
- [**生活/情感**](https://kexue.fm/category/Life-Feeling)
- [**资源共享**](https://kexue.fm/category/Resources)

[首页](https://kexue.fm/) [数学研究](https://kexue.fm/category/Mathematics) 让炼丹更科学一些（四）：新恒等式，新学习率

26Dec

# [让炼丹更科学一些（四）：新恒等式，新学习率](https://kexue.fm/archives/11494)

By 苏剑林 \|
2025-12-26 \|
5526位读者 \|

上篇文章 [《让炼丹更科学一些（三）：SGD的终点损失收敛》](https://kexue.fm/archives/11480) 中我们成功将收敛结论从平均损失转化成终点损失，得到了O(lnT/T−−−−−−√)O(ln⁡T/T)的收敛速度。然而，仔细思考之下我们会发现这个结果其实不大符合直觉：按照经验，终点损失应该更接近最优值才对，平均损失的收敛速度都能做到O(1/T−−√)O(1/T)，怎么终点收敛速度反而更慢呢？

这个问题的最新进展是 [《Optimal Linear Decay Learning Rate Schedules and Further Refinements》](https://papers.cool/arxiv/2310.07831)，论文先推广了之前证明的关键恒等式，然后指出学习率调度对终点收敛的重要性，由此将终点损失的收敛加速至O(1/T−−√)O(1/T)。

## 新恒等式 [\#](https://kexue.fm/archives/11494\#%E6%96%B0%E6%81%92%E7%AD%89%E5%BC%8F)

原论文的结果很丰富，我们将分多篇文章介绍，这篇文章主要顺着上一篇的思路先做个初步介绍。为了将平均损失的收敛结论转换成终点损失，上一篇文章引入的关键恒等式是

qT=1T∑t=1Tqt+∑k=1T−11k(k+1)∑t=T−kT(qt−qT−k)(1)(1)qT=1T∑t=1Tqt+∑k=1T−11k(k+1)∑t=T−kT(qt−qT−k)

这篇文章我们将它推广成加权平均版：定义wk:T≜∑Tt=kwtwk:T≜∑t=kTwt，那么有

qT=1w1:T∑t=1Twtqt+∑k=1T−1(1wk+1:T−1wk:T)∑t=kTwt(qt−qk)(2)(2)qT=1w1:T∑t=1Twtqt+∑k=1T−1(1wk+1:T−1wk:T)∑t=kTwt(qt−qk)

证明思路基本一致，设λk=∑Tt=T−k+1wt,Sk=1λk∑Tt=T−k+1wtqtλk=∑t=T−k+1Twt,Sk=1λk∑t=T−k+1Twtqt，那么

λkSk===λk+1Sk+1−wT−kqT−kλkSk+1+wT−k(Sk+1−qT−k)λkSk+1+wT−kλk+1∑t=T−kTwt(qt−qT−k)(3)(3)λkSk=λk+1Sk+1−wT−kqT−k=λkSk+1+wT−k(Sk+1−qT−k)=λkSk+1+wT−kλk+1∑t=T−kTwt(qt−qT−k)

两边除以λkλk，然后对k=1∼T−1k=1∼T−1求和得

S1=ST+∑k=1T−1wT−kλkλk+1∑t=T−kTwt(qt−qT−k)(4)(4)S1=ST+∑k=1T−1wT−kλkλk+1∑t=T−kTwt(qt−qT−k)

留意到wT−kλkλk+1=1λk−1λk+1wT−kλkλk+1=1λk−1λk+1，然后代入S1,STS1,ST的定义，得

qT=1∑Tt=1wt∑t=1Twtqt+∑k=1T−1(1∑Tt=T−k+1wt−1∑Tt=T−kwt)∑t=T−kTwt(qt−qT−k)(5)(5)qT=1∑t=1Twt∑t=1Twtqt+∑k=1T−1(1∑t=T−k+1Twt−1∑t=T−kTwt)∑t=T−kTwt(qt−qT−k)

最后用kk换元T−kT−k，即得式[(2)](https://kexue.fm/archives/11494#mjx-eqn-eq%3Aqt-g)(2)。

## 一般结论 [\#](https://kexue.fm/archives/11494\#%E4%B8%80%E8%88%AC%E7%BB%93%E8%AE%BA)

接着，我们还是从第二篇文章 [《让炼丹更科学一些（二）：将结论推广到无界域》](https://kexue.fm/archives/11469) 的核心不等式出发

∑t=1TηtE\[L(θt)−L(φ)\]≤∥θ1−φ∥22+G22∑t=1Tη2t(6)(6)∑t=1TηtE\[L(θt)−L(φ)\]≤‖θ1−φ‖22+G22∑t=1Tηt2

照着 [《让炼丹更科学一些（三）：SGD的终点损失收敛》](https://kexue.fm/archives/11480) 的“ [准备工作](https://kexue.fm/archives/11480#%E5%87%86%E5%A4%87%E5%B7%A5%E4%BD%9C)”一节的思路，我们将起点改为kk，然后代入φ=θkφ=θk，但不需要假设ηtηt的单调性来两边除以ηTηT，而是直接得到

∑t=kTηtE\[L(θt)−L(θk)\]≤G22∑t=kTη2t(7)(7)∑t=kTηtE\[L(θt)−L(θk)\]≤G22∑t=kTηt2

代入wt=ηt,qt=E\[L(θt)\]−L(θ∗)wt=ηt,qt=E\[L(θt)\]−L(θ∗)到恒等式[(2)](https://kexue.fm/archives/11494#mjx-eqn-eq%3Aqt-g)(2)，我们得到

E\[L(θT)−L(θ∗)\]=≤1η1:T∑t=1TηtE\[L(θt)−L(θ∗)\][(6)](https://kexue.fm/archives/11494#mjx-eqn-leq%3Aavg-2-mid3)+∑k=1T−1(1ηk+1:T−1ηk:T)∑t=kTηtE\[L(θt)−L(θk)\][(7)](https://kexue.fm/archives/11494#mjx-eqn-leq%3Aavg-2-mid4)∥θ1−φ∥22η1:T+G22η1:T∑t=1Tη2t+G22∑k=1T−1(1ηk+1:T−1ηk:T)∑t=kTη2t(8)(8)E\[L(θT)−L(θ∗)\]=1η1:T∑t=1TηtE\[L(θt)−L(θ∗)\]⏟(6)+∑k=1T−1(1ηk+1:T−1ηk:T)∑t=kTηtE\[L(θt)−L(θk)\]⏟(7)≤‖θ1−φ‖22η1:T+G22η1:T∑t=1Tηt2+G22∑k=1T−1(1ηk+1:T−1ηk:T)∑t=kTηt2

对第二项利用∑T−1k=1∑Tt=k=∑Tt=1∑min(t,T−1)k=1∑k=1T−1∑t=kT=∑t=1T∑k=1min(t,T−1)：

∑k=1T−1(1ηk+1:T−1ηk:T)∑t=kTη2t=∑t=1Tη2t∑k=1min(t,T−1)(1ηk+1:T−1ηk:T)=∑t=1Tη2t(1ηmin(t+1,T):T−1η1:T)(9)(9)∑k=1T−1(1ηk+1:T−1ηk:T)∑t=kTηt2=∑t=1Tηt2∑k=1min(t,T−1)(1ηk+1:T−1ηk:T)=∑t=1Tηt2(1ηmin(t+1,T):T−1η1:T)

代入到式[(8)](https://kexue.fm/archives/11494#mjx-eqn-leq%3Aavg-2-mid5)(8)得

E\[L(θT)−L(θ∗)\]≤∥θ1−φ∥22η1:T+G22∑t=1Tη2tηmin(t+1,T):T(10)(10)E\[L(θT)−L(θ∗)\]≤‖θ1−φ‖22η1:T+G22∑t=1Tηt2ηmin(t+1,T):T

这就是终点损失收敛的加强版结果，它不依赖于学习率的单调递减性，也不依赖于两端除以ηTηT的操作，从而给学习率调度提供了更灵活的空间。上一篇文章只相当于把η1:T,ηmin(t+1,T):Tη1:T,ηmin(t+1,T):T都简单替换成了TηT,max(1,T−t)ηTTηT,max(1,T−t)ηT，结论明显更为粗糙。这个结果是新的，但它跟原论文附录F中的定理10本质上是等价的。

## 加速收敛 [\#](https://kexue.fm/archives/11494\#%E5%8A%A0%E9%80%9F%E6%94%B6%E6%95%9B)

这一节我们将会看到，在适当的设置下，式[(10)](https://kexue.fm/archives/11494#mjx-eqn-leq%3Alast-2)(10)能实现O(1/T−−√)O(1/T)的收敛速度。这里“适当的设置”主要是学习率调度策略，不同于以往的常数学习率或α/t√α/t、α/tα/t这类无终点学习率，这一次我们选择“线性衰减（Linear Decay）”：

ηt=α(1−tT+1)(11)(11)ηt=α(1−tT+1)

这个学习率函数值得单开一行来强调一下，因为它是学习率策略的最佳实践之一，比如 [《Straight to Zero: Why Linearly Decaying the Learning Rate to Zero Works Best for LLMs》](https://papers.cool/arxiv/2502.15938) 就声称它比Cosine Decay还优，这体现了我们的讨论已经越来越接近实用场景。

直接逐一计算得：

η1:T=∑τ=1Tα(1−τT+1)=αT2ηt+1:T=∑τ=t+1Tα(1−τT+1)=α(T−t)(T+1−t)2(T+1)η2tηt+1:T=2α(T+1−t)(T−t)(T+1)≤4αT+1∑t=1Tη2tηmin(t+1,T):T=ηT+∑t=1T−1η2tηt+1:T≤αT+1+∑t=1T−14αT+1≤4α(12)(13)(14)(15)(12)η1:T=∑τ=1Tα(1−τT+1)=αT2(13)ηt+1:T=∑τ=t+1Tα(1−τT+1)=α(T−t)(T+1−t)2(T+1)(14)ηt2ηt+1:T=2α(T+1−t)(T−t)(T+1)≤4αT+1(15)∑t=1Tηt2ηmin(t+1,T):T=ηT+∑t=1T−1ηt2ηt+1:T≤αT+1+∑t=1T−14αT+1≤4α

将这些结果代入到式[(10)](https://kexue.fm/archives/11494#mjx-eqn-leq%3Alast-2)(10)即得

E\[L(θT)−L(θ∗)\]≤∥θ1−φ∥2αT+2G2α(16)(16)E\[L(θT)−L(θ∗)\]≤‖θ1−φ‖2αT+2G2α

取α=∥θ1−θ∗∥G2T√α=‖θ1−θ∗‖G2T，就可以让右端取到最小值，实现O(1/T−−√)O(1/T)的终点收敛速度。要指出的是，如果不引入更强的假设，O(1/T−−√)O(1/T)已经没法改进，这是由信息论保证的（参考 [《Information-theoretic lower bounds on the oracle complexity of stochastic convex optimization》](https://papers.cool/arxiv/1009.0571)），所以这已经是理论最优的收敛速度。

## 变分之法 [\#](https://kexue.fm/archives/11494\#%E5%8F%98%E5%88%86%E4%B9%8B%E6%B3%95)

可能有读者疑问，怎么看出线性衰减学习率能取得最优的收敛速度呢？或者更一般地问，如何求η1,η2,⋯,ηT≥0η1,η2,⋯,ηT≥0，使得不等式[(10)](https://kexue.fm/archives/11494#mjx-eqn-leq%3Alast-2)(10)右端取最小值？这是一个多元函数最小化问题，标准解法是求偏导，但精确解很复杂，不好观察性态。

这里我们考虑将它连续化，然后用 [变分法](https://kexue.fm/search/%E5%8F%98%E5%88%86%E6%B3%95/) 给出一个快速的启发式推导（当然快速的前提是读者熟悉变分法）。记St=ηmin(t+1,T):TSt=ηmin(t+1,T):T，那么对于t<T−1t<T−1都有ηt=St−1−St≈−S˙tηt=St−1−St≈−S˙t，我们统一用−S˙t−S˙t近似ηtηt、用积分近似求和，那么原始问题可以近似为最小化如下的积分函数

cS0+∫T0(S˙t)2Stdt(17)(17)cS0+∫0T(S˙t)2Stdt

根据定义ST=0ST=0，考虑先把S0S0固定，那么这就是一个边界固定的变分问题，代入 [欧拉-拉格朗日方程](https://en.wikipedia.org/wiki/Euler%E2%80%93Lagrange_equation) 得

ddt2S˙tSt=−(S˙t)2S2t(18)(18)ddt2S˙tSt=−(S˙t)2St2

容易解得St=S0(1−t/T)2St=S0(1−t/T)2，代入到[(17)](https://kexue.fm/archives/11494#mjx-eqn-eq%3Afunc-min)(17)得c/S0+4S0/Tc/S0+4S0/T，最小值在S0=cT/4−−−−√S0=cT/4取到，于是最优解是St=cT/4−−−−√(1−t/T)2St=cT/4(1−t/T)2，求导得ηt=−S˙t=c/T−−−√(1−t/T)ηt=−S˙t=c/T(1−t/T)，这就得到了线性衰减，连同前面的常数∝1/T−−√∝1/T都得到了。

现在这个结果还是近似，我们要代回原始的离散化格式做严格证明，此时ηt∝1−t/Tηt∝1−t/T会遇到分母为零的麻烦，所以要稍微变动一下，最终得到式[(11)](https://kexue.fm/archives/11494#mjx-eqn-eq%3Aliner-decay)(11)。

## 延伸思考 [\#](https://kexue.fm/archives/11494\#%E5%BB%B6%E4%BC%B8%E6%80%9D%E8%80%83)

在上面的推导和结论中，有几个值得我们特别注意的关键点，某种意义上，这是随机优化收敛理论的里程碑式的进展。

首先，如果学习率设为常数，那么结论[(10)](https://kexue.fm/archives/11494#mjx-eqn-leq%3Alast-2)(10)跟上一篇文章是一致的，我们已经证明过，它至多能达到O(lnT/T−−−−−−√)O(ln⁡T/T)的收敛速度，并不是最优的，而式[(11)](https://kexue.fm/archives/11494#mjx-eqn-eq%3Aliner-decay)(11)的线性衰减学习率则可以做到O(1/T−−√)O(1/T)。这一方面显示了学习率衰减对于终点收敛的必要性，另一方面也从理论上支撑了线性衰减策略。

不难证明，在前三篇文章中，最佳的收敛速度都在常数学习率下取到，但这个常数是跟训练总步数TT相关的，比如α/T−−√α/T，很多工作认为这是一个不足之处，它们会更偏爱α/t√α/t、α/tα/t之类的学习率策略，因为不需要预知训练步数TT，属于“随时停止、随时续训、想训练多少步就训练多少步”的无终点学习率策略。

然而，这类策略的实践表现通常并不佳。本文的新结论显示，切换到终点损失后，出现了一些新特性，收敛最快的既不是与TT有关的常数学习率，也不是TT无关的动态学习率，而应该是“二者兼之”，比如线性衰减。此外，实践常用的还有Cosine型衰减，它们的共同关键词是：终点更优、与TT有关、动态变化。

换言之，不存在“一劳永逸”的学习率策略，根据训练步数精调学习率策略，才能获得最佳的终点收敛结果。这其实跟现在的Scaling Law实践很吻合，比如 [Step Law](https://papers.cool/arxiv/2503.04715) 发现最优学习率和最优Batch Size都应该根据数据量精细调整，注意到数据量和Batch Size都给定后训练步数TT也就确定了，所以它们也可以说是训练步数TT的函数。

后面的某篇文章中，我们也会简要讨论这系列结论与如今的Scaling Law之间的联系，敬请大家期待。

## 文章小结 [\#](https://kexue.fm/archives/11494\#%E6%96%87%E7%AB%A0%E5%B0%8F%E7%BB%93)

在这篇文章中，我们推广了上篇的核心恒等式，然后得到了理论最佳的终点损失收敛速度。有意思的是，取得这一成绩的学习率策略并不是常数学习率，也不是传统的逆步数、逆步数平方根学习率，而是更贴近我们日常实践的线性衰减。接下来，我们还会继续探讨该结论背后的深刻意义。

_**转载到请包括本文地址：** [https://kexue.fm/archives/11494](https://kexue.fm/archives/11494 "让炼丹更科学一些（四）：新恒等式，新学习率")_

_**更详细的转载事宜请参考：**_ [《科学空间FAQ》](https://kexue.fm/archives/6508#%E6%96%87%E7%AB%A0%E5%A6%82%E4%BD%95%E8%BD%AC%E8%BD%BD/%E5%BC%95%E7%94%A8 "《科学空间FAQ》")

**如果您还有什么疑惑或建议，欢迎在下方评论区继续讨论。**

**如果您觉得本文还不错，欢迎 [分享](https://kexue.fm/archives/11494#share)/ [打赏](https://kexue.fm/archives/11494#pay) 本文。打赏并非要从中获得收益，而是希望知道科学空间获得了多少读者的真心关注。当然，如果你无视它，也不会影响你的阅读。再次表示欢迎和感谢！**

打赏

![科学空间](https://kexue.fm/usr/themes/geekg/payment/wx.png)

微信打赏

![科学空间](https://kexue.fm/usr/themes/geekg/payment/zfb.png)

支付宝打赏

因为网站后台对打赏并无记录，因此欢迎在打赏时候备注留言。

你还可以 [**点击这里**](http://mail.qq.com/cgi-bin/qm_share?t=qm_mailme&email=tN7d1drY3drrx8H0xcWa19vZ) 或在下方评论区留言来告知你的建议或需求。

**如果您需要引用本文，请参考：**

苏剑林. (Dec. 26, 2025). 《让炼丹更科学一些（四）：新恒等式，新学习率 》\[Blog post\]. Retrieved from [https://kexue.fm/archives/11494](https://kexue.fm/archives/11494)

@online{kexuefm-11494,

         title={让炼丹更科学一些（四）：新恒等式，新学习率},

         author={苏剑林},

         year={2025},

         month={Dec},

         url={\\url{https://kexue.fm/archives/11494}},

}


分类： [数学研究](https://kexue.fm/category/Mathematics)    标签： [学习率](https://kexue.fm/tag/%E5%AD%A6%E4%B9%A0%E7%8E%87/), [优化器](https://kexue.fm/tag/%E4%BC%98%E5%8C%96%E5%99%A8/), [sgd](https://kexue.fm/tag/sgd/), [炼丹](https://kexue.fm/tag/%E7%82%BC%E4%B8%B9/)[3 评论](https://kexue.fm/archives/11494#comments)

< [为什么DeltaNet要加L2 Normalize？](https://kexue.fm/archives/11486 "为什么DeltaNet要加L2 Normalize？") \| [让炼丹更科学一些（五）：基于梯度精调学习率](https://kexue.fm/archives/11530 "让炼丹更科学一些（五）：基于梯度精调学习率") >

### 你也许还对下面的内容感兴趣

- [让炼丹更科学一些（五）：基于梯度精调学习率](https://kexue.fm/archives/11530 "让炼丹更科学一些（五）：基于梯度精调学习率")
- [让炼丹更科学一些（三）：SGD的终点损失收敛](https://kexue.fm/archives/11480 "让炼丹更科学一些（三）：SGD的终点损失收敛")
- [让炼丹更科学一些（二）：将结论推广到无界域](https://kexue.fm/archives/11469 "让炼丹更科学一些（二）：将结论推广到无界域")
- [滑动平均视角下的权重衰减和学习率](https://kexue.fm/archives/11459 "滑动平均视角下的权重衰减和学习率")
- [Muon优化器指南：快速上手与关键细节](https://kexue.fm/archives/11416 "Muon优化器指南：快速上手与关键细节")
- [AdamW的Weight RMS的渐近估计（下）](https://kexue.fm/archives/11404 "AdamW的Weight RMS的渐近估计（下）")
- [流形上的最速下降：5\. 对偶梯度下降](https://kexue.fm/archives/11388 "流形上的最速下降：5. 对偶梯度下降")
- [MuP之上：1. 好模型的三个特征](https://kexue.fm/archives/11340 "MuP之上：1. 好模型的三个特征")
- [AdamW的Weight RMS的渐近估计（上）](https://kexue.fm/archives/11307 "AdamW的Weight RMS的渐近估计（上）")
- [重新思考学习率与Batch Size（四）：EMA](https://kexue.fm/archives/11301 "重新思考学习率与Batch Size（四）：EMA")

[发表你的看法](https://kexue.fm/archives/11494#comment_form)

TommyJiang

January 4th, 2026

苏神怎么看 DeepSeek 新出的 mHC，是不是和 Muon 的思想有点类似，都是从向量升级成矩阵？

[回复评论](https://kexue.fm/archives/11494/comment-page-1?replyTo=29067#respond-post-11494)

[苏剑林](https://kexue.fm/) 发表于
January 6th, 2026

这样说也对，确实将state从向量变成矩阵了。

[回复评论](https://kexue.fm/archives/11494/comment-page-1?replyTo=29087#respond-post-11494)

我

January 8th, 2026

似乎并非mHC提出矩阵的思想？之前hyper connection就是了

[回复评论](https://kexue.fm/archives/11494/comment-page-1?replyTo=29099#respond-post-11494)

[取消回复](https://kexue.fm/archives/11494#respond-post-11494)

你的大名

电子邮箱

个人网站（选填）

1\. 可以使用LaTeX代码，点击“预览效果”可查看效果；

2\. 可以通过点击评论楼层编号来引用该楼层；

3\. 网站可能会有点卡，如非确认评论失败，请 **不要重复点击提交**。

### 内容速览

[新恒等式](https://kexue.fm/archives/11494#%E6%96%B0%E6%81%92%E7%AD%89%E5%BC%8F)
[一般结论](https://kexue.fm/archives/11494#%E4%B8%80%E8%88%AC%E7%BB%93%E8%AE%BA)
[加速收敛](https://kexue.fm/archives/11494#%E5%8A%A0%E9%80%9F%E6%94%B6%E6%95%9B)
[变分之法](https://kexue.fm/archives/11494#%E5%8F%98%E5%88%86%E4%B9%8B%E6%B3%95)
[延伸思考](https://kexue.fm/archives/11494#%E5%BB%B6%E4%BC%B8%E6%80%9D%E8%80%83)
[文章小结](https://kexue.fm/archives/11494#%E6%96%87%E7%AB%A0%E5%B0%8F%E7%BB%93)

### 智能搜索

支持整句搜索！网站自动使用 [结巴分词](https://github.com/fxsjy/jieba) 进行分词，并结合ngrams排序算法给出合理的搜索结果。

### 热门标签

[生成模型](https://kexue.fm/tag/%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B/) [attention](https://kexue.fm/tag/attention/) [优化](https://kexue.fm/tag/%E4%BC%98%E5%8C%96/) [语言模型](https://kexue.fm/tag/%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/) [模型](https://kexue.fm/tag/%E6%A8%A1%E5%9E%8B/) [梯度](https://kexue.fm/tag/%E6%A2%AF%E5%BA%A6/) [网站](https://kexue.fm/tag/%E7%BD%91%E7%AB%99/) [概率](https://kexue.fm/tag/%E6%A6%82%E7%8E%87/) [矩阵](https://kexue.fm/tag/%E7%9F%A9%E9%98%B5/) [优化器](https://kexue.fm/tag/%E4%BC%98%E5%8C%96%E5%99%A8/) [转载](https://kexue.fm/tag/%E8%BD%AC%E8%BD%BD/) [微分方程](https://kexue.fm/tag/%E5%BE%AE%E5%88%86%E6%96%B9%E7%A8%8B/) [分析](https://kexue.fm/tag/%E5%88%86%E6%9E%90/) [天象](https://kexue.fm/tag/%E5%A4%A9%E8%B1%A1/) [深度学习](https://kexue.fm/tag/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/) [积分](https://kexue.fm/tag/%E7%A7%AF%E5%88%86/) [python](https://kexue.fm/tag/python/) [扩散](https://kexue.fm/tag/%E6%89%A9%E6%95%A3/) [力学](https://kexue.fm/tag/%E5%8A%9B%E5%AD%A6/) [无监督](https://kexue.fm/tag/%E6%97%A0%E7%9B%91%E7%9D%A3/) [几何](https://kexue.fm/tag/%E5%87%A0%E4%BD%95/) [节日](https://kexue.fm/tag/%E8%8A%82%E6%97%A5/) [生活](https://kexue.fm/tag/%E7%94%9F%E6%B4%BB/) [文本生成](https://kexue.fm/tag/%E6%96%87%E6%9C%AC%E7%94%9F%E6%88%90/) [数论](https://kexue.fm/tag/%E6%95%B0%E8%AE%BA/)

### 随机文章

- [【备忘】Python中断多重循环的几种思路](https://kexue.fm/archives/4159)
- [“二体+恒力”问题](https://kexue.fm/archives/1358)
- [多任务学习漫谈（一）：以损失之名](https://kexue.fm/archives/8870)
- [为什么DeltaNet要加L2 Normalize？](https://kexue.fm/archives/11486)
- [通过梯度近似寻找Normalization的替代品](https://kexue.fm/archives/10831)
- [椭圆的周长与面积](https://kexue.fm/archives/47)
- [矩阵平方根和逆平方根的高效计算](https://kexue.fm/archives/11158)
- [三百年之谜——费马大定理(历史+证明)](https://kexue.fm/archives/38)
- [玩转Keras之seq2seq自动生成标题](https://kexue.fm/archives/5861)
- [警察捉贼,追牛问题,导弹跟踪](https://kexue.fm/archives/1047)

### 最近评论

- [rynn](https://kexue.fm/archives/11328/comment-page-1#comment-29106): 现在推荐中不止有用vq做量化生成id进行自回归生成，也有用vq、rq量化物品的多模态嵌入，当做...
- [Bin](https://kexue.fm/archives/1990/comment-page-2#comment-29105): 今天偶然从某个论坛看到有人推荐您的博客，定睛一看竟然是华师同院的往届师兄！看到这篇2013年的...
- [Rapture D](https://kexue.fm/archives/11530/comment-page-1#comment-29104): 我有一个问题，为什么不考虑亥姆霍兹定理和斯托克斯公式。
- [mofheka](https://kexue.fm/archives/11390/comment-page-1#comment-29103): 苏神是还在用jax是么？最近在做基于Google Pathway的理念做一个动态版的MPMD框...
- [长琴](https://kexue.fm/archives/11530/comment-page-1#comment-29102): 看懂这篇博客也不是一件容易的事情。
- [AlexLi](https://kexue.fm/archives/9257/comment-page-4#comment-29101): 苏老师，请教一下(7)式中将 μ(xt)μ(xt) 传给 popo 进行推理的操作。 $x\_...
- [tyler\_zxc](https://kexue.fm/archives/7921/comment-page-2#comment-29100): "Performer的思想是将标准的Attention线性化，所以为什么不干脆直接训练一个线性...
- [我](https://kexue.fm/archives/11494/comment-page-1#comment-29099): 似乎并非mHC提出矩阵的思想？之前hyper connection就是了
- [winter](https://kexue.fm/archives/10847/comment-page-1#comment-29098): 苏神您好，假如对于比较均匀的attention weightP，往往呈现long tail分布...
- [苏剑林](https://kexue.fm/archives/8512/comment-page-2#comment-29097): KL散度、JS散度、W距离啥的，都行啊，看你喜欢哪个

### 友情链接

- [Cool Papers](https://papers.cool/)
- [数学研发](https://bbs.emath.ac.cn/)
- [Seatop](http://www.seatop.com.cn/)
- [Xiaoxia](https://xiaoxia.org/)
- [积分表-网络版](https://kexue.fm/sci/integral/index.html)
- [丝路博傲](http://blog.dvxj.com/)
- [数学之家](http://www.2math.cn/)
- [有趣天文奇观](http://interesting-sky.china-vo.org/)
- [TwistedW](http://www.twistedwg.com/)
- [godweiyang](https://godweiyang.com/)
- [AI柠檬](https://blog.ailemon.net/)
- [王登科-DK博客](https://greatdk.com/)
- [ESON](https://blog.eson.org/)
- [枫之羽](https://fzhiy.net/)
- [coding-zuo](https://coding-zuo.github.io/)
- [博科园](https://www.bokeyuan.net/)
- [孔皮皮的博客](https://www.kppkkp.top/)
- [运鹏的博客](https://yunpengtai.top/)
- [jiming.site](https://jiming.site/)
- [OmegaXYZ](https://www.omegaxyz.com/)
- [EAI猩球](https://www.robotech.ink/)
- [文举的博客](https://liwenju0.com/)
- [申请链接](https://kexue.fm/links.html)

[![署名-非商业用途-保持一致](https://kexue.fm/usr/themes/geekg/images/cc.gif)](http://creativecommons.org/licenses/by-nc-nd/2.5/cn/) 本站采用创作共用版权协议，要求署名、非商业用途和保持一致。转载本站内容必须也遵循“ [署名-非商业用途-保持一致](http://creativecommons.org/licenses/by-nc-nd/2.5/cn/)”的创作共用协议。



© 2009-2026 Scientific Spaces. All rights reserved. Theme by [laogui](http://www.laogui.com/). Powered by [Typecho](http://typecho.org/). 备案号: [粤ICP备09093259号-1/2](https://beian.miit.gov.cn/ "粤ICP备09093259号")。