## SEARCH

## MENU

- [打赏](https://kexue.fm/reward.html)
- [公式](https://kexue.fm/latex.html)
- [天象](https://kexue.fm/ac.html)
- [链接](https://kexue.fm/links.html)
- [时光](https://kexue.fm/me.html)
- [博览](https://kexue.fm/science.html)
- [归档](https://kexue.fm/content.html)

## CATEGORIES

- [千奇百怪](https://kexue.fm/category/Everything)
- [天文探索](https://kexue.fm/category/Astronomy)
- [数学研究](https://kexue.fm/category/Mathematics)
- [物理化学](https://kexue.fm/category/Phy-chem)
- [信息时代](https://kexue.fm/category/Big-Data)
- [生物自然](https://kexue.fm/category/Biology)
- [图片摄影](https://kexue.fm/category/Photograph)
- [问题百科](https://kexue.fm/category/Questions)
- [生活/情感](https://kexue.fm/category/Life-Feeling)
- [资源共享](https://kexue.fm/category/Resources)

## NEWPOSTS

- [通过msign来计算mclip（奇...](https://kexue.fm/archives/11006)
- [msign算子的Newton-Sc...](https://kexue.fm/archives/10996)
- [从无穷范数求导到等值振荡定理](https://kexue.fm/archives/10972)
- [生成扩散模型漫谈（三十）：从瞬时速...](https://kexue.fm/archives/10958)
- [MoE环游记：5、均匀分布的反思](https://kexue.fm/archives/10945)
- [msign算子的Newton-Sc...](https://kexue.fm/archives/10922)
- [Transformer升级之路：2...](https://kexue.fm/archives/10907)
- [一道概率不等式：盯着它到显然成立为止！](https://kexue.fm/archives/10902)
- [SVD的导数](https://kexue.fm/archives/10878)
- [智能家居之手搓一套能接入米家的零冷水装置](https://kexue.fm/archives/10869)

## COMMENTS

- [PengchengMa: 牛啊](https://kexue.fm/archives/10996/comment-page-1#comment-27811)
- [xczh: 已使用mean flow policy，一步推理效果确实惊人，...](https://kexue.fm/archives/10958/comment-page-1#comment-27810)
- [Cosine: 是不是因为shared experts每次都激活，而route...](https://kexue.fm/archives/10945/comment-page-1#comment-27809)
- [rpsun: 这样似乎与传统的经验正交函数之类的有相似之处。把样本的平均值减...](https://kexue.fm/archives/10699/comment-page-1#comment-27808)
- [贵阳机场接机: 怎么不更新啦](https://kexue.fm/archives/1490/comment-page-1#comment-27807)
- [czvzb: 具身智能模型目前主流也是在使用扩散和流匹配这类方法来预测动作。...](https://kexue.fm/archives/10958/comment-page-1#comment-27806)
- [Shawn\_yang: 苏神，关于您所说的：“推理阶段可以事先预估Routed Exp...](https://kexue.fm/archives/10945/comment-page-1#comment-27802)
- [OceanYU: 您好，关于由式（7）推导出高斯分布，我这里有一点问题，式（7）...](https://kexue.fm/archives/9164/comment-page-4#comment-27801)
- [jorjiang: 训练和prefill这个compute-bound阶段不做矩阵...](https://kexue.fm/archives/10907/comment-page-2#comment-27800)
- [amy: 苏老师，您有关注傅里叶旋转位置编码这篇工作吗，想知道您对这篇工...](https://kexue.fm/archives/10907/comment-page-2#comment-27799)

## USERLOGIN

- [登录](https://kexue.fm/admin/login.php)

[科学空间\|Scientific Spaces](https://kexue.fm)

- [登录](https://kexue.fm/admin/login.php)
- [打赏](https://kexue.fm/reward.html)
- [公式](https://kexue.fm/latex.html)
- [天象](https://kexue.fm/ac.html)
- [链接](https://kexue.fm/links.html)
- [时光](https://kexue.fm/me.html)
- [博览](https://kexue.fm/science.html)
- [归档](https://kexue.fm/content.html)

渴望成为一个小飞侠

- [欢迎订阅](https://kexue.fm/feed)
- [个性邮箱](https://kexue.fm/archives/119)
- [天象信息](https://kexue.fm/ac.html)
- [观测ISS](https://kexue.fm/archives/41)
- [LaTeX](https://kexue.fm/latex.html)
- [关于博主](https://kexue.fm/me.html)

欢迎访问“科学空间”，这里将与您共同探讨自然科学，回味人生百态；也期待大家的分享～

- [**千奇百怪** Everything](https://kexue.fm/category/Everything)
- [**天文探索** Astronomy](https://kexue.fm/category/Astronomy)
- [**数学研究** Mathematics](https://kexue.fm/category/Mathematics)
- [**物理化学** Phy-chem](https://kexue.fm/category/Phy-chem)
- [**信息时代** Big-Data](https://kexue.fm/category/Big-Data)
- [**生物自然** Biology](https://kexue.fm/category/Biology)
- [**图片摄影** Photograph](https://kexue.fm/category/Photograph)
- [**问题百科** Questions](https://kexue.fm/category/Questions)
- [**生活/情感** Life-Feeling](https://kexue.fm/category/Life-Feeling)
- [**资源共享** Resources](https://kexue.fm/category/Resources)

- [**千奇百怪**](https://kexue.fm/category/Everything)
- [**天文探索**](https://kexue.fm/category/Astronomy)
- [**数学研究**](https://kexue.fm/category/Mathematics)
- [**物理化学**](https://kexue.fm/category/Phy-chem)
- [**信息时代**](https://kexue.fm/category/Big-Data)
- [**生物自然**](https://kexue.fm/category/Biology)
- [**图片摄影**](https://kexue.fm/category/Photograph)
- [**问题百科**](https://kexue.fm/category/Questions)
- [**生活/情感**](https://kexue.fm/category/Life-Feeling)
- [**资源共享**](https://kexue.fm/category/Resources)

[首页](https://kexue.fm) [信息时代](https://kexue.fm/category/Big-Data) 抛开约束，增强模型：一行代码提升albert表现

29Jan

# [抛开约束，增强模型：一行代码提升albert表现](https://kexue.fm/archives/7187)

By 苏剑林 \|
2020-01-29 \|
92708位读者\|

本文标题看起来有点“标题党”了，不过所作改动放到bert4keras框架下，确实是一行代码的变动，至于是否有提升，这个笔者不敢打包票，不过测了几个算是比较有代表性的任务，均显示持平甚至有提升，所以标题说的也基本是事实。

那究竟是什么改动呢？其实一句话也能讲清楚：

> 在下游任务中，放弃albert的权重共享的约束，也就是把albert当bert用。

具体思路细节，请接着看下去～

## albert是什么 [\#](https://kexue.fm/archives/7187\#albert%E6%98%AF%E4%BB%80%E4%B9%88)

这个改动是专门给albert设计的，所以要理解这个改动，需要先知道albert是什么。在此还是花点篇幅对albert进行个简单的科普。这里假设读者已经对bert有了一定的了解，所以主要是比较albert跟bert的异同。

### 低秩分解 [\#](https://kexue.fm/archives/7187\#%E4%BD%8E%E7%A7%A9%E5%88%86%E8%A7%A3)

首先是Embedding层，以中文版bert base为例，token总数大约是20000，而Embedding层维度是768，所以Embedding层的总参数量大约是1500万，大概占了总参数量的1/6。albert第一个开刀的部分就是Embedding层，它把Embedding层弄成128维了，然后再通过$128\\times 768$的矩阵变换矩阵变回768维，这样Embedding层参数量就只有原来的1/6了，这就是所谓的低秩分解。

### 参数共享 [\#](https://kexue.fm/archives/7187\#%E5%8F%82%E6%95%B0%E5%85%B1%E4%BA%AB)

其次是transformer部分。在以bert为代表的transformer架构模型中，其核心是由self attention、layer norm、全连接（核大小为1的一维卷积）等组成的模块，这里称之为“transformer block”，而bert模型就是多个transformer block的堆叠。如下面左图实际上是bert base的示意图，它堆叠了12个transformer block：

bert base极简示意图

albert base极简示意图

注意到，在bert的设计里边，每个transformer block的输入和输出形状是一样的，这意味着当前block的输出作为当前block的输入也是合理的。这也告诉我们，同一个block其实是可以重复使用来迭代的，而不至于每一层都使用一个新的block。albert里边使用了一种最简单直接的方案：所有层都公用同一个transformer block（如上面右图所示）！这样一来，在albert base中，transformer block这部分参数量直接降低为原来bert base的1/12了。

### 简单评述 [\#](https://kexue.fm/archives/7187\#%E7%AE%80%E5%8D%95%E8%AF%84%E8%BF%B0)

除了以上两点之外，albert与bert的一个显著不同之处是在预训练阶段将NSP（Next Sentence Prediction）任务改为了SOP（Sentence-Order Prediction）任务，但是这并不属于模型架构上的，所以并不是本文要关心的，读者自行找相关资料了解即可。

总的来说，albert是一个为了降低参数量而设计的模型，并且希望这个参数量的降低能带来一定的正则化作用，从而降低过拟合风险，提升最终表现。但最后的结果是否如作者所愿呢？从“战绩”来看，albert问世之时用它最大版本的模型刷新了glue榜单，所以应该算是达到了作者的期望。但是，albert并非总是那么理想，albert也不是我们想象中的小模型。

对于一个模型来说，我们比较关心的是速度和效果两个指标。从上面两个bert和albert的图其实就可以看到，在预测阶段（前向传播），其实albert跟bert没啥差别，所以同一规格（比如大家都是base版本）下的albert和bert的预测速度是一样的，甚至更严格地说，albert还更慢些，因为albert的Embedding部分还多了一个矩阵运算。换句话说，albert不能带来预测速度的提升！

那同一规格的albert和bert，哪个效果好呢？其实albert论文已经给出了答案：在large以内的版本，albert效果比bert更差，只有在xlarge、xxlarge的版本时albert效果才开始优于bert。但是roberta方式的预训练部分弥补了bert的缺点，所以真正可以说稳定优于bert/roberta的albert版本就只有xxlarge。然而albert xxlarge是一个很庞大的模型，以至于我们很难把它跑起来。

所以，基本上可以说：（在大多数人能搞起的前提下） **在同样的预测速度下，albert效果更差；在同样的效果下，albert更慢。**

那训练阶段呢？前面还没有提到的一点是，albert的参数共享设计其实有很强的正则化作用，所以albert去掉的dropout。参数共享和去掉dropout这两点确实可以省一些显存，并且提高训练速度，但是笔者的评测是幅度只有10%～20%左右。也就是说，就算albert参数降低到bert的1/10甚至更多，并不意味着它的显存占用量能降低到1/10，也不意味着训练速度能提高10倍，相反，它只有小幅度的提升。

## 抛开共享约束 [\#](https://kexue.fm/archives/7187\#%E6%8A%9B%E5%BC%80%E5%85%B1%E4%BA%AB%E7%BA%A6%E6%9D%9F)

从前面的讨论中，我们能理解到几个事实：

> 1、只看预测的话，albert跟bert基本一致；
>
> 2、albert的参数共享对效果的作用基本是负面的。

既然这样，那么我们可以尝试一个新鲜的玩法：在针对下游任务进行finetune时，我们把参数共享这个约束去掉如何？也就是说，finetune的时候把albert当bert用，相当于每一个transformer block的初始化权重都一样的bert。

### 效果评测 [\#](https://kexue.fm/archives/7187\#%E6%95%88%E6%9E%9C%E8%AF%84%E6%B5%8B)

事不宜迟，测了效果再说话。这里挑了四个任务来测。为了保证可复现性，下面同一个实验都跑了三次，表格里显示的是三次结果的平均值。其中unshared版本就是指去掉参数共享后的模型。而训练速度那一列，指的是每个epoch所用的训练时间，这是在单卡TITAN RTX上跑的时间，仅供相对比较参考。

实验用 [bert4keras](https://github.com/bojone/bert4keras) 进行，对unshared版本，只需要在 `build_transformer_model` 时加载albert权重，并且设置 `model='albert_unshared'`，这就是标题所说的“一行代码”。

首先是比较简单的 [文本情感分类](https://github.com/bojone/bert4keras/blob/master/examples/task_sentiment_albert.py) 任务。
$$\\begin{array}{c\|c\|c\|c\|c}
\\hline
\\text{模型} & \\text{验证集(valid)} & \\text{训练速度} & \\text{第一个epoch结束时的指标值} & \\text{测试集(test)} \\\
\\hline
\\text{small\_unshared} & 94.66\\% & 38s & 90.75\\% & 94.35\\% \\\
\\text{small} & 94.57\\% & 33s & 91.02\\% & 94.52\\% \\\
\\hline
\\text{tiny\_unshared} & 94.02\\% & 23s & 88.09\\% & 94.13\\% \\\
\\text{tiny} & 94.14\\% & 20s & 90.18\\% & 93.78\\% \\\
\\hline
\\end{array}$$
去掉参数共享后，训练时间略有增加，这是预料之中的，至于模型表现各有优劣。考虑到这个任务准确率本身比较高了，可能显示不出模型间的差距，所以下面继续测试复杂一点的模型。

这次我们试试CLUE的 [IFLYTEK'长文本分类](https://github.com/CLUEbenchmark/CLUE)，结果如下：
$$\\begin{array}{c\|c\|c\|c}
\\hline
\\text{模型} & \\text{验证集(dev)} & \\text{训练速度} & \\text{第一个epoch结束时的指标值} \\\
\\hline
\\text{small\_unshared} & 57.73\\% & 27s & 49.35\\% \\\
\\text{small} & 57.14\\% & 24s & 48.21\\% \\\
\\hline
\\text{tiny\_unshared} & 55.91\\% & 16s & 47.89\\% \\\
\\text{tiny} & 56.42\\% & 14s & 43.84\\% \\\
\\hline
\\end{array}$$
这时候unshared版本的优势开始显示出来了，主要体现在整体上收敛更快（看第一个epoch的指标值），small版本最优效果明显更优，tiny版本最优效果略差，但通过精细调整学习率后，tiny\_unshared版本的最优效果其实是可以优于tiny版的（但这样一来变量太多了，表格显示的是严格的控制变量的结果）。

然后试试比较综合的任务： [信息抽取](https://github.com/bojone/bert4keras/blob/master/examples/task_relation_extraction.py)。结果如下：
$$\\begin{array}{c\|c\|c\|c}
\\hline
\\text{模型} & \\text{验证集(dev)} & \\text{训练速度} & \\text{第一个epoch结束时的指标值} \\\
\\hline
\\text{small\_unshared} & 77.89\\% & 375s & 61.11\\% \\\
\\text{small} & 77.69\\% & 335s & 46.58\\% \\\
\\hline
\\text{tiny\_unshared} & 76.44\\% & 235s & 49.74\\% \\\
\\text{tiny} & 75.94\\% & 215s & 31.66\\% \\\
\\hline
\\end{array}$$
可以看到，在比较综合性的复杂任务，unshared版本的模型已经稳定超过同规模的原模型。

最后一个是用 [seq2seq做阅读理解式问答](https://github.com/bojone/bert4keras/blob/master/examples/task_reading_comprehension_by_seq2seq.py)，结果如下：
$$\\begin{array}{c\|c\|c\|c}
\\hline
\\text{模型} & \\text{验证集(dev)} & \\text{训练速度} & \\text{第一个epoch结束时的指标值} \\\
\\hline
\\text{small\_unshared} & 68.80\\% & 607s & 57.02\\% \\\
\\text{small} & 66.66\\% & 582s & 50.93\\% \\\
\\hline
\\text{tiny\_unshared} & 66.15\\% & 455s & 48.64\\% \\\
\\text{tiny} & 63.41\\% & 443s & 37.47\\% \\\
\\hline
\\end{array}$$
这个任务主要目的是测试模型的文本生成能力。可以看到在此任务上，unshared版本的模型已经明显超过原版模型，甚至tiny版的unshared模型已经逼近原版small模型。

### 分析思考 [\#](https://kexue.fm/archives/7187\#%E5%88%86%E6%9E%90%E6%80%9D%E8%80%83)

上面的模型都是albert tiny/small的实验，其实base版也实验过，结论跟tiny和small版的基本一致，但是base版本（自然也包括large以及xlarge版本）实验时间过长，所以没有做完完整的实验（也没有重复三次），因此就不贴上了。但总的来说，可以感觉到tiny/small版本的结果基本上有代表性了。

上述实验标明，去掉参数共享后的albert，在下游任务中的表现基本能持平甚至超过原版albert，这显示了对于很多NLP任务来说，参数共享可能并不是一个很好的约束。读者可能纠结于“为什么到了xlarge甚至xxlarge规模的模型时，参数共享的albert又开始超过了不参数共享的bert了呢？”。这里笔者尝试给出一个解释。

从理论上来说，bert防止过拟合的手段有dropout和权重衰减，其中权重衰减在albert也用了，但dropout没有出现在albert中，所以可以往dropout角度思考。很多实验都表明，dropout确实是一种降低过拟合风险的有效策略，但已有的实验模型基本都远远比不上bert xlarge、bert xxlarge那么大，所以dropout在超大模型下的有效性依然值得商榷。事实上，dropout存在训练和推断的不一致问题，也就是“ **严格来讲训练模型和预测模型并不是同一个模型**”，个人感觉模型变大变深时，这种不一致性会进一步放大，所以个人认为dropout对于超大模型并不是一种有效的防止过拟合的方法。那bert里边直接去掉dropout呢？也不大行，因为去掉dropout就没有什么手段来抑制bert的过拟合了，参数量那么大，过拟合肯定严重。而albert去掉了dropout，通过参数共享来引入隐式的正则，使得模型变大变深不至于退化，甚至会表现更好。

反过来说，albert的参数共享性能要好，条件是要足够大、足够深，所以如果我们用的是base版本、small版本甚至是tiny版本时，反而不应该用参数共享，因为对于小模型来说参数共享反而是对模型表达能力的不必要的限制，所以这时候去掉参数共享表现反而更好些。

## 文章的小结 [\#](https://kexue.fm/archives/7187\#%E6%96%87%E7%AB%A0%E7%9A%84%E5%B0%8F%E7%BB%93)

本文实验了一个新鲜的玩法：finetune阶段把albert的参数共享去掉，把albert当bert用，在几个任务上发现这样做有着持平甚至超过原始albert的表现，最后给出了对albert以及此现象的个人理解。

_**转载到请包括本文地址：** [https://kexue.fm/archives/7187](https://kexue.fm/archives/7187)_

_**更详细的转载事宜请参考：**_ [《科学空间FAQ》](https://kexue.fm/archives/6508#%E6%96%87%E7%AB%A0%E5%A6%82%E4%BD%95%E8%BD%AC%E8%BD%BD/%E5%BC%95%E7%94%A8)

**如果您还有什么疑惑或建议，欢迎在下方评论区继续讨论。**

**如果您觉得本文还不错，欢迎 [分享](https://kexue.fm/archives/7187#share)/ [打赏](https://kexue.fm/archives/7187#pay) 本文。打赏并非要从中获得收益，而是希望知道科学空间获得了多少读者的真心关注。当然，如果你无视它，也不会影响你的阅读。再次表示欢迎和感谢！**

打赏

微信打赏

支付宝打赏

因为网站后台对打赏并无记录，因此欢迎在打赏时候备注留言。你还可以 [**点击这里**](http://mail.qq.com/cgi-bin/qm_share?t=qm_mailme&email=tN7d1drY3drrx8H0xcWa19vZ) 或在下方评论区留言来告知你的建议或需求。

**如果您需要引用本文，请参考：**

苏剑林. (Jan. 29, 2020). 《抛开约束，增强模型：一行代码提升albert表现 》\[Blog post\]. Retrieved from [https://kexue.fm/archives/7187](https://kexue.fm/archives/7187)

@online{kexuefm-7187,
        title={抛开约束，增强模型：一行代码提升albert表现},
        author={苏剑林},
        year={2020},
        month={Jan},
        url={\\url{https://kexue.fm/archives/7187}},
}

分类： [信息时代](https://kexue.fm/category/Big-Data)    标签： [语言模型](https://kexue.fm/tag/%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/), [attention](https://kexue.fm/tag/attention/)[23 评论](https://kexue.fm/archives/7187#comments)

< [从几何视角来理解模型参数的初始化策略](https://kexue.fm/archives/7180) \| [你的CRF层的学习率可能不够大](https://kexue.fm/archives/7196) >

### 你也许还对下面的内容感兴趣

- [Transformer升级之路：20、MLA究竟好在哪里？](https://kexue.fm/archives/10907)
- [Transformer升级之路：19、第二类旋转位置编码](https://kexue.fm/archives/10862)
- [细水长flow之TARFLOW：流模型满血归来？](https://kexue.fm/archives/10667)
- [“闭门造车”之多模态思路浅谈（三）：位置编码](https://kexue.fm/archives/10352)
- [Decoder-only的LLM为什么需要位置编码？](https://kexue.fm/archives/10347)
- [Monarch矩阵：计算高效的稀疏型矩阵分解](https://kexue.fm/archives/10249)
- [Transformer升级之路：18、RoPE的底数选择原则](https://kexue.fm/archives/10122)
- [缓存与效果的极限拉扯：从MHA、MQA、GQA到MLA](https://kexue.fm/archives/10091)
- [Transformer升级之路：17、多模态位置编码的简单思考](https://kexue.fm/archives/10040)
- [时空之章：将Attention视为平方复杂度的RNN](https://kexue.fm/archives/10017)

[发表你的看法](https://kexue.fm/archives/7187#comment_form)

Tensor10.0

January 30th, 2020

博主您好，之前看到您关于CapsNet网络的分享，请问您Github的网址是多少呢？

[回复评论](https://kexue.fm/archives/7187/comment-page-1?replyTo=12786#respond-post-7187)

[苏剑林](https://kexue.fm) 发表于
February 1st, 2020

https://github.com/bojone

[回复评论](https://kexue.fm/archives/7187/comment-page-1?replyTo=12787#respond-post-7187)

Tensor10.0 发表于
February 2nd, 2020

感谢！

[回复评论](https://kexue.fm/archives/7187/comment-page-1?replyTo=12788#respond-post-7187)

TYW

February 3rd, 2020

您好，我在代码中似乎并没有找到相关unshared的代码呀？

[回复评论](https://kexue.fm/archives/7187/comment-page-1?replyTo=12793#respond-post-7187)

[苏剑林](https://kexue.fm) 发表于
February 3rd, 2020

只需要在build\_bert\_model时加载albert权重，并且设置model='albert\_unshared'

[回复评论](https://kexue.fm/archives/7187/comment-page-1?replyTo=12794#respond-post-7187)

TYW 发表于
February 3rd, 2020

我按你的方法设置了，但是加载albert权重时报错：ValueError: Dimension 1 in both shapes must be equal, but are 768 and 128. Shapes are \[768,768\] and \[768,128\]. for 'Assign\_23' (op: 'Assign') with input shapes: \[768,768\], \[768,128\].

[回复评论](https://kexue.fm/archives/7187/comment-page-1?replyTo=12795#respond-post-7187)

[苏剑林](https://kexue.fm) 发表于
February 4th, 2020

你可以先尝试一下把
https://github.com/bojone/bert4keras/blob/master/examples/task\_sentiment\_albert.py
里边的model='albert'改为model='albert\_unshared'，看看有没有报错（注意bert4keras要最新版）。如果有，请贴错误信息；如果没有，那就请认真仔细地阅读README.md，搞清楚自己使用错误在哪。

[回复评论](https://kexue.fm/archives/7187/comment-page-1?replyTo=12796#respond-post-7187)

york

February 12th, 2020

bert防止过拟合的手段有dropout和权重衰减，这里的权重衰减指的是学习率吗，还是其它的？

[回复评论](https://kexue.fm/archives/7187/comment-page-1?replyTo=12838#respond-post-7187)

[苏剑林](https://kexue.fm) 发表于
February 12th, 2020

权重衰减指的是用了AdamW优化器，里边有个weight\_decay\_rate=0.01，理论上这有助于防止过拟合。

[回复评论](https://kexue.fm/archives/7187/comment-page-1?replyTo=12839#respond-post-7187)

Warren

February 23rd, 2020

请问下当我用了unshared之后，计算资源的消耗猛增，这是正常的吗？

[回复评论](https://kexue.fm/archives/7187/comment-page-1?replyTo=12900#respond-post-7187)

[苏剑林](https://kexue.fm) 发表于
February 24th, 2020

增加是有的，但是按照我的理解，应该算不上“猛增”吧？你具体是从什么增加到什么？

[回复评论](https://kexue.fm/archives/7187/comment-page-1?replyTo=12903#respond-post-7187)

Warren 发表于
February 25th, 2020

我在跑xxlarge 其他条件相同的情况下，unshared 一个epoch要200多个小时，而普通的只用3，4个小时。我用的是cpu来进行运算的。

[回复评论](https://kexue.fm/archives/7187/comment-page-1?replyTo=12911#respond-post-7187)

Warren 发表于
February 25th, 2020

如果是用base版本的话，unshare要2分钟，普通的要1分钟。

[回复评论](https://kexue.fm/archives/7187/comment-page-1?replyTo=12912#respond-post-7187)

[苏剑林](https://kexue.fm) 发表于
February 26th, 2020

CPU...没体验过。GPU的情况下，base版慢10～20%左右。

[回复评论](https://kexue.fm/archives/7187/comment-page-1?replyTo=12914#respond-post-7187)

akun

July 2nd, 2020

请问大佬TensorFlow版本如何设置unshared去除权重共享呢，没找到相关代码呀…

[回复评论](https://kexue.fm/archives/7187/comment-page-1?replyTo=13697#respond-post-7187)

[苏剑林](https://kexue.fm) 发表于
July 2nd, 2020

没有简单的方法...这本来就不在albert的涉及范围内，你要搞的话只能改它的源码了。

[回复评论](https://kexue.fm/archives/7187/comment-page-1?replyTo=13702#respond-post-7187)

[用ALBERT和ELECTRA之前，请确认你真的了解它们 – 闪念基因 – 个人技术分享](https://flashgene.com/archives/155638.html)

October 30th, 2020

\[...\]至于效果，其实ALBERT的原论文已经说得很清楚，如下表所示。参数共享会限制模型的表达能力，因此ALBERT的xlarge版才能持平BERT的large版，而要稳定超过它则需要xxlarge版，换句话说，只要版本规格小于xlarge，那幺同一规格的ALBERT效果都是不如BERT的。中文任务上的评测结果也是类似的，可以参考 这里 和 这里 。而且笔者之前还做过更极端的实验：加载ALBERT的权重，\[...\]

[回复评论](https://kexue.fm/archives/7187/comment-page-1?replyTo=14664#respond-post-7187)

[allen](https://www.linkedin.com/in/allenyllee/)

November 13th, 2020

但如果是dropout 的問題，那bert 把dropout 拿掉結果不就更好了嗎？但是bert 的作者卻沒有這麼做，是不是表示dropout 確實有其必要？

我倒有不同的看法。我認為不是dropout的問題，而是albert 預訓練的時候參數共享，相當於每一層都學到一樣多東西。相比bert的第1層到第12層從random 開始訓練，到最後第12層會學到跟第1層一樣多東西嗎？我認為未必，也許到第12層已經學不到什麼了。而你的做法，把預訓練過的albert 拿掉參數共享，其實就相當於從第1層到第12層都學到一樣多東西的bert，而且還不用dropout(因為預訓練時事實上只有一層的權重，其他層都共享，如果還dropout，反而限制了表達能力)。而原始的bert 如不加dropout，很可能就會遇到第12層已經學不到甚麼東西的狀況，因為光是前面幾層複雜度就高到足以overfitting了。

不過你這邊並沒有把albert unshared 跟bert 做比較對吧？我認為你那個所謂的提升，應該就是回復到bert本來的水準而已。如果是這樣，那麼這個做法其實就只有在預訓練時比較快，預測時拿掉參數共享可以得到跟bert 接近的水準，如此而已。

[回复评论](https://kexue.fm/archives/7187/comment-page-1?replyTo=14786#respond-post-7187)

[苏剑林](https://kexue.fm) 发表于
November 14th, 2020

你误解我意思了。我说“dropout对于超大模型并不是一种有效的防止过拟合的方法”，但不是说“没有dropout更好”，而是“dropout不够好，不够有力”，导致bert+dropout在超大模型下不够dropout好。

我修改一下描述吧～谢谢指出。

[回复评论](https://kexue.fm/archives/7187/comment-page-1?replyTo=14791#respond-post-7187)

信使六号

February 24th, 2021

ALBERT 不能简单地说是一个 transformer block 过 12 次吧，只是 attention 做了参数共享， FFN 并没有呀

[回复评论](https://kexue.fm/archives/7187/comment-page-1?replyTo=15600#respond-post-7187)

[苏剑林](https://kexue.fm) 发表于
February 24th, 2021

ALBERT就是“一个 transformer block 过 12 次”，整个block都参数共享的。请仔细看原文或者阅读源码。

[回复评论](https://kexue.fm/archives/7187/comment-page-1?replyTo=15601#respond-post-7187)

oklen

April 20th, 2021

我记的ALBERT的dropout问题这个在ICLR上审稿的时候，作者说：我们首次发现了Dropout损害性能。reviews：\*\*\*，那是因为你这个用的是参数共享，而且这个别人早发现过。

[回复评论](https://kexue.fm/archives/7187/comment-page-1?replyTo=16156#respond-post-7187)

[苏剑林](https://kexue.fm) 发表于
April 20th, 2021

哈哈哈，学习了

[回复评论](https://kexue.fm/archives/7187/comment-page-1?replyTo=16163#respond-post-7187)

[取消回复](https://kexue.fm/archives/7187#respond-post-7187)

你的大名

电子邮箱

个人网站（选填）

1\. 可以使用LaTeX代码，点击“预览效果”可查看效果；2. 可以通过点击评论楼层编号来引用该楼层；3. 网站可能会有点卡，如非确认评论失败，请不要重复点击提交。

### 内容速览

[albert是什么](https://kexue.fm/archives/7187#albert%E6%98%AF%E4%BB%80%E4%B9%88)
[低秩分解](https://kexue.fm/archives/7187#%E4%BD%8E%E7%A7%A9%E5%88%86%E8%A7%A3)
[参数共享](https://kexue.fm/archives/7187#%E5%8F%82%E6%95%B0%E5%85%B1%E4%BA%AB)
[简单评述](https://kexue.fm/archives/7187#%E7%AE%80%E5%8D%95%E8%AF%84%E8%BF%B0)
[抛开共享约束](https://kexue.fm/archives/7187#%E6%8A%9B%E5%BC%80%E5%85%B1%E4%BA%AB%E7%BA%A6%E6%9D%9F)
[效果评测](https://kexue.fm/archives/7187#%E6%95%88%E6%9E%9C%E8%AF%84%E6%B5%8B)
[分析思考](https://kexue.fm/archives/7187#%E5%88%86%E6%9E%90%E6%80%9D%E8%80%83)
[文章的小结](https://kexue.fm/archives/7187#%E6%96%87%E7%AB%A0%E7%9A%84%E5%B0%8F%E7%BB%93)

### 智能搜索

支持整句搜索！网站自动使用 [结巴分词](https://github.com/fxsjy/jieba) 进行分词，并结合ngrams排序算法给出合理的搜索结果。

### 热门标签

[生成模型](https://kexue.fm/tag/%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B/) [attention](https://kexue.fm/tag/attention/) [优化](https://kexue.fm/tag/%E4%BC%98%E5%8C%96/) [语言模型](https://kexue.fm/tag/%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/) [模型](https://kexue.fm/tag/%E6%A8%A1%E5%9E%8B/) [网站](https://kexue.fm/tag/%E7%BD%91%E7%AB%99/) [概率](https://kexue.fm/tag/%E6%A6%82%E7%8E%87/) [梯度](https://kexue.fm/tag/%E6%A2%AF%E5%BA%A6/) [转载](https://kexue.fm/tag/%E8%BD%AC%E8%BD%BD/) [微分方程](https://kexue.fm/tag/%E5%BE%AE%E5%88%86%E6%96%B9%E7%A8%8B/) [矩阵](https://kexue.fm/tag/%E7%9F%A9%E9%98%B5/) [天象](https://kexue.fm/tag/%E5%A4%A9%E8%B1%A1/) [分析](https://kexue.fm/tag/%E5%88%86%E6%9E%90/) [深度学习](https://kexue.fm/tag/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/) [积分](https://kexue.fm/tag/%E7%A7%AF%E5%88%86/) [python](https://kexue.fm/tag/python/) [优化器](https://kexue.fm/tag/%E4%BC%98%E5%8C%96%E5%99%A8/) [力学](https://kexue.fm/tag/%E5%8A%9B%E5%AD%A6/) [无监督](https://kexue.fm/tag/%E6%97%A0%E7%9B%91%E7%9D%A3/) [扩散](https://kexue.fm/tag/%E6%89%A9%E6%95%A3/) [几何](https://kexue.fm/tag/%E5%87%A0%E4%BD%95/) [节日](https://kexue.fm/tag/%E8%8A%82%E6%97%A5/) [生活](https://kexue.fm/tag/%E7%94%9F%E6%B4%BB/) [文本生成](https://kexue.fm/tag/%E6%96%87%E6%9C%AC%E7%94%9F%E6%88%90/) [数论](https://kexue.fm/tag/%E6%95%B0%E8%AE%BA/)

### 随机文章

- [你跳绳的时候，想过绳子的形状曲线是怎样的吗？](https://kexue.fm/archives/6784)
- [Tiger：一个“抠”到极致的优化器](https://kexue.fm/archives/9512)
- [测试一个强悍的功能——ScienceWord](https://kexue.fm/archives/285)
- [GlobalPointer：用统一的方式处理嵌套和非嵌套NER](https://kexue.fm/archives/8373)
- [【NASA每日一图】飞马座的星系](https://kexue.fm/archives/60)
- [【NASA每日一图】火星上的奇形黑色陨石](https://kexue.fm/archives/73)
- [2010诺贝尔生理学或医学奖公布](https://kexue.fm/archives/970)
- [Transformer升级之路：13、逆用Leaky ReRoPE](https://kexue.fm/archives/9728)
- [Transformer升级之路：16、“复盘”长度外推技术](https://kexue.fm/archives/9948)
- [哈勃定律——宇宙各向同性的体现](https://kexue.fm/archives/964)

### 最近评论

- [PengchengMa](https://kexue.fm/archives/10996/comment-page-1#comment-27811): 牛啊
- [xczh](https://kexue.fm/archives/10958/comment-page-1#comment-27810): 已使用mean flow policy，一步推理效果确实惊人，性能跟多步推理的diffusio...
- [Cosine](https://kexue.fm/archives/10945/comment-page-1#comment-27809): 是不是因为shared experts每次都激活，而routed experts是依概率被选中...
- [rpsun](https://kexue.fm/archives/10699/comment-page-1#comment-27808): 这样似乎与传统的经验正交函数之类的有相似之处。把样本的平均值减掉之后做正交分解。那么如果单纯地...
- [贵阳机场接机](https://kexue.fm/archives/1490/comment-page-1#comment-27807): 怎么不更新啦
- [czvzb](https://kexue.fm/archives/10958/comment-page-1#comment-27806): 具身智能模型目前主流也是在使用扩散和流匹配这类方法来预测动作。
苏神推荐你看这几篇文章：
1....
- [Shawn\_yang](https://kexue.fm/archives/10945/comment-page-1#comment-27802): 苏神，关于您所说的：“推理阶段可以事先预估Routed Expert的实际分布，只要细致地进行...
- [OceanYU](https://kexue.fm/archives/9164/comment-page-4#comment-27801): 您好，关于由式（7）推导出高斯分布，我这里有一点问题，式（7）只能保证关于x\_t-1是二次函数...
- [jorjiang](https://kexue.fm/archives/10907/comment-page-2#comment-27800): 训练和prefill这个compute-bound阶段不做矩阵吸收，这个用我这个解释更好理解了...
- [amy](https://kexue.fm/archives/10907/comment-page-2#comment-27799): 苏老师，您有关注傅里叶旋转位置编码这篇工作吗，想知道您对这篇工作的看法是什么，这篇工作可以wo...

### 友情链接

- [Cool Papers](https://papers.cool)
- [数学研发](https://bbs.emath.ac.cn)
- [Seatop](http://www.seatop.com.cn/)
- [Xiaoxia](https://xiaoxia.org/)
- [积分表-网络版](https://kexue.fm/sci/integral/index.html)
- [丝路博傲](http://blog.dvxj.com/)
- [ph4ntasy 饭特稀](http://www.ph4ntasy.com/)
- [数学之家](http://www.2math.cn/)
- [有趣天文奇观](http://interesting-sky.china-vo.org/)
- [TwistedW](http://www.twistedwg.com/)
- [godweiyang](https://godweiyang.com/)
- [AI柠檬](https://blog.ailemon.net/)
- [王登科-DK博客](https://greatdk.com)
- [ESON](https://blog.eson.org/)
- [枫之羽](https://fzhiy.net/)
- [Mathor's blog](https://wmathor.com/)
- [coding-zuo](https://coding-zuo.github.io/)
- [博科园](https://www.bokeyuan.net/)
- [孔皮皮的博客](https://www.kppkkp.top/)
- [运鹏的博客](https://yunpengtai.top/)
- [jiming.site](https://jiming.site/)
- [OmegaXYZ](https://www.omegaxyz.com/)
- [Blog by Eacls](https://www.eacls.top/)
- [EAI猩球](https://www.robotech.ink/)
- [文举的博客](https://liwenju0.com/)
- [用代码打点酱油](https://bruceyuan.com/)
- [申请链接](https://kexue.fm/links.html)

本站采用创作共用版权协议，要求署名、非商业用途和保持一致。转载本站内容必须也遵循“ [署名-非商业用途-保持一致](http://creativecommons.org/licenses/by-nc-nd/2.5/cn/)”的创作共用协议。
© 2009-2025 Scientific Spaces. All rights reserved. Theme by [laogui](http://www.laogui.com). Powered by [Typecho](http://typecho.org). 备案号: [粤ICP备09093259号-1/2](https://beian.miit.gov.cn/)。