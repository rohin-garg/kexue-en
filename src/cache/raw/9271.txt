生成扩散模型漫谈（十一）：统一扩散模型（应用篇） - 科学空间|Scientific Spaces
![MobileSideBar](https://kexue.fm/usr/themes/geekg/images/slide-button.png "MobileSideBar")
## SEARCH
## MENU
* [打赏](https://kexue.fm/reward.html)
* [公式](https://kexue.fm/latex.html)
* [天象](https://kexue.fm/ac.html)
* [链接](https://kexue.fm/links.html)
* [时光](https://kexue.fm/me.html)
* [博览](https://kexue.fm/science.html)
* [归档](https://kexue.fm/content.html)
## CATEGORIES
* [千奇百怪](https://kexue.fm/category/Everything)
* [天文探索](https://kexue.fm/category/Astronomy)
* [数学研究](https://kexue.fm/category/Mathematics)
* [物理化学](https://kexue.fm/category/Phy-chem)
* [信息时代](https://kexue.fm/category/Big-Data)
* [生物自然](https://kexue.fm/category/Biology)
* [图片摄影](https://kexue.fm/category/Photograph)
* [问题百科](https://kexue.fm/category/Questions)
* [生活/情感](https://kexue.fm/category/Life-Feeling)
* [资源共享](https://kexue.fm/category/Resources)
## NEWPOSTS
* [让炼丹更科学一些（二）：将结论推广...](https://kexue.fm/archives/11469)
* [滑动平均视角下的权重衰减和学习率](https://kexue.fm/archives/11459)
* [生成扩散模型漫谈（三十一）：预测数...](https://kexue.fm/archives/11428)
* [Muon优化器指南：快速上手与关键细节](https://kexue.fm/archives/11416)
* [AdamW的Weight RMS的...](https://kexue.fm/archives/11404)
* [n个正态随机数的最大值的渐近估计](https://kexue.fm/archives/11390)
* [流形上的最速下降：5. 对偶梯度下降](https://kexue.fm/archives/11388)
* [低精度Attention可能存在有...](https://kexue.fm/archives/11371)
* [MuP之上：1. 好模型的三个特征](https://kexue.fm/archives/11340)
* [随机矩阵的谱范数的快速估计](https://kexue.fm/archives/11335)
## COMMENTS
* [ykwen: 最近想了一个观点，由于每次更新都做了各个不同分量的均衡，所以对...](https://kexue.fm/archives/10739/comment-page-2#comment-28992)
* [wednesday: 训练好的var不应该是接近1，log\_var接近0吗，编码器输...](https://kexue.fm/archives/5253/comment-page-2#comment-28991)
* [wednesday: 这样我们就能达到我们的先验假设：p(Z)是标准正态分布。然后我...](https://kexue.fm/archives/5253/comment-page-18#comment-28990)
* [xiaojx: 还有一个小小的补充问题，在实际训练DDPM时，需要固定随机参数...](https://kexue.fm/archives/9181/comment-page-5#comment-28989)
* [xiaojx: 苏老师您好！非常感谢您的解读。不过我对您提到的的实验结论1“...](https://kexue.fm/archives/9181/comment-page-5#comment-28988)
* [xiaowu: 苏神的讲解非常详细，设置给出了更好的优化思路](https://kexue.fm/archives/10958/comment-page-3#comment-28987)
* [沈天放: 实际的硬件是，输入的A和B都是BF16/FP16，硬件是一个M...](https://kexue.fm/archives/11371/comment-page-1#comment-28986)
* [GGbond: 苏神，想问下$QK^T$为什么在公式(4)中写为$q^Tk$是...](https://kexue.fm/archives/11033/comment-page-3#comment-28983)
* [沈天放: Stochastic Rounding一般怎么做的？主要是硬件...](https://kexue.fm/archives/11371/comment-page-1#comment-28982)
* [winter: 苏老师，请问deltanet能学习到full attn的att...](https://kexue.fm/archives/11033/comment-page-2#comment-28981)
## USERLOGIN
* [登录](https://kexue.fm/admin/login.php)
[科学空间|Scientific Spaces](https://kexue.fm)
* [登录](https://kexue.fm/admin/login.php)
* [打赏](https://kexue.fm/reward.html)
* [公式](https://kexue.fm/latex.html)
* [天象](https://kexue.fm/ac.html)
* [链接](https://kexue.fm/links.html)
* [时光](https://kexue.fm/me.html)
* [博览](https://kexue.fm/science.html)
* [归档](https://kexue.fm/content.html)
渴望成为一个小飞侠* [![](https://kexue.fm/usr/themes/geekg/images/rss.png)
欢迎订阅](https://kexue.fm/feed)
* [![](https://kexue.fm/usr/themes/geekg/images/mail.png)
个性邮箱](https://kexue.fm/archives/119)
* [![](https://kexue.fm/usr/themes/geekg/images/Saturn.png)
天象信息](https://kexue.fm/ac.html)
* [![](https://kexue.fm/usr/themes/geekg/images/iss.png)
观测ISS](https://kexue.fm/archives/41)
* [![](https://kexue.fm/usr/themes/geekg/images/pi.png)
LaTeX](https://kexue.fm/latex.html)
* [![](https://kexue.fm/usr/themes/geekg/images/mlogo.png)
关于博主](https://kexue.fm/me.html)
欢迎访问“科学空间”，这里将与您共同探讨自然科学，回味人生百态；也期待大家的分享～* [**千奇百怪**Everything](https://kexue.fm/category/Everything)
* [**天文探索**Astronomy](https://kexue.fm/category/Astronomy)
* [**数学研究**Mathematics](https://kexue.fm/category/Mathematics)
* [**物理化学**Phy-chem](https://kexue.fm/category/Phy-chem)
* [**信息时代**Big-Data](https://kexue.fm/category/Big-Data)
* [**生物自然**Biology](https://kexue.fm/category/Biology)
* [**图片摄影**Photograph](https://kexue.fm/category/Photograph)
* [**问题百科**Questions](https://kexue.fm/category/Questions)
* [**生活/情感**Life-Feeling](https://kexue.fm/category/Life-Feeling)
* [**资源共享**Resources](https://kexue.fm/category/Resources)
* [**千奇百怪**](https://kexue.fm/category/Everything)
* [**天文探索**](https://kexue.fm/category/Astronomy)
* [**数学研究**](https://kexue.fm/category/Mathematics)
* [**物理化学**](https://kexue.fm/category/Phy-chem)
* [**信息时代**](https://kexue.fm/category/Big-Data)
* [**生物自然**](https://kexue.fm/category/Biology)
* [**图片摄影**](https://kexue.fm/category/Photograph)
* [**问题百科**](https://kexue.fm/category/Questions)
* [**生活/情感**](https://kexue.fm/category/Life-Feeling)
* [**资源共享**](https://kexue.fm/category/Resources)
[首页](https://kexue.fm)[信息时代](https://kexue.fm/category/Big-Data)生成扩散模型漫谈（十一）：统一扩散模型（应用篇）
21Sep
# [生成扩散模型漫谈（十一）：统一扩散模型（应用篇）](https://kexue.fm/archives/9271)
By苏剑林|2022-09-21|71508位读者|:
在[《生成扩散模型漫谈（十）：统一扩散模型（理论篇）》](https://kexue.fm/archives/9262)中，笔者自称构建了一个统一的模型框架（Unified Diffusion Model，UDM），它允许更一般的扩散方式和数据类型。那么UDM框架究竟能否实现如期目的呢？本文通过一些具体例子来演示其一般性。
## 框架回顾[#](#框架回顾)
首先，UDM通过选择噪声分布$q(\\boldsymbol{\\varepsilon})$和变换$\\boldsymbol{\\mathcal{F}}$来构建前向过程
\\begin{equation}\\boldsymbol{x}\_t = \\boldsymbol{\\mathcal{F}}\_t(\\boldsymbol{x}\_0,\\boldsymbol{\\varepsilon}),\\quad \\boldsymbol{\\varepsilon}\\sim q(\\boldsymbol{\\varepsilon})\\end{equation}
然后，通过如下的分解来实现反向过程$\\boldsymbol{x}\_{t-1}\\sim p(\\boldsymbol{x}\_{t-1}|\\boldsymbol{x}\_t)$的采样
\\begin{equation}\\hat{\\boldsymbol{x}}\_0\\sim p(\\boldsymbol{x}\_0|\\boldsymbol{x}\_t)\\quad \\& \quad& \quad \\boldsymbol{x}\_{t-1}\\sim p(\\boldsymbol{x}\_{t-1}|\\boldsymbol{x}\_t, \\boldsymbol{x}\_0=\\hat{\\boldsymbol{x}}\_0)\\end{equation}
其中$p(\\boldsymbol{x}\_0|\\boldsymbol{x}\_t)$就是用$\\boldsymbol{x}\_t$预估$\\boldsymbol{x}\_0$的概率，一般用简单分布$q(\\boldsymbol{x}\_0|\\boldsymbol{x}\_t)$来近似建模，训练目标基本上就是$-\\log q(\\boldsymbol{x}\_0|\\boldsymbol{x}\_t)$或其简单变体。当$\\boldsymbol{x}\_0$是连续型数据时，$q(\\boldsymbol{x}\_0|\\boldsymbol{x}\_t)$一般就取条件正态分布；当$\\boldsymbol{x}\_0$是离散型数据时，$q(\\boldsymbol{x}\_0|\\boldsymbol{x}\_t)$可以选择自回归模型或者非自回归模型。
至于$p(\\boldsymbol{x}\_{t-1}|\\boldsymbol{x}\_t, \\boldsymbol{x}\_0)$的最基准的选择就是
\\begin{equation}p(\\boldsymbol{x}\_{t-1}|\\boldsymbol{x}\_t, \\boldsymbol{x}\_0) = p(\\boldsymbol{x}\_{t-1}|\\boldsymbol{x}\_0)\\quad\\Leftrightarrow\\quad \\boldsymbol{x}\_{t-1}=\\boldsymbol{\\mathcal{F}}\_{t-1}(\\boldsymbol{x}\_0,\\boldsymbol{\\varepsilon})\\end{equation}
从这个基准出发，在不同的条件下可以得到不同的优化结果。当$\\boldsymbol{\\mathcal{F}}\_t(\\boldsymbol{x}\_0,\\boldsymbol{\\varepsilon})$关于$\\boldsymbol{\\varepsilon}$是可逆的，那么可以解出$\\boldsymbol{\\varepsilon} = \\boldsymbol{\\mathcal{F}}\_t^{-1}(\\boldsymbol{x}\_0,\\boldsymbol{x}\_t)$，然后得到更好的确定性采样方式
\\begin{equation}\\boldsymbol{x}\_{t-1} = \\boldsymbol{\\mathcal{F}}\_{t-1}(\\boldsymbol{x}\_0,\\boldsymbol{\\mathcal{F}}\_t^{-1}(\\boldsymbol{x}\_0,\\boldsymbol{x}\_t))\\end{equation}
更进一步，如果$q(\\boldsymbol{\\varepsilon})$是标准正态分布，那么可以得到
\\begin{equation}\\quad\\boldsymbol{x}\_{t-1} = \\boldsymbol{\\mathcal{F}}\_{t-1}(\\boldsymbol{x}\_0,\\sqrt{1 - \\tilde{\\sigma}\_t^2}\\boldsymbol{\\mathcal{F}}\_t^{-1}(\\boldsymbol{x}\_0,\\boldsymbol{x}\_t) + \\tilde{\\sigma}\_t \\boldsymbol{\\varepsilon})\\end{equation}
## 热之扩散[#](#热之扩散)
现在这一节中，我们证明“热扩散模型”是UDM的一个特例，这里的热扩散（Hot Diffusion）指的是前面介绍的[DDPM](https://kexue.fm/archives/9119)、[DDIM](https://kexue.fm/archives/9181)等主流的扩散模型，这个称呼出自下面的“冷扩散”论文中。
主流扩散模型处理的是连续型数据，以加性正态噪声来构建前向过程：\\begin{equation}\\boldsymbol{x}\_t = \\bar{\\alpha}\_t \\boldsymbol{x}\_0 + \\bar{\\beta}\_t \\boldsymbol{\\varepsilon},\\quad \\boldsymbol{\\varepsilon}\\sim \\mathcal{N}(\\boldsymbol{0},\\boldsymbol{I})\\end{equation}
$q(\\boldsymbol{x}\_0|\\boldsymbol{x}\_t)$的选择就是正态分布$\\mathcal{N}(\\boldsymbol{x}\_0;\\bar{\\boldsymbol{\\mu}}(\\boldsymbol{x}\_t),\\bar{\\sigma}\_t^2 \\boldsymbol{I})$，一般不将$\\bar{\\sigma}\_t$作为训练参数，所以略去常数项后就有
\\begin{equation}-\\log q(\\boldsymbol{x}\_0|\\boldsymbol{x}\_t) = \\frac{1}{2\\bar{\\sigma}\_t^2}\\Vert\\boldsymbol{x}\_0 - \\bar{\\boldsymbol{\\mu}}(\\boldsymbol{x}\_t)\\Vert^2\\end{equation}
进一步引入参数化$\\bar{\\boldsymbol{\\mu}}(\\boldsymbol{x}\_t) = \\frac{1}{\\bar{\\alpha}\_t}\\left(\\boldsymbol{x}\_t - \\bar{\\beta}\_t \\boldsymbol{\\epsilon}\_{\\boldsymbol{\\theta}}(\\boldsymbol{x}\_t, t)\\right)$并结合$\\boldsymbol{x}\_t = \\bar{\\alpha}\_t \\boldsymbol{x}\_0 + \\bar{\\beta}\_t \\boldsymbol{\\varepsilon}$得到
\\begin{equation}-\\log q(\\boldsymbol{x}\_0|\\boldsymbol{x}\_t) = \\frac{\\bar{\\beta}\_t^2}{2\\bar{\\sigma}\_t^2\\bar{\\alpha}\_t^2}\\left\\Vert\\boldsymbol{\\varepsilon} - \\boldsymbol{\\epsilon}\_{\\boldsymbol{\\theta}}(\\bar{\\alpha}\_t \\boldsymbol{x}\_0 + \\bar{\\beta}\_t \\boldsymbol{\\varepsilon}, t)\\right\\Vert^2\\end{equation}
实验显示略去前面的系数后效果更好，所以最终训练目标一般是$\\Vert\\boldsymbol{\\varepsilon} - \\boldsymbol{\\epsilon}\_{\\boldsymbol{\\theta}}(\\bar{\\alpha}\_t \\boldsymbol{x}\_0 + \\bar{\\beta}\_t \\boldsymbol{\\varepsilon}, t)\\Vert^2$。至于采样过程中$\\bar{\\sigma}\_t$的选择，可以参考[《生成扩散模型漫谈（七）：最优扩散方差估计（上）》](https://kexue.fm/archives/9245)、[《生成扩散模型漫谈（八）：最优扩散方差估计（下）》](https://kexue.fm/archives/9246)来进行。
最后，关于$p(\\boldsymbol{x}\_{t-1}|\\boldsymbol{x}\_t, \\boldsymbol{x}\_0)$我们有
\\begin{equation}\\begin{aligned}\\boldsymbol{x}\_{t-1} =&\, \ba&\, \bar{\\alpha}\_{t-1} \\boldsymbol{x}\_0 + \\bar{\\beta}\_{t-1} \\boldsymbol{\\varepsilon}\\\\
\\sim&\, \ba&\, \bar{\\alpha}\_{t-1} \\boldsymbol{x}\_0 + \\sqrt{\\bar{\\beta}\_{t-1}^2 - \\sigma\_t^2}\\boldsymbol{\\varepsilon}\_1 + \\sigma\_t\\boldsymbol{\\varepsilon}\_2\\end{aligned}
\\,\\,,\\quad \\boldsymbol{\\varepsilon},\\boldsymbol{\\varepsilon}\_1,\\boldsymbol{\\varepsilon}\_2\\sim \\mathcal{N}(\\boldsymbol{0},\\boldsymbol{I})\\end{equation}
从$\\boldsymbol{x}\_t = \\bar{\\alpha}\_t \\boldsymbol{x}\_0 + \\bar{\\beta}\_t \\boldsymbol{\\varepsilon}$解得$\\boldsymbol{\\varepsilon} = \\left.(\\boldsymbol{x}\_t - \\bar{\\alpha}\_t \\boldsymbol{x}\_0)\\right/ \\bar{\\beta}\_t$，替换掉$\\boldsymbol{\\varepsilon}\_1$，最终可以得到一般的$p(\\boldsymbol{x}\_{t-1}|\\boldsymbol{x}\_t, \\boldsymbol{x}\_0)$为
\\begin{equation}\\boldsymbol{x}\_{t-1} = \\bar{\\alpha}\_{t-1} \\boldsymbol{x}\_0 + \\sqrt{\\bar{\\beta}\_{t-1}^2 - \\sigma\_t^2}\\frac{\\boldsymbol{x}\_t - \\bar{\\alpha}\_t \\boldsymbol{x}\_0}{\\bar{\\beta}\_t} + \\sigma\_t\\boldsymbol{\\varepsilon},\\quad\\boldsymbol{\\varepsilon}\\sim \\mathcal{N}(\\boldsymbol{0},\\boldsymbol{I})\\end{equation}
而$\\hat{\\boldsymbol{x}}\_0\\sim p(\\boldsymbol{x}\_0|\\boldsymbol{x}\_t)$意味着
\\begin{equation}\\hat{\\boldsymbol{x}}\_0 = \\bar{\\boldsymbol{\\mu}}(\\boldsymbol{x}\_t) + \\bar{\\sigma}\_t \\boldsymbol{\\varepsilon} = \\frac{1}{\\bar{\\alpha}\_t}\\left(\\boldsymbol{x}\_t - \\bar{\\beta}\_t \\boldsymbol{\\epsilon}\_{\\boldsymbol{\\theta}}(\\boldsymbol{x}\_t, t)\\right) + \\bar{\\sigma}\_t \\boldsymbol{\\varepsilon}\\end{equation}
上面两式结合，就是最一般的主流扩散模型框架的反向过程，其中DDPM取了$\\bar{\\sigma}\_t=0,\\sigma\_t = \\frac{\\bar{\\beta}\_{t-1}\\beta\_t}{\\bar{\\beta}\_t}$，DDIM则取了$\\bar{\\sigma}\_t=0,\\sigma\_t = 0$，而Analytical-DPM则重新估计了最优的非零的$\\bar{\\sigma}\_t$。
## 冷之扩散[#](#冷之扩散)
接下来，我们证明[《Cold Diffusion: Inverting Arbitrary Image Transforms Without Noise》](https://papers.cool/arxiv/2208.09392)所介绍的“冷扩散（Cold Diffusion）”也是UDM的一个特例。Cold Diffusion处理的也是连续型数据，从论文标题可以看出，它着重于使用任意（无噪声的）变换来构建前向过程，据笔者所知，这是第一篇尝试一般前向过程的论文，UDM在构建过程中，受到了它的颇多启发，在此对原作者表示感谢。
Cold Diffusion通过确定性的变换$\\boldsymbol{x}\_t = \\boldsymbol{\\mathcal{F}}\_t(\\boldsymbol{x}\_0)$构建前向过程，为了方便后面的分析，我们引入更一般的前向过程
\\begin{equation}\\boldsymbol{x}\_t = \\boldsymbol{\\mathcal{F}}\_t(\\boldsymbol{x}\_0) + \\sigma \\boldsymbol{\\varepsilon},\\quad \\boldsymbol{\\varepsilon}\\sim q(\\boldsymbol{\\varepsilon})\\end{equation}
这里的变换$\\boldsymbol{\\mathcal{F}}$可以是对原始数据的任意破坏方式，对于图像来说有模糊、遮掩、池化等，如果需要确定性的变换，事后让$\\sigma\\to 0$即可。
接着，$q(\\boldsymbol{x}\_0|\\boldsymbol{x}\_t)$的选择为$l\_1$范数为度量的正态分布，即
\\begin{equation}q(\\boldsymbol{x}\_0|\\boldsymbol{x}\_t) = \\frac{1}{Z(\\tau)}\\int e^{-\\left.\\Vert\\boldsymbol{x}\_0 - \\boldsymbol{\\mathcal{G}}\_t(\\boldsymbol{x}\_t)\\Vert\_1\\right/\\tau}d\\boldsymbol{x}\_0\\end{equation}
其中$Z(\\tau)$是归一化因子。取$\\tau$为固定值，那么除去常数项后有$-\\log q(\\boldsymbol{x}\_0|\\boldsymbol{x}\_t)\\propto\\Vert\\boldsymbol{x}\_0 - \\boldsymbol{\\mathcal{G}}\_t(\\boldsymbol{x}\_t)\\Vert\_1$，结合$\\boldsymbol{x}\_t = \\boldsymbol{\\mathcal{F}}\_t(\\boldsymbol{x}\_0)$，得到训练目标为最小化
\\begin{equation}\\Vert\\boldsymbol{x}\_0 - \\boldsymbol{\\mathcal{G}}\_t(\\boldsymbol{\\mathcal{F}}\_t(\\boldsymbol{x}\_0))\\Vert\_1\\end{equation}
在反向过程中，Cold Diffusion直接忽略了$q(\\boldsymbol{x}\_0|\\boldsymbol{x}\_t)$的方差（即让$\\tau\\to 0$），这样得到$\\hat{\\boldsymbol{x}}\_0 = \\boldsymbol{\\mathcal{G}}\_t(\\boldsymbol{x}\_t)$。如果$p(\\boldsymbol{x}\_{t-1}|\\boldsymbol{x}\_t, \\boldsymbol{x}\_0)$直接取基准选择$p(\\boldsymbol{x}\_{t-1}|\\boldsymbol{x}\_0)$，即$\\boldsymbol{x}\_{t-1} = \\boldsymbol{\\mathcal{F}}\_{t-1}(\\boldsymbol{x}\_0) + \\sigma \\boldsymbol{\\varepsilon}$，那么代入$\\hat{\\boldsymbol{x}}\_0$并取$\\sigma\\to 0$的极限后就得到
\\begin{equation}\\hat{\\boldsymbol{x}}\_0=\\boldsymbol{\\mathcal{G}}\_t(\\boldsymbol{x}\_t),\\quad \\boldsymbol{x}\_{t-1} = \\boldsymbol{\\mathcal{F}}\_{t-1}(\\hat{\\boldsymbol{x}}\_0)\\end{equation}
这就是原论文的“Naive Sampling”。而如果从$\\boldsymbol{x}\_t = \\boldsymbol{\\mathcal{F}}\_t(\\boldsymbol{x}\_0) + \\sigma \\boldsymbol{\\varepsilon}$解出$\\boldsymbol{\\varepsilon} = \\left.(\\boldsymbol{x}\_t - \\boldsymbol{\\mathcal{F}}\_t(\\boldsymbol{x}\_0))\\right/\\sigma$后，代入$\\boldsymbol{x}\_{t-1} = \\boldsymbol{\\mathcal{F}}\_{t-1}(\\boldsymbol{x}\_0) + \\sigma \\boldsymbol{\\varepsilon}$中就得到
\\begin{equation}\\hat{\\boldsymbol{x}}\_0=\\boldsymbol{\\mathcal{G}}\_t(\\boldsymbol{x}\_t),\\quad \\boldsymbol{x}\_{t-1} = \\boldsymbol{x}\_t + \\boldsymbol{\\mathcal{F}}\_{t-1}(\\hat{\\boldsymbol{x}}\_0) - \\boldsymbol{\\mathcal{F}}\_t(\\hat{\\boldsymbol{x}}\_0)\\end{equation}
这就是原论文的“Improved Sampling”。
总的来说，Cold Diffusion首次成功实现了一般变换的前向过程的实现，但由于它过于强调“Without Noise”，所以它理论上有着无法弥补的缺陷。比如，对于$w\\times w\\times 3$的图片数据，Cold Diffusion在用模糊操作实现前向过程时，最终结果就相当于一个$3$维向量，而Cold Diffusion的反向过程也是确定性的，所以就是说Cold Diffusion通过一个确定性的变换，将$3w^2$维的图片变成了$3$维，然后又通过确定性的变换，将$3$维重建为$3w^2$维的图片，其中间过程必然有着严重的信息损失的，这必然会限制重建的清晰度，从而也限制了生成的清晰度。
要解决这个问题，就不能在前向或者反向过程中拒绝噪声的存在。因为噪声意味着不确定性，不确定性意味着“一对多”，“一对多”意味着允许“多对一”的前向过程，即允许信息损失的出现。事实上，Cold Diffusion本身就已经意识到$3$维的向量难以生成$3w^2$维的完整数据这个事实了，它在生成过程中，事实上还往这个$3$维向量加入了$3w^2$维的轻微随机噪声，实验显示这个操作提高了生成效果。而这个操作大致上就相当于$\\sigma \> 0$的前向过程了。
## 编辑模型[#](#编辑模型)
以上两个例子处理的都是连续型数据，而我们说过，UDM原则上不限定数据类型，这一节我们介绍一个离散型的例子，它显示基于编辑操作的文本生成模型，本质上也可以看成UDM的特例。
简单起见，我们考虑长度为$l$的定长句子生成，比如五言律诗、七言绝句等，变长句子不是不可以，而是细节上稍微复杂些。然后，我们将前向过程$\\boldsymbol{x}\_t = \\boldsymbol{\\mathcal{F}}\_t(\\boldsymbol{x}\_0,\\boldsymbol{\\varepsilon})$定义为“随机替换”，即
> 随机选句子中的$t$个token随机替换为别的token
其中$t\\leq l$时，当$t=l$时，此时$\\boldsymbol{x}\_t$就是$l$个完全随机组合的token。
此时$q(\\boldsymbol{x}\_0|\\boldsymbol{x}\_t)$就是用随机替换后的序列来预测原序列的模型，用自回归/非自回归模型均可，损失函数用交叉熵。注意此时$\\boldsymbol{\\mathcal{F}}\_t(\\boldsymbol{x}\_0,\\boldsymbol{\\varepsilon})$关于噪声必然是不可逆的（即给定$\\boldsymbol{x}\_0$和$\\boldsymbol{x}\_t$，从$\\boldsymbol{x}\_0$变到$\\boldsymbol{x}\_t$的方式不止有一种），因此我们只能用基准选择$p(\\boldsymbol{x}\_{t-1}|\\boldsymbol{x}\_t, \\boldsymbol{x}\_0)=p(\\boldsymbol{x}\_{t-1}|\\boldsymbol{x}\_0)$，这意味着生成过程是：
> 1、随机选$l$个token作为初始的$\boldsymbol{x}_l$；
> 2、从$q(\boldsymbol{x}_0|\boldsymbol{x}_t)$预测$\hat{\boldsymbol{x}}_0$；
> 3、随机选$\hat{\boldsymbol{x}}_0$的$t-1$个token随机替换为别的token，作为$\boldsymbol{x}_{t-1}$；
> 4、反复执行2、3步，直到得出最终的$\boldsymbol{x}_0$。
但是，这样的算法效果不会很好，因为第2步的预估成果往往会被第3步的随机替换“毁”掉不少，有点“一夜回到解放前”的感觉，要想提高效果，就必须要用更好的采样方案，这要求$\\boldsymbol{\\mathcal{F}}\_t(\\boldsymbol{x}\_0,\\boldsymbol{\\varepsilon})$关于噪声可逆，也就是从给定的$\\boldsymbol{x}\_0$和$\\boldsymbol{x}\_t$可以看出变换方式是怎样的。为此，我们规定前向过程为：
> 随机选句子中的$t$个token随机替换为
**> 不同**> 的token
它跟原来的区别是随机替换的过程中，原来的token必须替换为原来不一样的token，如果不做这个选择，则有可能采样到一样的token。做了这个限制后，我们可以直接对比$\\boldsymbol{x}\_0$和$\\boldsymbol{x}\_t$的差异，来看出修改了什么，从而将第3步的随机替换，换成由$\\hat{\\boldsymbol{x}}\_0$到$\\boldsymbol{x}\_t$的替换变换：
> 1、随机选$l$个token作为初始的$\boldsymbol{x}_l$；
> 2、从$q(\boldsymbol{x}_0|\boldsymbol{x}_t)$预测$\hat{\boldsymbol{x}}_0$，要求$\hat{\boldsymbol{x}}_0$与$\boldsymbol{x}_t$有$t$个不同token（用非自回归模型比较好实现）；
> 3、随机选$\boldsymbol{x}_t$中与$\hat{\boldsymbol{x}}_0$不同的token中的一个，替换为$\hat{\boldsymbol{x}}_0$对应位置的token，作为$\boldsymbol{x}_{t-1}$；
> 4、反复执行2、3步，直到得出最终的$\boldsymbol{x}_0$。
这样一来，每次的预测结果$\\hat{\\boldsymbol{x}}\_0$的有效部分（$\\hat{\\boldsymbol{x}}\_0$与$\\boldsymbol{x}\_t$相同的部分）都得以保留，并且$\\boldsymbol{x}\_{t-1}$与$\\boldsymbol{x}\_t$相比只修改了一个token，因此生成过程是渐进式的稳定生成。它跟普通的自回归模型区别则是去掉了从左往右的生成方向限制。
## 掩码模型[#](#掩码模型)
如果读者对上述模型还是很模糊，这里笔者再提供一个简单例子辅助理解。同样考虑长度为$l$的定长句子生成，我们将前向过程$\\boldsymbol{x}\_t = \\boldsymbol{\\mathcal{F}}\_t(\\boldsymbol{x}\_0,\\boldsymbol{\\varepsilon})$定义为“随机掩码”，即
> 随机选句子中的$t$个token随机替换为[MASK]
其中$t\\leq l$时，当$t=l$时，此时$\\boldsymbol{x}\_t$就是$l$个[MASK]。
此时$q(\\boldsymbol{x}\_0|\\boldsymbol{x}\_t)$就是用带[MASK]的序列来预测原序列的模型，用一般用类似BERT的MLM模型（非自回归模型）来实现，损失函数用交叉熵。基准的生成过程是
生成过程是：> 1、以$l$个[MASK]作为初始的$\boldsymbol{x}_l$；
> 2、从$q(\boldsymbol{x}_0|\boldsymbol{x}_t)$采样$\hat{\boldsymbol{x}}_0$；
> 3、随机选$\hat{\boldsymbol{x}}_0$的$t-1$个token随机替换为[MASK]，作为$\boldsymbol{x}_{t-1}$；
> 4、反复执行2、3步，直到得出最终的$\boldsymbol{x}_0$。
注意到，此时$\\boldsymbol{\\mathcal{F}}\_t(\\boldsymbol{x}\_0,\\boldsymbol{\\varepsilon})$关于噪声是可逆的，即我们完全可以从给定的$\\boldsymbol{x}\_0$和$\\boldsymbol{x}\_t$可以看出变换方式是怎样的（即哪些token被替换为了[MASK]）。因此可以构造改进版生成过程
> 1、以$l$个[MASK]作为初始的$\boldsymbol{x}_l$；
> 2、从$q(\boldsymbol{x}_0|\boldsymbol{x}_t)$采样$\hat{\boldsymbol{x}}_0$，注意只需采样那些原来是[MASK]的token，原来非[MASK]的不做改变；
> 3、从原来$\boldsymbol{x}_t$的$t$个[MASK]所在位置中随机选$t-1$个，将$\hat{\boldsymbol{x}}_0$的这些位置的token替换为[MASK]，作为$\boldsymbol{x}_{t-1}$；
> 4、反复执行2、3步，直到得出最终的$\boldsymbol{x}_0$。
当然，其实第2、3步可以合并为更直接的一步：
> 2 & 3、从$\boldsymbol{x}_t$的$t$个[MASK]所在位置中随机选$1$个，按$q(\boldsymbol{x}_0|\boldsymbol{x}_t)$对应位置的概率采样一个token替换上去，作为$\boldsymbol{x}_{t-1}$；
这跟基于MLM模型的Gibbs采样几乎一致了（参考[《【搜出来的文本】⋅（三）基于BERT的文本采样》](https://kexue.fm/archives/8119)）。从“编辑模型”和“掩码模型”两个例子我们应该可以大致体会到，很多“渐变式生成”的模型，都可以用UDM框架来重新表述。又或者反过来，我们能想到的任何渐变式生成方式，都可以尝试用UDM框架来构建其概率表述。
## 编码模型[#](#编码模型)
前面我们所讨论的前向过程都是无训练参数的，也就是说都是事先设计好的流程，但这其实也并不是必要的。我们可以将DDPM的扩散过程一般化为
\\begin{equation}\\boldsymbol{x}\_t = \\bar{\\alpha}\_t \\boldsymbol{\\mathcal{F}}(\\boldsymbol{x}\_0) + \\bar{\\beta}\_t \\boldsymbol{\\varepsilon},\\quad \\boldsymbol{\\varepsilon}\\sim \\mathcal{N}(\\boldsymbol{0},\\boldsymbol{I})\\end{equation}
其中$\\boldsymbol{\\mathcal{F}}(\\boldsymbol{x}\_0)$是对$\\boldsymbol{x}\_0$的编码模型，可以带训练参数。此时训练目标就是
\\begin{equation}-\\log q(\\boldsymbol{x}\_0|\\boldsymbol{x}\_t) = -\\log q(\\boldsymbol{x}\_0|\\bar{\\alpha}\_t\\boldsymbol{\\mathcal{F}}(\\boldsymbol{x}\_0) + \\bar{\\beta}\_t \\boldsymbol{\\varepsilon})\\end{equation}
只不过此时$\\boldsymbol{\\mathcal{F}}$也有训练参数。至于反向过程也是类似的，只不过最后采样到$\\hat{\\boldsymbol{x}}\_0\\sim q(\\boldsymbol{x}\_0|\\boldsymbol{x}\_1)$就直接返回$\\hat{\\boldsymbol{x}}\_0$了。特别地，由于多了一个编码模型$\\boldsymbol{\\mathcal{F}}$，所以输入$\\boldsymbol{x}\_0$既可以是离散型数据，也可以是连续型数据，它提供了类似VAE的将数据分布编码到隐变量的正态分布的一种方法。
## 文章小结[#](#文章小结)
本文主要应用上一篇文章所构建的统一扩散模型框架（Unified Diffusion Model，UDM）来推导几个具体的例子，包括主流的扩散模型、Cold Diffusion、文本编辑生成、编码模型等。
***转载到请包括本文地址：** [https://kexue.fm/archives/9271](https://kexue.fm/archives/9271)*
***更详细的转载事宜请参考：*** [《科学空间FAQ》](https://kexue.fm/archives/6508#文章如何转载/引用)
**如果您还有什么疑惑或建议，欢迎在下方评论区继续讨论。**
**如果您觉得本文还不错，欢迎[分享](#share)/[打赏](#pay)本文。打赏并非要从中获得收益，而是希望知道科学空间获得了多少读者的真心关注。当然，如果你无视它，也不会影响你的阅读。再次表示欢迎和感谢！**
打赏![科学空间](https://kexue.fm/usr/themes/geekg/payment/wx.png)
微信打赏![科学空间](https://kexue.fm/usr/themes/geekg/payment/zfb.png)
支付宝打赏因为网站后台对打赏并无记录，因此欢迎在打赏时候备注留言。 你还可以[**点击这里**](http://mail.qq.com/cgi-bin/qm_share?t=qm_mailme&email=tN7d1drY3drrx8H0xcWa19vZ)或在下方评论区留言来告知你的建议或需求。
**如果您需要引用本文，请参考：**
苏剑林. (Sep. 21, 2022). 《生成扩散模型漫谈（十一）：统一扩散模型（应用篇）》[Blog post]. Retrieved from[https://kexue.fm/archives/9271](https://kexue.fm/archives/9271)
@online{kexuefm-9271,
title={生成扩散模型漫谈（十一）：统一扩散模型（应用篇）},
author={苏剑林},
year={2022},
month={Sep},
url={\\url{https://kexue.fm/archives/9271}},
}
分类：[信息时代](https://kexue.fm/category/Big-Data) 标签：[统一](https://kexue.fm/tag/统一/),[生成模型](https://kexue.fm/tag/生成模型/),[DDPM](https://kexue.fm/tag/DDPM/),[扩散](https://kexue.fm/tag/扩散/)[18 评论](https://kexue.fm/archives/9271#comments)
&lt;[生成扩散模型漫谈（十）：统一扩散模型（理论篇）](https://kexue.fm/archives/9262)|[生成扩散模型漫谈（十二）：“硬刚”扩散ODE](https://kexue.fm/archives/9280)&gt;
### 你也许还对下面的内容感兴趣* [生成扩散模型漫谈（三十一）：预测数据而非噪声](https://kexue.fm/archives/11428)
* [DiVeQ：一种非常简洁的VQ训练方案](https://kexue.fm/archives/11328)
* [为什么线性注意力要加Short Conv？](https://kexue.fm/archives/11320)
* [Transformer升级之路：21、MLA好在哪里?（下）](https://kexue.fm/archives/11111)
* [线性注意力简史：从模仿、创新到反哺](https://kexue.fm/archives/11033)
* [生成扩散模型漫谈（三十）：从瞬时速度到平均速度](https://kexue.fm/archives/10958)
* [Transformer升级之路：20、MLA好在哪里?（上）](https://kexue.fm/archives/10907)
* [生成扩散模型漫谈（二十九）：用DDPM来离散编码](https://kexue.fm/archives/10711)
* [细水长flow之TARFLOW：流模型满血归来？](https://kexue.fm/archives/10667)
* [生成扩散模型漫谈（二十八）：分步理解一致性模型](https://kexue.fm/archives/10633)
[发表你的看法](#comment_form)
[Halo Master](https://borrowastep.net/)
September 29th, 2022
是不是说，噪声才是学习的关键？？我有一种感觉，也许，学习的难点就是噪声才是所谓的泛化/创造的本源。
之前的学习都选择性忽视，从Dropout开始，噪声才真正开始显露出其主导地位。
或者，构造更好的噪声才是未来的方向？那么什么是更好的噪声呢？这个我也没想明白，我猜是：噪声要在时间和空间和频率维度具有，连续性和局部性。[回复评论](https://kexue.fm/archives/9271/comment-page-1?replyTo=19961#respond-post-9271)
[苏剑林](https://kexue.fm)发表于 October 3rd, 2022
“噪声”改为“去噪”比较贴切？[回复评论](https://kexue.fm/archives/9271/comment-page-1?replyTo=19973#respond-post-9271)
Yuanzhi
October 8th, 2022
热扩散最后面，重新估计了最优的非零的，是不是多打了一个=0
[回复评论](https://kexue.fm/archives/9271/comment-page-1?replyTo=20004#respond-post-9271)
[苏剑林](https://kexue.fm)发表于 October 8th, 2022
嗯嗯，已修正，谢谢指出。[回复评论](https://kexue.fm/archives/9271/comment-page-1?replyTo=20023#respond-post-9271)
frankie
October 11th, 2022
感谢分享，想请教一下“只不过最后采样到$\\hat{x}\_0 \\sim q(x\_0|x\_t)$就直接返回$\\hat{x}\_0$了“这句话是什么意思？是不用修正吗？
[回复评论](https://kexue.fm/archives/9271/comment-page-1?replyTo=20055#respond-post-9271)
[苏剑林](https://kexue.fm)发表于 October 12th, 2022
应该是$\\hat{\\boldsymbol{x}}\_0\\sim q(\\boldsymbol{x}\_0|\\boldsymbol{x}\_1)$，已修正～
[回复评论](https://kexue.fm/archives/9271/comment-page-1?replyTo=20061#respond-post-9271)
frankie 发表于October 12th, 2022
谢谢回复，那就是和DDPM一样的意思，最后一步不加噪声
[回复评论](https://kexue.fm/archives/9271/comment-page-1?replyTo=20072#respond-post-9271)
leoozy
December 24th, 2022
请问对于cold diffusion是将一个3w^2维度的数据变成一个维度为3的该怎么理解呢。非常感谢
[回复评论](https://kexue.fm/archives/9271/comment-page-1?replyTo=20633#respond-post-9271)
[苏剑林](https://kexue.fm)发表于 December 28th, 2022
像模糊操作，是R、G、B每个通道独立执行的，模糊到最后，每个通道的$w\\times w$个像素，都都变成了同一个值，所以虽然还是$w\\times w\\times 3$的向量，但只有三个不同的值。
[回复评论](https://kexue.fm/archives/9271/comment-page-1?replyTo=20658#respond-post-9271)
Jingshuai Liu
January 9th, 2023
最后这个编码模型是不是就像stable diffusion?
[回复评论](https://kexue.fm/archives/9271/comment-page-1?replyTo=20724#respond-post-9271)
[苏剑林](https://kexue.fm)发表于 January 13th, 2023
你要说“像”，确实有一定的相似之处吧。[回复评论](https://kexue.fm/archives/9271/comment-page-1?replyTo=20740#respond-post-9271)
jackieX
May 30th, 2023
请问下最后的编码模型有其它的论文案例吗，有点不明白前向过程如果是参数化的，那么采样的时候输入应该是什么呢？还是噪声吗？[回复评论](https://kexue.fm/archives/9271/comment-page-1?replyTo=21807#respond-post-9271)
[苏剑林](https://kexue.fm)发表于 May 31st, 2023
即便前向过程是参数化的，它的终点也应该是一个纯粹的随机噪声呀，采样的输入就是随机噪声。案例好像有，但我没有记录～[回复评论](https://kexue.fm/archives/9271/comment-page-1?replyTo=21841#respond-post-9271)
龙行December 26th, 2023
苏神：1 公式9中方差可能应该去掉bar
2 公式14中用x\_0代替x\_t为什么可以将eps丢掉呢？
[回复评论](https://kexue.fm/archives/9271/comment-page-1?replyTo=23365#respond-post-9271)
[苏剑林](https://kexue.fm)发表于 December 30th, 2023
1、已经更正，谢谢；
2、公式$(12)$中的$\\boldsymbol{\\varepsilon}$只是辅助推导用的，最终推导出来的一切结果都要让$\\sigma\\to 0$。
[回复评论](https://kexue.fm/archives/9271/comment-page-1?replyTo=23404#respond-post-9271)
NEK
December 25th, 2024
您好,受益匪浅十分感谢.一个疑问,关于热之扩散式11下的DDPM取的$\\sigma\_t$是否应该为$\\sigma\_t=\\sqrt{\\frac{\\bar{\\beta}\_{t-1}}{\\bar{\\beta}\_{t}}}(1-\\alpha\_t^2)$.
[回复评论](https://kexue.fm/archives/9271/comment-page-1?replyTo=26078#respond-post-9271)
[苏剑林](https://kexue.fm)发表于 December 26th, 2024
否。前四篇都有说到这个。[回复评论](https://kexue.fm/archives/9271/comment-page-1?replyTo=26099#respond-post-9271)
NEK 发表于December 27th, 2024
懂了，真的受益匪浅十分感谢！！！[回复评论](https://kexue.fm/archives/9271/comment-page-1?replyTo=26109#respond-post-9271)
[取消回复](https://kexue.fm/archives/9271#respond-post-9271)
你的大名电子邮箱个人网站（选填）1. 可以使用LaTeX代码，点击“预览效果”可查看效果；
2. 可以通过点击评论楼层编号来引用该楼层；3. 网站可能会有点卡，如非确认评论失败，请**不要重复点击提交**。
********************
### 内容速览* [框架回顾](#框架回顾)
* [热之扩散](#热之扩散)
* [冷之扩散](#冷之扩散)
* [编辑模型](#编辑模型)
* [掩码模型](#掩码模型)
* [编码模型](#编码模型)
* [文章小结](#文章小结)
********************
### 智能搜索支持整句搜索！网站自动使用[结巴分词](https://github.com/fxsjy/jieba)进行分词，并结合ngrams排序算法给出合理的搜索结果。
********************
### 热门标签[生成模型](https://kexue.fm/tag/生成模型/)[attention](https://kexue.fm/tag/attention/)[优化](https://kexue.fm/tag/优化/)[语言模型](https://kexue.fm/tag/语言模型/)[模型](https://kexue.fm/tag/模型/)[网站](https://kexue.fm/tag/网站/)[梯度](https://kexue.fm/tag/梯度/)[概率](https://kexue.fm/tag/概率/)[矩阵](https://kexue.fm/tag/矩阵/)[转载](https://kexue.fm/tag/转载/)[优化器](https://kexue.fm/tag/优化器/)[微分方程](https://kexue.fm/tag/微分方程/)[分析](https://kexue.fm/tag/分析/)[天象](https://kexue.fm/tag/天象/)[深度学习](https://kexue.fm/tag/深度学习/)[积分](https://kexue.fm/tag/积分/)[python](https://kexue.fm/tag/python/)[扩散](https://kexue.fm/tag/扩散/)[力学](https://kexue.fm/tag/力学/)[无监督](https://kexue.fm/tag/无监督/)[几何](https://kexue.fm/tag/几何/)[节日](https://kexue.fm/tag/节日/)[生活](https://kexue.fm/tag/生活/)[文本生成](https://kexue.fm/tag/文本生成/)[数论](https://kexue.fm/tag/数论/)
********************
********************
### 随机文章* [【理解黎曼几何】5. 黎曼曲率](https://kexue.fm/archives/4014)
* [自己实现了一个bert4keras](https://kexue.fm/archives/6915)
* [【中文分词系列】 5. 基于语言模型的无监督分词](https://kexue.fm/archives/3956)
* [集训结束了——入选了IOAA](https://kexue.fm/archives/696)
* [混沌的世界——“星之轨迹”的研究](https://kexue.fm/archives/1525)
* [[遐想]细胞的进化是一次次“大吞并”？](https://kexue.fm/archives/1492)
* [哥本哈根气候大会召开情况](https://kexue.fm/archives/300)
* [从费马大定理谈起（六）：n=4（2）](https://kexue.fm/archives/2858)
* [能量视角下的GAN模型（一）：GAN＝“挖坑”＋“跳坑”](https://kexue.fm/archives/6316)
* [国际观月夜（InOMN）](https://kexue.fm/archives/809)
********************
********************
### 最近评论* [ykwen](https://kexue.fm/archives/10739/comment-page-2#comment-28992): 最近想了一个观点，由于每次更新都做了各个不同分量的均衡，所以对于W保持高秩可能会有帮助，这样有...
* [wednesday](https://kexue.fm/archives/5253/comment-page-2#comment-28991): 训练好的var不应该是接近1，log\_var接近0吗，编码器输出不是要逼近标准正态分布吗
* [wednesday](https://kexue.fm/archives/5253/comment-page-18#comment-28990): 这样我们就能达到我们的先验假设：p(Z)是标准正态分布。然后我们就可以放心地从N(0,I)中采...
* [xiaojx](https://kexue.fm/archives/9181/comment-page-5#comment-28989): 还有一个小小的补充问题，在实际训练DDPM时，需要固定随机参数的seed吗？
* [xiaojx](https://kexue.fm/archives/9181/comment-page-5#comment-28988): 苏老师您好！非常感谢您的解读。不过我对您提到的的实验结论1“可能跟直觉相反，生成过程中的$\\...
* [xiaowu](https://kexue.fm/archives/10958/comment-page-3#comment-28987): 苏神的讲解非常详细，设置给出了更好的优化思路* [沈天放](https://kexue.fm/archives/11371/comment-page-1#comment-28986): 实际的硬件是，输入的A和B都是BF16/FP16，硬件是一个MNK矩阵相乘，K个乘积，比如Ho...
* [GGbond](https://kexue.fm/archives/11033/comment-page-3#comment-28983): 苏神，想问下$QK^T$为什么在公式(4)中写为$q^Tk$是因为q，k，v都是列向量吗，那要...
* [沈天放](https://kexue.fm/archives/11371/comment-page-1#comment-28982): Stochastic Rounding一般怎么做的？主要是硬件电路里随机bit从哪里来，有的说...
* [winter](https://kexue.fm/archives/11033/comment-page-2#comment-28981): 苏老师，请问deltanet能学习到full attn的attn pattern吗？
********************
********************
### 友情链接* [Cool Papers](https://papers.cool)
* [数学研发](https://bbs.emath.ac.cn)
* [Seatop](http://www.seatop.com.cn/)
* [Xiaoxia](https://xiaoxia.org/)
* [积分表-网络版](https://kexue.fm/sci/integral/index.html)
* [丝路博傲](http://blog.dvxj.com/)
* [数学之家](http://www.2math.cn/)
* [有趣天文奇观](http://interesting-sky.china-vo.org/)
* [TwistedW](http://www.twistedwg.com/)
* [godweiyang](https://godweiyang.com/)
* [AI柠檬](https://blog.ailemon.net/)
* [王登科-DK博客](https://greatdk.com)
* [ESON](https://blog.eson.org/)
* [枫之羽](https://fzhiy.net/)
* [Mathor's blog](https://wmathor.com/)
* [coding-zuo](https://coding-zuo.github.io/)
* [博科园](https://www.bokeyuan.net/)
* [孔皮皮的博客](https://www.kppkkp.top/)
* [运鹏的博客](https://yunpengtai.top/)
* [jiming.site](https://jiming.site/)
* [OmegaXYZ](https://www.omegaxyz.com/)
* [EAI猩球](https://www.robotech.ink/)
* [文举的博客](https://liwenju0.com/)
* [申请链接](https://kexue.fm/links.html)
********************
[![署名-非商业用途-保持一致](https://kexue.fm/usr/themes/geekg/images/cc.gif)](http://creativecommons.org/licenses/by-nc-nd/2.5/cn/)本站采用创作共用版权协议，要求署名、非商业用途和保持一致。转载本站内容必须也遵循“[署名-非商业用途-保持一致](http://creativecommons.org/licenses/by-nc-nd/2.5/cn/)”的创作共用协议。
©2009-2025 Scientific Spaces. All rights reserved. Theme by[laogui](http://www.laogui.com). Powered by[Typecho](http://typecho.org). 备案号:[粤ICP备09093259号-1/2](https://beian.miit.gov.cn/)。