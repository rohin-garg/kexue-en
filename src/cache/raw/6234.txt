## SEARCH

## MENU

- [打赏](https://kexue.fm/reward.html)
- [公式](https://kexue.fm/latex.html)
- [天象](https://kexue.fm/ac.html)
- [链接](https://kexue.fm/links.html)
- [时光](https://kexue.fm/me.html)
- [博览](https://kexue.fm/science.html)
- [归档](https://kexue.fm/content.html)

## CATEGORIES

- [千奇百怪](https://kexue.fm/category/Everything)
- [天文探索](https://kexue.fm/category/Astronomy)
- [数学研究](https://kexue.fm/category/Mathematics)
- [物理化学](https://kexue.fm/category/Phy-chem)
- [信息时代](https://kexue.fm/category/Big-Data)
- [生物自然](https://kexue.fm/category/Biology)
- [图片摄影](https://kexue.fm/category/Photograph)
- [问题百科](https://kexue.fm/category/Questions)
- [生活/情感](https://kexue.fm/category/Life-Feeling)
- [资源共享](https://kexue.fm/category/Resources)

## NEWPOSTS

- [msign的导数](https://kexue.fm/archives/11025)
- [通过msign来计算mclip（奇...](https://kexue.fm/archives/11006)
- [msign算子的Newton-Sc...](https://kexue.fm/archives/10996)
- [等值振荡定理：最优多项式逼近的充要条件](https://kexue.fm/archives/10972)
- [生成扩散模型漫谈（三十）：从瞬时速...](https://kexue.fm/archives/10958)
- [MoE环游记：5、均匀分布的反思](https://kexue.fm/archives/10945)
- [msign算子的Newton-Sc...](https://kexue.fm/archives/10922)
- [Transformer升级之路：2...](https://kexue.fm/archives/10907)
- [一道概率不等式：盯着它到显然成立为止！](https://kexue.fm/archives/10902)
- [SVD的导数](https://kexue.fm/archives/10878)

## COMMENTS

- [盏一: 苏神, 请教一下\> 并且还可以证明它一定是正交矩阵是怎么证明的...](https://kexue.fm/archives/8397/comment-page-3#comment-27907)
- [sk: 请问公式14是怎么得出来的？](https://kexue.fm/archives/8265/comment-page-8#comment-27906)
- [tll1945tll1937: 真心实意的向大家请教问题：看了文章“对齐全量微调！这是我看过最...](https://kexue.fm/archives/10266/comment-page-1#comment-27901)
- [oYo\_logan: \[comment=27017\]苏剑林\[/comment\]苏神，...](https://kexue.fm/archives/10757/comment-page-1#comment-27897)
- [z123: 在参数矩阵较多的CNN小模型上，Muon会明显慢于Adam，这...](https://kexue.fm/archives/10592/comment-page-1#comment-27896)
- [dry: 苏神好，一直有个疑问，ReFlow构建的ODE是$dx\_t/d...](https://kexue.fm/archives/10958/comment-page-2#comment-27895)
- [tyj: 感觉和之前的一篇文章很像，应该算是concurrent wor...](https://kexue.fm/archives/10958/comment-page-2#comment-27894)
- [li6626: 苏老师，Normalizing Flow有了新进展，论文链接:...](https://kexue.fm/archives/10667/comment-page-1#comment-27893)
- [tesslqy: Evans那本书感觉就挺好的，不过长了一点且有点难（姜萍拿来装...](https://kexue.fm/archives/4718/comment-page-1#comment-27891)
- [Bauree: 渴望大图，万分感谢！](https://kexue.fm/archives/443/comment-page-1#comment-27890)

## USERLOGIN

- [登录](https://kexue.fm/admin/login.php)

[科学空间\|Scientific Spaces](https://kexue.fm)

- [登录](https://kexue.fm/admin/login.php)
- [打赏](https://kexue.fm/reward.html)
- [公式](https://kexue.fm/latex.html)
- [天象](https://kexue.fm/ac.html)
- [链接](https://kexue.fm/links.html)
- [时光](https://kexue.fm/me.html)
- [博览](https://kexue.fm/science.html)
- [归档](https://kexue.fm/content.html)

渴望成为一个小飞侠

- [欢迎订阅](https://kexue.fm/feed)
- [个性邮箱](https://kexue.fm/archives/119)
- [天象信息](https://kexue.fm/ac.html)
- [观测ISS](https://kexue.fm/archives/41)
- [LaTeX](https://kexue.fm/latex.html)
- [关于博主](https://kexue.fm/me.html)

欢迎访问“科学空间”，这里将与您共同探讨自然科学，回味人生百态；也期待大家的分享～

- [**千奇百怪** Everything](https://kexue.fm/category/Everything)
- [**天文探索** Astronomy](https://kexue.fm/category/Astronomy)
- [**数学研究** Mathematics](https://kexue.fm/category/Mathematics)
- [**物理化学** Phy-chem](https://kexue.fm/category/Phy-chem)
- [**信息时代** Big-Data](https://kexue.fm/category/Big-Data)
- [**生物自然** Biology](https://kexue.fm/category/Biology)
- [**图片摄影** Photograph](https://kexue.fm/category/Photograph)
- [**问题百科** Questions](https://kexue.fm/category/Questions)
- [**生活/情感** Life-Feeling](https://kexue.fm/category/Life-Feeling)
- [**资源共享** Resources](https://kexue.fm/category/Resources)

- [**千奇百怪**](https://kexue.fm/category/Everything)
- [**天文探索**](https://kexue.fm/category/Astronomy)
- [**数学研究**](https://kexue.fm/category/Mathematics)
- [**物理化学**](https://kexue.fm/category/Phy-chem)
- [**信息时代**](https://kexue.fm/category/Big-Data)
- [**生物自然**](https://kexue.fm/category/Biology)
- [**图片摄影**](https://kexue.fm/category/Photograph)
- [**问题百科**](https://kexue.fm/category/Questions)
- [**生活/情感**](https://kexue.fm/category/Life-Feeling)
- [**资源共享**](https://kexue.fm/category/Resources)

[首页](https://kexue.fm) [数学研究](https://kexue.fm/category/Mathematics) 从动力学角度看优化算法（二）：自适应学习率算法

20Dec

# [从动力学角度看优化算法（二）：自适应学习率算法](https://kexue.fm/archives/6234)

By 苏剑林 \|
2018-12-20 \|
57704位读者\|

在 [《从动力学角度看优化算法（一）：从SGD到动量加速》](https://kexue.fm/archives/5655) 一文中，我们提出SGD优化算法跟常微分方程（ODE）的数值解法其实是对应的，由此还可以很自然地分析SGD算法的收敛性质、动量加速的原理等等内容。

在这篇文章中，我们继续沿着这个思路，去理解优化算法中的自适应学习率算法。

## RMSprop [\#](https://kexue.fm/archives/6234\#RMSprop)

首先，我们看一个非常经典的自适应学习率优化算法：RMSprop。RMSprop虽然不是最早提出的自适应学习率的优化算法，但是它却是相当实用的一种，它是诸如Adam这样的更综合的算法的基石，通过它我们可以观察自适应学习率的优化算法是怎么做的。

### 算法概览 [\#](https://kexue.fm/archives/6234\#%E7%AE%97%E6%B3%95%E6%A6%82%E8%A7%88)

一般的梯度下降是这样的：
$$\\begin{equation}\\boldsymbol{\\theta}\_{n+1}=\\boldsymbol{\\theta}\_{n} - \\gamma \\nabla\_{\\boldsymbol{\\theta}} L(\\boldsymbol{\\theta}\_{n})\\end{equation}$$
很明显，这里的$\\gamma$是一个超参数，便是学习率，它可能需要在不同阶段做不同的调整。

而RMSprop则是
$$\\begin{equation}\\begin{aligned}\\boldsymbol{g}\_{n+1} =& \\nabla\_{\\boldsymbol{\\theta}} L(\\boldsymbol{\\theta}\_{n})\\\
\\boldsymbol{G}\_{n+1}=&\\lambda \\boldsymbol{G}\_{n} + (1 - \\lambda) \\boldsymbol{g}\_{n+1}\\otimes \\boldsymbol{g}\_{n+1}\\\
\\boldsymbol{\\theta}\_{n+1}=&\\boldsymbol{\\theta}\_{n} - \\frac{\\tilde{\\gamma}}{\\sqrt{\\boldsymbol{G}\_{n+1} + \\epsilon}}\\otimes \\boldsymbol{g}\_{n+1}
\\end{aligned}\\end{equation}$$

### 算法分析 [\#](https://kexue.fm/archives/6234\#%E7%AE%97%E6%B3%95%E5%88%86%E6%9E%90)

对比朴素的SGD，可以发现RMSprop在对$\\boldsymbol{\\theta}$的更新中，将原来是标量的学习率$\\gamma$，换成了一个向量
$$\\begin{equation}\\boldsymbol{\\gamma}=\\frac{\\tilde{\\gamma}}{\\sqrt{\\boldsymbol{G}\_{n+1} + \\epsilon}}\\end{equation}$$
如果把这个向量也看成是学习率，那么RMSprop就是找到了一个方案，能够给参数的每个分量分配不同的学习率。

这个学习率的调节，是通过因子$\\frac{1}{\\sqrt{\\boldsymbol{G}\_{n+1} + \\epsilon}}$来实现的，而$\\boldsymbol{G}\_{n+1}$则是梯度平方的滑动平均。本质上来说，“滑动平均”平均只是让训练过程更加平稳一些，它不是起到调节作用的原因，起作用的主要部分是“梯度”，也就是说，可以用梯度大小来调节学习率。

## 自适应学习率 [\#](https://kexue.fm/archives/6234\#%E8%87%AA%E9%80%82%E5%BA%94%E5%AD%A6%E4%B9%A0%E7%8E%87)

为什么用梯度大小可以来调节学习率呢？其实这个思想非常朴素～

### 极小值点和ODE [\#](https://kexue.fm/archives/6234\#%E6%9E%81%E5%B0%8F%E5%80%BC%E7%82%B9%E5%92%8CODE)

话不多说，简单起见，我们先从一个一维例子出发：假设我们要求$L(\\theta)$的一个极小值点，那么我们引入一个虚拟的时间参数$t$，转化为ODE
$$\\begin{equation}\\frac{d\\theta}{dt}=\\dot{\\theta} = - L'(\\theta)\\end{equation}$$
不难判断，$L(\\theta)$的一个极小值点就是这个方程的稳定的不动点，我们从任意的$\\theta\_0$出发，数值求解这个ODE，可以期望它最终会收敛于这个不动点，从而也就得到了一个极小值点。

最简单的欧拉解法，就是用$\\frac{\\theta\_{t+\\gamma}-\\theta\_t}{\\gamma}$去近似$\\dot{\\theta}$，从而得到
$$\\begin{equation}\\frac{\\theta\_{t+\\gamma}-\\theta\_t}{\\gamma} = - L'(\\theta\_t)\\end{equation}$$
也就是
$$\\begin{equation}\\theta\_{t+\\gamma} = \\theta\_t - \\gamma L'(\\theta\_t)\\end{equation}$$
这就是梯度下降法了，$\\theta\_{t+\\gamma}$相当于$\\theta\_{n+1}$，而$\\theta\_t$相当于$\\theta\_n$，也就是每步前进$\\gamma$那么多。

### 变学习率思想 [\#](https://kexue.fm/archives/6234\#%E5%8F%98%E5%AD%A6%E4%B9%A0%E7%8E%87%E6%80%9D%E6%83%B3)

问题是，$\\gamma$选多少为好呢？当然，从“用$\\frac{\\theta\_{t+\\gamma}-\\theta\_t}{\\gamma}$去近似$\\dot{\\theta}$”这个角度来看，当然是$\\gamma$越小越精确，但是$\\gamma$越小，需要的迭代次数就越多，也就是说计算量就越大，所以越小越好是很理想，但是不现实。

所以，最恰当的方案是：每一步够用就好。可是我们怎么知道够用了没有？

因为我们是用$\\frac{\\theta\_{t+\\gamma}-\\theta\_t}{\\gamma}$去近似$\\dot{\\theta}$的，那么就必须分析近似程度：根据泰勒级数，我们有
$$\\begin{equation}\\theta\_{t+\\gamma} = \\theta\_t + \\gamma \\dot{\\theta}\_t + \\mathcal{O}(\\gamma^2)\\end{equation}$$
在我们这里有$\\dot{\\theta} = - L'(\\theta)$，那么我们有
$$\\begin{equation}\\theta\_{t+\\gamma} = \\theta\_t - \\gamma L'(\\theta\_t) + \\mathcal{O}(\\gamma^2)\\end{equation}$$

可以期望，当$\\gamma$比较小的时候，误差项$\\mathcal{O}(\\gamma^2) < \\gamma \\left\|L'(\\theta\_t)\\right\|$，也就是说，在一定条件下，$\\gamma \\left\|L'(\\theta\_t)\\right\|$本身就是误差项的度量，如果我们将$\\gamma \\left\|L'(\\theta\_t)\\right\|$控制在一定的范围内，那么误差也被控制住了。即
$$\\begin{equation}\\gamma \\left\|L'(\\theta\_t)\\right\|\\leq \\tilde{\\gamma}\\end{equation}$$
其中$\\tilde{\\gamma}$是一个常数，甚至只需要简单地$\\gamma \\left\|L'(\\theta\_t)\\right\|=\\tilde{\\gamma}$（暂时忽略$L'(\\theta\_t)=0$的可能性，先观察整体的核心思想），也就是
$$\\begin{equation}\\gamma = \\frac{\\tilde{\\gamma}}{\\left\|L'(\\theta\_t)\\right\|}\\end{equation}$$
这样我们就通过梯度来调节了学习率。

### 滑动平均处理 [\#](https://kexue.fm/archives/6234\#%E6%BB%91%E5%8A%A8%E5%B9%B3%E5%9D%87%E5%A4%84%E7%90%86)

读者可能会诟病，把$\\gamma = \\tilde{\\gamma} / \\left\|L'(\\theta\_t)\\right\|$代入原来的迭代结果，不就是：
$$\\begin{equation}\\theta\_{t+\\tilde{\\gamma} / \\left\|L'(\\theta\_t)\\right\|} = \\theta\_t - \\tilde{\\gamma}\\cdot\\text{sign}\\big\[L'(\\theta\_t)\\big\]\\end{equation}$$
整个梯度你只用了它的符号信息，这是不是太浪费了？（过于平凡：也就是不管梯度大小如何，每次迭代$\\theta$都只是移动固定的长度。）

注意，从解ODE的角度看，其实这并没有毛病，因为ODE的解是一条轨迹$(t,\\theta(t))$，上面这样处理，虽然$\\theta$变得平凡了，但是$t$却变得不平凡了，也就是相当于$t,\\theta$的地位交换了，因此还是合理的。只不过，如果关心的是优化问题，也就是求$L(\\theta)$的极小值点的话，那么上式确实有点平凡了，因为如果每次迭代$\\theta$都只是移动固定的长度，那就有点像网格搜索了，太低效。

所以，为了改善这种不平凡的情况，又为了保留用梯度调节学习率的特征，我们可以把梯度“滑动平均”一下，结果就是
$$\\begin{equation}\\begin{aligned}G\_{t+\\tilde{\\gamma}}=&\\lambda G\_{t} + (1 - \\lambda) \|L'(\\theta\_t)\|^2\\\
\\gamma =& \\frac{\\tilde{\\gamma}}{\\sqrt{G\_{t+\\tilde{\\gamma}} + \\epsilon}}\\\
\\theta\_{t+\\gamma} =& \\theta\_t - \\gamma L'(\\theta\_t)
\\end{aligned}\\end{equation}$$
这个$\\lambda$是一个接近于1但是小于1的常数，这样的话$G\_t$在一定范围内就比较稳定，同时在一定程度上保留了梯度$L'(\\theta\_t)$本身的特性，所以用它来调节学习率算是一个比较“机智”的做法。为了避免$t+\\tilde{\\gamma},t+\\gamma$引起记号上的不适应，统一用$n,n+1$来表示下标，得到：
$$\\begin{equation}\\begin{aligned}G\_{n+1}=&\\lambda G\_{n} + (1 - \\lambda) \|L'(\\theta\_n)\|^2\\\
\\gamma =& \\frac{\\tilde{\\gamma}}{\\sqrt{G\_{n+1} + \\epsilon}}\\\
\\theta\_{n+1} =& \\theta\_n - \\gamma L'(\\theta\_n)
\\end{aligned}\\end{equation}\\label{eq:rmsprop-1}$$
这就是开头说的RMSprop算法了。

此外，滑动平均还有一个重要的原因，那就是我们要估算的实际上是梯度的平方在全样本中的均值，但我们每次只能算一个batch（所谓的mini-batch梯度下降），这样一来每次算出来的结果实际上是有偏的，而滑动平均在一定程度上能修正这种偏差。

### 高维情形分析 [\#](https://kexue.fm/archives/6234\#%E9%AB%98%E7%BB%B4%E6%83%85%E5%BD%A2%E5%88%86%E6%9E%90)

上面的讨论都是一维的情况，如果是多维情况，那怎么推广呢？

也许读者觉得很简单：把标量换成向量不就行了么？并没有这么简单，因为$\\eqref{eq:rmsprop-1}$推广到高维，至少有两种合理的选择：
$$\\begin{equation}\\begin{aligned}G\_{n+1}=&\\lambda G\_{n} + (1 - \\lambda) \\Vert \\nabla\_{\\boldsymbol{\\theta}}L(\\boldsymbol{\\theta}\_n)\\Vert^2\\\
\\gamma =& \\frac{\\tilde{\\gamma}}{\\sqrt{G\_{n+1} + \\epsilon}}\\\
\\boldsymbol{\\theta}\_{n+1} =& \\boldsymbol{\\theta}\_n - \\gamma \\nabla\_{\\boldsymbol{\\theta}}L(\\boldsymbol{\\theta}\_n)
\\end{aligned}\\end{equation}$$
或
$$\\begin{equation}\\begin{aligned}\\boldsymbol{G}\_{n+1}=&\\lambda \\boldsymbol{G}\_{n} + (1 - \\lambda)\\big(\\nabla\_{\\boldsymbol{\\theta}}L(\\boldsymbol{\\theta}\_n)\\otimes \\nabla\_{\\boldsymbol{\\theta}}L(\\boldsymbol{\\theta}\_n)\\big)\\\
\\boldsymbol{\\gamma} =& \\frac{\\tilde{\\gamma}}{\\sqrt{\\boldsymbol{G}\_{n+1} + \\epsilon}}\\\
\\boldsymbol{\\theta}\_{n+1} =& \\boldsymbol{\\theta}\_n - \\boldsymbol{\\gamma}\\otimes \\nabla\_{\\boldsymbol{\\theta}}L(\\boldsymbol{\\theta}\_n)
\\end{aligned}\\end{equation}\\label{eq:rmsprop-n}$$
前者用梯度的总模长来累积，最终保持了学习率的标量性；后者将梯度的每个分量分别累积，这种情况下调节后的学习率就变成了一个向量，相当于给每个参数都分配不同的学习率。要是从严格理论分析的角度来，其实第一种做法更加严密，但是从实验效果来看，却是第二种更为有效。

我们平时所说的RMSprop算法，都是指后者$\\eqref{eq:rmsprop-n}$。但是有很多喜欢纯SGD炼丹的朋友会诟病这种向量化的学习率实际上改变了梯度的方向，导致梯度不准，最终效果不够好。所以不喜欢向量化学习率的读者，不妨试验一下前者。

## 结论汇总 [\#](https://kexue.fm/archives/6234\#%E7%BB%93%E8%AE%BA%E6%B1%87%E6%80%BB)

本文再次从ODE的角度分析了优化算法，这次是从误差控制的角度给出了一种自适应学习率算法（RMSprop）的理解。至于我们更常用的Adam，则是RMSprop与动量加速的结合，这里就不赘述了。

将优化问题视为一个常微分方程的求解问题，这其实就是将优化问题变成了一个动力学问题，这样可以让我们从比较物理的视角去理解优化算法（哪怕只是直观而不严密的理解），甚至可以把一些ODE的理论结果拿过来用，后面笔者会试图再举一些这样的例子。

_**转载到请包括本文地址：** [https://kexue.fm/archives/6234](https://kexue.fm/archives/6234)_

_**更详细的转载事宜请参考：**_ [《科学空间FAQ》](https://kexue.fm/archives/6508#%E6%96%87%E7%AB%A0%E5%A6%82%E4%BD%95%E8%BD%AC%E8%BD%BD/%E5%BC%95%E7%94%A8)

**如果您还有什么疑惑或建议，欢迎在下方评论区继续讨论。**

**如果您觉得本文还不错，欢迎 [分享](https://kexue.fm/archives/6234#share)/ [打赏](https://kexue.fm/archives/6234#pay) 本文。打赏并非要从中获得收益，而是希望知道科学空间获得了多少读者的真心关注。当然，如果你无视它，也不会影响你的阅读。再次表示欢迎和感谢！**

打赏

微信打赏

支付宝打赏

因为网站后台对打赏并无记录，因此欢迎在打赏时候备注留言。你还可以 [**点击这里**](http://mail.qq.com/cgi-bin/qm_share?t=qm_mailme&email=tN7d1drY3drrx8H0xcWa19vZ) 或在下方评论区留言来告知你的建议或需求。

**如果您需要引用本文，请参考：**

苏剑林. (Dec. 20, 2018). 《从动力学角度看优化算法（二）：自适应学习率算法 》\[Blog post\]. Retrieved from [https://kexue.fm/archives/6234](https://kexue.fm/archives/6234)

@online{kexuefm-6234,
        title={从动力学角度看优化算法（二）：自适应学习率算法},
        author={苏剑林},
        year={2018},
        month={Dec},
        url={\\url{https://kexue.fm/archives/6234}},
}

分类： [数学研究](https://kexue.fm/category/Mathematics)    标签： [微分方程](https://kexue.fm/tag/%E5%BE%AE%E5%88%86%E6%96%B9%E7%A8%8B/), [动力学](https://kexue.fm/tag/%E5%8A%A8%E5%8A%9B%E5%AD%A6/), [数值计算](https://kexue.fm/tag/%E6%95%B0%E5%80%BC%E8%AE%A1%E7%AE%97/), [优化器](https://kexue.fm/tag/%E4%BC%98%E5%8C%96%E5%99%A8/)[11 评论](https://kexue.fm/archives/6234#comments)

< [BiGAN-QP：简单清晰的编码&生成模型](https://kexue.fm/archives/6214) \| [【学习清单】最近比较重要的GAN进展论文](https://kexue.fm/archives/6240) >

### 你也许还对下面的内容感兴趣

- [msign算子的Newton-Schulz迭代（下）](https://kexue.fm/archives/10996)
- [生成扩散模型漫谈（三十）：从瞬时速度到平均速度](https://kexue.fm/archives/10958)
- [msign算子的Newton-Schulz迭代（上）](https://kexue.fm/archives/10922)
- [高阶muP：更简明但更高明的谱条件缩放](https://kexue.fm/archives/10795)
- [初探muP：超参数的跨模型尺度迁移规律](https://kexue.fm/archives/10770)
- [Muon续集：为什么我们选择尝试Muon？](https://kexue.fm/archives/10739)
- [为什么梯度裁剪的默认模长是1？](https://kexue.fm/archives/10657)
- [从谱范数梯度到新式权重衰减的思考](https://kexue.fm/archives/10648)
- [生成扩散模型漫谈（二十八）：分步理解一致性模型](https://kexue.fm/archives/10633)
- [生成扩散模型漫谈（二十七）：将步长作为条件输入](https://kexue.fm/archives/10617)

[发表你的看法](https://kexue.fm/archives/6234#comment_form)

sss

December 27th, 2018

想问一下苏神，如何理解公式(2)和(5)中的$⊗$运算呢（这里是指克罗内克积吗？），有什么可以学习的相关材料呢？

十分感谢！

[回复评论](https://kexue.fm/archives/6234/comment-page-1?replyTo=10450#respond-post-6234)

[苏剑林](https://kexue.fm) 发表于
December 27th, 2018

$(x,y)\\otimes (a,b) = (xa, yb)$
其实就是逐元素对应相乘。

[回复评论](https://kexue.fm/archives/6234/comment-page-1?replyTo=10451#respond-post-6234)

ck

January 8th, 2019

谢谢，写的很棒，深入浅出。

我想问一下，(11) (12) 公式之间的文字说 sign 的算法低效是您自己的理解吗还是实验现象？ 因为我个人感觉(11)公式的形式很像控制里面的滑模控制，http://blog.sina.com.cn/s/blog\_712a88090102xjql.html； 看优缺点和我平时的仿真（控制），它是很快的。

[回复评论](https://kexue.fm/archives/6234/comment-page-1?replyTo=10507#respond-post-6234)

[苏剑林](https://kexue.fm) 发表于
January 9th, 2019

网格搜索太平凡了，肯定会很低效，这还需要验证吗？

[回复评论](https://kexue.fm/archives/6234/comment-page-1?replyTo=10516#respond-post-6234)

漠然

February 20th, 2023

整个梯度你只用了它的符号信息，这是不是太浪费了？（过于平凡：也就是不管梯度大小如何，每次迭代θ
都只是移动固定的长度。）

最近您解析的谷歌团队Lion优化器似乎就是使用固定长度代替了本身梯度长度，然后使用动量来重新加速不同方向的步长。这与除以梯度方差方法相比取得了更好的效果。

不知道我理解的正确吗？

[回复评论](https://kexue.fm/archives/6234/comment-page-1?replyTo=20970#respond-post-6234)

[苏剑林](https://kexue.fm) 发表于
February 20th, 2023

有点偏差。Lion是对动量加速之后的结果直接取$\\text{sign}$函数，所以最后每个更新量的分量确实是等绝对值的。

[回复评论](https://kexue.fm/archives/6234/comment-page-1?replyTo=20978#respond-post-6234)

漠然

February 21st, 2023

[@苏剑林\|comment-20978](https://kexue.fm/archives/6234/comment-page-1#comment-20978)

我可以理解为Lion每次更新的分量方向不同，但是数值相同，即类似四分量情况（1,-1,-1,1）这样吗？如果是这样的，那么Lion如何调节分量之间的尺度不同的？这个也是我对于Lion去掉$v$参数不解的地方。是使用动量的移动平均来指导分量的方向而非步长？

[回复评论](https://kexue.fm/archives/6234/comment-page-1?replyTo=20984#respond-post-6234)

[苏剑林](https://kexue.fm) 发表于
February 24th, 2023

只是单次的绝对值一样，但是正负的频率不一样呀，累加起来绝对值就不一样了。

[回复评论](https://kexue.fm/archives/6234/comment-page-1?replyTo=21004#respond-post-6234)

龙行

November 20th, 2023

苏神，(7)式第三个sita下标写错了

[回复评论](https://kexue.fm/archives/6234/comment-page-1?replyTo=23109#respond-post-6234)

[苏剑林](https://kexue.fm) 发表于
November 20th, 2023

已更正，感谢指出。

[回复评论](https://kexue.fm/archives/6234/comment-page-1?replyTo=23130#respond-post-6234)

龙行

November 21st, 2023

泰勒展开真的好用啊，这里一个泰展就把界说明了，印象还很深的就是Transformer的位置向量为什么加上去也有用，也是一个泰展就把相对位置和绝对位置说清楚了，感谢苏神。看这些内容真的很舒服。

[回复评论](https://kexue.fm/archives/6234/comment-page-1?replyTo=23139#respond-post-6234)

[取消回复](https://kexue.fm/archives/6234#respond-post-6234)

你的大名

电子邮箱

个人网站（选填）

1\. 可以使用LaTeX代码，点击“预览效果”可查看效果；2. 可以通过点击评论楼层编号来引用该楼层；3. 网站可能会有点卡，如非确认评论失败，请 **不要重复点击提交**。

### 内容速览

[RMSprop](https://kexue.fm/archives/6234#RMSprop)
[算法概览](https://kexue.fm/archives/6234#%E7%AE%97%E6%B3%95%E6%A6%82%E8%A7%88)
[算法分析](https://kexue.fm/archives/6234#%E7%AE%97%E6%B3%95%E5%88%86%E6%9E%90)
[自适应学习率](https://kexue.fm/archives/6234#%E8%87%AA%E9%80%82%E5%BA%94%E5%AD%A6%E4%B9%A0%E7%8E%87)
[极小值点和ODE](https://kexue.fm/archives/6234#%E6%9E%81%E5%B0%8F%E5%80%BC%E7%82%B9%E5%92%8CODE)
[变学习率思想](https://kexue.fm/archives/6234#%E5%8F%98%E5%AD%A6%E4%B9%A0%E7%8E%87%E6%80%9D%E6%83%B3)
[滑动平均处理](https://kexue.fm/archives/6234#%E6%BB%91%E5%8A%A8%E5%B9%B3%E5%9D%87%E5%A4%84%E7%90%86)
[高维情形分析](https://kexue.fm/archives/6234#%E9%AB%98%E7%BB%B4%E6%83%85%E5%BD%A2%E5%88%86%E6%9E%90)
[结论汇总](https://kexue.fm/archives/6234#%E7%BB%93%E8%AE%BA%E6%B1%87%E6%80%BB)

### 智能搜索

支持整句搜索！网站自动使用 [结巴分词](https://github.com/fxsjy/jieba) 进行分词，并结合ngrams排序算法给出合理的搜索结果。

### 热门标签

[生成模型](https://kexue.fm/tag/%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B/) [attention](https://kexue.fm/tag/attention/) [优化](https://kexue.fm/tag/%E4%BC%98%E5%8C%96/) [语言模型](https://kexue.fm/tag/%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/) [模型](https://kexue.fm/tag/%E6%A8%A1%E5%9E%8B/) [网站](https://kexue.fm/tag/%E7%BD%91%E7%AB%99/) [概率](https://kexue.fm/tag/%E6%A6%82%E7%8E%87/) [梯度](https://kexue.fm/tag/%E6%A2%AF%E5%BA%A6/) [转载](https://kexue.fm/tag/%E8%BD%AC%E8%BD%BD/) [微分方程](https://kexue.fm/tag/%E5%BE%AE%E5%88%86%E6%96%B9%E7%A8%8B/) [矩阵](https://kexue.fm/tag/%E7%9F%A9%E9%98%B5/) [天象](https://kexue.fm/tag/%E5%A4%A9%E8%B1%A1/) [分析](https://kexue.fm/tag/%E5%88%86%E6%9E%90/) [深度学习](https://kexue.fm/tag/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/) [积分](https://kexue.fm/tag/%E7%A7%AF%E5%88%86/) [python](https://kexue.fm/tag/python/) [优化器](https://kexue.fm/tag/%E4%BC%98%E5%8C%96%E5%99%A8/) [力学](https://kexue.fm/tag/%E5%8A%9B%E5%AD%A6/) [无监督](https://kexue.fm/tag/%E6%97%A0%E7%9B%91%E7%9D%A3/) [扩散](https://kexue.fm/tag/%E6%89%A9%E6%95%A3/) [几何](https://kexue.fm/tag/%E5%87%A0%E4%BD%95/) [节日](https://kexue.fm/tag/%E8%8A%82%E6%97%A5/) [生活](https://kexue.fm/tag/%E7%94%9F%E6%B4%BB/) [文本生成](https://kexue.fm/tag/%E6%96%87%E6%9C%AC%E7%94%9F%E6%88%90/) [数论](https://kexue.fm/tag/%E6%95%B0%E8%AE%BA/)

### 随机文章

- [纠缠的时空（三）：长度收缩和时间延缓](https://kexue.fm/archives/1971)
- [关于中国人获得诺贝尔奖的情况](https://kexue.fm/archives/167)
- [地球“黑暗”的一小时](https://kexue.fm/archives/554)
- [捉弄计划的失败——单摆周期](https://kexue.fm/archives/674)
- [七夕情缘\|寻找牛郎织女](https://kexue.fm/archives/93)
- [SVD的导数](https://kexue.fm/archives/10878)
- [不在家的国庆](https://kexue.fm/archives/2106)
- [科学空间：2010年7月重要天象](https://kexue.fm/archives/704)
- [天文望远镜拍到宇宙最美部分(图)](https://kexue.fm/archives/24)
- [话说金属活动性顺序](https://kexue.fm/archives/89)

### 最近评论

- [盏一](https://kexue.fm/archives/8397/comment-page-3#comment-27907): 苏神, 请教一下\> 并且还可以证明它一定是正交矩阵是怎么证明的. 我本来以为隐式利用了 $\\V...
- [sk](https://kexue.fm/archives/8265/comment-page-8#comment-27906): 请问公式14是怎么得出来的？
- [tll1945tll1937](https://kexue.fm/archives/10266/comment-page-1#comment-27901): 真心实意的向大家请教问题：看了文章“对齐全量微调！这是我看过最精彩的LoRA改进（二）”，我实...
- [oYo\_logan](https://kexue.fm/archives/10757/comment-page-1#comment-27897): \[comment=27017\]苏剑林\[/comment\]苏神，想请教一下，我理解在一个batc...
- [z123](https://kexue.fm/archives/10592/comment-page-1#comment-27896): 在参数矩阵较多的CNN小模型上，Muon会明显慢于Adam，这方面有什么优化提速的方案吗？
- [dry](https://kexue.fm/archives/10958/comment-page-2#comment-27895): 苏神好，一直有个疑问，ReFlow构建的ODE是$dx\_t/dt=x\_1-x\_0$，为什么这并...
- [tyj](https://kexue.fm/archives/10958/comment-page-2#comment-27894): 感觉和之前的一篇文章很像，应该算是concurrent work： https://arxiv...
- [li6626](https://kexue.fm/archives/10667/comment-page-1#comment-27893): 苏老师，Normalizing Flow有了新进展，论文链接:https://arxiv.or...
- [tesslqy](https://kexue.fm/archives/4718/comment-page-1#comment-27891): Evans那本书感觉就挺好的，不过长了一点且有点难（姜萍拿来装的就是这本书，可见流传度之广
- [Bauree](https://kexue.fm/archives/443/comment-page-1#comment-27890): 渴望大图，万分感谢！

### 友情链接

- [Cool Papers](https://papers.cool)
- [数学研发](https://bbs.emath.ac.cn)
- [Seatop](http://www.seatop.com.cn/)
- [Xiaoxia](https://xiaoxia.org/)
- [积分表-网络版](https://kexue.fm/sci/integral/index.html)
- [丝路博傲](http://blog.dvxj.com/)
- [ph4ntasy 饭特稀](http://www.ph4ntasy.com/)
- [数学之家](http://www.2math.cn/)
- [有趣天文奇观](http://interesting-sky.china-vo.org/)
- [TwistedW](http://www.twistedwg.com/)
- [godweiyang](https://godweiyang.com/)
- [AI柠檬](https://blog.ailemon.net/)
- [王登科-DK博客](https://greatdk.com)
- [ESON](https://blog.eson.org/)
- [枫之羽](https://fzhiy.net/)
- [Mathor's blog](https://wmathor.com/)
- [coding-zuo](https://coding-zuo.github.io/)
- [博科园](https://www.bokeyuan.net/)
- [孔皮皮的博客](https://www.kppkkp.top/)
- [运鹏的博客](https://yunpengtai.top/)
- [jiming.site](https://jiming.site/)
- [OmegaXYZ](https://www.omegaxyz.com/)
- [Blog by Eacls](https://www.eacls.top/)
- [EAI猩球](https://www.robotech.ink/)
- [文举的博客](https://liwenju0.com/)
- [用代码打点酱油](https://bruceyuan.com/)
- [申请链接](https://kexue.fm/links.html)

本站采用创作共用版权协议，要求署名、非商业用途和保持一致。转载本站内容必须也遵循“ [署名-非商业用途-保持一致](http://creativecommons.org/licenses/by-nc-nd/2.5/cn/)”的创作共用协议。
© 2009-2025 Scientific Spaces. All rights reserved. Theme by [laogui](http://www.laogui.com). Powered by [Typecho](http://typecho.org). 备案号: [粤ICP备09093259号-1/2](https://beian.miit.gov.cn/)。