Loading \[MathJax\]/jax/output/HTML-CSS/fonts/TeX/fontdata.js

![MobileSideBar](https://kexue.fm/usr/themes/geekg/images/slide-button.png)

## SEARCH

## MENU

- [打赏](https://kexue.fm/reward.html)
- [公式](https://kexue.fm/latex.html)
- [天象](https://kexue.fm/ac.html)
- [链接](https://kexue.fm/links.html)
- [时光](https://kexue.fm/me.html)
- [博览](https://kexue.fm/science.html)
- [归档](https://kexue.fm/content.html)

## CATEGORIES

- [千奇百怪](https://kexue.fm/category/Everything)
- [天文探索](https://kexue.fm/category/Astronomy)
- [数学研究](https://kexue.fm/category/Mathematics)
- [物理化学](https://kexue.fm/category/Phy-chem)
- [信息时代](https://kexue.fm/category/Big-Data)
- [生物自然](https://kexue.fm/category/Biology)
- [图片摄影](https://kexue.fm/category/Photograph)
- [问题百科](https://kexue.fm/category/Questions)
- [生活/情感](https://kexue.fm/category/Life-Feeling)
- [资源共享](https://kexue.fm/category/Resources)

## NEWPOSTS

- [让炼丹更科学一些（五）：基于梯度精...](https://kexue.fm/archives/11530)
- [让炼丹更科学一些（四）：新恒等式，...](https://kexue.fm/archives/11494)
- [为什么DeltaNet要加L2 N...](https://kexue.fm/archives/11486)
- [让炼丹更科学一些（三）：SGD的终...](https://kexue.fm/archives/11480)
- [让炼丹更科学一些（二）：将结论推广...](https://kexue.fm/archives/11469)
- [滑动平均视角下的权重衰减和学习率](https://kexue.fm/archives/11459)
- [生成扩散模型漫谈（三十一）：预测数...](https://kexue.fm/archives/11428)
- [Muon优化器指南：快速上手与关键细节](https://kexue.fm/archives/11416)
- [AdamW的Weight RMS的...](https://kexue.fm/archives/11404)
- [n个正态随机数的最大值的渐近估计](https://kexue.fm/archives/11390)

## COMMENTS

- [Bin: 今天偶然从某个论坛看到有人推荐您的博客，定睛一看竟然是华师同院...](https://kexue.fm/archives/1990/comment-page-2#comment-29105)
- [Rapture D: 我有一个问题，为什么不考虑亥姆霍兹定理和斯托克斯公式。](https://kexue.fm/archives/11530/comment-page-1#comment-29104)
- [mofheka: 苏神是还在用jax是么？最近在做基于Google Pathwa...](https://kexue.fm/archives/11390/comment-page-1#comment-29103)
- [长琴: 看懂这篇博客也不是一件容易的事情。](https://kexue.fm/archives/11530/comment-page-1#comment-29102)
- [AlexLi: 苏老师，请教一下(7)式中将 \\mu(x\_t) 传给 $p...](https://kexue.fm/archives/9257/comment-page-4#comment-29101)
- [tyler\_zxc: "Performer的思想是将标准的Attention线性化，...](https://kexue.fm/archives/7921/comment-page-2#comment-29100)
- [我: 似乎并非mHC提出矩阵的思想？之前hyper connecti...](https://kexue.fm/archives/11494/comment-page-1#comment-29099)
- [winter: 苏神您好，假如对于比较均匀的attention weightP...](https://kexue.fm/archives/10847/comment-page-1#comment-29098)
- [苏剑林: KL散度、JS散度、W距离啥的，都行啊，看你喜欢哪个](https://kexue.fm/archives/8512/comment-page-2#comment-29097)
- [苏剑林: 没有绝对公平的对比方法，主要看你关心什么。比如，如果只关心推理...](https://kexue.fm/archives/9119/comment-page-14#comment-29096)

## USERLOGIN

- [登录](https://kexue.fm/admin/login.php)

[科学空间\|Scientific Spaces](https://kexue.fm/)

- [登录](https://kexue.fm/admin/login.php)
- [打赏](https://kexue.fm/reward.html)
- [公式](https://kexue.fm/latex.html)
- [天象](https://kexue.fm/ac.html)
- [链接](https://kexue.fm/links.html)
- [时光](https://kexue.fm/me.html)
- [博览](https://kexue.fm/science.html)
- [归档](https://kexue.fm/content.html)

渴望成为一个小飞侠

- [![](https://kexue.fm/usr/themes/geekg/images/rss.png)\\
\\
欢迎订阅](https://kexue.fm/feed)
- [![](https://kexue.fm/usr/themes/geekg/images/mail.png)\\
\\
个性邮箱](https://kexue.fm/archives/119)
- [![](https://kexue.fm/usr/themes/geekg/images/Saturn.png)\\
\\
天象信息](https://kexue.fm/ac.html)
- [![](https://kexue.fm/usr/themes/geekg/images/iss.png)\\
\\
观测ISS](https://kexue.fm/archives/41)
- [![](https://kexue.fm/usr/themes/geekg/images/pi.png)\\
\\
LaTeX](https://kexue.fm/latex.html)
- [![](https://kexue.fm/usr/themes/geekg/images/mlogo.png)\\
\\
关于博主](https://kexue.fm/me.html)

欢迎访问“科学空间”，这里将与您共同探讨自然科学，回味人生百态；也期待大家的分享～

- [**千奇百怪** Everything](https://kexue.fm/category/Everything)
- [**天文探索** Astronomy](https://kexue.fm/category/Astronomy)
- [**数学研究** Mathematics](https://kexue.fm/category/Mathematics)
- [**物理化学** Phy-chem](https://kexue.fm/category/Phy-chem)
- [**信息时代** Big-Data](https://kexue.fm/category/Big-Data)
- [**生物自然** Biology](https://kexue.fm/category/Biology)
- [**图片摄影** Photograph](https://kexue.fm/category/Photograph)
- [**问题百科** Questions](https://kexue.fm/category/Questions)
- [**生活/情感** Life-Feeling](https://kexue.fm/category/Life-Feeling)
- [**资源共享** Resources](https://kexue.fm/category/Resources)

- [**千奇百怪**](https://kexue.fm/category/Everything)
- [**天文探索**](https://kexue.fm/category/Astronomy)
- [**数学研究**](https://kexue.fm/category/Mathematics)
- [**物理化学**](https://kexue.fm/category/Phy-chem)
- [**信息时代**](https://kexue.fm/category/Big-Data)
- [**生物自然**](https://kexue.fm/category/Biology)
- [**图片摄影**](https://kexue.fm/category/Photograph)
- [**问题百科**](https://kexue.fm/category/Questions)
- [**生活/情感**](https://kexue.fm/category/Life-Feeling)
- [**资源共享**](https://kexue.fm/category/Resources)

[首页](https://kexue.fm/) [数学研究](https://kexue.fm/category/Mathematics) “熵”不起：从熵、最大熵原理到最大熵模型（二）

11Dec

# [“熵”不起：从熵、最大熵原理到最大熵模型（二）](https://kexue.fm/archives/3552)

By 苏剑林 \|
2015-12-11 \|
113199位读者 \|

## 上集回顾 [\#](https://kexue.fm/archives/3552\#%E4%B8%8A%E9%9B%86%E5%9B%9E%E9%A1%BE)

在第一篇中，笔者介绍了“熵”这个概念，以及它的一些来龙去脉。熵的公式为

S=-\\sum\_x p(x)\\log p(x)\\tag{1}

或

S=-\\int p(x)\\log p(x) dx\\tag{2}

并且在第一篇中，我们知道熵既代表了不确定性，又代表了信息量，事实上它们是同一个概念。

说完了熵这个概念，接下来要说的是“最大熵原理”。最大熵原理告诉我们，当我们想要得到一个随机事件的概率分布时，如果没有足够的信息能够完全确定这个概率分布（可能是不能确定什么分布，也可能是知道分布的类型，但是还有若干个参数没确定），那么最为“保险”的方案是选择使得熵最大的分布。

## 最大熵原理 [\#](https://kexue.fm/archives/3552\#%E6%9C%80%E5%A4%A7%E7%86%B5%E5%8E%9F%E7%90%86)

### 承认我们的无知 [\#](https://kexue.fm/archives/3552\#%E6%89%BF%E8%AE%A4%E6%88%91%E4%BB%AC%E7%9A%84%E6%97%A0%E7%9F%A5)

很多文章在介绍最大熵原理的时候，会引用一句著名的句子——“不要把鸡蛋放在同一个篮子里”——来通俗地解释这个原理。然而，笔者窃以为这句话并没有抓住要点，并不能很好地体现最大熵原理的要义。笔者认为，对最大熵原理更恰当的解释是： **承认我们的无知！**

如何理解这句话跟最大熵原理的联系呢？我们已经知道，熵是不确定性的度量，最大熵，意味着最大的不确定性，那么很显然，我们要选取熵最大的分布，也就意味着我们选择了我们对它最无知的分布。换言之，承认我们对问题的无知，不要自欺欺人，也许我们可以相互欺骗，但是我们永远也欺骗不了大自然。

举例来说，抛一枚硬币，我们一般会认为正反两面的概率是一样的，这时候熵最大，我们对这个硬币处于最无知的状态；假如有一个人，很自信正面向上的概率是60%，反面是40%，那么他肯定是知道了硬币做过手脚，使得正面向上的概率大一些。因此，那个人对硬币的了解程度肯定比我们要高，而我们如果不了解这个信息，那么还是老老实实地承认我们的无知，只好认为正反面概率一样，不要做主观臆测（因为可以主观臆测的东西太多了，如果我们错误地认为反面的概率更高，那么将会使得我们更加血本无归。）

因此，选取熵最大的模型，意味着我们对自然的坦诚，而我们相信我们对自然坦诚了，自然也不会亏待我们。这边是最大熵原理的 **哲学意义** 所在。另外，承认我们的无知，也就意味着我们假设能从中获得最大的信息量，这在某种意义上，也符合最经济的法则。

### 估算概率 [\#](https://kexue.fm/archives/3552\#%E4%BC%B0%E7%AE%97%E6%A6%82%E7%8E%87)

说了那么多，还没有真正动手算些什么，显然单靠文字是很难有说服力的。首先，我们来解决最简单的一个问题，根据最大熵原理说明为什么我们认为抛硬币时，正负两面的概率都是\\frac{1}{2}。假设正面的概率为p，则熵为

S(p) = -p \\log p - (1-p)\\log (1-p)\\tag{15}

求导可以算得，当p=\\frac{1}{2}时，S(p)最大，所以我们猜测每个面的概率都是\\frac{1}{2}。类似地，可以证明，根据最大熵原理得抛骰子的时候，每个面的概率应该是\\frac{1}{6}，这是一个多元微积分的问题；如果有n种可能性，每种可能性的概率的概率应该是\\frac{1}{n}。

[![最大熵原理](https://kexue.fm/usr/uploads/2015/12/943070364.png)](https://kexue.fm/usr/uploads/2015/12/943070364.png "点击查看原图")

最大熵原理

要注意，以上都是假设我们是完全无知的情况下作出的估计。事实上，很多时候我们并没有那么无知，我们可以根据大量的统计，得出一些信息，来完善我们的认识。比如下面的例子：

> 一个快餐店提供3种食品：汉堡(1)、鸡肉(2)、鱼(3)。价格分别是1元、2元、3元。已知人们在这家店的平均消费是1.75元，求顾客购买这3种食品的概率。

这里 **“人们在这家店的平均消费是1.75元”** 就是我们根据大量统计得到的一个结果。如果我们假设我们是完全无知的，那么就得到购买每种食物的概率是\\frac{1}{3}，但是这样平均消费应该是2元，并不符合“人们在这家店的平均消费是1.75元”这个事实。换言之，我们对这个快餐店的购买情况并不至于完全无知，至少我们知道了“人们在这家店的平均消费是1.75元”，这个事实有助于我们更准确地估计概率。

## 估计框架 [\#](https://kexue.fm/archives/3552\#%E4%BC%B0%E8%AE%A1%E6%A1%86%E6%9E%B6)

### 离散型概率 [\#](https://kexue.fm/archives/3552\#%E7%A6%BB%E6%95%A3%E5%9E%8B%E6%A6%82%E7%8E%87)

这些有助于我们更准确估计概率的“事实”，有可能是我们的生活经验，也有可能是某种先验信息，但是不管怎样，它必然是统计出来的结果，数学的描述就是以期望形式给出的约束：

E\[f(x)\]=\\sum\_{x} p(x)f(x) = \\tau\\tag{16}

要注意，虽然现在我们并不至于完全无知，但是我们还是得承认自己是无知的，这种无知，实在已知一些事实的前提下的无知。用数学的语言说，就是现在问题变成了，在k个形如(16)式的约束之下，求(1)式的最大值。对于快餐店的例子，f(x)=x，即

\\sum\_{x=1,2,3}p(x)x=1.75\\tag{17}

求-\\sum\_x p(x)\\log p(x)的最大值。这种题目其实就是微积分中的 **带有约束的极值问题**，方法就是 **拉格朗日乘子法**，即引入参数\\lambda，那么原问题等价于求下式的极值

-\\sum\_x p(x)\\log p(x)-\\lambda\_0\\left(\\sum\_x p(x) -1\\right)-\\lambda\_1\\left(\\sum\_x p(x)x -1.75\\right)\\tag{18}

在(18)式中，我们引入了两个约束，第一个约束是通用的，即所有p(x)之和为1；第二个就是我们所认识到的“事实”。求导解得极值点为p(1) = 0.466,p(2) = 0.318,p(3)=0.216，这时候熵最大。

更一般地，假设有k个约束，那么就要求以下问题的极值：

\\begin{aligned}-\\sum\_x p(x)\\log p(x)&-\\lambda\_0\\left(\\sum\_x p(x) -1\\right)-\\lambda\_1\\left(\\sum\_x p(x)f\_1(x) -\\tau\_1\\right)\\\
&-\\dots-\\lambda\_k\\left(\\sum\_x p(x)f\_k(x) -\\tau\_k\\right)\\end{aligned}\\tag{19}

(19)式容易解得

p(x)=\\frac{1}{Z}\\exp\\left(-\\sum\_{i=1}^k \\lambda\_i f\_i (x)\\right)\\tag{20}

这里的Z是归一化因子，即

Z=\\sum\_x \\exp\\left(-\\sum\_{i=1}^k \\lambda\_i f\_i (x)\\right)\\tag{21}

将(20)式代入

\\sum\_x p(x)f\_i(x) -\\tau\_i=0,\\quad (i=1,2,\\dots,k)\\tag{22}

就可以解出各个未知的\\lambda\_i（遗憾的是，对于一般的f\_i(x)，上式并没有简单解，甚至数值求解都不容易，这使得最大熵相关的模型都比较难使用。）

### 连续型概率 [\#](https://kexue.fm/archives/3552\#%E8%BF%9E%E7%BB%AD%E5%9E%8B%E6%A6%82%E7%8E%87)

要注意，(19),(20),(21),(22)式是基于离散型概率推导的结果，连续型概率的结果类似，如(19)式对应于

\\begin{aligned}-\\int p(x)\\log p(x)dx &-\\lambda\_0\\left(\\int p(x)dx -1\\right)-\\lambda\_1\\left(\\int p(x)f\_1(x)dx -\\tau\_1\\right)\\\
&-\\dots-\\lambda\_k\\left(\\int p(x)f\_k(x)dx -\\tau\_k\\right)\\end{aligned}\\tag{23}

求(23)式的最大值事实上是一个变分问题，但是由于没有出现导数项，因此这个变分问题跟普通的求导没有多大区别，结果仍是

p(x)=\\frac{1}{Z}\\exp\\left(-\\sum\_{i=1}^k \\lambda\_i f\_i (x)\\right)\\tag{24}

而

Z=\\int \\exp\\left(-\\sum\_{i=1}^k \\lambda\_i f\_i (x)\\right)dx\\tag{25}

同样需要将式(24)代入

\\int p(x)f\_i(x)dx -\\tau\_i=0,\\quad (i=1,2,\\dots,k)\\tag{26}

解得各个参数\\lambda\_i。

### 连续型的例子 [\#](https://kexue.fm/archives/3552\#%E8%BF%9E%E7%BB%AD%E5%9E%8B%E7%9A%84%E4%BE%8B%E5%AD%90)

连续型的求解有时候会容易一些。比如假如只有一个约束，f(x)=x，即知道变量的平均值，那么概率分布为指数分布

p(x)=\\frac{1}{Z}\\exp\\left(-\\lambda x\\right)\\tag{27}

归一化因子为

\\int\_0^{\\infty} \\exp\\left(-\\lambda x\\right) dx = \\frac{1}{\\lambda}\\tag{28}

所以概率分布为

p(x)=\\lambda \\exp\\left(-\\lambda x\\right)\\tag{29}

还有一个约束为

\\tau=\\int\_0^{\\infty} \\lambda \\exp\\left(-\\lambda x\\right) x dx =\\frac{1}{\\lambda}\\tag{30}

所以结果为

p(x)=\\frac{1}{\\tau} \\exp\\left(-\\frac{x}{\\tau}\\right)\\tag{31}

还有，如果有两个约束，分别是f\_1 (x)=x,f\_2(x)=x^2，这相当于知道变量的平均值和方差，那么概率分布为 **正态分布**（注意，正态分布又出现了）

p(x)=\\frac{1}{Z}\\exp\\left(-\\lambda\_1 x-\\lambda\_2 x^2\\right)\\tag{32}

归一化因子为

\\int\_{-\\infty}^{\\infty} \\exp\\left(-\\lambda\_1 x-\\lambda\_2 x^2\\right) dx = \\sqrt{\\frac{\\pi}{\\lambda\_2}}\\exp\\left(\\frac{\\lambda\_1^2}{4\\lambda\_2}\\right)\\tag{33}

所以概率分布为

p(x)=\\sqrt{\\frac{\\lambda\_2}{\\pi}}\\exp\\left(-\\frac{\\lambda\_1^2}{4\\lambda\_2}\\right) \\exp\\left(-\\lambda\_1 x-\\lambda\_2 x^2\\right)\\tag{34}

两个约束为

\\begin{aligned}&\\tau\_1=\\int\_{-\\infty}^{\\infty} \\sqrt{\\frac{\\lambda\_2}{\\pi}}\\exp\\left(-\\frac{\\lambda\_1^2}{4\\lambda\_2}\\right) \\exp\\left(-\\lambda\_1 x-\\lambda\_2 x^2\\right) x dx =-\\frac{\\lambda\_1}{2\\lambda\_2}\\\
&\\tau\_2=\\int\_{-\\infty}^{\\infty} \\sqrt{\\frac{\\lambda\_2}{\\pi}}\\exp\\left(-\\frac{\\lambda\_1^2}{4\\lambda\_2}\\right) \\exp\\left(-\\lambda\_1 x-\\lambda\_2 x^2\\right) x^2 dx =\\frac{\\lambda\_1^2+2 \\lambda\_2}{4 \\lambda\_2^2}
\\end{aligned}\\tag{35}

将所解得的结果代入(34)式得到

p(x)=\\sqrt{\\frac{1}{2\\pi(\\tau\_2-\\tau\_1^2)}}\\exp\\left(-\\frac{(x-\\tau\_1)^2}{2(\\tau\_2-\\tau\_1^2)}\\right)\\tag{36}

注意\\tau\_2-\\tau\_1^2正好是方差，因此结果正好是均值为\\tau\_1、方差为\\tau\_2-\\tau\_1^2的正态分布！！这又成为了正态分布的一个来源！

_**转载到请包括本文地址：** [https://kexue.fm/archives/3552](https://kexue.fm/archives/3552 "“熵”不起：从熵、最大熵原理到最大熵模型（二）")_

_**更详细的转载事宜请参考：**_ [《科学空间FAQ》](https://kexue.fm/archives/6508#%E6%96%87%E7%AB%A0%E5%A6%82%E4%BD%95%E8%BD%AC%E8%BD%BD/%E5%BC%95%E7%94%A8 "《科学空间FAQ》")

**如果您还有什么疑惑或建议，欢迎在下方评论区继续讨论。**

**如果您觉得本文还不错，欢迎 [分享](https://kexue.fm/archives/3552#share)/ [打赏](https://kexue.fm/archives/3552#pay) 本文。打赏并非要从中获得收益，而是希望知道科学空间获得了多少读者的真心关注。当然，如果你无视它，也不会影响你的阅读。再次表示欢迎和感谢！**

打赏

![科学空间](https://kexue.fm/usr/themes/geekg/payment/wx.png)

微信打赏

![科学空间](https://kexue.fm/usr/themes/geekg/payment/zfb.png)

支付宝打赏

因为网站后台对打赏并无记录，因此欢迎在打赏时候备注留言。

你还可以 [**点击这里**](http://mail.qq.com/cgi-bin/qm_share?t=qm_mailme&email=tN7d1drY3drrx8H0xcWa19vZ) 或在下方评论区留言来告知你的建议或需求。

**如果您需要引用本文，请参考：**

苏剑林. (Dec. 11, 2015). 《“熵”不起：从熵、最大熵原理到最大熵模型（二） 》\[Blog post\]. Retrieved from [https://kexue.fm/archives/3552](https://kexue.fm/archives/3552)

@online{kexuefm-3552,

         title={“熵”不起：从熵、最大熵原理到最大熵模型（二）},

         author={苏剑林},

         year={2015},

         month={Dec},

         url={\\url{https://kexue.fm/archives/3552}},

}


分类： [数学研究](https://kexue.fm/category/Mathematics)    标签： [概率](https://kexue.fm/tag/%E6%A6%82%E7%8E%87/), [极值](https://kexue.fm/tag/%E6%9E%81%E5%80%BC/), [最大熵](https://kexue.fm/tag/%E6%9C%80%E5%A4%A7%E7%86%B5/), [熵](https://kexue.fm/tag/%E7%86%B5/)[19 评论](https://kexue.fm/archives/3552#comments)

< [人生苦短，我用Python！](https://kexue.fm/archives/3546 "人生苦短，我用Python！") \| [“熵”不起：从熵、最大熵原理到最大熵模型（三）](https://kexue.fm/archives/3567 "“熵”不起：从熵、最大熵原理到最大熵模型（三）") >

### 你也许还对下面的内容感兴趣

- [n个正态随机数的最大值的渐近估计](https://kexue.fm/archives/11390 "n个正态随机数的最大值的渐近估计")
- [一道概率不等式：盯着它到显然成立为止！](https://kexue.fm/archives/10902 "一道概率不等式：盯着它到显然成立为止！")
- [矩阵的有效秩（Effective Rank）](https://kexue.fm/archives/10847 "矩阵的有效秩（Effective Rank）")
- [Softmax后传：寻找Top-K的光滑近似](https://kexue.fm/archives/10373 "Softmax后传：寻找Top-K的光滑近似")
- [通向最优分布之路：概率空间的最小化](https://kexue.fm/archives/10289 "通向最优分布之路：概率空间的最小化")
- [通向概率分布之路：盘点Softmax及其替代品](https://kexue.fm/archives/10145 "通向概率分布之路：盘点Softmax及其替代品")
- [用傅里叶级数拟合一维概率密度函数](https://kexue.fm/archives/10007 "用傅里叶级数拟合一维概率密度函数")
- [注意力机制真的可以“集中注意力”吗？](https://kexue.fm/archives/9889 "注意力机制真的可以“集中注意力”吗？")
- [随机分词再探：从Viterbi Sampling到完美采样算法](https://kexue.fm/archives/9811 "随机分词再探：从Viterbi Sampling到完美采样算法")
- [EMO：基于最优传输思想设计的分类损失函数](https://kexue.fm/archives/9797 "EMO：基于最优传输思想设计的分类损失函数")

[发表你的看法](https://kexue.fm/archives/3552#comment_form)

1. [«](https://kexue.fm/archives/3552/comment-page-1#comments)
2. [1](https://kexue.fm/archives/3552/comment-page-1#comments)
3. [2](https://kexue.fm/archives/3552/comment-page-2#comments)

Nn

December 26th, 2023

归一化因子的积分域是怎么从约束中提现的，为什么第一个是0到无穷第二个是实数域

[回复评论](https://kexue.fm/archives/3552/comment-page-2?replyTo=23366#respond-post-3552)

[苏剑林](https://kexue.fm/) 发表于
December 30th, 2023

本质上来说，这应该是问题自己给定的，比如只给定均值，如果区域是整个实数，那么就不存在最大熵分布，给定一个有限区间或者\[0,\\infty)，才有对应的最大熵分布，以及给定的区间不同，最大熵分布的结果也略有不同。\
\
[回复评论](https://kexue.fm/archives/3552/comment-page-2?replyTo=23405#respond-post-3552)\
\
1. [«](https://kexue.fm/archives/3552/comment-page-1#comments)\
2. [1](https://kexue.fm/archives/3552/comment-page-1#comments)\
3. [2](https://kexue.fm/archives/3552/comment-page-2#comments)\
\
[取消回复](https://kexue.fm/archives/3552#respond-post-3552)\
\
你的大名\
\
电子邮箱\
\
个人网站（选填）\
\
1\. 可以使用LaTeX代码，点击“预览效果”可查看效果；\
\
2\. 可以通过点击评论楼层编号来引用该楼层；\
\
3\. 网站可能会有点卡，如非确认评论失败，请 **不要重复点击提交**。\
\
### 内容速览\
\
[上集回顾](https://kexue.fm/archives/3552#%E4%B8%8A%E9%9B%86%E5%9B%9E%E9%A1%BE)\
[最大熵原理](https://kexue.fm/archives/3552#%E6%9C%80%E5%A4%A7%E7%86%B5%E5%8E%9F%E7%90%86)\
[承认我们的无知](https://kexue.fm/archives/3552#%E6%89%BF%E8%AE%A4%E6%88%91%E4%BB%AC%E7%9A%84%E6%97%A0%E7%9F%A5)\
[估算概率](https://kexue.fm/archives/3552#%E4%BC%B0%E7%AE%97%E6%A6%82%E7%8E%87)\
[估计框架](https://kexue.fm/archives/3552#%E4%BC%B0%E8%AE%A1%E6%A1%86%E6%9E%B6)\
[离散型概率](https://kexue.fm/archives/3552#%E7%A6%BB%E6%95%A3%E5%9E%8B%E6%A6%82%E7%8E%87)\
[连续型概率](https://kexue.fm/archives/3552#%E8%BF%9E%E7%BB%AD%E5%9E%8B%E6%A6%82%E7%8E%87)\
[连续型的例子](https://kexue.fm/archives/3552#%E8%BF%9E%E7%BB%AD%E5%9E%8B%E7%9A%84%E4%BE%8B%E5%AD%90)\
\
### 智能搜索\
\
支持整句搜索！网站自动使用 [结巴分词](https://github.com/fxsjy/jieba) 进行分词，并结合ngrams排序算法给出合理的搜索结果。\
\
### 热门标签\
\
[生成模型](https://kexue.fm/tag/%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B/) [attention](https://kexue.fm/tag/attention/) [优化](https://kexue.fm/tag/%E4%BC%98%E5%8C%96/) [语言模型](https://kexue.fm/tag/%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/) [模型](https://kexue.fm/tag/%E6%A8%A1%E5%9E%8B/) [梯度](https://kexue.fm/tag/%E6%A2%AF%E5%BA%A6/) [网站](https://kexue.fm/tag/%E7%BD%91%E7%AB%99/) [概率](https://kexue.fm/tag/%E6%A6%82%E7%8E%87/) [矩阵](https://kexue.fm/tag/%E7%9F%A9%E9%98%B5/) [优化器](https://kexue.fm/tag/%E4%BC%98%E5%8C%96%E5%99%A8/) [转载](https://kexue.fm/tag/%E8%BD%AC%E8%BD%BD/) [微分方程](https://kexue.fm/tag/%E5%BE%AE%E5%88%86%E6%96%B9%E7%A8%8B/) [分析](https://kexue.fm/tag/%E5%88%86%E6%9E%90/) [天象](https://kexue.fm/tag/%E5%A4%A9%E8%B1%A1/) [深度学习](https://kexue.fm/tag/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/) [积分](https://kexue.fm/tag/%E7%A7%AF%E5%88%86/) [python](https://kexue.fm/tag/python/) [扩散](https://kexue.fm/tag/%E6%89%A9%E6%95%A3/) [力学](https://kexue.fm/tag/%E5%8A%9B%E5%AD%A6/) [无监督](https://kexue.fm/tag/%E6%97%A0%E7%9B%91%E7%9D%A3/) [几何](https://kexue.fm/tag/%E5%87%A0%E4%BD%95/) [节日](https://kexue.fm/tag/%E8%8A%82%E6%97%A5/) [生活](https://kexue.fm/tag/%E7%94%9F%E6%B4%BB/) [文本生成](https://kexue.fm/tag/%E6%96%87%E6%9C%AC%E7%94%9F%E6%88%90/) [数论](https://kexue.fm/tag/%E6%95%B0%E8%AE%BA/)\
\
### 随机文章\
\
- [ON-LSTM：用有序神经元表达层次结构](https://kexue.fm/archives/6621)\
- [从JL引理看熵不变性Attention](https://kexue.fm/archives/9588)\
- [寒假结束，今天上学了](https://kexue.fm/archives/470)\
- [bert4keras在手，baseline我有：CLUE基准代码](https://kexue.fm/archives/8739)\
- [今天你食了吗？（广东云浮观测日偏食之旅）](https://kexue.fm/archives/30)\
- [\[更新\]将向量乘法“退化”到复数](https://kexue.fm/archives/1188)\
- [天文望远镜拍到宇宙最美部分(图)](https://kexue.fm/archives/24)\
- [一道级数求和证明题(非数学归纳法)](https://kexue.fm/archives/49)\
- [msign算子的Newton-Schulz迭代（下）](https://kexue.fm/archives/10996)\
- [OCR技术浅探：1. 全文简述](https://kexue.fm/archives/3774)\
\
### 最近评论\
\
- [Bin](https://kexue.fm/archives/1990/comment-page-2#comment-29105): 今天偶然从某个论坛看到有人推荐您的博客，定睛一看竟然是华师同院的往届师兄！看到这篇2013年的...\
- [Rapture D](https://kexue.fm/archives/11530/comment-page-1#comment-29104): 我有一个问题，为什么不考虑亥姆霍兹定理和斯托克斯公式。\
- [mofheka](https://kexue.fm/archives/11390/comment-page-1#comment-29103): 苏神是还在用jax是么？最近在做基于Google Pathway的理念做一个动态版的MPMD框...\
- [长琴](https://kexue.fm/archives/11530/comment-page-1#comment-29102): 看懂这篇博客也不是一件容易的事情。\
- [AlexLi](https://kexue.fm/archives/9257/comment-page-4#comment-29101): 苏老师，请教一下(7)式中将 \\mu(x\_t) 传给 p\_o 进行推理的操作。 $x\_...\
- [tyler\_zxc](https://kexue.fm/archives/7921/comment-page-2#comment-29100): "Performer的思想是将标准的Attention线性化，所以为什么不干脆直接训练一个线性...\
- [我](https://kexue.fm/archives/11494/comment-page-1#comment-29099): 似乎并非mHC提出矩阵的思想？之前hyper connection就是了\
- [winter](https://kexue.fm/archives/10847/comment-page-1#comment-29098): 苏神您好，假如对于比较均匀的attention weightP，往往呈现long tail分布...\
- [苏剑林](https://kexue.fm/archives/8512/comment-page-2#comment-29097): KL散度、JS散度、W距离啥的，都行啊，看你喜欢哪个\
- [苏剑林](https://kexue.fm/archives/9119/comment-page-14#comment-29096): 没有绝对公平的对比方法，主要看你关心什么。比如，如果只关心推理成本和推理效果，那么有的方法可以...\
\
### 友情链接\
\
- [Cool Papers](https://papers.cool/)\
- [数学研发](https://bbs.emath.ac.cn/)\
- [Seatop](http://www.seatop.com.cn/)\
- [Xiaoxia](https://xiaoxia.org/)\
- [积分表-网络版](https://kexue.fm/sci/integral/index.html)\
- [丝路博傲](http://blog.dvxj.com/)\
- [数学之家](http://www.2math.cn/)\
- [有趣天文奇观](http://interesting-sky.china-vo.org/)\
- [TwistedW](http://www.twistedwg.com/)\
- [godweiyang](https://godweiyang.com/)\
- [AI柠檬](https://blog.ailemon.net/)\
- [王登科-DK博客](https://greatdk.com/)\
- [ESON](https://blog.eson.org/)\
- [枫之羽](https://fzhiy.net/)\
- [coding-zuo](https://coding-zuo.github.io/)\
- [博科园](https://www.bokeyuan.net/)\
- [孔皮皮的博客](https://www.kppkkp.top/)\
- [运鹏的博客](https://yunpengtai.top/)\
- [jiming.site](https://jiming.site/)\
- [OmegaXYZ](https://www.omegaxyz.com/)\
- [EAI猩球](https://www.robotech.ink/)\
- [文举的博客](https://liwenju0.com/)\
- [申请链接](https://kexue.fm/links.html)\
\
[![署名-非商业用途-保持一致](https://kexue.fm/usr/themes/geekg/images/cc.gif)](http://creativecommons.org/licenses/by-nc-nd/2.5/cn/) 本站采用创作共用版权协议，要求署名、非商业用途和保持一致。转载本站内容必须也遵循“ [署名-非商业用途-保持一致](http://creativecommons.org/licenses/by-nc-nd/2.5/cn/)”的创作共用协议。\
\
\
\
© 2009-2026 Scientific Spaces. All rights reserved. Theme by [laogui](http://www.laogui.com/). Powered by [Typecho](http://typecho.org/). 备案号: [粤ICP备09093259号-1/2](https://beian.miit.gov.cn/ "粤ICP备09093259号")。