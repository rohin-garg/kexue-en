## SEARCH

## MENU

- [打赏](https://kexue.fm/reward.html)
- [公式](https://kexue.fm/latex.html)
- [天象](https://kexue.fm/ac.html)
- [链接](https://kexue.fm/links.html)
- [时光](https://kexue.fm/me.html)
- [博览](https://kexue.fm/science.html)
- [归档](https://kexue.fm/content.html)

## CATEGORIES

- [千奇百怪](https://kexue.fm/category/Everything)
- [天文探索](https://kexue.fm/category/Astronomy)
- [数学研究](https://kexue.fm/category/Mathematics)
- [物理化学](https://kexue.fm/category/Phy-chem)
- [信息时代](https://kexue.fm/category/Big-Data)
- [生物自然](https://kexue.fm/category/Biology)
- [图片摄影](https://kexue.fm/category/Photograph)
- [问题百科](https://kexue.fm/category/Questions)
- [生活/情感](https://kexue.fm/category/Life-Feeling)
- [资源共享](https://kexue.fm/category/Resources)

## NEWPOSTS

- [重新思考学习率与Batch Siz...](https://kexue.fm/archives/11301)
- [重新思考学习率与Batch Siz...](https://kexue.fm/archives/11285)
- [重新思考学习率与Batch Siz...](https://kexue.fm/archives/11280)
- [为什么Adam的Update RM...](https://kexue.fm/archives/11267)
- [重新思考学习率与Batch Siz...](https://kexue.fm/archives/11260)
- [Cool Papers更新：简单适...](https://kexue.fm/archives/11250)
- [流形上的最速下降：4\. Muon ...](https://kexue.fm/archives/11241)
- [ReLU/GeLU/Swish的一...](https://kexue.fm/archives/11233)
- [流形上的最速下降：3\. Muon ...](https://kexue.fm/archives/11221)
- [流形上的最速下降：2\. Muon ...](https://kexue.fm/archives/11215)

## COMMENTS

- [mp4网: 申请友链\
站名：mp4网\
域名：http://mp4wang....](https://kexue.fm/links.html/comment-page-6#comment-28589)
- [FrankCai: 老师您好， 如果结论是【当ϵ越大，结果越接近SGD，“Surg...](https://kexue.fm/archives/10563/comment-page-1#comment-28588)
- [川zi: 看懂了， 谢谢大佬](https://kexue.fm/archives/9009/comment-page-3#comment-28587)
- [pb: 苏神您好，这两天读到“远程衰减”的部分时觉得有些奇怪，还请指正...](https://kexue.fm/archives/8265/comment-page-8#comment-28586)
- [苏剑林: 这就是diffusion模型的神奇之处，它有点直观，但又不完全...](https://kexue.fm/archives/9119/comment-page-13#comment-28585)
- [苏剑林: 平方或者线性复杂度，都是指关于序列长度$n$的总复杂度，$d$不算。](https://kexue.fm/archives/11033/comment-page-2#comment-28584)
- [苏剑林: cool papers的prompt其实没啥特殊的，关键的技巧...](https://kexue.fm/archives/9907/comment-page-4#comment-28583)
- [苏剑林: 已发](https://kexue.fm/archives/443/comment-page-1#comment-28582)
- [苏剑林: Muon不是“可以”用更大的学习率，它用更大的学习率是因为ms...](https://kexue.fm/archives/10592/comment-page-2#comment-28581)
- [苏剑林: 逻辑是：训练一个模型，约等于把训练集压缩到模型权重中，模型权重...](https://kexue.fm/archives/11033/comment-page-2#comment-28580)

## USERLOGIN

- [登录](https://kexue.fm/admin/login.php)

[科学空间\|Scientific Spaces](https://kexue.fm)

- [登录](https://kexue.fm/admin/login.php)
- [打赏](https://kexue.fm/reward.html)
- [公式](https://kexue.fm/latex.html)
- [天象](https://kexue.fm/ac.html)
- [链接](https://kexue.fm/links.html)
- [时光](https://kexue.fm/me.html)
- [博览](https://kexue.fm/science.html)
- [归档](https://kexue.fm/content.html)

渴望成为一个小飞侠

- [欢迎订阅](https://kexue.fm/feed)
- [个性邮箱](https://kexue.fm/archives/119)
- [天象信息](https://kexue.fm/ac.html)
- [观测ISS](https://kexue.fm/archives/41)
- [LaTeX](https://kexue.fm/latex.html)
- [关于博主](https://kexue.fm/me.html)

欢迎访问“科学空间”，这里将与您共同探讨自然科学，回味人生百态；也期待大家的分享～

- [**千奇百怪** Everything](https://kexue.fm/category/Everything)
- [**天文探索** Astronomy](https://kexue.fm/category/Astronomy)
- [**数学研究** Mathematics](https://kexue.fm/category/Mathematics)
- [**物理化学** Phy-chem](https://kexue.fm/category/Phy-chem)
- [**信息时代** Big-Data](https://kexue.fm/category/Big-Data)
- [**生物自然** Biology](https://kexue.fm/category/Biology)
- [**图片摄影** Photograph](https://kexue.fm/category/Photograph)
- [**问题百科** Questions](https://kexue.fm/category/Questions)
- [**生活/情感** Life-Feeling](https://kexue.fm/category/Life-Feeling)
- [**资源共享** Resources](https://kexue.fm/category/Resources)

- [**千奇百怪**](https://kexue.fm/category/Everything)
- [**天文探索**](https://kexue.fm/category/Astronomy)
- [**数学研究**](https://kexue.fm/category/Mathematics)
- [**物理化学**](https://kexue.fm/category/Phy-chem)
- [**信息时代**](https://kexue.fm/category/Big-Data)
- [**生物自然**](https://kexue.fm/category/Biology)
- [**图片摄影**](https://kexue.fm/category/Photograph)
- [**问题百科**](https://kexue.fm/category/Questions)
- [**生活/情感**](https://kexue.fm/category/Life-Feeling)
- [**资源共享**](https://kexue.fm/category/Resources)

[首页](https://kexue.fm) [信息时代](https://kexue.fm/category/Big-Data) 更别致的词向量模型(四)：模型的求解

19Nov

# [更别致的词向量模型(四)：模型的求解](https://kexue.fm/archives/4675)

By 苏剑林 \|
2017-11-19 \|
60821位读者\|

## 损失函数 [\#](https://kexue.fm/kexue.fm\#%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0)

现在，我们来定义loss，以便把各个词向量求解出来。用$\\tilde{P}$表示$P$的频率估计值，那么我们可以直接以下式为loss
\\\[\\sum\_{w\_i,w\_j}\\left(\\langle \\boldsymbol{v}\_i, \\boldsymbol{v}\_j\\rangle-\\log\\frac{\\tilde{P}(w\_i,w\_j)}{\\tilde{P}(w\_i)\\tilde{P}(w\_j)}\\right)^2\\tag{16}\\\]
相比之下，无论在参数量还是模型形式上，这个做法都比glove要简单，因此称之为simpler glove。glove模型是
\\\[\\sum\_{w\_i,w\_j}\\left(\\langle \\boldsymbol{v}\_i, \\boldsymbol{\\hat{v}}\_j\\rangle+b\_i+\\hat{b}\_j-\\log X\_{ij}\\right)^2\\tag{17}\\\]
在glove模型中，对中心词向量和上下文向量做了区分，然后最后模型建议输出的是两套词向量的求和，据说这效果会更好，这是一个比较勉强的trick，但也不是什么毛病。最大的问题是参数$b\_i,\\hat{b}\_j$也是可训练的，这使得模型是严重不适定的！我们有
\\\[\\begin{aligned}&\\sum\_{w\_i,w\_j}\\left(\\langle \\boldsymbol{v}\_i, \\boldsymbol{\\hat{v}}\_j\\rangle+b\_i+\\hat{b}\_j-\\log \\tilde{P}(w\_i,w\_j)\\right)^2\\\
=&\\sum\_{w\_i,w\_j}\\left\[\\langle \\boldsymbol{v}\_i+\\boldsymbol{c}, \\boldsymbol{\\hat{v}}\_j+\\boldsymbol{c}\\rangle+\\Big(b\_i-\\langle \\boldsymbol{v}\_i, \\boldsymbol{c}\\rangle - \\frac{\|\\boldsymbol{c}\|^2}{2}\\Big)\\right.\\\
&\\qquad\\qquad\\qquad\\qquad\\left.+\\Big(\\hat{b}\_j-\\langle \\boldsymbol{\\hat{v}}\_j, \\boldsymbol{c}\\rangle - \\frac{\|\\boldsymbol{c}\|^2}{2}\\Big)-\\log X\_{ij}\\right\]^2\\end{aligned}\\tag{18}\\\]
这就是说，如果你有了一组解，那么你将所有词向量加上任意一个常数向量后，它还是一组解！这个问题就严重了，我们无法预估得到的是哪组解，一旦加上的是一个非常大的常向量，那么各种度量都没意义了（比如任意两个词的cos值都接近1）。事实上，对glove生成的词向量进行验算就可以发现，glove生成的词向量，停用词的模长远大于一般词的模长，也就是说一堆词放在一起时，停用词的作用还明显些，这显然是不利用后续模型的优化的。（虽然从目前的关于glove的实验结果来看，是我强迫症了一些。）

## 互信息估算 [\#](https://kexue.fm/kexue.fm\#%E4%BA%92%E4%BF%A1%E6%81%AF%E4%BC%B0%E7%AE%97)

为了求解模型，首先要解决的第一个问题就是$P(w\_i,w\_j),P(w\_i),P(w\_j)$该怎么算呢？$P(w\_i),P(w\_j)$简单，直接统计估计就行了，但$P(w\_i,w\_j)$呢？怎样的两个词才算是共现了？当然，事实上不同的用途可以有不同的方案，比如我们可以认为同出现在一篇文章的两个词就是碰过一次面了，这种方案通常会对主题分类很有帮助，不过这种方案计算量太大。更常用的方案是选定一个固定的整数，记为window，每个词前后的window个词，都认为是跟这个词碰过面的。

一个值得留意的细节是：中心词与自身的共现要不要算进去？窗口的定义应该是跟中心词距离不超过window的词，那么应该要把它算上的，但如果算上，那没什么预测意义，因为这一项总是存在，如果不算上，那么会降低了词与自身的互信息。所以我们采用了一个小trick：不算入相同的共现项，让模型自己把这个学出来。也就是说，哪怕上下文（除中心词外）也出现了中心词，也不算进loss中，因为数据量本身是远远大于参数量的，所以这一项总可以学习出来。

## 权重和降采样 [\#](https://kexue.fm/kexue.fm\#%E6%9D%83%E9%87%8D%E5%92%8C%E9%99%8D%E9%87%87%E6%A0%B7)

glove模型定义了如下的权重公式：
\\\[\\lambda\_{ij}=\\Big(\\min\\{x\_{ij}/x\_{max}, 1\\}\\Big)^{\\alpha}\\tag{19}\\\]
其中$x\_{ij}$代表词对$(w\_i,w\_j)$的共现频数，$x\_{max},\\alpha$是固定的常数，通常取$x\_{max}=100,\\alpha=3/4$，也就是说，要对共现频数低的词对降权，它们更有可能是噪音，所以最后Golve的loss是
\\\[\\sum\_{w\_i,w\_j}\\lambda\_{ij}\\left(\\langle \\boldsymbol{v}\_i, \\boldsymbol{v}\_j\\rangle+b\_i+b\_j-\\log \\tilde{P}(w\_i,w\_j)\\right)^2\\tag{20}\\\]

在文本的模型中，继续沿用这一权重，但有所选择。首先，对频数作$\\alpha$次幂，相当于提高了低频项的权重，这跟word2vec的做法基本一致。值得思考的是$\\min$这个截断操作，如果进行这个截断，那么相当于大大降低了高频词的权重，有点像word2vec中的对高频词进行降采样，能够提升低频词的学习效果，但可能带来的后果是：高频词的模长没学好。我们可以在《模长的含义》这一小节中看到这一点。总的来说，不同的场景有不同的需求，因此我们在最后发布的源码中，允许用户自定义是否截断这个权重。

## Adagrad [\#](https://kexue.fm/kexue.fm\#Adagrad)

跟glove一样，我们同样使用Adagrad算法进行优化，使用Adagrad的原因是因为它大概是目前最简单的自适应学习率的算法。

**但是，我发现glove源码中的Adagrad算法写法是错的！！我不知道glove那样写是刻意的改进，还是笔误（感觉也不大可能笔误吧？），总之，如果我毫不改动它的迭代过程，照搬到本文的simpler glove模型中，很容易就出现各种无解的nan！如果写成标准的Adagrad，nan就不会出现了。**

选定一个词对$w\_i,w\_j$我们得到loss
\\\[L=\\lambda\_{ij}\\left(\\langle \\boldsymbol{v}\_i, \\boldsymbol{v}\_j\\rangle-\\log\\frac{\\tilde{P}(w\_i,w\_j)}{\\tilde{P}(w\_i)\\tilde{P}(w\_j)}\\right)^2\\tag{21}\\\]
它的梯度是
\\\[\\begin{aligned}\\nabla\_{\\boldsymbol{v}\_i} L=\\lambda\_{ij}\\left(\\langle \\boldsymbol{v}\_i, \\boldsymbol{v}\_j\\rangle-\\log\\frac{\\tilde{P}(w\_i,w\_j)}{\\tilde{P}(w\_i)\\tilde{P}(w\_j)}\\right)\\boldsymbol{v}\_j\\\
\\nabla\_{\\boldsymbol{v}\_j} L=\\lambda\_{ij}\\left(\\langle \\boldsymbol{v}\_i, \\boldsymbol{v}\_j\\rangle-\\log\\frac{\\tilde{P}(w\_i,w\_j)}{\\tilde{P}(w\_i)\\tilde{P}(w\_j)}\\right)\\boldsymbol{v}\_i
\\end{aligned}\\tag{22}\\\]
然后根据Adagrad算法的公式进行更新即可，默认的初始学习率选为$\\eta=0.1$，迭代公式为
\\\[\\left\\{\\begin{aligned}\\boldsymbol{g}\_{\\gamma}^{(n)} =& \\nabla\_{\\boldsymbol{v}\_{\\gamma}^{(n)}} L\\\
\\boldsymbol{G}\_{\\gamma}^{(n)} =& \\boldsymbol{G}\_{\\gamma}^{(n-1)} + \\boldsymbol{g}\_{\\gamma}^{(n)}\\otimes \\boldsymbol{g}\_{\\gamma}^{(n)}\\\
\\boldsymbol{v}\_{\\gamma}^{(n)} =& \\boldsymbol{v}\_{\\gamma}^{(n-1)} - \\frac{\\boldsymbol{g}\_{\\gamma}^{(n)}}{\\sqrt{\\boldsymbol{G}\_{\\gamma}^{(n-1)}}}\\eta
\\end{aligned}\\right.,\\,\\gamma=i,j
\\tag{23}\\\]
根据公式可以看出，Adagrad算法基本上是对loss的缩放不敏感的，换句话说，将loss乘上10倍，最终的优化效果基本没什么变化，但如果在随机梯度下降中，将loss乘上10倍，就等价于将学习率乘以10了。

_**转载到请包括本文地址：** [https://kexue.fm/archives/4675](https://kexue.fm/archives/4675)_

_**更详细的转载事宜请参考：**_ [《科学空间FAQ》](https://kexue.fm/archives/6508#%E6%96%87%E7%AB%A0%E5%A6%82%E4%BD%95%E8%BD%AC%E8%BD%BD/%E5%BC%95%E7%94%A8)

**如果您还有什么疑惑或建议，欢迎在下方评论区继续讨论。**

**如果您觉得本文还不错，欢迎 [分享](https://kexue.fm/kexue.fm#share)/ [打赏](https://kexue.fm/kexue.fm#pay) 本文。打赏并非要从中获得收益，而是希望知道科学空间获得了多少读者的真心关注。当然，如果你无视它，也不会影响你的阅读。再次表示欢迎和感谢！**

打赏

微信打赏

支付宝打赏

因为网站后台对打赏并无记录，因此欢迎在打赏时候备注留言。你还可以 [**点击这里**](http://mail.qq.com/cgi-bin/qm_share?t=qm_mailme&email=tN7d1drY3drrx8H0xcWa19vZ) 或在下方评论区留言来告知你的建议或需求。

**如果您需要引用本文，请参考：**

苏剑林. (Nov. 19, 2017). 《更别致的词向量模型(四)：模型的求解 》\[Blog post\]. Retrieved from [https://kexue.fm/archives/4675](https://kexue.fm/archives/4675)

@online{kexuefm-4675,
        title={更别致的词向量模型(四)：模型的求解},
        author={苏剑林},
        year={2017},
        month={Nov},
        url={\\url{https://kexue.fm/archives/4675}},
}

分类： [信息时代](https://kexue.fm/category/Big-Data)    标签： [优化](https://kexue.fm/tag/%E4%BC%98%E5%8C%96/), [词向量](https://kexue.fm/tag/%E8%AF%8D%E5%90%91%E9%87%8F/), [glove](https://kexue.fm/tag/glove/)[9 评论](https://kexue.fm/archives/4675#comments)

< [更别致的词向量模型(三)：描述相关的模型](https://kexue.fm/archives/4671) \| [更别致的词向量模型(五)：有趣的结果](https://kexue.fm/archives/4677) >

### 你也许还对下面的内容感兴趣

- [QK-Clip：让Muon在Scaleup之路上更进一步](https://kexue.fm/archives/11126)
- [Transformer升级之路：21、MLA好在哪里?（下）](https://kexue.fm/archives/11111)
- [MoE环游记：5、均匀分布的反思](https://kexue.fm/archives/10945)
- [Transformer升级之路：20、MLA好在哪里?（上）](https://kexue.fm/archives/10907)
- [MoE环游记：4、难处应当多投入](https://kexue.fm/archives/10815)
- [为什么梯度裁剪的默认模长是1？](https://kexue.fm/archives/10657)
- [从谱范数梯度到新式权重衰减的思考](https://kexue.fm/archives/10648)
- [从Hessian近似看自适应学习率优化器](https://kexue.fm/archives/10588)
- [通向最优分布之路：概率空间的最小化](https://kexue.fm/archives/10289)
- [缓存与效果的极限拉扯：从MHA、MQA、GQA到MLA](https://kexue.fm/archives/10091)

[发表你的看法](https://kexue.fm/kexue.fm#comment_form)

Ivan

November 29th, 2017

“所以我们采用了一个小trick：不算入相同的共现项，让模型自己把这个学出来。也就是说，哪怕上下文（除中心词外）也出现了中心词，也不算进loss中，因为数据量本身是远远大于参数量的，所以这一项总可以学习出来。”请问这里话里的学出来，具体怎样理解和在算法中怎样体现出来呢？

[回复评论](https://kexue.fm/archives/4675/comment-page-1?replyTo=8425#respond-post-4675)

[苏剑林](http://kexue.fm) 发表于
November 30th, 2017

体现就是：在一般的足够多的语料下，我们统计出来的共现项大概有数十亿，也就是$10^9\\sim 10^{10}$这个数量级，而如果我们保留30w的词语，每个词语用128维词向量，那么参数量不超过$10^8$，数据量差不多是参数量的100倍，就算不统计自身的共现项，那也就减少了30w的数据量，微不足道。

换个角度来讲，训练的过程就是依靠互信息把每个词的词向量“摆放”好，如果我们的模型是合理的，那么依靠别人的互信息就足够摆放好自己了。

[回复评论](https://kexue.fm/archives/4675/comment-page-1?replyTo=8435#respond-post-4675)

Ivan

December 3rd, 2017

请问基于16和17的两个模型实际效果有多大的差别？

[回复评论](https://kexue.fm/archives/4675/comment-page-1?replyTo=8444#respond-post-4675)

empenguin

May 7th, 2019

博主你好，最近在读word embedding相关的文章，读你的博客受到很大启发。关于GLOVE，我觉得文章出发点的直觉非常好，好的词向量应该满足条件概率的比例。个人认为，这种不对称的corpus statistics比对称的mutual information包含更多的信息。不过一直有个困惑想跟博主讨论下。GLOVE原文提到“Our final model should be invariant under this relabeling, but Eqn. (3) is not”。博主对于“invariant under this relabeling”怎么理解？

[回复评论](https://kexue.fm/archives/4675/comment-page-1?replyTo=11111#respond-post-4675)

[苏剑林](https://kexue.fm) 发表于
May 7th, 2019

大概的意思是模型应该在变换$w \\leftrightarrow \\tilde{w}, X \\leftrightarrow X^T$之下保持不变吧

[回复评论](https://kexue.fm/archives/4675/comment-page-1?replyTo=11114#respond-post-4675)

殷雅俊

July 15th, 2019

（23）的adagrad分母少了一个根号

[回复评论](https://kexue.fm/archives/4675/comment-page-1?replyTo=11615#respond-post-4675)

[苏剑林](https://kexue.fm) 发表于
July 16th, 2019

谢谢指出，已经修正～

[回复评论](https://kexue.fm/archives/4675/comment-page-1?replyTo=11622#respond-post-4675)

kkkfffccc

November 1st, 2020

batch选多大。。。

[回复评论](https://kexue.fm/archives/4675/comment-page-1?replyTo=14678#respond-post-4675)

[苏剑林](https://kexue.fm) 发表于
November 2nd, 2020

原生的glove和word2vec都是没有batch的概念，都是一个个样本地学的。

[回复评论](https://kexue.fm/archives/4675/comment-page-1?replyTo=14681#respond-post-4675)

[取消回复](https://kexue.fm/archives/4675#respond-post-4675)

你的大名

电子邮箱

个人网站（选填）

1\. 可以使用LaTeX代码，点击“预览效果”可查看效果；2. 可以通过点击评论楼层编号来引用该楼层；3. 网站可能会有点卡，如非确认评论失败，请 **不要重复点击提交**。

### 内容速览

[损失函数](https://kexue.fm/kexue.fm#%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0)
[互信息估算](https://kexue.fm/kexue.fm#%E4%BA%92%E4%BF%A1%E6%81%AF%E4%BC%B0%E7%AE%97)
[权重和降采样](https://kexue.fm/kexue.fm#%E6%9D%83%E9%87%8D%E5%92%8C%E9%99%8D%E9%87%87%E6%A0%B7)
[Adagrad](https://kexue.fm/kexue.fm#Adagrad)

### 智能搜索

支持整句搜索！网站自动使用 [结巴分词](https://github.com/fxsjy/jieba) 进行分词，并结合ngrams排序算法给出合理的搜索结果。

### 热门标签

[生成模型](https://kexue.fm/tag/%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B/) [attention](https://kexue.fm/tag/attention/) [优化](https://kexue.fm/tag/%E4%BC%98%E5%8C%96/) [语言模型](https://kexue.fm/tag/%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/) [模型](https://kexue.fm/tag/%E6%A8%A1%E5%9E%8B/) [网站](https://kexue.fm/tag/%E7%BD%91%E7%AB%99/) [概率](https://kexue.fm/tag/%E6%A6%82%E7%8E%87/) [梯度](https://kexue.fm/tag/%E6%A2%AF%E5%BA%A6/) [转载](https://kexue.fm/tag/%E8%BD%AC%E8%BD%BD/) [矩阵](https://kexue.fm/tag/%E7%9F%A9%E9%98%B5/) [微分方程](https://kexue.fm/tag/%E5%BE%AE%E5%88%86%E6%96%B9%E7%A8%8B/) [优化器](https://kexue.fm/tag/%E4%BC%98%E5%8C%96%E5%99%A8/) [分析](https://kexue.fm/tag/%E5%88%86%E6%9E%90/) [天象](https://kexue.fm/tag/%E5%A4%A9%E8%B1%A1/) [深度学习](https://kexue.fm/tag/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/) [积分](https://kexue.fm/tag/%E7%A7%AF%E5%88%86/) [python](https://kexue.fm/tag/python/) [力学](https://kexue.fm/tag/%E5%8A%9B%E5%AD%A6/) [无监督](https://kexue.fm/tag/%E6%97%A0%E7%9B%91%E7%9D%A3/) [扩散](https://kexue.fm/tag/%E6%89%A9%E6%95%A3/) [几何](https://kexue.fm/tag/%E5%87%A0%E4%BD%95/) [节日](https://kexue.fm/tag/%E8%8A%82%E6%97%A5/) [生活](https://kexue.fm/tag/%E7%94%9F%E6%B4%BB/) [文本生成](https://kexue.fm/tag/%E6%96%87%E6%9C%AC%E7%94%9F%E6%88%90/) [数论](https://kexue.fm/tag/%E6%95%B0%E8%AE%BA/)

### 随机文章

- [【NASA每日一图】土星的六年](https://kexue.fm/archives/109)
- [【语料】百度的中文问答数据集WebQA](https://kexue.fm/archives/4338)
- [更别致的词向量模型(一)：simpler glove](https://kexue.fm/archives/4667)
- [科学空间：2010年3月重要天象](https://kexue.fm/archives/488)
- [从局部到全局：语义相似度的测地线距离](https://kexue.fm/archives/9368)
- [高一新生活](https://kexue.fm/archives/110)
- [《量子力学与路径积分》习题解答V0.5](https://kexue.fm/archives/3692)
- [熵不变性Softmax的一个快速推导](https://kexue.fm/archives/9034)
- [Cool Papers更新：简单搭建了一个站内检索系统](https://kexue.fm/archives/10088)
- [最新调查解“毒”珠江：工业水污染触目惊心！](https://kexue.fm/archives/222)

### 最近评论

- [mp4网](https://kexue.fm/links.html/comment-page-6#comment-28589): 申请友链
站名：mp4网
域名：http://mp4wang.cc
描述：在线视频
- [FrankCai](https://kexue.fm/archives/10563/comment-page-1#comment-28588): 老师您好， 如果结论是【当ϵ越大，结果越接近SGD，“Surge现象”出现的概率就越低】，那我...
- [川zi](https://kexue.fm/archives/9009/comment-page-3#comment-28587): 看懂了， 谢谢大佬
- [pb](https://kexue.fm/archives/8265/comment-page-8#comment-28586): 苏神您好，这两天读到“远程衰减”的部分时觉得有些奇怪，还请指正。
文中的 upperbound...
- [苏剑林](https://kexue.fm/archives/9119/comment-page-13#comment-28585): 这就是diffusion模型的神奇之处，它有点直观，但又不完全可以单纯由直觉得到。你说的$\\b...
- [苏剑林](https://kexue.fm/archives/11033/comment-page-2#comment-28584): 平方或者线性复杂度，都是指关于序列长度$n$的总复杂度，$d$不算。
- [苏剑林](https://kexue.fm/archives/9907/comment-page-4#comment-28583): cool papers的prompt其实没啥特殊的，关键的技巧是一个问题一个问题地问，不要一次...
- [苏剑林](https://kexue.fm/archives/443/comment-page-1#comment-28582): 已发
- [苏剑林](https://kexue.fm/archives/10592/comment-page-2#comment-28581): Muon不是“可以”用更大的学习率，它用更大的学习率是因为msign的结果本身就是偏小的。举个...
- [苏剑林](https://kexue.fm/archives/11033/comment-page-2#comment-28580): 逻辑是：训练一个模型，约等于把训练集压缩到模型权重中，模型权重是固定大小的，所以本质上是将数目...

### 友情链接

- [Cool Papers](https://papers.cool)
- [数学研发](https://bbs.emath.ac.cn)
- [Seatop](http://www.seatop.com.cn/)
- [Xiaoxia](https://xiaoxia.org/)
- [积分表-网络版](https://kexue.fm/sci/integral/index.html)
- [丝路博傲](http://blog.dvxj.com/)
- [数学之家](http://www.2math.cn/)
- [有趣天文奇观](http://interesting-sky.china-vo.org/)
- [TwistedW](http://www.twistedwg.com/)
- [godweiyang](https://godweiyang.com/)
- [AI柠檬](https://blog.ailemon.net/)
- [王登科-DK博客](https://greatdk.com)
- [ESON](https://blog.eson.org/)
- [枫之羽](https://fzhiy.net/)
- [Mathor's blog](https://wmathor.com/)
- [coding-zuo](https://coding-zuo.github.io/)
- [博科园](https://www.bokeyuan.net/)
- [孔皮皮的博客](https://www.kppkkp.top/)
- [运鹏的博客](https://yunpengtai.top/)
- [jiming.site](https://jiming.site/)
- [OmegaXYZ](https://www.omegaxyz.com/)
- [EAI猩球](https://www.robotech.ink/)
- [文举的博客](https://liwenju0.com/)
- [用代码打点酱油](https://bruceyuan.com/)
- [Zhang's blog](https://armcvai.cn/)
- [申请链接](https://kexue.fm/links.html)

本站采用创作共用版权协议，要求署名、非商业用途和保持一致。转载本站内容必须也遵循“ [署名-非商业用途-保持一致](http://creativecommons.org/licenses/by-nc-nd/2.5/cn/)”的创作共用协议。
© 2009-2025 Scientific Spaces. All rights reserved. Theme by [laogui](http://www.laogui.com). Powered by [Typecho](http://typecho.org). 备案号: [粤ICP备09093259号-1/2](https://beian.miit.gov.cn/)。