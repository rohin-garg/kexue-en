Processing math: 0%

![MobileSideBar](https://kexue.fm/usr/themes/geekg/images/slide-button.png)

## SEARCH

## MENU

- [打赏](https://kexue.fm/reward.html)
- [公式](https://kexue.fm/latex.html)
- [天象](https://kexue.fm/ac.html)
- [链接](https://kexue.fm/links.html)
- [时光](https://kexue.fm/me.html)
- [博览](https://kexue.fm/science.html)
- [归档](https://kexue.fm/content.html)

## CATEGORIES

- [千奇百怪](https://kexue.fm/category/Everything)
- [天文探索](https://kexue.fm/category/Astronomy)
- [数学研究](https://kexue.fm/category/Mathematics)
- [物理化学](https://kexue.fm/category/Phy-chem)
- [信息时代](https://kexue.fm/category/Big-Data)
- [生物自然](https://kexue.fm/category/Biology)
- [图片摄影](https://kexue.fm/category/Photograph)
- [问题百科](https://kexue.fm/category/Questions)
- [生活/情感](https://kexue.fm/category/Life-Feeling)
- [资源共享](https://kexue.fm/category/Resources)

## NEWPOSTS

- [让炼丹更科学一些（五）：基于梯度精...](https://kexue.fm/archives/11530)
- [让炼丹更科学一些（四）：新恒等式，...](https://kexue.fm/archives/11494)
- [为什么DeltaNet要加L2 N...](https://kexue.fm/archives/11486)
- [让炼丹更科学一些（三）：SGD的终...](https://kexue.fm/archives/11480)
- [让炼丹更科学一些（二）：将结论推广...](https://kexue.fm/archives/11469)
- [滑动平均视角下的权重衰减和学习率](https://kexue.fm/archives/11459)
- [生成扩散模型漫谈（三十一）：预测数...](https://kexue.fm/archives/11428)
- [Muon优化器指南：快速上手与关键细节](https://kexue.fm/archives/11416)
- [AdamW的Weight RMS的...](https://kexue.fm/archives/11404)
- [n个正态随机数的最大值的渐近估计](https://kexue.fm/archives/11390)

## COMMENTS

- [Bin: 今天偶然从某个论坛看到有人推荐您的博客，定睛一看竟然是华师同院...](https://kexue.fm/archives/1990/comment-page-2#comment-29105)
- [Rapture D: 我有一个问题，为什么不考虑亥姆霍兹定理和斯托克斯公式。](https://kexue.fm/archives/11530/comment-page-1#comment-29104)
- [mofheka: 苏神是还在用jax是么？最近在做基于Google Pathwa...](https://kexue.fm/archives/11390/comment-page-1#comment-29103)
- [长琴: 看懂这篇博客也不是一件容易的事情。](https://kexue.fm/archives/11530/comment-page-1#comment-29102)
- [AlexLi: 苏老师，请教一下(7)式中将 \\mu(x\_t) 传给 $p...](https://kexue.fm/archives/9257/comment-page-4#comment-29101)
- [tyler\_zxc: "Performer的思想是将标准的Attention线性化，...](https://kexue.fm/archives/7921/comment-page-2#comment-29100)
- [我: 似乎并非mHC提出矩阵的思想？之前hyper connecti...](https://kexue.fm/archives/11494/comment-page-1#comment-29099)
- [winter: 苏神您好，假如对于比较均匀的attention weightP...](https://kexue.fm/archives/10847/comment-page-1#comment-29098)
- [苏剑林: KL散度、JS散度、W距离啥的，都行啊，看你喜欢哪个](https://kexue.fm/archives/8512/comment-page-2#comment-29097)
- [苏剑林: 没有绝对公平的对比方法，主要看你关心什么。比如，如果只关心推理...](https://kexue.fm/archives/9119/comment-page-14#comment-29096)

## USERLOGIN

- [登录](https://kexue.fm/admin/login.php)

[科学空间\|Scientific Spaces](https://kexue.fm/)

- [登录](https://kexue.fm/admin/login.php)
- [打赏](https://kexue.fm/reward.html)
- [公式](https://kexue.fm/latex.html)
- [天象](https://kexue.fm/ac.html)
- [链接](https://kexue.fm/links.html)
- [时光](https://kexue.fm/me.html)
- [博览](https://kexue.fm/science.html)
- [归档](https://kexue.fm/content.html)

渴望成为一个小飞侠

- [![](https://kexue.fm/usr/themes/geekg/images/rss.png)\\
\\
欢迎订阅](https://kexue.fm/feed)
- [![](https://kexue.fm/usr/themes/geekg/images/mail.png)\\
\\
个性邮箱](https://kexue.fm/archives/119)
- [![](https://kexue.fm/usr/themes/geekg/images/Saturn.png)\\
\\
天象信息](https://kexue.fm/ac.html)
- [![](https://kexue.fm/usr/themes/geekg/images/iss.png)\\
\\
观测ISS](https://kexue.fm/archives/41)
- [![](https://kexue.fm/usr/themes/geekg/images/pi.png)\\
\\
LaTeX](https://kexue.fm/latex.html)
- [![](https://kexue.fm/usr/themes/geekg/images/mlogo.png)\\
\\
关于博主](https://kexue.fm/me.html)

欢迎访问“科学空间”，这里将与您共同探讨自然科学，回味人生百态；也期待大家的分享～

- [**千奇百怪** Everything](https://kexue.fm/category/Everything)
- [**天文探索** Astronomy](https://kexue.fm/category/Astronomy)
- [**数学研究** Mathematics](https://kexue.fm/category/Mathematics)
- [**物理化学** Phy-chem](https://kexue.fm/category/Phy-chem)
- [**信息时代** Big-Data](https://kexue.fm/category/Big-Data)
- [**生物自然** Biology](https://kexue.fm/category/Biology)
- [**图片摄影** Photograph](https://kexue.fm/category/Photograph)
- [**问题百科** Questions](https://kexue.fm/category/Questions)
- [**生活/情感** Life-Feeling](https://kexue.fm/category/Life-Feeling)
- [**资源共享** Resources](https://kexue.fm/category/Resources)

- [**千奇百怪**](https://kexue.fm/category/Everything)
- [**天文探索**](https://kexue.fm/category/Astronomy)
- [**数学研究**](https://kexue.fm/category/Mathematics)
- [**物理化学**](https://kexue.fm/category/Phy-chem)
- [**信息时代**](https://kexue.fm/category/Big-Data)
- [**生物自然**](https://kexue.fm/category/Biology)
- [**图片摄影**](https://kexue.fm/category/Photograph)
- [**问题百科**](https://kexue.fm/category/Questions)
- [**生活/情感**](https://kexue.fm/category/Life-Feeling)
- [**资源共享**](https://kexue.fm/category/Resources)

[首页](https://kexue.fm/) [信息时代](https://kexue.fm/category/Big-Data) 基于双向LSTM和迁移学习的seq2seq核心实体识别

6Sep

# [基于双向LSTM和迁移学习的seq2seq核心实体识别](https://kexue.fm/archives/3942)

By 苏剑林 \|
2016-09-06 \|
205090位读者 \|

暑假期间做了一下 [百度和西安交大联合举办的核心实体识别竞赛](http://openresearch.baidu.com/online/artical.do?method=activityItemDetail&activityID=26eb630e-5839-452d-ad71-bf023a8d6327&navIndex=1)，最终的结果还不错，遂记录一下。模型的效果不是最好的，但是胜在“ **端到端**”，迁移性强，估计对大家会有一定的参考价值。

比赛的主题是“核心实体识别”，其实有两个任务：核心识别 \+ 实体识别。这两个任务虽然有关联，但在传统自然语言处理程序中，一般是将它们分开处理的，而这次需要将两个任务联合在一起。如果只看“核心识别”，那就是传统的关键词抽取任务了，不同的是，传统的纯粹基于统计的思路（如TF-IDF抽取）是行不通的，因为单句中的核心实体可能就只出现一次，这时候统计估计是不可靠的，最好能够从语义的角度来理解。我一开始就是从“核心识别”入手，使用的方法类似QA系统：

> 1、将句子分词，然后用Word2Vec训练词向量；
>
> 2、用卷积神经网络（在这种抽取式问题上，CNN效果往往比RNN要好）卷积一下，得到一个与词向量维度一样的输出；
>
> 3、损失函数就是输出向量跟训练样本的核心词向量的cos值。

于是要找到句子的核心词，我只需要给每个句子计算一个输出向量，然后比较它与句子中每个词的向量的cos值，降序排列就行了。这个方法的明显优势是运行速度很快。最终，我用这个模型在公开评测集上做到了0.35的准确率，后来感觉难以提升，就放弃了这个思路。

[![膜拜0.7的大神](https://kexue.fm/usr/uploads/2016/09/1333598934.png)](https://kexue.fm/usr/uploads/2016/09/1333598934.png "点击查看原图")

膜拜0.7的大神

为什么放弃？事实上，这个思路在“核心识别”这部分做得很好，但它的致命缺陷就是：它依赖于分词效果。分词系统往往会把一些长词构成的核心实体切开，比如“朱家花园”切分为“朱家/花园”，切分后要进行整合就难得多了。于是，我参照 [《【中文分词系列】 4\. 基于双向LSTM的seq2seq字标注》](https://kexue.fm/archives/3924/) 一文，用词标注的思路来做，因为这个思路不明显依赖于分词效果。最终我用这个思路做到了0.56的准确率。

大概步骤是：

> 1、将句子分词，然后用Word2Vec训练词向量；
>
> 2、将输出转化为5tag标注问题：b（核心实体首词）、m（核心实体中）、e（核心实体末词）、s（单词成核心实体）、x（非核心实体部分）；
>
> 3、用双层双向LSTM进行预测，用viterbi算法进行标注。

最后，需要一提的是，根据这种思路，甚至可以不分词就来做核心实体识别，但总的来说，还是分了词的效果会好一些，并且分词有利于降低句子长度（100字的句子分词后变成50词的句子），这有利于减少模型的参数个数。这里我们只需要一个简单的分词系统即可，并且不需要它们内置的新词发现功能。

### 迁移学习 [\#](https://kexue.fm/archives/3942\#%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0)

但是，用这个思路之前我是很不确定它最终的效果的。主要原因是：**百度给出了1.2万的训练样本，但却有20万的测试样本。**比例如此悬殊，效果似乎很难好起来。此外，有5个tag，其中相比x这个tag，其他四个tag的数量是很少的，只有1.2万的训练样本，似乎存在数据不充分的问题。

当然，实践是检验真理的唯一标准。这个思路的首次测试就达到了0.42的准确率，远高于我前面精心调节了大半个月的CNN思路，于是我就往这个思路继续做下去。在继续做下去之前，我分析了这种思路效果不错的原因。我觉得，主要原因有两个：一个是“迁移学习”，另外一个就是LSTM强大的捕捉语义的能力。

传统数据挖掘的训练模型，是纯粹在训练集上做的。但是我们很难保证，训练集跟测试集是一致的，准确来说，就是很难假设训练集和测试集的分布是一样的。于是乎，即使模型训练效果非常好，测试效果也可能一塌糊涂，这不是过拟合所致，这是训练集和测试集不一致所造成的。

解决（缓解）这个问题的一个思路就是“迁移学习”。迁移学习现在已经是比较综合的建模策略了，在此不详述。一般来说，它有两套方案：

> 1、在建模前迁移学习，即可以把训练集和测试集放在一起，来学习建模用到的特征，这样得来的特征已经包含了测试集的信息；
>
> 2、在建模后迁移学习，即如果测试集的测试效果还不错，比如0.5的准确率，想要提高准确率，可以把测试集连同它的预测结果一起，当做训练样本，跟原来的训练样本一起重新训练模型。

大家可能对第2点比较困惑，测试集的预测结果不是有错的吗？输入错误的结果还能提高准确率？托尔斯泰说过“幸福的家庭都是相似的，不幸的家庭各有各的不幸”，放到这里，我想说的是“正确的答案都是相同的，错误的答案各有各的不同”。也就是说，如果进行第2点训练，测试集中给出的正确答案，效果会累积，因为它们都是正确的（相同的）模式的出来的结果，但是错误的答案各有不同的错误模式，如果模型参数有限，不至于过拟合，那么模型就会抹平这些错误模式，从而倾向于正确的答案。当然，这个理解是否准确，请读者点评。另外，如果得到了新的预测结果，那么可以只取两次相同的预测结果作为训练样本，这样的正确答案的比例就更高了。

在这个比赛中，迁移学习体现在：

> 1、用训练语料和测试语料一起训练Word2Vec，使得词向量本捕捉了测试语料的语义；
>
> 2、用训练语料训练模型；
>
> 3、得到模型后，对测试语料预测，把预测结果跟训练语料一起训练新的模型；
>
> 4、用新的模型预测，模型效果会有一定提升；
>
> 5、对比两次预测结果，如果两次预测结果都一样，那说明这个预测结果很有可能是对的，用这部分“很有可能是对的”的测试结果来训练模型；
>
> 6、用更新的模型预测；
>
> 7、如果你愿意，可以继续重复第4、5、6步。

### 双向LSTM [\#](https://kexue.fm/archives/3942\#%E5%8F%8C%E5%90%91LSTM)

主要的模型结构：

```python

```

就是用了双层的双向LSTM（单层我也试过，多加一层效果好一些），保留LSTM每次的输出，然后对每个输出都做一下softmax，整个过程基本就是分词系统一样了。

[![model](https://kexue.fm/usr/uploads/2016/09/245762055.png)](https://kexue.fm/usr/uploads/2016/09/245762055.png "点击查看原图")

model

当然，这个模型的好坏，很大程度上还取决于词向量的质量。经过多次调试，我发现如下的词向量参数基本是最优的：

```python

```

也就是说，skip-gram的效果要比cbow要好，负样本采样的模式要比层次softmax要好，负样本的数目要适中，窗口大小也要适中。当然，这个所谓的“最优”，是多次人工调试后，我自己“直观感觉”的，欢迎大家做更多的测试。

### 关于比赛 [\#](https://kexue.fm/archives/3942\#%E5%85%B3%E4%BA%8E%E6%AF%94%E8%B5%9B)

**百度跟西安交大这个比赛其实去年我也留意到了，但是我去年还是菜鸟水平，没法做那么艰难的任务。今年尝试做了一下，感觉收获颇丰的。**

**首先，比赛是百度举行的，单凭这点已经很有吸引力了，因为通常感觉如果能得到百度的肯定，那是一件了不起的事情，所以挺期待这类比赛的（希望有时间参加吧），也希望百度的比赛越办越好哈（套话了～）。其次，在这个过程中，我对语言模型的例子、深度网络的搭建与使用等，都有了更加深入的认识了，比如CNN怎么用于语言任务、可以用于哪些语言任务，还有seq2seq的初步使用等。**

**很碰巧的是，这次是一个自然语言处理任务，上次泰迪杯是一个图像任务，两次加起来，我把自然语言处理和图像的基本任务都做了一遍，心里对这些任务的处理都比较有底了，感觉挺踏实的。**

### 完整代码 [\#](https://kexue.fm/archives/3942\#%E5%AE%8C%E6%95%B4%E4%BB%A3%E7%A0%81)

训练集链接: [https://pan.baidu.com/s/1i457nkL](https://pan.baidu.com/s/1i457nkL) 密码: stkp

**说明文件**

> 基于迁移学习和双向LSTM的核心实体识别
>
> ==============================================================
>
> 总的步骤（在train\_and\_predict.py中一一对应）
>
> ==============================================================
>
> 1、训练语料和测试语料都分词，目前用的是结巴分词；
>
> 2、转化为5tag标注问题，构建训练标签；
>
> 3、训练语料和测试语料一起训练Word2Vec模型；
>
> 4、用双层双向LSTM训练标注模型，基于seq2seq的思想；
>
> 5、用模型进行预测，预测准确率大约会在0.46～0.52波动；
>
> 6、将预测结果当作标签数据，与训练数据一起，重新训练模型；
>
> 7、用新模型预测，预测准确率会在0.5～0.55波动；
>
> 8、比较两次预测结果，取交集当做标签数据，与训练数据一起，重新训练模型；
>
> 9、用新模型预测，预测准确率基本保持在0.53～0.56。
>
> ==============================================================
>
> 编译环境：
>
> ==============================================================
>
> 硬件环境：
>
> 1、96G内存（事实上用到10G左右）
>
> 2、GTX960显卡（GPU加速训练）
>
> 软件环境：
>
> 1、CentOS 7
>
> 2、Python 2.7（以下均为Python第三方库）
>
> 3、结巴分词
>
> 4、Numpy
>
> 5、SciPy
>
> 6、Pandas
>
> 7、Keras（官方GitHub版本）
>
> 8、Gensim
>
> 9、H5PY
>
> 10、tqdm
>
> ==============================================================
>
> 文件使用说明：
>
> ==============================================================
>
> train\_and\_predict.py
>
> 包含了从训练到预测的整个过程，只要“未开放的验证数据”格式跟“开放的测试数据”opendata\_20w格式一样，那么就可以
>
> 与train\_and\_predict.py放在同一目录，然后运行
>
> python train\_and\_predict.py
>
> 就可以完成整个过程，并且会生成一系列文件：
>
> \-\-\------------------------------------------------------------
>
> word2vec\_words\_final.model，word2vec模型
>
> words\_seq2seq\_final\_1.model，首次得到的双层双向LSTM模型
>
> \-\-\- result1.txt，首次预测结果文件
>
> \-\-\- result1.zip，首次预测结果文件压缩包
>
> words\_seq2seq\_final\_2.model，通过第一次迁移学习后得到的模型
>
> \-\-\- result2.txt，再次预测结果文件
>
> \-\-\- result2.zip，再次预测结果文件压缩包
>
> words\_seq2seq\_final\_3.model，通过第二次迁移学习后得到的模型
>
> \-\-\- result3.txt，再次预测结果文件
>
> \-\-\- result3.zip，再次预测结果文件压缩包
>
> words\_seq2seq\_final\_4.model，通过第三次迁移学习后得到的模型
>
> \-\-\- result4.txt，再次预测结果文件
>
> \-\-\- result4.zip，再次预测结果文件压缩包
>
> words\_seq2seq\_final\_5.model，通过第四次迁移学习后得到的模型
>
> \-\-\- result5.txt，再次预测结果文件
>
> \-\-\- result5.zip，再次预测结果文件压缩包
>
> \-\-\-------------------------------------------------------------
>
> ==============================================================
>
> 思路说明：
>
> ==============================================================
>
> 迁移学习体现在：
>
> 1、用训练语料和测试语料一起训练Word2Vec，使得词向量本捕捉了测试语料的语义；
>
> 2、用训练语料训练模型；
>
> 3、得到模型后，对测试语料预测，把预测结果跟训练语料一起训练新的模型；
>
> 4、用新的模型预测，模型效果会有一定提升；
>
> 5、对比两次预测结果，如果两次预测结果都一样，那说明这个预测结果很有可能是对的，用这部分“很有可能是对的”的测试结果来训练模型；
>
> 6、用更新的模型预测；
>
> 7、如果你愿意，可以继续重复第4、5、6步。
>
> 双向LSTM的思路：
>
> 1、分词；
>
> 2、转换为5tag标注问题（0:非核心实体，1:单词的核心实体，2:多词核心实体的首词，3:多词核心实体的中间部分，4:多词核心实体的末词）；
>
> 3、通过双向LSTM，直接对输入句子输出预测标注序列；
>
> 4、通过viterbi算法来获得标注结果；
>
> 5、因为常规的LSTM存在后面的词比前面的词更重要的弊端，因此用双向LSTM。

**train\_and\_predict.py**（代码并没整理，仅供测试参考）

```python

```

_**转载到请包括本文地址：** [https://kexue.fm/archives/3942](https://kexue.fm/archives/3942 "基于双向LSTM和迁移学习的seq2seq核心实体识别")_

_**更详细的转载事宜请参考：**_ [《科学空间FAQ》](https://kexue.fm/archives/6508#%E6%96%87%E7%AB%A0%E5%A6%82%E4%BD%95%E8%BD%AC%E8%BD%BD/%E5%BC%95%E7%94%A8 "《科学空间FAQ》")

**如果您还有什么疑惑或建议，欢迎在下方评论区继续讨论。**

**如果您觉得本文还不错，欢迎 [分享](https://kexue.fm/archives/3942#share)/ [打赏](https://kexue.fm/archives/3942#pay) 本文。打赏并非要从中获得收益，而是希望知道科学空间获得了多少读者的真心关注。当然，如果你无视它，也不会影响你的阅读。再次表示欢迎和感谢！**

打赏

![科学空间](https://kexue.fm/usr/themes/geekg/payment/wx.png)

微信打赏

![科学空间](https://kexue.fm/usr/themes/geekg/payment/zfb.png)

支付宝打赏

因为网站后台对打赏并无记录，因此欢迎在打赏时候备注留言。

你还可以 [**点击这里**](http://mail.qq.com/cgi-bin/qm_share?t=qm_mailme&email=tN7d1drY3drrx8H0xcWa19vZ) 或在下方评论区留言来告知你的建议或需求。

**如果您需要引用本文，请参考：**

苏剑林. (Sep. 06, 2016). 《基于双向LSTM和迁移学习的seq2seq核心实体识别 》\[Blog post\]. Retrieved from [https://kexue.fm/archives/3942](https://kexue.fm/archives/3942)

@online{kexuefm-3942,

         title={基于双向LSTM和迁移学习的seq2seq核心实体识别},

         author={苏剑林},

         year={2016},

         month={Sep},

         url={\\url{https://kexue.fm/archives/3942}},

}


分类： [信息时代](https://kexue.fm/category/Big-Data)    标签： [python](https://kexue.fm/tag/python/), [自然语言处理](https://kexue.fm/tag/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/), [识别](https://kexue.fm/tag/%E8%AF%86%E5%88%AB/), [NER](https://kexue.fm/tag/NER/)[46 评论](https://kexue.fm/archives/3942#comments)

< [进驻中山大学南校区，折腾校园网](https://kexue.fm/archives/3936 "进驻中山大学南校区，折腾校园网") \| [【中文分词系列】 5\. 基于语言模型的无监督分词](https://kexue.fm/archives/3956 "【中文分词系列】 5. 基于语言模型的无监督分词") >

### 你也许还对下面的内容感兴趣

- [旁门左道之如何让Python的重试代码更加优雅](https://kexue.fm/archives/9938 "旁门左道之如何让Python的重试代码更加优雅")
- [GlobalPointer下的“KL散度”应该是怎样的？](https://kexue.fm/archives/9039 "GlobalPointer下的“KL散度”应该是怎样的？")
- [GPLinker：基于GlobalPointer的事件联合抽取](https://kexue.fm/archives/8926 "GPLinker：基于GlobalPointer的事件联合抽取")
- [GPLinker：基于GlobalPointer的实体关系联合抽取](https://kexue.fm/archives/8888 "GPLinker：基于GlobalPointer的实体关系联合抽取")
- [Efficient GlobalPointer：少点参数，多点效果](https://kexue.fm/archives/8877 "Efficient GlobalPointer：少点参数，多点效果")
- [有限内存下全局打乱几百G文件（Python）](https://kexue.fm/archives/8662 "有限内存下全局打乱几百G文件（Python）")
- [GlobalPointer：用统一的方式处理嵌套和非嵌套NER](https://kexue.fm/archives/8373 "GlobalPointer：用统一的方式处理嵌套和非嵌套NER")
- [一个二值化词向量模型，是怎么跟果蝇搭上关系的？](https://kexue.fm/archives/8159 "一个二值化词向量模型，是怎么跟果蝇搭上关系的？")
- [6个派生优化器的简单介绍及其实现](https://kexue.fm/archives/7094 "6个派生优化器的简单介绍及其实现")
- [JoSE：球面上的词向量和句向量](https://kexue.fm/archives/7063 "JoSE：球面上的词向量和句向量")

[发表你的看法](https://kexue.fm/archives/3942#comment_form)

1. [«](https://kexue.fm/archives/3942/comment-page-2#comments)
2. [1](https://kexue.fm/archives/3942/comment-page-1#comments)
3. [2](https://kexue.fm/archives/3942/comment-page-2#comments)
4. [3](https://kexue.fm/archives/3942/comment-page-3#comments)

xxwx

January 16th, 2021

重温了一下文章 有几处不解 1."即可以把训练集和测试集放在一起，来学习建模用到的特征，这样得来的特征已经包含了测试集的信息" 这样不会造成自己在用测试集测试的时候信息由于泄漏导致结果很好吗? 2.以上的这种方法的本质是否就是增加训练数据呢? 3."这不是过拟合所致，这是训练集和测试集不一致所造成的" 我个人感觉这种说法不太"讲道理" ,您的意思是训练集和测试集的分布不一致 , 但是即使是训练集内部的方差也有可能很大 (希望就以上3点和苏神讨论 , 没有不敬之意)

[回复评论](https://kexue.fm/archives/3942/comment-page-3?replyTo=15279#respond-post-3942)

[苏剑林](https://kexue.fm/) 发表于
January 18th, 2021

1、就算你说的有泄漏，那也只是泄漏测试集样本自身的信息，并没有泄漏测试集的标签信息，因此依然是合理的，相当于无监督学习到了测试集的分布信息而已；

2、这种理解就见仁见智了，合理就好；

3、过拟合是指对训练集的学习过度了，导致在测试集的效果不好；分布不一致是指两者的构建方式不完全一样，因此就算训练集学习得很合理，测试集效果也不会太好。这是对两种效果不好的情况的机理性分析，有什么不讲道理的？相反，如果你不会区分这两种情况，那么就没有办法给你下一步的调优指明方向了。

[回复评论](https://kexue.fm/archives/3942/comment-page-3?replyTo=15282#respond-post-3942)

xxwx 发表于
January 18th, 2021

明白了 很有收获 谢谢苏神

[回复评论](https://kexue.fm/archives/3942/comment-page-3?replyTo=15286#respond-post-3942)

吴孟龙

July 25th, 2021

请问一下，原始有5TAG的预料是怎么标注的呢？

[回复评论](https://kexue.fm/archives/3942/comment-page-3?replyTo=16989#respond-post-3942)

[苏剑林](https://kexue.fm/) 发表于
July 26th, 2021

原始的语料没有5tag，5tag是我们自己转换的，原始语料就是“原句子+核心实体”的形式。

[回复评论](https://kexue.fm/archives/3942/comment-page-3?replyTo=16995#respond-post-3942)

chuyg

April 15th, 2022

这感觉和命名实体识别没有异同啊，核心实体识别似乎就是命名实体识别？

[回复评论](https://kexue.fm/archives/3942/comment-page-3?replyTo=18962#respond-post-3942)

1. [«](https://kexue.fm/archives/3942/comment-page-2#comments)
2. [1](https://kexue.fm/archives/3942/comment-page-1#comments)
3. [2](https://kexue.fm/archives/3942/comment-page-2#comments)
4. [3](https://kexue.fm/archives/3942/comment-page-3#comments)

[取消回复](https://kexue.fm/archives/3942#respond-post-3942)

你的大名

电子邮箱

个人网站（选填）

1\. 可以使用LaTeX代码，点击“预览效果”可查看效果；

2\. 可以通过点击评论楼层编号来引用该楼层；

3\. 网站可能会有点卡，如非确认评论失败，请 **不要重复点击提交**。

### 内容速览

[迁移学习](https://kexue.fm/archives/3942#%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0)
[双向LSTM](https://kexue.fm/archives/3942#%E5%8F%8C%E5%90%91LSTM)
[关于比赛](https://kexue.fm/archives/3942#%E5%85%B3%E4%BA%8E%E6%AF%94%E8%B5%9B)
[完整代码](https://kexue.fm/archives/3942#%E5%AE%8C%E6%95%B4%E4%BB%A3%E7%A0%81)

### 智能搜索

支持整句搜索！网站自动使用 [结巴分词](https://github.com/fxsjy/jieba) 进行分词，并结合ngrams排序算法给出合理的搜索结果。

### 热门标签

[生成模型](https://kexue.fm/tag/%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B/) [attention](https://kexue.fm/tag/attention/) [优化](https://kexue.fm/tag/%E4%BC%98%E5%8C%96/) [语言模型](https://kexue.fm/tag/%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/) [模型](https://kexue.fm/tag/%E6%A8%A1%E5%9E%8B/) [梯度](https://kexue.fm/tag/%E6%A2%AF%E5%BA%A6/) [网站](https://kexue.fm/tag/%E7%BD%91%E7%AB%99/) [概率](https://kexue.fm/tag/%E6%A6%82%E7%8E%87/) [矩阵](https://kexue.fm/tag/%E7%9F%A9%E9%98%B5/) [优化器](https://kexue.fm/tag/%E4%BC%98%E5%8C%96%E5%99%A8/) [转载](https://kexue.fm/tag/%E8%BD%AC%E8%BD%BD/) [微分方程](https://kexue.fm/tag/%E5%BE%AE%E5%88%86%E6%96%B9%E7%A8%8B/) [分析](https://kexue.fm/tag/%E5%88%86%E6%9E%90/) [天象](https://kexue.fm/tag/%E5%A4%A9%E8%B1%A1/) [深度学习](https://kexue.fm/tag/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/) [积分](https://kexue.fm/tag/%E7%A7%AF%E5%88%86/) [python](https://kexue.fm/tag/python/) [扩散](https://kexue.fm/tag/%E6%89%A9%E6%95%A3/) [力学](https://kexue.fm/tag/%E5%8A%9B%E5%AD%A6/) [无监督](https://kexue.fm/tag/%E6%97%A0%E7%9B%91%E7%9D%A3/) [几何](https://kexue.fm/tag/%E5%87%A0%E4%BD%95/) [节日](https://kexue.fm/tag/%E8%8A%82%E6%97%A5/) [生活](https://kexue.fm/tag/%E7%94%9F%E6%B4%BB/) [文本生成](https://kexue.fm/tag/%E6%96%87%E6%9C%AC%E7%94%9F%E6%88%90/) [数论](https://kexue.fm/tag/%E6%95%B0%E8%AE%BA/)

### 随机文章

- [倒立单摆之分离频率](https://kexue.fm/archives/2471)
- [生成扩散模型漫谈（九）：条件控制生成结果](https://kexue.fm/archives/9257)
- [桃花开放了](https://kexue.fm/archives/1548)
- [BERT可以上几年级了？Seq2Seq“硬刚”小学数学应用题](https://kexue.fm/archives/7809)
- [“天地图”试用——很细致，有瑕疵](https://kexue.fm/archives/1039)
- [\[SETI-50周年\]茫茫宇宙觅知音](https://kexue.fm/archives/1205)
- [第一次拍摄天体（月球）！](https://kexue.fm/archives/377)
- [《新理解矩阵1》：矩阵是什么？](https://kexue.fm/archives/1765)
- [费曼积分法(7)：欧拉数学的综合](https://kexue.fm/archives/1946)
- [算符的艺术：差分、微分与伯努利数](https://kexue.fm/archives/3018)

### 最近评论

- [Bin](https://kexue.fm/archives/1990/comment-page-2#comment-29105): 今天偶然从某个论坛看到有人推荐您的博客，定睛一看竟然是华师同院的往届师兄！看到这篇2013年的...
- [Rapture D](https://kexue.fm/archives/11530/comment-page-1#comment-29104): 我有一个问题，为什么不考虑亥姆霍兹定理和斯托克斯公式。
- [mofheka](https://kexue.fm/archives/11390/comment-page-1#comment-29103): 苏神是还在用jax是么？最近在做基于Google Pathway的理念做一个动态版的MPMD框...
- [长琴](https://kexue.fm/archives/11530/comment-page-1#comment-29102): 看懂这篇博客也不是一件容易的事情。
- [AlexLi](https://kexue.fm/archives/9257/comment-page-4#comment-29101): 苏老师，请教一下(7)式中将 \\mu(x\_t) 传给 p\_o 进行推理的操作。 $x\_...
- [tyler\_zxc](https://kexue.fm/archives/7921/comment-page-2#comment-29100): "Performer的思想是将标准的Attention线性化，所以为什么不干脆直接训练一个线性...
- [我](https://kexue.fm/archives/11494/comment-page-1#comment-29099): 似乎并非mHC提出矩阵的思想？之前hyper connection就是了
- [winter](https://kexue.fm/archives/10847/comment-page-1#comment-29098): 苏神您好，假如对于比较均匀的attention weightP，往往呈现long tail分布...
- [苏剑林](https://kexue.fm/archives/8512/comment-page-2#comment-29097): KL散度、JS散度、W距离啥的，都行啊，看你喜欢哪个
- [苏剑林](https://kexue.fm/archives/9119/comment-page-14#comment-29096): 没有绝对公平的对比方法，主要看你关心什么。比如，如果只关心推理成本和推理效果，那么有的方法可以...

### 友情链接

- [Cool Papers](https://papers.cool/)
- [数学研发](https://bbs.emath.ac.cn/)
- [Seatop](http://www.seatop.com.cn/)
- [Xiaoxia](https://xiaoxia.org/)
- [积分表-网络版](https://kexue.fm/sci/integral/index.html)
- [丝路博傲](http://blog.dvxj.com/)
- [数学之家](http://www.2math.cn/)
- [有趣天文奇观](http://interesting-sky.china-vo.org/)
- [TwistedW](http://www.twistedwg.com/)
- [godweiyang](https://godweiyang.com/)
- [AI柠檬](https://blog.ailemon.net/)
- [王登科-DK博客](https://greatdk.com/)
- [ESON](https://blog.eson.org/)
- [枫之羽](https://fzhiy.net/)
- [coding-zuo](https://coding-zuo.github.io/)
- [博科园](https://www.bokeyuan.net/)
- [孔皮皮的博客](https://www.kppkkp.top/)
- [运鹏的博客](https://yunpengtai.top/)
- [jiming.site](https://jiming.site/)
- [OmegaXYZ](https://www.omegaxyz.com/)
- [EAI猩球](https://www.robotech.ink/)
- [文举的博客](https://liwenju0.com/)
- [申请链接](https://kexue.fm/links.html)

[![署名-非商业用途-保持一致](https://kexue.fm/usr/themes/geekg/images/cc.gif)](http://creativecommons.org/licenses/by-nc-nd/2.5/cn/) 本站采用创作共用版权协议，要求署名、非商业用途和保持一致。转载本站内容必须也遵循“ [署名-非商业用途-保持一致](http://creativecommons.org/licenses/by-nc-nd/2.5/cn/)”的创作共用协议。



© 2009-2026 Scientific Spaces. All rights reserved. Theme by [laogui](http://www.laogui.com/). Powered by [Typecho](http://typecho.org/). 备案号: [粤ICP备09093259号-1/2](https://beian.miit.gov.cn/ "粤ICP备09093259号")。