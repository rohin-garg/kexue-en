## SEARCH

## MENU

- [打赏](https://kexue.fm/reward.html)
- [公式](https://kexue.fm/latex.html)
- [天象](https://kexue.fm/ac.html)
- [链接](https://kexue.fm/links.html)
- [时光](https://kexue.fm/me.html)
- [博览](https://kexue.fm/science.html)
- [归档](https://kexue.fm/content.html)

## CATEGORIES

- [千奇百怪](https://kexue.fm/category/Everything)
- [天文探索](https://kexue.fm/category/Astronomy)
- [数学研究](https://kexue.fm/category/Mathematics)
- [物理化学](https://kexue.fm/category/Phy-chem)
- [信息时代](https://kexue.fm/category/Big-Data)
- [生物自然](https://kexue.fm/category/Biology)
- [图片摄影](https://kexue.fm/category/Photograph)
- [问题百科](https://kexue.fm/category/Questions)
- [生活/情感](https://kexue.fm/category/Life-Feeling)
- [资源共享](https://kexue.fm/category/Resources)

## NEWPOSTS

- [为什么DeltaNet要加L2 N...](https://kexue.fm/archives/11486)
- [让炼丹更科学一些（三）：SGD的终...](https://kexue.fm/archives/11480)
- [让炼丹更科学一些（二）：将结论推广...](https://kexue.fm/archives/11469)
- [滑动平均视角下的权重衰减和学习率](https://kexue.fm/archives/11459)
- [生成扩散模型漫谈（三十一）：预测数...](https://kexue.fm/archives/11428)
- [Muon优化器指南：快速上手与关键细节](https://kexue.fm/archives/11416)
- [AdamW的Weight RMS的...](https://kexue.fm/archives/11404)
- [n个正态随机数的最大值的渐近估计](https://kexue.fm/archives/11390)
- [流形上的最速下降：5\. 对偶梯度下降](https://kexue.fm/archives/11388)
- [低精度Attention可能存在有...](https://kexue.fm/archives/11371)

## COMMENTS

- [kaiyuan: 看了“Linear Transformers Are Secr...](https://kexue.fm/archives/11486/comment-page-1#comment-29036)
- [sog: 好的，符号相同，搞混了呃](https://kexue.fm/archives/11469/comment-page-1#comment-29035)
- [kerry: 还没有通读完后面的系列，提出一些拙见。\
降低方差这一节把原本的...](https://kexue.fm/archives/9119/comment-page-14#comment-29034)
- [Kevin Yin: I wrote https://research.novela...](https://kexue.fm/archives/11158/comment-page-1#comment-29033)
- [罗: 公式(6)显示出来是不是有点小问题？](https://kexue.fm/archives/11480/comment-page-1#comment-29032)
- [cmlin: 本人对这方面不太熟悉，想了解这三个条件的意义及动机，且希望这系...](https://kexue.fm/archives/11340/comment-page-1#comment-29031)
- [喝一口可乐: 理解了，感谢苏神回复，数学上给出建模分析确实清晰了很多，再次感...](https://kexue.fm/archives/10958/comment-page-3#comment-29030)
- [CuddleSabe1: 感觉普通的 flow matching 可以看成 degrad...](https://kexue.fm/archives/10958/comment-page-1#comment-29029)
- [岁月如书: 受教了，感谢](https://kexue.fm/archives/11126/comment-page-3#comment-29028)
- [苏剑林: 是](https://kexue.fm/archives/11126/comment-page-3#comment-29027)

## USERLOGIN

- [登录](https://kexue.fm/admin/login.php)

[科学空间\|Scientific Spaces](https://kexue.fm)

- [登录](https://kexue.fm/admin/login.php)
- [打赏](https://kexue.fm/reward.html)
- [公式](https://kexue.fm/latex.html)
- [天象](https://kexue.fm/ac.html)
- [链接](https://kexue.fm/links.html)
- [时光](https://kexue.fm/me.html)
- [博览](https://kexue.fm/science.html)
- [归档](https://kexue.fm/content.html)

渴望成为一个小飞侠

- [欢迎订阅](https://kexue.fm/feed)
- [个性邮箱](https://kexue.fm/archives/119)
- [天象信息](https://kexue.fm/ac.html)
- [观测ISS](https://kexue.fm/archives/41)
- [LaTeX](https://kexue.fm/latex.html)
- [关于博主](https://kexue.fm/me.html)

欢迎访问“科学空间”，这里将与您共同探讨自然科学，回味人生百态；也期待大家的分享～

- [**千奇百怪** Everything](https://kexue.fm/category/Everything)
- [**天文探索** Astronomy](https://kexue.fm/category/Astronomy)
- [**数学研究** Mathematics](https://kexue.fm/category/Mathematics)
- [**物理化学** Phy-chem](https://kexue.fm/category/Phy-chem)
- [**信息时代** Big-Data](https://kexue.fm/category/Big-Data)
- [**生物自然** Biology](https://kexue.fm/category/Biology)
- [**图片摄影** Photograph](https://kexue.fm/category/Photograph)
- [**问题百科** Questions](https://kexue.fm/category/Questions)
- [**生活/情感** Life-Feeling](https://kexue.fm/category/Life-Feeling)
- [**资源共享** Resources](https://kexue.fm/category/Resources)

- [**千奇百怪**](https://kexue.fm/category/Everything)
- [**天文探索**](https://kexue.fm/category/Astronomy)
- [**数学研究**](https://kexue.fm/category/Mathematics)
- [**物理化学**](https://kexue.fm/category/Phy-chem)
- [**信息时代**](https://kexue.fm/category/Big-Data)
- [**生物自然**](https://kexue.fm/category/Biology)
- [**图片摄影**](https://kexue.fm/category/Photograph)
- [**问题百科**](https://kexue.fm/category/Questions)
- [**生活/情感**](https://kexue.fm/category/Life-Feeling)
- [**资源共享**](https://kexue.fm/category/Resources)

[首页](https://kexue.fm) [信息时代](https://kexue.fm/category/Big-Data) 随机分词浅探：从Viterbi Decoding到Viterbi Sampling

16Sep

# [随机分词浅探：从Viterbi Decoding到Viterbi Sampling](https://kexue.fm/archives/9768)

By 苏剑林 \|
2023-09-16 \|
27956位读者\|

上一篇文章 [《大词表语言模型在续写任务上的一个问题及对策》](https://kexue.fm/archives/9762) 发布后，很快就有读者指出可以在训练阶段引入带有随机性的分词结果来解决同样的问题，并且已经有论文和实现。经过进一步查阅学习，笔者发现这是一个名为 [Subword Regularization](https://papers.cool/arxiv/1804.10959) 的技巧，最早应用在NMT（机器翻译）中，目前SentencePiece也有相应的实现。看起来这个技巧确实能缓解前述问题，甚至有助于增强语言模型的容错能力，所以就有了将它加进去 [BytePiece](https://kexue.fm/archives/9752) 的想法。

那么问题来了，如何将确定性分词改为随机性分词呢？BytePiece是基于Unigram模型的，它通过Viterbi算法找最大概率的分词方案，既然有概率，是否就可以自然地导出随机采样？本文来讨论这个问题，并分享自己的解决方案。

## 要点分析 [\#](https://kexue.fm/kexue.fm\#%E8%A6%81%E7%82%B9%E5%88%86%E6%9E%90)

现阶段，Unigram分词是直接输出最大概率的切分方案，通常这是一个确定性的输出。具体来说，假设$\\boldsymbol{w}=(w\_1,w\_2,\\cdots,w\_k)$代表一个切分方案，对应的打分为$P(\\boldsymbol{w})=p(w\_1)p(w\_2)\\cdots p(w\_k)$，$\\Omega(S)$代表句子$S$所有可能的切分方案的集合，那么分词算法可以描述为
\\begin{equation}\\boldsymbol{w}^\* = \\mathop{\\text{argmax}}\_{\\boldsymbol{w}\\in \\Omega(S)}P(\\boldsymbol{w})\\end{equation}
这可以通过Viterbi算法在线性时间内来完成，所以这个过程我们也称之为“Viterbi Decoding”。看起来，Unigram模型天然带有概率，所以似乎并不难将它改为依概率采样的形式，但细想之下才发现这并非一个平凡的问题，有很多细节上的困难需要克服。

笔者设想是模仿自回归语言模型设计一个递归采样流程，但这里最困难的地方是如何尽量保持原来的候选切分方案的排序不变，或者就算不能保持所有的排序不变，也至少满足最大概率不变，即Viterbi解码的最大概率路径$\\boldsymbol{w}^\*$应该对应所设计的递归采样算法的最大概率采样结果。由于所有切分方案$\\Omega(S)$构成一个有向无环图（DAG，Directed Acyclic Graph），笔者一开始以为直接在有向无环图上随机游走是一个可行方案，但再思考后发现很难设计适当的转移概率来保证最大概率路径不变（因为同一起点的不同边不是平权的，不能简单按照边的频率为权重做采样）。

## 已有方案 [\#](https://kexue.fm/kexue.fm\#%E5%B7%B2%E6%9C%89%E6%96%B9%E6%A1%88)

由于一时半会没有新想法，所以笔者决定去翻翻“参考答案”——看看Subword Regularization的原始论文 [《Subword Regularization: Improving Neural Network Translation Models with Multiple Subword Candidates》](https://papers.cool/arxiv/1804.10959) 是怎么做的。

然而，这个“标准答案”却让笔者有点哭笑不得。原来Subword Regularization的思路非常简单直接：先搜索出$P(\\boldsymbol{w})$最大的$n$个分词方案$\\boldsymbol{w}^\*\_1,\\boldsymbol{w}^\*\_2,\\cdots,\\boldsymbol{w}^\*\_n$（$n$-best segmentations），然后构建如下分布
\\begin{equation}p\_i = \\frac{P(\\boldsymbol{w}^\*\_i)^{\\alpha}}{\\sum\\limits\_{j=1}^n P(\\boldsymbol{w}^\*\_j)^{\\alpha}}\\end{equation}
对这$n$个方案进行依概率采样，其中$\\alpha > 0$是一个超参数。该算法已经集成在SentencePiece中，读者可以自行测试（使用方法参考 [这里](https://github.com/google/sentencepiece/tree/master#subword-regularization-and-bpe-dropout)）。

问题是，“简单直接”不代表着“高效”，尽管搜索top-$n$个分词方案最优方案的复杂度也是线性的（有兴趣的读者可以自行找找N-best Viterbi的资料)，但明显比只找top1的Viterbi Decoding要大很多（理论上是$n$倍复杂度），所以直接的后果是开启了随机采样后，会比确定性的分词要慢很多，所以这并非是笔者心中的理想采样方法。

## 个人思路 [\#](https://kexue.fm/kexue.fm\#%E4%B8%AA%E4%BA%BA%E6%80%9D%E8%B7%AF)

思路再度陷入了僵局。一筹莫展之际，笔者决定把思路再捋一捋：我们的目标是想要找到复杂度类似Viterbi Decoding的随机采样算法，既然如此，Viterbi Decoding本身应该是一个不错的突破点。于是，笔者再次翻开了分词代码， [当时的分词函数](https://github.com/bojone/bytepiece/blob/b65716b76938b3ac4124661a3367fc1c270373fa/bytepiece/faster.pyx) 长这个样：

```
def _tokenize(self, bytes text):
 cdef int e, k, s
 cdef double v, score
 cdef list routes = [(0, None)] + [(-INFINITY, None) for _ in text]
 cdef list tokens = []
 for e, (k, v) in self._automaton.iter(text):
 s, e = e - k + 1, e + 1
 score = routes[s][0] + v
 if score > routes[e][0]:
 routes[e] = score, s
 while text:
 s = routes[e][1]
 tokens.append(text[s:e])
 text, e = text[:s], s
 return tokens[::-1]
```

反复读了几遍，总算有了灵感：Viterbi Decoding的关键是 `if score > routes[e][0]:` 这一句，它代表保留截止到当前位置的最优切分方案，其中 `score` 是新切分方案分数（概率对数）， `routes[e][0]` 是历史最优分数，如果新方案更优则覆盖。这让笔者联想到了 [MCMC算法](https://kexue.fm/archives/8084) 的接受率设计，如果在这里引入随机采样，不就可以将分词结果随机化了？

我们用$r\\in \\{1, 0\\}$表示接受/拒绝新方案，由于这一步只是一个二元选择，所以将它概率化也非常简单：
\\begin{equation}
r\_i = \\left\\{\\begin{aligned}&\\,1\\,, \\,\\, s\_i > s\_{i-1} \\\
&\\,0\\,, \\,\\, \\text{else}\\end{aligned}\\right.\\qquad\\longrightarrow\\qquad
r\_i = \\left\\{\\begin{aligned}&\\,1\\,, \\,\\, \\varepsilon < \\sigma(\\alpha(s\_i - s\_{i-1})) \\\
&\\,0\\,, \\,\\, \\text{else}\\end{aligned}\\right.
\\end{equation}
这里$\\varepsilon\\sim U\[0,1\]$是均匀随机数，$\\alpha > 0$是超参数，$\\sigma(t)=1/(1+e^{-t})$是Sigmoid函数，$s\_i,s\_{i-1}$分别是新旧方案的得分（概率对数）。不难发现，左端的确定性采样对应$\\alpha\\to\\infty$的随机性采样。

这样，在Viterbi解码的基础上我们得到了一个非常自然、非常轻量级的随机采样算法，这里称之为“Viterbi Sampling”，实现它只需要将 `if score > routes[e][0]:` 这一判据换成带随机数的版本。由于Sigmoid函数的单调性，当$s\_i > s\_{i-1}$时，它自然会给新方案分配更大的概率，所以很明显原来的的最大概率切分在Viterbi Sampling之下也是最大概率结果，并且当$s\_i - s\_{i-1}$越大，$\\sigma(\\alpha(s\_i - s\_{i-1}))$也越大，这意味着原本得分越大的方案被采样到的概率也越高，一定程度上保持了切分方案的排序不变（尽管还没有证明一定严格保序，但从应用角度看，近似保序就够了）。

## 简单测试 [\#](https://kexue.fm/kexue.fm\#%E7%AE%80%E5%8D%95%E6%B5%8B%E8%AF%95)

从0.4.0版本开始，Viterbi Sampling就内置在BytePiece的分词函数中，只需要在 `tokenizer.tokenize` 或者 `tokenizer.encode` 时加入大于0的alpha参数，结果就是随机的：

```
import bytepiece
assert bytepiece.__version__ >= '0.4.0'

tokenizer = bytepiece.Tokenizer('bytepiece_160k.model')
text = '今天天气不错'
print(tokenizer.tokenize(text)) # alpha默认值为-1，alpha≤0 都代表确定性分词
for i in range(5):
 print(tokenizer.tokenize(text, alpha=0.1))

# [b'\xe4\xbb\x8a\xe5\xa4\xa9', b'\xe5\xa4\xa9\xe6\xb0\x94', b'\xe4\xb8\x8d\xe9\x94\x99']
# [b'\xe4\xbb\x8a\xe5\xa4\xa9', b'\xe5\xa4\xa9\xe6', b'\xb0\x94', b'\xe4\xb8\x8d\xe9\x94\x99']
# [b'\xe4\xbb\x8a\xe5\xa4\xa9', b'\xe5\xa4\xa9\xe6\xb0\x94', b'\xe4\xb8\x8d\xe9\x94\x99']
# [b'\xe4\xbb\x8a\xe5\xa4\xa9', b'\xe5\xa4\xa9\xe6\xb0\x94', b'\xe4\xb8', b'\x8d', b'\xe9\x94', b'\x99']
# [b'\xe4\xbb\x8a\xe5\xa4\xa9', b'\xe5\xa4\xa9', b'\xe6\xb0\x94', b'\xe4\xb8\x8d\xe9\x94\x99']
# [b'\xe4\xbb', b'\x8a\xe5\xa4\xa9', b'\xe5\xa4\xa9', b'\xe6\xb0\x94\xe4\xb8\x8d', b'\xe9\x94', b'\x99']

```

下面对比一下SentencePiece的Subword Regularization和BytePiece的Viterbi Sampling的速度（随机性分词时都设$\\alpha=0.1$）：
\\begin{array}{c\|cc}
\\hline
& \\text{确定性分词} & \\text{随机性分词} & \\\
\\hline
\\text{SP-BPE} & \\text{1.36M bytes/sec} & \\text{1.25M bytes/sec} \\\
\\text{SP-Unigram} & \\text{5.65M bytes/sec} & \\text{1.28M bytes/sec} \\\
\\text{BytePiece} & \\text{1.95M bytes/sec} & \\text{1.36M bytes/sec}\\\
\\hline
\\end{array}
可以看到，Subword Regularization（“SP-Unigram”这一行）开启之后，分词速度不到原来的1/4，这表明Subword Regularization的采样算法是相当低效的。相比之下，本文提出的Viterbi Sampling只下降了30%左右，效率显然更高，下降的部分在于随机数的生成和Sigmoid函数的计算，如果能进一步优化这两部分，速度还能进一步提升。至于BPE模型，它的随机分词叫做 [BPE Dropout](https://papers.cool/arxiv/1910.13267)，这是专属于BPE模型的方法，有兴趣的读者自行了解，这里就不介绍了。

## 文章小结 [\#](https://kexue.fm/kexue.fm\#%E6%96%87%E7%AB%A0%E5%B0%8F%E7%BB%93)

本文主要探讨了将Unigram分词模型的确定性分词改为随机性分词的策略。尽管已有名为“Subword Regularization”的方法可以实现这一目标，但其效率相对较低。为此，笔者提出了一种更高效的采样算法Viterbi Sampling，它仅需对确定性的Viterbi Decoding进行简单的修改，从而基本保持了原有的效率。实验证明，新的算法采样速度明显超越了Subword Regularization。相应的实现已经内置在BytePiece最新版中。

_**转载到请包括本文地址：** [https://kexue.fm/archives/9768](https://kexue.fm/archives/9768)_

_**更详细的转载事宜请参考：**_ [《科学空间FAQ》](https://kexue.fm/archives/6508#%E6%96%87%E7%AB%A0%E5%A6%82%E4%BD%95%E8%BD%AC%E8%BD%BD/%E5%BC%95%E7%94%A8)

**如果您还有什么疑惑或建议，欢迎在下方评论区继续讨论。**

**如果您觉得本文还不错，欢迎 [分享](https://kexue.fm/kexue.fm#share)/ [打赏](https://kexue.fm/kexue.fm#pay) 本文。打赏并非要从中获得收益，而是希望知道科学空间获得了多少读者的真心关注。当然，如果你无视它，也不会影响你的阅读。再次表示欢迎和感谢！**

打赏

微信打赏

支付宝打赏

因为网站后台对打赏并无记录，因此欢迎在打赏时候备注留言。你还可以 [**点击这里**](http://mail.qq.com/cgi-bin/qm_share?t=qm_mailme&email=tN7d1drY3drrx8H0xcWa19vZ) 或在下方评论区留言来告知你的建议或需求。

**如果您需要引用本文，请参考：**

苏剑林. (Sep. 16, 2023). 《随机分词浅探：从Viterbi Decoding到Viterbi Sampling 》\[Blog post\]. Retrieved from [https://kexue.fm/archives/9768](https://kexue.fm/archives/9768)

@online{kexuefm-9768,
        title={随机分词浅探：从Viterbi Decoding到Viterbi Sampling},
        author={苏剑林},
        year={2023},
        month={Sep},
        url={\\url{https://kexue.fm/archives/9768}},
}

分类： [信息时代](https://kexue.fm/category/Big-Data)    标签： [概率](https://kexue.fm/tag/%E6%A6%82%E7%8E%87/), [随机](https://kexue.fm/tag/%E9%9A%8F%E6%9C%BA/), [分词](https://kexue.fm/tag/%E5%88%86%E8%AF%8D/), [新词发现](https://kexue.fm/tag/%E6%96%B0%E8%AF%8D%E5%8F%91%E7%8E%B0/)[1 评论](https://kexue.fm/archives/9768#comments)

< [大词表语言模型在续写任务上的一个问题及对策](https://kexue.fm/archives/9762) \| [自然数集中 N = ab + c 时 a + b + c 的最小值](https://kexue.fm/archives/9775) >

### 你也许还对下面的内容感兴趣

- [n个正态随机数的最大值的渐近估计](https://kexue.fm/archives/11390)
- [一道概率不等式：盯着它到显然成立为止！](https://kexue.fm/archives/10902)
- [Softmax后传：寻找Top-K的光滑近似](https://kexue.fm/archives/10373)
- [通向最优分布之路：概率空间的最小化](https://kexue.fm/archives/10289)
- [通向概率分布之路：盘点Softmax及其替代品](https://kexue.fm/archives/10145)
- [用傅里叶级数拟合一维概率密度函数](https://kexue.fm/archives/10007)
- [随机分词再探：从Viterbi Sampling到完美采样算法](https://kexue.fm/archives/9811)
- [EMO：基于最优传输思想设计的分类损失函数](https://kexue.fm/archives/9797)
- [大词表语言模型在续写任务上的一个问题及对策](https://kexue.fm/archives/9762)
- [BytePiece：更纯粹、更高压缩率的Tokenizer](https://kexue.fm/archives/9752)

[发表你的看法](https://kexue.fm/kexue.fm#comment_form)

[随机分词再探：从Viterbi Sampling到完美采样算法 R11; AI 資訊](https://news.aitime.space/2023/10/61329/)

October 16th, 2023

\[...\]在文章《随机分词浅探：从Viterbi Decoding到Viterbi Sampling》中，笔者提出了一种名为“Viterbi Sampling”的随机分词算法，它只是在求最优解的Viterbi Decoding基础上进行小修改，保留了Viterbi算法的简单快速的特点，相比于已有的Subword Regularization明显更加高效。不过，知乎上的读者 @鶴舞 指出，当前的采样算法可能会\[...\]

[回复评论](https://kexue.fm/archives/9768/comment-page-1?replyTo=22892#respond-post-9768)

[取消回复](https://kexue.fm/archives/9768#respond-post-9768)

你的大名

电子邮箱

个人网站（选填）

1\. 可以使用LaTeX代码，点击“预览效果”可查看效果；2. 可以通过点击评论楼层编号来引用该楼层；3. 网站可能会有点卡，如非确认评论失败，请 **不要重复点击提交**。

### 内容速览

[要点分析](https://kexue.fm/kexue.fm#%E8%A6%81%E7%82%B9%E5%88%86%E6%9E%90)
[已有方案](https://kexue.fm/kexue.fm#%E5%B7%B2%E6%9C%89%E6%96%B9%E6%A1%88)
[个人思路](https://kexue.fm/kexue.fm#%E4%B8%AA%E4%BA%BA%E6%80%9D%E8%B7%AF)
[简单测试](https://kexue.fm/kexue.fm#%E7%AE%80%E5%8D%95%E6%B5%8B%E8%AF%95)
[文章小结](https://kexue.fm/kexue.fm#%E6%96%87%E7%AB%A0%E5%B0%8F%E7%BB%93)

### 智能搜索

支持整句搜索！网站自动使用 [结巴分词](https://github.com/fxsjy/jieba) 进行分词，并结合ngrams排序算法给出合理的搜索结果。

### 热门标签

[生成模型](https://kexue.fm/tag/%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B/) [attention](https://kexue.fm/tag/attention/) [优化](https://kexue.fm/tag/%E4%BC%98%E5%8C%96/) [语言模型](https://kexue.fm/tag/%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/) [模型](https://kexue.fm/tag/%E6%A8%A1%E5%9E%8B/) [网站](https://kexue.fm/tag/%E7%BD%91%E7%AB%99/) [梯度](https://kexue.fm/tag/%E6%A2%AF%E5%BA%A6/) [概率](https://kexue.fm/tag/%E6%A6%82%E7%8E%87/) [矩阵](https://kexue.fm/tag/%E7%9F%A9%E9%98%B5/) [优化器](https://kexue.fm/tag/%E4%BC%98%E5%8C%96%E5%99%A8/) [转载](https://kexue.fm/tag/%E8%BD%AC%E8%BD%BD/) [微分方程](https://kexue.fm/tag/%E5%BE%AE%E5%88%86%E6%96%B9%E7%A8%8B/) [分析](https://kexue.fm/tag/%E5%88%86%E6%9E%90/) [天象](https://kexue.fm/tag/%E5%A4%A9%E8%B1%A1/) [深度学习](https://kexue.fm/tag/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/) [积分](https://kexue.fm/tag/%E7%A7%AF%E5%88%86/) [python](https://kexue.fm/tag/python/) [扩散](https://kexue.fm/tag/%E6%89%A9%E6%95%A3/) [力学](https://kexue.fm/tag/%E5%8A%9B%E5%AD%A6/) [无监督](https://kexue.fm/tag/%E6%97%A0%E7%9B%91%E7%9D%A3/) [几何](https://kexue.fm/tag/%E5%87%A0%E4%BD%95/) [节日](https://kexue.fm/tag/%E8%8A%82%E6%97%A5/) [生活](https://kexue.fm/tag/%E7%94%9F%E6%B4%BB/) [文本生成](https://kexue.fm/tag/%E6%96%87%E6%9C%AC%E7%94%9F%E6%88%90/) [数论](https://kexue.fm/tag/%E6%95%B0%E8%AE%BA/)

### 随机文章

- [Transformer升级之路：13、逆用Leaky ReRoPE](https://kexue.fm/archives/9728)
- [Transformer升级之路：4、二维位置的旋转式位置编码](https://kexue.fm/archives/8397)
- [两百万前素数之和与前两百万素数之和](https://kexue.fm/archives/2612)
- [【NASA每日一图】经典的猎户座星云](https://kexue.fm/archives/102)
- [【中文分词系列】 3\. 字标注法与HMM模型](https://kexue.fm/archives/3922)
- [重新思考学习率与Batch Size（一）：现状](https://kexue.fm/archives/11260)
- [看完了刘亦菲版《倩女幽魂》](https://kexue.fm/archives/1333)
- [情人节？元宵节！](https://kexue.fm/archives/2360)
- [2010年全国天文奥赛终于可以报名了](https://kexue.fm/archives/332)
- [最新调查解“毒”珠江：工业水污染触目惊心！](https://kexue.fm/archives/222)

### 最近评论

- [kaiyuan](https://kexue.fm/archives/11486/comment-page-1#comment-29036): 看了“Linear Transformers Are Secretly Fast Weight...
- [sog](https://kexue.fm/archives/11469/comment-page-1#comment-29035): 好的，符号相同，搞混了呃
- [kerry](https://kexue.fm/archives/9119/comment-page-14#comment-29034): 还没有通读完后面的系列，提出一些拙见。
降低方差这一节把原本的目标“预测单步的noise”变成...
- [Kevin Yin](https://kexue.fm/archives/11158/comment-page-1#comment-29033): I wrote https://research.novelai.net/muonscale/...
- [罗](https://kexue.fm/archives/11480/comment-page-1#comment-29032): 公式(6)显示出来是不是有点小问题？
- [cmlin](https://kexue.fm/archives/11340/comment-page-1#comment-29031): 本人对这方面不太熟悉，想了解这三个条件的意义及动机，且希望这系列可以继续写下去。以下想发表一些...
- [喝一口可乐](https://kexue.fm/archives/10958/comment-page-3#comment-29030): 理解了，感谢苏神回复，数学上给出建模分析确实清晰了很多，再次感谢苏神回复！
- [CuddleSabe1](https://kexue.fm/archives/10958/comment-page-1#comment-29029): 感觉普通的 flow matching 可以看成 degrade-aware image de...
- [岁月如书](https://kexue.fm/archives/11126/comment-page-3#comment-29028): 受教了，感谢
- [苏剑林](https://kexue.fm/archives/11126/comment-page-3#comment-29027): 是

### 友情链接

- [Cool Papers](https://papers.cool)
- [数学研发](https://bbs.emath.ac.cn)
- [Seatop](http://www.seatop.com.cn/)
- [Xiaoxia](https://xiaoxia.org/)
- [积分表-网络版](https://kexue.fm/sci/integral/index.html)
- [丝路博傲](http://blog.dvxj.com/)
- [数学之家](http://www.2math.cn/)
- [有趣天文奇观](http://interesting-sky.china-vo.org/)
- [TwistedW](http://www.twistedwg.com/)
- [godweiyang](https://godweiyang.com/)
- [AI柠檬](https://blog.ailemon.net/)
- [王登科-DK博客](https://greatdk.com)
- [ESON](https://blog.eson.org/)
- [枫之羽](https://fzhiy.net/)
- [coding-zuo](https://coding-zuo.github.io/)
- [博科园](https://www.bokeyuan.net/)
- [孔皮皮的博客](https://www.kppkkp.top/)
- [运鹏的博客](https://yunpengtai.top/)
- [jiming.site](https://jiming.site/)
- [OmegaXYZ](https://www.omegaxyz.com/)
- [EAI猩球](https://www.robotech.ink/)
- [文举的博客](https://liwenju0.com/)
- [申请链接](https://kexue.fm/links.html)

本站采用创作共用版权协议，要求署名、非商业用途和保持一致。转载本站内容必须也遵循“ [署名-非商业用途-保持一致](http://creativecommons.org/licenses/by-nc-nd/2.5/cn/)”的创作共用协议。
© 2009-2025 Scientific Spaces. All rights reserved. Theme by [laogui](http://www.laogui.com). Powered by [Typecho](http://typecho.org). 备案号: [粤ICP备09093259号-1/2](https://beian.miit.gov.cn/)。