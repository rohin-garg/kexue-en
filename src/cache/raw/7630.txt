## SEARCH

## MENU

- [打赏](https://kexue.fm/reward.html)
- [公式](https://kexue.fm/latex.html)
- [天象](https://kexue.fm/ac.html)
- [链接](https://kexue.fm/links.html)
- [时光](https://kexue.fm/me.html)
- [博览](https://kexue.fm/science.html)
- [归档](https://kexue.fm/content.html)

## CATEGORIES

- [千奇百怪](https://kexue.fm/category/Everything)
- [天文探索](https://kexue.fm/category/Astronomy)
- [数学研究](https://kexue.fm/category/Mathematics)
- [物理化学](https://kexue.fm/category/Phy-chem)
- [信息时代](https://kexue.fm/category/Big-Data)
- [生物自然](https://kexue.fm/category/Biology)
- [图片摄影](https://kexue.fm/category/Photograph)
- [问题百科](https://kexue.fm/category/Questions)
- [生活/情感](https://kexue.fm/category/Life-Feeling)
- [资源共享](https://kexue.fm/category/Resources)

## NEWPOSTS

- [通过msign来计算奇异值裁剪mc...](https://kexue.fm/archives/11059)
- [矩阵符号函数mcsgn能计算什么？](https://kexue.fm/archives/11056)
- [线性注意力简史：从模仿、创新到反哺](https://kexue.fm/archives/11033)
- [msign的导数](https://kexue.fm/archives/11025)
- [通过msign来计算奇异值裁剪mc...](https://kexue.fm/archives/11006)
- [msign算子的Newton-Sc...](https://kexue.fm/archives/10996)
- [等值振荡定理：最优多项式逼近的充要条件](https://kexue.fm/archives/10972)
- [生成扩散模型漫谈（三十）：从瞬时速...](https://kexue.fm/archives/10958)
- [MoE环游记：5、均匀分布的反思](https://kexue.fm/archives/10945)
- [msign算子的Newton-Sc...](https://kexue.fm/archives/10922)

## COMMENTS

- [Leco: 请问LoRA的A,B矩阵初始化时，一个高斯随机一个全零还是只能...](https://kexue.fm/archives/9590/comment-page-2#comment-27984)
- [苏剑林: 如果你把你这里提到的数学都学通透了，数学基础基本上可以胜任95...](https://kexue.fm/archives/9119/comment-page-13#comment-27983)
- [苏剑林: 我跑过这个项目，效果是能复现的。“在 CIFAR-10 上效果...](https://kexue.fm/archives/10958/comment-page-2#comment-27982)
- [Henry Zha: 苏神你好，我是一名管理科学与工程专业的博士生，研究方向是结合人...](https://kexue.fm/archives/9119/comment-page-13#comment-27981)
- [SunlightZero: 我根据 https://github.com/haidog-y...](https://kexue.fm/archives/10958/comment-page-2#comment-27980)
- [苏剑林: 噢，是笔误，更正了，感谢指出。](https://kexue.fm/archives/11025/comment-page-1#comment-27979)
- [苏剑林: 这里有很多因素。如果推理数据跟训练数据同分布，那么理论上就是均...](https://kexue.fm/archives/10945/comment-page-1#comment-27978)
- [苏剑林: 目前看来给O加rmsnorm挺稳的，效果甚至还好点。我其实是直...](https://kexue.fm/archives/11033/comment-page-1#comment-27977)
- [苏剑林: 你应该说的是$\\exp(\\boldsymbol{Q}\\bold...](https://kexue.fm/archives/11033/comment-page-1#comment-27976)
- [苏剑林: 可以这么说吧，通过某种分母归一化的操作，导数格式都类似，毕竟公...](https://kexue.fm/archives/10831/comment-page-1#comment-27975)

## USERLOGIN

- [登录](https://kexue.fm/admin/login.php)

[科学空间\|Scientific Spaces](https://kexue.fm)

- [登录](https://kexue.fm/admin/login.php)
- [打赏](https://kexue.fm/reward.html)
- [公式](https://kexue.fm/latex.html)
- [天象](https://kexue.fm/ac.html)
- [链接](https://kexue.fm/links.html)
- [时光](https://kexue.fm/me.html)
- [博览](https://kexue.fm/science.html)
- [归档](https://kexue.fm/content.html)

渴望成为一个小飞侠

- [欢迎订阅](https://kexue.fm/feed)
- [个性邮箱](https://kexue.fm/archives/119)
- [天象信息](https://kexue.fm/ac.html)
- [观测ISS](https://kexue.fm/archives/41)
- [LaTeX](https://kexue.fm/latex.html)
- [关于博主](https://kexue.fm/me.html)

欢迎访问“科学空间”，这里将与您共同探讨自然科学，回味人生百态；也期待大家的分享～

- [**千奇百怪** Everything](https://kexue.fm/category/Everything)
- [**天文探索** Astronomy](https://kexue.fm/category/Astronomy)
- [**数学研究** Mathematics](https://kexue.fm/category/Mathematics)
- [**物理化学** Phy-chem](https://kexue.fm/category/Phy-chem)
- [**信息时代** Big-Data](https://kexue.fm/category/Big-Data)
- [**生物自然** Biology](https://kexue.fm/category/Biology)
- [**图片摄影** Photograph](https://kexue.fm/category/Photograph)
- [**问题百科** Questions](https://kexue.fm/category/Questions)
- [**生活/情感** Life-Feeling](https://kexue.fm/category/Life-Feeling)
- [**资源共享** Resources](https://kexue.fm/category/Resources)

- [**千奇百怪**](https://kexue.fm/category/Everything)
- [**天文探索**](https://kexue.fm/category/Astronomy)
- [**数学研究**](https://kexue.fm/category/Mathematics)
- [**物理化学**](https://kexue.fm/category/Phy-chem)
- [**信息时代**](https://kexue.fm/category/Big-Data)
- [**生物自然**](https://kexue.fm/category/Biology)
- [**图片摄影**](https://kexue.fm/category/Photograph)
- [**问题百科**](https://kexue.fm/category/Questions)
- [**生活/情感**](https://kexue.fm/category/Life-Feeling)
- [**资源共享**](https://kexue.fm/category/Resources)

[首页](https://kexue.fm) [信息时代](https://kexue.fm/category/Big-Data) 学会提问的BERT：端到端地从篇章中构建问答对

25Jul

# [学会提问的BERT：端到端地从篇章中构建问答对](https://kexue.fm/archives/7630)

By 苏剑林 \|
2020-07-25 \|
153624位读者\|

机器阅读理解任务，相比不少读者都有所了解了，简单来说就是从给定篇章中寻找给定问题的答案，即“篇章 \+ 问题 → 答案”这样的流程，笔者之前也写过一些关于阅读理解的文章，比如 [《基于CNN的阅读理解式问答模型：DGCNN》](https://kexue.fm/archives/5409) 等。至于问答对构建，则相当于是阅读理解的反任务，即“篇章 → 答案 \+ 问题”的流程，学术上一般直接叫“问题生成（Question Generation）”，因为大多数情况下，答案可以通过比较规则的随机选择，所以很多文章都只关心“篇章 \+ 答案 → 问题”这一步。

本文将带来一次全端到端的“ **篇章 → 答案 \+ 问题**”实践，包括模型介绍以及基于 [bert4keras](https://github.com/bojone/bert4keras) 的实现代码，欢迎读者尝试。

## 先看效果 [\#](https://kexue.fm/archives/7630\#%E5%85%88%E7%9C%8B%E6%95%88%E6%9E%9C)

> **输入篇章**：世界第二高山峰是乔戈里峰,位于中国境内。
> **生成问答**：世界第二高山峰叫什么名字?     乔戈里峰
>      乔戈利峰是世界第几高峰     第二
>
> **输入篇章**：7月28日，泰国将迎来十世王玛哈·哇集拉隆功国王的68岁诞辰。
> **生成问答**：泰国的十世王是谁     玛哈·哇集拉隆功国王
>      泰国的十世王玛哈·哇集拉隆多少岁？     68岁
>      泰国的玛哈·哇集拉隆功国王的生日是哪一天？     7月28日
>      玛哈·哇集拉隆功国王是哪个国家的     泰国
>
> **输入篇章**：泼水节，亦称宋干节，已有700年的历史，是傣族(中国)、德昂族最盛大的传统节日。
> **生成问答**：泼水节又称为什么     宋干节
>      泼水节是我国哪一民族一年中最盛大的节日？     傣族
>      泼水节有多少年历史了？     700年

## 思路分析 [\#](https://kexue.fm/archives/7630\#%E6%80%9D%E8%B7%AF%E5%88%86%E6%9E%90)

本文的目标是完全端到端地实现“篇章 → 答案 \+ 问题”，包括答案的选取也是由模型自动完成，不需要人工规则。其实说起来也很简单，就是用“BERT + UniLM”的方式来构建一个Seq2Seq模型（UniLM的Attention Mask，加上BERT的预训练权重），如果读者还不了解UniLM，欢迎先阅读 [《从语言模型到Seq2Seq：Transformer如戏，全靠Mask》](https://kexue.fm/archives/6933)。

笔者之前在文章 [《万能的seq2seq：基于seq2seq的阅读理解问答》](https://kexue.fm/archives/7115) 中也给出过通过Seq2Seq模型来做阅读理解的实现，即直接用Seq2Seq模型来构建$p\\big(\\text{答案}\\big\|\\text{篇章},\\text{问题}\\big)$，图示如下：

用Seq2Seq的思路做阅读理解

事实上，在上述模型的基础上稍微改动一下，将问题也列入生成的目标之中，就可以实现问答对生成了，即模型变为$p\\big(\\text{问题},\\text{答案}\\big\|\\text{篇章}\\big)$，如下图：

稍微修改一下，用来做问答对生成

但是，直觉上不难想到“篇章 → 答案”、“篇章 \+ 答案 → 问题”的难度应该是低于“篇章 \+ 问题 → 答案”的，所以我们将问题和答案的生成顺序调换一下，变为$p\\big(\\text{答案},\\text{问题}\\big\|\\text{篇章}\\big)$，最终的效果会更好：

先生成答案，再生成问题，效果更好些

## 实现分析 [\#](https://kexue.fm/archives/7630\#%E5%AE%9E%E7%8E%B0%E5%88%86%E6%9E%90)

模型就介绍到这里了，其实也没什么好说的，就是确定好哪些是输入、哪些是输出，然后“BERT + UniLM”套上去就行了。下面是笔者的参考实现：

> [**task\_question\_answer\_generation\_by\_seq2seq.py**](https://github.com/bojone/bert4keras/blob/master/examples/task_question_answer_generation_by_seq2seq.py)

这里值得讨论的是解码的思路。一般的Seq2Seq模型，解码到一个\[SEP\]就结束了，而本文的模型需要解码到两个\[SEP\]才能结束，截止到第一个\[SEP\]的是答案，而两个\[SEP\]之间的则是问题。理论上来说，从给定篇章中我们可以构建很多问答对，换句话说目标不是唯一的，所以我们不能用Beam Search之类的确定性解码算法，而是要用随机解码算法（相关概念可以参考 [《如何应对Seq2Seq中的“根本停不下来”问题？》](https://kexue.fm/archives/7500#%E8%A7%A3%E7%A0%81%E7%AE%97%E6%B3%95) 中的“解码算法”一节）。

但问题是，如果完全使用随机解码算法，那么生成的问题会过于“天马行空”，也就是可能会出现一些跟篇章无关的内容，比如篇章是“我国火星探测器天问一号发射成功”，生成的问题可能是“我国第一颗人造卫星是什么”，虽然相关，但是过于发散了。所以，这里建议使用一个折中的策略：用随机解码来生成答案，然后用确定性解码来生成问题，这样能尽量保证问题的可靠性。当然，如果读者更关心生成问题的多样性，那么全部使用随机解码也行，反正就自己调试啦。

读者还需要注意的是，上述参考脚本中并没有对答案进行约束，那么生成的答案可能并不是篇章中的片段。毕竟这只是个参考实现，离实用还有一定距离，请有兴趣的读者根据自己的需求自行理解和修改代码。此外，由于问答对构建已经完全变成了一个Seq2Seq问题，所以用来提升Seq2Seq性能的技巧都可以用来提高问答对的生成质量，比如之前讨论过的 [《Seq2Seq中Exposure Bias现象的浅析与对策》](https://kexue.fm/archives/7259)，这些都交给读者自己尝试了。

## 文章小结 [\#](https://kexue.fm/archives/7630\#%E6%96%87%E7%AB%A0%E5%B0%8F%E7%BB%93)

本文是一次端到端的问答对生成实践，主要是基于“BERT + UniLM”的Seq2Seq模型来直接根据篇章生成答案和问题，并讨论了关于解码的策略。总的来讲，本文的模型没有什么特殊之处，但是因为借助了BERT的预训练权重，最终生成的问答对质量颇有可圈可点之处。

_**转载到请包括本文地址：** [https://kexue.fm/archives/7630](https://kexue.fm/archives/7630)_

_**更详细的转载事宜请参考：**_ [《科学空间FAQ》](https://kexue.fm/archives/6508#%E6%96%87%E7%AB%A0%E5%A6%82%E4%BD%95%E8%BD%AC%E8%BD%BD/%E5%BC%95%E7%94%A8)

**如果您还有什么疑惑或建议，欢迎在下方评论区继续讨论。**

**如果您觉得本文还不错，欢迎 [分享](https://kexue.fm/archives/7630#share)/ [打赏](https://kexue.fm/archives/7630#pay) 本文。打赏并非要从中获得收益，而是希望知道科学空间获得了多少读者的真心关注。当然，如果你无视它，也不会影响你的阅读。再次表示欢迎和感谢！**

打赏

微信打赏

支付宝打赏

因为网站后台对打赏并无记录，因此欢迎在打赏时候备注留言。你还可以 [**点击这里**](http://mail.qq.com/cgi-bin/qm_share?t=qm_mailme&email=tN7d1drY3drrx8H0xcWa19vZ) 或在下方评论区留言来告知你的建议或需求。

**如果您需要引用本文，请参考：**

苏剑林. (Jul. 25, 2020). 《学会提问的BERT：端到端地从篇章中构建问答对 》\[Blog post\]. Retrieved from [https://kexue.fm/archives/7630](https://kexue.fm/archives/7630)

@online{kexuefm-7630,
        title={学会提问的BERT：端到端地从篇章中构建问答对},
        author={苏剑林},
        year={2020},
        month={Jul},
        url={\\url{https://kexue.fm/archives/7630}},
}

分类： [信息时代](https://kexue.fm/category/Big-Data)    标签： [语言模型](https://kexue.fm/tag/%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/), [文本生成](https://kexue.fm/tag/%E6%96%87%E6%9C%AC%E7%94%9F%E6%88%90/)[66 评论](https://kexue.fm/archives/7630#comments)

< [通过互信息思想来缓解类别不平衡问题](https://kexue.fm/archives/7615) \| [我们真的需要把训练集的损失降低到零吗？](https://kexue.fm/archives/7643) >

### 你也许还对下面的内容感兴趣

- [Transformer升级之路：20、MLA好在哪里?（上）](https://kexue.fm/archives/10907)
- [Transformer升级之路：19、第二类旋转位置编码](https://kexue.fm/archives/10862)
- [Decoder-only的LLM为什么需要位置编码？](https://kexue.fm/archives/10347)
- [Monarch矩阵：计算高效的稀疏型矩阵分解](https://kexue.fm/archives/10249)
- [缓存与效果的极限拉扯：从MHA、MQA、GQA到MLA](https://kexue.fm/archives/10091)
- [时空之章：将Attention视为平方复杂度的RNN](https://kexue.fm/archives/10017)
- [我在Performer中发现了Transformer-VQ的踪迹](https://kexue.fm/archives/9862)
- [预训练一下，Transformer的长序列成绩还能涨不少！](https://kexue.fm/archives/9787)
- [脑洞大开：非线性RNN居然也可以并行计算？](https://kexue.fm/archives/9783)
- [大词表语言模型在续写任务上的一个问题及对策](https://kexue.fm/archives/9762)

[发表你的看法](https://kexue.fm/archives/7630#comment_form)

1. [«](https://kexue.fm/archives/7630/comment-page-1#comments)
2. [1](https://kexue.fm/archives/7630/comment-page-1#comments)
3. [2](https://kexue.fm/archives/7630/comment-page-2#comments)

lxy444

November 20th, 2020

苏神你好，请教一下，如果基于多轮问答来构造一段文本输出，比如基于多轮问诊生成一段病历描述，请问有什么好的思路吗

[回复评论](https://kexue.fm/archives/7630/comment-page-2?replyTo=14857#respond-post-7630)

[苏剑林](https://kexue.fm) 发表于
November 21st, 2020

1\. 准备标注数据
2\. 套用seq2seq模型

[回复评论](https://kexue.fm/archives/7630/comment-page-2?replyTo=14861#respond-post-7630)

ppma

April 7th, 2021

感谢分享，请问就是使用task\_question\_answer\_generation\_by\_seq2seq.py代码中的默认超参达到文章开头提到的效果吗？

[回复评论](https://kexue.fm/archives/7630/comment-page-2?replyTo=16033#respond-post-7630)

[苏剑林](https://kexue.fm) 发表于
April 8th, 2021

是。

[回复评论](https://kexue.fm/archives/7630/comment-page-2?replyTo=16041#respond-post-7630)

yuelan

July 8th, 2021

感谢分享，我想请问一下你的task\_question\_answer\_generation\_by\_seq2seq.py代码中按照你本文的例子去生成问答对，如何可以做到生成这么多不同的对呢。
我尝试随机采样3个答案来生成问题，大部分情况出来的答案都是相同的。

[回复评论](https://kexue.fm/archives/7630/comment-page-2?replyTo=16850#respond-post-7630)

yuelan 发表于
July 8th, 2021

补充：对于同一个passage,重复qag.generate后生成的问答对也基本相同的（用的本文的例子）

[回复评论](https://kexue.fm/archives/7630/comment-page-2?replyTo=16854#respond-post-7630)

[苏剑林](https://kexue.fm) 发表于
July 8th, 2021

这本身就取决于你训练数据集的多样性，出现这样的结果，说明标注人员在标注数据的时候思维方向都很单一。

你可以尝试修改解码过程，将a、q的解码方式都改为random\_sample

[回复评论](https://kexue.fm/archives/7630/comment-page-2?replyTo=16861#respond-post-7630)

yuelan 发表于
July 9th, 2021

非常感谢回答。我想请问我用的是本文提供的百度知道和搜狗的数据集，如何可以复现您在文章中展示的效果呢

[回复评论](https://kexue.fm/archives/7630/comment-page-2?replyTo=16870#respond-post-7630)

[苏剑林](https://kexue.fm) 发表于
July 10th, 2021

我就是直接训练复现的，并没有保留什么。可以多保留几轮模型，对比一下效果。

[回复评论](https://kexue.fm/archives/7630/comment-page-2?replyTo=16879#respond-post-7630)

wnn 发表于
December 11th, 2021

同学，请问你是否能实现展示的效果了呢？我也遇到了同样的问题，期待你的回复～

[回复评论](https://kexue.fm/archives/7630/comment-page-2?replyTo=18001#respond-post-7630)

pppb

July 27th, 2021

苏神，我在训练的时候报了这个错误，这个是什么问题，搜了一下好像没有对应的解决办法，代码仍然在跑和更新权重。这个要怎么解决？

832/1000 \[=======================>......\] - ETA: 59s - loss: 0.7593Traceback (most recent call last):
File "qag\_with\_917.py", line 179, in
callbacks=\[evaluator\]
File "/dssg/home/zn\_hwm/.local/lib/python3.7/site-packages/keras/engine/training.py", line 1147, in fit
initial\_epoch=initial\_epoch)
File "/dssg/home/zn\_hwm/.local/lib/python3.7/site-packages/keras/legacy/interfaces.py", line 91, in wrapper
return func(\*args, \*\*kwargs)
File "/dssg/home/zn\_hwm/.local/lib/python3.7/site-packages/keras/engine/training.py", line 1732, in fit\_generator
initial\_epoch=initial\_epoch)
File "/dssg/home/zn\_hwm/.local/lib/python3.7/site-packages/keras/engine/training\_generator.py", line 220, in fit\_generator
reset\_metrics=False)
File "/dssg/home/zn\_hwm/.local/lib/python3.7/site-packages/keras/engine/training.py", line 1514, in train\_on\_batch
outputs = self.train\_function(ins)
File "/dssg/home/zn\_hwm/.conda/envs/corpus/lib/python3.7/site-packages/tensorflow/python/keras/backend.py", line 3253, in \_\_call\_\_
session = get\_session(inputs)
File "/dssg/home/zn\_hwm/.conda/envs/corpus/lib/python3.7/site-packages/tensorflow/python/keras/backend.py", line 462, in get\_session
\_initialize\_variables(session)
File "/dssg/home/zn\_hwm/.conda/envs/corpus/lib/python3.7/site-packages/tensorflow/python/keras/backend.py", line 873, in \_initialize\_variables
if not getattr(v, '\_keras\_initialized', False):
KeyboardInterrupt

[回复评论](https://kexue.fm/archives/7630/comment-page-2?replyTo=17017#respond-post-7630)

[苏剑林](https://kexue.fm) 发表于
July 28th, 2021

我没看到这是什么错误，这是你自己ctrl+c引起的错误吧...

[回复评论](https://kexue.fm/archives/7630/comment-page-2?replyTo=17024#respond-post-7630)

pppb

August 3rd, 2021

苏神你好呀，如果我是针对一个文段（可以提问好几个问答）来生成问答对的话，是不是比较难，因为模型最后生成的只有一个问答对。如果将文段分句后训练，又会出现分句会缺乏主语，或者有代词。这种问题怎么解决比较好？如果在分句后的句子开头都添加缺失的主语进行训练，训练完后，待生成问答对的句子也同样在开头添加对应的主语进行输入。这样可行吗？

[回复评论](https://kexue.fm/archives/7630/comment-page-2?replyTo=17051#respond-post-7630)

[苏剑林](https://kexue.fm) 发表于
August 5th, 2021

理论上，（本文的）模型最后可以生成无穷多的问答对，不存在只生成一个的问题。

[回复评论](https://kexue.fm/archives/7630/comment-page-2?replyTo=17057#respond-post-7630)

earl 发表于
October 19th, 2022

你好，你的工作有后续吗，主语缺失是如何解决的

[回复评论](https://kexue.fm/archives/7630/comment-page-2?replyTo=20132#respond-post-7630)

bugmaker

September 7th, 2021

苏神，我这边标注了1k+条数据，然后都是比较类似的数据，后面测试的数据也是跟这些类似的。step\_epoch是取数据量/batchsize？吗，epoch，step\_epoch，学习率这些取值有什么建议的吗？

[回复评论](https://kexue.fm/archives/7630/comment-page-2?replyTo=17286#respond-post-7630)

[苏剑林](https://kexue.fm) 发表于
September 8th, 2021

建议自行调试，形成自己的炼丹经验。

[回复评论](https://kexue.fm/archives/7630/comment-page-2?replyTo=17293#respond-post-7630)

ppap

March 27th, 2022

那如果我问题和答案都是同一个解码方式，如果数据量大的话。那是不是可以直接答案和问题之间可以不用SEP间隔了？直接CLS passge SEP a+q SEP输入。还是说加个SEP的话，模型会更好的识别答案的结束位置？

[回复评论](https://kexue.fm/archives/7630/comment-page-2?replyTo=18788#respond-post-7630)

[苏剑林](https://kexue.fm) 发表于
March 29th, 2022

就是为了区分答案的位置而已，你不用SEP用其他特殊标记也行。

[回复评论](https://kexue.fm/archives/7630/comment-page-2?replyTo=18800#respond-post-7630)

lishaojie

December 29th, 2023

请问苏神，打标时，答案是不是越短越准确？问题也是同样的道理吧？
问题或答案可以间隔字或词提取吗？

[回复评论](https://kexue.fm/archives/7630/comment-page-2?replyTo=23393#respond-post-7630)

[苏剑林](https://kexue.fm) 发表于
December 30th, 2023

现在可以直接用LLM提取了吧？

这篇文章的模型是生成式的，跟LLM本质上同理，所以理论上可以不连续，但实际效果如何是未知的。

[回复评论](https://kexue.fm/archives/7630/comment-page-2?replyTo=23411#respond-post-7630)

lishaojie 发表于
December 30th, 2023

感谢苏神！回头我对比一下

[回复评论](https://kexue.fm/archives/7630/comment-page-2?replyTo=23413#respond-post-7630)

shimin

April 23rd, 2024

我想问一下基于unilm的掩码机制让只有编码器的nezha模型可以用于文本生成任务，那面对未登录词，怎么使用copy机制了？copy机制不是需要编码器和解码器？这里好像不太好区分unilm+nezha中的编码器和解码器

[回复评论](https://kexue.fm/archives/7630/comment-page-2?replyTo=24191#respond-post-7630)

[苏剑林](https://kexue.fm) 发表于
April 25th, 2024

一般情况下使用subword-based的tokenizer，或者干脆使用目前比较流行的sentencepiece等，出现未登录词的概率几乎没有。

[回复评论](https://kexue.fm/archives/7630/comment-page-2?replyTo=24204#respond-post-7630)

Rnanprince

July 18th, 2024

你好，生成的答案太短了，，我调大max\_a\_len，会有错误tensorflow.python.framework.errors\_impl.InvalidArgumentError: Incompatible shapes: \[32,548,768\] vs. \[1,512,768\]

[回复评论](https://kexue.fm/archives/7630/comment-page-2?replyTo=24862#respond-post-7630)

[苏剑林](https://kexue.fm) 发表于
July 18th, 2024

训练集就只有这么短，调大也没用。

[回复评论](https://kexue.fm/archives/7630/comment-page-2?replyTo=24872#respond-post-7630)

Rnanprince 发表于
July 19th, 2024

我自己的的数据集中，的answer很长，所以想调大，，但是总是维度不匹配，请问有啥思路吗

[回复评论](https://kexue.fm/archives/7630/comment-page-2?replyTo=24878#respond-post-7630)

[苏剑林](https://kexue.fm) 发表于
July 24th, 2024

BERT的总长度不能超过512，看上去你的错误信息是因为超过了这个总长度。

另外就是如果有可能的话，直接用各种LLM api来生成效果更好。

[回复评论](https://kexue.fm/archives/7630/comment-page-2?replyTo=24892#respond-post-7630)

1. [«](https://kexue.fm/archives/7630/comment-page-1#comments)
2. [1](https://kexue.fm/archives/7630/comment-page-1#comments)
3. [2](https://kexue.fm/archives/7630/comment-page-2#comments)

[取消回复](https://kexue.fm/archives/7630#respond-post-7630)

你的大名

电子邮箱

个人网站（选填）

1\. 可以使用LaTeX代码，点击“预览效果”可查看效果；2. 可以通过点击评论楼层编号来引用该楼层；3. 网站可能会有点卡，如非确认评论失败，请 **不要重复点击提交**。

### 内容速览

[先看效果](https://kexue.fm/archives/7630#%E5%85%88%E7%9C%8B%E6%95%88%E6%9E%9C)
[思路分析](https://kexue.fm/archives/7630#%E6%80%9D%E8%B7%AF%E5%88%86%E6%9E%90)
[实现分析](https://kexue.fm/archives/7630#%E5%AE%9E%E7%8E%B0%E5%88%86%E6%9E%90)
[文章小结](https://kexue.fm/archives/7630#%E6%96%87%E7%AB%A0%E5%B0%8F%E7%BB%93)

### 智能搜索

支持整句搜索！网站自动使用 [结巴分词](https://github.com/fxsjy/jieba) 进行分词，并结合ngrams排序算法给出合理的搜索结果。

### 热门标签

[生成模型](https://kexue.fm/tag/%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B/) [attention](https://kexue.fm/tag/attention/) [优化](https://kexue.fm/tag/%E4%BC%98%E5%8C%96/) [语言模型](https://kexue.fm/tag/%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/) [模型](https://kexue.fm/tag/%E6%A8%A1%E5%9E%8B/) [网站](https://kexue.fm/tag/%E7%BD%91%E7%AB%99/) [概率](https://kexue.fm/tag/%E6%A6%82%E7%8E%87/) [梯度](https://kexue.fm/tag/%E6%A2%AF%E5%BA%A6/) [转载](https://kexue.fm/tag/%E8%BD%AC%E8%BD%BD/) [微分方程](https://kexue.fm/tag/%E5%BE%AE%E5%88%86%E6%96%B9%E7%A8%8B/) [矩阵](https://kexue.fm/tag/%E7%9F%A9%E9%98%B5/) [天象](https://kexue.fm/tag/%E5%A4%A9%E8%B1%A1/) [分析](https://kexue.fm/tag/%E5%88%86%E6%9E%90/) [深度学习](https://kexue.fm/tag/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/) [积分](https://kexue.fm/tag/%E7%A7%AF%E5%88%86/) [python](https://kexue.fm/tag/python/) [优化器](https://kexue.fm/tag/%E4%BC%98%E5%8C%96%E5%99%A8/) [力学](https://kexue.fm/tag/%E5%8A%9B%E5%AD%A6/) [无监督](https://kexue.fm/tag/%E6%97%A0%E7%9B%91%E7%9D%A3/) [扩散](https://kexue.fm/tag/%E6%89%A9%E6%95%A3/) [几何](https://kexue.fm/tag/%E5%87%A0%E4%BD%95/) [节日](https://kexue.fm/tag/%E8%8A%82%E6%97%A5/) [生活](https://kexue.fm/tag/%E7%94%9F%E6%B4%BB/) [文本生成](https://kexue.fm/tag/%E6%96%87%E6%9C%AC%E7%94%9F%E6%88%90/) [数论](https://kexue.fm/tag/%E6%95%B0%E8%AE%BA/)

### 随机文章

- [线性Attention的探索：Attention必须有个Softmax吗？](https://kexue.fm/archives/7546)
- [重温SSM（三）：HiPPO的高效计算（S4）](https://kexue.fm/archives/10162)
- [基于CNN和序列标注的对联机器人](https://kexue.fm/archives/6270)
- [结果恒为整数的多项式](https://kexue.fm/archives/3103)
- [从费马大定理谈起（七）：费马平方和定理](https://kexue.fm/archives/2886)
- [关于无理数及其和的证明](https://kexue.fm/archives/44)
- [【NASA每日一图】土星上的春分](https://kexue.fm/archives/99)
- [力的无穷分解与格林函数法](https://kexue.fm/archives/3092)
- [《虚拟的实在(4)》——质量是什么](https://kexue.fm/archives/2036)
- [让人惊叹的Johnson-Lindenstrauss引理：理论篇](https://kexue.fm/archives/8679)

### 最近评论

- [Leco](https://kexue.fm/archives/9590/comment-page-2#comment-27984): 请问LoRA的A,B矩阵初始化时，一个高斯随机一个全零还是只能A高斯，B全零呢？
- [苏剑林](https://kexue.fm/archives/9119/comment-page-13#comment-27983): 如果你把你这里提到的数学都学通透了，数学基础基本上可以胜任95%以上的场景了吧？至于“直觉”这...
- [苏剑林](https://kexue.fm/archives/10958/comment-page-2#comment-27982): 我跑过这个项目，效果是能复现的。“在 CIFAR-10 上效果非常差，生成的图片都是模糊的”是...
- [Henry Zha](https://kexue.fm/archives/9119/comment-page-13#comment-27981): 苏神你好，我是一名管理科学与工程专业的博士生，研究方向是结合人工智能模型建模用户行为之类的管理...
- [SunlightZero](https://kexue.fm/archives/10958/comment-page-2#comment-27980): 我根据 https://github.com/haidog-yaqub/MeanFlow 尝试...
- [苏剑林](https://kexue.fm/archives/11025/comment-page-1#comment-27979): 噢，是笔误，更正了，感谢指出。
- [苏剑林](https://kexue.fm/archives/10945/comment-page-1#comment-27978): 这里有很多因素。如果推理数据跟训练数据同分布，那么理论上就是均匀分布，但实际上同分布假设不一定...
- [苏剑林](https://kexue.fm/archives/11033/comment-page-1#comment-27977): 目前看来给O加rmsnorm挺稳的，效果甚至还好点。我其实是直接在flash attentio...
- [苏剑林](https://kexue.fm/archives/11033/comment-page-1#comment-27976): 你应该说的是$\\exp(\\boldsymbol{Q}\\boldsymbol{K}^{\\top}...
- [苏剑林](https://kexue.fm/archives/10831/comment-page-1#comment-27975): 可以这么说吧，通过某种分母归一化的操作，导数格式都类似，毕竟公式$(f/g)'=f'/g-fg...

### 友情链接

- [Cool Papers](https://papers.cool)
- [数学研发](https://bbs.emath.ac.cn)
- [Seatop](http://www.seatop.com.cn/)
- [Xiaoxia](https://xiaoxia.org/)
- [积分表-网络版](https://kexue.fm/sci/integral/index.html)
- [丝路博傲](http://blog.dvxj.com/)
- [ph4ntasy 饭特稀](http://www.ph4ntasy.com/)
- [数学之家](http://www.2math.cn/)
- [有趣天文奇观](http://interesting-sky.china-vo.org/)
- [TwistedW](http://www.twistedwg.com/)
- [godweiyang](https://godweiyang.com/)
- [AI柠檬](https://blog.ailemon.net/)
- [王登科-DK博客](https://greatdk.com)
- [ESON](https://blog.eson.org/)
- [枫之羽](https://fzhiy.net/)
- [Mathor's blog](https://wmathor.com/)
- [coding-zuo](https://coding-zuo.github.io/)
- [博科园](https://www.bokeyuan.net/)
- [孔皮皮的博客](https://www.kppkkp.top/)
- [运鹏的博客](https://yunpengtai.top/)
- [jiming.site](https://jiming.site/)
- [OmegaXYZ](https://www.omegaxyz.com/)
- [Blog by Eacls](https://www.eacls.top/)
- [EAI猩球](https://www.robotech.ink/)
- [文举的博客](https://liwenju0.com/)
- [用代码打点酱油](https://bruceyuan.com/)
- [申请链接](https://kexue.fm/links.html)

本站采用创作共用版权协议，要求署名、非商业用途和保持一致。转载本站内容必须也遵循“ [署名-非商业用途-保持一致](http://creativecommons.org/licenses/by-nc-nd/2.5/cn/)”的创作共用协议。
© 2009-2025 Scientific Spaces. All rights reserved. Theme by [laogui](http://www.laogui.com). Powered by [Typecho](http://typecho.org). 备案号: [粤ICP备09093259号-1/2](https://beian.miit.gov.cn/)。