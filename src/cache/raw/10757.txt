## SEARCH

## MENU

- [打赏](https://kexue.fm/reward.html)
- [公式](https://kexue.fm/latex.html)
- [天象](https://kexue.fm/ac.html)
- [链接](https://kexue.fm/links.html)
- [时光](https://kexue.fm/me.html)
- [博览](https://kexue.fm/science.html)
- [归档](https://kexue.fm/content.html)

## CATEGORIES

- [千奇百怪](https://kexue.fm/category/Everything)
- [天文探索](https://kexue.fm/category/Astronomy)
- [数学研究](https://kexue.fm/category/Mathematics)
- [物理化学](https://kexue.fm/category/Phy-chem)
- [信息时代](https://kexue.fm/category/Big-Data)
- [生物自然](https://kexue.fm/category/Biology)
- [图片摄影](https://kexue.fm/category/Photograph)
- [问题百科](https://kexue.fm/category/Questions)
- [生活/情感](https://kexue.fm/category/Life-Feeling)
- [资源共享](https://kexue.fm/category/Resources)

## NEWPOSTS

- [让炼丹更科学一些（四）：新恒等式，...](https://kexue.fm/archives/11494)
- [为什么DeltaNet要加L2 N...](https://kexue.fm/archives/11486)
- [让炼丹更科学一些（三）：SGD的终...](https://kexue.fm/archives/11480)
- [让炼丹更科学一些（二）：将结论推广...](https://kexue.fm/archives/11469)
- [滑动平均视角下的权重衰减和学习率](https://kexue.fm/archives/11459)
- [生成扩散模型漫谈（三十一）：预测数...](https://kexue.fm/archives/11428)
- [Muon优化器指南：快速上手与关键细节](https://kexue.fm/archives/11416)
- [AdamW的Weight RMS的...](https://kexue.fm/archives/11404)
- [n个正态随机数的最大值的渐近估计](https://kexue.fm/archives/11390)
- [流形上的最速下降：5\. 对偶梯度下降](https://kexue.fm/archives/11388)

## COMMENTS

- [李双良: 您好，我想请教下当batch size增加a倍时，单个epoc...](https://kexue.fm/archives/11459/comment-page-1#comment-29075)
- [jackory: 我测了一下RL finetune 100B+ MoE过程中使用...](https://kexue.fm/archives/11267/comment-page-1#comment-29074)
- [TinyZhang: 求教，拉普拉斯一节中，为什么需要d logp\_max(z)/d...](https://kexue.fm/archives/11390/comment-page-1#comment-29072)
- [chievesky: 苏老师您好，感谢您的分享，每次看您的文章都收获颇丰！我有一个不...](https://kexue.fm/archives/11428/comment-page-1#comment-29071)
- [kyokusanagi: 写得很不错](https://kexue.fm/archives/1791/comment-page-1#comment-29070)
- [kyokusanagi: 嗯，我正在寻找关于黎卡提方程的一般通解公式，确很难](https://kexue.fm/archives/1794/comment-page-1#comment-29069)
- [sgnxotsmicf2: 大佬！！！，moonlight严格遵循其公式$m\_t=\\bet...](https://kexue.fm/archives/11416/comment-page-1#comment-29068)
- [TommyJiang: 苏神怎么看 DeepSeek 新出的 mHC，是不是和 Muo...](https://kexue.fm/archives/11494/comment-page-1#comment-29067)
- [冯君贵: 苏老师你好，我想请教你一个东西，就是学会的模型滚动优化能有一次...](https://kexue.fm/archives/10958/comment-page-3#comment-29065)
- [JamesSand: 谢谢苏神我发现问题在 muon 的 adjust lr 上边如...](https://kexue.fm/archives/11416/comment-page-1#comment-29064)

## USERLOGIN

- [登录](https://kexue.fm/admin/login.php)

[科学空间\|Scientific Spaces](https://kexue.fm)

- [登录](https://kexue.fm/admin/login.php)
- [打赏](https://kexue.fm/reward.html)
- [公式](https://kexue.fm/latex.html)
- [天象](https://kexue.fm/ac.html)
- [链接](https://kexue.fm/links.html)
- [时光](https://kexue.fm/me.html)
- [博览](https://kexue.fm/science.html)
- [归档](https://kexue.fm/content.html)

渴望成为一个小飞侠

- [欢迎订阅](https://kexue.fm/feed)
- [个性邮箱](https://kexue.fm/archives/119)
- [天象信息](https://kexue.fm/ac.html)
- [观测ISS](https://kexue.fm/archives/41)
- [LaTeX](https://kexue.fm/latex.html)
- [关于博主](https://kexue.fm/me.html)

欢迎访问“科学空间”，这里将与您共同探讨自然科学，回味人生百态；也期待大家的分享～

- [**千奇百怪** Everything](https://kexue.fm/category/Everything)
- [**天文探索** Astronomy](https://kexue.fm/category/Astronomy)
- [**数学研究** Mathematics](https://kexue.fm/category/Mathematics)
- [**物理化学** Phy-chem](https://kexue.fm/category/Phy-chem)
- [**信息时代** Big-Data](https://kexue.fm/category/Big-Data)
- [**生物自然** Biology](https://kexue.fm/category/Biology)
- [**图片摄影** Photograph](https://kexue.fm/category/Photograph)
- [**问题百科** Questions](https://kexue.fm/category/Questions)
- [**生活/情感** Life-Feeling](https://kexue.fm/category/Life-Feeling)
- [**资源共享** Resources](https://kexue.fm/category/Resources)

- [**千奇百怪**](https://kexue.fm/category/Everything)
- [**天文探索**](https://kexue.fm/category/Astronomy)
- [**数学研究**](https://kexue.fm/category/Mathematics)
- [**物理化学**](https://kexue.fm/category/Phy-chem)
- [**信息时代**](https://kexue.fm/category/Big-Data)
- [**生物自然**](https://kexue.fm/category/Biology)
- [**图片摄影**](https://kexue.fm/category/Photograph)
- [**问题百科**](https://kexue.fm/category/Questions)
- [**生活/情感**](https://kexue.fm/category/Life-Feeling)
- [**资源共享**](https://kexue.fm/category/Resources)

[首页](https://kexue.fm) [信息时代](https://kexue.fm/category/Big-Data) MoE环游记：3、换个思路来分配

5Mar

# [MoE环游记：3、换个思路来分配](https://kexue.fm/archives/10757)

By 苏剑林 \|
2025-03-05 \|
109444位读者\|

这篇文章我们继续探讨MoE的负载均衡问题。在上一篇文章 [《MoE环游记：2、不患寡而患不均》](https://kexue.fm/archives/10735) 中，我们主要讨论了通过Aux Loss来促进负载均衡的思路。Aux Loss固然简单直观，但它也有一个明显的缺点——权重不好调——调低了无法促进均衡，调高了容易损害LM Loss，所以业界一直有寻找替代方案的尝试。

本文要分享的是名为“Loss-Free”的方案，由DeepSeek在 [《Auxiliary-Loss-Free Load Balancing Strategy for Mixture-of-Experts》](https://papers.cool/arxiv/2408.15664) 提出。和DeepSeek众多耀眼的开源作品相比，这篇论文也许不算起眼，但在笔者看来，它潜在的学术影响力可能远超其他工作，因为所提方法不仅简单有效，而且极具普适性，堪称经典。

## 方法大意 [\#](https://kexue.fm/kexue.fm\#%E6%96%B9%E6%B3%95%E5%A4%A7%E6%84%8F)

面对负载不均衡，Aux Loss的应对思路是通过额外的损失引导Router给出均衡的打分，而Loss-Free的想法则是换个新的分配思路，即不改变Router现有打分结果，而是改变$\\mathop{\\text{argtop}}\_k \\boldsymbol{\\rho}$这个分配方式。

其实这个方向此前也有过一些努力。比如2021年Facebook提出了 [BASE Layer](https://papers.cool/arxiv/2103.16716)，将Expert的分配视为 [线性指派问题](https://en.wikipedia.org/wiki/Assignment_problem)，即以负载均衡为约束条件，求在该约束之下Router总打分尽可能高的分配结果，这可以用 [匈牙利算法](https://en.wikipedia.org/wiki/Hungarian_algorithm) 等来解决。但该方案需要知道全体Token的打分，所以对于自回归式LLM来说，它只适用于训练，推理还是只能用$\\mathop{\\text{argtop}}\_k \\boldsymbol{\\rho}$，训练推理存在不一致性，并且由于目前求解算法的限制，它只适用于$k=1$的场景。

相比之下，Loss-Free的做法非常简单且有效，它留意到一个事实，即我们总可以引入一个偏置项$\\boldsymbol{b}$，使得$\\mathop{\\text{argtop}}\_k \\boldsymbol{\\rho} + \\boldsymbol{b}$的分配是均衡的，所以它将MoE的形式改为
\\begin{equation}\\boldsymbol{y} = \\sum\_{i\\in \\mathop{\\text{argtop}}\_k \\boldsymbol{\\rho}} \\rho\_i \\boldsymbol{e}\_i\\qquad\\to\\qquad \\boldsymbol{y} = \\sum\_{i\\in \\mathop{\\text{argtop}}\_k \\boldsymbol{\\rho} + \\boldsymbol{b}} \\rho\_i \\boldsymbol{e}\_i\\end{equation}
这里的$\\boldsymbol{b}$是输入无关的向量，由训练过程确定下来，训练完后它就保持不变，因此推理阶段也可以用，换言之训练和推理具有一致的形式。注意乘以$\\boldsymbol{e}\_i$的还是$\\rho\_i$而不是$\\rho\_i + b\_i$，也就是说$\\boldsymbol{b}$仅仅参与分配过程而不参与MoE的前向计算，所以我们对$\\boldsymbol{b}$或$\\boldsymbol{\\rho} + \\boldsymbol{b}$的正负性都没有特殊要求。

## 手搓梯度 [\#](https://kexue.fm/kexue.fm\#%E6%89%8B%E6%90%93%E6%A2%AF%E5%BA%A6)

怎么训练$\\boldsymbol{b}$呢？我们知道，$\\boldsymbol{b}$的优化方向自然是促进负载均衡，为此按照上一篇的记号，我们先定义$\\boldsymbol{f}=\[f\_1,f\_2,\\cdots,f\_n\]$：
\\begin{equation}f\_i = \\left\\{\\begin{aligned}1/k, \\quad i\\in \\mathop{\\text{argtop}}\\nolimits\_k \\boldsymbol{\\rho}+\\boldsymbol{b} \\\
0, \\quad i\\not\\in \\mathop{\\text{argtop}}\\nolimits\_k \\boldsymbol{\\rho}+\\boldsymbol{b}\\end{aligned}\\right.\\end{equation}
以及$\\boldsymbol{F}=\\mathbb{E}\[\\boldsymbol{f}\]$，这里的$\\boldsymbol{F}$自然就是在$\\boldsymbol{b}$偏置下Expert当前的负载分布了。借着我们定义均匀分布为$\\boldsymbol{Q}=(1/n,1/n,\\cdots,1/n)$，那么负载均衡就相当于最小化
\\begin{equation}\\mathcal{L}\_{\\text{aux}} = \\frac{1}{2}\\Vert\\boldsymbol{F} - \\boldsymbol{Q}\\Vert^2 = \\frac{1}{2}\\sum\_{i=1}^n (F\_i - 1/n)^2\\end{equation}
这个目标是不可导的，但有了上一篇的经验，我们知道STE（Straight-Through Estimator）可以解决这个问题。STE的关键是找一个可导且跟$\\boldsymbol{F}$具有同增减趋势的量作为$\\boldsymbol{F}$的光滑近似，这里我们的优化参数只有$\\boldsymbol{b}$，而它正好具有我们期望的性质（增大$b\_i$，$i$被选中的概率就更高，那么$F\_i$就更大），所以答案就呼之欲出了：
\\begin{equation}\\mathcal{L}\_{\\text{aux}} = \\frac{1}{2}\\Vert\\boldsymbol{b} + \\text{sg}\[\\boldsymbol{F}-\\boldsymbol{b}\] - \\boldsymbol{Q}\\Vert^2 = \\frac{1}{2}\\sum\_{i=1}^n (b\_i + \\text{sg}\[F\_i - b\_i\] - 1/n)^2\\end{equation}
它的梯度是
\\begin{equation}\\nabla\_{\\boldsymbol{b}}\\mathcal{L}\_{\\text{aux}} = \\frac{1}{2}\\nabla\_{\\boldsymbol{b}}\\Vert\\boldsymbol{b} + \\text{sg}\[\\boldsymbol{F}-\\boldsymbol{b}\] - \\boldsymbol{Q}\\Vert^2 = \\boldsymbol{F} - \\boldsymbol{Q}\\end{equation}
所以用梯度下降（SGD）来更新$\\boldsymbol{b}$就是
\\begin{equation}\\boldsymbol{b}\\leftarrow \\boldsymbol{b} - \\alpha (\\boldsymbol{F} - \\boldsymbol{Q})\\end{equation}
这里$\\alpha$是$\\boldsymbol{b}$的学习率。不过Loss-Free最终选择的更新规则略有不同，它选择的是符号梯度下降（SignSGD）：
\\begin{equation}\\boldsymbol{b}\\leftarrow \\boldsymbol{b} - \\alpha \\mathop{\\text{sign}}(\\boldsymbol{F} - \\boldsymbol{Q})\\label{eq:aux-loss-free}\\end{equation}
这个结果其实也很好理解，就是如果$F\_i$比$1/n$大，那么就调小一点$b\_i$，否则就增大一点$b\_i$。

## 改良版本 [\#](https://kexue.fm/kexue.fm\#%E6%94%B9%E8%89%AF%E7%89%88%E6%9C%AC)

除了加$\\mathop{\\text{sign}}$的符号梯度下降外，笔者发现直接对$\\boldsymbol{F} - \\boldsymbol{Q}$做RMS Norm（即Normalized SGD），在相同的$\\alpha$下往往能达到更好的均衡效果：
\\begin{equation}\\boldsymbol{b}\\leftarrow \\boldsymbol{b} - \\alpha\\frac{\\boldsymbol{F} - \\boldsymbol{Q}}{\\text{RMS}(\\boldsymbol{F} - \\boldsymbol{Q})}\\end{equation}
这里的$\\text{RMS}$是“Root Mean Square”，定义为
\\begin{equation}\\text{RMS}(\\boldsymbol{F} - \\boldsymbol{Q}) = \\sqrt{\\frac{1}{n}\\sum\_{i=1}^n (F\_i - Q\_i)^2}\\end{equation}
不难看出，加$\\mathop{\\text{sign}}$后的$\\mathop{\\text{sign}}(\\boldsymbol{F} - \\boldsymbol{Q})$和加RMS Norm后的$\\frac{\\boldsymbol{F} - \\boldsymbol{Q}}{\\text{RMS}(\\boldsymbol{F} - \\boldsymbol{Q})}$，它们的$\\text{RMS}$都是1，因此它们俩尺度上是大致相同的，所以我们可以使用相同的$\\alpha$。

简单来说，$\\mathop{\\text{sign}}$的问题在于不论$F\_i$与目标$Q\_i$的远近都使用同样的更新幅度，这导致原本就已经跟$Q\_i$比较接近的$F\_i$反而容易偏离原本已经达到的均衡，从而产生震荡；而RMS Norm则保留了$F\_i-Q\_i$之间的相对大小，更新幅度更加自适应一些，理论上更有助于促进均衡，实测效果也多是它更好。

## 一脉相承 [\#](https://kexue.fm/kexue.fm\#%E4%B8%80%E8%84%89%E7%9B%B8%E6%89%BF)

原论文在介绍Loss-Free时，并没有上述Aux Loss的推导过程，而是直接给出式$\\eqref{eq:aux-loss-free}$的更新规则，给人的感觉是给$\\boldsymbol{b}$“手搓”了梯度$\\mathop{\\text{sign}}(\\boldsymbol{F} - \\boldsymbol{Q})$，这也是它Loss-Free这个名字的来源。

然而，从本文给出的推导可以看出，更新规则$\\eqref{eq:aux-loss-free}$也完全可以从Aux Loss视角得到，两者是一脉相承的。看起来Loss-Free最直接的好处是不用调Aux Loss权重了，但它实际上也有个学习率参数$\\alpha$要调，尽管原论文已经帮我们搜好$\\alpha=0.001$这个默认值，但不可否认这个超参数是存在的。

在笔者看来，Loss-Free的本质创新并不是没有Aux Loss，而是隔离了Aux Loss和LM Loss的优化参数，从而达到了负载均衡和模型能力两不误的效果。其中最关键一步，是留意到“一个偏置项足以达到负载均衡”这一事实，然后就让Aux Loss只优化新引入的偏置$\\boldsymbol{b}$，而LM Loss则优化剩余参数，让Aux Loss对LM Loss的负面作用降到最低。

相比之下，常规的Aux Loss方案需要全体参数来促进负载均衡，而LM Loss优化的也是全体参数，两者的优化方向可能并不完全兼容，因此想找到一个最优的平衡点相对来说就更为困难。所以，Loss-Free基于“一个偏置项足以达到负载均衡”将两个Loss的优化参数隔离开来，是负载均衡问题的一个绝妙的解决办法。

## 相关细节 [\#](https://kexue.fm/kexue.fm\#%E7%9B%B8%E5%85%B3%E7%BB%86%E8%8A%82)

尽管Loss-Free已经足够简单明了，但是在使用的时候还要稍微注意一些细节。

首先，对于每个Batch的数据，我们应当先根据LM Loss来更新模型参数，然后再根据式$\\eqref{eq:aux-loss-free}$来更新$\\boldsymbol{b}$。这是因为$\\boldsymbol{b}$的更新依赖于全体Token的统计信息$\\boldsymbol{F}$，先更新$\\boldsymbol{b}$再更新模型其余参数的话，原则上会有泄漏未来信息的风险。虽然直观看来就一个向量$\\boldsymbol{b}$泄漏不了多少信息，但这个风险终归是存在的，因此要尽量去规避它。

其次，刚才我们说原论文已经调好$\\alpha=0.001$，但这个结果可能跟原论文用Sigmoid作为Router $\\boldsymbol{\\rho}$激活函数的选择是绑定的。原因也不难想，经过Sigmoid后，每个$\\rho\_i$相对比较独立，并且都在$(0,1)$内，$\\alpha=0.001$相当于说每一步的更新幅度约为千分之一，如果换Softmax、ReLU或者其他激活函数，那么就可能需要重调$\\alpha$了。

针对这个问题，笔者建议的做法是解耦Gate和Bias所用的激活函数，即
\\begin{equation}\\boldsymbol{y} = \\sum\_{i\\in \\mathop{\\text{argtop}}\_k \\boldsymbol{\\rho} + \\boldsymbol{b}} \\rho\_i \\boldsymbol{e}\_i\\qquad\\to\\qquad \\boldsymbol{y} = \\sum\_{i\\in \\mathop{\\text{argtop}}\_k \\boldsymbol{\\rho}^{(\\sigma)} + \\boldsymbol{b}} \\rho\_i^{(h)} \\boldsymbol{e}\_i\\end{equation}
其中$\\boldsymbol{\\rho}^{(\\sigma)} = \\sigma(\\boldsymbol{x}\\boldsymbol{W}^{(R)}), \\boldsymbol{\\rho}^{(h)} = h(\\boldsymbol{x}\\boldsymbol{W}^{(R)})$，$\\sigma(\\cdot)$是Sigmoid函数，$h(\\cdot)$是任意单调且值域非负的函数，说白了就是加上$\\boldsymbol{b}$的是Sigmoid激活的打分，这样我们就可以复用$\\alpha=0.001$，至于乘上Expert的Gate，我们可以用其他激活函数，只要它的单调性跟Sigmoid一致就行。

此外，由于更新规则$\\eqref{eq:aux-loss-free}$加了$\\text{sign}$函数，因此有可能训出绝对值大于1的$b\_i$，整体绝对值还可能越来越大，这些都是正常的，对模型效果不会有影响。实际上$\\boldsymbol{b}$有一个冗余的自由度，因为全体$b\_i$都加上同一个常数后，$\\mathop{\\text{argtop}}\_k \\boldsymbol{\\rho} + \\boldsymbol{b}$的结果不变。这个额外的自由度我们可以用来做其他好玩的事情（且听下回分解）。

## 延伸思考 [\#](https://kexue.fm/kexue.fm\#%E5%BB%B6%E4%BC%B8%E6%80%9D%E8%80%83)

除了MoE的负载均衡之外，Loss-Free的思想还可以应用到很多类似问题，比如VQ-VQE的编码表坍缩（Codebook Collapse），就可以用同样思路解决，而且相比之前介绍的“ [旋转技巧](https://kexue.fm/archives/10489)”、“ [线性变换技巧](https://kexue.fm/archives/10519)”显得更自然和普适。事实上，本文开篇的评价“Loss-Free潜在的学术影响力可能远超其他工作”，正是基于Loss-Free的普适性考虑的。

抛开具体的应用背景，从数学上来看，Loss-Free的贡献可以理解为给出了用梯度下降来求解指派问题的方法。一个经典的线性指派问题可以表示为：
\\begin{equation}\\min\_f \\sum\_{i=1}^n c\_{i, f(i)}\\end{equation}
其中$c\_{i,j}$是给定的成本函数，$f$是$\\{1,2,\\cdots,n\\}$到自身的双射。放到本文的背景下，$c\_{i,j}$不就相当于$n$个Token、$n$个Expert的打分，所求$f$不就是一个负载均衡的分配方案？求解此类问题的一般想法是在满足约束条件的空间里搜索尽可能优的解，而Loss-Free则反过来，先构建一个最优但不一定满足约束条件的解：
\\begin{equation}f(i) = \\mathop{\\text{argmin}}\_j c\_{i,j}\\end{equation}
这个解在分数上肯定是最优的，但不一定满足双射的条件，这里不满足双射就等价于负载不均衡。于是我们引入偏置
\\begin{equation}f(i) = \\mathop{\\text{argmin}}\_j c\_{i,j} + b\_j\\end{equation}
$b\_j$初始化为零，然后根据式$\\eqref{eq:aux-loss-free}$来更新，更新规则说白了就是哪个$j$出现出现次数多，那减少相应的$b\_j$，反之增加，直到出现双射为止。

## 文章小结 [\#](https://kexue.fm/kexue.fm\#%E6%96%87%E7%AB%A0%E5%B0%8F%E7%BB%93)

本文介绍了MoE负载均衡问题的Loss-Free方法，它由DeepSeek提出，其核心在于通过引入一个简单的偏置项来实现负载均衡。本文进一步思考了它与Aux Loss的联系，以及它在类似数学问题上的应用潜力。

_**转载到请包括本文地址：** [https://kexue.fm/archives/10757](https://kexue.fm/archives/10757)_

_**更详细的转载事宜请参考：**_ [《科学空间FAQ》](https://kexue.fm/archives/6508#%E6%96%87%E7%AB%A0%E5%A6%82%E4%BD%95%E8%BD%AC%E8%BD%BD/%E5%BC%95%E7%94%A8)

**如果您还有什么疑惑或建议，欢迎在下方评论区继续讨论。**

**如果您觉得本文还不错，欢迎 [分享](https://kexue.fm/kexue.fm#share)/ [打赏](https://kexue.fm/kexue.fm#pay) 本文。打赏并非要从中获得收益，而是希望知道科学空间获得了多少读者的真心关注。当然，如果你无视它，也不会影响你的阅读。再次表示欢迎和感谢！**

打赏

微信打赏

支付宝打赏

因为网站后台对打赏并无记录，因此欢迎在打赏时候备注留言。你还可以 [**点击这里**](http://mail.qq.com/cgi-bin/qm_share?t=qm_mailme&email=tN7d1drY3drrx8H0xcWa19vZ) 或在下方评论区留言来告知你的建议或需求。

**如果您需要引用本文，请参考：**

苏剑林. (Mar. 05, 2025). 《MoE环游记：3、换个思路来分配 》\[Blog post\]. Retrieved from [https://kexue.fm/archives/10757](https://kexue.fm/archives/10757)

@online{kexuefm-10757,
        title={MoE环游记：3、换个思路来分配},
        author={苏剑林},
        year={2025},
        month={Mar},
        url={\\url{https://kexue.fm/archives/10757}},
}

分类： [信息时代](https://kexue.fm/category/Big-Data)    标签： [最优](https://kexue.fm/tag/%E6%9C%80%E4%BC%98/), [损失函数](https://kexue.fm/tag/%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0/), [梯度](https://kexue.fm/tag/%E6%A2%AF%E5%BA%A6/), [moe](https://kexue.fm/tag/moe/)[69 评论](https://kexue.fm/archives/10757#comments)

< [Muon续集：为什么我们选择尝试Muon？](https://kexue.fm/archives/10739) \| [初探MuP：超参数的跨模型尺度迁移规律](https://kexue.fm/archives/10770) >

### 你也许还对下面的内容感兴趣

- [滑动平均视角下的权重衰减和学习率](https://kexue.fm/archives/11459)
- [生成扩散模型漫谈（三十一）：预测数据而非噪声](https://kexue.fm/archives/11428)
- [AdamW的Weight RMS的渐近估计（下）](https://kexue.fm/archives/11404)
- [DiVeQ：一种非常简洁的VQ训练方案](https://kexue.fm/archives/11328)
- [AdamW的Weight RMS的渐近估计（上）](https://kexue.fm/archives/11307)
- [为什么Adam的Update RMS是0.2？](https://kexue.fm/archives/11267)
- [重新思考学习率与Batch Size（一）：现状](https://kexue.fm/archives/11260)
- [msign的导数](https://kexue.fm/archives/11025)
- [等值振荡定理：最优多项式逼近的充要条件](https://kexue.fm/archives/10972)
- [MoE环游记：5、均匀分布的反思](https://kexue.fm/archives/10945)

[发表你的看法](https://kexue.fm/kexue.fm#comment_form)

1. [«](https://kexue.fm/archives/10757/comment-page-2#comments)
2. [1](https://kexue.fm/archives/10757/comment-page-1#comments)
3. [2](https://kexue.fm/archives/10757/comment-page-2#comments)
4. [3](https://kexue.fm/archives/10757/comment-page-3#comments)

silver

July 9th, 2025

求问文中的$e\_i$是啥？是ds论文的“Algorithm 1”中提到的“violation error”么

[回复评论](https://kexue.fm/archives/10757/comment-page-3?replyTo=28060#respond-post-10757)

[苏剑林](https://kexue.fm) 发表于
July 11th, 2025

你要不看看本系列第一篇？

[回复评论](https://kexue.fm/archives/10757/comment-page-3?replyTo=28079#respond-post-10757)

silver 发表于
July 20th, 2025

懂了，感谢

[回复评论](https://kexue.fm/archives/10757/comment-page-3?replyTo=28203#respond-post-10757)

liuy

November 11th, 2025

比如2021年Facebook提出了BASE Layer。请问一下这个的效果是很差吗，它给我的感觉和 dropout 差不多，不选 topk 专家感觉上类似于dropout掉对应专家（drop策略从随机变成使得负载均衡的策略）。

[回复评论](https://kexue.fm/archives/10757/comment-page-3?replyTo=28799#respond-post-10757)

[苏剑林](https://kexue.fm) 发表于
November 18th, 2025

我没实验过，但感觉太随机了，猜测稳定性不好。

[回复评论](https://kexue.fm/archives/10757/comment-page-3?replyTo=28831#respond-post-10757)

1. [«](https://kexue.fm/archives/10757/comment-page-2#comments)
2. [1](https://kexue.fm/archives/10757/comment-page-1#comments)
3. [2](https://kexue.fm/archives/10757/comment-page-2#comments)
4. [3](https://kexue.fm/archives/10757/comment-page-3#comments)

[取消回复](https://kexue.fm/archives/10757#respond-post-10757)

你的大名

电子邮箱

个人网站（选填）

1\. 可以使用LaTeX代码，点击“预览效果”可查看效果；2. 可以通过点击评论楼层编号来引用该楼层；3. 网站可能会有点卡，如非确认评论失败，请 **不要重复点击提交**。

### 内容速览

[方法大意](https://kexue.fm/kexue.fm#%E6%96%B9%E6%B3%95%E5%A4%A7%E6%84%8F)
[手搓梯度](https://kexue.fm/kexue.fm#%E6%89%8B%E6%90%93%E6%A2%AF%E5%BA%A6)
[改良版本](https://kexue.fm/kexue.fm#%E6%94%B9%E8%89%AF%E7%89%88%E6%9C%AC)
[一脉相承](https://kexue.fm/kexue.fm#%E4%B8%80%E8%84%89%E7%9B%B8%E6%89%BF)
[相关细节](https://kexue.fm/kexue.fm#%E7%9B%B8%E5%85%B3%E7%BB%86%E8%8A%82)
[延伸思考](https://kexue.fm/kexue.fm#%E5%BB%B6%E4%BC%B8%E6%80%9D%E8%80%83)
[文章小结](https://kexue.fm/kexue.fm#%E6%96%87%E7%AB%A0%E5%B0%8F%E7%BB%93)

### 智能搜索

支持整句搜索！网站自动使用 [结巴分词](https://github.com/fxsjy/jieba) 进行分词，并结合ngrams排序算法给出合理的搜索结果。

### 热门标签

[生成模型](https://kexue.fm/tag/%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B/) [attention](https://kexue.fm/tag/attention/) [优化](https://kexue.fm/tag/%E4%BC%98%E5%8C%96/) [语言模型](https://kexue.fm/tag/%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/) [模型](https://kexue.fm/tag/%E6%A8%A1%E5%9E%8B/) [网站](https://kexue.fm/tag/%E7%BD%91%E7%AB%99/) [梯度](https://kexue.fm/tag/%E6%A2%AF%E5%BA%A6/) [概率](https://kexue.fm/tag/%E6%A6%82%E7%8E%87/) [矩阵](https://kexue.fm/tag/%E7%9F%A9%E9%98%B5/) [优化器](https://kexue.fm/tag/%E4%BC%98%E5%8C%96%E5%99%A8/) [转载](https://kexue.fm/tag/%E8%BD%AC%E8%BD%BD/) [微分方程](https://kexue.fm/tag/%E5%BE%AE%E5%88%86%E6%96%B9%E7%A8%8B/) [分析](https://kexue.fm/tag/%E5%88%86%E6%9E%90/) [天象](https://kexue.fm/tag/%E5%A4%A9%E8%B1%A1/) [深度学习](https://kexue.fm/tag/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/) [积分](https://kexue.fm/tag/%E7%A7%AF%E5%88%86/) [python](https://kexue.fm/tag/python/) [扩散](https://kexue.fm/tag/%E6%89%A9%E6%95%A3/) [力学](https://kexue.fm/tag/%E5%8A%9B%E5%AD%A6/) [无监督](https://kexue.fm/tag/%E6%97%A0%E7%9B%91%E7%9D%A3/) [几何](https://kexue.fm/tag/%E5%87%A0%E4%BD%95/) [节日](https://kexue.fm/tag/%E8%8A%82%E6%97%A5/) [生活](https://kexue.fm/tag/%E7%94%9F%E6%B4%BB/) [文本生成](https://kexue.fm/tag/%E6%96%87%E6%9C%AC%E7%94%9F%E6%88%90/) [数论](https://kexue.fm/tag/%E6%95%B0%E8%AE%BA/)

### 随机文章

- [【NASA每日一图】微波背景辐射双极化](https://kexue.fm/archives/117)
- [关于e,i,π的那些鲜为人知的事儿...](https://kexue.fm/archives/1434)
- [流形上的最速下降：4\. Muon + 谱球面](https://kexue.fm/archives/11241)
- [从动力学角度看优化算法（五）：为什么学习率不宜过小？](https://kexue.fm/archives/7787)
- [当时七夕笑牵牛](https://kexue.fm/archives/1705)
- [【备忘】谈谈dropout](https://kexue.fm/archives/4521)
- [第一学期结束了](https://kexue.fm/archives/352)
- [从费马大定理谈起（十一）：有理点与切割线法](https://kexue.fm/archives/2996)
- [\[电子书\]《最小作用量原理与物理学的发展》](https://kexue.fm/archives/2055)
- [混沌的世界——“星之轨迹”的研究](https://kexue.fm/archives/1525)

### 最近评论

- [李双良](https://kexue.fm/archives/11459/comment-page-1#comment-29075): 您好，我想请教下当batch size增加a倍时，单个epoch内的训练迭代次数减小a倍，如果...
- [jackory](https://kexue.fm/archives/11267/comment-page-1#comment-29074): 我测了一下RL finetune 100B+ MoE过程中使用Adamw优化的 RMS nor...
- [TinyZhang](https://kexue.fm/archives/11390/comment-page-1#comment-29072): 求教，拉普拉斯一节中，为什么需要d logp\_max(z)/dz = 0，而不是d p\_max...
- [chievesky](https://kexue.fm/archives/11428/comment-page-1#comment-29071): 苏老师您好，感谢您的分享，每次看您的文章都收获颇丰！我有一个不太理解的点，您在文章中说如果有一...
- [kyokusanagi](https://kexue.fm/archives/1791/comment-page-1#comment-29070): 写得很不错
- [kyokusanagi](https://kexue.fm/archives/1794/comment-page-1#comment-29069): 嗯，我正在寻找关于黎卡提方程的一般通解公式，确很难
- [sgnxotsmicf2](https://kexue.fm/archives/11416/comment-page-1#comment-29068): 大佬！！！，moonlight严格遵循其公式$m\_t=\\beta m\_{t-1}+g\_t$，但...
- [TommyJiang](https://kexue.fm/archives/11494/comment-page-1#comment-29067): 苏神怎么看 DeepSeek 新出的 mHC，是不是和 Muon 的思想有点类似，都是从向量升...
- [冯君贵](https://kexue.fm/archives/10958/comment-page-3#comment-29065): 苏老师你好，我想请教你一个东西，就是学会的模型滚动优化能有一次性迭代多步的效果好吗？就是如果用...
- [JamesSand](https://kexue.fm/archives/11416/comment-page-1#comment-29064): 谢谢苏神我发现问题在 muon 的 adjust lr 上边如果我用 spectral\_nor...

### 友情链接

- [Cool Papers](https://papers.cool)
- [数学研发](https://bbs.emath.ac.cn)
- [Seatop](http://www.seatop.com.cn/)
- [Xiaoxia](https://xiaoxia.org/)
- [积分表-网络版](https://kexue.fm/sci/integral/index.html)
- [丝路博傲](http://blog.dvxj.com/)
- [数学之家](http://www.2math.cn/)
- [有趣天文奇观](http://interesting-sky.china-vo.org/)
- [TwistedW](http://www.twistedwg.com/)
- [godweiyang](https://godweiyang.com/)
- [AI柠檬](https://blog.ailemon.net/)
- [王登科-DK博客](https://greatdk.com)
- [ESON](https://blog.eson.org/)
- [枫之羽](https://fzhiy.net/)
- [coding-zuo](https://coding-zuo.github.io/)
- [博科园](https://www.bokeyuan.net/)
- [孔皮皮的博客](https://www.kppkkp.top/)
- [运鹏的博客](https://yunpengtai.top/)
- [jiming.site](https://jiming.site/)
- [OmegaXYZ](https://www.omegaxyz.com/)
- [EAI猩球](https://www.robotech.ink/)
- [文举的博客](https://liwenju0.com/)
- [申请链接](https://kexue.fm/links.html)

本站采用创作共用版权协议，要求署名、非商业用途和保持一致。转载本站内容必须也遵循“ [署名-非商业用途-保持一致](http://creativecommons.org/licenses/by-nc-nd/2.5/cn/)”的创作共用协议。
© 2009-2026 Scientific Spaces. All rights reserved. Theme by [laogui](http://www.laogui.com). Powered by [Typecho](http://typecho.org). 备案号: [粤ICP备09093259号-1/2](https://beian.miit.gov.cn/)。