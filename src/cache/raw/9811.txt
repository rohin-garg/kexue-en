## SEARCH

## MENU

- [打赏](https://kexue.fm/reward.html)
- [公式](https://kexue.fm/latex.html)
- [天象](https://kexue.fm/ac.html)
- [链接](https://kexue.fm/links.html)
- [时光](https://kexue.fm/me.html)
- [博览](https://kexue.fm/science.html)
- [归档](https://kexue.fm/content.html)

## CATEGORIES

- [千奇百怪](https://kexue.fm/category/Everything)
- [天文探索](https://kexue.fm/category/Astronomy)
- [数学研究](https://kexue.fm/category/Mathematics)
- [物理化学](https://kexue.fm/category/Phy-chem)
- [信息时代](https://kexue.fm/category/Big-Data)
- [生物自然](https://kexue.fm/category/Biology)
- [图片摄影](https://kexue.fm/category/Photograph)
- [问题百科](https://kexue.fm/category/Questions)
- [生活/情感](https://kexue.fm/category/Life-Feeling)
- [资源共享](https://kexue.fm/category/Resources)

## NEWPOSTS

- [重新思考学习率与Batch Siz...](https://kexue.fm/archives/11285)
- [重新思考学习率与Batch Siz...](https://kexue.fm/archives/11280)
- [为什么Adam的Update RM...](https://kexue.fm/archives/11267)
- [重新思考学习率与Batch Siz...](https://kexue.fm/archives/11260)
- [Cool Papers更新：简单适...](https://kexue.fm/archives/11250)
- [流形上的最速下降：4\. Muon ...](https://kexue.fm/archives/11241)
- [ReLU/GeLU/Swish的一...](https://kexue.fm/archives/11233)
- [流形上的最速下降：3\. Muon ...](https://kexue.fm/archives/11221)
- [流形上的最速下降：2\. Muon ...](https://kexue.fm/archives/11215)
- [基于树莓派Zero2W搭建一个随身旁路由](https://kexue.fm/archives/11206)

## COMMENTS

- [zyp: 您好，我在反向过程中有个疑问，您在\
μ(xt)=1αt(xt−...](https://kexue.fm/archives/9119/comment-page-13#comment-28572)
- [歪门正道: 喔，我看前面的文章设定有n >> d，如果deltanet中分...](https://kexue.fm/archives/11033/comment-page-2#comment-28571)
- [歪门正道: 你好苏老师，没有理解单位下三角矩阵求逆，复杂度是n^2那一步。...](https://kexue.fm/archives/11033/comment-page-2#comment-28570)
- [doggy: 苏神好，请问能提供调 kimi 的 prompt 吗？我在浏览...](https://kexue.fm/archives/9907/comment-page-4#comment-28569)
- [markchin: 苏佬发下大图吗？看了《银河铁道之夜》后深受触动，1459294...](https://kexue.fm/archives/443/comment-page-1#comment-28567)
- [kkkfm: 苏老师您好，我想请问一下您提到的使用Muon可以用更大的lea...](https://kexue.fm/archives/10592/comment-page-2#comment-28565)
- [flyingDog: 将K,V视作语料对(k1,v1),(k2,v2),⋯,(kt,...](https://kexue.fm/archives/11033/comment-page-2#comment-28564)
- [简明: 请教一下各位，（6）是有限项之和从而支撑抽样定理，但（6）的推...](https://kexue.fm/archives/3266/comment-page-1#comment-28563)
- [胡韬: 特征值的模长小于一？](https://kexue.fm/archives/11158/comment-page-1#comment-28561)
- [guanchanghao: 苏老师，最开始的物理中热传导方程的解为什么是左边那幅图呢？？没...](https://kexue.fm/archives/9359/comment-page-1#comment-28560)

## USERLOGIN

- [登录](https://kexue.fm/admin/login.php)

[科学空间\|Scientific Spaces](https://kexue.fm)

- [登录](https://kexue.fm/admin/login.php)
- [打赏](https://kexue.fm/reward.html)
- [公式](https://kexue.fm/latex.html)
- [天象](https://kexue.fm/ac.html)
- [链接](https://kexue.fm/links.html)
- [时光](https://kexue.fm/me.html)
- [博览](https://kexue.fm/science.html)
- [归档](https://kexue.fm/content.html)

渴望成为一个小飞侠

- [欢迎订阅](https://kexue.fm/feed)
- [个性邮箱](https://kexue.fm/archives/119)
- [天象信息](https://kexue.fm/ac.html)
- [观测ISS](https://kexue.fm/archives/41)
- [LaTeX](https://kexue.fm/latex.html)
- [关于博主](https://kexue.fm/me.html)

欢迎访问“科学空间”，这里将与您共同探讨自然科学，回味人生百态；也期待大家的分享～

- [**千奇百怪** Everything](https://kexue.fm/category/Everything)
- [**天文探索** Astronomy](https://kexue.fm/category/Astronomy)
- [**数学研究** Mathematics](https://kexue.fm/category/Mathematics)
- [**物理化学** Phy-chem](https://kexue.fm/category/Phy-chem)
- [**信息时代** Big-Data](https://kexue.fm/category/Big-Data)
- [**生物自然** Biology](https://kexue.fm/category/Biology)
- [**图片摄影** Photograph](https://kexue.fm/category/Photograph)
- [**问题百科** Questions](https://kexue.fm/category/Questions)
- [**生活/情感** Life-Feeling](https://kexue.fm/category/Life-Feeling)
- [**资源共享** Resources](https://kexue.fm/category/Resources)

- [**千奇百怪**](https://kexue.fm/category/Everything)
- [**天文探索**](https://kexue.fm/category/Astronomy)
- [**数学研究**](https://kexue.fm/category/Mathematics)
- [**物理化学**](https://kexue.fm/category/Phy-chem)
- [**信息时代**](https://kexue.fm/category/Big-Data)
- [**生物自然**](https://kexue.fm/category/Biology)
- [**图片摄影**](https://kexue.fm/category/Photograph)
- [**问题百科**](https://kexue.fm/category/Questions)
- [**生活/情感**](https://kexue.fm/category/Life-Feeling)
- [**资源共享**](https://kexue.fm/category/Resources)

[首页](https://kexue.fm) [信息时代](https://kexue.fm/category/Big-Data) 随机分词再探：从Viterbi Sampling到完美采样算法

16Oct

# [随机分词再探：从Viterbi Sampling到完美采样算法](https://kexue.fm/archives/9811)

By 苏剑林 \|
2023-10-16 \|
39278位读者\|

在文章 [《随机分词浅探：从Viterbi Decoding到Viterbi Sampling》](https://kexue.fm/archives/9768) 中，笔者提出了一种名为“Viterbi Sampling”的随机分词算法，它只是在求最优解的Viterbi Decoding基础上进行小修改，保留了Viterbi算法的简单快速的特点，相比于已有的 [Subword Regularization](https://papers.cool/arxiv/1804.10959) 明显更加高效。不过，知乎上的读者 [@鶴舞](https://www.zhihu.com/people/11f5cd888268129be2b1d9b298387f0d) 指出，当前的采样算法可能会在多次二选一“稀释”了部分方案的出现概率，直接后果是原本分数最高的切分并不是以最高概率出现。

经过仔细思考后，笔者发现相应的问题确实存在，当时为了尽快得到一种新的采样算法，在细节上的思考和处理确实比较粗糙。为此，本文将进一步完善Viterbi Sampling算法，并证明完善后的算法在效果上可以跟Subword Regularization等价的。

## 问题分析 [\#](https://kexue.fm/kexue.fm\#%E9%97%AE%E9%A2%98%E5%88%86%E6%9E%90)

首先，我们来看一下 [评论原话](https://zhuanlan.zhihu.com/p/658440073)：

> subword regularization中可以保证按概率数据（具有temperature超参数）。提出的方法中对于每个e，第一个算出的route会被多次1v1“挑战”，最终概率分布会不会和已有算法差蛮多的。
> 举个例子，watching三种分法watch ing，wat ching，和w atching概率都是三分之一，提出的方案的采样概率概率就会变成，前两个的概率是四分之一，第三个的概率是二分之一，是这样的吗？

其实评论里边已经说得很清晰了，如果读者还不理解的话，这里笔者稍微再展开一下。假设有三种切分方案，每种方案的得分都一样，那么我们自然是期望采样过程中每种方案的出现概率都是$1/3$。然而，Viterbi Sampling是将多选一的采样过程转化为多步的二选一：
\\begin{equation}
r\_i = \\left\\{\\begin{aligned}&\\,1\\,, \\,\\, s\_i > s\_{i-1} \\\
&\\,0\\,, \\,\\, \\text{else}\\end{aligned}\\right.\\qquad\\longrightarrow\\qquad
r\_i = \\left\\{\\begin{aligned}&\\,1\\,, \\,\\, \\varepsilon < \\sigma(\\alpha(s\_i - s\_{i-1})) \\\
&\\,0\\,, \\,\\, \\text{else}\\end{aligned}\\right.
\\end{equation}
这样一来，前面的两种切分方案先二选一，概率都是$\\frac{1/3}{1/3+1/3}=1/2$；选出来一个结果之后，又跟第三种方案放一起来二选一，由于概率是按照各自得分来算的，所以这时候各自的概率还是$1/2$。于是，在完整的采样过程中，前两种方案出现的概率是$1/4$，后一种方案出现的概率是$1/2$，越晚出现的方案相对来说越“占便宜”，而越早出现的方案概率被稀释得越严重。而很不巧的是，按照BytePiece的AC自动机的返回顺序，越长的词（通常来说得分越高）出现的次序会越靠前，所以在Viterbi Sampling中，得分越高的方案反而更容易被稀释概率。

## 解决办法 [\#](https://kexue.fm/kexue.fm\#%E8%A7%A3%E5%86%B3%E5%8A%9E%E6%B3%95)

现在看来，其实解决办法也很简单，每次进行二选一后，同时缓存累积概率就可以了，而从第二步开始，每次二选一时新进来的候选者不是跟已有候选者得分二选一，而是跟累积概率得分二选一，这就是俗称“ [水塘采样（Reservoir sampling）](https://en.wikipedia.org/wiki/Reservoir_sampling)”的算法。

用前面的例子来说，先进来两种切分方案，按照$\\frac{1/3}{1/3+1/3}=1/2$的概率选出一种，然后它们总的累积概率是$2/3$；接下来被选者跟新方案选一，新出现的方案被选到的概率应该是$\\frac{1/3}{2/3+1/3}=1/3$，也就是说要跟累积概率比，而不是跟被选者自己的概率比，这样完整的采样流程下来，每种切分方案出现的概率都是$1/3$。

对于Viterbi Sampling来说，每个终点位置会有多个切分方案，我们要对其进行多选一采样，被选中的概率是由各自的得分构造出来的$p\_i = e^{\\alpha s\_i}/Z$，$Z$是归一化因子。因为我们是递归处理的，所以我们不知道多选一的“多”是多少，也无法计算$Z$，不过这不重要，知道$e^{\\alpha s\_i}$就够了，因为计算每一步的条件采样概率其实也用不到完整的$Z$，而是需要递归的$Z\_i$：
\\begin{array}{c\|c\|c}
\\hline
\\text{Viterbi Decoding} & \\text{旧版 Viterbi Sampling} & \\text{新版 Viterbi Sampling} \\\
\\hline
r\_i = \\left\\{\\begin{aligned}&\\,1\\,, \\,\\, s\_i > s\_{i-1} \\\
&\\,0\\,, \\,\\, \\text{else}\\end{aligned}\\right. &
r\_i = \\left\\{\\begin{aligned}&\\,1\\,, \\,\\, \\varepsilon < \\sigma(\\alpha(s\_i - s\_{i-1})) \\\
&\\,0\\,, \\,\\, \\text{else}\\end{aligned}\\right. &
\\begin{aligned}Z\_i =&\\, Z\_{i - 1} + e^{\\alpha s\_i} \\\\[1pt\]
r\_i =&\\, \\left\\{\\begin{aligned}&\\,1\\,, \\,\\, \\varepsilon < e^{\\alpha s\_i} / Z\_i \\\
&\\,0\\,, \\,\\, \\text{else}\\end{aligned}\\right.\\end{aligned} \\\
\\hline
\\end{array}
实际计算时，由于指数爆炸的原因，直接缓存$Z\_i$大概率会有溢出风险，所以我们一般缓存的是它的对数$Z^{\\log}\_i$，并利用$\\text{logsumexp}$函数避免溢出：
\\begin{equation}
\\begin{aligned}&\\,Z^{\\log}\_i = \\text{logsumexp}(Z^{\\log}\_{i-1}, \\alpha s\_i) \\\
&\\qquad e^{\\alpha s\_i} / Z\_i \\to e^{\\alpha s\_i - Z^{\\log}\_i}
\\end{aligned},\\qquad \\text{logsumexp}(x,y) = \\left\\{\\begin{aligned}&\\,x + \\log(1+e^{y-x}),\\,\\, x \\geq y \\\
&\\,y + \\log(1 + e^{x-y}),\\,\\,x < y
\\end{aligned}\\right.
\\end{equation}
相应的实现已经内置在 `bytepiece>=0.5.0` 中。

## 完美采样 [\#](https://kexue.fm/kexue.fm\#%E5%AE%8C%E7%BE%8E%E9%87%87%E6%A0%B7)

总的来说，出现旧版Viterbi Sampling的缺陷，还是因为之前操之过急了，所以现在认真地给新版Viterbi Sampling补上数学证明。有意思的是，可以证明更新后的Viterbi Sampling跟Subword Regularization一样都是“完美采样”算法。

之前我们介绍过，Subword Regularization的做法非常“粗暴”，直接找出得分最高的$k$个切分方案，然后通过$p\_i = e^{\\alpha s\_i}/Z$的方式计算被选中的概率，其中$s\_i$是第$i$种方案的得分。这种做法除了复杂度高外没有任何毛病，当$k$不做限制（即找出全部切分方案）时，我们得到所有切分方案的一个随机采样，而每种方案被采样到的概率正比于$e^{\\alpha s\_i}$——是得分$s\_i$的单调增函数，即采样概率与得分的大小排序都是一样的，满足这两个条件的，笔者称之为“完美采样”。

### Decoding [\#](https://kexue.fm/kexue.fm\#Decoding)

为了证明新版Viterbi Sampling也是“完美采样”，我们先来回顾一下Viterbi Decoding。设有一个长度为$l$的字节串$c\_1,c\_2,\\cdots,c\_l$，用$S^\*(c\_1,c\_2,\\cdots,c\_l)$表示最优切分方案的得分，假设我们知道$c\_k,c\_{k+1}$之间一定会分开，那么必然有
\\begin{equation}S^\*(c\_1,c\_2,\\cdots,c\_l) = S^\*(c\_1,c\_2,\\cdots,c\_k) + S^\*(c\_{k+1},c\_{k+2},\\cdots,c\_l)\\end{equation}
也就是说，最优切分方案的子串，一定也是对应的子字节串的最优切分方案，这是动态规划的根本依据。当然，事实上我们不能预知哪一处会被切开，所以只能用枚举的方式：
\\begin{equation}S^\*(c\_1,c\_2,\\cdots,c\_l) = \\max\\left\\{\\begin{aligned}
&\\,\\color{green}{s\\left(\\overline{c\_1,\\cdots,c\_l}\\right)} \\\
\\color{red}{S^\*(c\_1)} \\,+&\\, \\color{green}{s\\left(\\overline{c\_2,\\cdots,c\_l}\\right)} \\\
\\color{red}{S^\*(c\_1,c\_2)} \\,+&\\, \\color{green}{s\\left(\\overline{c\_3,\\cdots,c\_l}\\right)} \\\
\\vdots \\\
\\color{red}{S^\*(c\_1,\\cdots,c\_{l-2})} \\,+&\\, \\color{green}{s\\left(\\overline{c\_{l-1},c\_l}\\right)} \\\
\\color{red}{S^\*(c\_1,\\cdots,c\_{l-1})} \\,+&\\, \\color{green}{s\\left(\\overline{c\_l}\\right)}
\\end{aligned}\\right\\}\\label{eq:core}\\end{equation}
其中$s\\left(\\overline{c\_1,\\cdots,c\_l}\\right)$是指字节串$c\_1, \\cdots,c\_l$作为一个token时的得分（如果它不是词表中的token，那么记为$-\\infty$）。这样一来，$S^\*(c\_1,c\_2,\\cdots,c\_l)$的计算就转化为$S^\*(c\_1),S^\*(c\_1,c\_2),\\cdots,S^\*(c\_1,\\cdots,c\_{l-1})$的计算，依此类推，$S^\*(c\_1,c\_2,\\cdots,c\_{l-1})$的计算又可以转化为$S^\*(c\_1),S^\*(c\_1,c\_2),\\cdots,S^\*(c\_1,\\cdots,c\_{l-2})$的计算，等等，也就是$S^\*$的结果是可以复用的。所以，整个流程总结下来就是一句话：

> 扫描到每一个位置时，都记录到当前位置的最优切分方案及其得分。

当然，直接按照式$\\eqref{eq:core}$进行递归的话，理论上复杂度是$\\mathcal{O}(l^2)$，但事实上不可能每个子字节串都是词表中的一个token，所以可以用Trie树、AC自动机等方法根据词表提前扫描好所有可能出现的token，那么复杂度就正比于搜索出来的候选token数，关于$l$是线性的，如果非要估计一个数值，那么假设词表中token的最大长度为$m$，那么长度为$l\\geq m$的字节串扫描出来的token数就不超过
\\begin{equation}l + (l - 1) + \\cdots + (l - m + 1) = lm - \\frac{1}{2}m(m-1) = \\mathcal{O}(lm)\\end{equation}

### Sampling [\#](https://kexue.fm/kexue.fm\#Sampling)

有了Decoding部分做铺垫后，理解Sampling就相对容易一些了。其实关键还是在式$\\eqref{eq:core}$，我们用$Z(c\_1,c\_2,\\cdots,c\_l)$表示字节串$c\_1,c\_2,\\cdots,c\_l$的所有切分方案的归一化因子（完美采样），那么有
\\begin{equation}Z(c\_1,c\_2,\\cdots,c\_l) = \\sum\\left\\{\\begin{aligned}
&\\,\\color{green}{e^{\\alpha\\cdot s\\left(\\overline{c\_1,\\cdots,c\_l}\\right)}} \\\
\\color{red}{Z(c\_1)} &\\, \\color{green}{e^{\\alpha\\cdot s\\left(\\overline{c\_2,\\cdots,c\_l}\\right)}} \\\
\\color{red}{Z(c\_1,c\_2)} &\\, \\color{green}{e^{\\alpha\\cdot s\\left(\\overline{c\_3,\\cdots,c\_l}\\right)}} \\\
\\vdots \\\
\\color{red}{Z(c\_1,\\cdots,c\_{l-2})} &\\, \\color{green}{e^{\\alpha\\cdot s\\left(\\overline{c\_{l-1},c\_l}\\right)}} \\\
\\color{red}{Z(c\_1,\\cdots,c\_{l-1})} &\\, \\color{green}{e^{\\alpha\\cdot s\\left(\\overline{c\_l}\\right)}}
\\end{aligned}\\right\\}\\label{eq:core-2}
\\end{equation}
这个等式也表明，要实现从$c\_1,c\_2,\\cdots,c\_l$的所有切分方案中按$e^{\\alpha s}$的比重采样，可以从$c\_1,\\cdots,c\_{l-1}$的所有切分方案中随机选一个然后接上token $\\overline{c\_l}$、从$c\_1,\\cdots,c\_{l-2}$的所有切分方案中随机选一个然后接上token $\\overline{c\_{l-1},c\_l}$、从$c\_1,\\cdots,c\_{l-3}$的所有切分方案中随机选一个然后接上token $\\overline{c\_{l-2},c\_{l-1},c\_l}$、...，得到这$l$个采样结果后，分别再以权重$Z(c\_1,\\cdots,c\_{l-1}) e^{\\alpha\\cdot s\\left(\\overline{c\_l}\\right)}$、$Z(c\_1,\\cdots,c\_{l-2}) e^{\\alpha\\cdot s\\left(\\overline{c\_{l-1},c\_l}\\right)}$、$Z(c\_1,\\cdots,c\_{l-3}) e^{\\alpha\\cdot s\\left(\\overline{c\_{l-2},c\_{l-1},c\_l}\\right)}$、...从中选一个。

接下来跟Decoding情形一样，$Z(c\_1,\\cdots,c\_{l-1})$的计算又可以重用$Z(c\_1),Z(c\_1,c\_2),\\cdots,Z(c\_1,\\cdots,c\_{l-2})$的结果，$Z(c\_1,\\cdots,c\_{l-2})$的计算又可以重用$Z(c\_1),Z(c\_1,c\_2),\\cdots,Z(c\_1,\\cdots,c\_{l-3})$的结果，等等，以及采样结果也都是可以重用的。于是类似地，那么整个Sampling算法也可以总结为一句话：

> 扫描到每一个位置时，都对以当前位置为终点的所有切分方案按照$e^{\\alpha s}$权重进行采样，记录采样结果以及累积权重$Z$。

如果两边取对数，那么式$\\eqref{eq:core-2}$可以等价地改写成
\\begin{equation}Z^{\\log}(c\_1,c\_2,\\cdots,c\_l) = \\text{logsumexp}\\left\\{\\begin{aligned}
&\\,\\color{green}{\\alpha\\cdot s\\left(\\overline{c\_1,\\cdots,c\_l}\\right)} \\\
\\color{red}{Z^{\\log}(c\_1)} \\,+&\\, \\color{green}{\\alpha\\cdot s\\left(\\overline{c\_2,\\cdots,c\_l}\\right)} \\\
\\color{red}{Z^{\\log}(c\_1,c\_2)} \\,+&\\, \\color{green}{\\alpha\\cdot s\\left(\\overline{c\_3,\\cdots,c\_l}\\right)} \\\
\\vdots \\\
\\color{red}{Z^{\\log}(c\_1,\\cdots,c\_{l-2})} \\,+&\\, \\color{green}{\\alpha\\cdot s\\left(\\overline{c\_{l-1},c\_l}\\right)} \\\
\\color{red}{Z^{\\log}(c\_1,\\cdots,c\_{l-1})} \\,+&\\, \\color{green}{\\alpha\\cdot s\\left(\\overline{c\_l}\\right)}
\\end{aligned}\\right\\}
\\end{equation}

跟Viterbi Decoding的式$\\eqref{eq:core}$区别就是$Z^{\\log}$代替了$S^\*$，$\\text{logsumexp}$代替了$\\max$，而$\\text{logsumexp}$正好是$\\max$的光滑近似，所以$\\alpha\\to\\infty$时能退化为Viterbi Decoding。另一方面，在实际计算时，同一终点的多个切分方案是逐一到达而不是一次性到达的，所以就需要将单步的“多选一”转化为多步的“二选一”，这就是“ [解决办法](https://kexue.fm/kexue.fm#%E8%A7%A3%E5%86%B3%E5%8A%9E%E6%B3%95)”一节所讨论的内容。至此，我们证明了（或者说从Viterbi Decoding出发重新推导了）修改后的Viterbi Sampling实际是跟Subword Regularization一样的完美采样算法。

## 文章小结 [\#](https://kexue.fm/kexue.fm\#%E6%96%87%E7%AB%A0%E5%B0%8F%E7%BB%93)

本文完善了之前提出的随机分词算法Viterbi Sampling，并从数学上证明了它在效果上跟Subword Regularization一样都是“完美采样”算法，而在使用上有着比Subword Regularization明显更高的效率。

_**转载到请包括本文地址：** [https://kexue.fm/archives/9811](https://kexue.fm/archives/9811)_

_**更详细的转载事宜请参考：**_ [《科学空间FAQ》](https://kexue.fm/archives/6508#%E6%96%87%E7%AB%A0%E5%A6%82%E4%BD%95%E8%BD%AC%E8%BD%BD/%E5%BC%95%E7%94%A8)

**如果您还有什么疑惑或建议，欢迎在下方评论区继续讨论。**

**如果您觉得本文还不错，欢迎 [分享](https://kexue.fm/kexue.fm#share)/ [打赏](https://kexue.fm/kexue.fm#pay) 本文。打赏并非要从中获得收益，而是希望知道科学空间获得了多少读者的真心关注。当然，如果你无视它，也不会影响你的阅读。再次表示欢迎和感谢！**

打赏

微信打赏

支付宝打赏

因为网站后台对打赏并无记录，因此欢迎在打赏时候备注留言。你还可以 [**点击这里**](http://mail.qq.com/cgi-bin/qm_share?t=qm_mailme&email=tN7d1drY3drrx8H0xcWa19vZ) 或在下方评论区留言来告知你的建议或需求。

**如果您需要引用本文，请参考：**

苏剑林. (Oct. 16, 2023). 《随机分词再探：从Viterbi Sampling到完美采样算法 》\[Blog post\]. Retrieved from [https://kexue.fm/archives/9811](https://kexue.fm/archives/9811)

@online{kexuefm-9811,
        title={随机分词再探：从Viterbi Sampling到完美采样算法},
        author={苏剑林},
        year={2023},
        month={Oct},
        url={\\url{https://kexue.fm/archives/9811}},
}

分类： [信息时代](https://kexue.fm/category/Big-Data)    标签： [概率](https://kexue.fm/tag/%E6%A6%82%E7%8E%87/), [随机](https://kexue.fm/tag/%E9%9A%8F%E6%9C%BA/), [优化](https://kexue.fm/tag/%E4%BC%98%E5%8C%96/), [分词](https://kexue.fm/tag/%E5%88%86%E8%AF%8D/), [采样](https://kexue.fm/tag/%E9%87%87%E6%A0%B7/)[1 评论](https://kexue.fm/archives/9811#comments)

< [EMO：基于最优传输思想设计的分类损失函数](https://kexue.fm/archives/9797) \| [从梯度最大化看Attention的Scale操作](https://kexue.fm/archives/9812) >

### 你也许还对下面的内容感兴趣

- [QK-Clip：让Muon在Scaleup之路上更进一步](https://kexue.fm/archives/11126)
- [Transformer升级之路：21、MLA好在哪里?（下）](https://kexue.fm/archives/11111)
- [生成扩散模型漫谈（三十）：从瞬时速度到平均速度](https://kexue.fm/archives/10958)
- [MoE环游记：5、均匀分布的反思](https://kexue.fm/archives/10945)
- [Transformer升级之路：20、MLA好在哪里?（上）](https://kexue.fm/archives/10907)
- [一道概率不等式：盯着它到显然成立为止！](https://kexue.fm/archives/10902)
- [MoE环游记：4、难处应当多投入](https://kexue.fm/archives/10815)
- [为什么梯度裁剪的默认模长是1？](https://kexue.fm/archives/10657)
- [从谱范数梯度到新式权重衰减的思考](https://kexue.fm/archives/10648)
- [生成扩散模型漫谈（二十八）：分步理解一致性模型](https://kexue.fm/archives/10633)

[发表你的看法](https://kexue.fm/kexue.fm#comment_form)

[随机分词再探：从Viterbi Sampling到完美采样算法 R11; AI 資訊](https://news.aitime.space/2023/10/61329/)

October 16th, 2023

\[...\]​Read More \[...\]

[回复评论](https://kexue.fm/archives/9811/comment-page-1?replyTo=22893#respond-post-9811)

[取消回复](https://kexue.fm/archives/9811#respond-post-9811)

你的大名

电子邮箱

个人网站（选填）

1\. 可以使用LaTeX代码，点击“预览效果”可查看效果；2. 可以通过点击评论楼层编号来引用该楼层；3. 网站可能会有点卡，如非确认评论失败，请 **不要重复点击提交**。

### 内容速览

[问题分析](https://kexue.fm/kexue.fm#%E9%97%AE%E9%A2%98%E5%88%86%E6%9E%90)
[解决办法](https://kexue.fm/kexue.fm#%E8%A7%A3%E5%86%B3%E5%8A%9E%E6%B3%95)
[完美采样](https://kexue.fm/kexue.fm#%E5%AE%8C%E7%BE%8E%E9%87%87%E6%A0%B7)
[Decoding](https://kexue.fm/kexue.fm#Decoding)
[Sampling](https://kexue.fm/kexue.fm#Sampling)
[文章小结](https://kexue.fm/kexue.fm#%E6%96%87%E7%AB%A0%E5%B0%8F%E7%BB%93)

### 智能搜索

支持整句搜索！网站自动使用 [结巴分词](https://github.com/fxsjy/jieba) 进行分词，并结合ngrams排序算法给出合理的搜索结果。

### 热门标签

[生成模型](https://kexue.fm/tag/%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B/) [attention](https://kexue.fm/tag/attention/) [优化](https://kexue.fm/tag/%E4%BC%98%E5%8C%96/) [语言模型](https://kexue.fm/tag/%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/) [模型](https://kexue.fm/tag/%E6%A8%A1%E5%9E%8B/) [网站](https://kexue.fm/tag/%E7%BD%91%E7%AB%99/) [概率](https://kexue.fm/tag/%E6%A6%82%E7%8E%87/) [梯度](https://kexue.fm/tag/%E6%A2%AF%E5%BA%A6/) [转载](https://kexue.fm/tag/%E8%BD%AC%E8%BD%BD/) [矩阵](https://kexue.fm/tag/%E7%9F%A9%E9%98%B5/) [微分方程](https://kexue.fm/tag/%E5%BE%AE%E5%88%86%E6%96%B9%E7%A8%8B/) [优化器](https://kexue.fm/tag/%E4%BC%98%E5%8C%96%E5%99%A8/) [分析](https://kexue.fm/tag/%E5%88%86%E6%9E%90/) [天象](https://kexue.fm/tag/%E5%A4%A9%E8%B1%A1/) [深度学习](https://kexue.fm/tag/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/) [积分](https://kexue.fm/tag/%E7%A7%AF%E5%88%86/) [python](https://kexue.fm/tag/python/) [力学](https://kexue.fm/tag/%E5%8A%9B%E5%AD%A6/) [无监督](https://kexue.fm/tag/%E6%97%A0%E7%9B%91%E7%9D%A3/) [扩散](https://kexue.fm/tag/%E6%89%A9%E6%95%A3/) [几何](https://kexue.fm/tag/%E5%87%A0%E4%BD%95/) [节日](https://kexue.fm/tag/%E8%8A%82%E6%97%A5/) [生活](https://kexue.fm/tag/%E7%94%9F%E6%B4%BB/) [文本生成](https://kexue.fm/tag/%E6%96%87%E6%9C%AC%E7%94%9F%E6%88%90/) [数论](https://kexue.fm/tag/%E6%95%B0%E8%AE%BA/)

### 随机文章

- [当生成模型肆虐：互联网将有“疯牛病”之忧？](https://kexue.fm/archives/9687)
- [百度实体链接比赛后记：行为建模和实体链接](https://kexue.fm/archives/6919)
- [训练1000层的Transformer究竟有什么困难？](https://kexue.fm/archives/8978)
- [庆祝圆周率(π)节！](https://kexue.fm/archives/524)
- [《虚拟的实在(1)》——为什么需要场？](https://kexue.fm/archives/1999)
- [随机分词浅探：从Viterbi Decoding到Viterbi Sampling](https://kexue.fm/archives/9768)
- [用复数化简二次曲线的尝试](https://kexue.fm/archives/1851)
- [“让Keras更酷一些！”：分层的学习率和自由的梯度](https://kexue.fm/archives/6418)
- [更别致的词向量模型(二)：对语言进行建模](https://kexue.fm/archives/4669)
- [AdaX优化器浅析（附开源实现）](https://kexue.fm/archives/7387)

### 最近评论

- [zyp](https://kexue.fm/archives/9119/comment-page-13#comment-28572): 您好，我在反向过程中有个疑问，您在
μ(xt)=1αt(xt−βtϵθ(xt,t))（8）
x...
- [歪门正道](https://kexue.fm/archives/11033/comment-page-2#comment-28571): 喔，我看前面的文章设定有n >> d，如果deltanet中分chunk选取chunksize...
- [歪门正道](https://kexue.fm/archives/11033/comment-page-2#comment-28570): 你好苏老师，没有理解单位下三角矩阵求逆，复杂度是n^2那一步。(I+B)U = V 这里的U和...
- [doggy](https://kexue.fm/archives/9907/comment-page-4#comment-28569): 苏神好，请问能提供调 kimi 的 prompt 吗？我在浏览一篇文章（Unsupervise...
- [markchin](https://kexue.fm/archives/443/comment-page-1#comment-28567): 苏佬发下大图吗？看了《银河铁道之夜》后深受触动，1459294449@qq.com,万分感谢。
- [kkkfm](https://kexue.fm/archives/10592/comment-page-2#comment-28565): 苏老师您好，我想请问一下您提到的使用Muon可以用更大的learning rate，是不是说明...
- [flyingDog](https://kexue.fm/archives/11033/comment-page-2#comment-28564): 将K,V视作语料对(k1,v1),(k2,v2),⋯,(kt,vt)，根据这些语料训练得到一个...
- [简明](https://kexue.fm/archives/3266/comment-page-1#comment-28563): 请教一下各位，（6）是有限项之和从而支撑抽样定理，但（6）的推导是基于频域是离散级数（4），这...
- [胡韬](https://kexue.fm/archives/11158/comment-page-1#comment-28561): 特征值的模长小于一？
- [guanchanghao](https://kexue.fm/archives/9359/comment-page-1#comment-28560): 苏老师，最开始的物理中热传导方程的解为什么是左边那幅图呢？？没有看懂那幅图的含义

### 友情链接

- [Cool Papers](https://papers.cool)
- [数学研发](https://bbs.emath.ac.cn)
- [Seatop](http://www.seatop.com.cn/)
- [Xiaoxia](https://xiaoxia.org/)
- [积分表-网络版](https://kexue.fm/sci/integral/index.html)
- [丝路博傲](http://blog.dvxj.com/)
- [数学之家](http://www.2math.cn/)
- [有趣天文奇观](http://interesting-sky.china-vo.org/)
- [TwistedW](http://www.twistedwg.com/)
- [godweiyang](https://godweiyang.com/)
- [AI柠檬](https://blog.ailemon.net/)
- [王登科-DK博客](https://greatdk.com)
- [ESON](https://blog.eson.org/)
- [枫之羽](https://fzhiy.net/)
- [Mathor's blog](https://wmathor.com/)
- [coding-zuo](https://coding-zuo.github.io/)
- [博科园](https://www.bokeyuan.net/)
- [孔皮皮的博客](https://www.kppkkp.top/)
- [运鹏的博客](https://yunpengtai.top/)
- [jiming.site](https://jiming.site/)
- [OmegaXYZ](https://www.omegaxyz.com/)
- [EAI猩球](https://www.robotech.ink/)
- [文举的博客](https://liwenju0.com/)
- [用代码打点酱油](https://bruceyuan.com/)
- [Zhang's blog](https://armcvai.cn/)
- [申请链接](https://kexue.fm/links.html)

本站采用创作共用版权协议，要求署名、非商业用途和保持一致。转载本站内容必须也遵循“ [署名-非商业用途-保持一致](http://creativecommons.org/licenses/by-nc-nd/2.5/cn/)”的创作共用协议。
© 2009-2025 Scientific Spaces. All rights reserved. Theme by [laogui](http://www.laogui.com). Powered by [Typecho](http://typecho.org). 备案号: [粤ICP备09093259号-1/2](https://beian.miit.gov.cn/)。