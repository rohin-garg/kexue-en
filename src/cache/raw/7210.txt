Loading \[MathJax\]/jax/element/mml/optable/BasicLatin.js

![MobileSideBar](https://kexue.fm/usr/themes/geekg/images/slide-button.png)

## SEARCH

## MENU

- [打赏](https://kexue.fm/reward.html)
- [公式](https://kexue.fm/latex.html)
- [天象](https://kexue.fm/ac.html)
- [链接](https://kexue.fm/links.html)
- [时光](https://kexue.fm/me.html)
- [博览](https://kexue.fm/science.html)
- [归档](https://kexue.fm/content.html)

## CATEGORIES

- [千奇百怪](https://kexue.fm/category/Everything)
- [天文探索](https://kexue.fm/category/Astronomy)
- [数学研究](https://kexue.fm/category/Mathematics)
- [物理化学](https://kexue.fm/category/Phy-chem)
- [信息时代](https://kexue.fm/category/Big-Data)
- [生物自然](https://kexue.fm/category/Biology)
- [图片摄影](https://kexue.fm/category/Photograph)
- [问题百科](https://kexue.fm/category/Questions)
- [生活/情感](https://kexue.fm/category/Life-Feeling)
- [资源共享](https://kexue.fm/category/Resources)

## NEWPOSTS

- [让炼丹更科学一些（五）：基于梯度精...](https://kexue.fm/archives/11530)
- [让炼丹更科学一些（四）：新恒等式，...](https://kexue.fm/archives/11494)
- [为什么DeltaNet要加L2 N...](https://kexue.fm/archives/11486)
- [让炼丹更科学一些（三）：SGD的终...](https://kexue.fm/archives/11480)
- [让炼丹更科学一些（二）：将结论推广...](https://kexue.fm/archives/11469)
- [滑动平均视角下的权重衰减和学习率](https://kexue.fm/archives/11459)
- [生成扩散模型漫谈（三十一）：预测数...](https://kexue.fm/archives/11428)
- [Muon优化器指南：快速上手与关键细节](https://kexue.fm/archives/11416)
- [AdamW的Weight RMS的...](https://kexue.fm/archives/11404)
- [n个正态随机数的最大值的渐近估计](https://kexue.fm/archives/11390)

## COMMENTS

- [Bin: 今天偶然从某个论坛看到有人推荐您的博客，定睛一看竟然是华师同院...](https://kexue.fm/archives/1990/comment-page-2#comment-29105)
- [Rapture D: 我有一个问题，为什么不考虑亥姆霍兹定理和斯托克斯公式。](https://kexue.fm/archives/11530/comment-page-1#comment-29104)
- [mofheka: 苏神是还在用jax是么？最近在做基于Google Pathwa...](https://kexue.fm/archives/11390/comment-page-1#comment-29103)
- [长琴: 看懂这篇博客也不是一件容易的事情。](https://kexue.fm/archives/11530/comment-page-1#comment-29102)
- [AlexLi: 苏老师，请教一下(7)式中将 μ(xt) 传给 $p...](https://kexue.fm/archives/9257/comment-page-4#comment-29101)
- [tyler\_zxc: "Performer的思想是将标准的Attention线性化，...](https://kexue.fm/archives/7921/comment-page-2#comment-29100)
- [我: 似乎并非mHC提出矩阵的思想？之前hyper connecti...](https://kexue.fm/archives/11494/comment-page-1#comment-29099)
- [winter: 苏神您好，假如对于比较均匀的attention weightP...](https://kexue.fm/archives/10847/comment-page-1#comment-29098)
- [苏剑林: KL散度、JS散度、W距离啥的，都行啊，看你喜欢哪个](https://kexue.fm/archives/8512/comment-page-2#comment-29097)
- [苏剑林: 没有绝对公平的对比方法，主要看你关心什么。比如，如果只关心推理...](https://kexue.fm/archives/9119/comment-page-14#comment-29096)

## USERLOGIN

- [登录](https://kexue.fm/admin/login.php)

[科学空间\|Scientific Spaces](https://kexue.fm/)

- [登录](https://kexue.fm/admin/login.php)
- [打赏](https://kexue.fm/reward.html)
- [公式](https://kexue.fm/latex.html)
- [天象](https://kexue.fm/ac.html)
- [链接](https://kexue.fm/links.html)
- [时光](https://kexue.fm/me.html)
- [博览](https://kexue.fm/science.html)
- [归档](https://kexue.fm/content.html)

渴望成为一个小飞侠

- [![](https://kexue.fm/usr/themes/geekg/images/rss.png)\\
\\
欢迎订阅](https://kexue.fm/feed)
- [![](https://kexue.fm/usr/themes/geekg/images/mail.png)\\
\\
个性邮箱](https://kexue.fm/archives/119)
- [![](https://kexue.fm/usr/themes/geekg/images/Saturn.png)\\
\\
天象信息](https://kexue.fm/ac.html)
- [![](https://kexue.fm/usr/themes/geekg/images/iss.png)\\
\\
观测ISS](https://kexue.fm/archives/41)
- [![](https://kexue.fm/usr/themes/geekg/images/pi.png)\\
\\
LaTeX](https://kexue.fm/latex.html)
- [![](https://kexue.fm/usr/themes/geekg/images/mlogo.png)\\
\\
关于博主](https://kexue.fm/me.html)

欢迎访问“科学空间”，这里将与您共同探讨自然科学，回味人生百态；也期待大家的分享～

- [**千奇百怪** Everything](https://kexue.fm/category/Everything)
- [**天文探索** Astronomy](https://kexue.fm/category/Astronomy)
- [**数学研究** Mathematics](https://kexue.fm/category/Mathematics)
- [**物理化学** Phy-chem](https://kexue.fm/category/Phy-chem)
- [**信息时代** Big-Data](https://kexue.fm/category/Big-Data)
- [**生物自然** Biology](https://kexue.fm/category/Biology)
- [**图片摄影** Photograph](https://kexue.fm/category/Photograph)
- [**问题百科** Questions](https://kexue.fm/category/Questions)
- [**生活/情感** Life-Feeling](https://kexue.fm/category/Life-Feeling)
- [**资源共享** Resources](https://kexue.fm/category/Resources)

- [**千奇百怪**](https://kexue.fm/category/Everything)
- [**天文探索**](https://kexue.fm/category/Astronomy)
- [**数学研究**](https://kexue.fm/category/Mathematics)
- [**物理化学**](https://kexue.fm/category/Phy-chem)
- [**信息时代**](https://kexue.fm/category/Big-Data)
- [**生物自然**](https://kexue.fm/category/Biology)
- [**图片摄影**](https://kexue.fm/category/Photograph)
- [**问题百科**](https://kexue.fm/category/Questions)
- [**生活/情感**](https://kexue.fm/category/Life-Feeling)
- [**资源共享**](https://kexue.fm/category/Resources)

[首页](https://kexue.fm/) [数学研究](https://kexue.fm/category/Mathematics) [信息时代](https://kexue.fm/category/Big-Data) Designing GANs：又一个GAN生产车间

13Feb

# [Designing GANs：又一个GAN生产车间](https://kexue.fm/archives/7210)

By 苏剑林 \|
2020-02-13 \|
43378位读者 \|

在2018年的文章里 [《f-GAN简介：GAN模型的生产车间》](https://kexue.fm/archives/6016) 笔者介绍了f-GAN，并评价其为GAN模型的“生产车间”，顾名思义，这是指它能按照固定的流程构造出很多不同形式的GAN模型来。前几天在arxiv上看到了新出的一篇论文 [《Designing GANs: A Likelihood Ratio Approach》](https://papers.cool/arxiv/2002.00865)（后面简称Designing GANs或原论文），发现它在做跟f-GAN同样的事情，但走的是一条截然不同的路（不过最后其实是殊途同归），整篇论文颇有意思，遂在此分享一番。

## f-GAN回顾 [\#](https://kexue.fm/archives/7210\#f-GAN%E5%9B%9E%E9%A1%BE)

从 [《f-GAN简介：GAN模型的生产车间》](https://kexue.fm/archives/6016) 中我们可以知道，f-GAN的首要步骤是找到满足如下条件的函数f：

> 1、f是非负实数到实数的映射（R∗→R）；
>
> 2、f(1)=0；
>
> 3、f是凸函数。

找到这样的函数后，就可以构造一个概率f散度出来，然后用一种称为“凸共轭”的技术将f散度转化为另一种形式（带max的形式，一般称为对偶形式），然后去\\min这个散度，就得到一个min-max过程，这便诞生了一种GAN模型。顺便说一下，f-GAN代表了一系列GAN模型，但不包含WGAN，不过WGAN的推导其实也遵循着类似的步骤，只不过它用到的概率度量是Wasserstein距离，并且Wasserstein距离转化为对偶形式的方法不一样，具体细节可以参考 [《从Wasserstein距离、对偶理论到WGAN》](https://kexue.fm/archives/6280)。

f-GAN在逻辑上并没有问题，但是根据它提供的步骤，我们总是需要先找到一个f散度，然后再转化为对偶形式。问题就是：既然我们只需要它的对偶形式，为什么不直接在对偶空间里边分析呢？这个疑问笔者之前在文章 [《不用L约束又不会梯度消失的GAN，了解一下？》](https://kexue.fm/archives/6163)，只不过在当时只是讨论了在对偶空间中概率散度的证明，并没有给出概率散度的构建方法，而Designing GANs正是补充了这一点。

## Designing GANs [\#](https://kexue.fm/archives/7210\#Designing%20GANs)

在这一节里，我们会探讨Designing GANs里边的思路和方法。不同于原论文中比较冗余的教科书式推导，本文将会通过逐步反推的过程来导出Designing GANs的结果，笔者认为这样更容易理解一些。有意思的是，理解整个推导过程事实上只需要非常基础的微积分知识。

### Total Variation [\#](https://kexue.fm/archives/7210\#Total%20Variation)

这里以一个称为Total Variation的概率散度为例子，让我们初步体会一下在对偶空间里边分析推导概率散度的要点。

首先，我们有

\\begin{equation}\|p-q\|=\\max\_{t\\in\[-1, 1\]} (p - q)t=\\max\_{t\\in\[-1, 1\]} pt - qt\\label{eq:tv-base}\\end{equation}

所以对于概率分布p(x),q(x)，我们也有

\\begin{equation}\|p(x)-q(x)\|=\\max\_{t(x)\\in\[-1, 1\]} p(x)t(x) - q(x)t(x)\\end{equation}

两边积分得（我们先不纠结积分和\\max的交换性）

\\begin{equation}\\begin{aligned}\\int\|p(x)-q(x)\|dx=&\\,\\max\_{t(x)\\in\[-1, 1\]} \\int \\big\[p(x)t(x) - q(x)t(x)\\big\]dx\\\
=&\\,\\max\_{t(x)\\in\[-1, 1\]} \\mathbb{E}\_{x\\sim p(x)}\[t(x)\] - \\mathbb{E}\_{x\\sim q(x)}\[t(x)\]
\\end{aligned}\\end{equation}

这里的\\int\|p(x)-q(x)\|dx就称为两个概率分布的 [Total Variation](https://en.wikipedia.org/wiki/Total_variation)，所以我们就通过这样的过程导出了Total Variation的对偶形式。如果固定p(x)，让q(x)逼近p(x)，那么就可以最小化Total Variation，即

\\begin{equation}\\min\_{q(x)}\\int\|p(x)-q(x)\|dx = \\min\_{q(x)}\\max\_{t(x)\\in\[-1, 1\]} \\mathbb{E}\_{x\\sim p(x)}\[t(x)\] - \\mathbb{E}\_{x\\sim q(x)}\[t(x)\]\\label{eq:tv-gan}\\end{equation}

这就得到了一种GAN形式。

回顾整个流程，抛开对Total Variation的先验认识，我们可以发现整个流程的核心其实是式\\eqref{eq:tv-base}，有了式\\eqref{eq:tv-base}之后，后面的事情都是顺理成章了。那么式\\eqref{eq:tv-base}有什么特点呢？事实上，它可以一般化为：

> **目标1** 寻找函数\\phi(t),\\psi(t)以及某个值域\\Omega，使得
> \\begin{equation}d(p, q) = \\max\_{t\\in\\Omega} p\\phi(t)+q\\psi(t)\\end{equation}
> 并且有d(p,q)\\geq 0以及d(p,q)=0\\Leftrightarrow p=q。

有了这样的\\phi(t),\\psi(t)之后，我们就可以导出跟\\eqref{eq:tv-gan}类似的GAN出来：

\\begin{equation}\\min\_{q(x)}\\int d(p(x), q(x)) dx = \\min\_{q(x)}\\max\_{t(x)\\in \\Omega} \\mathbb{E}\_{x\\sim p(x)}\[\\phi(t(x))\] + \\mathbb{E}\_{x\\sim q(x)}\[\\psi(t(x))\]\\label{eq:gan}\\end{equation}

### 求导找极大值 [\#](https://kexue.fm/archives/7210\#%E6%B1%82%E5%AF%BC%E6%89%BE%E6%9E%81%E5%A4%A7%E5%80%BC)

注意到“ **目标1**”里边p,q只是一个非负实数，\\phi(t),\\psi(t)也只是标量函数，所以整个目标纯粹是一元函数的极值问题，应该说已经相当简化了。甚至地，设r=q/p\\in\[0,+\\infty)，它还可以转化为更简洁的“ **目标2**”：\
\
> **目标2** 寻找函数\\phi(t),\\psi(t)以及某个值域\\Omega，使得\
> \\begin{equation}d(r) = \\max\_{t\\in\\Omega} \\phi(t)+r\\psi(t)\\end{equation}\
> 并且d(r)的最小值在r=1时取到。\
\
现在我们就来考察 **目标2**，简单起见，我们假设\\phi(t),\\psi(t)都是光滑函数，这样一来\\phi(t)+r\\psi(t)的最大值我们就可以通过求导来解决。事实上这样的设计已经有足够的代表性了，当有部分点不光滑时，我们可以用光滑函数近似，然后取极限，比如\\text{sign}(x)=\\lim\\limits\_{k\\to+\\infty}\\tanh(kx)。\
\
基于这样的假设，要求\\phi(t)+r\\psi(t)的最大值，首先需要求导并让它等于0：\
\
\\begin{equation}\\phi'(t)+r\\psi'(t)=0\\quad\\Rightarrow\\quad r = -\\frac{\\phi'(t)}{\\psi'(t)}\\triangleq \\omega^{-1}(t)\\label{eq:max0}\\end{equation}\
\
这里假设上述方程有唯一解，记为t=\\omega(r)，所以最后-\\frac{\\phi'(t)}{\\psi'(t)}相当于是\\omega^{-1}(t)，它是\\omega(r)的逆函数（再提醒，这里是逆函数，不是倒数）。同时，由于r\\in\[0,+\\infty)，所以t\\in \\Omega=\\omega(\[0,\\infty))，也就是说t的取值范围\\Omega就是\\omega(r)的值域。此外，既然能求逆，说明\\omega(r)要不就严格单调递增，要不就严格单调递减，不失一般性，这里假设\\omega(r)是严格单调递增的，所以\\omega^{-1}(t)也是严格单调递增的。\
\
导数为0只能说明t是一个极值点，但还不能保证是极大值点，我们来确定它是最大值的条件，现在我们有\
\
\\begin{equation}\\phi'(t)+r\\psi'(t)=\\big(r-\\omega^{-1}(t)\\big)\\psi'(t)\\end{equation}\
\
注意到r-\\omega^{-1}(t)是严格单调递减的，所以它只能有一个零点，并且它是先正后负的形式，为了让整个导函数也有这个特性，我们设\\psi'(t)\\triangleq \\rho(t) > 0，也就是\\rho(t)恒为正。这样一来，\\phi(t)+r\\psi(t)的导函数是连续的、先正后负的，所以\\phi(t)+r\\psi(t)只有一个极值点，且极值点为最大值点。\
\
### 求导找极小值 [\#](https://kexue.fm/archives/7210\#%E6%B1%82%E5%AF%BC%E6%89%BE%E6%9E%81%E5%B0%8F%E5%80%BC)\
\
先汇总一下到目前为止的结果：我们假设\\omega(r)是严格单调递增的，并且假设\\rho(t)在t\\in \\Omega时是恒正的，然后满足如下关系：\
\
\\begin{equation}\\left\\{\\begin{aligned}\\phi'(t)=&\\,-\\omega^{-1}(t)\\rho(t)\\\\
\\psi'(t)=&\\,\\rho(t)\\end{aligned}\
\\right.\\end{equation}\
\
这时候\\phi(t)+r\\psi(t)就存在唯一的极大值点t=\\omega(r)，并且它也是最大值点，对于 **目标2** 来说这已经完成了一半的内容。现在来考察当t=\\omega(r)时，d(r)=\\phi(\\omega(r))+r\\psi(\\omega(r))是否如我们所愿满足剩下的性质（即d(r)的最小值在r=1时取到）。\
\
继续求导\
\
\\begin{equation}d'(r)=\\big\[\\phi'(\\omega(r))+r\\psi'(\\omega(r))\\big\]\\omega'(r)+\\psi(\\omega(r))=\\psi(\\omega(r))\\label{eq:d-r}\\end{equation}\
\
其中第二个等号是因为根据式\\eqref{eq:max0}得方括号部分为0。现在只剩\\psi(\\omega(r))一项了，并且留意到我们假设了\\psi'(t)=\\rho(t) > 0，所以\\psi(t)是严格单调递增的，同时我们也假设了\\omega(r)是严格单调递增的，所以复合函数\\psi(\\omega(r))也是严格单调递增的。\
\
现在我们补充一个假设：\\psi(\\omega(1))=0，也就是说d'(1)=0，即r=1是d(r)的极值点，并且由于\\psi(\\omega(r))连续且严格单调递增，所以d'(r)是先负后正，因此r=1是d(r)的极小值点，也是最小值点。\
\
### GAN模型已经送达 [\#](https://kexue.fm/archives/7210\#GAN%E6%A8%A1%E5%9E%8B%E5%B7%B2%E7%BB%8F%E9%80%81%E8%BE%BE)\
\
到这里，我们的推导已经完成了，我们得到的条件是：\
\
> **结论1** 如果\\omega(r)是严格单调递增的，\\Omega=\\omega(\[0,+\\infty))，\\rho(t)在t\\in \\Omega时是恒正的，并且满足如下关系：\
> \\begin{equation}\\left\\{\\begin{aligned}\\phi'(t)=&\\,-\\omega^{-1}(t)\\rho(t)\\\\
> \\psi'(t)=&\\,\\rho(t)\\end{aligned}\
> \\right.\\end{equation}\
> 以及条件\\psi(\\omega(1))=0，具备这些条件的的\\phi(t),\\psi(t)就可以用来构建如\\eqref{eq:gan}的GAN模型。\
\
比如，我们来验算一下原始版本的GAN：\
\
\\begin{equation}\\begin{aligned}&\\,\\min\_{q(x)}\\max\_{t(x)} \\mathbb{E}\_{x\\sim p(x)}\[\\log (1-\\sigma(t(x)))\] + \\mathbb{E}\_{x\\sim q(x)}\[\\log \\sigma(t(x))\]\\\\
=&\\,\\min\_{q(x)}\\max\_{t(x)} \\mathbb{E}\_{x\\sim p(x)}\\left\[-\\log \\left(1+e^{t(x)}\\right)\\right\] + \\mathbb{E}\_{x\\sim q(x)}\\left\[-\\log \\left(1+e^{-t(x)}\\right)\\right\]\
\\end{aligned}\\label{eq:ori-gan}\\end{equation}\
\
其中\\sigma(\\cdot)是sigmoid激活函数，上述是让真样本的标签为0、假样本的标签为1的二分类交叉熵，等号右边是化简后的结果，由此观之\\phi(t)=-\\log \\left(1+e^t\\right)，\\psi(t)=-\\log \\left(1+e^{-t}\\right)，我们做个小调整，让\\psi(t)=\\log 2-\\log \\left(1+e^{-t}\\right)，显然这不影响原优化问题。现在我们有\\rho(t)=\\psi'(t)=\\frac{1}{1+e^t}，显然它是恒大于0的，以及\\omega^{-1}(t)=-\\phi'(t)/\\psi'(t)=e^t，即t=\\omega(r)=\\log r，显然它也是严格单调递增的，最后验算\\psi(\\omega(1))发现它确实等于0。\
\
由这个例子我们可以得到两点推论：\
\
> 1、\\psi(\\omega(1))=0这个条件并不是必须的，因为就算一开始不满足\\psi(\\omega(1))=0，我我们可以往\\psi(t)里边加上一个常数，使得它满足\\psi(\\omega(1))=0，而加上常数不会改变原来的优化问题；\
>\
> 2、如果调换一下标签，让真样本的标签为1、假样本的标签为0，那么得到的性质刚好相反，即算出来的\\rho(t)是恒小于0的，\\omega(r)的严格单调递减的。这说明，本文给出的 **结论1** 是构成GAN的充分条件而不是必要条件。\
\
不同形式的\\rho(t),\\omega(r)对于同一个GAN模型，比如我们选择t=\\omega(r)=\\frac{r}{r+1}，这时候r=\\omega^{-1}(t)=\\frac{t}{1-t}，然后我们选择\\rho(t)=\\frac{1}{t}，那么：\
\
\\begin{equation}\\left\\{\\begin{aligned}\\phi'(t)=&\\,-\\frac{t}{1-t}\\times\\frac{1}{t}\\\\
\\psi'(t)=&\\,\\frac{1}{t}\\end{aligned}\
\\right.\\end{equation}\
\
从而积分得到\\phi(t)=\\log(1-t),\\psi(t)。此外注意到\\Omega=\\omega(\[0,+\\infty))=\[0,1)，并且\\rho(t)=\\frac{1}{t}排除了t=0，所以可行域为(0,1)（事实上对于实验来说，边界点可以忽略），即导出的GAN为\
\
\\begin{equation}\\min\_{q(x)}\\max\_{t(x)\\in (0,1)} \\mathbb{E}\_{x\\sim p(x)}\[\\log (1-t(x))\] + \\mathbb{E}\_{x\\sim q(x)}\[\\log t(x)\]\\end{equation}\
\
这跟原始GAN是等价的，只不过没有显式写出使得t(x)\\in (0,1)的激活函数。\
\
再算一个例子。选择t=\\omega(r)=\\frac{1}{2}\\log r，即r=\\omega^{-1}(t)=e^{2t}，并且选择\\rho(t)=e^{-t}，可以算得\\phi(t)=-e^t,\\psi(t)=-e^{-t}，因此得到一个GAN变种：\
\
\\begin{equation}\\min\_{q(x)}\\max\_{t(x)} \\mathbb{E}\_{x\\sim p(x)}\\left\[-e^{t(x)}\\right\] + \\mathbb{E}\_{x\\sim q(x)}\\left\[-e^{-t(x)}\\right\]\\end{equation}\
\
原论文还用上述 **结论** 算了很多稀奇古怪的GAN，有兴趣的读者可以自行去阅读原论文，这里就不再重复推导了。\
\
## 思考与延伸 [\#](https://kexue.fm/archives/7210\#%E6%80%9D%E8%80%83%E4%B8%8E%E5%BB%B6%E4%BC%B8)\
\
本文的方法与之前的f-GAN有什么关联吗？本文的方法还有什么推广吗？此处给出笔者自己的答案。\
\
### 与f-GAN的联系 [\#](https://kexue.fm/archives/7210\#%E4%B8%8Ef-GAN%E7%9A%84%E8%81%94%E7%B3%BB)\
\
在上面的推导中，\\max这一步的结果d(r)，其中r=1是d(r)的最小值点，回顾 **目标1**，然后将d(r)代回到式\\eqref{eq:gan}中，就会发现我们优化的目标实际上是：\
\
\\begin{equation}\\int p(x) d\\left(\\frac{q(x)}{p(x)}\\right)dx\\label{eq:f-gan}\\end{equation}\
\
是不是有点眼熟？没错，它看起来就像是f散度的定义。并且回顾\\eqref{eq:d-r}处的推导，我们知道d(r)的导数是严格单调递增的，这说明d(r)是凸函数，所以上式确实就是一个f散度！也就是说，虽然这篇论文的作者走了一条看起来截然不同的路子，但实际上它的结果都可以通过f-GAN的结果导出来，并没有带来新的东西。\
\
那这篇论文是否完全等价于f-GAN呢？很遗憾，还不是，原论文所做的结果，只是f-GAN的一个子集，换句话说，它能导出的GAN模型变种，f-GAN都可以导出来，但是f-GAN能导出的GAN模型变种，它却不一定能导出来。\
\
因为回顾整个推导过程，它核心思想是直接将“点”的度量公式直接推广到“函数”，比如开头的\|p-q\|=0\\Leftrightarrow p=q推广到\\int \|p(x)-q(x)\|dx=0\\Leftrightarrow p(x)=q(x)，而正因为这个思想，所以所有推导过程都可以只在一元微积分下进行。但问题是，并不是所有的\\int d(p(x),q(x))dx=0\\Leftrightarrow p(x)=q(x)的结论都意味着有d(p,q)=0\\Leftrightarrow p=q，比如KL散度为\\int p(x)\\log \\frac{p(x)}{q(x)}dx，它等于0意味着p(x)=q(x)，但是p\\log\\frac{p}{q}=0不意味着一定p=q，因此，原论文至少没法导出KL散度对应的GAN出来。\
\
这样看来，原论文是不是就没有价值了？如果单看“产品”的话，确实是没有价值，因为它能产出的，f-GAN都能产出。但是我们不应当只关心“产品”，我们有时候也要关心“生产过程”。事实上， **笔者认为原论文的学术价值在于提供了一种直接在对偶空间中分析和发现GAN的参考方法，为我们了解GAN模型多添加了一个角度。**\
\
### 其实还可以推广 [\#](https://kexue.fm/archives/7210\#%E5%85%B6%E5%AE%9E%E8%BF%98%E5%8F%AF%E4%BB%A5%E6%8E%A8%E5%B9%BF)\
\
不管是f-GAN还是原论文，导出的GAN模型的生成器和判别器的loss都是同一个形式，只不过方向不同。但事实上，目前我们用得比较多的GAN变种，生成器和判别器的loss都不是同一个，比如原始GAN的一个更常用的形式是：\
\
\\begin{equation}\\begin{aligned}&\\,\\max\_{t(x)} \\mathbb{E}\_{x\\sim p(x)}\[\\log (1-\\sigma(t(x)))\] + \\mathbb{E}\_{x\\sim q(x)}\[\\log \\sigma(t(x))\]\\\\
&\\,\\min\_{q(x)} \\mathbb{E}\_{x\\sim q(x)}\[-\\log （1-\\sigma(t(x))）\]\
\\end{aligned}\\label{eq:ori-gan-2}\\end{equation}\
\
同样的例子还包括LSGAN、Hinge GAN等等。所以如果只考虑生成器和判别器的loss都是同样形式的变种，其实还是不够充分的。\
\
其实，这篇论文还可以再前进一步，得到比f-GAN更多的结果的，很可惜，作者们似乎把自己的思维绕到死胡同里边去了，并没有察觉到这一点。事实上做到这一点非常简单：在上面的过程中，通过\\max\\limits\_{t\\in\\Omega} \\phi(t)+r\\psi(t)这一步我们求出了t=\\omega(r)，然后代回原来的\\phi(t)+r\\psi(t)得到d(r)，但事实上我们没必要代回原来的式子呀，其实可以代回任意形如\\alpha(t)+r\\beta(t)的式子，根据式\\eqref{eq:f-gan}并结合开头列举的f散度的要求，只要d(r)=\\alpha(\\omega(r))+r\\beta(\\omega(r))是一个凸函数就行了（d(1)=0可以通过平移来实现），或者根据前面的推理d(r)是以r=1为最小值的任意函数也行。综合起来就是：\
\
> **结论2** 如果\\omega(r)是严格单调递增的，\\Omega=\\omega(\[0,+\\infty))，\\rho(t)在t\\in \\Omega时是恒正的，并且满足如下关系：\
> \\begin{equation}\\left\\{\\begin{aligned}\\phi'(t)=&\\,-\\omega^{-1}(t)\\rho(t)\\\\
> \\psi'(t)=&\\,\\rho(t)\\end{aligned}\
> \\right.\\end{equation}\
> 以及函数\\alpha(t),\\beta(t)使得d(r)=\\alpha(\\omega(r))+r\\beta(\\omega(r))是一个凸函数，或者使得d(r)是以r=1为最小值的函数，具备这些条件的的\\phi(t),\\psi(t),\\alpha(t),\\beta(t)就可以用来构建如下的GAN模型（其中\\min\\limits\_{q(x)}已经省去了与q(x)无关的\\alpha(t)部分）：\
> \\begin{equation}\\begin{aligned}\\max\_{t(x)\\in \\Omega} &\\,\\mathbb{E}\_{x\\sim p(x)}\[\\phi(t(x))\] + \\mathbb{E}\_{x\\sim q(x)}\[\\psi(t(x))\]\\\\
> \\min\_{q(x)}&\\,\\mathbb{E}\_{x\\sim q(x)}\[\\beta(t(x))\]\
> \\end{aligned}\\end{equation}\
\
### 推广后的一些例子 [\#](https://kexue.fm/archives/7210\#%E6%8E%A8%E5%B9%BF%E5%90%8E%E7%9A%84%E4%B8%80%E4%BA%9B%E4%BE%8B%E5%AD%90)\
\
用 **结论2** 来构造GAN就相当自由了，它可以构造出很多f-GAN找不出来的例子，因为它允许生成器和判别器的loss不一致。\
\
比如在算原始GAN的时候，我们得到t=\\omega(r)=\\log r，而r\\log r刚好就是凸函数，所以可以让\\alpha(t)=0,\\beta(t)=t（注意\\alpha(t)可以为0，\\beta(t)不可以，想想为什么？），得到d(r)=r\\log r，这时候的GAN为\
\
\\begin{equation}\\begin{aligned}\\max\_{t(x)} &\\,\\mathbb{E}\_{x\\sim p(x)}\[\\log (1-\\sigma(t(x)))\] + \\mathbb{E}\_{x\\sim q(x)}\[\\log \\sigma(t(x))\]\\\\
\\min\_{q(x)}&\\,\\mathbb{E}\_{x\\sim q(x)}\[t(x)\]\
\\end{aligned}\\end{equation}\
\
这便是一个挺好用的GAN变种。还有，既然r\\log r是凸函数，那么(1+r)\\log(1+r)也是，那么可以让\\alpha(t)=\\beta(t)=\\log(1+r)=\\log\\left(1+e^t\\right)，正好对应d(r)=(r+1)\\log(1+r)，并且\\log\\left(1+e^t\\right)=-\\log(1-\\sigma(t))，所以这时候对应的GAN就是\\eqref{eq:ori-gan-2}，这才是更常用的原始版本的GAN，比\\eqref{eq:ori-gan}更好用。\
\
我们再举一个例子。取t=\\omega(r)=\\frac{a + b r}{1 + r}，这里约定b > a，那么\\Omega=(a,b)（还是先不管边界点），并且r=\\omega^{-1}(t)=\\frac{t-a}{b-t}。我们取\\rho(t)=2(b-t)，那么它满足恒正的要求。这样解得\\phi(t)=-(t-a)^2,\\psi(t)=-(t-b)^2。接着取d(r)=\\frac{(r-1)^2}{r+1}，它显然是在r=1处取到最小值，然后设\\alpha(t)=\\beta(t)，试图反推\\beta(t)的形式，即d(r)=(1+r)\\beta(t)，推出\\beta(t)=\\left(\\frac{r-1}{r+1}\\right)^2，代入r=\\frac{t-a}{b-t}得到\\beta(t)=\\left(\\frac{2}{b-a}t-\\frac{a+b}{b-a}\\right)^2，简单起见我们可以让b-a=2，那么\\beta(t)=\\left(t-\\frac{a+b}{2}\\right)^2。最后得出的GAN形式为\
\
\\begin{equation}\\begin{aligned}\\max\_{t(x)\\in (a,b)} &\\,\\mathbb{E}\_{x\\sim p(x)}\\left\[-(t-a)^2\\right\] + \\mathbb{E}\_{x\\sim q(x)}\\left\[-(t-b)^2\\right\]\\\\
\\min\_{q(x)}&\\,\\mathbb{E}\_{x\\sim q(x)}\\left\[\\left(t-\\frac{a+b}{2}\\right)^2\\right\]\
\\end{aligned}\\end{equation}\
\
这其实就是LSGAN。读者可能会困惑LSGAN里边没有t(x)\\in (a,b)这个限制呀，这里怎么会有？事实上这个限制是可以去掉的，因为去掉这个限制后，对应的最优解还是在(a,b)里边。\
\
很显然，这些基于推广后的 **结论2** 所得到的GAN的变种是很有价值的，而且这些GAN变种是f-GAN没法得到的，因此如果原论文能补充这部分推广，那显得很漂亮了。\
\
## 再来个小结 [\#](https://kexue.fm/archives/7210\#%E5%86%8D%E6%9D%A5%E4%B8%AA%E5%B0%8F%E7%BB%93)\
\
本文分享了一篇直接在对偶空间设计GAN模型的论文，并且分析了它与f-GAN的联系，接着笔者对原论文的结果进行了简单的推广，使得它能设计更多样的GAN模型。\
\
最后，读者可能会疑问：f-GAN那里搞了那么多GAN出来，现在你这篇论文又搞那么多GAN出来，我们来来去去用的无非就那几个，搞那么多有什么价值呢？这个问题见仁见智了，这类工作更重要的应该是方法上的价值，实际应用的价值可能不会很大。不过笔者更想说的是：\
\
> 我也没说它有什么价值呀，我只是觉得它比较有意思罢了～\
\
_**转载到请包括本文地址：** [https://kexue.fm/archives/7210](https://kexue.fm/archives/7210 "Designing GANs：又一个GAN生产车间")_\
\
_**更详细的转载事宜请参考：**_ [《科学空间FAQ》](https://kexue.fm/archives/6508#%E6%96%87%E7%AB%A0%E5%A6%82%E4%BD%95%E8%BD%AC%E8%BD%BD/%E5%BC%95%E7%94%A8 "《科学空间FAQ》")\
\
**如果您还有什么疑惑或建议，欢迎在下方评论区继续讨论。**\
\
**如果您觉得本文还不错，欢迎 [分享](https://kexue.fm/archives/7210#share)/ [打赏](https://kexue.fm/archives/7210#pay) 本文。打赏并非要从中获得收益，而是希望知道科学空间获得了多少读者的真心关注。当然，如果你无视它，也不会影响你的阅读。再次表示欢迎和感谢！**\
\
打赏\
\
![科学空间](https://kexue.fm/usr/themes/geekg/payment/wx.png)\
\
微信打赏\
\
![科学空间](https://kexue.fm/usr/themes/geekg/payment/zfb.png)\
\
支付宝打赏\
\
因为网站后台对打赏并无记录，因此欢迎在打赏时候备注留言。\
\
你还可以 [**点击这里**](http://mail.qq.com/cgi-bin/qm_share?t=qm_mailme&email=tN7d1drY3drrx8H0xcWa19vZ) 或在下方评论区留言来告知你的建议或需求。\
\
**如果您需要引用本文，请参考：**\
\
苏剑林. (Feb. 13, 2020). 《Designing GANs：又一个GAN生产车间 》\[Blog post\]. Retrieved from [https://kexue.fm/archives/7210](https://kexue.fm/archives/7210)\
\
@online{kexuefm-7210,\
\
         title={Designing GANs：又一个GAN生产车间},\
\
         author={苏剑林},\
\
         year={2020},\
\
         month={Feb},\
\
         url={\\url{https://kexue.fm/archives/7210}},\
\
}\
\
\
分类： [数学研究](https://kexue.fm/category/Mathematics), [信息时代](https://kexue.fm/category/Big-Data)    标签： [微积分](https://kexue.fm/tag/%E5%BE%AE%E7%A7%AF%E5%88%86/), [GAN](https://kexue.fm/tag/GAN/), [生成模型](https://kexue.fm/tag/%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B/)[4 评论](https://kexue.fm/archives/7210#comments)\
\
< [你的CRF层的学习率可能不够大](https://kexue.fm/archives/7196 "你的CRF层的学习率可能不够大") \| [CRF用过了，不妨再了解下更快的MEMM？](https://kexue.fm/archives/7213 "CRF用过了，不妨再了解下更快的MEMM？") >\
\
### 你也许还对下面的内容感兴趣\
\
- [生成扩散模型漫谈（三十一）：预测数据而非噪声](https://kexue.fm/archives/11428 "生成扩散模型漫谈（三十一）：预测数据而非噪声")\
- [DiVeQ：一种非常简洁的VQ训练方案](https://kexue.fm/archives/11328 "DiVeQ：一种非常简洁的VQ训练方案")\
- [为什么线性注意力要加Short Conv？](https://kexue.fm/archives/11320 "为什么线性注意力要加Short Conv？")\
- [Transformer升级之路：21、MLA好在哪里?（下）](https://kexue.fm/archives/11111 "Transformer升级之路：21、MLA好在哪里?（下）")\
- [线性注意力简史：从模仿、创新到反哺](https://kexue.fm/archives/11033 "线性注意力简史：从模仿、创新到反哺")\
- [msign的导数](https://kexue.fm/archives/11025 "msign的导数")\
- [生成扩散模型漫谈（三十）：从瞬时速度到平均速度](https://kexue.fm/archives/10958 "生成扩散模型漫谈（三十）：从瞬时速度到平均速度")\
- [Transformer升级之路：20、MLA好在哪里?（上）](https://kexue.fm/archives/10907 "Transformer升级之路：20、MLA好在哪里?（上）")\
- [SVD的导数](https://kexue.fm/archives/10878 "SVD的导数")\
- [生成扩散模型漫谈（二十九）：用DDPM来离散编码](https://kexue.fm/archives/10711 "生成扩散模型漫谈（二十九）：用DDPM来离散编码")\
\
[发表你的看法](https://kexue.fm/archives/7210#comment_form)\
\
朱孟垚\
\
February 20th, 2020\
\
偶然检查到您的几篇笔记文章，写的很清楚明白，质量很高。\
\
希望您能不吝分享一直努力下去，您的工作对于很多学习者来说意义很大，如果可以的话想向您请教一些问题。\
\
[回复评论](https://kexue.fm/archives/7210/comment-page-1?replyTo=12893#respond-post-7210)\
\
[苏剑林](https://kexue.fm/) 发表于\
February 21st, 2020\
\
感觉你的支持和鼓励，我会尽力的，欢迎多多交流～\
\
[回复评论](https://kexue.fm/archives/7210/comment-page-1?replyTo=12894#respond-post-7210)\
\
sxsong\
\
May 18th, 2021\
\
'并且由于ψ(ω(r))连续且严格单调递增，所以d′(r)是先负后正'\
\
您好,请问这里的d'(r)是否为笔误? 单调递增导函数应为恒正.\
\
[回复评论](https://kexue.fm/archives/7210/comment-page-1?replyTo=16434#respond-post-7210)\
\
[苏剑林](https://kexue.fm/) 发表于\
September 3rd, 2021\
\
没有错误，请看(11)，是d'(r)=\\psi(\\omega(r))。\
\
[回复评论](https://kexue.fm/archives/7210/comment-page-1?replyTo=17257#respond-post-7210)\
\
[取消回复](https://kexue.fm/archives/7210#respond-post-7210)\
\
你的大名\
\
电子邮箱\
\
个人网站（选填）\
\
1\. 可以使用LaTeX代码，点击“预览效果”可查看效果；\
\
2\. 可以通过点击评论楼层编号来引用该楼层；\
\
3\. 网站可能会有点卡，如非确认评论失败，请 **不要重复点击提交**。\
\
### 内容速览\
\
[f-GAN回顾](https://kexue.fm/archives/7210#f-GAN%E5%9B%9E%E9%A1%BE)\
[Designing GANs](https://kexue.fm/archives/7210#Designing%20GANs)\
[Total Variation](https://kexue.fm/archives/7210#Total%20Variation)\
[求导找极大值](https://kexue.fm/archives/7210#%E6%B1%82%E5%AF%BC%E6%89%BE%E6%9E%81%E5%A4%A7%E5%80%BC)\
[求导找极小值](https://kexue.fm/archives/7210#%E6%B1%82%E5%AF%BC%E6%89%BE%E6%9E%81%E5%B0%8F%E5%80%BC)\
[GAN模型已经送达](https://kexue.fm/archives/7210#GAN%E6%A8%A1%E5%9E%8B%E5%B7%B2%E7%BB%8F%E9%80%81%E8%BE%BE)\
[思考与延伸](https://kexue.fm/archives/7210#%E6%80%9D%E8%80%83%E4%B8%8E%E5%BB%B6%E4%BC%B8)\
[与f-GAN的联系](https://kexue.fm/archives/7210#%E4%B8%8Ef-GAN%E7%9A%84%E8%81%94%E7%B3%BB)\
[其实还可以推广](https://kexue.fm/archives/7210#%E5%85%B6%E5%AE%9E%E8%BF%98%E5%8F%AF%E4%BB%A5%E6%8E%A8%E5%B9%BF)\
[推广后的一些例子](https://kexue.fm/archives/7210#%E6%8E%A8%E5%B9%BF%E5%90%8E%E7%9A%84%E4%B8%80%E4%BA%9B%E4%BE%8B%E5%AD%90)\
[再来个小结](https://kexue.fm/archives/7210#%E5%86%8D%E6%9D%A5%E4%B8%AA%E5%B0%8F%E7%BB%93)\
\
### 智能搜索\
\
支持整句搜索！网站自动使用 [结巴分词](https://github.com/fxsjy/jieba) 进行分词，并结合ngrams排序算法给出合理的搜索结果。\
\
### 热门标签\
\
[生成模型](https://kexue.fm/tag/%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B/) [attention](https://kexue.fm/tag/attention/) [优化](https://kexue.fm/tag/%E4%BC%98%E5%8C%96/) [语言模型](https://kexue.fm/tag/%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/) [模型](https://kexue.fm/tag/%E6%A8%A1%E5%9E%8B/) [梯度](https://kexue.fm/tag/%E6%A2%AF%E5%BA%A6/) [网站](https://kexue.fm/tag/%E7%BD%91%E7%AB%99/) [概率](https://kexue.fm/tag/%E6%A6%82%E7%8E%87/) [矩阵](https://kexue.fm/tag/%E7%9F%A9%E9%98%B5/) [优化器](https://kexue.fm/tag/%E4%BC%98%E5%8C%96%E5%99%A8/) [转载](https://kexue.fm/tag/%E8%BD%AC%E8%BD%BD/) [微分方程](https://kexue.fm/tag/%E5%BE%AE%E5%88%86%E6%96%B9%E7%A8%8B/) [分析](https://kexue.fm/tag/%E5%88%86%E6%9E%90/) [天象](https://kexue.fm/tag/%E5%A4%A9%E8%B1%A1/) [深度学习](https://kexue.fm/tag/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/) [积分](https://kexue.fm/tag/%E7%A7%AF%E5%88%86/) [python](https://kexue.fm/tag/python/) [扩散](https://kexue.fm/tag/%E6%89%A9%E6%95%A3/) [力学](https://kexue.fm/tag/%E5%8A%9B%E5%AD%A6/) [无监督](https://kexue.fm/tag/%E6%97%A0%E7%9B%91%E7%9D%A3/) [几何](https://kexue.fm/tag/%E5%87%A0%E4%BD%95/) [节日](https://kexue.fm/tag/%E8%8A%82%E6%97%A5/) [生活](https://kexue.fm/tag/%E7%94%9F%E6%B4%BB/) [文本生成](https://kexue.fm/tag/%E6%96%87%E6%9C%AC%E7%94%9F%E6%88%90/) [数论](https://kexue.fm/tag/%E6%95%B0%E8%AE%BA/)\
\
### 随机文章\
\
- [简单做了个Logo~](https://kexue.fm/archives/2709)\
- [力学系统及其对偶性（一）](https://kexue.fm/archives/2121)\
- [初试在Python中使用PARI/GP](https://kexue.fm/archives/2775)\
- [【NASA每日一图】撞击目标：凯布斯月球坑](https://kexue.fm/archives/178)\
- [这样的世界之最你见过没有？](https://kexue.fm/archives/36)\
- [“末日”的快乐！](https://kexue.fm/archives/1840)\
- [利用“熄火保护 \+ 通断器”实现燃气灶智能关火](https://kexue.fm/archives/10394)\
- [科学空间\|Scientific Spaces 介绍](https://kexue.fm/archives/12)\
- [用Numpy实现高效的Apriori算法](https://kexue.fm/archives/5525)\
- [【NASA每日一图】明亮的超新星爆发](https://kexue.fm/archives/48)\
\
### 最近评论\
\
- [Bin](https://kexue.fm/archives/1990/comment-page-2#comment-29105): 今天偶然从某个论坛看到有人推荐您的博客，定睛一看竟然是华师同院的往届师兄！看到这篇2013年的...\
- [Rapture D](https://kexue.fm/archives/11530/comment-page-1#comment-29104): 我有一个问题，为什么不考虑亥姆霍兹定理和斯托克斯公式。\
- [mofheka](https://kexue.fm/archives/11390/comment-page-1#comment-29103): 苏神是还在用jax是么？最近在做基于Google Pathway的理念做一个动态版的MPMD框...\
- [长琴](https://kexue.fm/archives/11530/comment-page-1#comment-29102): 看懂这篇博客也不是一件容易的事情。\
- [AlexLi](https://kexue.fm/archives/9257/comment-page-4#comment-29101): 苏老师，请教一下(7)式中将 \\mu(x\_t) 传给 p\_o 进行推理的操作。 $x\_...\
- [tyler\_zxc](https://kexue.fm/archives/7921/comment-page-2#comment-29100): "Performer的思想是将标准的Attention线性化，所以为什么不干脆直接训练一个线性...\
- [我](https://kexue.fm/archives/11494/comment-page-1#comment-29099): 似乎并非mHC提出矩阵的思想？之前hyper connection就是了\
- [winter](https://kexue.fm/archives/10847/comment-page-1#comment-29098): 苏神您好，假如对于比较均匀的attention weightP，往往呈现long tail分布...\
- [苏剑林](https://kexue.fm/archives/8512/comment-page-2#comment-29097): KL散度、JS散度、W距离啥的，都行啊，看你喜欢哪个\
- [苏剑林](https://kexue.fm/archives/9119/comment-page-14#comment-29096): 没有绝对公平的对比方法，主要看你关心什么。比如，如果只关心推理成本和推理效果，那么有的方法可以...\
\
### 友情链接\
\
- [Cool Papers](https://papers.cool/)\
- [数学研发](https://bbs.emath.ac.cn/)\
- [Seatop](http://www.seatop.com.cn/)\
- [Xiaoxia](https://xiaoxia.org/)\
- [积分表-网络版](https://kexue.fm/sci/integral/index.html)\
- [丝路博傲](http://blog.dvxj.com/)\
- [数学之家](http://www.2math.cn/)\
- [有趣天文奇观](http://interesting-sky.china-vo.org/)\
- [TwistedW](http://www.twistedwg.com/)\
- [godweiyang](https://godweiyang.com/)\
- [AI柠檬](https://blog.ailemon.net/)\
- [王登科-DK博客](https://greatdk.com/)\
- [ESON](https://blog.eson.org/)\
- [枫之羽](https://fzhiy.net/)\
- [coding-zuo](https://coding-zuo.github.io/)\
- [博科园](https://www.bokeyuan.net/)\
- [孔皮皮的博客](https://www.kppkkp.top/)\
- [运鹏的博客](https://yunpengtai.top/)\
- [jiming.site](https://jiming.site/)\
- [OmegaXYZ](https://www.omegaxyz.com/)\
- [EAI猩球](https://www.robotech.ink/)\
- [文举的博客](https://liwenju0.com/)\
- [申请链接](https://kexue.fm/links.html)\
\
[![署名-非商业用途-保持一致](https://kexue.fm/usr/themes/geekg/images/cc.gif)](http://creativecommons.org/licenses/by-nc-nd/2.5/cn/) 本站采用创作共用版权协议，要求署名、非商业用途和保持一致。转载本站内容必须也遵循“ [署名-非商业用途-保持一致](http://creativecommons.org/licenses/by-nc-nd/2.5/cn/)”的创作共用协议。\
\
\
\
© 2009-2026 Scientific Spaces. All rights reserved. Theme by [laogui](http://www.laogui.com/). Powered by [Typecho](http://typecho.org/). 备案号: [粤ICP备09093259号-1/2](https://beian.miit.gov.cn/ "粤ICP备09093259号")。