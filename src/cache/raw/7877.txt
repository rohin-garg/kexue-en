## SEARCH

## MENU

- [打赏](https://kexue.fm/reward.html)
- [公式](https://kexue.fm/latex.html)
- [天象](https://kexue.fm/ac.html)
- [链接](https://kexue.fm/links.html)
- [时光](https://kexue.fm/me.html)
- [博览](https://kexue.fm/science.html)
- [归档](https://kexue.fm/content.html)

## CATEGORIES

- [千奇百怪](https://kexue.fm/category/Everything)
- [天文探索](https://kexue.fm/category/Astronomy)
- [数学研究](https://kexue.fm/category/Mathematics)
- [物理化学](https://kexue.fm/category/Phy-chem)
- [信息时代](https://kexue.fm/category/Big-Data)
- [生物自然](https://kexue.fm/category/Biology)
- [图片摄影](https://kexue.fm/category/Photograph)
- [问题百科](https://kexue.fm/category/Questions)
- [生活/情感](https://kexue.fm/category/Life-Feeling)
- [资源共享](https://kexue.fm/category/Resources)

## NEWPOSTS

- [从无穷范数求导到等值振荡定理](https://kexue.fm/archives/10972)
- [生成扩散模型漫谈（三十）：从瞬时速...](https://kexue.fm/archives/10958)
- [MoE环游记：5、均匀分布的反思](https://kexue.fm/archives/10945)
- [msign算子的Newton-Sc...](https://kexue.fm/archives/10922)
- [Transformer升级之路：2...](https://kexue.fm/archives/10907)
- [一道概率不等式：盯着它到显然成立为止！](https://kexue.fm/archives/10902)
- [SVD的导数](https://kexue.fm/archives/10878)
- [智能家居之手搓一套能接入米家的零冷水装置](https://kexue.fm/archives/10869)
- [Transformer升级之路：1...](https://kexue.fm/archives/10862)
- [矩阵的有效秩（Effective ...](https://kexue.fm/archives/10847)

## COMMENTS

- [OceanYU: 您好，关于由式（7）推导出高斯分布，我这里有一点问题，式（7）...](https://kexue.fm/archives/9164/comment-page-4#comment-27801)
- [jorjiang: 训练和prefill这个compute-bound阶段不做矩阵...](https://kexue.fm/archives/10907/comment-page-2#comment-27800)
- [amy: 苏老师，您有关注傅里叶旋转位置编码这篇工作吗，想知道您对这篇工...](https://kexue.fm/archives/10907/comment-page-2#comment-27799)
- [jiurizz: 在2\*shared experts + 160\*routed ...](https://kexue.fm/archives/10945/comment-page-1#comment-27798)
- [开水: 感谢苏老师回复，论文appendix里面写了实验是load b...](https://kexue.fm/archives/10945/comment-page-1#comment-27797)
- [苏剑林: 欢迎作者，这篇文章确实在收藏夹了，结果还没来得及看，抱歉哈，马...](https://kexue.fm/archives/10958/comment-page-1#comment-27796)
- [苏剑林: “将存储kv cache改为存储降维后的Embedding X...](https://kexue.fm/archives/10907/comment-page-2#comment-27795)
- [苏剑林: 呃，是这样的，Moonlight-16B-A3B实际上对应的是...](https://kexue.fm/archives/10945/comment-page-1#comment-27794)
- [Jiaming Song: 推销一下我们前一段时间的工作，这个其实已经达到了你说的三个标准...](https://kexue.fm/archives/10958/comment-page-1#comment-27793)
- [ZhouTimeMachine: 感谢您的回复！这么一看，我才注意到能从 $p(x\_0)$ 变换...](https://kexue.fm/archives/9497/comment-page-2#comment-27792)

## USERLOGIN

- [登录](https://kexue.fm/admin/login.php)

[科学空间\|Scientific Spaces](https://kexue.fm)

- [登录](https://kexue.fm/admin/login.php)
- [打赏](https://kexue.fm/reward.html)
- [公式](https://kexue.fm/latex.html)
- [天象](https://kexue.fm/ac.html)
- [链接](https://kexue.fm/links.html)
- [时光](https://kexue.fm/me.html)
- [博览](https://kexue.fm/science.html)
- [归档](https://kexue.fm/content.html)

渴望成为一个小飞侠

- [欢迎订阅](https://kexue.fm/feed)
- [个性邮箱](https://kexue.fm/archives/119)
- [天象信息](https://kexue.fm/ac.html)
- [观测ISS](https://kexue.fm/archives/41)
- [LaTeX](https://kexue.fm/latex.html)
- [关于博主](https://kexue.fm/me.html)

欢迎访问“科学空间”，这里将与您共同探讨自然科学，回味人生百态；也期待大家的分享～

- [**千奇百怪** Everything](https://kexue.fm/category/Everything)
- [**天文探索** Astronomy](https://kexue.fm/category/Astronomy)
- [**数学研究** Mathematics](https://kexue.fm/category/Mathematics)
- [**物理化学** Phy-chem](https://kexue.fm/category/Phy-chem)
- [**信息时代** Big-Data](https://kexue.fm/category/Big-Data)
- [**生物自然** Biology](https://kexue.fm/category/Biology)
- [**图片摄影** Photograph](https://kexue.fm/category/Photograph)
- [**问题百科** Questions](https://kexue.fm/category/Questions)
- [**生活/情感** Life-Feeling](https://kexue.fm/category/Life-Feeling)
- [**资源共享** Resources](https://kexue.fm/category/Resources)

- [**千奇百怪**](https://kexue.fm/category/Everything)
- [**天文探索**](https://kexue.fm/category/Astronomy)
- [**数学研究**](https://kexue.fm/category/Mathematics)
- [**物理化学**](https://kexue.fm/category/Phy-chem)
- [**信息时代**](https://kexue.fm/category/Big-Data)
- [**生物自然**](https://kexue.fm/category/Biology)
- [**图片摄影**](https://kexue.fm/category/Photograph)
- [**问题百科**](https://kexue.fm/category/Questions)
- [**生活/情感**](https://kexue.fm/category/Life-Feeling)
- [**资源共享**](https://kexue.fm/category/Resources)

[首页](https://kexue.fm) [信息时代](https://kexue.fm/category/Big-Data) 当GPT遇上中国象棋：写过文章解过题，要不再来下盘棋？

11Nov

# [当GPT遇上中国象棋：写过文章解过题，要不再来下盘棋？](https://kexue.fm/archives/7877)

By 苏剑林 \|
2020-11-11 \|
65102位读者\|

中国象棋

不知道读者有没有看过量子位年初的文章 [《最强写作AI竟然学会象棋和作曲，语言模型跨界操作引热议，在线求战》](https://mp.weixin.qq.com/s/PQysIztZpkDse4hMS0tw3Q)，里边提到有网友用GPT2模型训练了一个下国际象棋的模型。笔者一直在想，这么有趣的事情怎么可以没有中文版呢？对于国际象棋来说，其中文版自然就是中国象棋了，于是我一直有想着把它的结果在中国象棋上面复现一下。拖了大半年，在最近几天终于把这个事情完成了，在此跟大家分享一下。

> **象棋谱式** 将军不离九宫内，士止相随不出官。象飞四方营四角，马行一步一尖冲。炮须隔子打一子，车行直路任西东。唯卒只能行一步，过河横进退无踪。

## 背棋谱 [\#](https://kexue.fm/archives/7877\#%E8%83%8C%E6%A3%8B%E8%B0%B1)

其实，简单看看量子位的文章，就能理解GPT2下象棋的原理了，无非就是“背棋谱”。简单来说，就是象棋的棋谱可以表示为一个连续的文本字符串，而GPT2正是擅长于背诵文本，因此可以用GPT2把人类的棋谱都背诵下来，而对于下棋来说，就可以看成是根据已经走的部分棋谱背诵下一步棋谱了，因此整个任务理论上确实是可以用GPT2完成。

为了完成这个任务，我们就需要了解计算机是如何记谱的。关于记谱的标准，比较通用的是ICCS记谱法和FEN局面表示法，其细节可以参考文章 [《中国象棋电脑应用规范(二)：着法表示》](https://www.xqbase.com/protocol/cchess_move.htm) 和 [《中国象棋电脑应用规范(二)：FEN文件格式》](https://www.xqbase.com/protocol/cchess_fen.htm)。

### ICCS记谱 [\#](https://kexue.fm/archives/7877\#ICCS%E8%AE%B0%E8%B0%B1)

简单来说，ICCS记谱就是将棋盘用如下图的横纵坐标表示，每一步走法只需要记录起始坐标，比如“h2e2”就是指将原来位于坐标(h, 2)的子移动到(e, 2)，如果是当前局面是新开局，那么这就对应着走法“炮二平五”。这样一来每一步就只需要4个字符来记录了，$n$步的棋谱就变成了$4n$长度的字符串了。当然，如果要输入到模型的话，不一定非得要按照这样的方式来，比如我也可以把“h2”只用一个id表示、“e2”用另一个id表示，也就是每个格点都用一个坐标而不是两个坐标来描述，这样每一步的只需要两个id来记录，以此来缩小棋谱的序列长度，这没有什么定法，有兴趣大家自己改进着完就好。

中国象棋棋盘ICCS坐标示意图

### FEN局面 [\#](https://kexue.fm/archives/7877\#FEN%E5%B1%80%E9%9D%A2)

至于FEN局面表示法，则是用来表示当前局面有哪些子，轮到谁走。本文所建模的棋谱实际上都是全局棋谱，所以实际上本文的模型不需要用到它（局面都是默认的新开局面），不过为了方便有兴趣的读者做出改进，这里也简单介绍下。所谓FEN表示法，主要就是想办法表示出每一行有哪些子，比如开局表示为“rnbakabnr/9/1c5c1/p1p1p1p1p/9/9/P1P1P1P1P/1C5C1/9/RNBAKABNR w \- \- 0 1”。其中红色部分表示局面，小写表示黑方，大写表示红方，不同行之间用/隔开，字母含义如下表。这样，“rnbakabnr”就表示第一行为黑子的“車馬象士將士象馬車”，“9”表示第二行9个点都是空白的，“1c5c1”表示第三行是“1个空白+1个黑砲+5个空白+1个黑砲+1个空白”，等等；绿色部分表示轮到哪一方走子，“w”表示红方，“b”表示黑方；剩下部分一般不大重要，有兴趣的读者自己去看链接就行。

\\begin{array}{c}
\\text{中国象棋fen表示法含义表} \\\
{\\begin{array}{c\|c\|c\|c\|c\|c\|c}
\\hline
\\color{red}{R}/\\color{black}{r} & \\color{red}{N}/\\color{black}{n} & \\color{red}{B}/\\color{black}{b} & \\color{red}{A}/\\color{black}{a} & \\color{red}{K}/\\color{black}{k} & \\color{red}{C}/\\color{black}{c} & \\color{red}{P}/\\color{black}{p} & \\text{阿拉伯数字}m\\\
\\hline
\\color{red}{\\text{俥}}/\\color{black}{\\text{車}} & \\color{red}{\\text{傌}}/\\color{black}{\\text{馬}} & \\color{red}{\\text{相}}/\\color{black}{\\text{象}} & \\color{red}{\\text{仕}}/\\color{black}{\\text{士}} & \\color{red}{\\text{帥}}/\\color{black}{\\text{將}} & \\color{red}{\\text{炮}}/\\color{black}{\\text{砲}} & \\color{red}{\\text{兵}}/\\color{black}{\\text{卒}} & \\text{表示连续}m\\text{个空位} \\\
\\hline
\\end{array}}
\\end{array}

## 建模型 [\#](https://kexue.fm/archives/7877\#%E5%BB%BA%E6%A8%A1%E5%9E%8B)

看了上述对记谱表示的介绍，我们知道，不管是每步的走法还是局面的表示，都被我们转化为了一串文本了，而对于象棋的推演都是以局面与走法作为输入输出的，所以理论上来说象棋的建模完全就是一个“文本处理”问题！这便是GPT2下象棋的理论依据了。本质上来讲，GPT也就是BERT加上语言模型的Attention Mask，所以这样的做法我们说是BERT下象棋或者GPT下象棋都行～

### 代码分享 [\#](https://kexue.fm/archives/7877\#%E4%BB%A3%E7%A0%81%E5%88%86%E4%BA%AB)

模型原理就没什么好写的了，之前就有文章 [《从语言模型到Seq2Seq：Transformer如戏，全靠Mask》](https://kexue.fm/archives/6933) 介绍，相关的例子有 [《基于Conditional Layer Normalization的条件文本生成》](https://kexue.fm/archives/7124)、 [《BERT可以上几年级了？Seq2Seq“硬刚”小学数学应用题》](https://kexue.fm/archives/7809) 等，读者可以自行翻看。本文的处理其实很简单，就是只保留全局棋谱，将棋谱的ICCS记法当成一个长句子，然后训练一个语言模型。

**项目链接： [https://github.com/bojone/gpt\_cchess](https://github.com/bojone/gpt_cchess)**

训练过程使用渐进式训练，即逐步增加序列长度，而不是一次性使用同一的长度，有些文章将这种做法称之为“课程学习（Curriculum Learning）”。这种做法能提高模型的训练速度和收敛速度（一开始序列更短，训练速度更快，也更容易收敛），直观来看就是让模型先学习“开局”，然后再学习“开局+中局”，最后学习“开局+中局+残局”，逐步增加难度。训练前加载了BERT的权重，可能读者会疑问BERT的权重还能跟棋谱有关系？其实没什么关系，但是不管怎么说，用BERT的权重比完全随机初始化的权重要好点，收敛会快一点点。

### 测试一下 [\#](https://kexue.fm/archives/7877\#%E6%B5%8B%E8%AF%95%E4%B8%80%E4%B8%8B)

模型脚本还包含了一个可以跟模型交互式下棋的实现，读者可以自行体验一下，这个交互式下棋使用了python的 [cchess模块](https://github.com/walker8088/cchess) 来辅助实现，在此表示感谢。GPT本身是一个生成模型，但是在决定下一步棋走什么的时候，笔者并不是用生成式方法（因为无约束生成有可能输出不可行的走法），而是用打分式的方法，即直接生成当前局面的所有可行走法，然后输入到模型打分，取分数最高的那个走法，这样就保证模型输出的每一步都是可行的，保证了可以跟AI一直对局下去，直到分出输赢。

交互式下棋效果

也许读者可能会有疑问，枚举所有可行走法计算量会不会很大？其实，对于每个局面来说，可行走法并不多，可以通过简单论证它不会超过111种（是不是有点意外？中国象棋每一步的候选走法不超过111种，而不是一个非常大的数字），所以这一步的batch\_size不会超过111，因此是可以接受的。

> 推导过程很简单：1个车或炮最多有17种走法，2车2炮最多有68种走法；兵如果都过河了，那么每个兵最多有3种走法，5个兵最多有15种走法；1个马最多有8种走法，2个马就是16种；2个相最多有6种走法（1个在中间，1个在边，4+2）；1个士在花心最多有4种走法（2个士反而相互堵塞）；最后的帅最多有2种走法。因此结果是68+15+16+6+4+2=111。具体局面设计可以参考数学研发论坛的 [《一个中国象棋局面设计难题》](https://bbs.emath.ac.cn/thread-17051-1-1.html)。

大家最关心的可能就是这样弄出来的模型棋力究竟怎样？笔者简单跟它测了一下，大概的结论是：基本上可以开一个比较好的局，开局的时候具有不错的应变能力，不过一旦到了中局之后，应变能力会大大下降。对于吃子不是很敏感，也就是说当你乱吃它的子的时候，它可能不会应对。可以看出这些其实都是纯背棋谱的缺点。当然，前面说了每一步的输出都是可行，因此你可以跟它一直玩下去，直到把棋下完。

## 谈改进 [\#](https://kexue.fm/archives/7877\#%E8%B0%88%E6%94%B9%E8%BF%9B)

应该有读者会问能不能自己跟自己对弈来提高棋力？理论上当然是可以的，但很遗憾这里没有实现，一是没那个心思实现，二是没那个算力实现。此外，增加模型大小应该也能进一步提升棋力，要注意笔者上述结果只用了Base版本（1亿参数）的模型，本文开头提到的网友用GPT2下国际象棋可是用了15亿参数的GPT2，是我们的15倍。还有一个改进的地方，那就是上面的建模中我们直接学习了整个对局棋谱，按道理为了更好的棋力我们可以只学习赢家的走法，不能学习输家的走法。

当然，这些做法就算有提升，估计也是有限的，归根结底，这跟我们所理解的下棋原理不一样。我们下棋是根据局面形势往前推的，但上述的语言模型做法则没有局面这个概念，或者说它的局面需要用已经走的所有步骤来确定，这对于中后局来说历史步数太多，确实有点强模型所难了。改进方法其实也很简单，改为“以局面为输入、以走法为输出”就好了，前面我们说了，局面也可以用FEN表示法表示为一个文本，因此这也是只是个Seq2Seq任务而已。

除此之外，还有一些别的做法，比如我们可以把赢家的每一个局面都当作正样本，输家的每一个局面都当作负样本，那么就可以训练一个二分类模型，来判断局面优劣，有了这个判断函数，我们也可以直接枚举每一个可行走法，根据判断函数的结果来选择最优下法。而局面可以表示为文本，这就意味这我们将下棋变成了一个文本分类任务了。

总之，得益于Transformer模型对文本的强大的建模能力，这使得我们对下棋的建模思路也变得简单多样起来了～

## 总小结 [\#](https://kexue.fm/archives/7877\#%E6%80%BB%E5%B0%8F%E7%BB%93)

本文尝试了通过bert4keras用GPT来下中国象棋的做法，主要思路是通过“语言模型背棋谱”的方式来让模型具有预测下一步的能力，并谈及了一些改进思路。尽管本文的做法并非对下棋这个任务进行建模的标准做法，但通过这样的方式，能让我们进一步体会到语言模型的强大之处。

欢迎大家报告自己所训练的下棋模型的棋力智商，哈哈～

送你一对象

_**转载到请包括本文地址：** [https://kexue.fm/archives/7877](https://kexue.fm/archives/7877)_

_**更详细的转载事宜请参考：**_ [《科学空间FAQ》](https://kexue.fm/archives/6508#%E6%96%87%E7%AB%A0%E5%A6%82%E4%BD%95%E8%BD%AC%E8%BD%BD/%E5%BC%95%E7%94%A8)

**如果您还有什么疑惑或建议，欢迎在下方评论区继续讨论。**

**如果您觉得本文还不错，欢迎 [分享](https://kexue.fm/archives/7877#share)/ [打赏](https://kexue.fm/archives/7877#pay) 本文。打赏并非要从中获得收益，而是希望知道科学空间获得了多少读者的真心关注。当然，如果你无视它，也不会影响你的阅读。再次表示欢迎和感谢！**

打赏

微信打赏

支付宝打赏

因为网站后台对打赏并无记录，因此欢迎在打赏时候备注留言。你还可以 [**点击这里**](http://mail.qq.com/cgi-bin/qm_share?t=qm_mailme&email=tN7d1drY3drrx8H0xcWa19vZ) 或在下方评论区留言来告知你的建议或需求。

**如果您需要引用本文，请参考：**

苏剑林. (Nov. 11, 2020). 《当GPT遇上中国象棋：写过文章解过题，要不再来下盘棋？ 》\[Blog post\]. Retrieved from [https://kexue.fm/archives/7877](https://kexue.fm/archives/7877)

@online{kexuefm-7877,
        title={当GPT遇上中国象棋：写过文章解过题，要不再来下盘棋？},
        author={苏剑林},
        year={2020},
        month={Nov},
        url={\\url{https://kexue.fm/archives/7877}},
}

分类： [信息时代](https://kexue.fm/category/Big-Data)    标签： [中国象棋](https://kexue.fm/tag/%E4%B8%AD%E5%9B%BD%E8%B1%A1%E6%A3%8B/), [语言模型](https://kexue.fm/tag/%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/), [attention](https://kexue.fm/tag/attention/)[19 评论](https://kexue.fm/archives/7877#comments)

< [那个屠榜的T5模型，现在可以在中文上玩玩了](https://kexue.fm/archives/7867) \| [也来谈谈RNN的梯度消失/爆炸问题](https://kexue.fm/archives/7888) >

### 你也许还对下面的内容感兴趣

- [Transformer升级之路：20、MLA究竟好在哪里？](https://kexue.fm/archives/10907)
- [Transformer升级之路：19、第二类旋转位置编码](https://kexue.fm/archives/10862)
- [细水长flow之TARFLOW：流模型满血归来？](https://kexue.fm/archives/10667)
- [“闭门造车”之多模态思路浅谈（三）：位置编码](https://kexue.fm/archives/10352)
- [Decoder-only的LLM为什么需要位置编码？](https://kexue.fm/archives/10347)
- [Monarch矩阵：计算高效的稀疏型矩阵分解](https://kexue.fm/archives/10249)
- [Transformer升级之路：18、RoPE的底数选择原则](https://kexue.fm/archives/10122)
- [缓存与效果的极限拉扯：从MHA、MQA、GQA到MLA](https://kexue.fm/archives/10091)
- [Transformer升级之路：17、多模态位置编码的简单思考](https://kexue.fm/archives/10040)
- [时空之章：将Attention视为平方复杂度的RNN](https://kexue.fm/archives/10017)

[发表你的看法](https://kexue.fm/archives/7877#comment_form)

云天河

November 12th, 2020

苏神 可否分享一个 训练好的权重，这个batchsize的roberta训练太吃力

[回复评论](https://kexue.fm/archives/7877/comment-page-1?replyTo=14755#respond-post-7877)

[苏剑林](https://kexue.fm) 发表于
November 12th, 2020

我就知道有人会提这个需求...已经分享了，请刷新到github首页找～

[回复评论](https://kexue.fm/archives/7877/comment-page-1?replyTo=14760#respond-post-7877)

MECH SIHAO

November 12th, 2020

苏神，用bert4keras的pretraining代码微调模型，职能保存ckpt文件，请问怎么保存bert\_model.ckpt.meta文件呢？

[回复评论](https://kexue.fm/archives/7877/comment-page-1?replyTo=14769#respond-post-7877)

MECH SIHAO 发表于
November 13th, 2020

就是把训练好的roberta模型保存成标准的tf格式，以便其他人调用，或者有什么方法能转换成tf的标准模型呢，求解

[回复评论](https://kexue.fm/archives/7877/comment-page-1?replyTo=14770#respond-post-7877)

[苏剑林](https://kexue.fm) 发表于
November 13th, 2020

有save\_weights\_as\_checkpoint方法可以转，最新版的save\_weights\_as\_checkpoint支持保存meta了（旧版的没有meta）。

[回复评论](https://kexue.fm/archives/7877/comment-page-1?replyTo=14771#respond-post-7877)

MECH SIHAO 发表于
November 13th, 2020

苏神说的是Transformer类下的save\_weights\_as\_checkpoint方法吗？但是在pretraining这个函数中，训练的模型train\_model没有这个方法

[回复评论](https://kexue.fm/archives/7877/comment-page-1?replyTo=14772#respond-post-7877)

MECH SIHAO 发表于
November 13th, 2020

苏神我直接在：
with strategy.scope():
train\_model = build\_transformer\_model\_for\_pretraining()
train\_model.summary()
这段代码前返回了bert：
with strategy.scope():
bert, train\_model = build\_transformer\_model\_for\_pretraining()
train\_model.summary()
然后在回调函数里面用bert.save\_weights\_as\_checkpoint(model\_save\_path)
这样确实可以保存tf标准模型了，你看这样可以么？

[回复评论](https://kexue.fm/archives/7877/comment-page-1?replyTo=14773#respond-post-7877)

[苏剑林](https://kexue.fm) 发表于
November 13th, 2020

是的，就是这样。

[回复评论](https://kexue.fm/archives/7877/comment-page-1?replyTo=14774#respond-post-7877)

MECH SIHAO 发表于
November 13th, 2020

好的，谢谢苏神

MECH SIHAO 发表于
November 13th, 2020

那，苏神，save\_weights\_as\_checkpoint的转换方法可以告诉下吗？就是之前没用这种方法保存的模型，现在想转换成tf标准模型，要怎么做呢？

[苏剑林](https://kexue.fm) 发表于
November 13th, 2020

[@MECH SIHAO\|comment-14780](https://kexue.fm/archives/7877/comment-page-1#comment-14780)

很简单啊，你只需要用load\_weights把原来的模型加载进去，然后再用save\_weights\_as\_checkpoint保存不就行了～

MECH SIHAO 发表于
November 13th, 2020

我刚开始也是这样想的，我是用pretraining代码，把train\_model.fit给删除了，然后直接bert.save\_weights\_as\_checkpoint(model\_save\_path)，
但是原来用save\_model\_weights保存的模型，在导入时会报错：
Key bert/embeddings/word\_embeddings not found in checkpoint

[苏剑林](https://kexue.fm) 发表于
November 14th, 2020

[@MECH SIHAO\|comment-14783](https://kexue.fm/archives/7877/comment-page-1#comment-14783)

我说的是你用原来的方法建立好模型，然后用model.load\_weights加载你之前保存好的权重，再用save\_weights\_as\_checkpoint转换。

MECH SIHAO 发表于
November 15th, 2020

好的，谢谢苏神

AIML

November 18th, 2020

主要还是有错进错出的对局，不能把赢家的每一个局面都当作正样本，反之亦然

[回复评论](https://kexue.fm/archives/7877/comment-page-1?replyTo=14816#respond-post-7877)

[苏剑林](https://kexue.fm) 发表于
November 18th, 2020

相对而言吧，高手也有漏招的时候呀，但是高手的漏招一般都比普通人要高级了。（捂脸）

[回复评论](https://kexue.fm/archives/7877/comment-page-1?replyTo=14823#respond-post-7877)

明

April 7th, 2021

苏老师，可以考虑试着出一个Curriculum Learning的博客吗？最近看ACL2020有关bert+Curriculum Learning感觉很吃力（论文：Curriculum Learning for Natural Language Understanding）

[回复评论](https://kexue.fm/archives/7877/comment-page-1?replyTo=16029#respond-post-7877)

[苏剑林](https://kexue.fm) 发表于
April 8th, 2021

对这个课题不关心，以后有机会再说吧。

[回复评论](https://kexue.fm/archives/7877/comment-page-1?replyTo=16037#respond-post-7877)

wu

June 7th, 2021

说到底还是要使用增强学习呀

[回复评论](https://kexue.fm/archives/7877/comment-page-1?replyTo=16594#respond-post-7877)

[取消回复](https://kexue.fm/archives/7877#respond-post-7877)

你的大名

电子邮箱

个人网站（选填）

1\. 可以使用LaTeX代码，点击“预览效果”可查看效果；2. 可以通过点击评论楼层编号来引用该楼层；3. 网站可能会有点卡，如非确认评论失败，请不要重复点击提交。

### 内容速览

[背棋谱](https://kexue.fm/archives/7877#%E8%83%8C%E6%A3%8B%E8%B0%B1)
[ICCS记谱](https://kexue.fm/archives/7877#ICCS%E8%AE%B0%E8%B0%B1)
[FEN局面](https://kexue.fm/archives/7877#FEN%E5%B1%80%E9%9D%A2)
[建模型](https://kexue.fm/archives/7877#%E5%BB%BA%E6%A8%A1%E5%9E%8B)
[代码分享](https://kexue.fm/archives/7877#%E4%BB%A3%E7%A0%81%E5%88%86%E4%BA%AB)
[测试一下](https://kexue.fm/archives/7877#%E6%B5%8B%E8%AF%95%E4%B8%80%E4%B8%8B)
[谈改进](https://kexue.fm/archives/7877#%E8%B0%88%E6%94%B9%E8%BF%9B)
[总小结](https://kexue.fm/archives/7877#%E6%80%BB%E5%B0%8F%E7%BB%93)

### 智能搜索

支持整句搜索！网站自动使用 [结巴分词](https://github.com/fxsjy/jieba) 进行分词，并结合ngrams排序算法给出合理的搜索结果。

### 热门标签

[生成模型](https://kexue.fm/tag/%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B/) [attention](https://kexue.fm/tag/attention/) [优化](https://kexue.fm/tag/%E4%BC%98%E5%8C%96/) [语言模型](https://kexue.fm/tag/%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/) [模型](https://kexue.fm/tag/%E6%A8%A1%E5%9E%8B/) [网站](https://kexue.fm/tag/%E7%BD%91%E7%AB%99/) [概率](https://kexue.fm/tag/%E6%A6%82%E7%8E%87/) [梯度](https://kexue.fm/tag/%E6%A2%AF%E5%BA%A6/) [转载](https://kexue.fm/tag/%E8%BD%AC%E8%BD%BD/) [微分方程](https://kexue.fm/tag/%E5%BE%AE%E5%88%86%E6%96%B9%E7%A8%8B/) [天象](https://kexue.fm/tag/%E5%A4%A9%E8%B1%A1/) [矩阵](https://kexue.fm/tag/%E7%9F%A9%E9%98%B5/) [分析](https://kexue.fm/tag/%E5%88%86%E6%9E%90/) [深度学习](https://kexue.fm/tag/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/) [积分](https://kexue.fm/tag/%E7%A7%AF%E5%88%86/) [python](https://kexue.fm/tag/python/) [力学](https://kexue.fm/tag/%E5%8A%9B%E5%AD%A6/) [无监督](https://kexue.fm/tag/%E6%97%A0%E7%9B%91%E7%9D%A3/) [优化器](https://kexue.fm/tag/%E4%BC%98%E5%8C%96%E5%99%A8/) [扩散](https://kexue.fm/tag/%E6%89%A9%E6%95%A3/) [几何](https://kexue.fm/tag/%E5%87%A0%E4%BD%95/) [节日](https://kexue.fm/tag/%E8%8A%82%E6%97%A5/) [生活](https://kexue.fm/tag/%E7%94%9F%E6%B4%BB/) [文本生成](https://kexue.fm/tag/%E6%96%87%E6%9C%AC%E7%94%9F%E6%88%90/) [数论](https://kexue.fm/tag/%E6%95%B0%E8%AE%BA/)

### 随机文章

- [一维弹簧的运动（上）](https://kexue.fm/archives/2430)
- [函数图像旋转公式（“想当然”的教训）](https://kexue.fm/archives/416)
- [看完了刘亦菲版《倩女幽魂》](https://kexue.fm/archives/1333)
- [爱恩斯坦的狭义相对论论文(中文/图片)](https://kexue.fm/archives/259)
- [MoE环游记：4、难处应当多投入](https://kexue.fm/archives/10815)
- [借助变分法变换坐标](https://kexue.fm/archives/3181)
- [湖泊沉积物引来争议：是否彗星造成冰期灾难？](https://kexue.fm/archives/61)
- [一辈子的朋友，一生的情感](https://kexue.fm/archives/115)
- [I care ! 我在乎，我行动！\| 绿色和平](https://kexue.fm/archives/84)
- [为什么勒贝格积分比黎曼积分强？](https://kexue.fm/archives/4083)

### 最近评论

- [OceanYU](https://kexue.fm/archives/9164/comment-page-4#comment-27801): 您好，关于由式（7）推导出高斯分布，我这里有一点问题，式（7）只能保证关于x\_t-1是二次函数...
- [jorjiang](https://kexue.fm/archives/10907/comment-page-2#comment-27800): 训练和prefill这个compute-bound阶段不做矩阵吸收，这个用我这个解释更好理解了...
- [amy](https://kexue.fm/archives/10907/comment-page-2#comment-27799): 苏老师，您有关注傅里叶旋转位置编码这篇工作吗，想知道您对这篇工作的看法是什么，这篇工作可以wo...
- [jiurizz](https://kexue.fm/archives/10945/comment-page-1#comment-27798): 在2\*shared experts + 160\*routed expert + top6的配置...
- [开水](https://kexue.fm/archives/10945/comment-page-1#comment-27797): 感谢苏老师回复，论文appendix里面写了实验是load balance loss，正交lo...
- [苏剑林](https://kexue.fm/archives/10958/comment-page-1#comment-27796): 欢迎作者，这篇文章确实在收藏夹了，结果还没来得及看，抱歉哈，马上学习。
- [苏剑林](https://kexue.fm/archives/10907/comment-page-2#comment-27795): “将存储kv cache改为存储降维后的Embedding X”这句话没错，但是按照我的理解，...
- [苏剑林](https://kexue.fm/archives/10945/comment-page-1#comment-27794): 呃，是这样的，Moonlight-16B-A3B实际上对应的是 scaling\_factor(...
- [Jiaming Song](https://kexue.fm/archives/10958/comment-page-1#comment-27793): 推销一下我们前一段时间的工作，这个其实已经达到了你说的三个标准，并且不需要stop\_grad或...
- [ZhouTimeMachine](https://kexue.fm/archives/9497/comment-page-2#comment-27792): 感谢您的回复！这么一看，我才注意到能从 $p(x\_0)$ 变换到 $p(x\_1)$ 实际上是每...

### 友情链接

- [Cool Papers](https://papers.cool)
- [数学研发](https://bbs.emath.ac.cn)
- [Seatop](http://www.seatop.com.cn/)
- [Xiaoxia](https://xiaoxia.org/)
- [积分表-网络版](https://kexue.fm/sci/integral/index.html)
- [丝路博傲](http://blog.dvxj.com/)
- [ph4ntasy 饭特稀](http://www.ph4ntasy.com/)
- [数学之家](http://www.2math.cn/)
- [有趣天文奇观](http://interesting-sky.china-vo.org/)
- [TwistedW](http://www.twistedwg.com/)
- [godweiyang](https://godweiyang.com/)
- [AI柠檬](https://blog.ailemon.net/)
- [王登科-DK博客](https://greatdk.com)
- [ESON](https://blog.eson.org/)
- [枫之羽](https://fzhiy.net/)
- [Mathor's blog](https://wmathor.com/)
- [coding-zuo](https://coding-zuo.github.io/)
- [博科园](https://www.bokeyuan.net/)
- [孔皮皮的博客](https://www.kppkkp.top/)
- [运鹏的博客](https://yunpengtai.top/)
- [jiming.site](https://jiming.site/)
- [OmegaXYZ](https://www.omegaxyz.com/)
- [Blog by Eacls](https://www.eacls.top/)
- [EAI猩球](https://www.robotech.ink/)
- [文举的博客](https://liwenju0.com/)
- [用代码打点酱油](https://bruceyuan.com/)
- [申请链接](https://kexue.fm/links.html)

本站采用创作共用版权协议，要求署名、非商业用途和保持一致。转载本站内容必须也遵循“ [署名-非商业用途-保持一致](http://creativecommons.org/licenses/by-nc-nd/2.5/cn/)”的创作共用协议。
© 2009-2025 Scientific Spaces. All rights reserved. Theme by [laogui](http://www.laogui.com). Powered by [Typecho](http://typecho.org). 备案号: [粤ICP备09093259号-1/2](https://beian.miit.gov.cn/)。