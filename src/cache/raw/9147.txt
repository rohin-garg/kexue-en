“维度灾难”之Hubness现象浅析 - 科学空间|Scientific Spaces
![MobileSideBar](https://kexue.fm/usr/themes/geekg/images/slide-button.png "MobileSideBar")
## SEARCH
## MENU
* [打赏](https://kexue.fm/reward.html)
* [公式](https://kexue.fm/latex.html)
* [天象](https://kexue.fm/ac.html)
* [链接](https://kexue.fm/links.html)
* [时光](https://kexue.fm/me.html)
* [博览](https://kexue.fm/science.html)
* [归档](https://kexue.fm/content.html)
## CATEGORIES
* [千奇百怪](https://kexue.fm/category/Everything)
* [天文探索](https://kexue.fm/category/Astronomy)
* [数学研究](https://kexue.fm/category/Mathematics)
* [物理化学](https://kexue.fm/category/Phy-chem)
* [信息时代](https://kexue.fm/category/Big-Data)
* [生物自然](https://kexue.fm/category/Biology)
* [图片摄影](https://kexue.fm/category/Photograph)
* [问题百科](https://kexue.fm/category/Questions)
* [生活/情感](https://kexue.fm/category/Life-Feeling)
* [资源共享](https://kexue.fm/category/Resources)
## NEWPOSTS
* [让炼丹更科学一些（三）：SGD的终...](https://kexue.fm/archives/11480)
* [让炼丹更科学一些（二）：将结论推广...](https://kexue.fm/archives/11469)
* [滑动平均视角下的权重衰减和学习率](https://kexue.fm/archives/11459)
* [生成扩散模型漫谈（三十一）：预测数...](https://kexue.fm/archives/11428)
* [Muon优化器指南：快速上手与关键细节](https://kexue.fm/archives/11416)
* [AdamW的Weight RMS的...](https://kexue.fm/archives/11404)
* [n个正态随机数的最大值的渐近估计](https://kexue.fm/archives/11390)
* [流形上的最速下降：5. 对偶梯度下降](https://kexue.fm/archives/11388)
* [低精度Attention可能存在有...](https://kexue.fm/archives/11371)
* [MuP之上：1. 好模型的三个特征](https://kexue.fm/archives/11340)
## COMMENTS
* [CQU-ljx: 苏老师，我想借用归一化流模型做一个两个图像之间的可逆映射，但是...](https://kexue.fm/archives/10667/comment-page-1#comment-29014)
* [CQU-ljx: 苏老师，我想借用归一化流模型做一个两个图像之间的可逆映射，但是...](https://kexue.fm/archives/10667/comment-page-1#comment-29013)
* [透明: 我15年前后的时候参加过，当时决赛还因为什么原因从山西改到了北...](https://kexue.fm/archives/7611/comment-page-1#comment-29012)
* [whc: https://arxiv.org/abs/2502.0256...](https://kexue.fm/archives/9403/comment-page-1#comment-29011)
* [sog: 式3那里，对应上一篇文章的式19，我怎么感觉$\\varphi$...](https://kexue.fm/archives/11469/comment-page-1#comment-29010)
* [岁月如书: qwen团队的gated attention看上去对这个max...](https://kexue.fm/archives/11126/comment-page-3#comment-29009)
* [Yifan GUO: 我打脸了，写了代码快速验证了一下，softmax对应的effi...](https://kexue.fm/archives/7546/comment-page-4#comment-29008)
* [Yifan GUO: 《Efficient Attention: Attention...](https://kexue.fm/archives/7546/comment-page-4#comment-29007)
* [岁月如书: muon怎么就丢了奇异值，奇异值用来做weight decay...](https://kexue.fm/archives/11459/comment-page-1#comment-29006)
* [Yifan GUO: Oh，我貌似理解了，或许我可以这样给自己解释：
如果分母的作用...](https://kexue.fm/archives/11033/comment-page-3#comment-29005)
## USERLOGIN
* [登录](https://kexue.fm/admin/login.php)
[科学空间|Scientific Spaces](https://kexue.fm)
* [登录](https://kexue.fm/admin/login.php)
* [打赏](https://kexue.fm/reward.html)
* [公式](https://kexue.fm/latex.html)
* [天象](https://kexue.fm/ac.html)
* [链接](https://kexue.fm/links.html)
* [时光](https://kexue.fm/me.html)
* [博览](https://kexue.fm/science.html)
* [归档](https://kexue.fm/content.html)
渴望成为一个小飞侠* [![](https://kexue.fm/usr/themes/geekg/images/rss.png)
欢迎订阅](https://kexue.fm/feed)
* [![](https://kexue.fm/usr/themes/geekg/images/mail.png)
个性邮箱](https://kexue.fm/archives/119)
* [![](https://kexue.fm/usr/themes/geekg/images/Saturn.png)
天象信息](https://kexue.fm/ac.html)
* [![](https://kexue.fm/usr/themes/geekg/images/iss.png)
观测ISS](https://kexue.fm/archives/41)
* [![](https://kexue.fm/usr/themes/geekg/images/pi.png)
LaTeX](https://kexue.fm/latex.html)
* [![](https://kexue.fm/usr/themes/geekg/images/mlogo.png)
关于博主](https://kexue.fm/me.html)
欢迎访问“科学空间”，这里将与您共同探讨自然科学，回味人生百态；也期待大家的分享～* [**千奇百怪**Everything](https://kexue.fm/category/Everything)
* [**天文探索**Astronomy](https://kexue.fm/category/Astronomy)
* [**数学研究**Mathematics](https://kexue.fm/category/Mathematics)
* [**物理化学**Phy-chem](https://kexue.fm/category/Phy-chem)
* [**信息时代**Big-Data](https://kexue.fm/category/Big-Data)
* [**生物自然**Biology](https://kexue.fm/category/Biology)
* [**图片摄影**Photograph](https://kexue.fm/category/Photograph)
* [**问题百科**Questions](https://kexue.fm/category/Questions)
* [**生活/情感**Life-Feeling](https://kexue.fm/category/Life-Feeling)
* [**资源共享**Resources](https://kexue.fm/category/Resources)
* [**千奇百怪**](https://kexue.fm/category/Everything)
* [**天文探索**](https://kexue.fm/category/Astronomy)
* [**数学研究**](https://kexue.fm/category/Mathematics)
* [**物理化学**](https://kexue.fm/category/Phy-chem)
* [**信息时代**](https://kexue.fm/category/Big-Data)
* [**生物自然**](https://kexue.fm/category/Biology)
* [**图片摄影**](https://kexue.fm/category/Photograph)
* [**问题百科**](https://kexue.fm/category/Questions)
* [**生活/情感**](https://kexue.fm/category/Life-Feeling)
* [**资源共享**](https://kexue.fm/category/Resources)
[首页](https://kexue.fm)[信息时代](https://kexue.fm/category/Big-Data)“维度灾难”之Hubness现象浅析
28Jun
# [“维度灾难”之Hubness现象浅析](https://kexue.fm/archives/9147)
By苏剑林|2022-06-28|59352位读者|:
这几天读到论文[《Exploring and Exploiting Hubness Priors for High-Quality GAN Latent Sampling》](https://papers.cool/arxiv/2206.06014)，了解到了一个新的名词“Hubness现象”，说的是高维空间中的一种聚集效应，本质上是“维度灾难”的体现之一。论文借助Hubness的概念得到了一个提升GAN模型生成质量的方案，看起来还蛮有意思。所以笔者就顺便去学习了一下Hubness现象的相关内容，记录在此，供大家参考。
## 坍缩的球[#](#坍缩的球)
“维度灾难”是一个很宽泛的概念，所有在高维空间中与相应的二维、三维空间版本出入很大的结论，都可以称之为“维度灾难”，比如[《n维空间下两个随机向量的夹角分布》](https://kexue.fm/archives/7076)中介绍的“高维空间中任何两个向量几乎都是垂直的”。其中，有不少维度灾难现象有着同一个源头——“高维空间单位球与其外切正方体的体积之比逐渐坍缩至0”，包括本文的主题“Hubness现象”亦是如此。
在[《鬼斧神工：求n维球的体积》](https://kexue.fm/archives/3154)中，我们推导过$n$维球的体积公式，从中可知$n$维单位球的体积为
\\begin{equation}V\_n = \\frac{\\pi^{n/2}}{\\Gamma\\left(\\frac{n}{2}+1\\right)}\\end{equation}
对应的外切正方体边长为$2$，体积自然为$2^n$，所以对应的体积比为$V\_n / 2^n$，其图像如下图：
[![n 维球与外切正方体的体积之比](https://kexue.fm/usr/uploads/2022/06/1517429447.png)](https://kexue.fm/usr/uploads/2022/06/1517429447.png)
n 维球与外切正方体的体积之比可以看到，随着维度的增大，这个比例很快就趋于0了。这个结论的一个形象说法是“随着维度增加，球变得越来越微不足道”，它告诉我们，如果通过“均匀分布 + 拒绝采样”的方式去实现球内的均匀采样，那么在高维空间中效率将会非常低（拒绝率接近100%）。还有一种理解方式是“高维球内的点大部分都集中在球表面附近”，球中心到球表面附近的区域占比越来越小。
## Hubness现象[#](#Hubness现象)
现在我们转到Hubness现象，它说的是在高维空间中随机选一批点，那么“总有一些点经常出现在其他点的$k$邻近中”。
具体怎么理解这句话呢？假设我们有$N$个点$x\_1,x\_2,\\cdots,x\_N$，对于每个$x\_i$，我们都可以找出与之最相近的$k$个点，这$k$个点都称为“$x\_i$的$k$邻近”。有了$k$邻近的概念后，我们可以统计每个点出现在其他点的$k$邻近的次数，这个次数称为“Hub值”，也就是说Hub值越大，它就越容易出现在其他点的$k$邻近中。
所以，Hubness现象说的是：总有那么几个点，它的Hub值显然特别大。如果Hub值代表着“财富”，那么一个形象的比喻就是“80%的财富集中在20%的人手中”，并且随着维度增大，这个“贫富差距”就越来越大；如果Hub值代表着“人脉”，那么也可以形象地比喻为“社群中总有那么几个人拥有非常广泛的人脉资源”。
Hubness现象是怎么出现的呢？其实也跟前一节说的$n$维球的坍缩有关。我们知道，与所有点距离平方和最小的点，正好是均值点：
\\begin{equation}\\frac{1}{N} \\sum\_{i=1}^N x\_i = c^\* = \\mathop{\\text{argmin}}\_c \\sum\_{i=1}^N \\Vert x\_i - c\\Vert^2\\end{equation}
这也就意味着，在均值向量附近的点，与所有点的平均距离较小，有更大的机会成为更多点的$k$邻近。而$n$维球的坍缩现象则告诉我们，“均值向量附近的点”，即以均值向量为球心的一个球邻域，其占比是非常小的。于是就出现了“非常少的点出现在很多点的$k$邻近中”这一现象了。当然，这里的均值向量是比较直观的理解，在一般的数据点中，应该是越靠近密度中心的点，其Hub值会变得越大。
## 提升采样[#](#提升采样)
那么本文开头说的提升GAN模型生成质量的方案，跟Hubness现象又有什么关系呢？论文[《Exploring and Exploiting Hubness Priors for High-Quality GAN Latent Sampling》](https://papers.cool/arxiv/2206.06014)提出了一个先验假设：Hub值越大，对应点的生成质量就越好。
具体来说，一般GAN的采样生成流程是$z\\sim \\mathcal{N}(0,1), x=G(z)$，我们可以从$\\mathcal{N}(0,1)$中先采样$N$个样本点$z\_1,z\_2,\\cdots,z\_N$，然后就可以算出每个样本点的Hub值，原论文发现Hub值跟生成质量是正相关的，所以只保留Hub值大于等于阈值$t$的样本点用来做生成。这是一种“事前”的筛选思路，参考代码如下：
```
`def get\_z\_samples(size, t=50): """通过Hub值对采样结果进行筛选 """ Z = np.empty((0, z\_dim)) while len(Z)\ t] Z = np.concatenate([Z, z], 0)[:size] print('%s / %s' % (len(Z), size)) return Z`
```
为什么通过Hub值来筛选呢？由前面的讨论可以知道，Hub值越大，那么就越接近样本中心，其实更准确率来说是接近密度中心，意味着周围有很多临近点，那么它就不大可能是没有被充分训练的离群点，因此采样质量相对高一些。论文的多个实验结果肯定了这一结论。
[![基于Hub值进行筛选的生成质量对比](https://kexue.fm/usr/uploads/2022/06/430095810.jpg)](https://kexue.fm/usr/uploads/2022/06/430095810.jpg)
基于Hub值进行筛选的生成质量对比
## 文章小结[#](#文章小结)
本文主要简介了“维度灾难”中的Hubness现象，并介绍了它在提升GAN生成质量方面的应用。
***转载到请包括本文地址：** [https://kexue.fm/archives/9147](https://kexue.fm/archives/9147)*
***更详细的转载事宜请参考：*** [《科学空间FAQ》](https://kexue.fm/archives/6508#文章如何转载/引用)
**如果您还有什么疑惑或建议，欢迎在下方评论区继续讨论。**
**如果您觉得本文还不错，欢迎[分享](#share)/[打赏](#pay)本文。打赏并非要从中获得收益，而是希望知道科学空间获得了多少读者的真心关注。当然，如果你无视它，也不会影响你的阅读。再次表示欢迎和感谢！**
打赏![科学空间](https://kexue.fm/usr/themes/geekg/payment/wx.png)
微信打赏![科学空间](https://kexue.fm/usr/themes/geekg/payment/zfb.png)
支付宝打赏因为网站后台对打赏并无记录，因此欢迎在打赏时候备注留言。 你还可以[**点击这里**](http://mail.qq.com/cgi-bin/qm_share?t=qm_mailme&email=tN7d1drY3drrx8H0xcWa19vZ)或在下方评论区留言来告知你的建议或需求。
**如果您需要引用本文，请参考：**
苏剑林. (Jun. 28, 2022). 《“维度灾难”之Hubness现象浅析 》[Blog post]. Retrieved from[https://kexue.fm/archives/9147](https://kexue.fm/archives/9147)
@online{kexuefm-9147,
title={“维度灾难”之Hubness现象浅析},
author={苏剑林},
year={2022},
month={Jun},
url={\\url{https://kexue.fm/archives/9147}},
}
分类：[信息时代](https://kexue.fm/category/Big-Data) 标签：[维度](https://kexue.fm/tag/维度/),[GAN](https://kexue.fm/tag/GAN/),[生成模型](https://kexue.fm/tag/生成模型/)[12 评论](https://kexue.fm/archives/9147#comments)
&lt;[Ladder Side-Tuning：预训练模型的“过墙梯”](https://kexue.fm/archives/9138)|[生成扩散模型漫谈（二）：DDPM = 自回归式VAE](https://kexue.fm/archives/9152)&gt;
### 你也许还对下面的内容感兴趣* [生成扩散模型漫谈（三十一）：预测数据而非噪声](https://kexue.fm/archives/11428)
* [DiVeQ：一种非常简洁的VQ训练方案](https://kexue.fm/archives/11328)
* [为什么线性注意力要加Short Conv？](https://kexue.fm/archives/11320)
* [Transformer升级之路：21、MLA好在哪里?（下）](https://kexue.fm/archives/11111)
* [线性注意力简史：从模仿、创新到反哺](https://kexue.fm/archives/11033)
* [生成扩散模型漫谈（三十）：从瞬时速度到平均速度](https://kexue.fm/archives/10958)
* [Transformer升级之路：20、MLA好在哪里?（上）](https://kexue.fm/archives/10907)
* [生成扩散模型漫谈（二十九）：用DDPM来离散编码](https://kexue.fm/archives/10711)
* [细水长flow之TARFLOW：流模型满血归来？](https://kexue.fm/archives/10667)
* [生成扩散模型漫谈（二十八）：分步理解一致性模型](https://kexue.fm/archives/10633)
[发表你的看法](#comment_form)
李子涵June 28th, 2022
苏神，这个拒绝采样的结果相当于改变了采样的先验分布（$\\mathcal{N}(0, 1)$）对吧？那结合“靠近密度中心”这个观点来看的话，是不是就相当于在某种“截断正态”或“聚拢正态”上面采样呢？例如训练的时候根据标准正态采样，生成的时候我就把方差缩小再采样（\\摊手.jpg）
[回复评论](https://kexue.fm/archives/9147/comment-page-1?replyTo=19361#respond-post-9147)
QF
June 29th, 2022
苏神，利用Hubness现象来筛选采样点，和直接选取采样点均值附近的若干点来生成图片，有什么区别吗？
[回复评论](https://kexue.fm/archives/9147/comment-page-1?replyTo=19368#respond-post-9147)
[苏剑林](https://kexue.fm)发表于 June 30th, 2022
很多截断技巧的思想都是相似的，所以你问我“有什么区别”，我只能说它们就是不同的方法和流程，而思想上没有本质区别，哪个更好我也不清楚。我只是介绍Hubness现象，顺带介绍它这个应用。
[回复评论](https://kexue.fm/archives/9147/comment-page-1?replyTo=19381#respond-post-9147)
Xin Li
June 30th, 2022
很可能使用小很多的初始变量维度就能大幅提升性能。参加论文https://arxiv.org/abs/2110.04627
[回复评论](https://kexue.fm/archives/9147/comment-page-1?replyTo=19372#respond-post-9147)
Xin Li 发表于June 30th, 2022
我想到降低维度的想法的时候也是考虑的类似的问题。一试，这个反直觉的方法竟然效果非常之好！[回复评论](https://kexue.fm/archives/9147/comment-page-1?replyTo=19373#respond-post-9147)
Ziqiang Li 发表于June 30th, 2022
在有些setting中降低隐空间维度确实是一个简单有限的提点方法。
[回复评论](https://kexue.fm/archives/9147/comment-page-1?replyTo=19375#respond-post-9147)
[苏剑林](https://kexue.fm)发表于 June 30th, 2022
你是说隐变量的维度降低吗？那确实有可能可行，减少了冗余维度。感谢推荐。[回复评论](https://kexue.fm/archives/9147/comment-page-1?replyTo=19383#respond-post-9147)
Xin Li 发表于July 1st, 2022
还要加上l2 normalization.具体可以见论文
[回复评论](https://kexue.fm/archives/9147/comment-page-1?replyTo=19394#respond-post-9147)
starfruit007
July 27th, 2022
想到了一个直观的理解n维球坍缩的方式，不知是否合理：假设某一点在一个维度上落入球体的概率为0.99，那么，在n维上，这个点落入球体的概率就是0.99^n, 当n越大，这个概率就趋近于0。（应该不够严谨，感觉比较直观）
今天刚接触ZSL，第一次听到Hubness问题，没想到在苏神这边看到了分享，真巧\~
[回复评论](https://kexue.fm/archives/9147/comment-page-1?replyTo=19547#respond-post-9147)
[苏剑林](https://kexue.fm)发表于 July 29th, 2022
简单理解尚可，但事实上降低的速度远远快于指数下降。[回复评论](https://kexue.fm/archives/9147/comment-page-1?replyTo=19555#respond-post-9147)
shmax
November 17th, 2022
\> 任何两个向量机几乎”向量？[回复评论](https://kexue.fm/archives/9147/comment-page-1?replyTo=20388#respond-post-9147)
[苏剑林](https://kexue.fm)发表于 November 22nd, 2022
嗯嗯，谢谢，已修正。[回复评论](https://kexue.fm/archives/9147/comment-page-1?replyTo=20439#respond-post-9147)
[取消回复](https://kexue.fm/archives/9147#respond-post-9147)
你的大名电子邮箱个人网站（选填）1. 可以使用LaTeX代码，点击“预览效果”可查看效果；
2. 可以通过点击评论楼层编号来引用该楼层；3. 网站可能会有点卡，如非确认评论失败，请**不要重复点击提交**。
********************
### 内容速览* [坍缩的球](#坍缩的球)
* [Hubness现象](#Hubness现象)
* [提升采样](#提升采样)
* [文章小结](#文章小结)
********************
### 智能搜索支持整句搜索！网站自动使用[结巴分词](https://github.com/fxsjy/jieba)进行分词，并结合ngrams排序算法给出合理的搜索结果。
********************
### 热门标签[生成模型](https://kexue.fm/tag/生成模型/)[attention](https://kexue.fm/tag/attention/)[优化](https://kexue.fm/tag/优化/)[语言模型](https://kexue.fm/tag/语言模型/)[模型](https://kexue.fm/tag/模型/)[网站](https://kexue.fm/tag/网站/)[梯度](https://kexue.fm/tag/梯度/)[概率](https://kexue.fm/tag/概率/)[矩阵](https://kexue.fm/tag/矩阵/)[优化器](https://kexue.fm/tag/优化器/)[转载](https://kexue.fm/tag/转载/)[微分方程](https://kexue.fm/tag/微分方程/)[分析](https://kexue.fm/tag/分析/)[天象](https://kexue.fm/tag/天象/)[深度学习](https://kexue.fm/tag/深度学习/)[积分](https://kexue.fm/tag/积分/)[python](https://kexue.fm/tag/python/)[扩散](https://kexue.fm/tag/扩散/)[力学](https://kexue.fm/tag/力学/)[无监督](https://kexue.fm/tag/无监督/)[几何](https://kexue.fm/tag/几何/)[节日](https://kexue.fm/tag/节日/)[生活](https://kexue.fm/tag/生活/)[文本生成](https://kexue.fm/tag/文本生成/)[数论](https://kexue.fm/tag/数论/)
********************
********************
### 随机文章* [变分自编码器（四）：一步到位的聚类方案](https://kexue.fm/archives/5887)
* [FLASH：可能是近来最有意思的高效Transformer设计](https://kexue.fm/archives/8934)
* [以自然数幂为系数的幂级数](https://kexue.fm/archives/986)
* [《新理解矩阵2》：矩阵是什么？](https://kexue.fm/archives/1768)
* [【中文分词系列】 1. 基于AC自动机的快速分词](https://kexue.fm/archives/3908)
* [msign的导数](https://kexue.fm/archives/11025)
* [一道自然数的数学题](https://kexue.fm/archives/35)
* [中秋再见！](https://kexue.fm/archives/930)
* [从重参数的角度看离散概率分布的构建](https://kexue.fm/archives/9085)
* [【翻译】巨型望远镜：要继续，就得有牺牲！](https://kexue.fm/archives/3354)
********************
********************
### 最近评论* [CQU-ljx](https://kexue.fm/archives/10667/comment-page-1#comment-29014): 苏老师，我想借用归一化流模型做一个两个图像之间的可逆映射，但是这两个图像尺寸大小不一样，您可以...
* [CQU-ljx](https://kexue.fm/archives/10667/comment-page-1#comment-29013): 苏老师，我想借用归一化流模型做一个两个图像之间的可逆映射，但是这两个图像尺寸大小不一样，您可以...
* [透明](https://kexue.fm/archives/7611/comment-page-1#comment-29012): 我15年前后的时候参加过，当时决赛还因为什么原因从山西改到了北京来着。那时候国家金奖的课题还是...
* [whc](https://kexue.fm/archives/9403/comment-page-1#comment-29011): https://arxiv.org/abs/2502.02562 里提出两种低成本parame...
* [sog](https://kexue.fm/archives/11469/comment-page-1#comment-29010): 式3那里，对应上一篇文章的式19，我怎么感觉$\\varphi$取的是$\\theta\_t-\\et...
* [岁月如书](https://kexue.fm/archives/11126/comment-page-3#comment-29009): qwen团队的gated attention看上去对这个maxlogit会比较有用。毕竟他们都...
* [Yifan GUO](https://kexue.fm/archives/7546/comment-page-4#comment-29008): 我打脸了，写了代码快速验证了一下，softmax对应的efficient attn这样妙用so...
* [Yifan GUO](https://kexue.fm/archives/7546/comment-page-4#comment-29007): 《Efficient Attention: Attention with Linear Com...
* [岁月如书](https://kexue.fm/archives/11459/comment-page-1#comment-29006): muon怎么就丢了奇异值，奇异值用来做weight decay没有可行性么
* [Yifan GUO](https://kexue.fm/archives/11033/comment-page-3#comment-29005): Oh，我貌似理解了，或许我可以这样给自己解释：
如果分母的作用确实只是保持数值稳定性的话，那这...
********************
********************
### 友情链接* [Cool Papers](https://papers.cool)
* [数学研发](https://bbs.emath.ac.cn)
* [Seatop](http://www.seatop.com.cn/)
* [Xiaoxia](https://xiaoxia.org/)
* [积分表-网络版](https://kexue.fm/sci/integral/index.html)
* [丝路博傲](http://blog.dvxj.com/)
* [数学之家](http://www.2math.cn/)
* [有趣天文奇观](http://interesting-sky.china-vo.org/)
* [TwistedW](http://www.twistedwg.com/)
* [godweiyang](https://godweiyang.com/)
* [AI柠檬](https://blog.ailemon.net/)
* [王登科-DK博客](https://greatdk.com)
* [ESON](https://blog.eson.org/)
* [枫之羽](https://fzhiy.net/)
* [coding-zuo](https://coding-zuo.github.io/)
* [博科园](https://www.bokeyuan.net/)
* [孔皮皮的博客](https://www.kppkkp.top/)
* [运鹏的博客](https://yunpengtai.top/)
* [jiming.site](https://jiming.site/)
* [OmegaXYZ](https://www.omegaxyz.com/)
* [EAI猩球](https://www.robotech.ink/)
* [文举的博客](https://liwenju0.com/)
* [申请链接](https://kexue.fm/links.html)
********************
[![署名-非商业用途-保持一致](https://kexue.fm/usr/themes/geekg/images/cc.gif)](http://creativecommons.org/licenses/by-nc-nd/2.5/cn/)本站采用创作共用版权协议，要求署名、非商业用途和保持一致。转载本站内容必须也遵循“[署名-非商业用途-保持一致](http://creativecommons.org/licenses/by-nc-nd/2.5/cn/)”的创作共用协议。
©2009-2025 Scientific Spaces. All rights reserved. Theme by[laogui](http://www.laogui.com). Powered by[Typecho](http://typecho.org). 备案号:[粤ICP备09093259号-1/2](https://beian.miit.gov.cn/)。