## SEARCH

## MENU

- [打赏](https://kexue.fm/reward.html)
- [公式](https://kexue.fm/latex.html)
- [天象](https://kexue.fm/ac.html)
- [链接](https://kexue.fm/links.html)
- [时光](https://kexue.fm/me.html)
- [博览](https://kexue.fm/science.html)
- [归档](https://kexue.fm/content.html)

## CATEGORIES

- [千奇百怪](https://kexue.fm/category/Everything)
- [天文探索](https://kexue.fm/category/Astronomy)
- [数学研究](https://kexue.fm/category/Mathematics)
- [物理化学](https://kexue.fm/category/Phy-chem)
- [信息时代](https://kexue.fm/category/Big-Data)
- [生物自然](https://kexue.fm/category/Biology)
- [图片摄影](https://kexue.fm/category/Photograph)
- [问题百科](https://kexue.fm/category/Questions)
- [生活/情感](https://kexue.fm/category/Life-Feeling)
- [资源共享](https://kexue.fm/category/Resources)

## NEWPOSTS

- [线性注意力简史：从模仿、创新到反哺](https://kexue.fm/archives/11033)
- [msign的导数](https://kexue.fm/archives/11025)
- [通过msign来计算mclip（奇...](https://kexue.fm/archives/11006)
- [msign算子的Newton-Sc...](https://kexue.fm/archives/10996)
- [等值振荡定理：最优多项式逼近的充要条件](https://kexue.fm/archives/10972)
- [生成扩散模型漫谈（三十）：从瞬时速...](https://kexue.fm/archives/10958)
- [MoE环游记：5、均匀分布的反思](https://kexue.fm/archives/10945)
- [msign算子的Newton-Sc...](https://kexue.fm/archives/10922)
- [Transformer升级之路：2...](https://kexue.fm/archives/10907)
- [一道概率不等式：盯着它到显然成立为止！](https://kexue.fm/archives/10902)

## COMMENTS

- [musicfish1973: 好的设计都是相似的,haha](https://kexue.fm/archives/8009/comment-page-1#comment-27953)
- [忍者猫: 这优化器的作者真的应该给你打钱](https://kexue.fm/archives/10592/comment-page-2#comment-27952)
- [Chaofa Yuan: 写得太好了](https://kexue.fm/archives/11033/comment-page-1#comment-27951)
- [Skyler Lin: respect苏神！](https://kexue.fm/archives/11033/comment-page-1#comment-27949)
- [宋佳铭: 对，个人感觉mean flow就是continuous tim...](https://kexue.fm/archives/10958/comment-page-1#comment-27947)
- [宋佳铭: 的确，对sg这个事情我感觉如果是用‘归纳’法做是不太能避免的，...](https://kexue.fm/archives/10958/comment-page-1#comment-27946)
- [MoFHeka: 苏老师您好，请问一下这套结论在稀疏参数上应该如何应用？比如大规...](https://kexue.fm/archives/10542/comment-page-1#comment-27945)
- [苏剑林: Temp LoRA倒是有印象，其实思想是一样的，如果我单独开一...](https://kexue.fm/archives/11033/comment-page-1#comment-27944)
- [苏剑林: 你搜搜mamba、rwkv甚至rnn做vision的工作，其实...](https://kexue.fm/archives/11033/comment-page-1#comment-27943)
- [苏剑林: 问题1可以看看 https://kexue.fm/archiv...](https://kexue.fm/archives/9379/comment-page-1#comment-27942)

## USERLOGIN

- [登录](https://kexue.fm/admin/login.php)

[科学空间\|Scientific Spaces](https://kexue.fm)

- [登录](https://kexue.fm/admin/login.php)
- [打赏](https://kexue.fm/reward.html)
- [公式](https://kexue.fm/latex.html)
- [天象](https://kexue.fm/ac.html)
- [链接](https://kexue.fm/links.html)
- [时光](https://kexue.fm/me.html)
- [博览](https://kexue.fm/science.html)
- [归档](https://kexue.fm/content.html)

渴望成为一个小飞侠

- [欢迎订阅](https://kexue.fm/feed)
- [个性邮箱](https://kexue.fm/archives/119)
- [天象信息](https://kexue.fm/ac.html)
- [观测ISS](https://kexue.fm/archives/41)
- [LaTeX](https://kexue.fm/latex.html)
- [关于博主](https://kexue.fm/me.html)

欢迎访问“科学空间”，这里将与您共同探讨自然科学，回味人生百态；也期待大家的分享～

- [**千奇百怪** Everything](https://kexue.fm/category/Everything)
- [**天文探索** Astronomy](https://kexue.fm/category/Astronomy)
- [**数学研究** Mathematics](https://kexue.fm/category/Mathematics)
- [**物理化学** Phy-chem](https://kexue.fm/category/Phy-chem)
- [**信息时代** Big-Data](https://kexue.fm/category/Big-Data)
- [**生物自然** Biology](https://kexue.fm/category/Biology)
- [**图片摄影** Photograph](https://kexue.fm/category/Photograph)
- [**问题百科** Questions](https://kexue.fm/category/Questions)
- [**生活/情感** Life-Feeling](https://kexue.fm/category/Life-Feeling)
- [**资源共享** Resources](https://kexue.fm/category/Resources)

- [**千奇百怪**](https://kexue.fm/category/Everything)
- [**天文探索**](https://kexue.fm/category/Astronomy)
- [**数学研究**](https://kexue.fm/category/Mathematics)
- [**物理化学**](https://kexue.fm/category/Phy-chem)
- [**信息时代**](https://kexue.fm/category/Big-Data)
- [**生物自然**](https://kexue.fm/category/Biology)
- [**图片摄影**](https://kexue.fm/category/Photograph)
- [**问题百科**](https://kexue.fm/category/Questions)
- [**生活/情感**](https://kexue.fm/category/Life-Feeling)
- [**资源共享**](https://kexue.fm/category/Resources)

[首页](https://kexue.fm) [信息时代](https://kexue.fm/category/Big-Data) 那个屠榜的T5模型，现在可以在中文上玩玩了

6Nov

# [那个屠榜的T5模型，现在可以在中文上玩玩了](https://kexue.fm/archives/7867)

By 苏剑林 \|
2020-11-06 \|
161141位读者\|

不知道大家对Google去年的屠榜之作T5还有没有印象？就是那个打着“万事皆可Seq2Seq”的旗号、最大搞了110亿参数、一举刷新了GLUE、SuperGLUE等多个NLP榜单的模型，而且过去一年了，T5仍然是 [SuperGLUE](https://super.gluebenchmark.com/) 榜单上的第一，目前还稳妥地拉开着第二名2%的差距。然而，对于中文界的朋友来说，T5可能没有什么存在感，原因很简单：没有中文版T5可用。不过这个现状要改变了，因为Google最近放出了多国语言版的T5（mT5），里边当然是包含了中文语言。虽然不是纯正的中文版，但也能凑合着用一下。

“万事皆可Seq2Seq”的T5

本文将会对T5模型做一个简单的回顾与介绍，然后再介绍一下如何在bert4keras中调用mT5模型来做中文任务。作为一个原生的Seq2Seq预训练模型，mT5在文本生成任务上的表现还是相当不错的，非常值得一试。

## T5 [\#](https://kexue.fm/archives/7867\#T5)

跟BERT一样，T5也是Google出品的预训练模型，来自论文为 [《Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer》](https://papers.cool/arxiv/1910.10683)，Github为 [text-to-text-transfer-transformer](https://github.com/google-research/text-to-text-transfer-transformer)。T5的理念就是“万事皆可Seq2Seq”，它使用了标准的Encoder-Decoder模型，并且构建了无监督/有监督的文本生成预训练任务，最终将效果推向了一个新高度。

### 训练 [\#](https://kexue.fm/archives/7867\#%E8%AE%AD%E7%BB%83)

T5的预训练包含无监督和有监督两部分。无监督部分使用的是Google构建的近800G的语料（论文称之为C4），而训练目标则跟BERT类似，只不过改成了Seq2Seq版本，我们可以将它看成一个高级版的完形填空问题：

> **输入：** 明月几时有，\[M0\]问青天，不知\[M1\]，今夕是何年？我欲\[M2\]归去，又恐琼楼玉宇，高处\[M3\]；起舞\[M4\]清影，何似在人间。 **输出：**\[M0\]把酒\[M1\]天上宫阙\[M2\]乘风\[M3\]不胜寒\[M4\]弄

而有监督部分，则是收集了常见的NLP监督任务数据，并也统一转化为SeqSeq任务来训练。比如情感分类可以这样转化：

> **输入：** 识别该句子的情感倾向：这趟北京之旅我感觉很不错。 **输出：** 正面

主题分类可以这样转化：

> **输入：** 下面是一则什么新闻？八个月了，终于又能在赛场上看到女排姑娘们了。 **输出：** 体育

阅读理解可以这样转化：

> **输入：** 阅读理解：特朗普与拜登共同竞选下一任美国总统。根据上述信息回答问题：特朗普是哪国人？ **输出：** 美国

可以看到，这种转化跟GPT2、GPT3、PET的思想都是一致的，都是希望用文字把我们要做的任务表达出来，然后都转化为文字的预测，读者还可以翻看旧作 [《必须要GPT3吗？不，BERT的MLM模型也能小样本学习》](https://kexue.fm/archives/7764) 了解相关内容。总的来说，在我们的内部实验里边，模型够大、数据够多以及有监督预训练都是T5成功的关键因素，“万事皆可Seq2Seq”则提供了有效地融合这些关键因素的方案。

### 结果 [\#](https://kexue.fm/archives/7867\#%E7%BB%93%E6%9E%9C)

T5的主要战绩汇总如下表：

T5的战绩汇总

除了屠了多个榜单之外，T5还对整个训练流程中很多可调的超参数都调试了一遍，比如模型架构究竟用标准的Encoder-Decoder好还是UniLM那种结构好，无监督预训练任务究竟是BERT的方式好还是其他方式好，随机Mask的比例是不是15%最好，等等，最后给出了如下的表格，并还很遗憾地表达了一句“其实我们觉得T5的实验做得还不是很充分”，颇有一种“走别人的路，让别人无路可走”的感觉。当然，不管怎样，这些炼丹结果还是值得每一位要做语言模型的同学好好看看，或许能让我们少走一些弯路。

T5那巨细无遗的“炼丹宝典”（点击可以看大图）

## mT5 [\#](https://kexue.fm/archives/7867\#mT5)

至于mT5，即Multilingual T5，T5的多国语言版，出自最近的论文 [《mT5: A massively multilingual pre-trained text-to-text transformer》](https://papers.cool/arxiv/2010.11934)，Github为 [multilingual-t5](https://github.com/google-research/multilingual-t5)，这也是将多语种NLP任务的榜单推到了一个新高度了。当然，对我们来说，最重要的是mT5里边包含了中文，因此我们终于有机会在中文任务中尝试下T5了。

### T5.1.1 [\#](https://kexue.fm/archives/7867\#T5.1.1)

总的来说，mT5跟T5一脉相承的，整体基本一样，但在模型结构方面，mT5使用的是T5.1.1方案，在此对它做个基本的介绍。

很多人都不知道的是，自从在去年10月发布后，T5在今年还经历了一次低调的小升级，具体细节可以查看 [Github链接](https://github.com/google-research/text-to-text-transfer-transformer/blob/master/released_checkpoints.md)，官方把升级前的T5称为T5.1.0，而升级后的叫做T5.1.1。它主要的改动来自论文 [《GLU Variants Improve Transformer》](https://papers.cool/arxiv/2002.05202)，主要是借用了 [《Language Modeling with Gated Convolutional Networks》](https://papers.cool/arxiv/1612.08083) 的GLU（Gated Linear Unit）来增强FFN部分的效果。具体来说，原来T5的FFN为（T5没有Bias）
\\begin{equation}\\text{FFN}(x)=\\text{relu}(xW\_1)W\_2\\end{equation}
现在改为了
\\begin{equation}\\text{FFN}\_{\\text{GEGLU}}(x)=\\big(\\text{gelu}(xW\_1)\\otimes xW\_2\\big)W\_3\\end{equation}
也就是把relu激活的第一个变化层改为了gelu激活的门控线性单元，这样FFN层增加了50%参数，但是从论文效果看效果明显增加。此外，T5.1.1还对Embedding层做了改动，原来在T5.1.0中，Encoder和Decoder的Embedding层、Decoder最后预测概率分布的Softmax层都是共享同一个Embedding矩阵的，现在T5.1.1只让Encoder和Decoder的Embedding层共享，而Decoder最后预测概率分布的Softmax层则用了一个独立的Embedding矩阵，当然这会让参数量大大增加，但Google的结论说这样做效果会更好，其结论被总结在最近的论文 [《Rethinking embedding coupling in pre-trained language models》](https://papers.cool/arxiv/2010.12821) 中。还有最后一点改动，T5.1.1在预训练阶段去掉了Dropout，而只有在下游微调阶段才使用Dropout。

经过这些调整后，Google重新训练并开放了全系列的T5.1.1模型，其下载地址可以在刚才的Github链接找到，注意T5.1.1只做了无监督预训练，但效果依然相当出色。由于T5.1.1提升明显，所以mT5也就继续使用了T5.1.1结构了

### 结果 [\#](https://kexue.fm/archives/7867\#%E7%BB%93%E6%9E%9C)

mT5其实就是重新构建了多国语言版的数据集mC4，然后使用T5.1.1方案训练了一波，技术路线上没有什么明显的创新。关于训练细节，大家观察下原论文就好，论文其实也不长，毕竟T5已经把路都给铺好了。

至于mT5的战绩，主要就是集中在下面这张表内了：

mT5的“战绩”

读者可能会有疑问，这种多国语言版的该用什么方式评测？简单的话，我们可以直接在此基础上finetune一个跨语种的机器翻译任务，看看效果的提升。但事实上，对于多国语言版模型，研究人员更关心的是它在跨语种任务上的Zero Shot表现，说白了，就是同一种任务，在一个语种上进行finetune，其模型能不能直接用于其余语种？这也是上图中“Cross-lingual zero-shot transfer (models fine-tuned on English data only)”的含义了，可以看到，mT5的表现还是相当出色的。

## 实践 [\#](https://kexue.fm/archives/7867\#%E5%AE%9E%E8%B7%B5)

终于到了大家喜闻乐见的实践时间了，这里我们简单介绍一下在bert4keras上使用mT5模型来做中文文本生成任务的流程和技巧。bert4keras从0.9.1版本开始支持调用mT5模型，如果要进行下述实验的读者，请先将bert4keras升级到0.9.1版或以上。

**Github链接： [https://github.com/bojone/t5\_in\_bert4keras](https://github.com/bojone/t5_in_bert4keras)**

### 基本 [\#](https://kexue.fm/archives/7867\#%E5%9F%BA%E6%9C%AC)

用bert4keras把mT5模型加载到keras中的基本代码为

```
# 模型路径
config_path = '/root/kg/bert/mt5/mt5_small/t5_config.json'
checkpoint_path = '/root/kg/bert/mt5/mt5_small/model.ckpt-1000000'
spm_path = '/root/kg/bert/mt5/sentencepiece.model'

# 加载分词器
tokenizer = SpTokenizer(spm_path, token_start=None, token_end=' ')

# 加载模型
t5 = build_transformer_model(
 config_path=config_path,
 checkpoint_path=checkpoint_path,
 model='t5.1.1',
 return_keras_model=False,
 name='T5',
)

encoder = t5.encoder
decoder = t5.decoder
model = t5.model
```

可以看到跟在bert4keras中加载BERT没太大区别，其中 `t5_config.json` 的构建、 `model.ckpt-1000000` 的下载在Github上都有详细介绍，大家请移步去看。完整代码（训练和解码细节）在Github上也可以找到，这里就不展开了。

值得一提的是，对于中文来说，tokenizer给出的结果是带有词的，即对于中文来说mT5是以词为单位的，只不过词颗粒度会比较少。这进一步说明了我们之前的工作 [《提速不掉点：基于词颗粒度的中文WoBERT](https://kexue.fm/archives/7758)》的改进方向是正确的。

### 中文 [\#](https://kexue.fm/archives/7867\#%E4%B8%AD%E6%96%87)

相信看本博客的读者多数都只关心中文任务，部分读者可能也会关心英文任务，应该鲜有读者会关心中英文以外的任务了。然而，mT5涵盖了101种语言，总词表有25万，而且它采用的T5.1.1结构的Softmax还不共享参数，这就导致了Embedding层占用了相当多的参数量，比如mT5 small的参数量为3亿，其中Embedding相关的就占了2.5亿，关键是里边的大部分参数我们都用不上，纯粹是不必要的浪费。因此，对于主要关心中文任务的我们来说，有必要精简一下这个Embedding层了。

对模型的精简很简单，只需要在两个Embedding矩阵中删除不需要的行就行了，关键在于如何决定要保留的token，以及如何得到一个精简后的sentencepiece模型。决定要保留的token，简单来想就是把中文的token保留下来，但是也不只是中文，英文的也要保留一部分，看上去似乎只是一个正则表达式的问题，实际上没那么简单，用英文字母的也不一定是英语，用中文字的也不一定是中文，这是个让人纠结的事情。于是笔者想了另外一个办法：用这个25万token的tokenizer对笔者收集的几十G中文语料分词，统计分词结果，然后按照词频选择前面的部分（最后保留了3万多个token）。这样虽然费时一些，但是比较靠谱，能确保把我们比较需要的token保留下来。决定词表后，就要修改得到一个新的sentencepiece模型，这也有点麻烦，但最终经过搜索后还是把这个事情解决了，处理方法都分享在Github上。

经过这样处理后，要构建新的模型，则只需要多加三行代码 `keep_tokens` 相关的代码，所需要的显存就大大降低，并且中文生成的效果基本不变了：

```
# 模型路径
config_path = '/root/kg/bert/mt5/mt5_base/t5_config.json'
checkpoint_path = '/root/kg/bert/mt5/mt5_base/model.ckpt-1000000'
spm_path = '/root/kg/bert/mt5/sentencepiece_cn.model'
keep_tokens_path = '/root/kg/bert/mt5/sentencepiece_cn_keep_tokens.json'

# 加载分词器
tokenizer = SpTokenizer(spm_path, token_start=None, token_end=' ')
keep_tokens = json.load(open(keep_tokens_path))

# 加载模型
t5 = build_transformer_model(
 config_path=config_path,
 checkpoint_path=checkpoint_path,
 keep_tokens=keep_tokens,
 model='t5.1.1',
 return_keras_model=False,
 name='T5',
)

encoder = t5.encoder
decoder = t5.decoder
model = t5.model
```

### 效果 [\#](https://kexue.fm/archives/7867\#%E6%95%88%E6%9E%9C)

最后，大家应该是关心折腾了这么久，生成效果究竟有没有提升，有没有使用的价值？这样说吧，用mT5 small版本finetune出来的CSL标题生成模型，BLEU指标能持平基于WoBERT的UniLM模型，并且解码速度快130%；而用mT5 base版本finetune出来的CSL标题生成模型，指标能超过基于WoBERT的UniLM模型1%以上，并且解码速度也能快60%。

\\begin{array}{c}
\\text{CSL摘要生成实验结果 (beam size=1)}\\\
{\\begin{array}{c\|cccc\|c}
\\hline
& \\text{Rouge-L} & \\text{Rouge-1} & \\text{Rouge-2} & \\text{BLEU} & \\text{解码速度}\\\
\\hline
\\text{BERT base} & 63.81 & 65.45 & 54.91 & 45.52 & \\text{1x}\\\
\\text{WoBERT base} & 66.38 & 68.22 & 57.83 & 47.76 & \\text{1.1x}\\\
\\hline
\\text{mT5 small} & 65.14 & 67.08 & 56.71 & 47.69 & \\text{2.3x}\\\
\\text{mT5 base} & \\textbf{66.81} & \\textbf{68.94} & \\textbf{58.49} & \\textbf{49.49} & \\text{1.6x}\\\
\\hline
\\end{array}}
\\end{array}

说白了，确实是又快又好。至于设备要求，平时跑过BERT base的同学，基本都应该能跑起mT5 small/base版，甚至large版也可以尝试一下，至于XL和XXL，那就比较难搞了，建议还是放弃吧。更多的惊喜，还是大家自己去挖掘吧～～对了，顺便需要提醒一下，微调T5模型的时候，学习率要比微调BERT大10倍以上才行（即$10^{-4}$级别，BERT一般是$10^{-5}$级别），这是两者模型架构差异决定的。

## 小结 [\#](https://kexue.fm/archives/7867\#%E5%B0%8F%E7%BB%93)

本文回顾了一下Google去年发布的T5模型，然后介绍了最近发布的多国语言版的mT5，最后介绍了如何在bert4keras中微调mT5来做中文任务，结果显示mT5在中文生成上有着很不错的表现，值得做文本生成任务的同学一试。

_**转载到请包括本文地址：** [https://kexue.fm/archives/7867](https://kexue.fm/archives/7867)_

_**更详细的转载事宜请参考：**_ [《科学空间FAQ》](https://kexue.fm/archives/6508#%E6%96%87%E7%AB%A0%E5%A6%82%E4%BD%95%E8%BD%AC%E8%BD%BD/%E5%BC%95%E7%94%A8)

**如果您还有什么疑惑或建议，欢迎在下方评论区继续讨论。**

**如果您觉得本文还不错，欢迎 [分享](https://kexue.fm/archives/7867#share)/ [打赏](https://kexue.fm/archives/7867#pay) 本文。打赏并非要从中获得收益，而是希望知道科学空间获得了多少读者的真心关注。当然，如果你无视它，也不会影响你的阅读。再次表示欢迎和感谢！**

打赏

微信打赏

支付宝打赏

因为网站后台对打赏并无记录，因此欢迎在打赏时候备注留言。你还可以 [**点击这里**](http://mail.qq.com/cgi-bin/qm_share?t=qm_mailme&email=tN7d1drY3drrx8H0xcWa19vZ) 或在下方评论区留言来告知你的建议或需求。

**如果您需要引用本文，请参考：**

苏剑林. (Nov. 06, 2020). 《那个屠榜的T5模型，现在可以在中文上玩玩了 》\[Blog post\]. Retrieved from [https://kexue.fm/archives/7867](https://kexue.fm/archives/7867)

@online{kexuefm-7867,
        title={那个屠榜的T5模型，现在可以在中文上玩玩了},
        author={苏剑林},
        year={2020},
        month={Nov},
        url={\\url{https://kexue.fm/archives/7867}},
}

分类： [信息时代](https://kexue.fm/category/Big-Data)    标签： [语言模型](https://kexue.fm/tag/%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/), [文本生成](https://kexue.fm/tag/%E6%96%87%E6%9C%AC%E7%94%9F%E6%88%90/), [attention](https://kexue.fm/tag/attention/)[50 评论](https://kexue.fm/archives/7867#comments)

< [用ALBERT和ELECTRA之前，请确认你真的了解它们](https://kexue.fm/archives/7846) \| [当GPT遇上中国象棋：写过文章解过题，要不再来下盘棋？](https://kexue.fm/archives/7877) >

### 你也许还对下面的内容感兴趣

- [线性注意力简史：从模仿、创新到反哺](https://kexue.fm/archives/11033)
- [Transformer升级之路：20、MLA究竟好在哪里？](https://kexue.fm/archives/10907)
- [Transformer升级之路：19、第二类旋转位置编码](https://kexue.fm/archives/10862)
- [细水长flow之TARFLOW：流模型满血归来？](https://kexue.fm/archives/10667)
- [“闭门造车”之多模态思路浅谈（三）：位置编码](https://kexue.fm/archives/10352)
- [Decoder-only的LLM为什么需要位置编码？](https://kexue.fm/archives/10347)
- [Monarch矩阵：计算高效的稀疏型矩阵分解](https://kexue.fm/archives/10249)
- [Transformer升级之路：18、RoPE的底数选择原则](https://kexue.fm/archives/10122)
- [缓存与效果的极限拉扯：从MHA、MQA、GQA到MLA](https://kexue.fm/archives/10091)
- [Transformer升级之路：17、多模态位置编码的简单思考](https://kexue.fm/archives/10040)

[发表你的看法](https://kexue.fm/archives/7867#comment_form)

1. [«](https://kexue.fm/archives/7867/comment-page-2#comments)
2. [1](https://kexue.fm/archives/7867/comment-page-1#comments)
3. [2](https://kexue.fm/archives/7867/comment-page-2#comments)
4. [3](https://kexue.fm/archives/7867/comment-page-3#comments)

[中文生成模型T5-Pegasus详解与实践\_Johngo学长](https://www.johngo689.com/260353/)

December 14th, 2022

\[...\]\[4\] 那个屠榜的T5模型，现在可以在中文上玩玩了\[...\]

[回复评论](https://kexue.fm/archives/7867/comment-page-3?replyTo=20555#respond-post-7867)

[中文生成模型T5-Pegasus详解与实践\_Johngo学长](https://www.johngo689.com/706992/)

July 21st, 2023

\[...\]\[4\] 那个屠榜的T5模型，现在可以在中文上玩玩了\[...\]

[回复评论](https://kexue.fm/archives/7867/comment-page-3?replyTo=22316#respond-post-7867)

[中文生成模型T5-Pegasus详解与实践 \| Coding栈](https://www.itcode1024.com/190671/)

July 22nd, 2023

\[...\]\[4\] 那个屠榜的T5模型，现在可以在中文上玩玩了\[...\]

[回复评论](https://kexue.fm/archives/7867/comment-page-3?replyTo=22331#respond-post-7867)

1. [«](https://kexue.fm/archives/7867/comment-page-2#comments)
2. [1](https://kexue.fm/archives/7867/comment-page-1#comments)
3. [2](https://kexue.fm/archives/7867/comment-page-2#comments)
4. [3](https://kexue.fm/archives/7867/comment-page-3#comments)

[取消回复](https://kexue.fm/archives/7867#respond-post-7867)

你的大名

电子邮箱

个人网站（选填）

1\. 可以使用LaTeX代码，点击“预览效果”可查看效果；2. 可以通过点击评论楼层编号来引用该楼层；3. 网站可能会有点卡，如非确认评论失败，请 **不要重复点击提交**。

### 内容速览

[T5](https://kexue.fm/archives/7867#T5)
[训练](https://kexue.fm/archives/7867#%E8%AE%AD%E7%BB%83)
[结果](https://kexue.fm/archives/7867#%E7%BB%93%E6%9E%9C)
[mT5](https://kexue.fm/archives/7867#mT5)
[T5.1.1](https://kexue.fm/archives/7867#T5.1.1)
[结果](https://kexue.fm/archives/7867#%E7%BB%93%E6%9E%9C)
[实践](https://kexue.fm/archives/7867#%E5%AE%9E%E8%B7%B5)
[基本](https://kexue.fm/archives/7867#%E5%9F%BA%E6%9C%AC)
[中文](https://kexue.fm/archives/7867#%E4%B8%AD%E6%96%87)
[效果](https://kexue.fm/archives/7867#%E6%95%88%E6%9E%9C)
[小结](https://kexue.fm/archives/7867#%E5%B0%8F%E7%BB%93)

### 智能搜索

支持整句搜索！网站自动使用 [结巴分词](https://github.com/fxsjy/jieba) 进行分词，并结合ngrams排序算法给出合理的搜索结果。

### 热门标签

[生成模型](https://kexue.fm/tag/%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B/) [attention](https://kexue.fm/tag/attention/) [优化](https://kexue.fm/tag/%E4%BC%98%E5%8C%96/) [语言模型](https://kexue.fm/tag/%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/) [模型](https://kexue.fm/tag/%E6%A8%A1%E5%9E%8B/) [网站](https://kexue.fm/tag/%E7%BD%91%E7%AB%99/) [概率](https://kexue.fm/tag/%E6%A6%82%E7%8E%87/) [梯度](https://kexue.fm/tag/%E6%A2%AF%E5%BA%A6/) [转载](https://kexue.fm/tag/%E8%BD%AC%E8%BD%BD/) [微分方程](https://kexue.fm/tag/%E5%BE%AE%E5%88%86%E6%96%B9%E7%A8%8B/) [矩阵](https://kexue.fm/tag/%E7%9F%A9%E9%98%B5/) [天象](https://kexue.fm/tag/%E5%A4%A9%E8%B1%A1/) [分析](https://kexue.fm/tag/%E5%88%86%E6%9E%90/) [深度学习](https://kexue.fm/tag/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/) [积分](https://kexue.fm/tag/%E7%A7%AF%E5%88%86/) [python](https://kexue.fm/tag/python/) [优化器](https://kexue.fm/tag/%E4%BC%98%E5%8C%96%E5%99%A8/) [力学](https://kexue.fm/tag/%E5%8A%9B%E5%AD%A6/) [无监督](https://kexue.fm/tag/%E6%97%A0%E7%9B%91%E7%9D%A3/) [扩散](https://kexue.fm/tag/%E6%89%A9%E6%95%A3/) [几何](https://kexue.fm/tag/%E5%87%A0%E4%BD%95/) [节日](https://kexue.fm/tag/%E8%8A%82%E6%97%A5/) [生活](https://kexue.fm/tag/%E7%94%9F%E6%B4%BB/) [文本生成](https://kexue.fm/tag/%E6%96%87%E6%9C%AC%E7%94%9F%E6%88%90/) [数论](https://kexue.fm/tag/%E6%95%B0%E8%AE%BA/)

### 随机文章

- [用RecomposIt简单给图片换背景](https://kexue.fm/archives/622)
- [果壳中的条件随机场(CRF In A Nutshell)](https://kexue.fm/archives/4695)
- [基于DGCNN和概率图的轻量级信息抽取模型](https://kexue.fm/archives/6671)
- [更别致的词向量模型(四)：模型的求解](https://kexue.fm/archives/4675)
- [生成函数法与整数的分拆](https://kexue.fm/archives/2942)
- [【NASA每日一图】木星的新疤痕](https://kexue.fm/archives/45)
- [构造一个显式的、总是可逆的矩阵](https://kexue.fm/archives/6407)
- [科学空间：2011年6月重要天象](https://kexue.fm/archives/1370)
- [殊途同归的策略梯度与零阶优化](https://kexue.fm/archives/7737)
- [生活中的趣味数学：同一天生日概率有多大](https://kexue.fm/archives/40)

### 最近评论

- [musicfish1973](https://kexue.fm/archives/8009/comment-page-1#comment-27953): 好的设计都是相似的,haha
- [忍者猫](https://kexue.fm/archives/10592/comment-page-2#comment-27952): 这优化器的作者真的应该给你打钱
- [Chaofa Yuan](https://kexue.fm/archives/11033/comment-page-1#comment-27951): 写得太好了
- [Skyler Lin](https://kexue.fm/archives/11033/comment-page-1#comment-27949): respect苏神！
- [宋佳铭](https://kexue.fm/archives/10958/comment-page-1#comment-27947): 对，个人感觉mean flow就是continuous time CTM
- [宋佳铭](https://kexue.fm/archives/10958/comment-page-1#comment-27946): 的确，对sg这个事情我感觉如果是用‘归纳’法做是不太能避免的，因为毕竟是用步长短的模型去约束步...
- [MoFHeka](https://kexue.fm/archives/10542/comment-page-1#comment-27945): 苏老师您好，请问一下这套结论在稀疏参数上应该如何应用？比如大规模稀疏Embedding，每个B...
- [苏剑林](https://kexue.fm/archives/11033/comment-page-1#comment-27944): Temp LoRA倒是有印象，其实思想是一样的，如果我单独开一篇文章介绍TTT的话，应该会提到...
- [苏剑林](https://kexue.fm/archives/11033/comment-page-1#comment-27943): 你搜搜mamba、rwkv甚至rnn做vision的工作，其实不少。不过多数确实像你说的，正反...
- [苏剑林](https://kexue.fm/archives/9379/comment-page-1#comment-27942): 问题1可以看看 https://kexue.fm/archives/4718 ，简单来说就是点...

### 友情链接

- [Cool Papers](https://papers.cool)
- [数学研发](https://bbs.emath.ac.cn)
- [Seatop](http://www.seatop.com.cn/)
- [Xiaoxia](https://xiaoxia.org/)
- [积分表-网络版](https://kexue.fm/sci/integral/index.html)
- [丝路博傲](http://blog.dvxj.com/)
- [ph4ntasy 饭特稀](http://www.ph4ntasy.com/)
- [数学之家](http://www.2math.cn/)
- [有趣天文奇观](http://interesting-sky.china-vo.org/)
- [TwistedW](http://www.twistedwg.com/)
- [godweiyang](https://godweiyang.com/)
- [AI柠檬](https://blog.ailemon.net/)
- [王登科-DK博客](https://greatdk.com)
- [ESON](https://blog.eson.org/)
- [枫之羽](https://fzhiy.net/)
- [Mathor's blog](https://wmathor.com/)
- [coding-zuo](https://coding-zuo.github.io/)
- [博科园](https://www.bokeyuan.net/)
- [孔皮皮的博客](https://www.kppkkp.top/)
- [运鹏的博客](https://yunpengtai.top/)
- [jiming.site](https://jiming.site/)
- [OmegaXYZ](https://www.omegaxyz.com/)
- [Blog by Eacls](https://www.eacls.top/)
- [EAI猩球](https://www.robotech.ink/)
- [文举的博客](https://liwenju0.com/)
- [用代码打点酱油](https://bruceyuan.com/)
- [申请链接](https://kexue.fm/links.html)

本站采用创作共用版权协议，要求署名、非商业用途和保持一致。转载本站内容必须也遵循“ [署名-非商业用途-保持一致](http://creativecommons.org/licenses/by-nc-nd/2.5/cn/)”的创作共用协议。
© 2009-2025 Scientific Spaces. All rights reserved. Theme by [laogui](http://www.laogui.com). Powered by [Typecho](http://typecho.org). 备案号: [粤ICP备09093259号-1/2](https://beian.miit.gov.cn/)。