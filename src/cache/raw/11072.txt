## SEARCH

## MENU

- [打赏](https://kexue.fm/reward.html)
- [公式](https://kexue.fm/latex.html)
- [天象](https://kexue.fm/ac.html)
- [链接](https://kexue.fm/links.html)
- [时光](https://kexue.fm/me.html)
- [博览](https://kexue.fm/science.html)
- [归档](https://kexue.fm/content.html)

## CATEGORIES

- [千奇百怪](https://kexue.fm/category/Everything)
- [天文探索](https://kexue.fm/category/Astronomy)
- [数学研究](https://kexue.fm/category/Mathematics)
- [物理化学](https://kexue.fm/category/Phy-chem)
- [信息时代](https://kexue.fm/category/Big-Data)
- [生物自然](https://kexue.fm/category/Biology)
- [图片摄影](https://kexue.fm/category/Photograph)
- [问题百科](https://kexue.fm/category/Questions)
- [生活/情感](https://kexue.fm/category/Life-Feeling)
- [资源共享](https://kexue.fm/category/Resources)

## NEWPOSTS

- [让炼丹更科学一些（三）：SGD的终...](https://kexue.fm/archives/11480)
- [让炼丹更科学一些（二）：将结论推广...](https://kexue.fm/archives/11469)
- [滑动平均视角下的权重衰减和学习率](https://kexue.fm/archives/11459)
- [生成扩散模型漫谈（三十一）：预测数...](https://kexue.fm/archives/11428)
- [Muon优化器指南：快速上手与关键细节](https://kexue.fm/archives/11416)
- [AdamW的Weight RMS的...](https://kexue.fm/archives/11404)
- [n个正态随机数的最大值的渐近估计](https://kexue.fm/archives/11390)
- [流形上的最速下降：5\. 对偶梯度下降](https://kexue.fm/archives/11388)
- [低精度Attention可能存在有...](https://kexue.fm/archives/11371)
- [MuP之上：1. 好模型的三个特征](https://kexue.fm/archives/11340)

## COMMENTS

- [sog: 好的，符号相同，搞混了呃](https://kexue.fm/archives/11469/comment-page-1#comment-29035)
- [kerry: 还没有通读完后面的系列，提出一些拙见。\
降低方差这一节把原本的...](https://kexue.fm/archives/9119/comment-page-14#comment-29034)
- [Kevin Yin: I wrote https://research.novela...](https://kexue.fm/archives/11158/comment-page-1#comment-29033)
- [罗: 公式(6)显示出来是不是有点小问题？](https://kexue.fm/archives/11480/comment-page-1#comment-29032)
- [cmlin: 本人对这方面不太熟悉，想了解这三个条件的意义及动机，且希望这系...](https://kexue.fm/archives/11340/comment-page-1#comment-29031)
- [喝一口可乐: 理解了，感谢苏神回复，数学上给出建模分析确实清晰了很多，再次感...](https://kexue.fm/archives/10958/comment-page-3#comment-29030)
- [CuddleSabe1: 感觉普通的 flow matching 可以看成 degrad...](https://kexue.fm/archives/10958/comment-page-1#comment-29029)
- [岁月如书: 受教了，感谢](https://kexue.fm/archives/11126/comment-page-3#comment-29028)
- [苏剑林: 是](https://kexue.fm/archives/11126/comment-page-3#comment-29027)
- [岁月如书: 哦哦，原来是有实验结论，那是我盲目了。多问一句，你说的atte...](https://kexue.fm/archives/11126/comment-page-3#comment-29026)

## USERLOGIN

- [登录](https://kexue.fm/admin/login.php)

[科学空间\|Scientific Spaces](https://kexue.fm)

- [登录](https://kexue.fm/admin/login.php)
- [打赏](https://kexue.fm/reward.html)
- [公式](https://kexue.fm/latex.html)
- [天象](https://kexue.fm/ac.html)
- [链接](https://kexue.fm/links.html)
- [时光](https://kexue.fm/me.html)
- [博览](https://kexue.fm/science.html)
- [归档](https://kexue.fm/content.html)

渴望成为一个小飞侠

- [欢迎订阅](https://kexue.fm/feed)
- [个性邮箱](https://kexue.fm/archives/119)
- [天象信息](https://kexue.fm/ac.html)
- [观测ISS](https://kexue.fm/archives/41)
- [LaTeX](https://kexue.fm/latex.html)
- [关于博主](https://kexue.fm/me.html)

欢迎访问“科学空间”，这里将与您共同探讨自然科学，回味人生百态；也期待大家的分享～

- [**千奇百怪** Everything](https://kexue.fm/category/Everything)
- [**天文探索** Astronomy](https://kexue.fm/category/Astronomy)
- [**数学研究** Mathematics](https://kexue.fm/category/Mathematics)
- [**物理化学** Phy-chem](https://kexue.fm/category/Phy-chem)
- [**信息时代** Big-Data](https://kexue.fm/category/Big-Data)
- [**生物自然** Biology](https://kexue.fm/category/Biology)
- [**图片摄影** Photograph](https://kexue.fm/category/Photograph)
- [**问题百科** Questions](https://kexue.fm/category/Questions)
- [**生活/情感** Life-Feeling](https://kexue.fm/category/Life-Feeling)
- [**资源共享** Resources](https://kexue.fm/category/Resources)

- [**千奇百怪**](https://kexue.fm/category/Everything)
- [**天文探索**](https://kexue.fm/category/Astronomy)
- [**数学研究**](https://kexue.fm/category/Mathematics)
- [**物理化学**](https://kexue.fm/category/Phy-chem)
- [**信息时代**](https://kexue.fm/category/Big-Data)
- [**生物自然**](https://kexue.fm/category/Biology)
- [**图片摄影**](https://kexue.fm/category/Photograph)
- [**问题百科**](https://kexue.fm/category/Questions)
- [**生活/情感**](https://kexue.fm/category/Life-Feeling)
- [**资源共享**](https://kexue.fm/category/Resources)

[首页](https://kexue.fm) [数学研究](https://kexue.fm/category/Mathematics) “对角+低秩”三角阵的高效求逆方法

1Jul

# [“对角+低秩”三角阵的高效求逆方法](https://kexue.fm/archives/11072)

By 苏剑林 \|
2025-07-01 \|
28676位读者\|

从文章 [《线性注意力简史：从模仿、创新到反哺》](https://kexue.fm/archives/11033) 我们可以发现，DeltaNet及其后的线性Attention模型，基本上都关联到了逆矩阵$(\\boldsymbol{I} + \\boldsymbol{K}\\boldsymbol{K}^{\\top}\\odot\\boldsymbol{M}^-)^{-1}$。本文就专门来探讨一下这类具有“对角+低秩”特点的三角矩阵的逆矩阵计算。

## 基本结果 [\#](https://kexue.fm/kexue.fm\#%E5%9F%BA%E6%9C%AC%E7%BB%93%E6%9E%9C)

我们将问题一般地定义如下：

> 给定矩阵$\\boldsymbol{Q},\\boldsymbol{K}\\in\\mathbb{R}^{n\\times d}$和对角矩阵$\\boldsymbol{\\Lambda}\\in\\mathbb{R}^{n\\times n}$，满足$n\\gg d$，定义
> \\begin{equation}\\boldsymbol{T} = \\boldsymbol{\\Lambda} + \\boldsymbol{Q}\\boldsymbol{K}^{\\top}\\odot\\boldsymbol{M}^-\\end{equation}
> 其中$\\boldsymbol{M}^-=\\boldsymbol{M} - \\boldsymbol{I}$，矩阵$\\boldsymbol{M}$定义为
> \\begin{equation}M\_{i,j} = \\left\\{\\begin{aligned} &1, &i \\geq j \\\ &0, &i < j\\end{aligned}\\right.\\end{equation}
> 现在要求逆矩阵$\\boldsymbol{T}^{-1}$，并且证明其复杂度是$\\mathcal{O}(n^2)$。

首先，如果没有$\\odot\\boldsymbol{M}^-$的下三角阵约束，那么它可以直接由“ [Woodbury恒等式](https://en.wikipedia.org/wiki/Woodbury_matrix_identity)”解决：
\\begin{equation}(\\boldsymbol{\\Lambda} + \\boldsymbol{Q}\\boldsymbol{K}^{\\top})^{-1} = \\boldsymbol{\\Lambda}^{-1} - \\boldsymbol{\\Lambda}^{-1} \\boldsymbol{Q}(\\boldsymbol{I} + \\boldsymbol{K}^{\\top}\\boldsymbol{\\Lambda}^{-1}\\boldsymbol{Q})^{-1}\\boldsymbol{K}^{\\top}\\boldsymbol{\\Lambda}^{-1}\\end{equation}
容易验证右端的计算复杂度是$\\mathcal{O}(n^2)$的。然而，加上$\\odot\\boldsymbol{M}^-$后，$\\boldsymbol{T}$本身就不再具有“对角+低秩”的结构，因此不能直接由该恒等式解决了。针对下三角矩阵这一特点，一个基本的思路是递归，因为我们有分块矩阵恒等式
\\begin{equation}\\begin{bmatrix}\\boldsymbol{A} & \\boldsymbol{0} \\\ \\boldsymbol{C} & \\boldsymbol{B}\\end{bmatrix}^{-1} = \\begin{bmatrix}\\boldsymbol{A}^{-1} & \\boldsymbol{0} \\\ -\\boldsymbol{B}^{-1}\\boldsymbol{C}\\boldsymbol{A}^{-1} & \\boldsymbol{B}^{-1}\\end{bmatrix}\\end{equation}
这允许我们将$\\boldsymbol{T}^{-1}$转化递归形式（约定：没有括号的情况下，切片的优先级最高）
\\begin{equation}\\boldsymbol{T}\_{\[:l+1,:l+1\]}^{-1} = \\begin{bmatrix}\\boldsymbol{T}\_{\[:l,:l\]}^{-1} & \\boldsymbol{0} \\\ -\\boldsymbol{T}\_{\[l:l+1,l:l+1\]}^{-1}\\boldsymbol{T}\_{\[l:l+1,:l\]}\\boldsymbol{T}\_{\[:l,:l\]}^{-1} & \\boldsymbol{T}\_{\[l:l+1,l:l+1\]}^{-1}\\end{bmatrix}\\end{equation}
其中主要计算是$\\boldsymbol{T}\_{\[l:l+1,:l\]}\\boldsymbol{T}\_{\[:l,:l\]}^{-1}$，它是一个$1\\times l$和$l\\times l$矩阵相乘，复杂度是$\\mathcal{O}(\\mathcal{l^2})$，即每一步迭代的复杂度是平方增长的，所以总复杂度是$\\mathcal{O}(n^3)$。

## 低秩结构 [\#](https://kexue.fm/kexue.fm\#%E4%BD%8E%E7%A7%A9%E7%BB%93%E6%9E%84)

当然，这是因为我们还没用上$\\boldsymbol{T}$（$\\odot\\boldsymbol{M}^-$前）的低秩结构，现在我们把它利用起来，那么将会得到$\\boldsymbol{T}\_{\[l:l+1,:l\]} = \\boldsymbol{Q}\_{\[l:l+1\]}\\boldsymbol{K}\_{\[:l\]}^{\\top}$，代入上式得：
\\begin{equation}\\boldsymbol{T}\_{\[:l+1,:l+1\]}^{-1} = \\begin{bmatrix}\\boldsymbol{T}\_{\[:l,:l\]}^{-1} & \\boldsymbol{0} \\\ -\\boldsymbol{T}\_{\[l:l+1,l:l+1\]}^{-1}\\boldsymbol{Q}\_{\[l:l+1\]}\\boldsymbol{K}\_{\[:l\]}^{\\top}\\boldsymbol{T}\_{\[:l,:l\]}^{-1} & \\boldsymbol{T}\_{\[l:l+1,l:l+1\]}^{-1}\\end{bmatrix}\\end{equation}
注意$\\boldsymbol{K}\_{\[:l\]}^{\\top}\\boldsymbol{T}\_{\[:l,:l\]}^{-1}\\in\\mathbb{R}^{d\\times l}$，如果我们能以它为递归变量，那么每一步迭代的复杂度就只是$\\mathcal{O}(l)$，总复杂度就能成功降到$\\mathcal{O}(n^2)$。根据这个思路，我们有
\\begin{equation}\\begin{aligned}
\\boldsymbol{K}\_{\[:l+1\]}^{\\top}\\boldsymbol{T}\_{\[:l+1,:l+1\]}^{-1} =&\\, \\begin{bmatrix}\\boldsymbol{K}\_{\[:l\]}^{\\top} & \\boldsymbol{K}\_{\[l:l+1\]}^{\\top}\\end{bmatrix}\\begin{bmatrix}\\boldsymbol{T}\_{\[:l,:l\]}^{-1} & \\boldsymbol{0} \\\ -\\boldsymbol{T}\_{\[l:l+1,l:l+1\]}^{-1}\\boldsymbol{Q}\_{\[l:l+1\]}\\boldsymbol{K}\_{\[:l\]}^{\\top}\\boldsymbol{T}\_{\[:l,:l\]}^{-1} & \\boldsymbol{T}\_{\[l:l+1,l:l+1\]}^{-1}\\end{bmatrix} \\\\[6pt\]
=&\\, \\begin{bmatrix}\\boldsymbol{K}\_{\[:l\]}^{\\top}\\boldsymbol{T}\_{\[:l,:l\]}^{-1} & \\boldsymbol{0}\\end{bmatrix} + \\boldsymbol{K}\_{\[l:l+1\]}^{\\top}\\underbrace{\\begin{bmatrix}-\\boldsymbol{T}\_{\[l:l+1,l:l+1\]}^{-1}\\boldsymbol{Q}\_{\[l:l+1\]}\\boldsymbol{K}\_{\[:l\]}^{\\top}\\boldsymbol{T}\_{\[:l,:l\]}^{-1} & \\boldsymbol{T}\_{\[l:l+1,l:l+1\]}^{-1}\\end{bmatrix}}\_{\\text{实际上就是 }(\\boldsymbol{T}^{-1})\_{\[l:l+1,:l+1\]}}\\end{aligned}\\end{equation}
可以看到这个递归过程也没有涉及到$\\mathcal{O}(l^2)$的运算，因此思路是可行的，只需要引入一个新变量来缓存$\\boldsymbol{K}\_{\[:l\]}^{\\top}\\boldsymbol{T}\_{\[:l,:l\]}^{-1}$。如果我们将$l+1$换成$l+c$，那么就可以得到chunk格式的递归。

测试代码如下：

```
import numpy as np

n, d, c = 1000, 100, 200
Q = np.random.randn(n, d) / d**0.5
K = np.random.randn(n, d) / d**0.5
T = np.tril(Q @ K.T, -1) + np.eye(n)

Y, Z = np.zeros((n, n)), np.zeros((d, n))
for l in range(0, n, c):
 Y[l:l + c, l:l + c] = np.linalg.inv(T[l:l + c, l:l + c])
 Y[l:l + c, :l] = - Y[l:l + c, l:l + c] @ Q[l:l + c] @ Z[:, :l]
 Z[:, :l + c] += K[l:l + c].T @ Y[l:l + c, :l + c]

np.allclose(Y @ T, np.eye(n))
```

## 乘法计算 [\#](https://kexue.fm/kexue.fm\#%E4%B9%98%E6%B3%95%E8%AE%A1%E7%AE%97)

基于同样的思路，我们还可以证明：

> 对于任意矩阵$\\boldsymbol{V}\\in\\mathbb{R}^{n\\times d}$，计算$\\boldsymbol{T}^{-1}\\boldsymbol{V}$只需要$\\mathcal{O}(n)$的复杂度。

证明只需要把前述过程稍微改动一下。首先有
\\begin{equation}\\begin{aligned}
(\\boldsymbol{T}^{-1}\\boldsymbol{V})\_{\[:l+1\]} =&\\, \\boldsymbol{T}\_{\[:l+1,:l+1\]}^{-1}\\boldsymbol{V}\_{\[:l+1\]} \\\\[6pt\]
=&\\, \\begin{bmatrix}\\boldsymbol{T}\_{\[:l,:l\]}^{-1} & \\boldsymbol{0} \\\ -\\boldsymbol{T}\_{\[l:l+1,l:l+1\]}^{-1}\\boldsymbol{Q}\_{\[l:l+1\]}\\boldsymbol{K}\_{\[:l\]}^{\\top}\\boldsymbol{T}\_{\[:l,:l\]}^{-1} & \\boldsymbol{T}\_{\[l:l+1,l:l+1\]}^{-1}\\end{bmatrix}\\begin{bmatrix}\\boldsymbol{V}\_{\[:l\]} \\\ \\boldsymbol{V}\_{\[l:l+1\]}\\end{bmatrix} \\\\[6pt\]
=&\\, \\begin{bmatrix}\\boldsymbol{T}\_{\[:l,:l\]}^{-1}\\boldsymbol{V}\_{\[:l\]} \\\ -\\boldsymbol{T}\_{\[l:l+1,l:l+1\]}^{-1}\\boldsymbol{Q}\_{\[l:l+1\]}\\boldsymbol{K}\_{\[:l\]}^{\\top}\\boldsymbol{T}\_{\[:l,:l\]}^{-1}\\boldsymbol{V}\_{\[:l\]} + \\boldsymbol{T}\_{\[l:l+1,l:l+1\]}^{-1}\\boldsymbol{V}\_{\[l:l+1\]}\\end{bmatrix} \\\\[6pt\]
=&\\, \\begin{bmatrix}(\\boldsymbol{T}^{-1}\\boldsymbol{V})\_{\[:l\]} \\\ \\boldsymbol{T}\_{\[l:l+1,l:l+1\]}^{-1}(\\boldsymbol{V}\_{\[l:l+1\]} - \\boldsymbol{Q}\_{\[l:l+1\]}\\boldsymbol{K}\_{\[:l\]}^{\\top}(\\boldsymbol{T}^{-1}\\boldsymbol{V})\_{\[:l\]})\\end{bmatrix}
\\end{aligned}\\end{equation}
然后
\\begin{equation}\\begin{aligned}
\\boldsymbol{K}\_{\[:l+1\]}^{\\top}(\\boldsymbol{T}^{-1}\\boldsymbol{V})\_{\[:l+1\]} =&\\, \\begin{bmatrix}\\boldsymbol{K}\_{\[:l\]}^{\\top} & \\boldsymbol{K}\_{\[l:l+1\]}^{\\top}\\end{bmatrix}\\begin{bmatrix}(\\boldsymbol{T}^{-1}\\boldsymbol{V})\_{\[:l\]} \\\ (\\boldsymbol{T}^{-1}\\boldsymbol{V})\_{\[l:l+1\]} \\end{bmatrix} \\\\[8pt\]
=&\\,\\boldsymbol{K}\_{\[:l\]}^{\\top}(\\boldsymbol{T}^{-1}\\boldsymbol{V})\_{\[:l\]} + \\boldsymbol{K}\_{\[l:l+1\]}^{\\top}(\\boldsymbol{T}^{-1}\\boldsymbol{V})\_{\[l:l+1\]}
\\end{aligned}\\end{equation}
因此，只需要缓存$\\boldsymbol{K}\_{\[:l\]}^{\\top}(\\boldsymbol{T}^{-1}\\boldsymbol{V})\_{\[:l\]}\\in\\mathbb{R}^{d\\times d}$，就可以使得每步的计算复杂度与$l$无关，因此总复杂度是$\\mathcal{O}(n)$。同样，只需要将$l+1$换成$l+c$就可以得到chunk格式。

测试代码如下：

```
import numpy as np

n, d, c = 1000, 100, 200
Q = np.random.randn(n, d) / d**0.5
K = np.random.randn(n, d) / d**0.5
V = np.random.randn(n, d) / d**0.5
T = np.tril(Q @ K.T, -1) + np.eye(n)

Y, Z = np.zeros((n, d)), np.zeros((d, d))
for l in range(0, n, c):
 X = np.linalg.inv(T[l:l + c, l:l + c])
 Y[l:l + c] = X @ (V[l:l + c] - Q[l:l + c] @ Z)
 Z += K[l:l + c].T @ Y[l:l + c]

np.allclose(T @ Y, V)
```

## 文章小结 [\#](https://kexue.fm/kexue.fm\#%E6%96%87%E7%AB%A0%E5%B0%8F%E7%BB%93)

本文讨论了“对角+低秩”特点的三角矩阵求逆问题，这类矩阵普遍出现在新式线性Attention模型中。

_**转载到请包括本文地址：** [https://kexue.fm/archives/11072](https://kexue.fm/archives/11072)_

_**更详细的转载事宜请参考：**_ [《科学空间FAQ》](https://kexue.fm/archives/6508#%E6%96%87%E7%AB%A0%E5%A6%82%E4%BD%95%E8%BD%AC%E8%BD%BD/%E5%BC%95%E7%94%A8)

**如果您还有什么疑惑或建议，欢迎在下方评论区继续讨论。**

**如果您觉得本文还不错，欢迎 [分享](https://kexue.fm/kexue.fm#share)/ [打赏](https://kexue.fm/kexue.fm#pay) 本文。打赏并非要从中获得收益，而是希望知道科学空间获得了多少读者的真心关注。当然，如果你无视它，也不会影响你的阅读。再次表示欢迎和感谢！**

打赏

微信打赏

支付宝打赏

因为网站后台对打赏并无记录，因此欢迎在打赏时候备注留言。你还可以 [**点击这里**](http://mail.qq.com/cgi-bin/qm_share?t=qm_mailme&email=tN7d1drY3drrx8H0xcWa19vZ) 或在下方评论区留言来告知你的建议或需求。

**如果您需要引用本文，请参考：**

苏剑林. (Jul. 01, 2025). 《“对角+低秩”三角阵的高效求逆方法 》\[Blog post\]. Retrieved from [https://kexue.fm/archives/11072](https://kexue.fm/archives/11072)

@online{kexuefm-11072,
        title={“对角+低秩”三角阵的高效求逆方法},
        author={苏剑林},
        year={2025},
        month={Jul},
        url={\\url{https://kexue.fm/archives/11072}},
}

分类： [数学研究](https://kexue.fm/category/Mathematics)    标签： [计算](https://kexue.fm/tag/%E8%AE%A1%E7%AE%97/), [矩阵](https://kexue.fm/tag/%E7%9F%A9%E9%98%B5/), [RNN](https://kexue.fm/tag/RNN/), [attention](https://kexue.fm/tag/attention/)[15 评论](https://kexue.fm/archives/11072#comments)

< [通过msign来计算奇异值裁剪mclip（下）](https://kexue.fm/archives/11059) \| [Transformer升级之路：21、MLA好在哪里?（下）](https://kexue.fm/archives/11111) >

### 你也许还对下面的内容感兴趣

- [Muon优化器指南：快速上手与关键细节](https://kexue.fm/archives/11416)
- [流形上的最速下降：5\. 对偶梯度下降](https://kexue.fm/archives/11388)
- [低精度Attention可能存在有偏的舍入误差](https://kexue.fm/archives/11371)
- [随机矩阵的谱范数的快速估计](https://kexue.fm/archives/11335)
- [为什么线性注意力要加Short Conv？](https://kexue.fm/archives/11320)
- [流形上的最速下降：4\. Muon + 谱球面](https://kexue.fm/archives/11241)
- [流形上的最速下降：3\. Muon + Stiefel](https://kexue.fm/archives/11221)
- [流形上的最速下降：2\. Muon + 正交](https://kexue.fm/archives/11215)
- [矩阵r次方根和逆r次方根的高效计算](https://kexue.fm/archives/11175)
- [矩阵平方根和逆平方根的高效计算](https://kexue.fm/archives/11158)

[发表你的看法](https://kexue.fm/kexue.fm#comment_form)

Kuo

July 1st, 2025

这几天在看杨松琳文章，并行运算很优雅。两个事实使得deltanet并行运算成为可能，一是outer products的求和等效两个矩阵相乘，self attention系统等效DAG

[回复评论](https://kexue.fm/archives/11072/comment-page-1?replyTo=28023#respond-post-11072)

Kuo 发表于
August 15th, 2025

在看了FLA 源码后，感觉止步于读论文实在太轻松了。。。
这是求逆代码 https://github.com/fla-org/flash-linear-attention/blob/main/fla/ops/utils/solve\_tril.py#L71

先计算一个小的16\*16 block，用的 forward substitution 算法。假设$(I+A)(I+X)=I$, 求解$X$。由于$A$是strict lower triangular，问题就变成了递归计算$A$的行向量的线性组合，并更新行向量作为下一步的计算基础。也可以 16\*16 block 为基础推广到更大的 chunk size，先计算对角block，再计算边角

[回复评论](https://kexue.fm/archives/11072/comment-page-1?replyTo=28392#respond-post-11072)

[苏剑林](https://kexue.fm) 发表于
August 16th, 2025

那必须的，高效的实现需要进一步切分block，甚至还要手动写bwd函数。不过就本文的公式而言，写一个青春版应该足够了。

[回复评论](https://kexue.fm/archives/11072/comment-page-1?replyTo=28403#respond-post-11072)

lidhrandom

July 7th, 2025

Equation 3的等号右侧第二项的第一个${\\Lambda^{-1}}$疑似不应取逆

[回复评论](https://kexue.fm/archives/11072/comment-page-1?replyTo=28054#respond-post-11072)

[苏剑林](https://kexue.fm) 发表于
July 11th, 2025

有逆的，参考 https://en.wikipedia.org/wiki/Woodbury\_matrix\_identity

[回复评论](https://kexue.fm/archives/11072/comment-page-1?replyTo=28075#respond-post-11072)

liangzhh

July 7th, 2025

谢谢大佬的分享，感觉中间有两个手误敲错，式(9)最后应该是加号，另外是chunk而不是chuck吧？

[回复评论](https://kexue.fm/archives/11072/comment-page-1?replyTo=28055#respond-post-11072)

[苏剑林](https://kexue.fm) 发表于
July 11th, 2025

谢谢指出。chunk这个词我打错过N次了hhh，怎么看它都不像一个英文单词的模样。

[回复评论](https://kexue.fm/archives/11072/comment-page-1?replyTo=28076#respond-post-11072)

kingdeewang

July 26th, 2025

想请教一下，为什么公式3的复杂度是$\\mathcal{O}(n^2)$

[回复评论](https://kexue.fm/archives/11072/comment-page-1?replyTo=28239#respond-post-11072)

[苏剑林](https://kexue.fm) 发表于
July 30th, 2025

每一步矩阵乘法至多有一侧矩阵出现$n\\times n$形状。

[回复评论](https://kexue.fm/archives/11072/comment-page-1?replyTo=28256#respond-post-11072)

kingdeewang

August 9th, 2025

还是想请教一下
\\begin{equation} ({I} + {K}^{\\top}{\\Lambda}^{-1} {Q})^{-1}\\end{equation}
这个公式是怎么计算的？

[回复评论](https://kexue.fm/archives/11072/comment-page-1?replyTo=28337#respond-post-11072)

[苏剑林](https://kexue.fm) 发表于
August 12th, 2025

直接按照公式算啊。$\\boldsymbol{\\Lambda}$是$n\\times n$的对角阵，$\\boldsymbol{\\Lambda}^{-1}$只要$\\mathcal{O}(n)$；$\\boldsymbol{\\Lambda}^{-1}\\boldsymbol{Q}$是对角矩阵乘$n\\times d$矩阵，只要$\\mathcal{O}(nd)$；$\\boldsymbol{K}^{\\top}\\boldsymbol{\\Lambda}^{-1}\\boldsymbol{Q}$是$d\\times n$矩阵乘$n\\times d$矩阵，只要$\\mathcal{O}(nd^2)$；最后是$d\\times d$矩阵的逆，只要$\\mathcal{O}(d^3)$。

全程关于$n$的复杂度至多是$\\mathcal{O}(n)$的。

[回复评论](https://kexue.fm/archives/11072/comment-page-1?replyTo=28365#respond-post-11072)

kingdeewang 发表于
August 12th, 2025

明白了，多谢

[回复评论](https://kexue.fm/archives/11072/comment-page-1?replyTo=28374#respond-post-11072)

kio

August 14th, 2025

这个方法做不了带exp(qk)核的吧，这里是把K^T T拆出来了

[回复评论](https://kexue.fm/archives/11072/comment-page-1?replyTo=28388#respond-post-11072)

Kuo 发表于
August 16th, 2025

苏老师综述的线性注意力反哺，其中PaTH Attention 就将这个求逆形式与softmax attention 做了结合，
[https://kexue.fm/archives/11033](https://kexue.fm/archives/11033)
算也不算？

[回复评论](https://kexue.fm/archives/11072/comment-page-1?replyTo=28395#respond-post-11072)

[苏剑林](https://kexue.fm) 发表于
August 16th, 2025

带$\\exp$就完全没有低秩结构了，这个没有办法，至多用解方程的思路降低到二次。

[回复评论](https://kexue.fm/archives/11072/comment-page-1?replyTo=28400#respond-post-11072)

[取消回复](https://kexue.fm/archives/11072#respond-post-11072)

你的大名

电子邮箱

个人网站（选填）

1\. 可以使用LaTeX代码，点击“预览效果”可查看效果；2. 可以通过点击评论楼层编号来引用该楼层；3. 网站可能会有点卡，如非确认评论失败，请 **不要重复点击提交**。

### 内容速览

[基本结果](https://kexue.fm/kexue.fm#%E5%9F%BA%E6%9C%AC%E7%BB%93%E6%9E%9C)
[低秩结构](https://kexue.fm/kexue.fm#%E4%BD%8E%E7%A7%A9%E7%BB%93%E6%9E%84)
[乘法计算](https://kexue.fm/kexue.fm#%E4%B9%98%E6%B3%95%E8%AE%A1%E7%AE%97)
[文章小结](https://kexue.fm/kexue.fm#%E6%96%87%E7%AB%A0%E5%B0%8F%E7%BB%93)

### 智能搜索

支持整句搜索！网站自动使用 [结巴分词](https://github.com/fxsjy/jieba) 进行分词，并结合ngrams排序算法给出合理的搜索结果。

### 热门标签

[生成模型](https://kexue.fm/tag/%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B/) [attention](https://kexue.fm/tag/attention/) [优化](https://kexue.fm/tag/%E4%BC%98%E5%8C%96/) [语言模型](https://kexue.fm/tag/%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/) [模型](https://kexue.fm/tag/%E6%A8%A1%E5%9E%8B/) [网站](https://kexue.fm/tag/%E7%BD%91%E7%AB%99/) [梯度](https://kexue.fm/tag/%E6%A2%AF%E5%BA%A6/) [概率](https://kexue.fm/tag/%E6%A6%82%E7%8E%87/) [矩阵](https://kexue.fm/tag/%E7%9F%A9%E9%98%B5/) [优化器](https://kexue.fm/tag/%E4%BC%98%E5%8C%96%E5%99%A8/) [转载](https://kexue.fm/tag/%E8%BD%AC%E8%BD%BD/) [微分方程](https://kexue.fm/tag/%E5%BE%AE%E5%88%86%E6%96%B9%E7%A8%8B/) [分析](https://kexue.fm/tag/%E5%88%86%E6%9E%90/) [天象](https://kexue.fm/tag/%E5%A4%A9%E8%B1%A1/) [深度学习](https://kexue.fm/tag/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/) [积分](https://kexue.fm/tag/%E7%A7%AF%E5%88%86/) [python](https://kexue.fm/tag/python/) [扩散](https://kexue.fm/tag/%E6%89%A9%E6%95%A3/) [力学](https://kexue.fm/tag/%E5%8A%9B%E5%AD%A6/) [无监督](https://kexue.fm/tag/%E6%97%A0%E7%9B%91%E7%9D%A3/) [几何](https://kexue.fm/tag/%E5%87%A0%E4%BD%95/) [节日](https://kexue.fm/tag/%E8%8A%82%E6%97%A5/) [生活](https://kexue.fm/tag/%E7%94%9F%E6%B4%BB/) [文本生成](https://kexue.fm/tag/%E6%96%87%E6%9C%AC%E7%94%9F%E6%88%90/) [数论](https://kexue.fm/tag/%E6%95%B0%E8%AE%BA/)

### 随机文章

- [那个屠榜的T5模型，现在可以在中文上玩玩了](https://kexue.fm/archives/7867)
- [EAE：自编码器 + BN + 最大熵 = 生成模型](https://kexue.fm/archives/7343)
- [变分与理论力学略览](https://kexue.fm/archives/1304)
- [《自然极值》系列——5.最速降线的故事](https://kexue.fm/archives/1094)
- [\[更新\]将向量乘法“退化”到复数](https://kexue.fm/archives/1188)
- [分享一个slide：花式自然语言处理](https://kexue.fm/archives/4823)
- [【NASA每日一图】太阳系中的木卫三](https://kexue.fm/archives/130)
- [2012北约自主招生数学](https://kexue.fm/archives/1551)
- [我们真的需要把训练集的损失降低到零吗？](https://kexue.fm/archives/7643)
- [从费马大定理谈起（七）：费马平方和定理](https://kexue.fm/archives/2886)

### 最近评论

- [sog](https://kexue.fm/archives/11469/comment-page-1#comment-29035): 好的，符号相同，搞混了呃
- [kerry](https://kexue.fm/archives/9119/comment-page-14#comment-29034): 还没有通读完后面的系列，提出一些拙见。
降低方差这一节把原本的目标“预测单步的noise”变成...
- [Kevin Yin](https://kexue.fm/archives/11158/comment-page-1#comment-29033): I wrote https://research.novelai.net/muonscale/...
- [罗](https://kexue.fm/archives/11480/comment-page-1#comment-29032): 公式(6)显示出来是不是有点小问题？
- [cmlin](https://kexue.fm/archives/11340/comment-page-1#comment-29031): 本人对这方面不太熟悉，想了解这三个条件的意义及动机，且希望这系列可以继续写下去。以下想发表一些...
- [喝一口可乐](https://kexue.fm/archives/10958/comment-page-3#comment-29030): 理解了，感谢苏神回复，数学上给出建模分析确实清晰了很多，再次感谢苏神回复！
- [CuddleSabe1](https://kexue.fm/archives/10958/comment-page-1#comment-29029): 感觉普通的 flow matching 可以看成 degrade-aware image de...
- [岁月如书](https://kexue.fm/archives/11126/comment-page-3#comment-29028): 受教了，感谢
- [苏剑林](https://kexue.fm/archives/11126/comment-page-3#comment-29027): 是
- [岁月如书](https://kexue.fm/archives/11126/comment-page-3#comment-29026): 哦哦，原来是有实验结论，那是我盲目了。多问一句，你说的attention + output g...

### 友情链接

- [Cool Papers](https://papers.cool)
- [数学研发](https://bbs.emath.ac.cn)
- [Seatop](http://www.seatop.com.cn/)
- [Xiaoxia](https://xiaoxia.org/)
- [积分表-网络版](https://kexue.fm/sci/integral/index.html)
- [丝路博傲](http://blog.dvxj.com/)
- [数学之家](http://www.2math.cn/)
- [有趣天文奇观](http://interesting-sky.china-vo.org/)
- [TwistedW](http://www.twistedwg.com/)
- [godweiyang](https://godweiyang.com/)
- [AI柠檬](https://blog.ailemon.net/)
- [王登科-DK博客](https://greatdk.com)
- [ESON](https://blog.eson.org/)
- [枫之羽](https://fzhiy.net/)
- [coding-zuo](https://coding-zuo.github.io/)
- [博科园](https://www.bokeyuan.net/)
- [孔皮皮的博客](https://www.kppkkp.top/)
- [运鹏的博客](https://yunpengtai.top/)
- [jiming.site](https://jiming.site/)
- [OmegaXYZ](https://www.omegaxyz.com/)
- [EAI猩球](https://www.robotech.ink/)
- [文举的博客](https://liwenju0.com/)
- [申请链接](https://kexue.fm/links.html)

本站采用创作共用版权协议，要求署名、非商业用途和保持一致。转载本站内容必须也遵循“ [署名-非商业用途-保持一致](http://creativecommons.org/licenses/by-nc-nd/2.5/cn/)”的创作共用协议。
© 2009-2025 Scientific Spaces. All rights reserved. Theme by [laogui](http://www.laogui.com). Powered by [Typecho](http://typecho.org). 备案号: [粤ICP备09093259号-1/2](https://beian.miit.gov.cn/)。