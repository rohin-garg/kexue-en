## SEARCH

## MENU

- [打赏](https://kexue.fm/reward.html)
- [公式](https://kexue.fm/latex.html)
- [天象](https://kexue.fm/ac.html)
- [链接](https://kexue.fm/links.html)
- [时光](https://kexue.fm/me.html)
- [博览](https://kexue.fm/science.html)
- [归档](https://kexue.fm/content.html)

## CATEGORIES

- [千奇百怪](https://kexue.fm/category/Everything)
- [天文探索](https://kexue.fm/category/Astronomy)
- [数学研究](https://kexue.fm/category/Mathematics)
- [物理化学](https://kexue.fm/category/Phy-chem)
- [信息时代](https://kexue.fm/category/Big-Data)
- [生物自然](https://kexue.fm/category/Biology)
- [图片摄影](https://kexue.fm/category/Photograph)
- [问题百科](https://kexue.fm/category/Questions)
- [生活/情感](https://kexue.fm/category/Life-Feeling)
- [资源共享](https://kexue.fm/category/Resources)

## NEWPOSTS

- [通过msign来计算奇异值裁剪mc...](https://kexue.fm/archives/11059)
- [矩阵符号函数mcsgn能计算什么？](https://kexue.fm/archives/11056)
- [线性注意力简史：从模仿、创新到反哺](https://kexue.fm/archives/11033)
- [msign的导数](https://kexue.fm/archives/11025)
- [通过msign来计算奇异值裁剪mc...](https://kexue.fm/archives/11006)
- [msign算子的Newton-Sc...](https://kexue.fm/archives/10996)
- [等值振荡定理：最优多项式逼近的充要条件](https://kexue.fm/archives/10972)
- [生成扩散模型漫谈（三十）：从瞬时速...](https://kexue.fm/archives/10958)
- [MoE环游记：5、均匀分布的反思](https://kexue.fm/archives/10945)
- [msign算子的Newton-Sc...](https://kexue.fm/archives/10922)

## COMMENTS

- [Leco: 请问LoRA的A,B矩阵初始化时，一个高斯随机一个全零还是只能...](https://kexue.fm/archives/9590/comment-page-2#comment-27984)
- [苏剑林: 如果你把你这里提到的数学都学通透了，数学基础基本上可以胜任95...](https://kexue.fm/archives/9119/comment-page-13#comment-27983)
- [苏剑林: 我跑过这个项目，效果是能复现的。“在 CIFAR-10 上效果...](https://kexue.fm/archives/10958/comment-page-2#comment-27982)
- [Henry Zha: 苏神你好，我是一名管理科学与工程专业的博士生，研究方向是结合人...](https://kexue.fm/archives/9119/comment-page-13#comment-27981)
- [SunlightZero: 我根据 https://github.com/haidog-y...](https://kexue.fm/archives/10958/comment-page-2#comment-27980)
- [苏剑林: 噢，是笔误，更正了，感谢指出。](https://kexue.fm/archives/11025/comment-page-1#comment-27979)
- [苏剑林: 这里有很多因素。如果推理数据跟训练数据同分布，那么理论上就是均...](https://kexue.fm/archives/10945/comment-page-1#comment-27978)
- [苏剑林: 目前看来给O加rmsnorm挺稳的，效果甚至还好点。我其实是直...](https://kexue.fm/archives/11033/comment-page-1#comment-27977)
- [苏剑林: 你应该说的是$\\exp(\\boldsymbol{Q}\\bold...](https://kexue.fm/archives/11033/comment-page-1#comment-27976)
- [苏剑林: 可以这么说吧，通过某种分母归一化的操作，导数格式都类似，毕竟公...](https://kexue.fm/archives/10831/comment-page-1#comment-27975)

## USERLOGIN

- [登录](https://kexue.fm/admin/login.php)

[科学空间\|Scientific Spaces](https://kexue.fm)

- [登录](https://kexue.fm/admin/login.php)
- [打赏](https://kexue.fm/reward.html)
- [公式](https://kexue.fm/latex.html)
- [天象](https://kexue.fm/ac.html)
- [链接](https://kexue.fm/links.html)
- [时光](https://kexue.fm/me.html)
- [博览](https://kexue.fm/science.html)
- [归档](https://kexue.fm/content.html)

渴望成为一个小飞侠

- [欢迎订阅](https://kexue.fm/feed)
- [个性邮箱](https://kexue.fm/archives/119)
- [天象信息](https://kexue.fm/ac.html)
- [观测ISS](https://kexue.fm/archives/41)
- [LaTeX](https://kexue.fm/latex.html)
- [关于博主](https://kexue.fm/me.html)

欢迎访问“科学空间”，这里将与您共同探讨自然科学，回味人生百态；也期待大家的分享～

- [**千奇百怪** Everything](https://kexue.fm/category/Everything)
- [**天文探索** Astronomy](https://kexue.fm/category/Astronomy)
- [**数学研究** Mathematics](https://kexue.fm/category/Mathematics)
- [**物理化学** Phy-chem](https://kexue.fm/category/Phy-chem)
- [**信息时代** Big-Data](https://kexue.fm/category/Big-Data)
- [**生物自然** Biology](https://kexue.fm/category/Biology)
- [**图片摄影** Photograph](https://kexue.fm/category/Photograph)
- [**问题百科** Questions](https://kexue.fm/category/Questions)
- [**生活/情感** Life-Feeling](https://kexue.fm/category/Life-Feeling)
- [**资源共享** Resources](https://kexue.fm/category/Resources)

- [**千奇百怪**](https://kexue.fm/category/Everything)
- [**天文探索**](https://kexue.fm/category/Astronomy)
- [**数学研究**](https://kexue.fm/category/Mathematics)
- [**物理化学**](https://kexue.fm/category/Phy-chem)
- [**信息时代**](https://kexue.fm/category/Big-Data)
- [**生物自然**](https://kexue.fm/category/Biology)
- [**图片摄影**](https://kexue.fm/category/Photograph)
- [**问题百科**](https://kexue.fm/category/Questions)
- [**生活/情感**](https://kexue.fm/category/Life-Feeling)
- [**资源共享**](https://kexue.fm/category/Resources)

[首页](https://kexue.fm) [信息时代](https://kexue.fm/category/Big-Data) 生成扩散模型漫谈（三十）：从瞬时速度到平均速度

26May

# [生成扩散模型漫谈（三十）：从瞬时速度到平均速度](https://kexue.fm/archives/10958)

By 苏剑林 \|
2025-05-26 \|
21336位读者\|

众所周知，生成速度慢是扩散模型一直以来的痛点，而为了解决这个问题，大家可谓“八仙过海，各显神通”，提出了各式各样的解决方案，然而长久以来并没一项工作能够脱颖而出，成为标配。什么样的工作能够达到这个标准呢？在笔者看来，它至少满足几个条件：

> 1、数学原理清晰，能够揭示出快速生成的本质所在；
>
> 2、能够单目标从零训练，不需要对抗、蒸馏等额外手段；
>
> 3、单步生成接近SOTA，可以通过增加步数提升效果。

根据笔者的阅读经历，几乎没有一项工作能同时满足这三个标准。然而，就在几天前，arXiv出了一篇 [《Mean Flows for One-step Generative Modeling》](https://papers.cool/arxiv/2505.13447)（简称“MeanFlow”），看上去非常有潜力。接下来，我们将以此为契机，讨论一下相关思路和进展。

## 现有思路 [\#](https://kexue.fm/archives/10958\#%E7%8E%B0%E6%9C%89%E6%80%9D%E8%B7%AF)

扩散模型的生成加速工作已经有非常多，本博客前面也简单介绍过一些。总的来说，加速思路大体上可以分为三类。

第一，将扩散模型转化为SDE/ODE，然后研究更高效的求解器，代表作是 [DPM-Solver](https://papers.cool/arxiv/2206.00927) 及其一系列后续改进。然而，这个思路通常只能将生成的NFE（Number of Function Evaluations）降到10左右，再低就会明显降低生成质量。这是因为求解器的收敛速度通常都是正比于步长的若干次方，当NFE很小时步长就无法很小，所以收敛不够快以至于没法用。

第二，通过蒸馏将训练好的扩散模型转化为更少步数的生成器，由此衍生出来的工作和方案也非常多，我们此前介绍过其中的一种名为 [SiD](https://kexue.fm/archives/10085) 的方案。蒸馏算是比较常规和通用的思路，但缺点也是共同的，即需要额外的训练成本，并非从零训练的方案。有些工作为了蒸馏到单步生成器，还加上了对抗训练等多重优化策略，整个方案往往过于复杂。

第二，基于一致性模型（Consistency Model，CM），包括我们在 [《生成扩散模型漫谈（二十八）：分步理解一致性模型》](https://kexue.fm/archives/10633) 简单介绍的CM、它的连续版本 [sCM](https://papers.cool/arxiv/2410.11081) 以及 [CTM](https://papers.cool/arxiv/2310.02279) 等。CM是自成一派的思路，可以从零训练得到NFE很小的模型，也可以用于蒸馏，但CM的目标依赖于EMA或者stop\_gradient运算，意味着它耦合了优化器动力学，这通常给人一种说不清道不明的感觉。

## 瞬时速度 [\#](https://kexue.fm/archives/10958\#%E7%9E%AC%E6%97%B6%E9%80%9F%E5%BA%A6)

到目前为止，生成NFE最小的扩散模型，基本上都是ODE，因为确定性模型往往更容易分析和求解。本文同样只关注ODE式扩散，所用框架是 [《生成扩散模型漫谈（十七）：构建ODE的一般步骤（下）》](https://kexue.fm/archives/9497) 介绍的ReFlow，它跟 [Flow Matching](https://papers.cool/arxiv/2210.02747) 本质是相通的，但更加直观。

ODE式扩散，是希望学习一个ODE
\\begin{equation}\\frac{d\\boldsymbol{x}\_t}{dt} = \\boldsymbol{v}\_{\\boldsymbol{\\theta}}(\\boldsymbol{x}\_t,t)\\label{eq:ode}\\end{equation}
来构建一个$\\boldsymbol{x}\_1\\to \\boldsymbol{x}\_0$的变换。具体来说，设$\\boldsymbol{x}\_1\\sim p\_1(\\boldsymbol{x}\_1)$是某个容易采样的随机噪声，$\\boldsymbol{x}\_0\\sim p\_0(\\boldsymbol{x}\_0)$则是目标分布的真实样本，我们希望能够通过上述ODE，实现随机噪声到目标样本的变换，即随机采样一个$\\boldsymbol{x}\_1\\sim p\_1(\\boldsymbol{x}\_1)$作为初值，求解上述ODE得到的$\\boldsymbol{x}\_0$就是$p\_0(\\boldsymbol{x}\_0)$的样本。

如果将$t$看成时间，$\\boldsymbol{x}\_t$看成位移，那么$d\\boldsymbol{x}\_t/dt$就是瞬时速度，所以ODE式扩散就是瞬时速度的建模。那怎么训练$\\boldsymbol{v}\_{\\boldsymbol{\\theta}}(\\boldsymbol{x}\_t,t)$呢？ReFlow提出了一种非常直观的方法：首先构建$\\boldsymbol{x}\_0$与$\\boldsymbol{x}\_1$的任意插值方式，如最简单的线性插值$\\boldsymbol{x}\_t=(1-t)\\boldsymbol{x}\_0 + t \\boldsymbol{x}\_1$，那么对$t$求导得
\\begin{equation}\\frac{d\\boldsymbol{x}\_t}{dt} = \\boldsymbol{x}\_1 - \\boldsymbol{x}\_0\\end{equation}
这是个极简单的ODE，但不符合我们的要求，因为$\\boldsymbol{x}\_0$是我们的目标，它不应该出现在ODE中。对此，ReFlow提出一个非常符合直觉的想法——用$\\boldsymbol{v}\_{\\boldsymbol{\\theta}}(\\boldsymbol{x}\_t,t)$去逼近$\\boldsymbol{x}\_1 - \\boldsymbol{x}\_0$：
\\begin{equation}\\mathbb{E}\_{t,\\boldsymbol{x}\_0,\\boldsymbol{x}\_1}\\left\[\\Vert\\boldsymbol{v}\_{\\boldsymbol{\\theta}}(\\boldsymbol{x}\_t,t) - (\\boldsymbol{x}\_1 - \\boldsymbol{x}\_0)\\Vert^2\\right\]\\label{eq:loss-reflow}\\end{equation}
这就是ReFlow的目标函数。值得指出的是：1）ReFlow理论上允许$\\boldsymbol{x}\_0$与$\\boldsymbol{x}\_1$的任意插值方式；2）ReFlow虽然直观，但理论上也是严格的，可以证明它的最优解确实是我们所求的ODE。相关细节大家请参考 [《生成扩散模型漫谈（十七）：构建ODE的一般步骤（下）》](https://kexue.fm/archives/9497) 以及原论文。

## 平均速度 [\#](https://kexue.fm/archives/10958\#%E5%B9%B3%E5%9D%87%E9%80%9F%E5%BA%A6)

然而，ODE仅仅是一个纯数学形式，实际求解还是需要离散化，比如最简单的欧拉格式：
\\begin{equation}\\boldsymbol{x}\_{t - \\Delta t} = \\boldsymbol{x}\_t - \\boldsymbol{v}\_{\\boldsymbol{\\theta}}(\\boldsymbol{x}\_t,t) \\Delta t\\end{equation}
从$1$到$0$的NFE是$1/\\Delta t$，想要NFE小等价于$\\Delta t$大。然而，ReFlow的理论基础是精确的ODE，即精确求解ODE时才能实现目标样本的生成，这意味着$\\Delta t$越小越好，跟我们的期望相背。尽管ReFlow声称使用直线插值可以让ODE的轨迹变得更直，从而允许更大的$\\Delta t$，但实际轨迹终究是弯曲的，$\\Delta t$很难接近1，所以ReFlow很难实现一步生成。

归根结底，ODE本来就是$\\Delta t\\to 0$的东西，我们非要将它用于$\\Delta t \\to 1$，还要求它效果好，这本身就是“强模型所难”了。所以说，更换建模目标，而不是继续“为难”模型，才是实现更快生成的本质思路。为此，我们考虑对式$\\eqref{eq:ode}$两端进行积分
\\begin{equation}\\boldsymbol{x}\_t - \\boldsymbol{x}\_r = \\int\_r^t \\boldsymbol{v}\_{\\boldsymbol{\\theta}}(\\boldsymbol{x}\_{\\tau},\\tau) d\\tau = (t-r)\\times \\frac{1}{t-r}\\int\_r^t \\boldsymbol{v}\_{\\boldsymbol{\\theta}}(\\boldsymbol{x}\_{\\tau},\\tau) d\\tau\\end{equation}
如果我们可以建模
\\begin{equation} \\boldsymbol{u}\_{\\boldsymbol{\\theta}}(\\boldsymbol{x}\_t, r, t) \\triangleq \\frac{1}{t-r}\\int\_r^t \\boldsymbol{v}\_{\\boldsymbol{\\theta}}(\\boldsymbol{x}\_{\\tau},\\tau) d\\tau\\end{equation}
那么就有$\\boldsymbol{x}\_0 = \\boldsymbol{x}\_1 - \\boldsymbol{u}\_{\\boldsymbol{\\theta}}(\\boldsymbol{x}\_1, 0, 1)$，即理论上可以精准地实现一步生成，而不必求诸于近似关系。如果说$\\boldsymbol{v}\_{\\boldsymbol{\\theta}}(\\boldsymbol{x}\_t,t)$是$t$时刻的瞬时速度，那么很显然$\\boldsymbol{u}\_{\\boldsymbol{\\theta}}(\\boldsymbol{x}\_t, r, t)$是$\[r,t\]$时间段内的平均速度。也就是说，为了加速生成甚至一步生成，我们的建模目标应该是平均速度，而不是ODE的瞬时速度。

## 恒等变换 [\#](https://kexue.fm/archives/10958\#%E6%81%92%E7%AD%89%E5%8F%98%E6%8D%A2)

当然，从瞬时速度到平均速度的转变并不难想，真正难的地方是如何给它构建损失函数。ReFlow只告诉我们如何给瞬时速度构建损失函数，对平均速度的训练我们是一无所知。

接下来很自然的想法是“化未知为已知”，即以平均速度$\\boldsymbol{u}\_{\\boldsymbol{\\theta}}(\\boldsymbol{x}\_t, r, t)$来为出发点来构建瞬时速度$\\boldsymbol{v}\_{\\boldsymbol{\\theta}}(\\boldsymbol{x}\_t,t)$，然后代入ReFlow的目标函数，这需要我们去推导两者之间的恒等变换。从$\\boldsymbol{u}\_{\\boldsymbol{\\theta}}(\\boldsymbol{x}\_t, r, t)$的定义我们得到
\\begin{equation} \\int\_r^t \\boldsymbol{v}\_{\\boldsymbol{\\theta}}(\\boldsymbol{x}\_{\\tau},\\tau) d\\tau = (t-r)\\boldsymbol{u}\_{\\boldsymbol{\\theta}}(\\boldsymbol{x}\_t, r, t) \\end{equation}
两边对$t$求导，得到
\\begin{equation}\\begin{aligned}
\\boldsymbol{v}\_{\\boldsymbol{\\theta}}(\\boldsymbol{x}\_t,t) =&\\, \\boldsymbol{u}\_{\\boldsymbol{\\theta}}(\\boldsymbol{x}\_t, r, t) + (t-r)\\frac{d}{dt}\\boldsymbol{u}\_{\\boldsymbol{\\theta}}(\\boldsymbol{x}\_t, r, t) \\\
=&\\, \\boldsymbol{u}\_{\\boldsymbol{\\theta}}(\\boldsymbol{x}\_t, r, t) + (t-r)\\left\[\\frac{d\\boldsymbol{x}\_t}{dt}\\cdot\\frac{\\partial}{\\partial \\boldsymbol{x}\_t}\\boldsymbol{u}\_{\\boldsymbol{\\theta}}(\\boldsymbol{x}\_t, r, t) + \\frac{\\partial}{\\partial t}\\boldsymbol{u}\_{\\boldsymbol{\\theta}}(\\boldsymbol{x}\_t, r, t)\\right\]
\\end{aligned}\\label{eq:id1}\\end{equation}
这便是$\\boldsymbol{v}\_{\\boldsymbol{\\theta}}(\\boldsymbol{x}\_t,t)$跟$\\boldsymbol{u}\_{\\boldsymbol{\\theta}}(\\boldsymbol{x}\_t, r, t)$的第一个恒等关系。有第一自然就有第二，第二个恒等关系由平均速度的定义得到：
\\begin{equation}\\boldsymbol{v}\_{\\boldsymbol{\\theta}}(\\boldsymbol{x}\_t,t) = \\lim\_{r\\to t}\\frac{1}{t-r}\\int\_r^t \\boldsymbol{v}\_{\\boldsymbol{\\theta}}(\\boldsymbol{x}\_{\\tau},\\tau) d\\tau = \\boldsymbol{u}\_{\\boldsymbol{\\theta}}(\\boldsymbol{x}\_t, t, t)\\label{eq:id2}\\end{equation}
说白了，无限小区间内的平均速度，就等于瞬时速度。

## 第一目标 [\#](https://kexue.fm/archives/10958\#%E7%AC%AC%E4%B8%80%E7%9B%AE%E6%A0%87)

根据$d\\boldsymbol{x}\_t/dt = \\boldsymbol{v}\_{\\boldsymbol{\\theta}}(\\boldsymbol{x}\_t,t)$以及恒等式$\\eqref{eq:id2}$，我们可以将恒等式$\\eqref{eq:id1}$的$d\\boldsymbol{x}\_t/dt$换成$\\boldsymbol{v}\_{\\boldsymbol{\\theta}}(\\boldsymbol{x}\_t,t)$或者$\\boldsymbol{u}\_{\\boldsymbol{\\theta}}(\\boldsymbol{x}\_t, t, t)$，前者是隐式关系，我们后面再谈，我们先看后者，此时有：
\\begin{equation}\\boldsymbol{v}\_{\\boldsymbol{\\theta}}(\\boldsymbol{x}\_t,t) = \\boldsymbol{u}\_{\\boldsymbol{\\theta}}(\\boldsymbol{x}\_t, r, t) + (t-r)\\left\[\\boldsymbol{u}\_{\\boldsymbol{\\theta}}(\\boldsymbol{x}\_t, t, t)\\cdot\\frac{\\partial}{\\partial \\boldsymbol{x}\_t}\\boldsymbol{u}\_{\\boldsymbol{\\theta}}(\\boldsymbol{x}\_t, r, t) + \\frac{\\partial}{\\partial t}\\boldsymbol{u}\_{\\boldsymbol{\\theta}}(\\boldsymbol{x}\_t, r, t)\\right\]\\end{equation}
代入ReFlow，我们得到可以用来训练$\\boldsymbol{u}\_{\\boldsymbol{\\theta}}(\\boldsymbol{x}\_t, r, t)$的第一个目标函数：
\\begin{equation}\\mathbb{E}\_{r,t,\\boldsymbol{x}\_0,\\boldsymbol{x}\_1}\\left\[\\left\\Vert\\boldsymbol{u}\_{\\boldsymbol{\\theta}}(\\boldsymbol{x}\_t, r, t) + (t-r)\\left\[\\boldsymbol{u}\_{\\boldsymbol{\\theta}}(\\boldsymbol{x}\_t, t, t)\\cdot\\frac{\\partial}{\\partial \\boldsymbol{x}\_t}\\boldsymbol{u}\_{\\boldsymbol{\\theta}}(\\boldsymbol{x}\_t, r, t) + \\frac{\\partial}{\\partial t}\\boldsymbol{u}\_{\\boldsymbol{\\theta}}(\\boldsymbol{x}\_t, r, t)\\right\] - (\\boldsymbol{x}\_1 - \\boldsymbol{x}\_0)\\right\\Vert^2\\right\]\\label{eq:loss-1}\\end{equation}
这是一个非常理想的结果，它满足我们对生成模型目标函数的所有期望：

> 1、单个显式的最小化目标；
>
> 2、没有EMA、stop\_gradient等运算；
>
> 3、理论上有保证（ReFlow）。

这些特性意味着，不管我们用什么优化算法，只要我们能找到上式的最小值点，那么它就是我们想要的平均速度模型，即理论上能够实现一步生成的生成模型。换句话说，它具备了扩散模型的训练简单和理论保证，又能像GAN那样一步生成，还不用求神拜佛保佑模型别“想不开”而训崩。

## JVP运算 [\#](https://kexue.fm/archives/10958\#JVP%E8%BF%90%E7%AE%97)

不过，对于部分读者来说，目标函数$\\eqref{eq:loss-1}$的实现还是有点困难的，因为它涉及到普通用户比较少见的“雅可比向量积（Jacobian-Vector Product，JVP）”。具体来说，我们可以将目标函数内方括号部分写成：
\\begin{equation}\\underbrace{\\left\[\\boldsymbol{u}\_{\\boldsymbol{\\theta}}(\\boldsymbol{x}\_t, t, t),0,1\\right\] \\\\[10pt\]}\_{\\text{向量}}\\cdot\\underbrace{\\left\[\\frac{\\partial}{\\partial \\boldsymbol{x}\_t}\\boldsymbol{u}\_{\\boldsymbol{\\theta}}(\\boldsymbol{x}\_t, r, t), \\frac{\\partial}{\\partial r}\\boldsymbol{u}\_{\\boldsymbol{\\theta}}(\\boldsymbol{x}\_t, r, t), \\frac{\\partial}{\\partial t}\\boldsymbol{u}\_{\\boldsymbol{\\theta}}(\\boldsymbol{x}\_t, r, t)\\right\] \\\\[10pt\]}\_{\\text{雅可比矩阵}}\\end{equation}
即$\\boldsymbol{u}\_{\\boldsymbol{\\theta}}(\\boldsymbol{x}\_t, r, t)$的雅可比矩阵与给定向量$\[\\boldsymbol{u}\_{\\boldsymbol{\\theta}}(\\boldsymbol{x}\_t, t, t),0,1\]$的乘法，结果是一个跟$\\boldsymbol{u}\_{\\boldsymbol{\\theta}}(\\boldsymbol{x}\_t, r, t)$大小一致的向量，这种运算就叫做JVP，在Jax、Torch里边都有现成实现，比如Jax的参考代码是：

```
u = lambda xt, r, t: diffusion_model(weights, [xt, r, t])
urt, durt = jax.jvp(u, (xt, r, t), (u(xt, t, t), r * 0, t * 0 + 1))
```

其中 `urt` 就是$\\boldsymbol{u}\_{\\boldsymbol{\\theta}}(\\boldsymbol{x}\_t, r, t)$，而 `durt` 就是对应的JVP结果，Torch的用法也类似。了解JVP运算后，目标函数$\\eqref{eq:loss-1}$的实现就基本上没有难度了。

## 第二目标 [\#](https://kexue.fm/archives/10958\#%E7%AC%AC%E4%BA%8C%E7%9B%AE%E6%A0%87)

如果要说目标函数$\\eqref{eq:loss-1}$的缺点，在笔者看来只有一个，那就是计算量相对偏大。这是因为它要进行两次不同的前向传播$\\boldsymbol{u}\_{\\boldsymbol{\\theta}}(\\boldsymbol{x}\_t, r, t)$和$\\boldsymbol{u}\_{\\boldsymbol{\\theta}}(\\boldsymbol{x}\_t, t, t)$，然后JVP求了一次梯度，用基于梯度下降优化时还要再求一次梯度，所以它本质上要求二阶梯度，跟以往的 [WGAN-GP](https://kexue.fm/archives/4439) 类似。

为了降低计算量，我们可以考虑给JVP部分加上stop\_gradient运算（$\\newcommand{\\sg}\[1\]{\\color{skyblue}{\\mathop{\\text{sg}}\\left\[\\color{blue}{#1}\\right\]}}\\sg{\\cdot}$）：
\\begin{equation}\\mathbb{E}\_{r,t,\\boldsymbol{x}\_0,\\boldsymbol{x}\_1}\\left\[\\left\\Vert\\boldsymbol{u}\_{\\boldsymbol{\\theta}}(\\boldsymbol{x}\_t, r, t) + (t-r)\\sg{\\boldsymbol{u}\_{\\boldsymbol{\\theta}}(\\boldsymbol{x}\_t, t, t)\\cdot\\frac{\\partial}{\\partial \\boldsymbol{x}\_t}\\boldsymbol{u}\_{\\boldsymbol{\\theta}}(\\boldsymbol{x}\_t, r, t) + \\frac{\\partial}{\\partial t}\\boldsymbol{u}\_{\\boldsymbol{\\theta}}(\\boldsymbol{x}\_t, r, t)} - (\\boldsymbol{x}\_1 - \\boldsymbol{x}\_0)\\right\\Vert^2\\right\]\\label{eq:loss-2}\\end{equation}
这样就避免了对JVP再次求梯度（但依然需要两次前向传播）。实测结果显示，相比第一目标$\\eqref{eq:loss-1}$，上述目标在梯度优化器下训练速度能够快将近一倍，并且效果目测无损。

注意，这里的stop\_gradient单纯是出于减少计算量的目的，实际优化方向依然是损失函数值越小越好，这跟CM系列模型尤其是sCM是不一样的，它们的损失函数只是具有等效梯度的等效损失，并不一定是越小越好，它们的stop\_gradient往往是必须的，一旦去掉几乎可以肯定会训练崩溃。

## 第三目标 [\#](https://kexue.fm/archives/10958\#%E7%AC%AC%E4%B8%89%E7%9B%AE%E6%A0%87)

前面我们提到，处理恒等式$\\eqref{eq:id1}$中的$d\\boldsymbol{x}\_t/dt$的另一个方案是将其换成$\\boldsymbol{v}\_{\\boldsymbol{\\theta}}(\\boldsymbol{x}\_t,t)$，这将导致
\\begin{equation}\\boldsymbol{v}\_{\\boldsymbol{\\theta}}(\\boldsymbol{x}\_t,t) = \\boldsymbol{u}\_{\\boldsymbol{\\theta}}(\\boldsymbol{x}\_t, r, t) + (t-r)\\left\[\\boldsymbol{v}\_{\\boldsymbol{\\theta}}(\\boldsymbol{x}\_t,t)\\cdot\\frac{\\partial}{\\partial \\boldsymbol{x}\_t}\\boldsymbol{u}\_{\\boldsymbol{\\theta}}(\\boldsymbol{x}\_t, r, t) + \\frac{\\partial}{\\partial t}\\boldsymbol{u}\_{\\boldsymbol{\\theta}}(\\boldsymbol{x}\_t, r, t)\\right\]\\end{equation}
如果要从中解出$\\boldsymbol{v}\_{\\boldsymbol{\\theta}}(\\boldsymbol{x}\_t,t)$，结果将是
\\begin{equation}\\boldsymbol{v}\_{\\boldsymbol{\\theta}}(\\boldsymbol{x}\_t,t) = \\left\[\\boldsymbol{u}\_{\\boldsymbol{\\theta}}(\\boldsymbol{x}\_t, r, t) + (t-r)\\frac{\\partial}{\\partial t}\\boldsymbol{u}\_{\\boldsymbol{\\theta}}(\\boldsymbol{x}\_t, r, t)\\right\]\\cdot\\left\[\\boldsymbol{I} - (t-r)\\frac{\\partial}{\\partial \\boldsymbol{x}\_t}\\boldsymbol{u}\_{\\boldsymbol{\\theta}}(\\boldsymbol{x}\_t, r, t)\\right\]^{-1}\\end{equation}
这涉及到了非常庞大的矩阵求逆，因此并不现实。MeanFlow给出了一个折中方案：既然$d\\boldsymbol{x}\_t/dt = \\boldsymbol{v}\_{\\boldsymbol{\\theta}}(\\boldsymbol{x}\_t,t)$的回归目标是$\\boldsymbol{x}\_1-\\boldsymbol{x}\_0$，那干脆把$d\\boldsymbol{x}\_t/dt$换成$\\boldsymbol{x}\_1-\\boldsymbol{x}\_0$好了，于是目标函数变成
\\begin{equation}\\mathbb{E}\_{r,t,\\boldsymbol{x}\_0,\\boldsymbol{x}\_1}\\left\[\\left\\Vert\\boldsymbol{u}\_{\\boldsymbol{\\theta}}(\\boldsymbol{x}\_t, r, t) + (t-r)\\left\[(\\boldsymbol{x}\_1-\\boldsymbol{x}\_0)\\cdot\\frac{\\partial}{\\partial \\boldsymbol{x}\_t}\\boldsymbol{u}\_{\\boldsymbol{\\theta}}(\\boldsymbol{x}\_t, r, t) + \\frac{\\partial}{\\partial t}\\boldsymbol{u}\_{\\boldsymbol{\\theta}}(\\boldsymbol{x}\_t, r, t)\\right\] - (\\boldsymbol{x}\_1 - \\boldsymbol{x}\_0)\\right\\Vert^2\\right\]\\end{equation}
然而，此时的$\\boldsymbol{x}\_1-\\boldsymbol{x}\_0$既是回归目标，又出现在模型$\\boldsymbol{v}\_{\\boldsymbol{\\theta}}(\\boldsymbol{x}\_t,t)$的定义中，难免会有一种“标签泄漏”的感觉。为了避免这个问题，MeanFlow采取的办法同样是给JVP部分加上stop\_gradient：
\\begin{equation}\\mathbb{E}\_{r,t,\\boldsymbol{x}\_0,\\boldsymbol{x}\_1}\\left\[\\left\\Vert\\boldsymbol{u}\_{\\boldsymbol{\\theta}}(\\boldsymbol{x}\_t, r, t) + (t-r)\\sg{(\\boldsymbol{x}\_1-\\boldsymbol{x}\_0)\\cdot\\frac{\\partial}{\\partial \\boldsymbol{x}\_t}\\boldsymbol{u}\_{\\boldsymbol{\\theta}}(\\boldsymbol{x}\_t, r, t) + \\frac{\\partial}{\\partial t}\\boldsymbol{u}\_{\\boldsymbol{\\theta}}(\\boldsymbol{x}\_t, r, t)} - (\\boldsymbol{x}\_1 - \\boldsymbol{x}\_0)\\right\\Vert^2\\right\]\\label{eq:loss-3}\\end{equation}
这就是MeanFlow最终所用的损失函数，这里我们称之为“第三目标”。相比第二目标$\\eqref{eq:loss-2}$，它少了一次前向传播$\\boldsymbol{u}\_{\\boldsymbol{\\theta}}(\\boldsymbol{x}\_t, t, t)$，所以训练速度会更快一些。但此时“标签泄漏”的引入和stop\_gradient的对策，使得第三目标的训练跟梯度优化器是耦合的，这就跟CM一样，多了一些说不清道不明的神秘感。

论文实验结果表明，加上$\\sg{\\cdot}$的目标$\\eqref{eq:loss-3}$是能训出合理结果的，那如果去掉它呢？笔者向作者请教过，他表明去掉$\\sg{\\cdot}$后，训练依然能收敛，能多步生成，但没有一步生成能力了。其实这也不难理解，因为$r=t$时不管有没有$\\sg{\\cdot}$，目标函数都退化为ReFlow：
\\begin{equation}\\mathbb{E}\_{t,\\boldsymbol{x}\_0,\\boldsymbol{x}\_1}\\left\[\\Vert\\boldsymbol{u}\_{\\boldsymbol{\\theta}}(\\boldsymbol{x}\_t, t, t) - (\\boldsymbol{x}\_1 - \\boldsymbol{x}\_0)\\Vert^2\\right\]\\label{eq:loss-reflow-2}\\end{equation}
也就是说MeanFlow总有ReFlow在背后“兜底”，因此怎样也不至于太差。而去掉$\\sg{\\cdot}$后，“标签泄漏”的负面影响加剧，因此就不如加上它了。

## 证明一下 [\#](https://kexue.fm/archives/10958\#%E8%AF%81%E6%98%8E%E4%B8%80%E4%B8%8B)

我们能否像ReFlow一样，从理论上证明第三目标$\\eqref{eq:loss-3}$的最优解确实是我们期望的平均速度模型呢？让我们尝试一下。首先我们回顾证明ReFlow的两个关键引理：

> 1、$\\mathop{\\text{argmin}}\_{\\boldsymbol{\\mu}}\\mathbb{E}\[\\Vert\\boldsymbol{\\mu} - \\boldsymbol{x}\\Vert^2\] = \\mathbb{E}\[\\boldsymbol{x}\]$，即最小化$\\boldsymbol{\\mu}$与$\\boldsymbol{x}$的平方误差，最优解是$\\boldsymbol{x}$的均值；
>
> 2、按照分布轨迹$\\boldsymbol{x}\_t=(1-t)\\boldsymbol{x}\_0 + t \\boldsymbol{x}\_1$将$\\boldsymbol{x}\_1$变到$\\boldsymbol{x}\_0$的ODE形式解是$d\\boldsymbol{x}\_t/dt = \\mathbb{E}\_{\\boldsymbol{x}\_0\|\\boldsymbol{x}\_t}\[\\boldsymbol{x}\_1-\\boldsymbol{x}\_0\]$。

其中，引理1的证明比较简单，直接对$\\boldsymbol{\\mu}$求梯度得$\\mathbb{E}\[\\boldsymbol{\\mu} - \\boldsymbol{x}\] = \\boldsymbol{\\mu} - \\mathbb{E}\[\\boldsymbol{x}\]$，令它等于零即可；引理2的证明细节则需要看 [《生成扩散模型漫谈（十七）：构建ODE的一般步骤（下）》](https://kexue.fm/archives/9497)，其中$\\mathbb{E}\_{\\boldsymbol{x}\_0\|\\boldsymbol{x}\_t}\[\\boldsymbol{x}\_1-\\boldsymbol{x}\_0\]$是需要先利用$\\boldsymbol{x}\_t=(1-t)\\boldsymbol{x}\_0 + t \\boldsymbol{x}\_1$消去$\\boldsymbol{x}\_1$，得到一个$\\boldsymbol{x}\_0,\\boldsymbol{x}\_t$的函数，然后对分布$p\_t(\\boldsymbol{x}\_0\|\\boldsymbol{x}\_t)$求期望，结果是关于$t,\\boldsymbol{x}\_t$的函数。

利用引理1，我们可以证明ReFlow的目标函数$\\eqref{eq:loss-reflow}$的理论最优解就是$\\boldsymbol{v}\_{\\boldsymbol{\\theta}^\*}(\\boldsymbol{x}\_t,t) = \\mathbb{E}\_{\\boldsymbol{x}\_0\|\\boldsymbol{x}\_t}\[\\boldsymbol{x}\_1-\\boldsymbol{x}\_0\]$，结合引理2就得到$d\\boldsymbol{x}\_t/dt=\\boldsymbol{v}\_{\\boldsymbol{\\theta}^\*}(\\boldsymbol{x}\_t,t)$是我们所求的ODE。第三目标$\\eqref{eq:loss-3}$的证明类似，由于里边有$\\sg{\\cdot}$，对$\\boldsymbol{u}\_{\\boldsymbol{\\theta}}(\\boldsymbol{x}\_t, r, t)$求梯度并让它等于零的结果是
\\begin{equation}\\begin{aligned}
\\boldsymbol{0} =&\\, \\boldsymbol{u}\_{\\boldsymbol{\\theta}^\*}(\\boldsymbol{x}\_t, r, t) + \\mathbb{E}\_{\\boldsymbol{x}\_0\|\\boldsymbol{x}\_t}\\left\[(t-r)\\left\[(\\boldsymbol{x}\_1-\\boldsymbol{x}\_0)\\cdot\\frac{\\partial}{\\partial \\boldsymbol{x}\_t}\\boldsymbol{u}\_{\\boldsymbol{\\theta}^\*}(\\boldsymbol{x}\_t, r, t) + \\frac{\\partial}{\\partial t}\\boldsymbol{u}\_{\\boldsymbol{\\theta}^\*}(\\boldsymbol{x}\_t, r, t)\\right\] - (\\boldsymbol{x}\_1 - \\boldsymbol{x}\_0)\\right\] \\\
=&\\, \\boldsymbol{u}\_{\\boldsymbol{\\theta}^\*}(\\boldsymbol{x}\_t, r, t) + (t-r)\\left\[\\mathbb{E}\_{\\boldsymbol{x}\_0\|\\boldsymbol{x}\_t}\[\\boldsymbol{x}\_1-\\boldsymbol{x}\_0\]\\cdot\\frac{\\partial}{\\partial \\boldsymbol{x}\_t}\\boldsymbol{u}\_{\\boldsymbol{\\theta}^\*}(\\boldsymbol{x}\_t, r, t) + \\frac{\\partial}{\\partial t}\\boldsymbol{u}\_{\\boldsymbol{\\theta}^\*}(\\boldsymbol{x}\_t, r, t)\\right\] - \\mathbb{E}\_{\\boldsymbol{x}\_0\|\\boldsymbol{x}\_t}\[\\boldsymbol{x}\_1 - \\boldsymbol{x}\_0\] \\\
=&\\, \\boldsymbol{u}\_{\\boldsymbol{\\theta}^\*}(\\boldsymbol{x}\_t, r, t) + (t-r)\\left\[\\frac{d\\boldsymbol{x}\_t}{dt}\\cdot\\frac{\\partial}{\\partial \\boldsymbol{x}\_t}\\boldsymbol{u}\_{\\boldsymbol{\\theta}^\*}(\\boldsymbol{x}\_t, r, t) + \\frac{\\partial}{\\partial t}\\boldsymbol{u}\_{\\boldsymbol{\\theta}^\*}(\\boldsymbol{x}\_t, r, t)\\right\] - \\frac{d\\boldsymbol{x}\_t}{dt} \\\
=&\\, \\frac{d}{dt}\\left\[(t - r) \\boldsymbol{u}\_{\\boldsymbol{\\theta}^\*}(\\boldsymbol{x}\_t, r, t) - (\\boldsymbol{x}\_t - \\boldsymbol{x}\_r)\\right\] \\\
\\end{aligned}\\end{equation}
所以在适当的边界条件下就有$\\boldsymbol{x}\_t - \\boldsymbol{x}\_r = (t - r) \\boldsymbol{u}\_{\\boldsymbol{\\theta}^\*}(\\boldsymbol{x}\_t, r, t)$，即我们期望的平均速度模型。

这个过程的关键是$\\sg{\\cdot}$的引入避免了对JVP部分求梯度，从而简化了梯度表达式并得到了正确的结果。如果去掉$\\sg{\\cdot}$的话，上式右端就要多乘一项JVP部分对$\\boldsymbol{u}\_{\\boldsymbol{\\theta}^\*}(\\boldsymbol{x}\_t, r, t)$的雅可比矩阵，结果就是最后无法将$\\frac{d}{dt}\\left\[(t - r) \\boldsymbol{u}\_{\\boldsymbol{\\theta}^\*}(\\boldsymbol{x}\_t, r, t) - (\\boldsymbol{x}\_t - \\boldsymbol{x}\_r)\\right\]$这一项分离出来，而引入$\\sg{\\cdot}$的数学意义便是为了解决此问题。

当然，笔者还是那句话，$\\sg{\\cdot}$的引入也使得整个模型的训练耦合了梯度优化器，多了一丝不清晰的感觉。此时梯度等于零的点，顶多算是一个驻点而非（局部）最小值点，所以稳定性也不明朗，这其实也是所有耦合$\\sg{\\cdot}$的模型的共性。

## 相关工作 [\#](https://kexue.fm/archives/10958\#%E7%9B%B8%E5%85%B3%E5%B7%A5%E4%BD%9C)

非常有趣的是，我们之前介绍过的两篇加速生成的文章 [《生成扩散模型漫谈（二十一）：中值定理加速ODE采样》](https://kexue.fm/archives/9881) 和 [《生成扩散模型漫谈（二十七）：将步长作为条件输入》](https://kexue.fm/archives/10617)，也都是以平均速度为核心的，并且思想上可以说是一脉相承的。尽管作者之间未必相互有联系，但他们的工作内容上确实给人一种承前启后的连贯感。

在中值定理篇，作者已经意识到了平均速度
\\begin{equation}\\frac{1}{t-r}\\int\_r^t \\boldsymbol{v}\_{\\boldsymbol{\\theta}}(\\boldsymbol{x}\_{\\tau},\\tau) d\\tau\\end{equation}
的重要性，但他的做法是类比一维函数的积分中值定理，试图寻找$s\\in\[r,t\]$使得$\\boldsymbol{v}\_{\\boldsymbol{\\theta}}(\\boldsymbol{x}\_s,s)$等于平均速度。这本质上还是寻找高阶Solver的思想，但不再是Training-Free，而是需要少量的蒸馏步骤，对Solver来说算是一个小突破。

而步长输入篇所提的Shortcut模型，则几乎已经触碰到了MeanFlow，因为步长作为额外输入，跟MeanFlow的双时间参数$r,t$实质是等价的，不同的是它是直接以平均速度的性质作为额外的正则项来训练模型。用本文的记号，平均速度应该满足的性质是
\\begin{equation}\\boldsymbol{u}\_{\\boldsymbol{\\theta}}(\\boldsymbol{x}\_t, r, t) = \\frac{1}{2}\\left\[\\boldsymbol{u}\_{\\boldsymbol{\\theta}}\\left(\\boldsymbol{x}\_t, s, t\\right) + \\boldsymbol{u}\_{\\boldsymbol{\\theta}}\\left(\\boldsymbol{x}\_s, r, s\\right)\\right\]\\end{equation}
其中$s = (r+t)/2$。所以Shortcut干脆以它来构建正则项
\\begin{equation}\\left\\Vert\\boldsymbol{u}\_{\\boldsymbol{\\theta}}(\\boldsymbol{x}\_t, r, t) - \\frac{1}{2}\\sg{\\boldsymbol{u}\_{\\boldsymbol{\\theta}}(\\boldsymbol{x}\_t, s, t) + \\boldsymbol{u}\_{\\boldsymbol{\\theta}}(\\boldsymbol{x}\_s, r, s)}\\right\\Vert^2\\end{equation}
跟ReFlow的目标$\\eqref{eq:loss-reflow-2}$混合训练，实际训练中$\\boldsymbol{x}\_s = \\boldsymbol{x}\_t - (t-s)\\boldsymbol{u}\_{\\boldsymbol{\\theta}}(\\boldsymbol{x}\_t, s, t)$，$\\sg{\\cdot}$的引入在笔者看来主要也是为了节省计算量。Shortcut模型其实比MeanFlow更直观，但由于没有恒等变换和ReFlow带来的严格理论支撑，使得它看上去更多是一个过渡期的经验产物。

## 一致模型 [\#](https://kexue.fm/archives/10958\#%E4%B8%80%E8%87%B4%E6%A8%A1%E5%9E%8B)

最后我们再来讨论一下一致性模型。由于CM、sCM珠玉在前，MeanFlow的成功实际上也借鉴了它们的经验，尤其是给JVP加$\\sg{\\cdot}$的操作，这在原论文中也有提到。当然，MeanFlow作者之一何恺明老师本身也是操控梯度的大师（比如 [SimSiam](https://kexue.fm/archives/7980)），所以MeanFlow的出现看起来是非常水到渠成的。

离散的CM我们在 [《生成扩散模型漫谈（二十八）：分步理解一致性模型》](https://kexue.fm/archives/10633) 仔细分析过，如果将其中CM的EMA算符换成stop\_gradient，求梯度并取$\\Delta t\\to 0$的极限，那么就得到了 [《Simplifying, Stabilizing and Scaling Continuous-Time Consistency Models》](https://papers.cool/arxiv/2410.11081) 中的sCM的目标函数：
\\begin{equation}\\boldsymbol{f}\_{\\boldsymbol{\\theta}}(\\boldsymbol{x}\_t, t)\\cdot \\frac{d}{dt}\\boldsymbol{f}\_{\\sg{\\boldsymbol{\\theta}}}(\\boldsymbol{x}\_t, t) = \\boldsymbol{f}\_{\\boldsymbol{\\theta}}(\\boldsymbol{x}\_t, t)\\cdot\\sg{\\frac{d\\boldsymbol{x}\_t}{dt}\\cdot\\frac{\\partial}{\\partial \\boldsymbol{x}\_t}\\boldsymbol{f}\_{\\boldsymbol{\\theta}}(\\boldsymbol{x}\_t, t) + \\frac{\\partial}{\\partial t}\\boldsymbol{f}\_{\\boldsymbol{\\theta}}(\\boldsymbol{x}\_t, t)}\\label{eq:loss-scm}\\end{equation}
如果将$\\frac{d\\boldsymbol{x}\_t}{dt}$换成$\\boldsymbol{x}\_1 - \\boldsymbol{x}\_0$，然后记$\\boldsymbol{f}\_{\\boldsymbol{\\theta}}(\\boldsymbol{x}\_t, t) = \\boldsymbol{x}\_t - t\\boldsymbol{u}\_{\\boldsymbol{\\theta}}(\\boldsymbol{x}\_t , 0, t)$，那么它的梯度跟$r=0$时的MeanFlow第三目标$\\eqref{eq:loss-3}$是等价的：
\\begin{equation}\\begin{aligned}
\\nabla\_{\\boldsymbol{\\theta}}\\eqref{eq:loss-scm} =&\\, \\nabla\_{\\boldsymbol{\\theta}}\\boldsymbol{f}\_{\\boldsymbol{\\theta}}(\\boldsymbol{x}\_t, t)\\cdot \\left\[\\frac{d\\boldsymbol{x}\_t}{dt}\\cdot\\frac{\\partial}{\\partial \\boldsymbol{x}\_t}\\boldsymbol{f}\_{\\boldsymbol{\\theta}}(\\boldsymbol{x}\_t, t) + \\frac{\\partial}{\\partial t}\\boldsymbol{f}\_{\\boldsymbol{\\theta}}(\\boldsymbol{x}\_t, t)\\right\] \\\\[10pt\]
=&\\, -t\\nabla\_{\\boldsymbol{\\theta}}\\boldsymbol{u}\_{\\boldsymbol{\\theta}}(\\boldsymbol{x}\_t, 0, t)\\cdot \\left\[\\frac{d\\boldsymbol{x}\_t}{dt} - t\\frac{d\\boldsymbol{x}\_t}{dt}\\cdot\\frac{\\partial}{\\partial \\boldsymbol{x}\_t}\\boldsymbol{u}\_{\\boldsymbol{\\theta}}(\\boldsymbol{x}\_t, 0, t) - \\boldsymbol{u}\_{\\boldsymbol{\\theta}}(\\boldsymbol{x}\_t, 0, t) - t\\frac{\\partial}{\\partial t}\\boldsymbol{u}\_{\\boldsymbol{\\theta}}(\\boldsymbol{x}\_t, 0, t)\\right\] \\\\[10pt\]
=&\\, t\\nabla\_{\\boldsymbol{\\theta}}\\boldsymbol{u}\_{\\boldsymbol{\\theta}}(\\boldsymbol{x}\_t, 0, t)\\cdot \\left\[\\boldsymbol{u}\_{\\boldsymbol{\\theta}}(\\boldsymbol{x}\_t, 0, t) + t\\left\[\\frac{d\\boldsymbol{x}\_t}{dt}\\cdot\\frac{\\partial}{\\partial \\boldsymbol{x}\_t}\\boldsymbol{u}\_{\\boldsymbol{\\theta}}(\\boldsymbol{x}\_t, 0, t) + \\frac{\\partial}{\\partial t}\\boldsymbol{u}\_{\\boldsymbol{\\theta}}(\\boldsymbol{x}\_t, 0, t)\\right\]- \\frac{d\\boldsymbol{x}\_t}{dt}\\right\] \\\\[10pt\]
=&\\, \\frac{t}{2}\\nabla\_{\\boldsymbol{\\theta}}\\left\\Vert\\boldsymbol{u}\_{\\boldsymbol{\\theta}}(\\boldsymbol{x}\_t, 0, t) + t\\sg{\\frac{d\\boldsymbol{x}\_t}{dt}\\cdot\\frac{\\partial}{\\partial \\boldsymbol{x}\_t}\\boldsymbol{u}\_{\\boldsymbol{\\theta}}(\\boldsymbol{x}\_t, 0, t) + \\frac{\\partial}{\\partial t}\\boldsymbol{u}\_{\\boldsymbol{\\theta}}(\\boldsymbol{x}\_t, 0, t)}- \\frac{d\\boldsymbol{x}\_t}{dt}\\right\\Vert^2 \\\\[10pt\]
\\sim &\\, \\left.\\nabla\_{\\boldsymbol{\\theta}}\\eqref{eq:loss-3}\\right\|\_{r=0}
\\end{aligned}\\end{equation}

所以，从这个角度看，sCM是MeanFlow在$r=0$时的一个特例。正如前面所说，引入另外的时间参数$r$可以让ReFlow给MeanFlow“兜底”（$r=t$时），从而更好地避免训崩，这是它的优点之一。当然，从sCM出发其实也可以引入双时间参数，得到跟第三目标完全相同的结果，但从个人的审美来看，CM、sCM的物理意义终究不如MeanFlow平均速度的诠释直观。

此外，平均速度和ReFlow结合的出发点，还可以得到另外的第一目标$\\eqref{eq:loss-1}$和第二目标$\\eqref{eq:loss-2}$，这对于像笔者这样的stop\_gradient洁癖患者来说是非常舒适和漂亮的结果。在笔者看来，从计算成本出发，我们是可以考虑给损失函数加上stop\_gradient，但推导的第一性原理和基本结果不应该跟stop\_gradient耦合，否则意味着它跟优化器和动力学是强耦合的，这并不是一个本质结果应有的表现。

## 文章小结 [\#](https://kexue.fm/archives/10958\#%E6%96%87%E7%AB%A0%E5%B0%8F%E7%BB%93)

本文以最近出来的MeanFlow为中心，讨论了“平均速度”视角下的扩散模型加速生成思路。

_**转载到请包括本文地址：** [https://kexue.fm/archives/10958](https://kexue.fm/archives/10958)_

_**更详细的转载事宜请参考：**_ [《科学空间FAQ》](https://kexue.fm/archives/6508#%E6%96%87%E7%AB%A0%E5%A6%82%E4%BD%95%E8%BD%AC%E8%BD%BD/%E5%BC%95%E7%94%A8)

**如果您还有什么疑惑或建议，欢迎在下方评论区继续讨论。**

**如果您觉得本文还不错，欢迎 [分享](https://kexue.fm/archives/10958#share)/ [打赏](https://kexue.fm/archives/10958#pay) 本文。打赏并非要从中获得收益，而是希望知道科学空间获得了多少读者的真心关注。当然，如果你无视它，也不会影响你的阅读。再次表示欢迎和感谢！**

打赏

微信打赏

支付宝打赏

因为网站后台对打赏并无记录，因此欢迎在打赏时候备注留言。你还可以 [**点击这里**](http://mail.qq.com/cgi-bin/qm_share?t=qm_mailme&email=tN7d1drY3drrx8H0xcWa19vZ) 或在下方评论区留言来告知你的建议或需求。

**如果您需要引用本文，请参考：**

苏剑林. (May. 26, 2025). 《生成扩散模型漫谈（三十）：从瞬时速度到平均速度 》\[Blog post\]. Retrieved from [https://kexue.fm/archives/10958](https://kexue.fm/archives/10958)

@online{kexuefm-10958,
        title={生成扩散模型漫谈（三十）：从瞬时速度到平均速度},
        author={苏剑林},
        year={2025},
        month={May},
        url={\\url{https://kexue.fm/archives/10958}},
}

分类： [信息时代](https://kexue.fm/category/Big-Data)    标签： [微分方程](https://kexue.fm/tag/%E5%BE%AE%E5%88%86%E6%96%B9%E7%A8%8B/), [生成模型](https://kexue.fm/tag/%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B/), [采样](https://kexue.fm/tag/%E9%87%87%E6%A0%B7/), [扩散](https://kexue.fm/tag/%E6%89%A9%E6%95%A3/)[42 评论](https://kexue.fm/archives/10958#comments)

< [MoE环游记：5、均匀分布的反思](https://kexue.fm/archives/10945) \| [等值振荡定理：最优多项式逼近的充要条件](https://kexue.fm/archives/10972) >

### 你也许还对下面的内容感兴趣

- [线性注意力简史：从模仿、创新到反哺](https://kexue.fm/archives/11033)
- [Transformer升级之路：20、MLA好在哪里?（上）](https://kexue.fm/archives/10907)
- [生成扩散模型漫谈（二十九）：用DDPM来离散编码](https://kexue.fm/archives/10711)
- [细水长flow之TARFLOW：流模型满血归来？](https://kexue.fm/archives/10667)
- [生成扩散模型漫谈（二十八）：分步理解一致性模型](https://kexue.fm/archives/10633)
- [生成扩散模型漫谈（二十七）：将步长作为条件输入](https://kexue.fm/archives/10617)
- [生成扩散模型漫谈（二十六）：基于恒等式的蒸馏（下）](https://kexue.fm/archives/10567)
- [VQ的又一技巧：给编码表加一个线性变换](https://kexue.fm/archives/10519)
- [VQ的旋转技巧：梯度直通估计的一般推广](https://kexue.fm/archives/10489)
- [通向最优分布之路：概率空间的最小化](https://kexue.fm/archives/10289)

[发表你的看法](https://kexue.fm/archives/10958#comment_form)

1. [«](https://kexue.fm/archives/10958/comment-page-1#comments)
2. [1](https://kexue.fm/archives/10958/comment-page-1#comments)
3. [2](https://kexue.fm/archives/10958/comment-page-2#comments)

tyj

June 15th, 2025

感觉和之前的一篇文章很像，应该算是concurrent work： https://arxiv.org/abs/2504.20179

[回复评论](https://kexue.fm/archives/10958/comment-page-2?replyTo=27894#respond-post-10958)

[苏剑林](https://kexue.fm) 发表于
June 20th, 2025

IFM我看了，我开始也以为是类似工作，看起来也是平均速度导向，但我不理解为啥它的平均速度要同时显式出现$\\boldsymbol{x}\_t$和$\\boldsymbol{x}\_0$，这不是自找麻烦？这导致我压根看不懂IFM的思路...

[回复评论](https://kexue.fm/archives/10958/comment-page-2?replyTo=27931#respond-post-10958)

dry

June 15th, 2025

苏神好，一直有个疑问，ReFlow构建的ODE是$dx\_t/dt=x\_1-x\_0$，为什么这并不意味着任意时间点瞬时速度或者任意时间段的平均速度都是$x\_1-x\_0$，而我们还需要使用MeanFlow呢？

[回复评论](https://kexue.fm/archives/10958/comment-page-2?replyTo=27895#respond-post-10958)

[苏剑林](https://kexue.fm) 发表于
June 20th, 2025

ReFlow的ODE是$d\\boldsymbol{x}\_t/dt = \\mathbb{E}\_{\\boldsymbol{x}\_0\|\\boldsymbol{x}\_t}\[\\boldsymbol{x}\_1-\\boldsymbol{x}\_0\]$，以$\\boldsymbol{x}\_1-\\boldsymbol{x}\_0$为标签只是它的训练方式，学习结果是数学期望，至于$d\\boldsymbol{x}\_t/dt=\\boldsymbol{x}\_1-\\boldsymbol{x}\_0$只是一种直观的理解方式。

[回复评论](https://kexue.fm/archives/10958/comment-page-2?replyTo=27932#respond-post-10958)

hyizhu

June 19th, 2025

想要请教一个问题，mean flow和rectified flow比，多步生成效果好似乎很容易理解。但是在单步生成的时候，rec\_flow(x0, 1) 和 mean\_flow(x0, 0, 1)的信息量不应该是一样的吗？训练的时候这个情况下他们的target似乎也是一样的？为什么mean flow单步生成效果会好非常多？(x0, 0, 1)和(x0, 1)比似乎没有多出信息？

[回复评论](https://kexue.fm/archives/10958/comment-page-2?replyTo=27919#respond-post-10958)

[苏剑林](https://kexue.fm) 发表于
June 20th, 2025

你的“信息量”怎么定义？直观来说，reflow训练的是切线模型，而一步生成需要的是割线模型，meanflow正是训练割线模型，所以meanflow可以一步生成。信息量在这里怎么理解？

[回复评论](https://kexue.fm/archives/10958/comment-page-2?replyTo=27941#respond-post-10958)

行云流水0

June 24th, 2025

各路大神，有使用meanflow在工业级的复杂任务上做出效果的吗？是在什么样的场景中做的？模型多大呢？有偿求交流，微信号guoyi19930

[回复评论](https://kexue.fm/archives/10958/comment-page-2?replyTo=27961#respond-post-10958)

SunlightZero

June 25th, 2025

我根据 https://github.com/haidog-yaqub/MeanFlow 尝试训练了一下，感觉在 CIFAR-10 上效果非常差，生成的图片都是模糊的。不知道苏神是否有尝试训练呢？训练效果怎么样？

[回复评论](https://kexue.fm/archives/10958/comment-page-2?replyTo=27980#respond-post-10958)

[苏剑林](https://kexue.fm) 发表于
June 25th, 2025

我跑过这个项目，效果是能复现的。“在 CIFAR-10 上效果非常差，生成的图片都是模糊的”是指你连repo里边的示例效果都跑不出来？还是说你跑出来了但是觉得示例效果很模糊？

[回复评论](https://kexue.fm/archives/10958/comment-page-2?replyTo=27982#respond-post-10958)

1. [«](https://kexue.fm/archives/10958/comment-page-1#comments)
2. [1](https://kexue.fm/archives/10958/comment-page-1#comments)
3. [2](https://kexue.fm/archives/10958/comment-page-2#comments)

[取消回复](https://kexue.fm/archives/10958#respond-post-10958)

你的大名

电子邮箱

个人网站（选填）

1\. 可以使用LaTeX代码，点击“预览效果”可查看效果；2. 可以通过点击评论楼层编号来引用该楼层；3. 网站可能会有点卡，如非确认评论失败，请 **不要重复点击提交**。

### 内容速览

[现有思路](https://kexue.fm/archives/10958#%E7%8E%B0%E6%9C%89%E6%80%9D%E8%B7%AF)
[瞬时速度](https://kexue.fm/archives/10958#%E7%9E%AC%E6%97%B6%E9%80%9F%E5%BA%A6)
[平均速度](https://kexue.fm/archives/10958#%E5%B9%B3%E5%9D%87%E9%80%9F%E5%BA%A6)
[恒等变换](https://kexue.fm/archives/10958#%E6%81%92%E7%AD%89%E5%8F%98%E6%8D%A2)
[第一目标](https://kexue.fm/archives/10958#%E7%AC%AC%E4%B8%80%E7%9B%AE%E6%A0%87)
[JVP运算](https://kexue.fm/archives/10958#JVP%E8%BF%90%E7%AE%97)
[第二目标](https://kexue.fm/archives/10958#%E7%AC%AC%E4%BA%8C%E7%9B%AE%E6%A0%87)
[第三目标](https://kexue.fm/archives/10958#%E7%AC%AC%E4%B8%89%E7%9B%AE%E6%A0%87)
[证明一下](https://kexue.fm/archives/10958#%E8%AF%81%E6%98%8E%E4%B8%80%E4%B8%8B)
[相关工作](https://kexue.fm/archives/10958#%E7%9B%B8%E5%85%B3%E5%B7%A5%E4%BD%9C)
[一致模型](https://kexue.fm/archives/10958#%E4%B8%80%E8%87%B4%E6%A8%A1%E5%9E%8B)
[文章小结](https://kexue.fm/archives/10958#%E6%96%87%E7%AB%A0%E5%B0%8F%E7%BB%93)

### 智能搜索

支持整句搜索！网站自动使用 [结巴分词](https://github.com/fxsjy/jieba) 进行分词，并结合ngrams排序算法给出合理的搜索结果。

### 热门标签

[生成模型](https://kexue.fm/tag/%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B/) [attention](https://kexue.fm/tag/attention/) [优化](https://kexue.fm/tag/%E4%BC%98%E5%8C%96/) [语言模型](https://kexue.fm/tag/%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/) [模型](https://kexue.fm/tag/%E6%A8%A1%E5%9E%8B/) [网站](https://kexue.fm/tag/%E7%BD%91%E7%AB%99/) [概率](https://kexue.fm/tag/%E6%A6%82%E7%8E%87/) [梯度](https://kexue.fm/tag/%E6%A2%AF%E5%BA%A6/) [转载](https://kexue.fm/tag/%E8%BD%AC%E8%BD%BD/) [微分方程](https://kexue.fm/tag/%E5%BE%AE%E5%88%86%E6%96%B9%E7%A8%8B/) [矩阵](https://kexue.fm/tag/%E7%9F%A9%E9%98%B5/) [天象](https://kexue.fm/tag/%E5%A4%A9%E8%B1%A1/) [分析](https://kexue.fm/tag/%E5%88%86%E6%9E%90/) [深度学习](https://kexue.fm/tag/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/) [积分](https://kexue.fm/tag/%E7%A7%AF%E5%88%86/) [python](https://kexue.fm/tag/python/) [优化器](https://kexue.fm/tag/%E4%BC%98%E5%8C%96%E5%99%A8/) [力学](https://kexue.fm/tag/%E5%8A%9B%E5%AD%A6/) [无监督](https://kexue.fm/tag/%E6%97%A0%E7%9B%91%E7%9D%A3/) [扩散](https://kexue.fm/tag/%E6%89%A9%E6%95%A3/) [几何](https://kexue.fm/tag/%E5%87%A0%E4%BD%95/) [节日](https://kexue.fm/tag/%E8%8A%82%E6%97%A5/) [生活](https://kexue.fm/tag/%E7%94%9F%E6%B4%BB/) [文本生成](https://kexue.fm/tag/%E6%96%87%E6%9C%AC%E7%94%9F%E6%88%90/) [数论](https://kexue.fm/tag/%E6%95%B0%E8%AE%BA/)

### 随机文章

- [“韦森特”来袭](https://kexue.fm/archives/1675)
- [《交换代数导引》参考答案](https://kexue.fm/archives/4486)
- [澳大利亚网站请您向外星人问好](https://kexue.fm/archives/77)
- [通用爬虫探索（二）：落实到论坛爬取上](https://kexue.fm/archives/4422)
- [【NASA每日一图】沙子般的 NGC 1313](https://kexue.fm/archives/64)
- [哥本哈根气候大会召开情况](https://kexue.fm/archives/300)
- [【NASA每日一图】极光、流星、雪夜](https://kexue.fm/archives/302)
- [最小熵原理（五）：“层层递进”之社区发现与聚类](https://kexue.fm/archives/7006)
- [生活中的趣味数学：同一天生日概率有多大](https://kexue.fm/archives/40)
- [钱学森：人生书写时代](https://kexue.fm/archives/237)

### 最近评论

- [Leco](https://kexue.fm/archives/9590/comment-page-2#comment-27984): 请问LoRA的A,B矩阵初始化时，一个高斯随机一个全零还是只能A高斯，B全零呢？
- [苏剑林](https://kexue.fm/archives/9119/comment-page-13#comment-27983): 如果你把你这里提到的数学都学通透了，数学基础基本上可以胜任95%以上的场景了吧？至于“直觉”这...
- [苏剑林](https://kexue.fm/archives/10958/comment-page-2#comment-27982): 我跑过这个项目，效果是能复现的。“在 CIFAR-10 上效果非常差，生成的图片都是模糊的”是...
- [Henry Zha](https://kexue.fm/archives/9119/comment-page-13#comment-27981): 苏神你好，我是一名管理科学与工程专业的博士生，研究方向是结合人工智能模型建模用户行为之类的管理...
- [SunlightZero](https://kexue.fm/archives/10958/comment-page-2#comment-27980): 我根据 https://github.com/haidog-yaqub/MeanFlow 尝试...
- [苏剑林](https://kexue.fm/archives/11025/comment-page-1#comment-27979): 噢，是笔误，更正了，感谢指出。
- [苏剑林](https://kexue.fm/archives/10945/comment-page-1#comment-27978): 这里有很多因素。如果推理数据跟训练数据同分布，那么理论上就是均匀分布，但实际上同分布假设不一定...
- [苏剑林](https://kexue.fm/archives/11033/comment-page-1#comment-27977): 目前看来给O加rmsnorm挺稳的，效果甚至还好点。我其实是直接在flash attentio...
- [苏剑林](https://kexue.fm/archives/11033/comment-page-1#comment-27976): 你应该说的是$\\exp(\\boldsymbol{Q}\\boldsymbol{K}^{\\top}...
- [苏剑林](https://kexue.fm/archives/10831/comment-page-1#comment-27975): 可以这么说吧，通过某种分母归一化的操作，导数格式都类似，毕竟公式$(f/g)'=f'/g-fg...

### 友情链接

- [Cool Papers](https://papers.cool)
- [数学研发](https://bbs.emath.ac.cn)
- [Seatop](http://www.seatop.com.cn/)
- [Xiaoxia](https://xiaoxia.org/)
- [积分表-网络版](https://kexue.fm/sci/integral/index.html)
- [丝路博傲](http://blog.dvxj.com/)
- [ph4ntasy 饭特稀](http://www.ph4ntasy.com/)
- [数学之家](http://www.2math.cn/)
- [有趣天文奇观](http://interesting-sky.china-vo.org/)
- [TwistedW](http://www.twistedwg.com/)
- [godweiyang](https://godweiyang.com/)
- [AI柠檬](https://blog.ailemon.net/)
- [王登科-DK博客](https://greatdk.com)
- [ESON](https://blog.eson.org/)
- [枫之羽](https://fzhiy.net/)
- [Mathor's blog](https://wmathor.com/)
- [coding-zuo](https://coding-zuo.github.io/)
- [博科园](https://www.bokeyuan.net/)
- [孔皮皮的博客](https://www.kppkkp.top/)
- [运鹏的博客](https://yunpengtai.top/)
- [jiming.site](https://jiming.site/)
- [OmegaXYZ](https://www.omegaxyz.com/)
- [Blog by Eacls](https://www.eacls.top/)
- [EAI猩球](https://www.robotech.ink/)
- [文举的博客](https://liwenju0.com/)
- [用代码打点酱油](https://bruceyuan.com/)
- [申请链接](https://kexue.fm/links.html)

本站采用创作共用版权协议，要求署名、非商业用途和保持一致。转载本站内容必须也遵循“ [署名-非商业用途-保持一致](http://creativecommons.org/licenses/by-nc-nd/2.5/cn/)”的创作共用协议。
© 2009-2025 Scientific Spaces. All rights reserved. Theme by [laogui](http://www.laogui.com). Powered by [Typecho](http://typecho.org). 备案号: [粤ICP备09093259号-1/2](https://beian.miit.gov.cn/)。