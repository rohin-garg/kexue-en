## SEARCH

## MENU

- [打赏](https://kexue.fm/reward.html)
- [公式](https://kexue.fm/latex.html)
- [天象](https://kexue.fm/ac.html)
- [链接](https://kexue.fm/links.html)
- [时光](https://kexue.fm/me.html)
- [博览](https://kexue.fm/science.html)
- [归档](https://kexue.fm/content.html)

## CATEGORIES

- [千奇百怪](https://kexue.fm/category/Everything)
- [天文探索](https://kexue.fm/category/Astronomy)
- [数学研究](https://kexue.fm/category/Mathematics)
- [物理化学](https://kexue.fm/category/Phy-chem)
- [信息时代](https://kexue.fm/category/Big-Data)
- [生物自然](https://kexue.fm/category/Biology)
- [图片摄影](https://kexue.fm/category/Photograph)
- [问题百科](https://kexue.fm/category/Questions)
- [生活/情感](https://kexue.fm/category/Life-Feeling)
- [资源共享](https://kexue.fm/category/Resources)

## NEWPOSTS

- [“对角+低秩”三角阵的高效求逆方法](https://kexue.fm/archives/11072)
- [通过msign来计算奇异值裁剪mc...](https://kexue.fm/archives/11059)
- [矩阵符号函数mcsgn能计算什么？](https://kexue.fm/archives/11056)
- [线性注意力简史：从模仿、创新到反哺](https://kexue.fm/archives/11033)
- [msign的导数](https://kexue.fm/archives/11025)
- [通过msign来计算奇异值裁剪mc...](https://kexue.fm/archives/11006)
- [msign算子的Newton-Sc...](https://kexue.fm/archives/10996)
- [等值振荡定理：最优多项式逼近的充要条件](https://kexue.fm/archives/10972)
- [生成扩散模型漫谈（三十）：从瞬时速...](https://kexue.fm/archives/10958)
- [MoE环游记：5、均匀分布的反思](https://kexue.fm/archives/10945)

## COMMENTS

- [阿呱: 你好我也碰到了类似的问题，请问您是怎么解决的呀？想参考一下方法](https://kexue.fm/archives/7359/comment-page-8#comment-28061)
- [silver: 求问文中的$e\_i$是啥？是ds论文的“Algorithm 1...](https://kexue.fm/archives/10757/comment-page-3#comment-28060)
- [Truenobility303: 谢谢苏神的详细解答！](https://kexue.fm/archives/10739/comment-page-2#comment-28059)
- [Truenobility303: 不好意思我的表述可能会误导性说错了，核心问题不在2\. 我觉得问...](https://kexue.fm/archives/10795/comment-page-1#comment-28058)
- [kw: 把所有M直接换成全1矩阵就行吧，比如DeltaNet变成$(Q...](https://kexue.fm/archives/11033/comment-page-1#comment-28057)
- [WB: 非常清楚的blog。我有一个小问题想问一下，推导的时候用的是不...](https://kexue.fm/archives/10795/comment-page-1#comment-28056)
- [liangzhh: 谢谢大佬的分享，感觉中间有两个手误敲错，式(9)最后应该是加号...](https://kexue.fm/archives/11072/comment-page-1#comment-28055)
- [lidhrandom: Equation 3的等号右侧第二项的第一个${\\Lambda...](https://kexue.fm/archives/11072/comment-page-1#comment-28054)
- [Kuo: 在 $PaTH$ 论文章节 \`UT Transform for...](https://kexue.fm/archives/11033/comment-page-1#comment-28053)
- [Fanhao: 假定Hessian阵正定，那不是意味着$L(\\theta)$是...](https://kexue.fm/archives/10542/comment-page-1#comment-28052)

## USERLOGIN

- [登录](https://kexue.fm/admin/login.php)

[科学空间\|Scientific Spaces](https://kexue.fm)

- [登录](https://kexue.fm/admin/login.php)
- [打赏](https://kexue.fm/reward.html)
- [公式](https://kexue.fm/latex.html)
- [天象](https://kexue.fm/ac.html)
- [链接](https://kexue.fm/links.html)
- [时光](https://kexue.fm/me.html)
- [博览](https://kexue.fm/science.html)
- [归档](https://kexue.fm/content.html)

渴望成为一个小飞侠

- [欢迎订阅](https://kexue.fm/feed)
- [个性邮箱](https://kexue.fm/archives/119)
- [天象信息](https://kexue.fm/ac.html)
- [观测ISS](https://kexue.fm/archives/41)
- [LaTeX](https://kexue.fm/latex.html)
- [关于博主](https://kexue.fm/me.html)

欢迎访问“科学空间”，这里将与您共同探讨自然科学，回味人生百态；也期待大家的分享～

- [**千奇百怪** Everything](https://kexue.fm/category/Everything)
- [**天文探索** Astronomy](https://kexue.fm/category/Astronomy)
- [**数学研究** Mathematics](https://kexue.fm/category/Mathematics)
- [**物理化学** Phy-chem](https://kexue.fm/category/Phy-chem)
- [**信息时代** Big-Data](https://kexue.fm/category/Big-Data)
- [**生物自然** Biology](https://kexue.fm/category/Biology)
- [**图片摄影** Photograph](https://kexue.fm/category/Photograph)
- [**问题百科** Questions](https://kexue.fm/category/Questions)
- [**生活/情感** Life-Feeling](https://kexue.fm/category/Life-Feeling)
- [**资源共享** Resources](https://kexue.fm/category/Resources)

- [**千奇百怪**](https://kexue.fm/category/Everything)
- [**天文探索**](https://kexue.fm/category/Astronomy)
- [**数学研究**](https://kexue.fm/category/Mathematics)
- [**物理化学**](https://kexue.fm/category/Phy-chem)
- [**信息时代**](https://kexue.fm/category/Big-Data)
- [**生物自然**](https://kexue.fm/category/Biology)
- [**图片摄影**](https://kexue.fm/category/Photograph)
- [**问题百科**](https://kexue.fm/category/Questions)
- [**生活/情感**](https://kexue.fm/category/Life-Feeling)
- [**资源共享**](https://kexue.fm/category/Resources)

[首页](https://kexue.fm) [信息时代](https://kexue.fm/category/Big-Data) Transformer升级之路：15、Key归一化助力长度外推

20Nov

# [Transformer升级之路：15、Key归一化助力长度外推](https://kexue.fm/archives/9859)

By 苏剑林 \|
2023-11-20 \|
79884位读者\|

大体上，我们可以将目前Transformer的长度外推技术分为两类：一类是事后修改，比如 [NTK-RoPE](https://kexue.fm/archives/9675)、 [YaRN](https://papers.cool/arxiv/2309.00071)、 [ReRoPE](https://kexue.fm/archives/9708) 等，这类方法的特点是直接修改推理模型，无需微调就能达到一定的长度外推效果，但缺点是它们都无法保持模型在训练长度内的恒等性；另一类自然是事前修改，如 [ALIBI](https://kexue.fm/archives/9431#ALIBI)、 [KERPLE](https://kexue.fm/archives/9431#KERPLE)、 [XPOS](https://kexue.fm/archives/9431#XPOS) 以及 [HWFA](https://kexue.fm/archives/9603) 等，它们可以不加改动地实现一定的长度外推，但相应的改动需要在训练之前就引入，因此无法不微调地用于现成模型，并且这类方法是否能够Scale Up还没得到广泛认可。

在这篇文章中，笔者将介绍一种意外发现的长度外推方案——“KeyNorm”——对Attention的Key序列做L2 Normalization，很明显它属于事前修改一类，但对Attention机制的修改非常小，因此看上去非常有希望能够Scale Up。

## 最初动机 [\#](https://kexue.fm/archives/9859\#%E6%9C%80%E5%88%9D%E5%8A%A8%E6%9C%BA)

之所以说“意外发现”，是因为该改动的原始动机并不是长度外推，而是尝试替换Scaled Dot-Product Attention中的Scale方式。我们知道，Attention的标准定义是（本文主要考虑Causal场景）
\\begin{equation}\\boldsymbol{o}\_i = \\frac{\\sum\_{j = 1}^i\\exp\\left(\\frac{\\boldsymbol{q}\_i\\cdot \\boldsymbol{k}\_j}{\\sqrt{d}}\\right)\\boldsymbol{v}\_j}{\\sum\_{j = 1}^i\\exp\\left(\\frac{\\boldsymbol{q}\_i\\cdot \\boldsymbol{k}\_j}{\\sqrt{d}}\\right)},\\quad \\boldsymbol{q}\_i,\\boldsymbol{k}\_j\\in\\mathbb{R}^d\\label{eq:sdpa}\\end{equation}
其中，Scale因子$\\frac{1}{\\sqrt{d}}$我们已经多次进行过解释甚至推广，比如 [《浅谈Transformer的初始化、参数化与标准化》](https://kexue.fm/archives/8620#NTK%E5%8F%82%E6%95%B0%E5%8C%96)、 [《从熵不变性看Attention的Scale操作》](https://kexue.fm/archives/8823)、 [《从梯度最大化看Attention的Scale操作》](https://kexue.fm/archives/9812) 等。标准的推导是在“$\\boldsymbol{q}\_i,\\boldsymbol{k}\_j$均独立地采样自“均值为0、方差为1”的分布”的假设下进行的，而在该假设之下，我们还有
\\begin{equation}\\Vert\\boldsymbol{q}\_i\\Vert\\approx \\sqrt{d},\\quad \\Vert\\boldsymbol{k}\_j\\Vert\\approx \\sqrt{d}\\end{equation}
这是因为
\\begin{equation}\\Vert\\boldsymbol{x}\\Vert^2 = \\sum\_{i=1}^d x\_i^2 = d\\times\\frac{1}{d}\\sum\_{i=1}^d x\_i^2\\approx d\\,\\mathbb{E}\_{x\\sim\\mathcal{N}(0,1)}\[x^2\] = d\\end{equation}
相关推广还可以参考 [《让人惊叹的Johnson-Lindenstrauss引理：理论篇》](https://kexue.fm/archives/8679#%E5%BC%95%E7%90%86%E7%9A%84%E5%BC%95%E7%90%86)。这个近似式意味着，在Attention的初始阶段式$\\eqref{eq:sdpa}$与下面两个变体有着相同的效果：
\\begin{align}\\color{red}{\\text{Q}}\\text{uery}\\color{red}{\\text{N}}\\text{orm:}\\quad\\boldsymbol{o}\_i =&\\, \\frac{\\sum\_{j = 1}^i\\exp\\left(\\tilde{\\boldsymbol{q}}\_i\\cdot \\boldsymbol{k}\_j\\right)\\boldsymbol{v}\_j}{\\sum\_{j = 1}^i\\exp\\left(\\tilde{\\boldsymbol{q}}\_i\\cdot \\boldsymbol{k}\_j\\right)},\\qquad \\tilde{\\boldsymbol{q}}\_i = \\frac{\\boldsymbol{q}\_i}{\\Vert\\boldsymbol{q}\_i\\Vert} \\\\[5pt\]
\\color{red}{\\text{K}}\\text{ey}\\color{red}{\\text{N}}\\text{orm:}\\quad\\boldsymbol{o}\_i =&\\, \\frac{\\sum\_{j = 1}^i\\exp\\left(\\boldsymbol{q}\_i\\cdot \\tilde{\\boldsymbol{k}}\_j\\right)\\boldsymbol{v}\_j}{\\sum\_{j = 1}^i\\exp\\left(\\boldsymbol{q}\_i\\cdot \\tilde{\\boldsymbol{k}}\_j\\right)},\\qquad \\tilde{\\boldsymbol{k}}\_j = \\frac{\\boldsymbol{k}\_j}{\\Vert\\boldsymbol{k}\_j\\Vert}
\\end{align}
因此，就有了验证这两个变体与标准的式$\\eqref{eq:sdpa}$哪个更优的想法。为了描述的方便，我们可以相应地称为“Query/Key-Normalized Dot-Product Attention”，分别简称为“QNA”和“KNA”。

此外，既然可以QueryNorm和KeyNorm，那么自然也可以考虑两者都Norm一下，所以我们将如下“Scaled Cosine Attention（CosA）”也一并进行实验：
\\begin{equation}\\boldsymbol{o}\_i = \\frac{\\sum\_{j = 1}^i\\exp\\left(\\lambda\\,\\tilde{\\boldsymbol{q}}\_i\\cdot \\tilde{\\boldsymbol{k}}\_j\\right)\\boldsymbol{v}\_j}{\\sum\_{j = 1}^i\\exp\\left(\\lambda\\,\\tilde{\\boldsymbol{q}}\_i\\cdot \\tilde{\\boldsymbol{k}}\_j\\right)} = \\frac{\\sum\_{j = 1}^i\\exp\\left(\\lambda\\cos(\\boldsymbol{q}\_i,\\boldsymbol{k}\_j)\\right)\\boldsymbol{v}\_j}{\\sum\_{j = 1}^i\\exp\\left(\\lambda\\cos(\\boldsymbol{q}\_i,\\boldsymbol{k}\_j)\\right)}
\\end{equation}
其中$\\lambda$采用 [《从梯度最大化看Attention的Scale操作》](https://kexue.fm/archives/9812) 中的结果，即$\\lambda = 4\\log n$（原文是3.5，但下面训练长度比较小，改为4更精准一些），其中$n$固定为训练长度的一半，或者动态取位置id加1。

## 先看结果 [\#](https://kexue.fm/archives/9859\#%E5%85%88%E7%9C%8B%E7%BB%93%E6%9E%9C)

沿着 [之前](https://kexue.fm/archives/9731#%E5%AE%9E%E9%AA%8C) 做长度外推的实验设置，都是1亿参数的小模型， [GAU](https://kexue.fm/archives/9052) 架构，训练相同的步数（时间有限，这个步数下其实模型还没训练充分），训练长度512，并考虑外推到4096长度，实验结果如下表。其中Baseline就是式$\\eqref{eq:sdpa}$，$\\text{-}\\log n$就是加入 [《从熵不变性看Attention的Scale操作》](https://kexue.fm/archives/8823) 介绍的长度相关的缩放因子。评价指标是语言模型的逐token准确率，越大越好。
\\begin{array}{c\|cc}
\\hline
\\text{测试长度} & 512(\\text{训练}) & 4096(\\text{重复}) & 4096(\\text{不重复}) \\\
\\hline
\\text{Baseline} & 49.41\\% & 24.17\\% & 23.16\\% \\\
\\text{Baseline-}\\log n & 49.40\\% & 24.60\\% & 24.02\\% \\\
\\hline
\\text{QNA} & 49.55\\% & 22.45\\% & 22.18\\% \\\
\\text{QNA-}\\log n & 49.42\\% & 19.55\\% & 18.74\\% \\\
\\text{KNA} & 49.60\\% & 61.08\\% & 47.69\\% \\\
\\text{KNA-}\\log n & 49.58\\% & 63.17\\% & 46.40\\%\\\
\\text{CosA} & 49.73\\% & 58.90\\% & 46.98\\% \\\
\\text{CosA-}\\log n & 49.67\\% & 64.74\\% & 48.95\\% \\\
\\hline
\\end{array}
从表格中我们可以看出：1、不管是QueryNorm还是KeyNorm，都在训练长度上取得了更好的效果，虽然这个优势非常微弱，大概率随着训练的进一步推进可以忽略不计，但这个优势非常稳定，暗示着让训练更加平稳的可能性；2、 **KeyNorm对长度外推的提升非常明显**，这就是实验结果中的“意外之喜”！

注意，跟NTK-RoPE、YaRN需要修改推理阶段的模型不同，这里的KNA和CosA的长度外推在推理阶段是完全不做改动的。因此，可能有读者想知道，既然KNA和CosA推理时不加改动外推效果都这么好了，如果配合NTK-RoPE、YaRN等外推技巧，效果会不会“更上一层楼”？对此，笔者也进行了测试，结果如下表：
\\begin{array}{c\|cc}
\\hline
\\text{测试长度} & 512(\\text{训练}) & 4096(\\text{重复}) & 4096(\\text{不重复}) \\\
\\hline
\\text{Baseline} & 49.41\\% & 24.17\\% & 23.16\\% \\\
\\text{Baseline-NTK} & 49.41\\% & 60.57\\% & 42.20\\% \\\
\\text{Baseline-YaRN} & 49.41\\% & 80.10\\% & 47.45\\% \\\
\\text{Baseline-ReRoPE} & 49.41\\% & 76.11\\% & 47.82\\% \\\
\\hline
\\text{Baseline-}\\log n & 49.40\\% & 24.60\\% & 24.02\\% \\\
\\text{Baseline-}\\log n\\text{-NTK} & 49.40\\% & 75.86\\% & 47.06\\% \\\
\\text{Baseline-}\\log n\\text{-YaRN} & 49.40\\% & 82.57\\% & 46.52\\% \\\
\\text{Baseline-}\\log n\\text{-ReRoPE} & 49.40\\% & 85.47\\% & 48.87\\% \\\
\\hline
\\text{QNA} & 49.55\\% & 22.45\\% & 22.18\\% \\\
\\text{QNA-NTK} & 49.55\\% & 52.28\\% & 39.88\\% \\\
\\text{QNA-YaRN} & 49.55\\% & 82.53\\% & 47.50\\% \\\
\\text{QNA-ReRoPE} & 49.55\\% & 78.22\\% & 47.72\\% \\\
\\hline
\\text{QNA-}\\log n & 49.42\\% & 19.55\\% & 18.74\\% \\\
\\text{QNA-}\\log n\\text{-NTK} & 49.42\\% & 57.44\\% & 41.56\\% \\\
\\text{QNA-}\\log n\\text{-YaRN} & 49.42\\% & 80.08\\% & 45.16\\% \\\
\\text{QNA-}\\log n\\text{-ReRoPE} & 49.42\\% & 84.71\\% & 48.31\\% \\\
\\hline
\\text{KNA} & 49.60\\% & 61.08\\% & 47.69\\% \\\
\\text{KNA-NTK} & 49.60\\% & 64.44\\% & 43.02\\% \\\
\\text{KNA-YaRN} & 49.60\\% & 84.19\\% & 47.44\\% \\\
\\text{KNA-ReRoPE} & 49.60\\% & 77.76\\% & 47.73\\% \\\
\\hline
\\text{KNA-}\\log n & 49.58\\% & 63.17\\% & 46.40\\%\\\
\\text{KNA-}\\log n\\text{-NTK} & 49.58\\% & 79.05\\% & 47.43\\%\\\
\\text{KNA-}\\log n\\text{-YaRN} & 49.58\\% & 83.95\\% & 47.16\\%\\\
\\text{KNA-}\\log n\\text{-ReRoPE} & 49.58\\% & 85.48\\% & 48.78\\%\\\
\\hline
\\text{CosA} & 49.73\\% & 58.90\\% & 46.98\\% \\\
\\text{CosA-NTK} & 49.73\\% & 62.50\\% & 42.77\\% \\\
\\text{CosA-YaRN} & 49.73\\% & 83.40\\% & 47.80\\% \\\
\\text{CosA-ReRoPE} & 49.73\\% & 77.82\\% & 47.80\\% \\\
\\hline
\\text{CosA-}\\log n & 49.67\\% & 64.74\\% & 48.39\\% \\\
\\text{CosA-}\\log n\\text{-NTK} & 49.67\\% & 78.97\\% & 47.46\\% \\\
\\text{CosA-}\\log n\\text{-YaRN} & 49.67\\% & 82.28\\% & 45.72\\% \\\
\\text{CosA-}\\log n\\text{-ReRoPE} & 49.67\\% & 85.67\\% & 48.39\\% \\\
\\hline
\\end{array}
这个表比较啰嗦，主要是为了让大家对主流长度外推技巧的效果差异有一个全面的感知，大家选择自己感兴趣的维度比较就好，但要注意如果看长度外推效果的话，应该以“不重复”一列为主，“重复”一列为辅。从上表看，结果着实有点让人意外，KeyNorm似乎“免疫”了已有的RoPE外推技巧，NTK、YaRN等技巧叠加上去并没有明显提升，甚至可能会下降，不过总体来看“重复”一列还是有显著提升的，不显著的是“不重复”一列。这些结果表明，KeyNorm依然有着无法有效识别超出训练长度的位置（所以“重复”的结果不高）的问题，但有效地避免了PPL爆炸问题（所以“不重复”的结果还不错）。

这对做Long Context的同学来说可能是个好消息：一方面，KeyNorm不像ALIBI、KERPLE等，它的长度外推不用加Local约束，训练完成后也不做任何修改，纯属是“免费的午餐”，甚至看上去加了KeyNorm后训练效果都变好了；另一方面，也因为它是非Local的，所以可以更长文本继续训练，并且继续训练时再也不用纠结是选 [PI](https://papers.cool/arxiv/2306.15595) 还是 [ABF](https://papers.cool/arxiv/2309.16039) 了，对于KeyNorm来说，啥也不改就行。

## 原理分析 [\#](https://kexue.fm/archives/9859\#%E5%8E%9F%E7%90%86%E5%88%86%E6%9E%90)

尽管这是个意外发现，但我们仍需要尝试去解释它，不然它就一直只是个意外。所以这一节我们尝试来思考，为什么KeyNorm会有助于长度外推。

让我们重新回到式$\\eqref{eq:sdpa}$，第$i$个token与第$j$个token的相关性打分由内积完成：
\\begin{equation}s(j\|i) = \\boldsymbol{q}\_i\\cdot \\boldsymbol{k}\_j = \\Vert\\boldsymbol{q}\_i\\Vert \\Vert\\boldsymbol{k}\_j\\Vert \\cos(\\boldsymbol{q}\_i,\\boldsymbol{k}\_j),\\quad p(j\|i) = \\frac{\\exp\\left(\\frac{s(j\|i)}{\\sqrt{d}}\\right)}{\\sum\_{j=1}^i \\exp\\left(\\frac{s(j\|i)}{\\sqrt{d}}\\right)}\\end{equation}
第二个等号，我们从几何意义出发，将它分解为了各自模长与夹角余弦的乘积。注意力$p(j\|i)$是一个条件概率，$\\Vert\\boldsymbol{q}\_i\\Vert$只跟当前位置$i$有关，它不改变注意力的相对大小，而只改变 [稀疏程度](https://kexue.fm/archives/9595)；$\\Vert\\boldsymbol{k}\_j\\Vert$则有能力改变$p(j\|i)$的相对大小，但它不涉及到$i,j$的交互，可以用来表达一些绝对信号，比如 [Scissorhands](https://papers.cool/arxiv/2305.17118) 表明某些绝对位置的token的注意力一直都会很高，这就有可能用$\\Vert\\boldsymbol{k}\_j\\Vert$来表达；剩下的$\\cos(\\boldsymbol{q}\_i,\\boldsymbol{k}\_j)$就是用来表达$i,j$的交互，它是自由度最大的一项。

很明显，为了提高某个位置$j$的相对重要性，模型有两个选择：1、增大模长$\\Vert\\boldsymbol{k}\_j\\Vert$；2、增大$\\cos(\\boldsymbol{q}\_i,\\boldsymbol{k}\_j)$，即缩小$\\boldsymbol{q}\_i,\\boldsymbol{k}\_j$的夹角大小。然而，由于“ [维度灾难](https://kexue.fm/archives/7076)”的存在，在高维空间中显著地改变夹角大小相对来说没有那么容易，所以如果能靠增大模长$\\Vert\\boldsymbol{k}\_j\\Vert$完成的，模型会优先选择通过增大模长$\\Vert\\boldsymbol{k}\_j\\Vert$来完成，这导致的直接后果是：$\\cos(\\boldsymbol{q}\_i,\\boldsymbol{k}\_j)$的训练可能并不充分。

这里笔者作出一个断言（猜测）：

> $\\cos(\\boldsymbol{q}\_i,\\boldsymbol{k}\_j)$的训练不充分是Attention无法长度外推的主要原因。

$\\cos(\\boldsymbol{q}\_i,\\boldsymbol{k}\_j)$的训练不充分，是指被训练过的$\\boldsymbol{q}\_i,\\boldsymbol{k}\_j$的夹角只是一个有限的集合，而进行长度外推时，它要面对一个更大的集合，从而无法进行正确的预测。仔细思考 [YaRN](https://papers.cool/arxiv/2309.00071) 一文的推导就会发现，NTK、YaRN之所以有效，是因为修改了推理阶段RoPE的实现，使得$\\boldsymbol{q}\_i,\\boldsymbol{k}\_j$的夹角落到原本训练阶段的有限集合中，避免面对没见过的更大的集合，转外推为内插；ReRoPE则更加干脆，直接截断Window以外的相对位置，这使得推理阶段的位置编码都不会“面生”。这些技巧一定程度上都间接地验证了这个断言。

从这个断言出发，KeyNorm的长度外推起因就变得简单了。不论是只进行KeyNorm的KNA，还是QueryNorm、KeyNorm都做的CosA，它们都将$\\Vert\\boldsymbol{k}\_j\\Vert$从Attention的定义中排除掉了，于是为了改变$j$的相对重要性，模型就只有“调整$\\cos(\\boldsymbol{q}\_i,\\boldsymbol{k}\_j)$”这一个选择，这将会使得模型更加充分地训练和利用$\\cos(\\boldsymbol{q}\_i,\\boldsymbol{k}\_j)$，从而间接促进了长度外推性。此外，笔者也实验过“KeyNorm + NoPE”的组合，但并没有发现长度外推性，这说明RoPE也在KeyNorm的长度外推中担任重要角色。事实上这也不难理解，RoPE对$\\boldsymbol{q}\_i,\\boldsymbol{k}\_j$进行旋转，更有助于扩大训练期间$\\cos(\\boldsymbol{q}\_i,\\boldsymbol{k}\_j)$的范围，从而使得$\\cos(\\boldsymbol{q}\_i,\\boldsymbol{k}\_j)$的训练更为充分。

有没有工作已经尝试过QueryNorm和KeyNorm了呢？有。2020年的论文 [《Query-Key Normalization for Transformers》](https://papers.cool/arxiv/2010.04245) 曾实验过CosA，论文还提出了一个类似的长度对数的Scale因子，但没有讨论到长度外推问题。此外，今年初Google的论文 [《Scaling Vision Transformers to 22 Billion Parameters》](https://papers.cool/arxiv/2302.05442) 也在Query和Key加了Norm，但加的是LayerNorm，LayerNorm或者RMSNorm都带有可学的gamma参数，这使得Norm之后的向量模长未必为常数，因此并不好说是否能达到本文一样的长度外推效果。

## 文章小结 [\#](https://kexue.fm/archives/9859\#%E6%96%87%E7%AB%A0%E5%B0%8F%E7%BB%93)

本文介绍了笔者意外发现的一种长度外推方案“KeyNorm”——对Attention的Key序列进行L2归一化，在训练长度上取得了更好的效果，并在长度外推方面表现出显著的提升。它属于“事前修改”方案，跟其他事前修改方案如ALIBI、KERPLE等相比，它没有Local约束，因此更有希望能够Scale Up；相比于NTK-RoPE、YaRN等“事后修改”方案，它在外推的时候则不会损失训练长度内的性能。

_**转载到请包括本文地址：** [https://kexue.fm/archives/9859](https://kexue.fm/archives/9859)_

_**更详细的转载事宜请参考：**_ [《科学空间FAQ》](https://kexue.fm/archives/6508#%E6%96%87%E7%AB%A0%E5%A6%82%E4%BD%95%E8%BD%AC%E8%BD%BD/%E5%BC%95%E7%94%A8)

**如果您还有什么疑惑或建议，欢迎在下方评论区继续讨论。**

**如果您觉得本文还不错，欢迎 [分享](https://kexue.fm/archives/9859#share)/ [打赏](https://kexue.fm/archives/9859#pay) 本文。打赏并非要从中获得收益，而是希望知道科学空间获得了多少读者的真心关注。当然，如果你无视它，也不会影响你的阅读。再次表示欢迎和感谢！**

打赏

微信打赏

支付宝打赏

因为网站后台对打赏并无记录，因此欢迎在打赏时候备注留言。你还可以 [**点击这里**](http://mail.qq.com/cgi-bin/qm_share?t=qm_mailme&email=tN7d1drY3drrx8H0xcWa19vZ) 或在下方评论区留言来告知你的建议或需求。

**如果您需要引用本文，请参考：**

苏剑林. (Nov. 20, 2023). 《Transformer升级之路：15、Key归一化助力长度外推 》\[Blog post\]. Retrieved from [https://kexue.fm/archives/9859](https://kexue.fm/archives/9859)

@online{kexuefm-9859,
        title={Transformer升级之路：15、Key归一化助力长度外推},
        author={苏剑林},
        year={2023},
        month={Nov},
        url={\\url{https://kexue.fm/archives/9859}},
}

分类： [信息时代](https://kexue.fm/category/Big-Data)    标签： [attention](https://kexue.fm/tag/attention/), [位置编码](https://kexue.fm/tag/%E4%BD%8D%E7%BD%AE%E7%BC%96%E7%A0%81/), [泛化](https://kexue.fm/tag/%E6%B3%9B%E5%8C%96/), [外推](https://kexue.fm/tag/%E5%A4%96%E6%8E%A8/)[28 评论](https://kexue.fm/archives/9859#comments)

< [【生活杂记】炒锅的尽头是铁锅](https://kexue.fm/archives/9855) \| [我在Performer中发现了Transformer-VQ的踪迹](https://kexue.fm/archives/9862) >

### 你也许还对下面的内容感兴趣

- [“对角+低秩”三角阵的高效求逆方法](https://kexue.fm/archives/11072)
- [线性注意力简史：从模仿、创新到反哺](https://kexue.fm/archives/11033)
- [Transformer升级之路：20、MLA好在哪里?（上）](https://kexue.fm/archives/10907)
- [Transformer升级之路：19、第二类旋转位置编码](https://kexue.fm/archives/10862)
- [细水长flow之TARFLOW：流模型满血归来？](https://kexue.fm/archives/10667)
- [“闭门造车”之多模态思路浅谈（三）：位置编码](https://kexue.fm/archives/10352)
- [Decoder-only的LLM为什么需要位置编码？](https://kexue.fm/archives/10347)
- [Transformer升级之路：18、RoPE的底数选择原则](https://kexue.fm/archives/10122)
- [缓存与效果的极限拉扯：从MHA、MQA、GQA到MLA](https://kexue.fm/archives/10091)
- [Transformer升级之路：17、多模态位置编码的简单思考](https://kexue.fm/archives/10040)

[发表你的看法](https://kexue.fm/archives/9859#comment_form)

1. [«](https://kexue.fm/archives/9859/comment-page-1#comments)
2. [1](https://kexue.fm/archives/9859/comment-page-1#comments)
3. [2](https://kexue.fm/archives/9859/comment-page-2#comments)

[探秘Transformer系列之（10）R12; 自注意力 \| 呱唧呱唧网](http://www.itfaba.com/jishufenxian/205620.html)

March 6th, 2025

\[...\]Transformer升级之路：15、Key归一化助力长度外推 苏剑林\[...\]

[回复评论](https://kexue.fm/archives/9859/comment-page-2?replyTo=27001#respond-post-9859)

[探秘Transformer系列之（14）R12; 残差网络和归一化 \| 呱唧呱唧网](http://www.itfaba.com/jishufenxian/207380.html)

March 17th, 2025

\[...\]Transformer升级之路：15、Key归一化助力长度外推 苏剑林\[...\]

[回复评论](https://kexue.fm/archives/9859/comment-page-2?replyTo=27153#respond-post-9859)

[探秘Transformer系列之（23）R12; 长度外推 \| 呱唧呱唧网](http://www.itfaba.com/jishufenxian/211006.html)

April 6th, 2025

\[...\]Transformer升级之路：15、Key归一化助力长度外推\[...\]

[回复评论](https://kexue.fm/archives/9859/comment-page-2?replyTo=27328#respond-post-9859)

1. [«](https://kexue.fm/archives/9859/comment-page-1#comments)
2. [1](https://kexue.fm/archives/9859/comment-page-1#comments)
3. [2](https://kexue.fm/archives/9859/comment-page-2#comments)

[取消回复](https://kexue.fm/archives/9859#respond-post-9859)

你的大名

电子邮箱

个人网站（选填）

1\. 可以使用LaTeX代码，点击“预览效果”可查看效果；2. 可以通过点击评论楼层编号来引用该楼层；3. 网站可能会有点卡，如非确认评论失败，请 **不要重复点击提交**。

### 内容速览

[最初动机](https://kexue.fm/archives/9859#%E6%9C%80%E5%88%9D%E5%8A%A8%E6%9C%BA)
[先看结果](https://kexue.fm/archives/9859#%E5%85%88%E7%9C%8B%E7%BB%93%E6%9E%9C)
[原理分析](https://kexue.fm/archives/9859#%E5%8E%9F%E7%90%86%E5%88%86%E6%9E%90)
[文章小结](https://kexue.fm/archives/9859#%E6%96%87%E7%AB%A0%E5%B0%8F%E7%BB%93)

### 智能搜索

支持整句搜索！网站自动使用 [结巴分词](https://github.com/fxsjy/jieba) 进行分词，并结合ngrams排序算法给出合理的搜索结果。

### 热门标签

[生成模型](https://kexue.fm/tag/%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B/) [attention](https://kexue.fm/tag/attention/) [优化](https://kexue.fm/tag/%E4%BC%98%E5%8C%96/) [语言模型](https://kexue.fm/tag/%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/) [模型](https://kexue.fm/tag/%E6%A8%A1%E5%9E%8B/) [网站](https://kexue.fm/tag/%E7%BD%91%E7%AB%99/) [概率](https://kexue.fm/tag/%E6%A6%82%E7%8E%87/) [梯度](https://kexue.fm/tag/%E6%A2%AF%E5%BA%A6/) [转载](https://kexue.fm/tag/%E8%BD%AC%E8%BD%BD/) [矩阵](https://kexue.fm/tag/%E7%9F%A9%E9%98%B5/) [微分方程](https://kexue.fm/tag/%E5%BE%AE%E5%88%86%E6%96%B9%E7%A8%8B/) [天象](https://kexue.fm/tag/%E5%A4%A9%E8%B1%A1/) [分析](https://kexue.fm/tag/%E5%88%86%E6%9E%90/) [深度学习](https://kexue.fm/tag/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/) [积分](https://kexue.fm/tag/%E7%A7%AF%E5%88%86/) [python](https://kexue.fm/tag/python/) [优化器](https://kexue.fm/tag/%E4%BC%98%E5%8C%96%E5%99%A8/) [力学](https://kexue.fm/tag/%E5%8A%9B%E5%AD%A6/) [无监督](https://kexue.fm/tag/%E6%97%A0%E7%9B%91%E7%9D%A3/) [扩散](https://kexue.fm/tag/%E6%89%A9%E6%95%A3/) [几何](https://kexue.fm/tag/%E5%87%A0%E4%BD%95/) [节日](https://kexue.fm/tag/%E8%8A%82%E6%97%A5/) [生活](https://kexue.fm/tag/%E7%94%9F%E6%B4%BB/) [文本生成](https://kexue.fm/tag/%E6%96%87%E6%9C%AC%E7%94%9F%E6%88%90/) [数论](https://kexue.fm/tag/%E6%95%B0%E8%AE%BA/)

### 随机文章

- [科学空间添加新域名kexue.fm](https://kexue.fm/archives/4356)
- [旋转的弹簧将如何伸长？](https://kexue.fm/archives/782)
- [生成扩散模型漫谈（二十六）：基于恒等式的蒸馏（下）](https://kexue.fm/archives/10567)
- [msign算子的Newton-Schulz迭代（下）](https://kexue.fm/archives/10996)
- [太阳帆技术的粗浅分析](https://kexue.fm/archives/1023)
- [变分自编码器（六）：从几何视角来理解VAE的尝试](https://kexue.fm/archives/7725)
- [2010年诺贝尔文学奖落户秘鲁](https://kexue.fm/archives/981)
- [2010年诺贝尔化学奖出炉,美日科学家分享](https://kexue.fm/archives/979)
- [门控注意力单元（GAU）还需要Warmup吗？](https://kexue.fm/archives/8990)
- [有限素域上的乘法群是循环群](https://kexue.fm/archives/3200)

### 最近评论

- [阿呱](https://kexue.fm/archives/7359/comment-page-8#comment-28061): 你好我也碰到了类似的问题，请问您是怎么解决的呀？想参考一下方法
- [silver](https://kexue.fm/archives/10757/comment-page-3#comment-28060): 求问文中的$e\_i$是啥？是ds论文的“Algorithm 1”中提到的“violation ...
- [Truenobility303](https://kexue.fm/archives/10739/comment-page-2#comment-28059): 谢谢苏神的详细解答！
- [Truenobility303](https://kexue.fm/archives/10795/comment-page-1#comment-28058): 不好意思我的表述可能会误导性说错了，核心问题不在2\. 我觉得问题在于整套论述都基于谱条件满足那...
- [kw](https://kexue.fm/archives/11033/comment-page-1#comment-28057): 把所有M直接换成全1矩阵就行吧，比如DeltaNet变成$(QK^⊤)(I+KK^⊤⊙(1-I...
- [WB](https://kexue.fm/archives/10795/comment-page-1#comment-28056): 非常清楚的blog。我有一个小问题想问一下，推导的时候用的是不等式（10），这里左边O(1)，...
- [liangzhh](https://kexue.fm/archives/11072/comment-page-1#comment-28055): 谢谢大佬的分享，感觉中间有两个手误敲错，式(9)最后应该是加号，另外是chunk而不是chuck吧？
- [lidhrandom](https://kexue.fm/archives/11072/comment-page-1#comment-28054): Equation 3的等号右侧第二项的第一个${\\Lambda^{-1}}$疑似不应取逆
- [Kuo](https://kexue.fm/archives/11033/comment-page-1#comment-28053): 在 $PaTH$ 论文章节 \`UT Transform for Products of Hou...
- [Fanhao](https://kexue.fm/archives/10542/comment-page-1#comment-28052): 假定Hessian阵正定，那不是意味着$L(\\theta)$是$\\theta$的凸函数吗？这一...

### 友情链接

- [Cool Papers](https://papers.cool)
- [数学研发](https://bbs.emath.ac.cn)
- [Seatop](http://www.seatop.com.cn/)
- [Xiaoxia](https://xiaoxia.org/)
- [积分表-网络版](https://kexue.fm/sci/integral/index.html)
- [丝路博傲](http://blog.dvxj.com/)
- [ph4ntasy 饭特稀](http://www.ph4ntasy.com/)
- [数学之家](http://www.2math.cn/)
- [有趣天文奇观](http://interesting-sky.china-vo.org/)
- [TwistedW](http://www.twistedwg.com/)
- [godweiyang](https://godweiyang.com/)
- [AI柠檬](https://blog.ailemon.net/)
- [王登科-DK博客](https://greatdk.com)
- [ESON](https://blog.eson.org/)
- [枫之羽](https://fzhiy.net/)
- [Mathor's blog](https://wmathor.com/)
- [coding-zuo](https://coding-zuo.github.io/)
- [博科园](https://www.bokeyuan.net/)
- [孔皮皮的博客](https://www.kppkkp.top/)
- [运鹏的博客](https://yunpengtai.top/)
- [jiming.site](https://jiming.site/)
- [OmegaXYZ](https://www.omegaxyz.com/)
- [Blog by Eacls](https://www.eacls.top/)
- [EAI猩球](https://www.robotech.ink/)
- [文举的博客](https://liwenju0.com/)
- [用代码打点酱油](https://bruceyuan.com/)
- [申请链接](https://kexue.fm/links.html)

本站采用创作共用版权协议，要求署名、非商业用途和保持一致。转载本站内容必须也遵循“ [署名-非商业用途-保持一致](http://creativecommons.org/licenses/by-nc-nd/2.5/cn/)”的创作共用协议。
© 2009-2025 Scientific Spaces. All rights reserved. Theme by [laogui](http://www.laogui.com). Powered by [Typecho](http://typecho.org). 备案号: [粤ICP备09093259号-1/2](https://beian.miit.gov.cn/)。