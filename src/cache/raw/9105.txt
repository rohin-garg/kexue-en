## SEARCH

## MENU

- [打赏](https://kexue.fm/reward.html)
- [公式](https://kexue.fm/latex.html)
- [天象](https://kexue.fm/ac.html)
- [链接](https://kexue.fm/links.html)
- [时光](https://kexue.fm/me.html)
- [博览](https://kexue.fm/science.html)
- [归档](https://kexue.fm/content.html)

## CATEGORIES

- [千奇百怪](https://kexue.fm/category/Everything)
- [天文探索](https://kexue.fm/category/Astronomy)
- [数学研究](https://kexue.fm/category/Mathematics)
- [物理化学](https://kexue.fm/category/Phy-chem)
- [信息时代](https://kexue.fm/category/Big-Data)
- [生物自然](https://kexue.fm/category/Biology)
- [图片摄影](https://kexue.fm/category/Photograph)
- [问题百科](https://kexue.fm/category/Questions)
- [生活/情感](https://kexue.fm/category/Life-Feeling)
- [资源共享](https://kexue.fm/category/Resources)

## NEWPOSTS

- [msign算子的Newton-Sc...](https://kexue.fm/archives/10996)
- [从无穷范数求导到等值振荡定理](https://kexue.fm/archives/10972)
- [生成扩散模型漫谈（三十）：从瞬时速...](https://kexue.fm/archives/10958)
- [MoE环游记：5、均匀分布的反思](https://kexue.fm/archives/10945)
- [msign算子的Newton-Sc...](https://kexue.fm/archives/10922)
- [Transformer升级之路：2...](https://kexue.fm/archives/10907)
- [一道概率不等式：盯着它到显然成立为止！](https://kexue.fm/archives/10902)
- [SVD的导数](https://kexue.fm/archives/10878)
- [智能家居之手搓一套能接入米家的零冷水装置](https://kexue.fm/archives/10869)
- [Transformer升级之路：1...](https://kexue.fm/archives/10862)

## COMMENTS

- [rpsun: 这样似乎与传统的经验正交函数之类的有相似之处。把样本的平均值减...](https://kexue.fm/archives/10699/comment-page-1#comment-27808)
- [贵阳机场接机: 怎么不更新啦](https://kexue.fm/archives/1490/comment-page-1#comment-27807)
- [czvzb: 具身智能模型目前主流也是在使用扩散和流匹配这类方法来预测动作。...](https://kexue.fm/archives/10958/comment-page-1#comment-27806)
- [Shawn\_yang: 不好意思，以为网页卡了0.0点了三下](https://kexue.fm/archives/10945/comment-page-1#comment-27805)
- [Shawn\_yang: 苏神，关于您所说的：“推理阶段可以事先预估Routed Exp...](https://kexue.fm/archives/10945/comment-page-1#comment-27804)
- [Shawn\_yang: 苏神，关于您所说的：“推理阶段可以事先预估Routed Exp...](https://kexue.fm/archives/10945/comment-page-1#comment-27803)
- [Shawn\_yang: 苏神，关于您所说的：“推理阶段可以事先预估Routed Exp...](https://kexue.fm/archives/10945/comment-page-1#comment-27802)
- [OceanYU: 您好，关于由式（7）推导出高斯分布，我这里有一点问题，式（7）...](https://kexue.fm/archives/9164/comment-page-4#comment-27801)
- [jorjiang: 训练和prefill这个compute-bound阶段不做矩阵...](https://kexue.fm/archives/10907/comment-page-2#comment-27800)
- [amy: 苏老师，您有关注傅里叶旋转位置编码这篇工作吗，想知道您对这篇工...](https://kexue.fm/archives/10907/comment-page-2#comment-27799)

## USERLOGIN

- [登录](https://kexue.fm/admin/login.php)

[科学空间\|Scientific Spaces](https://kexue.fm)

- [登录](https://kexue.fm/admin/login.php)
- [打赏](https://kexue.fm/reward.html)
- [公式](https://kexue.fm/latex.html)
- [天象](https://kexue.fm/ac.html)
- [链接](https://kexue.fm/links.html)
- [时光](https://kexue.fm/me.html)
- [博览](https://kexue.fm/science.html)
- [归档](https://kexue.fm/content.html)

渴望成为一个小飞侠

- [欢迎订阅](https://kexue.fm/feed)
- [个性邮箱](https://kexue.fm/archives/119)
- [天象信息](https://kexue.fm/ac.html)
- [观测ISS](https://kexue.fm/archives/41)
- [LaTeX](https://kexue.fm/latex.html)
- [关于博主](https://kexue.fm/me.html)

欢迎访问“科学空间”，这里将与您共同探讨自然科学，回味人生百态；也期待大家的分享～

- [**千奇百怪** Everything](https://kexue.fm/category/Everything)
- [**天文探索** Astronomy](https://kexue.fm/category/Astronomy)
- [**数学研究** Mathematics](https://kexue.fm/category/Mathematics)
- [**物理化学** Phy-chem](https://kexue.fm/category/Phy-chem)
- [**信息时代** Big-Data](https://kexue.fm/category/Big-Data)
- [**生物自然** Biology](https://kexue.fm/category/Biology)
- [**图片摄影** Photograph](https://kexue.fm/category/Photograph)
- [**问题百科** Questions](https://kexue.fm/category/Questions)
- [**生活/情感** Life-Feeling](https://kexue.fm/category/Life-Feeling)
- [**资源共享** Resources](https://kexue.fm/category/Resources)

- [**千奇百怪**](https://kexue.fm/category/Everything)
- [**天文探索**](https://kexue.fm/category/Astronomy)
- [**数学研究**](https://kexue.fm/category/Mathematics)
- [**物理化学**](https://kexue.fm/category/Phy-chem)
- [**信息时代**](https://kexue.fm/category/Big-Data)
- [**生物自然**](https://kexue.fm/category/Biology)
- [**图片摄影**](https://kexue.fm/category/Photograph)
- [**问题百科**](https://kexue.fm/category/Questions)
- [**生活/情感**](https://kexue.fm/category/Life-Feeling)
- [**资源共享**](https://kexue.fm/category/Resources)

[首页](https://kexue.fm) [信息时代](https://kexue.fm/category/Big-Data) 相对位置编码Transformer的一个理论缺陷与对策

7Jun

# [相对位置编码Transformer的一个理论缺陷与对策](https://kexue.fm/archives/9105)

By 苏剑林 \|
2022-06-07 \|
114607位读者\|

位置编码是Transformer中很重要的一环，在 [《让研究人员绞尽脑汁的Transformer位置编码》](https://kexue.fm/archives/8130) 中我们就总结了一些常见的位置编码设计。大体上，我们将Transformer的位置编码分为“绝对位置编码”和“相对位置编码”两类，其中“相对位置编码”在众多NLP/CV的实验表现相对来说更加好些。

然而，我们可以发现，目前相对位置编码几乎都是在Softmax之前的Attention矩阵上进行操作的，这种施加方式实际上都存在一个理论上的缺陷，使得Transformer无法成为“万能拟合器”。本文就来分析这个问题，并探讨一些解决方案。

## 简单探针 [\#](https://kexue.fm/archives/9105\#%E7%AE%80%E5%8D%95%E6%8E%A2%E9%92%88)

顾名思义，位置编码就是用来给模型补充上位置信息的。那么，如何判断一个模型有没有足够的识别位置的能力呢？笔者之前曾构思过一个简单的探针实验：

> 对于一个有识别位置能力的模型，应该有能力准确实现如下映射
> \\begin{equation}\\begin{array}{lc}
> \\text{输入：} & \[0, 0, \\cdots, 0, 0\] \\\
> & \\downarrow\\\
> \\text{输出：} & \[1, 2, \\cdots, n-1, n\]
> \\end{array}\\end{equation}

也就是说，输入$n$个0，能有序地输出位置编号$1\\sim n$。这个探针实验的思想很简单，即模型如果有能力做到这一点，说明识别位置是模型自身具备的能力，跟外部输入无关，这正是我们想要的。不难发现，绝对位置由于是直接施加在输入上的，所以它很容易能够完成探针测试。

## 无法胜任 [\#](https://kexue.fm/archives/9105\#%E6%97%A0%E6%B3%95%E8%83%9C%E4%BB%BB)

然而，当笔者带着这个简单的探针实验去思考带有相对位置编码的Transformer模型时，却发现它们几乎都不能完成上述任务。

具体来说，除了 [《Self-Attention with Relative Position Representations》](https://papers.cool/arxiv/1803.02155) 所提出的设计外，其余所有相对位置编码（包括笔者所提的 [RoPE](https://kexue.fm/archives/8265)）都只修改了Softmax前的Attention矩阵，那么带有相对位置信息的Attention矩阵依然是一个概率矩阵（即每一行求和等于1）。

另一方面，对于Transformer模型来说，Token之间的交互的唯一来源是Self Attention的$\\boldsymbol{A}\\boldsymbol{V}$这一步，或者写成$\\boldsymbol{o}\_i = \\sum\\limits\_j a\_{i,j}\\boldsymbol{v}\_j$。相同的输入意味着每个$\\boldsymbol{v}\_j$都是相同的，所以
\\begin{equation}\\boldsymbol{o}\_i = \\sum\_j a\_{i,j}\\boldsymbol{v}\_j = \\sum\_j a\_{i,j}\\boldsymbol{v} = \\left(\\sum\_j a\_{i,j}\\right)\\boldsymbol{v} = \\boldsymbol{v}\\end{equation}
这意味着每个$\\boldsymbol{o}\_i$也是相同的。换句话说，模型的每个位置自始至终都输出相同的结果，所以模型根本不可能输出各不相同的$\[1, 2, \\cdots, n-1, n\]$。

类似的发现也出现在最近的论文 [《Your Transformer May Not be as Powerful as You Expect》](https://papers.cool/arxiv/2205.13401) 中，作者构建了略有不同的例子来演示相对位置编码Transformer的拟合能力缺陷问题，两者异曲同工、不谋而合了。此外，本文开头说的是“万能拟合”，那解决了这个反例是不是就能做到“万能拟合”了呢？该论文也有相应的理论分析来肯定这一事实，这里就不详述了。

## 初步方案 [\#](https://kexue.fm/archives/9105\#%E5%88%9D%E6%AD%A5%E6%96%B9%E6%A1%88)

稍加思考就可以发现，其实问题主要出在Attention矩阵的每一行求和等于1，要解决这个问题，想办法打破这个约束就行了。为此， [《Your Transformer May Not be as Powerful as You Expect》](https://papers.cool/arxiv/2205.13401) 在其发现之上进一步提出了如下设计
\\begin{equation}\\boldsymbol{O} = (\\boldsymbol{A}\\odot \\boldsymbol{C})\\boldsymbol{V}\\quad \\text{或者等价地}\\quad\\boldsymbol{o}\_i = \\sum\_j a\_{i,j}c\_{i,j}\\boldsymbol{v}\_j\\end{equation}
其中$\\boldsymbol{C}$是一个可训练的参数矩阵，$\\odot$是逐位相乘（ [Hadamard积](https://en.wikipedia.org/wiki/Hadamard_product_(matrices))）。为了使得整个模型依然只包含相对位置信息（因为本文就是讨论相对位置编码Transfomrer的缺陷），我们要约束$\\boldsymbol{C}$为 [Toeplitz矩阵](https://en.wikipedia.org/wiki/Toeplitz_matrix)，即$c\_{i,j}=g(i-j)$。

有了$\\boldsymbol{C}$的加入，$\\boldsymbol{A}\\odot \\boldsymbol{C}$作为一个整体，每一行的和显然不一定为1，从而打破了这个限制，因此是可以解决问题的（更多的实验结果请自行看原论文）。但这样一来，引入了新的参数矩阵不说，由于$\\boldsymbol{C}$本身是有限大小的，所以它就不能很好地支持变长输入（或者矩阵$\\boldsymbol{C}$相应地要做一些截断，即$c\_{i,j}=g(\\text{clip}(i-j, p\_{\\min}, p\_{\\max}))$的形式），总的来说显得不够简洁优雅。

## 去掉分母 [\#](https://kexue.fm/archives/9105\#%E5%8E%BB%E6%8E%89%E5%88%86%E6%AF%8D)

再次回到问题所在：Attention矩阵的每一行求和等于1。是什么操作导致了这一现象呢？答案很显然，是Softmax：
\\begin{equation}a\_{i,j} = \\frac{e^{b\_{i,j}}}{\\sum\\limits\_j e^{b\_{i,j}}}\\end{equation}
这里的$\\boldsymbol{B}=(b\_{i,j})$是Softmax前的矩阵。很明显，就是“除以$\\sum\\limits\_j e^{b\_{i,j}}$”这一步导致了$\\sum\\limits\_j a\_{i,j}=1$，那么一个很直接的想法就是：

> 如果我不想$\\sum\\limits\_j a\_{i,j}=1$，那么干脆别除以$\\sum\\limits\_j e^{b\_{i,j}}$就行了？

事实上确实可以！实验结果显示，不除以该分母的Transformer确实能成功地完成前述探针测试。此时就不得不感概一下 [GAU](https://kexue.fm/archives/8934) 的“先见之明”了，它提出的新式Attention直接是$\\text{relu}^2$激活然后简单除以$n$来归一化，避免了$\\sum\\limits\_j a\_{i,j}=1$，从而增强了模型的理论能力（当然也许作者根本没想那么多，是笔者想象的成分居多）。

## 新归一化 [\#](https://kexue.fm/archives/9105\#%E6%96%B0%E5%BD%92%E4%B8%80%E5%8C%96)

然而，我们在 [《听说Attention与Softmax更配哦～》](https://kexue.fm/archives/9019) 发现像GAU里的不进行概率归一化的Attention设计可能存在外推能力欠佳的问题。也就是说，进行概率归一化导致了前面说的理论缺陷，简单地除以$n$来归一化则外推能力可能欠佳，有没有同时能兼顾两者的方案呢？

让我们再发散一下脑洞。从范数的角度来看，$\\sum\\limits\_j e^{b\_{i,j}}$实际上是向量$e^{b\_{i,:}}$的$l\_1$范数，所以Softmax实际上就是向量的$e^{b\_{i,:}}$的$l\_1$归一化操作，那么要避免$\\sum\\limits\_j a\_{i,j}=1$，又有保留归一化，换成其他的归一化操作是否可以呢？比如$l\_2$归一化：
\\begin{equation}a\_{i,j} = \\frac{e^{b\_{i,j}}}{\\sqrt{\\sum\\limits\_j e^{2b\_{i,j}}}}\\end{equation}

经过笔者测试，这种$l\_2$归一化的Attention，确实能成功完成探针实验。那么，这个改动对我们更关心的NLP预训练场景有没有帮助呢？笔者也做了相应的对比实验，结果是分两部分：

> 1、对于标准的Attention + FFN组合，应用$l\_2$归一化Attention之前要缩小一下Attention的$\\boldsymbol{W}\_V,\\boldsymbol{W}\_O$的初始方差，实验结果则是略差于常规的$l\_1$归一化Attention；
>
> 2、对于全GAU的架构，可以直接应用$l\_2$归一化Attention，不需要改动初始化，实验结果则是略优于常规的$l\_1$归一化Attention。

两者的差别大概是源于它们本身的初始化方式不同，在标准的Attention + FFN组合中，初始Attention矩阵接近一个均匀矩阵（每个数都相同），而在 [《门控注意力单元（GAU）还需要Warmup吗？》](https://kexue.fm/archives/8990) 我们则分析过，GAU的初始Attention矩阵更接近一个单位阵（的若干倍）。

## 峰回路转 [\#](https://kexue.fm/archives/9105\#%E5%B3%B0%E5%9B%9E%E8%B7%AF%E8%BD%AC)

再次纵观前文，我们发现是因为“每个$\\boldsymbol{v}\_j$都是相同的”，所以“$\\sum\\limits\_j a\_{i,j}=1$的模型无法完成探针实验”。但如果每个$\\boldsymbol{v}\_j$不全相同呢？

我们知道，从BERT开始，主流的Transformer模型都是像“\[CLS\]SENT\[SEP\]”设计输入的，也就是在输入前后会附加一些标记性的Token，如果我们将这些标记Token当作模型的一部分而不是输入（也就是说输入“\[CLS\]0 0 ⋯ 0 0\[SEP\]”而不是全0），那么是否有可能完成探针呢？

笔者也对此做了实验，发现对输入补充上标记行Token后，不需要对相对位置编码Transformer的其他部分做修改，确实也能够完成探针实验。这结果就有点啼笑皆非了，原来BERT的作者们也很有“先见之明”啊，所添加的特殊Token \[CLS\]、\[SEP\]还有辅助定位的作用，我们分析那么久的理论缺陷，居然就这样被两个特殊Token解决了。这不禁让人想起 [《How Much Position Information Do Convolutional Neural Networks Encode?》](https://papers.cool/arxiv/2001.08248) 所提到的“CNN是通过padding来识别绝对位置的”这一结论，两者有一定的相通之处。

当然，这也不意味着我们前面的思考全无意义。比如对GAU模型来说，Attention换用$l\_2$归一化确确实实有加快收敛、轻微提升效果的作用。此外，既然可以接受$l\_2$归一化，那么$e^{b\_{i,j}}$是不是还可以换成一般的激活函数（比如去掉非负性约束）呢？笔者也简单做了“$\\text{swish}(b\_{i,j})$ + $l\_2$归一化”的实验，发现有一定的可行性。从这个角度来看，$l\_2$归一化下的Attention实际上有更多的拓展空间。

## 曲终人散 [\#](https://kexue.fm/archives/9105\#%E6%9B%B2%E7%BB%88%E4%BA%BA%E6%95%A3)

本文分析了相对位置编码Transformer的一个隐含缺陷，并探讨了相应的对策，从中引申出关于Attention矩阵的非负性、归一化方式的思考。

_**转载到请包括本文地址：** [https://kexue.fm/archives/9105](https://kexue.fm/archives/9105)_

_**更详细的转载事宜请参考：**_ [《科学空间FAQ》](https://kexue.fm/archives/6508#%E6%96%87%E7%AB%A0%E5%A6%82%E4%BD%95%E8%BD%AC%E8%BD%BD/%E5%BC%95%E7%94%A8)

**如果您还有什么疑惑或建议，欢迎在下方评论区继续讨论。**

**如果您觉得本文还不错，欢迎 [分享](https://kexue.fm/archives/9105#share)/ [打赏](https://kexue.fm/archives/9105#pay) 本文。打赏并非要从中获得收益，而是希望知道科学空间获得了多少读者的真心关注。当然，如果你无视它，也不会影响你的阅读。再次表示欢迎和感谢！**

打赏

微信打赏

支付宝打赏

因为网站后台对打赏并无记录，因此欢迎在打赏时候备注留言。你还可以 [**点击这里**](http://mail.qq.com/cgi-bin/qm_share?t=qm_mailme&email=tN7d1drY3drrx8H0xcWa19vZ) 或在下方评论区留言来告知你的建议或需求。

**如果您需要引用本文，请参考：**

苏剑林. (Jun. 07, 2022). 《相对位置编码Transformer的一个理论缺陷与对策 》\[Blog post\]. Retrieved from [https://kexue.fm/archives/9105](https://kexue.fm/archives/9105)

@online{kexuefm-9105,
        title={相对位置编码Transformer的一个理论缺陷与对策},
        author={苏剑林},
        year={2022},
        month={Jun},
        url={\\url{https://kexue.fm/archives/9105}},
}

分类： [信息时代](https://kexue.fm/category/Big-Data)    标签： [语言模型](https://kexue.fm/tag/%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/), [attention](https://kexue.fm/tag/attention/), [位置编码](https://kexue.fm/tag/%E4%BD%8D%E7%BD%AE%E7%BC%96%E7%A0%81/)[30 评论](https://kexue.fm/archives/9105#comments)

< [如何训练你的准确率？](https://kexue.fm/archives/9098) \| [生成扩散模型漫谈（一）：DDPM = 拆楼 + 建楼](https://kexue.fm/archives/9119) >

### 你也许还对下面的内容感兴趣

- [Transformer升级之路：20、MLA究竟好在哪里？](https://kexue.fm/archives/10907)
- [Transformer升级之路：19、第二类旋转位置编码](https://kexue.fm/archives/10862)
- [细水长flow之TARFLOW：流模型满血归来？](https://kexue.fm/archives/10667)
- [“闭门造车”之多模态思路浅谈（三）：位置编码](https://kexue.fm/archives/10352)
- [Decoder-only的LLM为什么需要位置编码？](https://kexue.fm/archives/10347)
- [Monarch矩阵：计算高效的稀疏型矩阵分解](https://kexue.fm/archives/10249)
- [Transformer升级之路：18、RoPE的底数选择原则](https://kexue.fm/archives/10122)
- [缓存与效果的极限拉扯：从MHA、MQA、GQA到MLA](https://kexue.fm/archives/10091)
- [Transformer升级之路：17、多模态位置编码的简单思考](https://kexue.fm/archives/10040)
- [时空之章：将Attention视为平方复杂度的RNN](https://kexue.fm/archives/10017)

[发表你的看法](https://kexue.fm/archives/9105#comment_form)

1. [«](https://kexue.fm/archives/9105/comment-page-1#comments)
2. [1](https://kexue.fm/archives/9105/comment-page-1#comments)
3. [2](https://kexue.fm/archives/9105/comment-page-2#comments)

Allen7575

January 26th, 2024

關於式(5)，我腦洞大開，想到若使用複數來表示如何？

$$b\_{m,n} = c\_{m,n} + id\_{m,n}$$

$$a\_{m,n} = \\frac{e^{b\_{m,n}}}{\\sqrt{\\sum\\limits\_n e^{2b\_{m,n}}}} $$
$$ = \\frac{e^{c\_{m,n} + id\_{m,n}}}{\\sqrt{\\sum\\limits\_n e^{2(c\_{m,n} + id\_{m,n})}}}$$

$$ let \ e^{c\_{m,n} + id\_{m,n}} = x\_{m,n} + iy\_{m,n} $$

$$ = \\frac{x\_{m,n} + iy\_{m,n}}{\\sqrt{\\sum\\limits\_n \|(x\_{m,n} + iy\_{m,n})\|^2}}$$

$$ = \\frac{x\_{m,n}}{\\sqrt{\\sum\\limits\_n \|(x\_{m,n} + iy\_{m,n})\|^2}} + i \\frac{y\_{m,n}}{\\sqrt{\\sum\\limits\_n \|(x\_{m,n} + iy\_{m,n})\|^2}}$$

$$ = \\bar{x}\_{m,n} + i \\bar{y}\_{m,n}$$

由於虛部基本上就是另外一個維度，所以我可以用兩組 Dense 輸出兩組 attention score, 一組代表實部$x$，一組代表虛部$y$，最後兩個 attention 分別乘上 value, 得到兩組 context，再串接起來。

以上的形式，類似量子力學中機率波的形式，而機率波震幅的平方，在物理上即代表機率。因此，可以取 $p\_{m,n} = \\bar{x}\_{m,n}^2 + \\bar{y}\_{m,n}^2 $ 為 $ query\_ m 對 value\_n$ 的機率。

在物理上，虛部代表波的「相位」，所以或許也可以這樣解釋：
使用 $l\_2$ norm 隱含了學習到複數的相位，而相位可代表角度的訊息，也就是某種位置訊息。因此能夠完成你的探針實驗。

我實際做了實驗，確實是可以訓練起來的，而且完全不需要使用 softmax 。只要注意避免因為padding 造成 $l\_2$ norm 為0，需要在padding 後加上一個很小的值。

不過，雖然不用softmax，因為計算$l\_2$ norm 需要先計算出attention socre，所以不能用來做linear attention。

至於效果就不能確定了，因為我只有小資料，也沒有大量GPU，沒辦法做評測。

[回复评论](https://kexue.fm/archives/9105/comment-page-2?replyTo=23603#respond-post-9105)

[苏剑林](https://kexue.fm) 发表于
January 28th, 2024

这个看起来比较适合纯复数的神经网络，直接将复数attention矩阵乘以value就好，不用分两块。要不然实数中夹点复数，总感觉不大优雅的样子

[回复评论](https://kexue.fm/archives/9105/comment-page-2?replyTo=23617#respond-post-9105)

小小鼠标

September 18th, 2024

有没有可能，transformer并不必要要成为“万能拟合器”，他只需要完成主要关键数据进行抽取。

[回复评论](https://kexue.fm/archives/9105/comment-page-2?replyTo=25239#respond-post-9105)

[苏剑林](https://kexue.fm) 发表于
September 20th, 2024

何为“主要关键数据进行抽取”？为什么“主要关键数据进行抽取”不需要万能拟合能力？

[回复评论](https://kexue.fm/archives/9105/comment-page-2?replyTo=25258#respond-post-9105)

1. [«](https://kexue.fm/archives/9105/comment-page-1#comments)
2. [1](https://kexue.fm/archives/9105/comment-page-1#comments)
3. [2](https://kexue.fm/archives/9105/comment-page-2#comments)

[取消回复](https://kexue.fm/archives/9105#respond-post-9105)

你的大名

电子邮箱

个人网站（选填）

1\. 可以使用LaTeX代码，点击“预览效果”可查看效果；2. 可以通过点击评论楼层编号来引用该楼层；3. 网站可能会有点卡，如非确认评论失败，请不要重复点击提交。

### 内容速览

[简单探针](https://kexue.fm/archives/9105#%E7%AE%80%E5%8D%95%E6%8E%A2%E9%92%88)
[无法胜任](https://kexue.fm/archives/9105#%E6%97%A0%E6%B3%95%E8%83%9C%E4%BB%BB)
[初步方案](https://kexue.fm/archives/9105#%E5%88%9D%E6%AD%A5%E6%96%B9%E6%A1%88)
[去掉分母](https://kexue.fm/archives/9105#%E5%8E%BB%E6%8E%89%E5%88%86%E6%AF%8D)
[新归一化](https://kexue.fm/archives/9105#%E6%96%B0%E5%BD%92%E4%B8%80%E5%8C%96)
[峰回路转](https://kexue.fm/archives/9105#%E5%B3%B0%E5%9B%9E%E8%B7%AF%E8%BD%AC)
[曲终人散](https://kexue.fm/archives/9105#%E6%9B%B2%E7%BB%88%E4%BA%BA%E6%95%A3)

### 智能搜索

支持整句搜索！网站自动使用 [结巴分词](https://github.com/fxsjy/jieba) 进行分词，并结合ngrams排序算法给出合理的搜索结果。

### 热门标签

[生成模型](https://kexue.fm/tag/%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B/) [attention](https://kexue.fm/tag/attention/) [优化](https://kexue.fm/tag/%E4%BC%98%E5%8C%96/) [语言模型](https://kexue.fm/tag/%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/) [模型](https://kexue.fm/tag/%E6%A8%A1%E5%9E%8B/) [网站](https://kexue.fm/tag/%E7%BD%91%E7%AB%99/) [概率](https://kexue.fm/tag/%E6%A6%82%E7%8E%87/) [梯度](https://kexue.fm/tag/%E6%A2%AF%E5%BA%A6/) [转载](https://kexue.fm/tag/%E8%BD%AC%E8%BD%BD/) [微分方程](https://kexue.fm/tag/%E5%BE%AE%E5%88%86%E6%96%B9%E7%A8%8B/) [天象](https://kexue.fm/tag/%E5%A4%A9%E8%B1%A1/) [矩阵](https://kexue.fm/tag/%E7%9F%A9%E9%98%B5/) [分析](https://kexue.fm/tag/%E5%88%86%E6%9E%90/) [深度学习](https://kexue.fm/tag/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/) [积分](https://kexue.fm/tag/%E7%A7%AF%E5%88%86/) [python](https://kexue.fm/tag/python/) [优化器](https://kexue.fm/tag/%E4%BC%98%E5%8C%96%E5%99%A8/) [力学](https://kexue.fm/tag/%E5%8A%9B%E5%AD%A6/) [无监督](https://kexue.fm/tag/%E6%97%A0%E7%9B%91%E7%9D%A3/) [扩散](https://kexue.fm/tag/%E6%89%A9%E6%95%A3/) [几何](https://kexue.fm/tag/%E5%87%A0%E4%BD%95/) [节日](https://kexue.fm/tag/%E8%8A%82%E6%97%A5/) [生活](https://kexue.fm/tag/%E7%94%9F%E6%B4%BB/) [文本生成](https://kexue.fm/tag/%E6%96%87%E6%9C%AC%E7%94%9F%E6%88%90/) [数论](https://kexue.fm/tag/%E6%95%B0%E8%AE%BA/)

### 随机文章

- [费曼积分法——积分符号内取微分(1)](https://kexue.fm/archives/1615)
- [轻微的扰动——摄动法简介(2)](https://kexue.fm/archives/1909)
- [这个星期对微分方程的认识](https://kexue.fm/archives/1045)
- [齐次对称多项式初等表示的新尝试](https://kexue.fm/archives/2020)
- [AdaX优化器浅析（附开源实现）](https://kexue.fm/archives/7387)
- [fashion mnist的一个baseline (MobileNet 95%)](https://kexue.fm/archives/4556)
- [“闭门造车”之多模态思路浅谈（二）：自回归](https://kexue.fm/archives/10197)
- [“天地图”试用——很细致，有瑕疵](https://kexue.fm/archives/1039)
- [两男一女分享2009年诺贝尔化学奖](https://kexue.fm/archives/171)
- [生成扩散模型漫谈（二十七）：将步长作为条件输入](https://kexue.fm/archives/10617)

### 最近评论

- [rpsun](https://kexue.fm/archives/10699/comment-page-1#comment-27808): 这样似乎与传统的经验正交函数之类的有相似之处。把样本的平均值减掉之后做正交分解。那么如果单纯地...
- [贵阳机场接机](https://kexue.fm/archives/1490/comment-page-1#comment-27807): 怎么不更新啦
- [czvzb](https://kexue.fm/archives/10958/comment-page-1#comment-27806): 具身智能模型目前主流也是在使用扩散和流匹配这类方法来预测动作。
苏神推荐你看这几篇文章：
1....
- [Shawn\_yang](https://kexue.fm/archives/10945/comment-page-1#comment-27805): 不好意思，以为网页卡了0.0点了三下
- [Shawn\_yang](https://kexue.fm/archives/10945/comment-page-1#comment-27804): 苏神，关于您所说的：“推理阶段可以事先预估Routed Expert的实际分布，只要细致地进行...
- [Shawn\_yang](https://kexue.fm/archives/10945/comment-page-1#comment-27803): 苏神，关于您所说的：“推理阶段可以事先预估Routed Expert的实际分布，只要细致地进行...
- [Shawn\_yang](https://kexue.fm/archives/10945/comment-page-1#comment-27802): 苏神，关于您所说的：“推理阶段可以事先预估Routed Expert的实际分布，只要细致地进行...
- [OceanYU](https://kexue.fm/archives/9164/comment-page-4#comment-27801): 您好，关于由式（7）推导出高斯分布，我这里有一点问题，式（7）只能保证关于x\_t-1是二次函数...
- [jorjiang](https://kexue.fm/archives/10907/comment-page-2#comment-27800): 训练和prefill这个compute-bound阶段不做矩阵吸收，这个用我这个解释更好理解了...
- [amy](https://kexue.fm/archives/10907/comment-page-2#comment-27799): 苏老师，您有关注傅里叶旋转位置编码这篇工作吗，想知道您对这篇工作的看法是什么，这篇工作可以wo...

### 友情链接

- [Cool Papers](https://papers.cool)
- [数学研发](https://bbs.emath.ac.cn)
- [Seatop](http://www.seatop.com.cn/)
- [Xiaoxia](https://xiaoxia.org/)
- [积分表-网络版](https://kexue.fm/sci/integral/index.html)
- [丝路博傲](http://blog.dvxj.com/)
- [ph4ntasy 饭特稀](http://www.ph4ntasy.com/)
- [数学之家](http://www.2math.cn/)
- [有趣天文奇观](http://interesting-sky.china-vo.org/)
- [TwistedW](http://www.twistedwg.com/)
- [godweiyang](https://godweiyang.com/)
- [AI柠檬](https://blog.ailemon.net/)
- [王登科-DK博客](https://greatdk.com)
- [ESON](https://blog.eson.org/)
- [枫之羽](https://fzhiy.net/)
- [Mathor's blog](https://wmathor.com/)
- [coding-zuo](https://coding-zuo.github.io/)
- [博科园](https://www.bokeyuan.net/)
- [孔皮皮的博客](https://www.kppkkp.top/)
- [运鹏的博客](https://yunpengtai.top/)
- [jiming.site](https://jiming.site/)
- [OmegaXYZ](https://www.omegaxyz.com/)
- [Blog by Eacls](https://www.eacls.top/)
- [EAI猩球](https://www.robotech.ink/)
- [文举的博客](https://liwenju0.com/)
- [用代码打点酱油](https://bruceyuan.com/)
- [申请链接](https://kexue.fm/links.html)

本站采用创作共用版权协议，要求署名、非商业用途和保持一致。转载本站内容必须也遵循“ [署名-非商业用途-保持一致](http://creativecommons.org/licenses/by-nc-nd/2.5/cn/)”的创作共用协议。
© 2009-2025 Scientific Spaces. All rights reserved. Theme by [laogui](http://www.laogui.com). Powered by [Typecho](http://typecho.org). 备案号: [粤ICP备09093259号-1/2](https://beian.miit.gov.cn/)。