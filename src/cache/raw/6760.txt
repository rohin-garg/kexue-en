![MobileSideBar](https://kexue.fm/usr/themes/geekg/images/slide-button.png)

## SEARCH

## MENU

- [打赏](https://kexue.fm/reward.html)
- [公式](https://kexue.fm/latex.html)
- [天象](https://kexue.fm/ac.html)
- [链接](https://kexue.fm/links.html)
- [时光](https://kexue.fm/me.html)
- [博览](https://kexue.fm/science.html)
- [归档](https://kexue.fm/content.html)

## CATEGORIES

- [千奇百怪](https://kexue.fm/category/Everything)
- [天文探索](https://kexue.fm/category/Astronomy)
- [数学研究](https://kexue.fm/category/Mathematics)
- [物理化学](https://kexue.fm/category/Phy-chem)
- [信息时代](https://kexue.fm/category/Big-Data)
- [生物自然](https://kexue.fm/category/Biology)
- [图片摄影](https://kexue.fm/category/Photograph)
- [问题百科](https://kexue.fm/category/Questions)
- [生活/情感](https://kexue.fm/category/Life-Feeling)
- [资源共享](https://kexue.fm/category/Resources)

## NEWPOSTS

- [SVD的导数](https://kexue.fm/archives/10878)
- [智能家居之手搓一套能接入米家的零冷水装置](https://kexue.fm/archives/10869)
- [Transformer升级之路：1...](https://kexue.fm/archives/10862)
- [矩阵的有效秩（Effective ...](https://kexue.fm/archives/10847)
- [通过梯度近似寻找Normaliza...](https://kexue.fm/archives/10831)
- [MoE环游记：4、难处应当多投入](https://kexue.fm/archives/10815)
- [高阶muP：更简明但更高明的谱条件缩放](https://kexue.fm/archives/10795)
- [初探muP：超参数的跨模型尺度迁移规律](https://kexue.fm/archives/10770)
- [MoE环游记：3、换个思路来分配](https://kexue.fm/archives/10757)
- [Muon续集：为什么我们选择尝试M...](https://kexue.fm/archives/10739)

## COMMENTS

- [SunlightZero: 在《Step-by-Step Diffusion: An El...](https://kexue.fm/archives/9164/comment-page-4#comment-27497)
- [苏剑林: 1、明白了，我将$q\_{\\phi}(z\|x)$看成$q\_{\\p...](https://kexue.fm/archives/5239/comment-page-3#comment-27496)
- [Suahi: 谢谢苏老师的回复！1\. 首先回复您为什么ELBO不带KL，并不...](https://kexue.fm/archives/5239/comment-page-3#comment-27493)
- [eular: 是的，当$k$比较大时会出现这种情况。](https://kexue.fm/archives/10373/comment-page-1#comment-27492)
- [苏剑林: 肯定是$\\mathbb{E}\_{x \\sim p\_{data}...](https://kexue.fm/archives/5239/comment-page-3#comment-27491)
- [苏剑林: 你的意思是$\\lambda(\\boldsymbol{x}) <...](https://kexue.fm/archives/10373/comment-page-1#comment-27490)
- [苏剑林: 好问题，下一篇文章可能会讨论这个问题](https://kexue.fm/archives/10735/comment-page-1#comment-27489)
- [苏剑林: 不大熟悉，但都diffusion forcing了，还有必要CM吗](https://kexue.fm/archives/10633/comment-page-1#comment-27488)
- [苏剑林: 简单看了一下，好像没什么新东西呀？还是我看漏了什么？](https://kexue.fm/archives/10711/comment-page-2#comment-27487)
- [苏剑林: 感谢指出，已修正。](https://kexue.fm/archives/8601/comment-page-1#comment-27486)

## USERLOGIN

- [登录](https://kexue.fm/admin/login.php)

[科学空间\|Scientific Spaces](https://kexue.fm)

- [登录](https://kexue.fm/admin/login.php)
- [打赏](https://kexue.fm/reward.html)
- [公式](https://kexue.fm/latex.html)
- [天象](https://kexue.fm/ac.html)
- [链接](https://kexue.fm/links.html)
- [时光](https://kexue.fm/me.html)
- [博览](https://kexue.fm/science.html)
- [归档](https://kexue.fm/content.html)

渴望成为一个小飞侠

- [![](https://kexue.fm/usr/themes/geekg/images/rss.png)\
\
欢迎订阅](https://kexue.fm/feed)
- [![](https://kexue.fm/usr/themes/geekg/images/mail.png)\
\
个性邮箱](https://kexue.fm/archives/119)
- [![](https://kexue.fm/usr/themes/geekg/images/Saturn.png)\
\
天象信息](https://kexue.fm/ac.html)
- [![](https://kexue.fm/usr/themes/geekg/images/iss.png)\
\
观测ISS](https://kexue.fm/archives/41)
- [![](https://kexue.fm/usr/themes/geekg/images/pi.png)\
\
LaTeX](https://kexue.fm/latex.html)
- [![](https://kexue.fm/usr/themes/geekg/images/mlogo.png)\
\
关于博主](https://kexue.fm/me.html)

欢迎访问“科学空间”，这里将与您共同探讨自然科学，回味人生百态；也期待大家的分享～

- [**千奇百怪** Everything](https://kexue.fm/category/Everything)
- [**天文探索** Astronomy](https://kexue.fm/category/Astronomy)
- [**数学研究** Mathematics](https://kexue.fm/category/Mathematics)
- [**物理化学** Phy-chem](https://kexue.fm/category/Phy-chem)
- [**信息时代** Big-Data](https://kexue.fm/category/Big-Data)
- [**生物自然** Biology](https://kexue.fm/category/Biology)
- [**图片摄影** Photograph](https://kexue.fm/category/Photograph)
- [**问题百科** Questions](https://kexue.fm/category/Questions)
- [**生活/情感** Life-Feeling](https://kexue.fm/category/Life-Feeling)
- [**资源共享** Resources](https://kexue.fm/category/Resources)

- [**千奇百怪**](https://kexue.fm/category/Everything)
- [**天文探索**](https://kexue.fm/category/Astronomy)
- [**数学研究**](https://kexue.fm/category/Mathematics)
- [**物理化学**](https://kexue.fm/category/Phy-chem)
- [**信息时代**](https://kexue.fm/category/Big-Data)
- [**生物自然**](https://kexue.fm/category/Biology)
- [**图片摄影**](https://kexue.fm/category/Photograph)
- [**问题百科**](https://kexue.fm/category/Questions)
- [**生活/情感**](https://kexue.fm/category/Life-Feeling)
- [**资源共享**](https://kexue.fm/category/Resources)

[首页](https://kexue.fm) [信息时代](https://kexue.fm/category/Big-Data) VQ-VAE的简明介绍：量子化自编码器

24Jun

# [VQ-VAE的简明介绍：量子化自编码器](https://kexue.fm/archives/6760)

By 苏剑林 \|
2019-06-24 \|
380943位读者\|

印象中很早之前就看到过VQ-VAE，当时对它并没有什么兴趣，而最近有两件事情重新引起了我对它的兴趣。一是 [VQ-VAE-2](https://papers.cool/arxiv/1906.00446) 实现了能够匹配BigGAN的生成效果（来自 [机器之心的报道](https://mp.weixin.qq.com/s/GJr-YtV84eV1KbkyVSkcBA)）；二是我最近看一篇NLP论文 [《Unsupervised Paraphrasing without Translation》](https://papers.cool/arxiv/1905.12752) 时发现里边也用到了VQ-VAE。这两件事情表明VQ-VAE应该是一个颇为通用和有意思的模型，所以我决定好好读读它。

[![个人复现的VQ-VAE在CelebA上的重构效果。可以留意到细节保留得还不错，但稍微放大后能留意到仍有一些模糊感。](https://kexue.fm/usr/uploads/2019/06/3057025890.png)](https://kexue.fm/usr/uploads/2019/06/3057025890.png)

个人复现的VQ-VAE在CelebA上的重构效果。可以留意到细节保留得还不错，但稍微放大后能留意到仍有一些模糊感。

## 模型综述 [\#](https://kexue.fm/archives/6760\#%E6%A8%A1%E5%9E%8B%E7%BB%BC%E8%BF%B0)

VQ-VAE（Vector Quantised - Variational AutoEncoder）首先出现在论 [《Neural Discrete Representation Learning》](https://papers.cool/arxiv/1711.00937)，跟VQ-VAE-2一样，都是Google团队的大作。

### 有趣却玄虚 [\#](https://kexue.fm/archives/6760\#%E6%9C%89%E8%B6%A3%E5%8D%B4%E7%8E%84%E8%99%9A)

作为一个自编码器，VQ-VAE的一个明显特征是它编码出的编码向量是离散的，换句话说，它最后得到的编码向量的每个元素都是一个整数，这也就是“Quantised”的含义，我们可以称之为“量子化”（跟量子力学的“量子”一样，都包含离散化的意思）。

明明整个模型都是连续的、可导的，但最终得到的编码向量却是离散的，并且重构效果看起来还很清晰（如文章开头的图），这至少意味着VQ-VAE会包含一些有意思、有价值的技巧，值得我们学习一番。不过，读了原论文之后，总感觉原论文写得有点难懂。这种难懂不是像ON-LSTM原论文那样的晦涩难懂，而是有种“故弄玄虚”的感觉。

首先，你读完整篇论文就会明白，VQ-VAE其实就是一个AE（自编码器）而不是VAE（变分自编码器），我不知道作者出于什么目的非得用概率的语言来沾VAE的边，这明显加大了读懂这篇论文的难度。其次，VQ-VAE的核心步骤之一是Straight-Through Estimator，这是将隐变量离散化后的优化技巧，在原论文中没有稍微详细的讲解，以至于必须看源码才能更好地知道它说啥。最后，论文的核心思想也没有很好地交代清楚，给人的感觉是纯粹在介绍模型本身而没有介绍模型思想。

### PixelCNN [\#](https://kexue.fm/archives/6760\#PixelCNN)

要追溯VQ-VAE的思想，就不得不谈到自回归模型。可以说，VQ-VAE做生成模型的思路，源于 [PixelRNN](https://papers.cool/arxiv/1601.06759)、 [PixelCNN](https://papers.cool/arxiv/1606.05328) 之类的自回归模型，这类模型留意到我们要生成的图像，实际上是离散的而不是连续的。以cifar10的图像为例，它是$32\\times 32$大小的3通道图像，换言之它是一个$32\\times 32\\times 3$的矩阵，矩阵的每个元素是0～255的任意一个整数，这样一来，我们可以将它看成是一个长度为$32\\times 32\\times 3=3072$的句子，而词表的大小是256，从而用语言模型的方法，来逐像素地、递归地生成一张图片（传入前面的所有像素，来预测下一个像素），这就是所谓的自回归方法：

\\begin{equation}p(x)=p(x\_1)p(x\_2\|x\_1)\\dots p(x\_{3n^2}\|x\_1,x\_2,\\dots,x\_{3n^2-1})\\end{equation}

其中$p(x\_1),p(x\_2\|x\_1),\\dots,p(x\_{3n^2}\|x\_1,x\_2,\\dots,x\_{3n^2-1})$每一个都是256分类问题，只不过所依赖的条件有所不同。

PixelRNN、PixelCNN网上都有一定的资料介绍了，这里不再赘述，我感觉其实也可以蹭着Bert的热潮，去搞个PixelAtt（Attention）来做它。自回归模型的研究主要集中在两方面：一方面是如何设计这个递归顺序，使得模型可以更好地生成采样，因为图像的序列不是简单的一维序列，它至少是二维的，更多情况是三维的，这种情况下你是“从左往右再从上到下”、“从上到下再从左往右”、“先中间再四周”或者是其他顺序，都很大程度上影响着生成效果；另一方面是研究如何加速采样过程。在我读到的文献里，自回归模型比较新的成果是ICLR 2019的工作 [《Generating High Fidelity Images with Subscale Pixel Networks and Multidimensional Upscaling》](https://papers.cool/arxiv/1812.01608)。

自回归的方法很稳妥，也能有效地做概率估计，但它有一个最致命的缺点： **慢**。因为它是逐像素地生成的，所以要每个像素地进行随机采样，上面举例的cifar10已经算是小图像的，目前做图像生成好歹也要做到$128\\times 128\\times 3$的才有说服力了吧，这总像素接近5万个（想想看要生成一个长度为5万的句子），真要逐像素生成会非常耗时。而且这么长的序列，不管是RNN还是CNN模型都无法很好地捕捉这么长的依赖。

原始的自回归还有一个问题，就是割裂了类别之间的联系。虽然说因为每个像素是离散的，所以看成256分类问题也无妨，但事实上连续像素之间的差别是很小的，纯粹的分类问题捕捉到这种联系。更数学化地说，就是我们的目标函数交叉熵是$-\\log p\_t$，假如目标像素是100，如果我预测成99，因为类别不同了，那么$p\_t$就接近于0，$-\\log p\_t$就很大，从而带来一个很大的损失。但从视觉上来看，像素值是100还是99差别不大，不应该有这么大的损失。

## VQ-VAE [\#](https://kexue.fm/archives/6760\#VQ-VAE)

针对自回归模型的固有毛病，VQ-VAE提出的解决方案是：先降维，然后再对编码向量用PixelCNN建模。

### 降维离散化 [\#](https://kexue.fm/archives/6760\#%E9%99%8D%E7%BB%B4%E7%A6%BB%E6%95%A3%E5%8C%96)

看上去这个方案很自然，似乎没什么特别的，但事实上一点都不自然。

因为PixelCNN生成的离散序列，你想用PixelCNN建模编码向量，那就意味着编码向量也是离散的才行。而我们常见的降维手段，比如自编码器，生成的编码向量都是连续性变量，无法直接生成离散变量。同时，生成离散型变量往往还意味着存在梯度消失的问题。还有，降维、重构这个过程，如何保证重构之后出现的图像不失真？如果失真得太严重，甚至还比不上普通的VAE的话，那么VQ-VAE也没什么存在价值了。

幸运的是，VQ-VAE确实提供了有效的训练策略解决了这两个问题。

### 最邻近重构 [\#](https://kexue.fm/archives/6760\#%E6%9C%80%E9%82%BB%E8%BF%91%E9%87%8D%E6%9E%84)

在VQ-VAE中，一张$n\\times n\\times 3$的图片$x$先被传入一个$encoder$中，得到连续的编码向量$z$：

\\begin{equation}z = encoder(x)\\end{equation}

这里的$z$是一个大小为$d$的向量。另外，VQ-VAE还维护一个Embedding层，我们也可以称为编码表，记为

\\begin{equation}E = \[e\_1, e\_2, \\dots, e\_K\]\\end{equation}

这里每个$e\_i$都是一个大小为$d$的向量。接着，VQ-VAE通过最邻近搜索，将$z$映射为这$K$个向量之一：

\\begin{equation}z\\to e\_k,\\quad k = \\mathop{\\text{argmin}}\_j \\Vert z - e\_j\\Vert\_2\\end{equation}

我们可以将$z$对应的编码表向量记为$z\_q$，我们认为$z\_q$才是最后的编码结果。最后将$z\_q$传入一个$decoder$，希望重构原图$\\hat{x}=decoder(z\_q)$。

整个流程是：

\\begin{equation}x\\xrightarrow{encoder} z \\xrightarrow{\\text{最邻近}} z\_q \\xrightarrow{decoder}\\hat{x}\\end{equation}

这样一来，因为$z\_q$是编码表$E$中的向量之一，所以它实际上就等价于$1,2,\\dots,K$这$K$个整数之一，因此这整个流程相当于将整张图片编码为了一个整数。

当然，上述过程是比较简化的，如果只编码为一个向量，重构时难免失真，而且泛化性难以得到保证。所以实际编码时直接用多层卷积将$x$编码为$m\\times m$个大小为$d$的向量：

\\begin{equation}z = \\begin{pmatrix}z\_{11} & z\_{12} & \\dots & z\_{1m}\\\

z\_{21} & z\_{22} & \\dots & z\_{2m}\\\

\\vdots & \\vdots & \\ddots & \\vdots\\\

z\_{m1} & z\_{m2} & \\dots & z\_{mm}\\\

\\end{pmatrix}\\end{equation}

也就是说，$z$的总大小为$m\\times m\\times d$，它依然保留着位置结构，然后每个向量都用前述方法映射为编码表中的一个，就得到一个同样大小的$z\_q$，然后再用它来重构。这样一来， **$z\_q$也等价于一个$m\\times m$的整数矩阵，这就实现了离散型编码**。

### 自行设计梯度 [\#](https://kexue.fm/archives/6760\#%E8%87%AA%E8%A1%8C%E8%AE%BE%E8%AE%A1%E6%A2%AF%E5%BA%A6)

我们知道，如果是普通的自编码器，直接用下述loss进行训练即可：

\\begin{equation}\\Vert x - decoder(z)\\Vert\_2^2\\end{equation}

但是，在VQ-VAE中，我们用来重构的是$z\_q$而不是$z$，那么似乎应该用这个loss才对：

\\begin{equation}\\Vert x - decoder(z\_q)\\Vert\_2^2\\end{equation}

但问题是$z\_q$的构建过程包含了$\\text{argmin}$，这个操作是没梯度的，所以如果用第二个loss的话，我们没法更新$encoder$。

换言之，我们的目标其实是$\\Vert x - decoder(z\_q)\\Vert\_2^2$最小，但是却不好优化，而$\\Vert x - decoder(z)\\Vert\_2^2$容易优化，但却不是我们的优化目标。那怎么办呢？当然，一个很粗暴的方法是两个都用：

\\begin{equation}\\Vert x - decoder(z)\\Vert\_2^2 + \\Vert x - decoder(z\_q)\\Vert\_2^2\\end{equation}

但这样并不好，因为最小化$\\Vert x - decoder(z)\\Vert\_2^2$并不是我们的目标，会带来额外的约束。

VQ-VAE使用了一个很精巧也很直接的方法，称为Straight-Through Estimator，你也可以称之为“直通估计”，它最早源于Benjio的论文 [《Estimating or Propagating Gradients Through Stochastic Neurons for Conditional Computation》](https://papers.cool/arxiv/1308.3432)，在VQ-VAE原论文中也是直接抛出这篇论文而没有做什么讲解。但事实上直接读这篇原始论文是一个很不友好的选择，还不如直接读源代码。

事实上Straight-Through的思想很简单，就是前向传播的时候可以用想要的变量（哪怕不可导），而反向传播的时候，用你自己为它所设计的梯度。根据这个思想，我们设计的目标函数是：

\\begin{equation}\\Vert x - decoder(z + sg\[z\_q - z\])\\Vert\_2^2\\end{equation}

其中$sg$是stop gradient的意思，就是不要它的梯度。这样一来，前向传播计算（求loss）的时候，就直接等价于$decoder(z + z\_q - z)=decoder(z\_q)$，然后反向传播（求梯度）的时候，由于$z\_q - z$不提供梯度，所以它也等价于$decoder(z)$，这个就允许我们对$encoder$进行优化了。

顺便说一下，基于这个思想，我们可以为很多函数自己自定义梯度，比如$x + sg\[\\text{relu}(x) - x\]$就是将$\\text{relu}(x)$的梯度定义为恒为1，但是在误差计算时又跟$\\text{relu}(x)$本身等价。当然，用同样的方法我们可以随便指定一个函数的梯度，至于有没有实用价值，则要具体任务具体分析了。

### 维护编码表 [\#](https://kexue.fm/archives/6760\#%E7%BB%B4%E6%8A%A4%E7%BC%96%E7%A0%81%E8%A1%A8)

要注意，根据VQ-VAE的最邻近搜索的设计，我们应该期望$z\_q$和$z$是很接近的（事实上编码表$E$的每个向量类似各个$z$的聚类中心出现），但事实上未必如此，即使$\\Vert x - decoder(z)\\Vert\_2^2$和$\\Vert x - decoder(z\_q)\\Vert\_2^2$都很小，也不意味着$z\_q$和$z$差别很小（即$f(z\_1)=f(z\_2)$不意味着$z\_1 = z\_2$）。

所以，为了让$z\_q$和$z$更接近，我们可以直接地将$\\Vert z - z\_q\\Vert\_2^2$加入到loss中：

\\begin{equation}\\Vert x - decoder(z + sg\[z\_q - z\])\\Vert\_2^2 + \\beta \\Vert z - z\_q\\Vert\_2^2\\end{equation}

除此之外，还可以做得更仔细一些。由于编码表（$z\_q$）相对是比较自由的，而$z$要尽力保证重构效果，所以我们应当尽量“让$z\_q$去靠近$z$”而不是“让$z$去靠近$z\_q$”，而因为$\\Vert z\_q - z\\Vert\_2^2$的梯度等于对$z\_q$的梯度加上对$z$的梯度，所以我们将它等价地分解为

\\begin{equation}\\Vert sg\[z\] - z\_q\\Vert\_2^2 + \\Vert z - sg\[z\_q\]\\Vert\_2^2\\end{equation}

第一项相等于固定$z$，让$z\_q$靠近$z$，第二项则反过来固定$z\_q$，让$z$靠近$z\_q$。注意这个“等价”是对于反向传播（求梯度）来说的，对于前向传播（求loss）它是原来的两倍。根据我们刚才的讨论，我们希望“让$z\_q$去靠近$z$”多于“让$z$去靠近$z\_q$”，所以可以调一下最终的loss比例：

\\begin{equation}\\Vert x - decoder(z + sg\[z\_q - z\])\\Vert\_2^2 + \\beta \\Vert sg\[z\] - z\_q\\Vert\_2^2 + \\gamma \\Vert z - sg\[z\_q\]\\Vert\_2^2\\end{equation}

其中$\\gamma < \\beta$，在原论文中使用的是$\\gamma = 0.25 \\beta$。

（注：还可以用滑动平均的方式更新编码表，详情请看原论文。）

### 拟合编码分布 [\#](https://kexue.fm/archives/6760\#%E6%8B%9F%E5%90%88%E7%BC%96%E7%A0%81%E5%88%86%E5%B8%83)

经过上述一大通设计之后，我们终于将图片编码为了$m\\times m$的整数矩阵了，由于这个$m\\times m$的矩阵一定程度上也保留了原来输入图片的位置信息，所以我们可以用自回归模型比如PixelCNN，来对编码矩阵进行拟合（即建模先验分布）。通过PixelCNN得到编码分布后，就可以随机生成一个新的编码矩阵，然后通过编码表$E$映射为3维的实数矩阵$z\_q$（行\*列\*编码维度），最后经过$deocder$得到一张图片。

一般来说，现在的$m\\times m$比原来的$n\\times n\\times 3$要小得多，比如我在用CelebA数据做实验的时候，原来$128\\times 128\\times 3$的图可以编码为$32\\times 32$的编码而基本不失真，所以用自回归模型对编码矩阵进行建模，要比直接对原始图片进行建模要容易得多。

### 个人的复现 [\#](https://kexue.fm/archives/6760\#%E4%B8%AA%E4%BA%BA%E7%9A%84%E5%A4%8D%E7%8E%B0)

这是自己用Keras实现的VQ-VAE（Python 2.7 + Tensorflow 1.8 + Keras 2.2.4，其中模型部分参考了 [这个](https://github.com/nadavbh12/VQ-VAE)）：

> **[https://github.com/bojone/vae/blob/master/vq\_vae\_keras.py](https://github.com/bojone/vae/blob/master/vq_vae_keras.py)**

这个脚本的正文部分只包含VQ-VAE的编码和重构（文章开头的图就是笔者用这个脚本重构的，可见重构效果还可以），没有包含用PixelCNN建模先验分布。不过最后的注释那里包含了一个用Attention来建模先验分布的例子，用Attention建模先验分布后，随机采样的效果如下：

[![个人用PixelAtt建模先验分布后的随机采样效果（随机挑选的，没有经过筛选）](https://kexue.fm/usr/uploads/2019/06/2483775767.png)](https://kexue.fm/usr/uploads/2019/06/2483775767.png)

个人用PixelAtt建模先验分布后的随机采样效果（随机挑选的，没有经过筛选）

效果图一定程度上表明这样的随机采样是可行的，但是这样的生成效果不能说很好。我用PixelAtt而不是PixelCNN的原因是在我的复现里PixelCNN效果比PixelAtt还差得多，所以PixelAtt是有一定优势的，但缺点是PixelAtt太耗显存，容易OOM。不过我个人的复现不够好也不意味着这套方法不够好，可能是我没调好的原因，也能使网络不够深之类的。我个人是比较看好这种离散化的编码研究的。

## 最后的总结 [\#](https://kexue.fm/archives/6760\#%E6%9C%80%E5%90%8E%E7%9A%84%E6%80%BB%E7%BB%93)

到此，总算把VQ-VAE用自己认为比较好的方式讲清楚了。纵观全文，其实没有任何VAE的味道，所以我说它其实就是一个AE，一个编码为离散型向量的AE。它能重构出比较清晰的图像，则是因为它编码时保留了足够大的feature map～

如果弄懂了VQ-VAE，那么它新出的2.0版本也就没什么难理解的了，VQ-VAE-2相比VQ-VAE几乎没有本质上的技术更新，只不过把编码和解码都分两层来做了（一层整体，一层局部），从而使得生成图像的模糊感更少（相比至少是少很多了，但其实你认真看VQ-VAE-2的大图，还是有略微的模糊感的）。

不过值得肯定的是，VQ-VAE整个模型还是挺有意思，离散型编码、用Straight-Through的方法为梯度赋值等新奇特点，非常值得我们认真学习，能加深我们对深度学习的模型和优化的认识（梯度你都能设计了，还担心设计不好模型吗？）。

_**转载到请包括本文地址：** [https://kexue.fm/archives/6760](https://kexue.fm/archives/6760)_

_**更详细的转载事宜请参考：**_ [《科学空间FAQ》](https://kexue.fm/archives/6508#%E6%96%87%E7%AB%A0%E5%A6%82%E4%BD%95%E8%BD%AC%E8%BD%BD/%E5%BC%95%E7%94%A8)

**如果您还有什么疑惑或建议，欢迎在下方评论区继续讨论。**

**如果您觉得本文还不错，欢迎 [分享](https://kexue.fm/archives/6760#share)/ [打赏](https://kexue.fm/archives/6760#pay) 本文。打赏并非要从中获得收益，而是希望知道科学空间获得了多少读者的真心关注。当然，如果你无视它，也不会影响你的阅读。再次表示欢迎和感谢！**

打赏

![科学空间](https://kexue.fm/usr/themes/geekg/payment/wx.png)

微信打赏

![科学空间](https://kexue.fm/usr/themes/geekg/payment/zfb.png)

支付宝打赏

因为网站后台对打赏并无记录，因此欢迎在打赏时候备注留言。

你还可以 [**点击这里**](http://mail.qq.com/cgi-bin/qm_share?t=qm_mailme&email=tN7d1drY3drrx8H0xcWa19vZ) 或在下方评论区留言来告知你的建议或需求。

**如果您需要引用本文，请参考：**

苏剑林. (Jun. 24, 2019). 《VQ-VAE的简明介绍：量子化自编码器 》\[Blog post\]. Retrieved from [https://kexue.fm/archives/6760](https://kexue.fm/archives/6760)

@online{kexuefm-6760,

        title={VQ-VAE的简明介绍：量子化自编码器},

        author={苏剑林},

        year={2019},

        month={Jun},

        url={\\url{https://kexue.fm/archives/6760}},

}

分类： [信息时代](https://kexue.fm/category/Big-Data)    标签： [无监督](https://kexue.fm/tag/%E6%97%A0%E7%9B%91%E7%9D%A3/), [生成模型](https://kexue.fm/tag/%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B/), [编码](https://kexue.fm/tag/%E7%BC%96%E7%A0%81/), [离散化](https://kexue.fm/tag/%E7%A6%BB%E6%95%A3%E5%8C%96/)[151 评论](https://kexue.fm/archives/6760#comments)

< [简述无偏估计和有偏估计](https://kexue.fm/archives/6747) \| [基于Bert的NL2SQL模型：一个简明的Baseline](https://kexue.fm/archives/6771) >

### 你也许还对下面的内容感兴趣

- [生成扩散模型漫谈（二十九）：用DDPM来离散编码](https://kexue.fm/archives/10711)
- [细水长flow之TARFLOW：流模型满血归来？](https://kexue.fm/archives/10667)
- [生成扩散模型漫谈（二十八）：分步理解一致性模型](https://kexue.fm/archives/10633)
- [生成扩散模型漫谈（二十七）：将步长作为条件输入](https://kexue.fm/archives/10617)
- [生成扩散模型漫谈（二十六）：基于恒等式的蒸馏（下）](https://kexue.fm/archives/10567)
- [VQ的又一技巧：给编码表加一个线性变换](https://kexue.fm/archives/10519)
- [VQ的旋转技巧：梯度直通估计的一般推广](https://kexue.fm/archives/10489)
- [“闭门造车”之多模态思路浅谈（二）：自回归](https://kexue.fm/archives/10197)
- [缓存与效果的极限拉扯：从MHA、MQA、GQA到MLA](https://kexue.fm/archives/10091)
- [生成扩散模型漫谈（二十五）：基于恒等式的蒸馏（上）](https://kexue.fm/archives/10085)

[发表你的看法](https://kexue.fm/archives/6760#comment_form)

1. [«](https://kexue.fm/archives/6760/comment-page-6#comments)
2. [1](https://kexue.fm/archives/6760/comment-page-1#comments)
3. ...
4. [4](https://kexue.fm/archives/6760/comment-page-4#comments)
5. [5](https://kexue.fm/archives/6760/comment-page-5#comments)
6. [6](https://kexue.fm/archives/6760/comment-page-6#comments)
7. [7](https://kexue.fm/archives/6760/comment-page-7#comments)

[zhb](https://github.com/zhb2000)

February 18th, 2024

这么来看的话 VQ-VAE 做的是“图像 tokenizer”的工作，生成模型部分交给自回归模型 PixelCNN 去负责了。

[回复评论](https://kexue.fm/archives/6760/comment-page-7?replyTo=23705#respond-post-6760)

[苏剑林](https://kexue.fm) 发表于
February 21st, 2024

是的。如果你是独立意识到这件事情的话，那么说明想象力相当丰富了～

[回复评论](https://kexue.fm/archives/6760/comment-page-7?replyTo=23730#respond-post-6760)

dididi

February 21st, 2024

苏神，一个小问题，拟合编码分布后，整数矩阵通过编码表E映射所得到的应该不是“浮点数矩阵$z\_q$”,应该是一个“$m \\times m \\times d$的feature map”,然后输入decoder。

[回复评论](https://kexue.fm/archives/6760/comment-page-7?replyTo=23713#respond-post-6760)

[苏剑林](https://kexue.fm) 发表于
February 21st, 2024

好的，我改为了“3维的实数矩阵”

[回复评论](https://kexue.fm/archives/6760/comment-page-7?replyTo=23737#respond-post-6760)

Swaggyzhang

March 18th, 2024

苏神，您在公式（12）前几行提到的“因为$\|\|z\_q−z\|\|\_2^2$的梯度等于对$z\_q$的梯度加上对$z$的梯度”,是否可以理解为：“$\\frac{\\partial \|\| z\_q - z\|\|\_2^2}{\\partial z\_q}\\vec{i} + \\frac{\\partial \|\| z\_q - z\|\|\_2^2}{\\partial z}\\vec{j}$”?

[回复评论](https://kexue.fm/archives/6760/comment-page-7?replyTo=23943#respond-post-6760)

[苏剑林](https://kexue.fm) 发表于
March 21st, 2024

对

[回复评论](https://kexue.fm/archives/6760/comment-page-7?replyTo=23979#respond-post-6760)

Kevin Chen

April 3rd, 2024

苏神，请问一下，如果只是要对图像做tokenization，不考虑用decoder做生成，是不是码表就不是必要的了，例如直接取z向量做一次fsq是不是就可以得到图像token

[回复评论](https://kexue.fm/archives/6760/comment-page-7?replyTo=24059#respond-post-6760)

benjamin

April 18th, 2024

您好，请问一下：

一直很不理解，VAE的提出就是为了解决AE在隐空间不连续的问题，VQ-VAE却是为了解决VAE不离散的问题，是否可以认为离散性：VQ-VAE>AE>VAE

[回复评论](https://kexue.fm/archives/6760/comment-page-7?replyTo=24155#respond-post-6760)

[苏剑林](https://kexue.fm) 发表于
April 18th, 2024

没有正则项的AE，是会导致特征之间的孤立，没有泛化能力，这个算不算你的离散性定义？

[回复评论](https://kexue.fm/archives/6760/comment-page-7?replyTo=24174#respond-post-6760)

lux

April 21st, 2024

请问一下，为什么E中的K个向量就足够表达z中所有类的中心？

[回复评论](https://kexue.fm/archives/6760/comment-page-7?replyTo=24182#respond-post-6760)

[苏剑林](https://kexue.fm) 发表于
April 25th, 2024

这是双向促进的，某种意义上我们也是通过训练迫使它聚成$K$类。

[回复评论](https://kexue.fm/archives/6760/comment-page-7?replyTo=24198#respond-post-6760)

linzangsc

June 8th, 2024

苏神，请教一下，关于zq需要更新的比z快这个说法，有没有更容易理解的解释呢？z和zq不都承担着重建的任务吗，怎么理解zq相对自由？

[回复评论](https://kexue.fm/archives/6760/comment-page-7?replyTo=24522#respond-post-6760)

[苏剑林](https://kexue.fm) 发表于
June 9th, 2024

$z$本身已经有来自Autoencoder的梯度来，所以它关于编码表的梯度可以适当降低，而$z\_q$的唯一梯度就是编码表对齐这部分。

直观来讲，VQ-VAE的训练过程可以分为两部分来理解，一部分是Autoencoder得到$z$，另一部分就是对$z$进行K-Means聚类，$z\_q$就相当于聚类中心，$z$的主要信息来源于Autoencoder的重建，$z\_q$的主要信息来源于$z$的聚类结果。

[回复评论](https://kexue.fm/archives/6760/comment-page-7?replyTo=24532#respond-post-6760)

chaofangbianmian 发表于
April 15th, 2025

苏神，“Zq的唯一梯度就是编码表对齐这部分”，关于这句我有个疑问。Zq为什么没有根据重构的梯度完成参数更新呢？ 我理解查表的过程只是影响了Zq到Z之间梯度的传递，然后通过STE把Zq处的梯度复制到了Z，以此进行后续encoder的参数优化。而不影响Zq根据梯度优化参数。

[回复评论](https://kexue.fm/archives/6760/comment-page-7?replyTo=27402#respond-post-6760)

[苏剑林](https://kexue.fm) 发表于
April 21st, 2025

非常好的问题。其实可以保留decoder给$z\_q$的梯度，但这里仍需要要额外的正则项来促进$z\_q$与$z$的近似性，才能使得VQ近似更好地成立，所以来自decoder的直接梯度反而没那么重要了。

[回复评论](https://kexue.fm/archives/6760/comment-page-7?replyTo=27438#respond-post-6760)

yxj

March 6th, 2025

苏老师，这个编码表E是怎么来的呢

[回复评论](https://kexue.fm/archives/6760/comment-page-7?replyTo=27021#respond-post-6760)

[苏剑林](https://kexue.fm) 发表于
March 8th, 2025

随机初始化然后优化器训练来的。

[回复评论](https://kexue.fm/archives/6760/comment-page-7?replyTo=27036#respond-post-6760)

Yukki

March 10th, 2025

苏老师您好，结合随机初始化和维护编码表过程来看\[ [@苏剑林\|comment-27036](https://kexue.fm/archives/6760/comment-page-7#comment-27036)\]，是不是说明训练出来的codebook本质是随机分布的。训练多个vqvae来生成同一类别的图片，不同vqvae选取的对应codebook向量的index也是不同的？

[回复评论](https://kexue.fm/archives/6760/comment-page-7?replyTo=27067#respond-post-6760)

[苏剑林](https://kexue.fm) 发表于
March 13th, 2025

我不确定你想表达的意思，我猜测你应该是想表达“codebook里的code是无序的”？如果是的话，那么你的理解没有错，重复训练一次，index确实可能不一样。

[回复评论](https://kexue.fm/archives/6760/comment-page-7?replyTo=27099#respond-post-6760)

1. [«](https://kexue.fm/archives/6760/comment-page-6#comments)
2. [1](https://kexue.fm/archives/6760/comment-page-1#comments)
3. ...
4. [4](https://kexue.fm/archives/6760/comment-page-4#comments)
5. [5](https://kexue.fm/archives/6760/comment-page-5#comments)
6. [6](https://kexue.fm/archives/6760/comment-page-6#comments)
7. [7](https://kexue.fm/archives/6760/comment-page-7#comments)

[取消回复](https://kexue.fm/archives/6760#respond-post-6760)

你的大名

电子邮箱

个人网站（选填）

1\. 可以使用LaTeX代码，点击“预览效果”可查看效果；

2\. 可以通过点击评论楼层编号来引用该楼层；

3\. 网站可能会有点卡，如非确认评论失败，请不要重复点击提交。

### 内容速览

[模型综述](https://kexue.fm/archives/6760#%E6%A8%A1%E5%9E%8B%E7%BB%BC%E8%BF%B0)
[有趣却玄虚](https://kexue.fm/archives/6760#%E6%9C%89%E8%B6%A3%E5%8D%B4%E7%8E%84%E8%99%9A)
[PixelCNN](https://kexue.fm/archives/6760#PixelCNN)
[VQ-VAE](https://kexue.fm/archives/6760#VQ-VAE)
[降维离散化](https://kexue.fm/archives/6760#%E9%99%8D%E7%BB%B4%E7%A6%BB%E6%95%A3%E5%8C%96)
[最邻近重构](https://kexue.fm/archives/6760#%E6%9C%80%E9%82%BB%E8%BF%91%E9%87%8D%E6%9E%84)
[自行设计梯度](https://kexue.fm/archives/6760#%E8%87%AA%E8%A1%8C%E8%AE%BE%E8%AE%A1%E6%A2%AF%E5%BA%A6)
[维护编码表](https://kexue.fm/archives/6760#%E7%BB%B4%E6%8A%A4%E7%BC%96%E7%A0%81%E8%A1%A8)
[拟合编码分布](https://kexue.fm/archives/6760#%E6%8B%9F%E5%90%88%E7%BC%96%E7%A0%81%E5%88%86%E5%B8%83)
[个人的复现](https://kexue.fm/archives/6760#%E4%B8%AA%E4%BA%BA%E7%9A%84%E5%A4%8D%E7%8E%B0)
[最后的总结](https://kexue.fm/archives/6760#%E6%9C%80%E5%90%8E%E7%9A%84%E6%80%BB%E7%BB%93)

### 智能搜索

支持整句搜索！网站自动使用 [结巴分词](https://github.com/fxsjy/jieba) 进行分词，并结合ngrams排序算法给出合理的搜索结果。

### 热门标签

[生成模型](https://kexue.fm/tag/%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B/) [attention](https://kexue.fm/tag/attention/) [模型](https://kexue.fm/tag/%E6%A8%A1%E5%9E%8B/) [语言模型](https://kexue.fm/tag/%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/) [优化](https://kexue.fm/tag/%E4%BC%98%E5%8C%96/) [网站](https://kexue.fm/tag/%E7%BD%91%E7%AB%99/) [概率](https://kexue.fm/tag/%E6%A6%82%E7%8E%87/) [梯度](https://kexue.fm/tag/%E6%A2%AF%E5%BA%A6/) [转载](https://kexue.fm/tag/%E8%BD%AC%E8%BD%BD/) [微分方程](https://kexue.fm/tag/%E5%BE%AE%E5%88%86%E6%96%B9%E7%A8%8B/) [天象](https://kexue.fm/tag/%E5%A4%A9%E8%B1%A1/) [矩阵](https://kexue.fm/tag/%E7%9F%A9%E9%98%B5/) [深度学习](https://kexue.fm/tag/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/) [分析](https://kexue.fm/tag/%E5%88%86%E6%9E%90/) [积分](https://kexue.fm/tag/%E7%A7%AF%E5%88%86/) [python](https://kexue.fm/tag/python/) [力学](https://kexue.fm/tag/%E5%8A%9B%E5%AD%A6/) [无监督](https://kexue.fm/tag/%E6%97%A0%E7%9B%91%E7%9D%A3/) [几何](https://kexue.fm/tag/%E5%87%A0%E4%BD%95/) [优化器](https://kexue.fm/tag/%E4%BC%98%E5%8C%96%E5%99%A8/) [扩散](https://kexue.fm/tag/%E6%89%A9%E6%95%A3/) [节日](https://kexue.fm/tag/%E8%8A%82%E6%97%A5/) [生活](https://kexue.fm/tag/%E7%94%9F%E6%B4%BB/) [文本生成](https://kexue.fm/tag/%E6%96%87%E6%9C%AC%E7%94%9F%E6%88%90/) [数论](https://kexue.fm/tag/%E6%95%B0%E8%AE%BA/)

### 随机文章

- [第一次拍摄天体（月球）！](https://kexue.fm/archives/377)
- [FLASH：可能是近来最有意思的高效Transformer设计](https://kexue.fm/archives/8934)
- [【分享】兴隆山的双子座流星雨](https://kexue.fm/archives/3580)
- [重新写了之前的新词发现算法：更快更好的新词发现](https://kexue.fm/archives/6920)
- [【NASA每日一图】仙王座的NGC 7822](https://kexue.fm/archives/105)
- [2019年全年天象](https://kexue.fm/archives/6257)
- [“让Keras更酷一些！”：随意的输出和灵活的归一化](https://kexue.fm/archives/6311)
- [数学竞赛广东预赛\|组成三角形的概率](https://kexue.fm/archives/1477)
- [变分自编码器（八）：估计样本概率密度](https://kexue.fm/archives/8791)
- [科学空间：2011年7月重要天象](https://kexue.fm/archives/1389)

### 最近评论

- [SunlightZero](https://kexue.fm/archives/9164/comment-page-4#comment-27497): 在《Step-by-Step Diffusion: An Elementary Tutoria...
- [苏剑林](https://kexue.fm/archives/5239/comment-page-3#comment-27496): 1、明白了，我将$q\_{\\phi}(z\|x)$看成$q\_{\\phi}(x\|z)$了（ELBO厌...
- [Suahi](https://kexue.fm/archives/5239/comment-page-3#comment-27493): 谢谢苏老师的回复！1\. 首先回复您为什么ELBO不带KL，并不是最终损失函数形式，需要做如下变...
- [eular](https://kexue.fm/archives/10373/comment-page-1#comment-27492): 是的，当$k$比较大时会出现这种情况。
- [苏剑林](https://kexue.fm/archives/5239/comment-page-3#comment-27491): 肯定是$\\mathbb{E}\_{x \\sim p\_{data}(x)}\[\\log(p\_{\\th...
- [苏剑林](https://kexue.fm/archives/10373/comment-page-1#comment-27490): 你的意思是$\\lambda(\\boldsymbol{x}) < x\_{\\min}$，一般情况下...
- [苏剑林](https://kexue.fm/archives/10735/comment-page-1#comment-27489): 好问题，下一篇文章可能会讨论这个问题
- [苏剑林](https://kexue.fm/archives/10633/comment-page-1#comment-27488): 不大熟悉，但都diffusion forcing了，还有必要CM吗
- [苏剑林](https://kexue.fm/archives/10711/comment-page-2#comment-27487): 简单看了一下，好像没什么新东西呀？还是我看漏了什么？
- [苏剑林](https://kexue.fm/archives/8601/comment-page-1#comment-27486): 感谢指出，已修正。

### 友情链接

- [Cool Papers](https://papers.cool)
- [数学研发](https://bbs.emath.ac.cn)
- [Seatop](http://www.seatop.com.cn/)
- [Xiaoxia](https://xiaoxia.org/)
- [积分表-网络版](https://kexue.fm/sci/integral/index.html)
- [丝路博傲](http://blog.dvxj.com/)
- [ph4ntasy 饭特稀](http://www.ph4ntasy.com/)
- [数学之家](http://www.2math.cn/)
- [有趣天文奇观](http://interesting-sky.china-vo.org/)
- [TwistedW](http://www.twistedwg.com/)
- [godweiyang](https://godweiyang.com/)
- [AI柠檬](https://blog.ailemon.net/)
- [王登科-DK博客](https://greatdk.com)
- [ESON](https://blog.eson.org/)
- [枫之羽](https://fzhiy.net/)
- [Mathor's blog](https://wmathor.com/)
- [coding-zuo](https://coding-zuo.github.io/)
- [博科园](https://www.bokeyuan.net/)
- [孔皮皮的博客](https://www.kppkkp.top/)
- [运鹏的博客](https://yunpengtai.top/)
- [jiming.site](https://jiming.site/)
- [OmegaXYZ](https://www.omegaxyz.com/)
- [Blog by Eacls](https://www.eacls.top/)
- [EAI猩球](https://www.robotech.ink/)
- [文举的博客](https://liwenju0.com/)
- [申请链接](https://kexue.fm/links.html)

[![署名-非商业用途-保持一致](https://kexue.fm/usr/themes/geekg/images/cc.gif)](http://creativecommons.org/licenses/by-nc-nd/2.5/cn/) 本站采用创作共用版权协议，要求署名、非商业用途和保持一致。转载本站内容必须也遵循“ [署名-非商业用途-保持一致](http://creativecommons.org/licenses/by-nc-nd/2.5/cn/)”的创作共用协议。

© 2009-2025 Scientific Spaces. All rights reserved. Theme by [laogui](http://www.laogui.com). Powered by [Typecho](http://typecho.org). 备案号: [粤ICP备09093259号-1/2](https://beian.miit.gov.cn/)。