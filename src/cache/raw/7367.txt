![MobileSideBar](https://kexue.fm/usr/themes/geekg/images/slide-button.png)

## SEARCH

## MENU

- [打赏](https://kexue.fm/reward.html)
- [公式](https://kexue.fm/latex.html)
- [天象](https://kexue.fm/ac.html)
- [链接](https://kexue.fm/links.html)
- [时光](https://kexue.fm/me.html)
- [博览](https://kexue.fm/science.html)
- [归档](https://kexue.fm/content.html)

## CATEGORIES

- [千奇百怪](https://kexue.fm/category/Everything)
- [天文探索](https://kexue.fm/category/Astronomy)
- [数学研究](https://kexue.fm/category/Mathematics)
- [物理化学](https://kexue.fm/category/Phy-chem)
- [信息时代](https://kexue.fm/category/Big-Data)
- [生物自然](https://kexue.fm/category/Biology)
- [图片摄影](https://kexue.fm/category/Photograph)
- [问题百科](https://kexue.fm/category/Questions)
- [生活/情感](https://kexue.fm/category/Life-Feeling)
- [资源共享](https://kexue.fm/category/Resources)

## NEWPOSTS

- [通向概率分布之路：盘点Softma...](https://kexue.fm/archives/10145)
- [重温SSM（二）：HiPPO的一些...](https://kexue.fm/archives/10137)
- [Transformer升级之路：1...](https://kexue.fm/archives/10122)
- [重温SSM（一）：线性系统和HiP...](https://kexue.fm/archives/10114)
- [缓存与效果的极限拉扯：从MHA、M...](https://kexue.fm/archives/10091)
- [Cool Papers更新：简单搭...](https://kexue.fm/archives/10088)
- [以蒸馏的名义：“从去噪自编码器到生...](https://kexue.fm/archives/10085)
- [生成扩散模型漫谈（二十四）：少走捷...](https://kexue.fm/archives/10077)
- [生成扩散模型漫谈（二十三）：信噪比...](https://kexue.fm/archives/10055)
- [生成扩散模型漫谈（二十二）：信噪比...](https://kexue.fm/archives/10047)

## COMMENTS

- [silingtong: 大神,请教一下，为什么HF的deepseek的apply\_ro...](https://kexue.fm/archives/10091/comment-page-2#comment-24560)
- [lxl-: 请教一个问题，一些加速模型（如 SDXL-turbo等）的 C...](https://kexue.fm/archives/9257/comment-page-3#comment-24559)
- [Wen Fei: 您好，苏神，我有一点不明白。l2 norm对所有token和b...](https://kexue.fm/archives/9859/comment-page-1#comment-24558)
- [Wen Fei: 现在能讲一下 质量的来源了嘛？ 在相对论里，质量和能量是一样的...](https://kexue.fm/archives/2036/comment-page-1#comment-24557)
- [bill: 有人使用VQGAN的训练方法对比过VQ和FSQ吗？这应该是FS...](https://kexue.fm/archives/9826/comment-page-2#comment-24556)
- [NirVa: 为啥不做利用cookie保存star？](https://kexue.fm/archives/9978/comment-page-1#comment-24555)
- [lcz: 苏神，为什么“找一个在整个实数域上都单调递增的函数，而且增长速...](https://kexue.fm/archives/3290/comment-page-2#comment-24554)
- [周名远: Kiro: 很高兴你通过实践验证了SiD的稳定性。SDXL的d...](https://kexue.fm/archives/10085/comment-page-1#comment-24553)
- [苏剑林: 刚刷到这篇paper，它是每个像素都视为一个token，这种做...](https://kexue.fm/archives/9984/comment-page-2#comment-24552)
- [苏剑林: 谢谢，已更正。](https://kexue.fm/archives/10114/comment-page-1#comment-24551)

## USERLOGIN

- [登录](https://kexue.fm/admin/login.php)

[科学空间\|Scientific Spaces](https://kexue.fm)

- [登录](https://kexue.fm/admin/login.php)
- [打赏](https://kexue.fm/reward.html)
- [公式](https://kexue.fm/latex.html)
- [天象](https://kexue.fm/ac.html)
- [链接](https://kexue.fm/links.html)
- [时光](https://kexue.fm/me.html)
- [博览](https://kexue.fm/science.html)
- [归档](https://kexue.fm/content.html)

渴望成为一个小飞侠

- [![](https://kexue.fm/usr/themes/geekg/images/rss.png)\
\
欢迎订阅](https://kexue.fm/feed)
- [![](https://kexue.fm/usr/themes/geekg/images/mail.png)\
\
个性邮箱](https://kexue.fm/archives/119)
- [![](https://kexue.fm/usr/themes/geekg/images/Saturn.png)\
\
天象信息](https://kexue.fm/ac.html)
- [![](https://kexue.fm/usr/themes/geekg/images/iss.png)\
\
观测ISS](https://kexue.fm/archives/41)
- [![](https://kexue.fm/usr/themes/geekg/images/pi.png)\
\
LaTeX](https://kexue.fm/latex.html)
- [![](https://kexue.fm/usr/themes/geekg/images/mlogo.png)\
\
关于博主](https://kexue.fm/me.html)

欢迎访问“科学空间”，这里将与您共同探讨自然科学，回味人生百态；也期待大家的分享～

- [**千奇百怪** Everything](https://kexue.fm/category/Everything)
- [**天文探索** Astronomy](https://kexue.fm/category/Astronomy)
- [**数学研究** Mathematics](https://kexue.fm/category/Mathematics)
- [**物理化学** Phy-chem](https://kexue.fm/category/Phy-chem)
- [**信息时代** Big-Data](https://kexue.fm/category/Big-Data)
- [**生物自然** Biology](https://kexue.fm/category/Biology)
- [**图片摄影** Photograph](https://kexue.fm/category/Photograph)
- [**问题百科** Questions](https://kexue.fm/category/Questions)
- [**生活/情感** Life-Feeling](https://kexue.fm/category/Life-Feeling)
- [**资源共享** Resources](https://kexue.fm/category/Resources)

- [**千奇百怪**](https://kexue.fm/category/Everything)
- [**天文探索**](https://kexue.fm/category/Astronomy)
- [**数学研究**](https://kexue.fm/category/Mathematics)
- [**物理化学**](https://kexue.fm/category/Phy-chem)
- [**信息时代**](https://kexue.fm/category/Big-Data)
- [**生物自然**](https://kexue.fm/category/Biology)
- [**图片摄影**](https://kexue.fm/category/Photograph)
- [**问题百科**](https://kexue.fm/category/Questions)
- [**生活/情感**](https://kexue.fm/category/Life-Feeling)
- [**资源共享**](https://kexue.fm/category/Resources)

[首页](https://kexue.fm) [信息时代](https://kexue.fm/category/Big-Data) 节省显存的重计算技巧也有了Keras版了

29Apr

# [节省显存的重计算技巧也有了Keras版了](https://kexue.fm/archives/7367)

By 苏剑林 \|
2020-04-29 \|
41662位读者\|

不少读者最近可能留意到了公众号文章 [《BERT重计算：用22.5%的训练时间节省5倍的显存开销（附代码）》](https://mp.weixin.qq.com/s/CmIVwGFqrSD0wcSN_hgH1A)，里边介绍了一个叫做“重计算”的技巧，简单来说就是用来省显存的方法，让平均训练速度慢一点，但batch\_size可以增大好几倍。该技巧首先发布于论文 [《Training Deep Nets with Sublinear Memory Cost》](https://papers.cool/arxiv/1604.06174)，其实在2016年就已经提出了，只不过似乎还没有特别流行起来。

## 探索 [\#](https://kexue.fm/archives/7367\#%E6%8E%A2%E7%B4%A2)

公众号文章提到该技巧在pytorch和paddlepaddle都有原生实现了，但tensorflow还没有。但事实上从tensorflow 1.8开始，tensorflow就已经自带了该功能了，当时被列入了 `tf.contrib` 这个子库中，而从tensorflow 1.15开始，它就被内置为tensorflow的主函数之一，那就是 `tf.recompute_grad`。

找到 `tf.recompute_grad` 之后，笔者就琢磨了一下它的用法，经过一番折腾，最终居然真的成功地用起来了，居然成功地让 `batch_size` 从48增加到了144！然而，在继续整理测试的过程中，发现这玩意居然在tensorflow 2.x是失效的...于是再折腾了两天，查找了各种资料并反复调试，最终算是成功地补充了这一缺陷。

最后是笔者自己的开源实现：

> **Github地址： [https://github.com/bojone/keras\_recompute](https://github.com/bojone/keras_recompute)**

该实现已经内置在 [bert4keras](https://github.com/bojone/bert4keras) 中，使用bert4keras的读者可以升级到最新版本（0.7.5+）来测试该功能。

## 使用 [\#](https://kexue.fm/archives/7367\#%E4%BD%BF%E7%94%A8)

笔者的实现也命名为 `recompute_grad`，它是一个装饰器，用于自定义Keras层的 `call` 函数，比如

```
from recompute import recompute_grad

class MyLayer(Layer):
 @recompute_grad
 def call(self, inputs):
 return inputs * 2

```

对于已经存在的层，可以通过继承的方式来装饰：

```
from recompute import recompute_grad

class MyDense(Dense):
 @recompute_grad
 def call(self, inputs):
 return super(MyDense, self).call(inputs)

```

自定义好层之后，在代码中嵌入自定义层，然后在执行代码之前，加入环境变量 `RECOMPUTE=1` 来启用重计算。

注意：不是在总模型里插入了 `@recompute_grad`，就能达到省内存的目的，而是要在每个层都插入 `@recompute_grad` 才能更好地省显存。简单来说，就是插入的 `@recompute_grad` 越多，就省显存。具体原因请仔细理解重计算的原理。

## 效果 [\#](https://kexue.fm/archives/7367\#%E6%95%88%E6%9E%9C)

bert4keras 0.7.5+已经内置了重计算，直接传入环境变量 `RECOMPUTE=1` 就会启用重计算，读者可以自行尝试，大概的效果是：

> 1、在BERT Base版本下，batch\_size可以增大为原来的3倍左右；
>
> 2、在BERT Large版本下，batch\_size可以增大为原来的4倍左右；
>
> 3、平均每个样本的训练时间大约增加25%；
>
> 4、理论上，层数越多，batch\_size可以增大的倍数越大。

## 环境 [\#](https://kexue.fm/archives/7367\#%E7%8E%AF%E5%A2%83)

在下面的环境下测试通过：

> tensorflow 1.14 + keras 2.3.1
>
> tensorflow 1.15 + keras 2.3.1
>
> tensorflow 2.0 + keras 2.3.1
>
> tensorflow 2.1 + keras 2.3.1
>
> tensorflow 2.0 + 自带tf.keras
>
> tensorflow 2.1 + 自带tf.keras

确认不支持的环境：

> tensorflow 1.x + 自带tf.keras

欢迎报告更多的测试结果。

顺便说一下， **强烈建议用keras 2.3.1配合tensorflow 1.x/2.x来跑，强烈不建议使用tensorflow 2.x自带的tf.keras来跑**。

## 参考 [\#](https://kexue.fm/archives/7367\#%E5%8F%82%E8%80%83)

最后，笔者的实现主要参考自如下两个源码，在此表示感谢。

> [https://github.com/davisyoshida/tf2-gradient-checkpointing](https://github.com/davisyoshida/tf2-gradient-checkpointing)
>
> [https://github.com/tensorflow/tensorflow/blob/v2.1.0/tensorflow/python/ops/custom\_gradient.py#L454-L499](https://github.com/tensorflow/tensorflow/blob/v2.1.0/tensorflow/python/ops/custom_gradient.py#L454-L499)

_**转载到请包括本文地址：** [https://kexue.fm/archives/7367](https://kexue.fm/archives/7367)_

_**更详细的转载事宜请参考：**_ [《科学空间FAQ》](https://kexue.fm/archives/6508#%E6%96%87%E7%AB%A0%E5%A6%82%E4%BD%95%E8%BD%AC%E8%BD%BD/%E5%BC%95%E7%94%A8)

**如果您还有什么疑惑或建议，欢迎在下方评论区继续讨论。**

**如果您觉得本文还不错，欢迎 [分享](https://kexue.fm/archives/7367#share)/ [打赏](https://kexue.fm/archives/7367#pay) 本文。打赏并非要从中获得收益，而是希望知道科学空间获得了多少读者的真心关注。当然，如果你无视它，也不会影响你的阅读。再次表示欢迎和感谢！**

打赏

![科学空间](https://kexue.fm/usr/themes/geekg/payment/wx.png)

微信打赏

![科学空间](https://kexue.fm/usr/themes/geekg/payment/zfb.png)

支付宝打赏

因为网站后台对打赏并无记录，因此欢迎在打赏时候备注留言。

你还可以 [**点击这里**](http://mail.qq.com/cgi-bin/qm_share?t=qm_mailme&email=tN7d1drY3drrx8H0xcWa19vZ) 或在下方评论区留言来告知你的建议或需求。

**如果您需要引用本文，请参考：**

苏剑林. (Apr. 29, 2020). 《节省显存的重计算技巧也有了Keras版了 》\[Blog post\]. Retrieved from [https://kexue.fm/archives/7367](https://kexue.fm/archives/7367)

@online{kexuefm-7367,

        title={节省显存的重计算技巧也有了Keras版了},

        author={苏剑林},

        year={2020},

        month={Apr},

        url={\\url{https://kexue.fm/archives/7367}},

}

分类： [信息时代](https://kexue.fm/category/Big-Data)    标签： [模型](https://kexue.fm/tag/%E6%A8%A1%E5%9E%8B/), [深度学习](https://kexue.fm/tag/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/), [keras](https://kexue.fm/tag/keras/)[11 评论](https://kexue.fm/archives/7367#comments)

< [将“Softmax+交叉熵”推广到多标签分类问题](https://kexue.fm/archives/7359) \| [变分自编码器（五）：VAE + BN = 更好的VAE](https://kexue.fm/archives/7381) >

### 你也许还对下面的内容感兴趣

- [基于量子化假设推导模型的尺度定律（Scaling Law）](https://kexue.fm/archives/9607)
- [Tiger：一个“抠”到极致的优化器](https://kexue.fm/archives/9512)
- [在bert4keras中使用混合精度和XLA加速训练](https://kexue.fm/archives/9059)
- [为什么需要残差？一个来自DeepNet的视角](https://kexue.fm/archives/8994)
- [门控注意力单元（GAU）还需要Warmup吗？](https://kexue.fm/archives/8990)
- [多任务学习漫谈（三）：分主次之序](https://kexue.fm/archives/8907)
- [多任务学习漫谈（二）：行梯度之事](https://kexue.fm/archives/8896)
- [Efficient GlobalPointer：少点参数，多点效果](https://kexue.fm/archives/8877)
- [多任务学习漫谈（一）：以损失之名](https://kexue.fm/archives/8870)
- [Seq2Seq+前缀树：检索任务新范式（以KgCLUE为例）](https://kexue.fm/archives/8802)

[发表你的看法](https://kexue.fm/archives/7367#comment_form)

[Best](http://sa)

April 30th, 2020

自定义keras层，总感觉不好验证【也不知道自己写的对不对】，苏神是怎么做的

[回复评论](https://kexue.fm/archives/7367/comment-page-1?replyTo=13248#respond-post-7367)

[苏剑林](https://kexue.fm) 发表于
April 30th, 2020

不好验证什么？

[回复评论](https://kexue.fm/archives/7367/comment-page-1?replyTo=13249#respond-post-7367)

gzupanda

May 6th, 2020

试试公式$a^{2} + b^{2} = c^{2}$

[回复评论](https://kexue.fm/archives/7367/comment-page-1?replyTo=13296#respond-post-7367)

jjjjohnson

May 6th, 2020

请教一下 如果用预训练的模型 比如resnet50 这个重计算怎么用？谢谢！

[回复评论](https://kexue.fm/archives/7367/comment-page-1?replyTo=13298#respond-post-7367)

a people

June 7th, 2020

请教一下，为什么在创建模型（model=Model(inputs,outputs)）时报错

AttributeError: 'NoneType' object has no attribute '\_inbound\_nodes'

[回复评论](https://kexue.fm/archives/7367/comment-page-1?replyTo=13508#respond-post-7367)

a people 发表于
June 7th, 2020

呃，别在意这个，这是我从tf.keras切到keras冒出来的东西

[回复评论](https://kexue.fm/archives/7367/comment-page-1?replyTo=13509#respond-post-7367)

[苏剑林](https://kexue.fm) 发表于
June 8th, 2020

不清楚。可能是tf.keras和keras混用了吧

[回复评论](https://kexue.fm/archives/7367/comment-page-1?replyTo=13513#respond-post-7367)

bling

March 9th, 2021

您好，我用的是tensorfolow 1.12的版本，请问重计算在contrib库的什么地方呢？

[回复评论](https://kexue.fm/archives/7367/comment-page-1?replyTo=15710#respond-post-7367)

[苏剑林](https://kexue.fm) 发表于
March 9th, 2021

不知道，我不用tf 1.12

[回复评论](https://kexue.fm/archives/7367/comment-page-1?replyTo=15712#respond-post-7367)

bob

April 20th, 2021

请问苏佬，我搭建模型的时候使用了build\_transformer\_model函数，我是将Transformer基类中的call方法加上@recompute\_grad，但是这样貌似行不通，从32批次增加到64批次报OOM了，请问我应该怎样实现

[回复评论](https://kexue.fm/archives/7367/comment-page-1?replyTo=16165#respond-post-7367)

[苏剑林](https://kexue.fm) 发表于
April 21st, 2021

先认真读读本文。

[回复评论](https://kexue.fm/archives/7367/comment-page-1?replyTo=16169#respond-post-7367)

[取消回复](https://kexue.fm/archives/7367#respond-post-7367)

你的大名

电子邮箱

个人网站（选填）

1\. 可以在评论中使用LaTeX代码，点击“预览效果”可即时查看效果，点击 [这里](https://kexue.fm/content.html) 可以查看更多内容；

2\. 可以通过点击评论楼层编号来引用该楼层；

3\. **提交评论之前，建议复制一下评论内容，避免提交失败导致辛苦打的字没了。**

### 内容速览

[探索](https://kexue.fm/archives/7367#%E6%8E%A2%E7%B4%A2)
[使用](https://kexue.fm/archives/7367#%E4%BD%BF%E7%94%A8)
[效果](https://kexue.fm/archives/7367#%E6%95%88%E6%9E%9C)
[环境](https://kexue.fm/archives/7367#%E7%8E%AF%E5%A2%83)
[参考](https://kexue.fm/archives/7367#%E5%8F%82%E8%80%83)

### 智能搜索

支持整句搜索！网站自动使用 [结巴分词](https://github.com/fxsjy/jieba) 进行分词，并结合ngrams排序算法给出合理的搜索结果。

### 热门标签

[生成模型](https://kexue.fm/tag/%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B/) [attention](https://kexue.fm/tag/attention/) [模型](https://kexue.fm/tag/%E6%A8%A1%E5%9E%8B/) [语言模型](https://kexue.fm/tag/%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/) [优化](https://kexue.fm/tag/%E4%BC%98%E5%8C%96/) [概率](https://kexue.fm/tag/%E6%A6%82%E7%8E%87/) [网站](https://kexue.fm/tag/%E7%BD%91%E7%AB%99/) [转载](https://kexue.fm/tag/%E8%BD%AC%E8%BD%BD/) [微分方程](https://kexue.fm/tag/%E5%BE%AE%E5%88%86%E6%96%B9%E7%A8%8B/) [天象](https://kexue.fm/tag/%E5%A4%A9%E8%B1%A1/) [深度学习](https://kexue.fm/tag/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/) [积分](https://kexue.fm/tag/%E7%A7%AF%E5%88%86/) [分析](https://kexue.fm/tag/%E5%88%86%E6%9E%90/) [python](https://kexue.fm/tag/python/) [梯度](https://kexue.fm/tag/%E6%A2%AF%E5%BA%A6/) [无监督](https://kexue.fm/tag/%E6%97%A0%E7%9B%91%E7%9D%A3/) [力学](https://kexue.fm/tag/%E5%8A%9B%E5%AD%A6/) [节日](https://kexue.fm/tag/%E8%8A%82%E6%97%A5/) [几何](https://kexue.fm/tag/%E5%87%A0%E4%BD%95/) [文本生成](https://kexue.fm/tag/%E6%96%87%E6%9C%AC%E7%94%9F%E6%88%90/) [数论](https://kexue.fm/tag/%E6%95%B0%E8%AE%BA/) [矩阵](https://kexue.fm/tag/%E7%9F%A9%E9%98%B5/) [GAN](https://kexue.fm/tag/GAN/) [生活](https://kexue.fm/tag/%E7%94%9F%E6%B4%BB/) [扩散](https://kexue.fm/tag/%E6%89%A9%E6%95%A3/)

### 随机文章

- [增强typecho的搜索功能](https://kexue.fm/archives/4797)
- [万能的seq2seq：基于seq2seq的阅读理解问答](https://kexue.fm/archives/7115)
- [通用爬虫探索（三）：效果展示与代码](https://kexue.fm/archives/4430)
- [校准你的钟表（时间科普网站）](https://kexue.fm/archives/79)
- [《虚拟的实在(4)》——质量是什么](https://kexue.fm/archives/2036)
- [天体力学巨匠——拉普拉斯](https://kexue.fm/archives/1779)
- [用bert4keras做三元组抽取](https://kexue.fm/archives/7161)
- [如何在科学空间输入数学公式？——LaTeX帮助](https://kexue.fm/archives/83)
- [【NASA每日一图】超新星遗骸E0102-72](https://kexue.fm/archives/112)
- [生活\|我家的几只小鸡](https://kexue.fm/archives/177)

### 最近评论

- [silingtong](https://kexue.fm/archives/10091/comment-page-2#comment-24560): 大神,请教一下，为什么HF的deepseek的apply\_rotary\_pos\_emb函数中，...
- [lxl-](https://kexue.fm/archives/9257/comment-page-3#comment-24559): 请教一个问题，一些加速模型（如 SDXL-turbo等）的 CFG 都非常小，反向关键词都不起...
- [Wen Fei](https://kexue.fm/archives/9859/comment-page-1#comment-24558): 您好，苏神，我有一点不明白。l2 norm对所有token和batch求平均？那推理过程呢，t...
- [Wen Fei](https://kexue.fm/archives/2036/comment-page-1#comment-24557): 现在能讲一下 质量的来源了嘛？ 在相对论里，质量和能量是一样的东西，应该是不同坐标系下的投影。...
- [bill](https://kexue.fm/archives/9826/comment-page-2#comment-24556): 有人使用VQGAN的训练方法对比过VQ和FSQ吗？这应该是FSQ论文的实现方式。我训练出来无论...
- [NirVa](https://kexue.fm/archives/9978/comment-page-1#comment-24555): 为啥不做利用cookie保存star？
- [lcz](https://kexue.fm/archives/3290/comment-page-2#comment-24554): 苏神，为什么“找一个在整个实数域上都单调递增的函数，而且增长速度要快于线性增长，然后求和，最后...
- [周名远](https://kexue.fm/archives/10085/comment-page-1#comment-24553): Kiro: 很高兴你通过实践验证了SiD的稳定性。SDXL的distill目前实验还没具体开展...
- [苏剑林](https://kexue.fm/archives/9984/comment-page-2#comment-24552): 刚刷到这篇paper，它是每个像素都视为一个token，这种做法远比我说的激进，而且它自己越承...
- [苏剑林](https://kexue.fm/archives/10114/comment-page-1#comment-24551): 谢谢，已更正。

### 友情链接

- [Cool Papers](https://papers.cool)
- [数学研发](https://bbs.emath.ac.cn)
- [Seatop](http://www.seatop.com.cn/)
- [Xiaoxia](https://xiaoxia.org/)
- [积分表-网络版](https://kexue.fm/sci/integral/index.html)
- [丝路博傲](http://blog.dvxj.com/)
- [ph4ntasy 饭特稀](http://www.ph4ntasy.com/)
- [数学之家](http://www.2math.cn/)
- [有趣天文奇观](http://interesting-sky.china-vo.org/)
- [bsky](https://bsky.spaces.ac.cn/)
- [TwistedW](http://www.twistedwg.com/)
- [godweiyang](https://godweiyang.com/)
- [AI柠檬](https://blog.ailemon.net/)
- [王登科-DK博客](https://greatdk.com)
- [ESON](https://blog.eson.org/)
- [枫之羽](https://fzhiy.net/)
- [Mathor's blog](https://wmathor.com/)
- [孙云增的博客](https://sunyunzeng.com/)
- [coding-zuo](https://coding-zuo.github.io/)
- [博科园](https://www.bokeyuan.net/)
- [孔皮皮的博客](https://www.kppkkp.top/)
- [运鹏的博客](https://yunpengtai.top/)
- [jiming.site](https://jiming.site/)
- [OmegaXYZ](https://www.omegaxyz.com/)
- [Blog by Eacls](https://www.eacls.top/)
- [申请链接](https://kexue.fm/links.html)

[![署名-非商业用途-保持一致](https://kexue.fm/usr/themes/geekg/images/cc.gif)](http://creativecommons.org/licenses/by-nc-nd/2.5/cn/) 本站采用创作共用版权协议，要求署名、非商业用途和保持一致。转载本站内容必须也遵循“ [署名-非商业用途-保持一致](http://creativecommons.org/licenses/by-nc-nd/2.5/cn/)”的创作共用协议。

© 2009-2024 Scientific Spaces. All rights reserved. Theme by [laogui](http://www.laogui.com). Powered by [Typecho](http://typecho.org). 备案号: [粤ICP备09093259号-1/2](https://beian.miit.gov.cn/)。