## SEARCH

## MENU

- [打赏](https://kexue.fm/reward.html)
- [公式](https://kexue.fm/latex.html)
- [天象](https://kexue.fm/ac.html)
- [链接](https://kexue.fm/links.html)
- [时光](https://kexue.fm/me.html)
- [博览](https://kexue.fm/science.html)
- [归档](https://kexue.fm/content.html)

## CATEGORIES

- [千奇百怪](https://kexue.fm/category/Everything)
- [天文探索](https://kexue.fm/category/Astronomy)
- [数学研究](https://kexue.fm/category/Mathematics)
- [物理化学](https://kexue.fm/category/Phy-chem)
- [信息时代](https://kexue.fm/category/Big-Data)
- [生物自然](https://kexue.fm/category/Biology)
- [图片摄影](https://kexue.fm/category/Photograph)
- [问题百科](https://kexue.fm/category/Questions)
- [生活/情感](https://kexue.fm/category/Life-Feeling)
- [资源共享](https://kexue.fm/category/Resources)

## NEWPOSTS

- [为什么Adam的Update RM...](https://kexue.fm/archives/11267)
- [重新思考学习率与Batch Siz...](https://kexue.fm/archives/11260)
- [Cool Papers更新：简单适...](https://kexue.fm/archives/11250)
- [流形上的最速下降：4\. Muon ...](https://kexue.fm/archives/11241)
- [ReLU/GeLU/Swish的一...](https://kexue.fm/archives/11233)
- [流形上的最速下降：3\. Muon ...](https://kexue.fm/archives/11221)
- [流形上的最速下降：2\. Muon ...](https://kexue.fm/archives/11215)
- [基于树莓派Zero2W搭建一个随身旁路由](https://kexue.fm/archives/11206)
- [流形上的最速下降：1\. SGD ...](https://kexue.fm/archives/11196)
- [矩阵r次方根和逆r次方根的高效计算](https://kexue.fm/archives/11175)

## COMMENTS

- [Lxj: 苏神好，没有学过格林函数法，找了很多资料但都不太是很明白，求解...](https://kexue.fm/archives/9379/comment-page-1#comment-28534)
- [Lxj: 苏神好，没有学过格林函数法，找了很多资料但都不太是很明白，求解](https://kexue.fm/archives/9379/comment-page-1#comment-28533)
- [xxk: 12 个数会有 66 个差，然而差为 19 到 36 的情况最...](https://kexue.fm/archives/35/comment-page-1#comment-28532)
- [gapeng: kimi k2形式上推导了一个公式，最后数值模拟在0.23左右...](https://kexue.fm/archives/11267/comment-page-1#comment-28531)
- [Evan1024: 太牛了！](https://kexue.fm/archives/11267/comment-page-1#comment-28530)
- [ameowcat: 苏神您好，有个问题想请教一下，最近扩散模型的推理优化有一篇文章...](https://kexue.fm/archives/10958/comment-page-3#comment-28529)
- [Eliot: 2 实现loss-free with budget应当是在当前...](https://kexue.fm/archives/10815/comment-page-1#comment-28528)
- [Eliot: 继续阅读这2份代码后，大概结论如下\
1 megatron-lm...](https://kexue.fm/archives/10815/comment-page-1#comment-28527)
- [lzyyzl: 帮忙解惑一下\
1 文中提到本文主题是求O=msign(M)的导...](https://kexue.fm/archives/11025/comment-page-1#comment-28525)
- [z: 牛](https://kexue.fm/archives/11267/comment-page-1#comment-28524)

## USERLOGIN

- [登录](https://kexue.fm/admin/login.php)

[科学空间\|Scientific Spaces](https://kexue.fm)

- [登录](https://kexue.fm/admin/login.php)
- [打赏](https://kexue.fm/reward.html)
- [公式](https://kexue.fm/latex.html)
- [天象](https://kexue.fm/ac.html)
- [链接](https://kexue.fm/links.html)
- [时光](https://kexue.fm/me.html)
- [博览](https://kexue.fm/science.html)
- [归档](https://kexue.fm/content.html)

渴望成为一个小飞侠

- [欢迎订阅](https://kexue.fm/feed)
- [个性邮箱](https://kexue.fm/archives/119)
- [天象信息](https://kexue.fm/ac.html)
- [观测ISS](https://kexue.fm/archives/41)
- [LaTeX](https://kexue.fm/latex.html)
- [关于博主](https://kexue.fm/me.html)

欢迎访问“科学空间”，这里将与您共同探讨自然科学，回味人生百态；也期待大家的分享～

- [**千奇百怪** Everything](https://kexue.fm/category/Everything)
- [**天文探索** Astronomy](https://kexue.fm/category/Astronomy)
- [**数学研究** Mathematics](https://kexue.fm/category/Mathematics)
- [**物理化学** Phy-chem](https://kexue.fm/category/Phy-chem)
- [**信息时代** Big-Data](https://kexue.fm/category/Big-Data)
- [**生物自然** Biology](https://kexue.fm/category/Biology)
- [**图片摄影** Photograph](https://kexue.fm/category/Photograph)
- [**问题百科** Questions](https://kexue.fm/category/Questions)
- [**生活/情感** Life-Feeling](https://kexue.fm/category/Life-Feeling)
- [**资源共享** Resources](https://kexue.fm/category/Resources)

- [**千奇百怪**](https://kexue.fm/category/Everything)
- [**天文探索**](https://kexue.fm/category/Astronomy)
- [**数学研究**](https://kexue.fm/category/Mathematics)
- [**物理化学**](https://kexue.fm/category/Phy-chem)
- [**信息时代**](https://kexue.fm/category/Big-Data)
- [**生物自然**](https://kexue.fm/category/Biology)
- [**图片摄影**](https://kexue.fm/category/Photograph)
- [**问题百科**](https://kexue.fm/category/Questions)
- [**生活/情感**](https://kexue.fm/category/Life-Feeling)
- [**资源共享**](https://kexue.fm/category/Resources)

[首页](https://kexue.fm) [信息时代](https://kexue.fm/category/Big-Data) “让Keras更酷一些！”：分层的学习率和自由的梯度

10Mar

# [“让Keras更酷一些！”：分层的学习率和自由的梯度](https://kexue.fm/archives/6418)

By 苏剑林 \|
2019-03-10 \|
119751位读者\|

高举“ [让Keras更酷一些！](https://kexue.fm/search/%E8%AE%A9Keras%E6%9B%B4%E9%85%B7%E4%B8%80%E4%BA%9B/)”大旗，让Keras无限可能～

今天我们会用Keras做到两件很重要的事情：分层设置学习率和灵活操作梯度。

首先是 **分层设置学习率**，这个用途很明显，比如我们在fine tune已有模型的时候，有些时候我们会固定一些层，但有时候我们又不想固定它，而是想要它以比其他层更低的学习率去更新，这个需求就是分层设置学习率了。对于在Keras中分层设置学习率，网上也有一定的探讨，结论都是要通过重写优化器来实现。显然这种方法不论在实现上还是使用上都不友好。

然后是 **操作梯度**。操作梯度一个最直接的例子是梯度裁剪，也就是把梯度控制在某个范围内，Keras内置了这个方法。但是Keras内置的是全局的梯度裁剪，假如我要给每个梯度设置不同的裁剪方式呢？甚至我有其他的操作梯度的思路，那要怎么实施呢？不会又是重写优化器吧？

本文就来为上述问题给出尽可能简单的解决方案。

## 分层的学习率 [\#](https://kexue.fm/kexue.fm\#%E5%88%86%E5%B1%82%E7%9A%84%E5%AD%A6%E4%B9%A0%E7%8E%87)

对于分层设置学习率这个事情，重写优化器当然是可行的，但是太麻烦。如果要寻求更简单的方案，我们需要一些数学知识来指导我们怎么进行。

### 参数变换下的优化 [\#](https://kexue.fm/kexue.fm\#%E5%8F%82%E6%95%B0%E5%8F%98%E6%8D%A2%E4%B8%8B%E7%9A%84%E4%BC%98%E5%8C%96)

首先我们考虑梯度下降的更新公式：
\\begin{equation}\\boldsymbol{\\theta}\_{n+1}=\\boldsymbol{\\theta}\_{n}-\\alpha \\frac{\\partial L(\\boldsymbol{\\theta}\_{n})}{\\partial \\boldsymbol{\\theta}\_n}\\label{eq:sgd-1}\\end{equation}
其中$L$是带参数$\\boldsymbol{\\theta}$的loss函数，$\\alpha$是学习率，$\\frac{\\partial L(\\boldsymbol{\\theta}\_{n})}{\\partial \\boldsymbol{\\theta}\_n}$是梯度，有时候我们也写成$\\nabla\_{\\boldsymbol{\\theta}} L(\\boldsymbol{\\theta}\_{n})$。记号是很随意的，关键是理解它的含义～

然后我们考虑变换$\\boldsymbol{\\theta}=\\lambda \\boldsymbol{\\phi}$，其中$\\lambda$是一个固定的标量，$\\boldsymbol{\\phi}$也是参数。现在我们来优化$\\boldsymbol{\\phi}$，相应的更新公式为：
\\begin{equation}\\begin{aligned}\\boldsymbol{\\phi}\_{n+1}=&\\boldsymbol{\\phi}\_{n}-\\alpha \\frac{\\partial L(\\lambda\\boldsymbol{\\phi}\_{n})}{\\partial \\boldsymbol{\\phi}\_n}\\\
=&\\boldsymbol{\\phi}\_{n}-\\alpha \\frac{\\partial L(\\boldsymbol{\\theta}\_{n})}{\\partial \\boldsymbol{\\theta}\_n}\\frac{\\partial \\boldsymbol{\\theta}\_{n}}{\\partial \\boldsymbol{\\phi}\_n}\\\
=&\\boldsymbol{\\phi}\_{n}-\\lambda\\alpha \\frac{\\partial L(\\boldsymbol{\\theta}\_{n})}{\\partial \\boldsymbol{\\theta}\_n}\\end{aligned}\\end{equation}
其中第二个等号其实就是链式法则。现在我们在两边乘上$\\lambda$，得到
\\begin{equation}\\lambda\\boldsymbol{\\phi}\_{n+1}=\\lambda\\boldsymbol{\\phi}\_{n}-\\lambda^2\\alpha \\frac{\\partial L(\\boldsymbol{\\theta}\_{n})}{\\partial \\boldsymbol{\\theta}\_n}\\quad\\Rightarrow\\quad\\boldsymbol{\\theta}\_{n+1}=\\boldsymbol{\\theta}\_{n}-\\lambda^2\\alpha \\frac{\\partial L(\\boldsymbol{\\theta}\_{n})}{\\partial \\boldsymbol{\\theta}\_n}\\label{eq:sgd-2}\\end{equation}
对比$\\eqref{eq:sgd-1}$和$\\eqref{eq:sgd-2}$，大家能明白我想说什么了吧：

> 在SGD优化器中，如果做参数变换$\\boldsymbol{\\theta}=\\lambda \\boldsymbol{\\phi}$，那么等价的结果是学习率从$\\alpha$变成了$\\lambda^2\\alpha$。

不过，在自适应学习率优化器（比如RMSprop、Adam等），情况有点不一样，因为自适应学习率使用梯度（作为分母）来调整了学习率，抵消了一个$\\lambda$，从而（请有兴趣的读者自己推导一下）

> 在RMSprop、Adam等自适应学习率优化器中，如果做参数变换$\\boldsymbol{\\theta}=\\lambda \\boldsymbol{\\phi}$，那么等价的结果是学习率从$\\alpha$变成了$\\lambda\\alpha$。

### 移花接木调整学习率 [\#](https://kexue.fm/kexue.fm\#%E7%A7%BB%E8%8A%B1%E6%8E%A5%E6%9C%A8%E8%B0%83%E6%95%B4%E5%AD%A6%E4%B9%A0%E7%8E%87)

有了前面这两个结论，我们就只需要想办法实现参数变换，而不需要自己重写优化器，来实现逐层设置学习率了。

实现参数变换的方法也不难，之前我们在 [《 “让Keras更酷一些！”：随意的输出和灵活的归一化》一文](https://kexue.fm/archives/6311#%E7%81%B5%E6%B4%BB%E7%9A%84%E6%9D%83%E9%87%8D%E5%BD%92%E4%B8%80%E5%8C%96) 讨论权重归一化的时候已经讲过方法了。因为Keras在构建一个层的时候，实际上是分开了 `build` 和 `call` 两个步骤，我们可以在 `build` 之后插一些操作，然后再调用 `call` 就行了。

下面是一个封装好的实现：

```
import keras.backend as K

class SetLearningRate:
 """层的一个包装，用来设置当前层的学习率
 """

 def __init__(self, layer, lamb, is_ada=False):
 self.layer = layer
 self.lamb = lamb # 学习率比例
 self.is_ada = is_ada # 是否自适应学习率优化器

 def __call__(self, inputs):
 with K.name_scope(self.layer.name):
 if not self.layer.built:
 input_shape = K.int_shape(inputs)
 self.layer.build(input_shape)
 self.layer.built = True
 if self.layer._initial_weights is not None:
 self.layer.set_weights(self.layer._initial_weights)
 for key in ['kernel', 'bias', 'embeddings', 'depthwise_kernel', 'pointwise_kernel', 'recurrent_kernel', 'gamma', 'beta']:
 if hasattr(self.layer, key):
 weight = getattr(self.layer, key)
 if self.is_ada:
 lamb = self.lamb # 自适应学习率优化器直接保持lamb比例
 else:
 lamb = self.lamb**0.5 # SGD（包括动量加速），lamb要开平方
 K.set_value(weight, K.eval(weight) / lamb) # 更改初始化
 setattr(self.layer, key, weight * lamb) # 按比例替换
 return self.layer(inputs)
```

使用示例：

```
x_in = Input(shape=(None,))
x = x_in

# 默认情况下是x = Embedding(100, 1000, weights=[word_vecs])(x)
# 下面这一句表示：后面将会用自适应学习率优化器，并且Embedding层以总体的十分之一的学习率更新。
# word_vecs是预训练好的词向量
x = SetLearningRate(Embedding(100, 1000, weights=[word_vecs]), 0.1, True)(x)

# 后面部分自己想象了～
x = LSTM(100)(x)

model = Model(x_in, x)
model.compile(loss='mse', optimizer='adam') # 用自适应学习率优化器优化
```

几个注意事项：

> 1、目前这种方式，只能用于自己动手写代码来构建模型的时候插入，无法对建立好的模型进行操作。
>
> 2、如果有预训练权重，有两种加载方法。第一种是像刚才的使用示例一样，在定义层的时候通过weights参数传入；第二种方法是建立好模型后（已经在相应的地方插入好SetLearningRate），用model.set\_weights(weights)来赋值，其中weights是“在SetLearningRate的位置已经被除以了$\\lambda$或$\\sqrt{\\lambda}$的原来模型的预训练权重”。
>
> 3、加载预训练权重的第二种方法看起来有点不知所云，但如果你已经理解了这一节的原理，那么应该能知道我在说什么。因为设置学习率是通过weight \* lamb来实现的，所以weight的初始化要变为weight / lamb。
>
> 4、这个操作基本上不可逆，比如你一开始设置了Embedding层以总体的1/10比例的学习率来更新，那么 **很难** 在这个基础上，再将它改为1/5或者其他比例。（当然，如果你真的彻底搞懂了这一节的原理，并且也弄懂了加载预训练权重的第二种方法，那么还是有办法的，那时候相信你也能搞出来）。
>
> 5、这种做法有以上限制，是因为我们不想通过修改或者重写优化器的方式来实现这个功能。如果你决定要自己修改优化器，请参考 [《“让Keras更酷一些！”：小众的自定义优化器》](https://kexue.fm/archives/5879)。

## 自由的梯度操作 [\#](https://kexue.fm/kexue.fm\#%E8%87%AA%E7%94%B1%E7%9A%84%E6%A2%AF%E5%BA%A6%E6%93%8D%E4%BD%9C)

在这部分内容中，我们将学习对梯度的更为自由的控制。这部分内容涉及到对优化器的修改，但不需要完全重写优化器。

### Keras优化器的结构 [\#](https://kexue.fm/kexue.fm\#Keras%E4%BC%98%E5%8C%96%E5%99%A8%E7%9A%84%E7%BB%93%E6%9E%84)

要修改优化器，必须先要了解Keras优化器的结构。在 [《“让Keras更酷一些！”：小众的自定义优化器》](https://kexue.fm/archives/5879) 一文我们已经初步看过了，现在我们重新看一遍。

Keras优化器的代码在
[https://github.com/keras-team/keras/blob/master/keras/optimizers.py](https://github.com/keras-team/keras/blob/master/keras/optimizers.py)

随便观察一个优化器，就会发现你要自定义一个优化器，只需要继承 `Optimizer` 类，然后定义 `get_updates` 方法。但本文我们不想做新的优化器，只是想要对梯度有所控制。可以看到，梯度的获取其实是在父类 `Optimizer` 的 `get_gradients` 方法中：

```
 def get_gradients(self, loss, params):
 grads = K.gradients(loss, params)
 if None in grads:
 raise ValueError('An operation has `None` for gradient. '
 'Please make sure that all of your ops have a '
 'gradient defined (i.e. are differentiable). '
 'Common ops without gradient: '
 'K.argmax, K.round, K.eval.')
 if hasattr(self, 'clipnorm') and self.clipnorm > 0:
 norm = K.sqrt(sum([K.sum(K.square(g)) for g in grads]))
 grads = [clip_norm(g, self.clipnorm, norm) for g in grads]
 if hasattr(self, 'clipvalue') and self.clipvalue > 0:
 grads = [K.clip(g, -self.clipvalue, self.clipvalue) for g in grads]
 return grads
```

其中方法中的第一句就是获取原始梯度的，后面则提供了两种梯度裁剪方法。不难想到，只需要重写优化器的 `get_gradients` 方法，就可以实现对梯度的任意操作了，而且这个操作不影响优化器的更新步骤（即不影响 `get_updates` 方法）。

### 处处皆对象：覆盖即可 [\#](https://kexue.fm/kexue.fm\#%E5%A4%84%E5%A4%84%E7%9A%86%E5%AF%B9%E8%B1%A1%EF%BC%9A%E8%A6%86%E7%9B%96%E5%8D%B3%E5%8F%AF)

怎么能做到只修改 `get_gradients` 方法呢？这得益于Python的哲学——“处处皆对象”。Python是一门面向对象的编程语言，Python中几乎你能碰到的一切变量都是一个对象。我们说 `get_gradients` 是优化器的一个方法，也可以说 `get_gradients` 的一个属性（对象），既然是属性，直接覆盖赋值即可。

我们来举一个最粗暴的例子（恶作剧）：

```
def our_get_gradients(loss, params):
 return [K.zeros_like(p) for p in params]

adam_opt = Adam(1e-3)
adam_opt.get_gradients = our_get_gradients

model.compile(loss='categorical_crossentropy',
 optimizer=adam_opt)
```

其实这个例子很无聊，就是把所有梯度置零了（然后你怎么优化它都不动了...），但这个恶作剧例子已经足够有代表性了——你可以将所有梯度置零，你也可以将梯度做任意你喜欢的操作。比如将梯度按照$l\_1$范数而非$l\_2$范数裁剪，又或者做其他调整～

假如我只想操作部分层的梯度怎么办？那也简单，你在定义层的时候需要起一个能区分的名字，然后根据 `params` 的名字做不同的操作即可。都到这一步了，我相信基本是“一法通，万法皆通”的了。

## 飘逸的Keras [\#](https://kexue.fm/kexue.fm\#%E9%A3%98%E9%80%B8%E7%9A%84Keras)

也许在很多人眼中，Keras就是一个好用但是封装得很“死”的高层框架，但在我眼里，我只看到了它无限的灵活性～

那是一个无懈可击的封装。

_**转载到请包括本文地址：** [https://kexue.fm/archives/6418](https://kexue.fm/archives/6418)_

_**更详细的转载事宜请参考：**_ [《科学空间FAQ》](https://kexue.fm/archives/6508#%E6%96%87%E7%AB%A0%E5%A6%82%E4%BD%95%E8%BD%AC%E8%BD%BD/%E5%BC%95%E7%94%A8)

**如果您还有什么疑惑或建议，欢迎在下方评论区继续讨论。**

**如果您觉得本文还不错，欢迎 [分享](https://kexue.fm/kexue.fm#share)/ [打赏](https://kexue.fm/kexue.fm#pay) 本文。打赏并非要从中获得收益，而是希望知道科学空间获得了多少读者的真心关注。当然，如果你无视它，也不会影响你的阅读。再次表示欢迎和感谢！**

打赏

微信打赏

支付宝打赏

因为网站后台对打赏并无记录，因此欢迎在打赏时候备注留言。你还可以 [**点击这里**](http://mail.qq.com/cgi-bin/qm_share?t=qm_mailme&email=tN7d1drY3drrx8H0xcWa19vZ) 或在下方评论区留言来告知你的建议或需求。

**如果您需要引用本文，请参考：**

苏剑林. (Mar. 10, 2019). 《“让Keras更酷一些！”：分层的学习率和自由的梯度 》\[Blog post\]. Retrieved from [https://kexue.fm/archives/6418](https://kexue.fm/archives/6418)

@online{kexuefm-6418,
        title={“让Keras更酷一些！”：分层的学习率和自由的梯度},
        author={苏剑林},
        year={2019},
        month={Mar},
        url={\\url{https://kexue.fm/archives/6418}},
}

分类： [信息时代](https://kexue.fm/category/Big-Data)    标签： [模型](https://kexue.fm/tag/%E6%A8%A1%E5%9E%8B/), [keras](https://kexue.fm/tag/keras/), [梯度](https://kexue.fm/tag/%E6%A2%AF%E5%BA%A6/), [学习率](https://kexue.fm/tag/%E5%AD%A6%E4%B9%A0%E7%8E%87/)[29 评论](https://kexue.fm/archives/6418#comments)

< [O-GAN：简单修改，让GAN的判别器变成一个编码器！](https://kexue.fm/archives/6409) \| [圆周率节快乐！\|\| 原来已经写了十年博客～](https://kexue.fm/archives/6469) >

### 你也许还对下面的内容感兴趣

- [为什么Adam的Update RMS是0.2？](https://kexue.fm/archives/11267)
- [重新思考学习率与Batch Size的关系（一）：现状](https://kexue.fm/archives/11260)
- [msign的导数](https://kexue.fm/archives/11025)
- [SVD的导数](https://kexue.fm/archives/10878)
- [通过梯度近似寻找Normalization的替代品](https://kexue.fm/archives/10831)
- [MoE环游记：4、难处应当多投入](https://kexue.fm/archives/10815)
- [高阶muP：更简明但更高明的谱条件缩放](https://kexue.fm/archives/10795)
- [初探muP：超参数的跨模型尺度迁移规律](https://kexue.fm/archives/10770)
- [MoE环游记：3、换个思路来分配](https://kexue.fm/archives/10757)
- [Muon续集：为什么我们选择尝试Muon？](https://kexue.fm/archives/10739)

[发表你的看法](https://kexue.fm/kexue.fm#comment_form)

1. [«](https://kexue.fm/archives/6418/comment-page-1#comments)
2. [1](https://kexue.fm/archives/6418/comment-page-1#comments)
3. [2](https://kexue.fm/archives/6418/comment-page-2#comments)

[关于CRF层的学习率【转载以学习、回忆】\_Johngo学长](https://www.johngo689.com/120021/)

October 9th, 2022

\[...\]（附注：增大学习率的实现技巧可以参考《”让Keras更酷一些！”：分层的学习率和自由的梯度》。）\[...\]

[回复评论](https://kexue.fm/archives/6418/comment-page-2?replyTo=20033#respond-post-6418)

[关于CRF层的学习率【转载以学习、回忆】 \| Coding栈](https://www.itcode1024.com/72369/)

December 21st, 2022

\[...\]（附注：增大学习率的实现技巧可以参考《"让Keras更酷一些！"：分层的学习率和自由的梯度》。）\[...\]

[回复评论](https://kexue.fm/archives/6418/comment-page-2?replyTo=20613#respond-post-6418)

1. [«](https://kexue.fm/archives/6418/comment-page-1#comments)
2. [1](https://kexue.fm/archives/6418/comment-page-1#comments)
3. [2](https://kexue.fm/archives/6418/comment-page-2#comments)

[取消回复](https://kexue.fm/archives/6418#respond-post-6418)

你的大名

电子邮箱

个人网站（选填）

1\. 可以使用LaTeX代码，点击“预览效果”可查看效果；2. 可以通过点击评论楼层编号来引用该楼层；3. 网站可能会有点卡，如非确认评论失败，请 **不要重复点击提交**。

### 内容速览

[分层的学习率](https://kexue.fm/kexue.fm#%E5%88%86%E5%B1%82%E7%9A%84%E5%AD%A6%E4%B9%A0%E7%8E%87)
[参数变换下的优化](https://kexue.fm/kexue.fm#%E5%8F%82%E6%95%B0%E5%8F%98%E6%8D%A2%E4%B8%8B%E7%9A%84%E4%BC%98%E5%8C%96)
[移花接木调整学习率](https://kexue.fm/kexue.fm#%E7%A7%BB%E8%8A%B1%E6%8E%A5%E6%9C%A8%E8%B0%83%E6%95%B4%E5%AD%A6%E4%B9%A0%E7%8E%87)
[自由的梯度操作](https://kexue.fm/kexue.fm#%E8%87%AA%E7%94%B1%E7%9A%84%E6%A2%AF%E5%BA%A6%E6%93%8D%E4%BD%9C)
[Keras优化器的结构](https://kexue.fm/kexue.fm#Keras%E4%BC%98%E5%8C%96%E5%99%A8%E7%9A%84%E7%BB%93%E6%9E%84)
[处处皆对象：覆盖即可](https://kexue.fm/kexue.fm#%E5%A4%84%E5%A4%84%E7%9A%86%E5%AF%B9%E8%B1%A1%EF%BC%9A%E8%A6%86%E7%9B%96%E5%8D%B3%E5%8F%AF)
[飘逸的Keras](https://kexue.fm/kexue.fm#%E9%A3%98%E9%80%B8%E7%9A%84Keras)

### 智能搜索

支持整句搜索！网站自动使用 [结巴分词](https://github.com/fxsjy/jieba) 进行分词，并结合ngrams排序算法给出合理的搜索结果。

### 热门标签

[生成模型](https://kexue.fm/tag/%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B/) [attention](https://kexue.fm/tag/attention/) [优化](https://kexue.fm/tag/%E4%BC%98%E5%8C%96/) [语言模型](https://kexue.fm/tag/%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/) [模型](https://kexue.fm/tag/%E6%A8%A1%E5%9E%8B/) [网站](https://kexue.fm/tag/%E7%BD%91%E7%AB%99/) [概率](https://kexue.fm/tag/%E6%A6%82%E7%8E%87/) [梯度](https://kexue.fm/tag/%E6%A2%AF%E5%BA%A6/) [转载](https://kexue.fm/tag/%E8%BD%AC%E8%BD%BD/) [矩阵](https://kexue.fm/tag/%E7%9F%A9%E9%98%B5/) [微分方程](https://kexue.fm/tag/%E5%BE%AE%E5%88%86%E6%96%B9%E7%A8%8B/) [分析](https://kexue.fm/tag/%E5%88%86%E6%9E%90/) [优化器](https://kexue.fm/tag/%E4%BC%98%E5%8C%96%E5%99%A8/) [天象](https://kexue.fm/tag/%E5%A4%A9%E8%B1%A1/) [深度学习](https://kexue.fm/tag/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/) [积分](https://kexue.fm/tag/%E7%A7%AF%E5%88%86/) [python](https://kexue.fm/tag/python/) [力学](https://kexue.fm/tag/%E5%8A%9B%E5%AD%A6/) [无监督](https://kexue.fm/tag/%E6%97%A0%E7%9B%91%E7%9D%A3/) [扩散](https://kexue.fm/tag/%E6%89%A9%E6%95%A3/) [几何](https://kexue.fm/tag/%E5%87%A0%E4%BD%95/) [节日](https://kexue.fm/tag/%E8%8A%82%E6%97%A5/) [生活](https://kexue.fm/tag/%E7%94%9F%E6%B4%BB/) [文本生成](https://kexue.fm/tag/%E6%96%87%E6%9C%AC%E7%94%9F%E6%88%90/) [数论](https://kexue.fm/tag/%E6%95%B0%E8%AE%BA/)

### 随机文章

- [相对运动的一道妙题！](https://kexue.fm/archives/1795)
- [幂等生成网络IGN：试图将判别和生成合二为一的GAN](https://kexue.fm/archives/9969)
- [脑洞大开：非线性RNN居然也可以并行计算？](https://kexue.fm/archives/9783)
- [【备忘】Python中断多重循环的几种思路](https://kexue.fm/archives/4159)
- [《向量》系列——5.平面向量微分方程与复数](https://kexue.fm/archives/963)
- [8天长假结束了](https://kexue.fm/archives/180)
- [朋友们，来瓶汽水吧！有趣的换汽水问题](https://kexue.fm/archives/3495)
- [【语料】2500万中文三元组！](https://kexue.fm/archives/4359)
- [澳大利亚网站请您向外星人问好](https://kexue.fm/archives/77)
- [【空间天文网】2010年天文月历](https://kexue.fm/archives/303)

### 最近评论

- [Lxj](https://kexue.fm/archives/9379/comment-page-1#comment-28534): 苏神好，没有学过格林函数法，找了很多资料但都不太是很明白，求解。苏神有没有兴趣出一期科普一下
- [Lxj](https://kexue.fm/archives/9379/comment-page-1#comment-28533): 苏神好，没有学过格林函数法，找了很多资料但都不太是很明白，求解
- [xxk](https://kexue.fm/archives/35/comment-page-1#comment-28532): 12 个数会有 66 个差，然而差为 19 到 36 的情况最多出现一次，最后问题相当于 48...
- [gapeng](https://kexue.fm/archives/11267/comment-page-1#comment-28531): kimi k2形式上推导了一个公式，最后数值模拟在0.23左右。偏离$\\beta\_1=0.9,...
- [Evan1024](https://kexue.fm/archives/11267/comment-page-1#comment-28530): 太牛了！
- [ameowcat](https://kexue.fm/archives/10958/comment-page-3#comment-28529): 苏神您好，有个问题想请教一下，最近扩散模型的推理优化有一篇文章也是使用ode：https://...
- [Eliot](https://kexue.fm/archives/10815/comment-page-1#comment-28528): 2 实现loss-free with budget应当是在当前Megatron-LM基础上应当...
- [Eliot](https://kexue.fm/archives/10815/comment-page-1#comment-28527): 继续阅读这2份代码后，大概结论如下
1 megatron-lm应当只实现了经典的loss-fr...
- [lzyyzl](https://kexue.fm/archives/11025/comment-page-1#comment-28525): 帮忙解惑一下
1 文中提到本文主题是求O=msign(M)的导数。将∇ML表示为∇OL的函数也...
- [z](https://kexue.fm/archives/11267/comment-page-1#comment-28524): 牛

### 友情链接

- [Cool Papers](https://papers.cool)
- [数学研发](https://bbs.emath.ac.cn)
- [Seatop](http://www.seatop.com.cn/)
- [Xiaoxia](https://xiaoxia.org/)
- [积分表-网络版](https://kexue.fm/sci/integral/index.html)
- [丝路博傲](http://blog.dvxj.com/)
- [数学之家](http://www.2math.cn/)
- [有趣天文奇观](http://interesting-sky.china-vo.org/)
- [TwistedW](http://www.twistedwg.com/)
- [godweiyang](https://godweiyang.com/)
- [AI柠檬](https://blog.ailemon.net/)
- [王登科-DK博客](https://greatdk.com)
- [ESON](https://blog.eson.org/)
- [枫之羽](https://fzhiy.net/)
- [Mathor's blog](https://wmathor.com/)
- [coding-zuo](https://coding-zuo.github.io/)
- [博科园](https://www.bokeyuan.net/)
- [孔皮皮的博客](https://www.kppkkp.top/)
- [运鹏的博客](https://yunpengtai.top/)
- [jiming.site](https://jiming.site/)
- [OmegaXYZ](https://www.omegaxyz.com/)
- [EAI猩球](https://www.robotech.ink/)
- [文举的博客](https://liwenju0.com/)
- [用代码打点酱油](https://bruceyuan.com/)
- [Zhang's blog](https://armcvai.cn/)
- [申请链接](https://kexue.fm/links.html)

本站采用创作共用版权协议，要求署名、非商业用途和保持一致。转载本站内容必须也遵循“ [署名-非商业用途-保持一致](http://creativecommons.org/licenses/by-nc-nd/2.5/cn/)”的创作共用协议。
© 2009-2025 Scientific Spaces. All rights reserved. Theme by [laogui](http://www.laogui.com). Powered by [Typecho](http://typecho.org). 备案号: [粤ICP备09093259号-1/2](https://beian.miit.gov.cn/)。