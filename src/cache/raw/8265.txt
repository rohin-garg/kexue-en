Transformer升级之路：2、博采众长的旋转式位置编码 - 科学空间|Scientific Spaces
![MobileSideBar](https://kexue.fm/usr/themes/geekg/images/slide-button.png "MobileSideBar")
## SEARCH
## MENU
* [打赏](https://kexue.fm/reward.html)
* [公式](https://kexue.fm/latex.html)
* [天象](https://kexue.fm/ac.html)
* [链接](https://kexue.fm/links.html)
* [时光](https://kexue.fm/me.html)
* [博览](https://kexue.fm/science.html)
* [归档](https://kexue.fm/content.html)
## CATEGORIES
* [千奇百怪](https://kexue.fm/category/Everything)
* [天文探索](https://kexue.fm/category/Astronomy)
* [数学研究](https://kexue.fm/category/Mathematics)
* [物理化学](https://kexue.fm/category/Phy-chem)
* [信息时代](https://kexue.fm/category/Big-Data)
* [生物自然](https://kexue.fm/category/Biology)
* [图片摄影](https://kexue.fm/category/Photograph)
* [问题百科](https://kexue.fm/category/Questions)
* [生活/情感](https://kexue.fm/category/Life-Feeling)
* [资源共享](https://kexue.fm/category/Resources)
## NEWPOSTS
* [让炼丹更科学一些（四）：新恒等式，...](https://kexue.fm/archives/11494)
* [为什么DeltaNet要加L2 N...](https://kexue.fm/archives/11486)
* [让炼丹更科学一些（三）：SGD的终...](https://kexue.fm/archives/11480)
* [让炼丹更科学一些（二）：将结论推广...](https://kexue.fm/archives/11469)
* [滑动平均视角下的权重衰减和学习率](https://kexue.fm/archives/11459)
* [生成扩散模型漫谈（三十一）：预测数...](https://kexue.fm/archives/11428)
* [Muon优化器指南：快速上手与关键细节](https://kexue.fm/archives/11416)
* [AdamW的Weight RMS的...](https://kexue.fm/archives/11404)
* [n个正态随机数的最大值的渐近估计](https://kexue.fm/archives/11390)
* [流形上的最速下降：5. 对偶梯度下降](https://kexue.fm/archives/11388)
## COMMENTS
* [且寻: same。本来想转成二重积分的，但想半天整不出来，搞出来这个半...](https://kexue.fm/archives/11480/comment-page-1#comment-29057)
* [苏剑林: 是梯度均值为零的假设。这个问题不是在“数值模拟”一节讨论过了吗...](https://kexue.fm/archives/11267/comment-page-1#comment-29056)
* [苏剑林: X与单位阵的平均平方误差(mse)，作为它跟单位阵的差距，有什...](https://kexue.fm/archives/7180/comment-page-2#comment-29055)
* [苏剑林: 从公式$(12)$到公式$(15)$，都在推导和解释你说的这个...](https://kexue.fm/archives/9209/comment-page-8#comment-29054)
* [苏剑林: 相对位置编码，似乎没有太多选择了，要不RoPE这种，算是乘性了...](https://kexue.fm/archives/8130/comment-page-7#comment-29053)
* [苏剑林: 学习了一下，感觉这个更多是证明，而不是理解？而且这个证明也没有...](https://kexue.fm/archives/11480/comment-page-1#comment-29052)
* [苏剑林: 那还不如直接softmax attention？](https://kexue.fm/archives/11320/comment-page-1#comment-29051)
* [苏剑林: 可以啊，L1 Norm一定大于等于L2 Norm，所以L1 N...](https://kexue.fm/archives/11486/comment-page-1#comment-29050)
* [苏剑林: 你本来要用数值模拟算两重积分，我现在帮你把一重积分算出来了，你...](https://kexue.fm/archives/9119/comment-page-14#comment-29049)
* [苏剑林: learned, thanks](https://kexue.fm/archives/11158/comment-page-1#comment-29048)
## USERLOGIN
* [登录](https://kexue.fm/admin/login.php)
[科学空间|Scientific Spaces](https://kexue.fm)
* [登录](https://kexue.fm/admin/login.php)
* [打赏](https://kexue.fm/reward.html)
* [公式](https://kexue.fm/latex.html)
* [天象](https://kexue.fm/ac.html)
* [链接](https://kexue.fm/links.html)
* [时光](https://kexue.fm/me.html)
* [博览](https://kexue.fm/science.html)
* [归档](https://kexue.fm/content.html)
渴望成为一个小飞侠* [![](https://kexue.fm/usr/themes/geekg/images/rss.png)
欢迎订阅](https://kexue.fm/feed)
* [![](https://kexue.fm/usr/themes/geekg/images/mail.png)
个性邮箱](https://kexue.fm/archives/119)
* [![](https://kexue.fm/usr/themes/geekg/images/Saturn.png)
天象信息](https://kexue.fm/ac.html)
* [![](https://kexue.fm/usr/themes/geekg/images/iss.png)
观测ISS](https://kexue.fm/archives/41)
* [![](https://kexue.fm/usr/themes/geekg/images/pi.png)
LaTeX](https://kexue.fm/latex.html)
* [![](https://kexue.fm/usr/themes/geekg/images/mlogo.png)
关于博主](https://kexue.fm/me.html)
欢迎访问“科学空间”，这里将与您共同探讨自然科学，回味人生百态；也期待大家的分享～* [**千奇百怪**Everything](https://kexue.fm/category/Everything)
* [**天文探索**Astronomy](https://kexue.fm/category/Astronomy)
* [**数学研究**Mathematics](https://kexue.fm/category/Mathematics)
* [**物理化学**Phy-chem](https://kexue.fm/category/Phy-chem)
* [**信息时代**Big-Data](https://kexue.fm/category/Big-Data)
* [**生物自然**Biology](https://kexue.fm/category/Biology)
* [**图片摄影**Photograph](https://kexue.fm/category/Photograph)
* [**问题百科**Questions](https://kexue.fm/category/Questions)
* [**生活/情感**Life-Feeling](https://kexue.fm/category/Life-Feeling)
* [**资源共享**Resources](https://kexue.fm/category/Resources)
* [**千奇百怪**](https://kexue.fm/category/Everything)
* [**天文探索**](https://kexue.fm/category/Astronomy)
* [**数学研究**](https://kexue.fm/category/Mathematics)
* [**物理化学**](https://kexue.fm/category/Phy-chem)
* [**信息时代**](https://kexue.fm/category/Big-Data)
* [**生物自然**](https://kexue.fm/category/Biology)
* [**图片摄影**](https://kexue.fm/category/Photograph)
* [**问题百科**](https://kexue.fm/category/Questions)
* [**生活/情感**](https://kexue.fm/category/Life-Feeling)
* [**资源共享**](https://kexue.fm/category/Resources)
[首页](https://kexue.fm)[信息时代](https://kexue.fm/category/Big-Data)Transformer升级之路：2、博采众长的旋转式位置编码
23Mar
# [Transformer升级之路：2、博采众长的旋转式位置编码](https://kexue.fm/archives/8265)
By苏剑林|2021-03-23|581938位读者|:
上一篇文章中，我们对原始的Sinusoidal位置编码做了较为详细的推导和理解，总的感觉是Sinusoidal位置编码是一种“想要成为相对位置编码的绝对位置编码”。一般来说，绝对位置编码具有实现简单、计算速度快等优点，而相对位置编码则直接地体现了相对位置信号，跟我们的直观理解吻合，实际性能往往也更好。由此可见，如果可以通过绝对位置编码的方式实现相对位置编码，那么就是“集各家之所长”、“鱼与熊掌兼得”了。Sinusoidal位置编码隐约做到了这一点，但并不够好。
本文将会介绍我们自研的Rotary Transformer（RoFormer）模型，它的主要改动是应用了笔者构思的“旋转式位置编码（Rotary Position Embedding，RoPE）”，这是一种配合Attention机制能达到“绝对位置编码的方式实现相对位置编码”的设计。而也正因为这种设计，它还是目前唯一一种可用于线性Attention的相对位置编码。
**> RoFormer：
[> https://github.com/ZhuiyiTechnology/roformer
](https://github.com/ZhuiyiTechnology/roformer)**
## 基本思路[#](#基本思路)
在之前的文章[《让研究人员绞尽脑汁的Transformer位置编码》](https://kexue.fm/archives/8130)中我们就简要介绍过RoPE，当时称之为“融合式”，本文则更加详细地介绍它的来源与性质。在RoPE中，我们的出发点就是“通过绝对位置编码的方式实现相对位置编码”，这样做既有理论上的优雅之处，也有实践上的实用之处，比如它可以拓展到线性Attention中就是主要因为这一点。
为了达到这个目的，我们假设通过下述运算来给$\\boldsymbol{q},\\boldsymbol{k}$添加绝对位置信息：
\\begin{equation}\\tilde{\\boldsymbol{q}}\_m = \\boldsymbol{f}(\\boldsymbol{q}, m), \\quad\\tilde{\\boldsymbol{k}}\_n = \\boldsymbol{f}(\\boldsymbol{k}, n)\\end{equation}
也就是说，我们分别为$\\boldsymbol{q},\\boldsymbol{k}$设计操作$\\boldsymbol{f}(\\cdot, m),\\boldsymbol{f}(\\cdot, n)$，使得经过该操作后，$\\tilde{\\boldsymbol{q}}\_m,\\tilde{\\boldsymbol{k}}\_n$就带有了位置$m,n$的绝对位置信息。Attention的核心运算是内积，所以我们希望的内积的结果带有相对位置信息，因此假设存在恒等关系：
\\begin{equation}\\langle\\boldsymbol{f}(\\boldsymbol{q}, m), \\boldsymbol{f}(\\boldsymbol{k}, n)\\rangle = g(\\boldsymbol{q},\\boldsymbol{k},m-n)\\end{equation}
所以我们要求出该恒等式的一个（尽可能简单的）解。求解过程还需要一些初始条件，显然我们可以合理地设$\\boldsymbol{f}(\\boldsymbol{q}, 0)=\\boldsymbol{q}$和$\\boldsymbol{f}(\\boldsymbol{k}, 0)=\\boldsymbol{k}$。
## 求解过程[#](#求解过程)
同上一篇思路一样，我们先考虑二维情形，然后借助复数来求解。在复数中有$\\langle\\boldsymbol{q},\\boldsymbol{k}\\rangle=\\text{Re}[\\boldsymbol{q}\\boldsymbol{k}^\*]$，$\\text{Re}[]$代表复数的实部，所以我们有
\\begin{equation}\\text{Re}[\\boldsymbol{f}(\\boldsymbol{q}, m)\\boldsymbol{f}^\*(\\boldsymbol{k}, n)] = g(\\boldsymbol{q},\\boldsymbol{k},m-n)\\end{equation}
简单起见，我们假设存在复数$\\boldsymbol{g}(\\boldsymbol{q},\\boldsymbol{k},m-n)$，使得$\\boldsymbol{f}(\\boldsymbol{q}, m)\\boldsymbol{f}^\*(\\boldsymbol{k}, n) = \\boldsymbol{g}(\\boldsymbol{q},\\boldsymbol{k},m-n)$，然后我们用复数的指数形式，设
\\begin{equation}\\begin{aligned}
\\boldsymbol{f}(\\boldsymbol{q}, m) =&\, R_f&\, R_f (\\boldsymbol{q}, m)e^{\\text{i}\\Theta\_f(\\boldsymbol{q}, m)} \\\\
\\boldsymbol{f}(\\boldsymbol{k}, n) =&\, R_f&\, R_f (\\boldsymbol{k}, n)e^{\\text{i}\\Theta\_f(\\boldsymbol{k}, n)} \\\\
\\boldsymbol{g}(\\boldsymbol{q}, \\boldsymbol{k}, m-n) =&\, R_g&\, R_g (\\boldsymbol{q}, \\boldsymbol{k}, m-n)e^{\\text{i}\\Theta\_g(\\boldsymbol{q}, \\boldsymbol{k}, m-n)} \\\\
\\end{aligned}\\end{equation}
那么代入方程后就得到方程组\\begin{equation}\\begin{aligned}
R\_f (\\boldsymbol{q}, m) R\_f (\\boldsymbol{k}, n) =&\, R_g&\, R_g (\\boldsymbol{q}, \\boldsymbol{k}, m-n) \\\\
\\Theta\_f (\\boldsymbol{q}, m) - \\Theta\_f (\\boldsymbol{k}, n) =&\, \Th&\, \Theta\_g (\\boldsymbol{q}, \\boldsymbol{k}, m-n)
\\end{aligned}\\end{equation}
对于第一个方程，代入$m=n$得到
\\begin{equation}R\_f (\\boldsymbol{q}, m) R\_f (\\boldsymbol{k}, m) = R\_g (\\boldsymbol{q}, \\boldsymbol{k}, 0) = R\_f (\\boldsymbol{q}, 0) R\_f (\\boldsymbol{k}, 0) = \\Vert \\boldsymbol{q}\\Vert \\Vert \\boldsymbol{k}\\Vert\\end{equation}
最后一个等号源于初始条件$\\boldsymbol{f}(\\boldsymbol{q}, 0)=\\boldsymbol{q}$和$\\boldsymbol{f}(\\boldsymbol{k}, 0)=\\boldsymbol{k}$。所以现在我们可以很简单地设$R\_f (\\boldsymbol{q}, m)=\\Vert \\boldsymbol{q}\\Vert, R\_f (\\boldsymbol{k}, m)=\\Vert \\boldsymbol{k}\\Vert$，即它不依赖于$m$。至于第二个方程，同样代入$m=n$得到
\\begin{equation}\\Theta\_f (\\boldsymbol{q}, m) - \\Theta\_f (\\boldsymbol{k}, m) = \\Theta\_g (\\boldsymbol{q}, \\boldsymbol{k}, 0) = \\Theta\_f (\\boldsymbol{q}, 0) - \\Theta\_f (\\boldsymbol{k}, 0) = \\Theta (\\boldsymbol{q}) - \\Theta (\\boldsymbol{k})\\end{equation}
这里的$\\Theta (\\boldsymbol{q}),\\Theta (\\boldsymbol{k})$是$\\boldsymbol{q},\\boldsymbol{k}$本身的幅角，最后一个等号同样源于初始条件。根据上式得到$\\Theta\_f (\\boldsymbol{q}, m) - \\Theta (\\boldsymbol{q}) = \\Theta\_f (\\boldsymbol{k}, m) - \\Theta (\\boldsymbol{k})$，所以$\\Theta\_f (\\boldsymbol{q}, m) - \\Theta (\\boldsymbol{q})$应该是一个只与$m$相关、跟$\\boldsymbol{q}$无关的函数，记为$\\varphi(m)$，即$\\Theta\_f (\\boldsymbol{q}, m) = \\Theta (\\boldsymbol{q}) + \\varphi(m)$。接着代入$n=m-1$，整理得到
\\begin{equation}\\varphi(m) - \\varphi(m-1) = \\Theta\_g (\\boldsymbol{q}, \\boldsymbol{k}, 1) + \\Theta (\\boldsymbol{k}) - \\Theta (\\boldsymbol{q})\\end{equation}
即$\\{\\varphi(m)\\}$是等差数列，设右端为$\\theta$，那么就解得$\\varphi(m)=m\\theta$。
## 编码形式[#](#编码形式)
综上，我们得到二维情况下用复数表示的RoPE：
\\begin{equation}
\\boldsymbol{f}(\\boldsymbol{q}, m) = R\_f (\\boldsymbol{q}, m)e^{\\text{i}\\Theta\_f(\\boldsymbol{q}, m)}
= \\Vert q\\Vert e^{\\text{i}(\\Theta(\\boldsymbol{q}) + m\\theta)} = \\boldsymbol{q} e^{\\text{i}m\\theta}\\end{equation}
根据复数乘法的几何意义，该变换实际上对应着向量的旋转，所以我们称之为“旋转式位置编码”，它还可以写成矩阵形式：\\begin{equation}
\\boldsymbol{f}(\\boldsymbol{q}, m) =\\begin{pmatrix}\\cos m\\theta & -\sin& -\sin m\\theta\\\\ \\sin m\\theta & \cos & \cos m\\theta\\end{pmatrix} \\begin{pmatrix}q\_0 \\\\ q\_1\\end{pmatrix}\\end{equation}
由于内积满足线性叠加性，因此任意偶数维的RoPE，我们都可以表示为二维情形的拼接，即
\\begin{equation}\\scriptsize{\\underbrace{\\begin{pmatrix}
\\cos m\\theta\_0 & -\sin& -\sin m\\theta\_0 & 0 & 0& 0 & 0 & \cdot& \cdots & 0 & 0& 0 & 0 \\\\
\\sin m\\theta\_0 & \cos & \cos m\\theta\_0 & 0 & 0& 0 & 0 & \cdot& \cdots & 0 & 0& 0 & 0 \\\\
0 & 0 & \& 0 & \cos m\\theta\_1 & -\sin& -\sin m\\theta\_1 & \cdot& \cdots & 0 & 0& 0 & 0 \\\\
0 & 0 & \& 0 & \sin m\\theta\_1 & \cos & \cos m\\theta\_1 & \cdot& \cdots & 0 & 0& 0 & 0 \\\\
\\vdots & \vdot& \vdots & \vdot& \vdots & \vdot& \vdots & \ddot& \ddots & \vdot& \vdots & \vdot& \vdots \\\\
0 & 0 & 0& 0 & 0 & 0 & \& 0 & \cdots & \cos & \cos m\\theta\_{d/2-1} & -\sin& -\sin m\\theta\_{d/2-1} \\\\
0 & 0 & 0& 0 & 0 & 0 & \& 0 & \cdots & \sin & \sin m\\theta\_{d/2-1} & \cos & \cos m\\theta\_{d/2-1} \\\\
\\end{pmatrix}}\_{\\boldsymbol{\\mathcal{R}}\_m} \\begin{pmatrix}q\_0 \\\\ q\_1 \\\\ q\_2 \\\\ q\_3 \\\\ \\vdots \\\\ q\_{d-2} \\\\ q\_{d-1}\\end{pmatrix}}\\end{equation}
也就是说，给位置为$m$的向量$\\boldsymbol{q}$乘上矩阵$\\boldsymbol{\\mathcal{R}}\_m$、位置为$n$的向量$\\boldsymbol{k}$乘上矩阵$\\boldsymbol{\\mathcal{R}}\_n$，用变换后的$\\boldsymbol{Q},\\boldsymbol{K}$序列做Attention，那么Attention就自动包含相对位置信息了，因为成立恒等式：
\\begin{equation}(\\boldsymbol{\\mathcal{R}}\_m \\boldsymbol{q})^{\\top}(\\boldsymbol{\\mathcal{R}}\_n \\boldsymbol{k}) = \\boldsymbol{q}^{\\top} \\boldsymbol{\\mathcal{R}}\_m^{\\top}\\boldsymbol{\\mathcal{R}}\_n \\boldsymbol{k} = \\boldsymbol{q}^{\\top} \\boldsymbol{\\mathcal{R}}\_{n-m} \\boldsymbol{k}\\end{equation}
值得指出的是，$\\boldsymbol{\\mathcal{R}}\_m$是一个正交矩阵，它不会改变向量的模长，因此通常来说它不会改变原模型的稳定性。
由于$\\boldsymbol{\\mathcal{R}}\_m$的稀疏性，所以直接用矩阵乘法来实现会很浪费算力，推荐通过下述方式来实现RoPE：
\\begin{equation}\\begin{pmatrix}q\_0 \\\\ q\_1 \\\\ q\_2 \\\\ q\_3 \\\\ \\vdots \\\\ q\_{d-2} \\\\ q\_{d-1}
\\end{pmatrix}\\otimes\\begin{pmatrix}\\cos m\\theta\_0 \\\\ \\cos m\\theta\_0 \\\\ \\cos m\\theta\_1 \\\\ \\cos m\\theta\_1 \\\\ \\vdots \\\\ \\cos m\\theta\_{d/2-1} \\\\ \\cos m\\theta\_{d/2-1}
\\end{pmatrix} + \\begin{pmatrix}-q\_1 \\\\ q\_0 \\\\ -q\_3 \\\\ q\_2 \\\\ \\vdots \\\\ -q\_{d-1} \\\\ q\_{d-2}
\\end{pmatrix}\\otimes\\begin{pmatrix}\\sin m\\theta\_0 \\\\ \\sin m\\theta\_0 \\\\ \\sin m\\theta\_1 \\\\ \\sin m\\theta\_1 \\\\ \\vdots \\\\ \\sin m\\theta\_{d/2-1} \\\\ \\sin m\\theta\_{d/2-1}
\\end{pmatrix}\\end{equation}
其中$\\otimes$是逐位对应相乘，即Numpy、Tensorflow等计算框架中的$\*$运算。从这个实现也可以看到，RoPE可以视为是乘性位置编码的变体。
## 远程衰减[#](#远程衰减)
可以看到，RoPE形式上和Sinusoidal位置编码有点相似，只不过Sinusoidal位置编码是加性的，而RoPE可以视为乘性的。在$\\theta\_i$的选择上，我们同样沿用了Sinusoidal位置编码的方案，即$\\theta\_i = 10000^{-2i/d}$，它可以带来一定的远程衰减性。
具体证明如下：将$\\boldsymbol{q},\\boldsymbol{k}$两两分组后，它们加上RoPE后的内积可以用复数乘法表示为
\\begin{equation}
(\\boldsymbol{\\mathcal{R}}\_m \\boldsymbol{q})^{\\top}(\\boldsymbol{\\mathcal{R}}\_n \\boldsymbol{k}) = \\text{Re}\\left[\\sum\_{i=0}^{d/2-1}\\boldsymbol{q}\_{[2i:2i+1]}\\boldsymbol{k}\_{[2i:2i+1]}^\* e^{\\text{i}(m-n)\\theta\_i}\\right]\\end{equation}
记$h\_i = \\boldsymbol{q}\_{[2i:2i+1]}\\boldsymbol{k}\_{[2i:2i+1]}^\*, S\_j = \\sum\\limits\_{i=0}^{j-1} e^{\\text{i}(m-n)\\theta\_i}$，并约定$h\_{d/2}=0,S\_0=0$，那么由[Abel变换（分部求和法）](https://zh.wikipedia.org/wiki/分部求和法)可以得到：
\\begin{equation}\\sum\_{i=0}^{d/2-1}\\boldsymbol{q}\_{[2i:2i+1]}\\boldsymbol{k}\_{[2i:2i+1]}^\* e^{\\text{i}(m-n)\\theta\_i} = \\sum\_{i=0}^{d/2-1} h\_i (S\_{i
+1} - S\_i) = -\\sum\_{i=0}^{d/2-1} S\_{i+1}(h\_{i+1} - h\_i)\\end{equation}
所以\\begin{equation}\\begin{aligned}
\\left|\\sum\_{i=0}^{d/2-1}\\boldsymbol{q}\_{[2i:2i+1]}\\boldsymbol{k}\_{[2i:2i+1]}^\* e^{\\text{i}(m-n)\\theta\_i}\\right| =&\, \le&\, \left|\\sum\_{i=0}^{d/2-1} S\_{i+1}(h\_{i+1} - h\_i)\\right| \\\\
\\leq&\, \su&\, \sum\_{i=0}^{d/2-1} |S\_{i+1}| |h\_{i+1} - h\_i| \\\\
\\leq&\, \le&\, \left(\\max\_i |h\_{i+1} - h\_i|\\right)\\sum\_{i=0}^{d/2-1} |S\_{i+1}|
\\end{aligned}\\end{equation}
因此我们可以考察$\\frac{1}{d/2}\\sum\\limits\_{i=1}^{d/2} |S\_i|$随着相对距离的变化情况来作为衰减性的体现，Mathematica代码如下：
```
`d = 128; \\[Theta][t\_] = 10000^(-2\*t/d); f[m\_] = Sum[ Norm[Sum[Exp[I\*m\*\\[Theta][i]], {i, 0, j}]], {j, 0, d/2 - 1}]/(d/2); Plot[f[m], {m, 0, 256}, AxesLabel -\> {相对距离, 相对大小}]`
```
结果如下图：[![RoPE的远程衰减性（d=128）](https://kexue.fm/usr/uploads/2021/03/1347893165.png)](https://kexue.fm/usr/uploads/2021/03/1347893165.png)
RoPE的远程衰减性（d=128）
从图中我们可以可以看到随着相对距离的变大，内积结果有衰减趋势的出现。因此，选择$\\theta\_i = 10000^{-2i/d}$，确实能带来一定的远程衰减性。当然，同上一篇文章说的一样，能带来远程衰减性的不止这个选择，几乎任意的光滑单调函数都可以，这里只是沿用了已有的选择而已。笔者还试过以$\\theta\_i = 10000^{-2i/d}$为初始化，将$\\theta\_i$视为可训练参数，然后训练一段时间后发现$\\theta\_i$并没有显著更新，因此干脆就直接固定$\\theta\_i = 10000^{-2i/d}$了。
## 线性场景[#](#线性场景)
最后，我们指出，RoPE是目前唯一一种可以用于线性Attention的相对位置编码。这是因为其他的相对位置编码，都是直接基于Attention矩阵进行操作的，但是线性Attention并没有事先算出Attention矩阵，因此也就不存在操作Attention矩阵的做法，所以其他的方案无法应用到线性Attention中。而对于RoPE来说，它是用绝对位置编码的方式来实现相对位置编码，不需要操作Attention矩阵，因此有了应用到线性Attention的可能性。
关于线性Attention的介绍，这里不再重复，有需要的读者请参考[《线性Attention的探索：Attention必须有个Softmax吗？》](https://kexue.fm/archives/7546)。线性Attention的常见形式是：
\\begin{equation}Attention(\\boldsymbol{Q},\\boldsymbol{K},\\boldsymbol{V})\_i = \\frac{\\sum\\limits\_{j=1}^n \\text{sim}(\\boldsymbol{q}\_i, \\boldsymbol{k}\_j)\\boldsymbol{v}\_j}{\\sum\\limits\_{j=1}^n \\text{sim}(\\boldsymbol{q}\_i, \\boldsymbol{k}\_j)} = \\frac{\\sum\\limits\_{j=1}^n \\phi(\\boldsymbol{q}\_i)^{\\top} \\varphi(\\boldsymbol{k}\_j)\\boldsymbol{v}\_j}{\\sum\\limits\_{j=1}^n \\phi(\\boldsymbol{q}\_i)^{\\top} \\varphi(\\boldsymbol{k}\_j)}\\end{equation}
其中$\\phi,\\varphi$是值域非负的激活函数。可以看到，线性Attention也是基于内积的，所以很自然的想法是可以将RoPE插入到内积中：
\\begin{equation}\\frac{\\sum\\limits\_{j=1}^n [\\boldsymbol{\\mathcal{R}}\_i\\phi(\\boldsymbol{q}\_i)]^{\\top} [\\boldsymbol{\\mathcal{R}}\_j\\varphi(\\boldsymbol{k}\_j)]\\boldsymbol{v}\_j}{\\sum\\limits\_{j=1}^n [\\boldsymbol{\\mathcal{R}}\_i\\phi(\\boldsymbol{q}\_i)]^{\\top} [\\boldsymbol{\\mathcal{R}}\_j\\varphi(\\boldsymbol{k}\_j)]}\\end{equation}
但这样存在的问题是，内积$[\\boldsymbol{\\mathcal{R}}\_i\\phi(\\boldsymbol{q}\_i)]^{\\top} [\\boldsymbol{\\mathcal{R}}\_j\\varphi(\\boldsymbol{k}\_j)]$可能为负数，因此它不再是常规的概率注意力，而且分母有为0的风险，可能会带来优化上的不稳定。考虑到$\\boldsymbol{\\mathcal{R}}\_i,\\boldsymbol{\\mathcal{R}}\_j$都是正交矩阵，它不改变向量的模长，因此我们可以抛弃常规的概率归一化要求，使用如下运算作为一种新的线性Attention：
\\begin{equation}\\frac{\\sum\\limits\_{j=1}^n [\\boldsymbol{\\mathcal{R}}\_i\\phi(\\boldsymbol{q}\_i)]^{\\top} [\\boldsymbol{\\mathcal{R}}\_j\\varphi(\\boldsymbol{k}\_j)]\\boldsymbol{v}\_j}{\\sum\\limits\_{j=1}^n \\phi(\\boldsymbol{q}\_i)^{\\top} \\varphi(\\boldsymbol{k}\_j)}\\end{equation}
也就是说，RoPE只插入分子中，而分母则不改变，这样的注意力不再是基于概率的（注意力矩阵不再满足非负归一性），但它某种意义上来说也是一个归一化方案，而且也没有证据表明非概率式的注意力就不好（比如[Nyströmformer](https://kexue.fm/archives/8180)也算是没有严格依据概率分布的方式构建注意力），所以我们将它作为候选方案之一进行实验，而我们初步的实验结果显示这样的线性Attention也是有效的。
此外，笔者在[《线性Attention的探索：Attention必须有个Softmax吗？》](https://kexue.fm/archives/7546)中还提出过另外一种线性Attention方案：$\\text{sim}(\\boldsymbol{q}\_i, \\boldsymbol{k}\_j) = 1 + \\left( \\frac{\\boldsymbol{q}\_i}{\\Vert \\boldsymbol{q}\_i\\Vert}\\right)^{\\top}\\left(\\frac{\\boldsymbol{k}\_j}{\\Vert \\boldsymbol{k}\_j\\Vert}\\right)$，它不依赖于值域的非负性，而RoPE也不改变模长，因此RoPE可以直接应用于此类线性Attention，并且不改变它的概率意义。
## 模型开源[#](#模型开源)
RoFormer的第一版模型，我们已经完成训练并开源到了Github中：
**> RoFormer：
[> https://github.com/ZhuiyiTechnology/roformer
](https://github.com/ZhuiyiTechnology/roformer)**
简单来说，RoFormer是一个绝对位置编码替换为RoPE的[WoBERT](https://github.com/ZhuiyiTechnology/WoBERT)模型，它跟其他模型的结构对比如下：
\\begin{array}{c|cccc}
\\hline
& \text& \text{BERT} & \text& \text{WoBERT} & \text& \text{NEZHA} & \text& \text{RoFormer} \\\\
\\hline
\\text{token单位} & \text& \text{字} & \text& \text{词} & \text& \text{字} & \text& \text{词} & \\
\\text{位置编码} & \text& \text{绝对位置} & \text& \text{绝对位置} & \text& \text{经典式相对位置} & \text& \text{RoPE}\\\\
\\hline
\\end{array}
在预训练上，我们以WoBERT Plus为基础，采用了多个长度和batch size交替训练的方式，让模型能提前适应不同的训练场景：
\\begin{array}{c|ccccc}
\\hline
& \text& \text{maxlen} & \text& \text{batch size} & \text& \text{训练步数} & \text& \text{最终loss} & \text& \text{最终acc}\\\\
\\hline
1 & 512 && 512 & 256 & 20\te& 20\text{万} & 1.73 & 1.73 & 65.0\& 65.0\%\\\\
2 & 1536 & 1536 & 256 && 256 & 1.25\\text{万} & 1.61 & 1.61 & 66.8\& 66.8\%\\\\
3 & 256 && 256 & 256 & 12\te& 12\text{万} & 1.75 & 1.75 & 64.6\& 64.6\%\\\\
4 & 128 && 128 & 512 & 8\tex& 8\text{万} & 1.83 & 1.83 & 63.4\& 63.4\%\\\\
5 & 1536 & 1536 & 256 && 256 & 1\\text{万} & 1.58 & 1.58 & 67.4\& 67.4\%\\\\
6 & 512 && 512 & 512 & 3\tex& 3\text{万} & 1.66 & 1.66 & 66.2\& 66.2\%\\\\
\\hline
\\end{array}
从表格还可以看到，增大序列长度，预训练的准确率反而有所提升，这侧面体现了RoFormer长文本语义的处理效果，也体现了RoPE具有良好的外推能力。在短文本任务上，RoFormer与WoBERT的表现类似，RoFormer的主要特点是可以直接处理任意长的文本。下面是我们在[CAIL2019-SCM](https://papers.cool/arxiv/1911.08962)任务上的实验结果：
\\begin{array}{c|cc}
\\hline
& \text& \text{验证集} & \text& \text{测试集} \\\\
\\hline
\\text{BERT-512} & 64.13& 64.13\\% & 67.77& 67.77\\% \\\\
\\text{WoBERT-512} & 64.07& 64.07\\% & 68.10& 68.10\\% \\\\
\\text{RoFormer-512} & 64.13& 64.13\\% & 68.29& 68.29\\% \\\\
\\text{RoFormer-1024} & \text& \textbf{66.07%} & \text& \textbf{69.79%} \\\\
\\hline
\\end{array}
其中$\\text{-}$后面的参数是微调时截断的maxlen，可以看到RoFormer确实能较好地处理长文本语义，至于设备要求，在24G显存的卡上跑maxlen=1024，batch\_size可以跑到8以上。目前中文任务中笔者也就找到这个任务比较适合作为长文本能力的测试，所以长文本方面只测了这个任务，欢迎读者进行测试或推荐其他评测任务。
当然，尽管理论上RoFormer能处理任意长度的序列，但目前RoFormer还是具有平方复杂度的，我们也正在训练基于线性Attention的RoFormer模型，实验完成后也会开源放出，请大家期待。
（注：RoPE和RoFormer已经整理成文[《RoFormer: Enhanced Transformer with Rotary Position Embedding》](https://papers.cool/arxiv/2104.09864)提交到了Arxiv，欢迎使用和引用哈哈～）
## 文章小结[#](#文章小结)
本文介绍了我们自研的旋转式位置编码RoPE以及对应的预训练模型RoFormer。从理论上来看，RoPE与Sinusoidal位置编码有些相通之处，但RoPE不依赖于泰勒展开，更具严谨性与可解释性；从预训练模型RoFormer的结果来看，RoPE具有良好的外推性，应用到Transformer中体现出较好的处理长文本的能力。此外，RoPE还是目前唯一一种可用于线性Attention的相对位置编码。
***转载到请包括本文地址：** [https://kexue.fm/archives/8265](https://kexue.fm/archives/8265)*
***更详细的转载事宜请参考：*** [《科学空间FAQ》](https://kexue.fm/archives/6508#文章如何转载/引用)
**如果您还有什么疑惑或建议，欢迎在下方评论区继续讨论。**
**如果您觉得本文还不错，欢迎[分享](#share)/[打赏](#pay)本文。打赏并非要从中获得收益，而是希望知道科学空间获得了多少读者的真心关注。当然，如果你无视它，也不会影响你的阅读。再次表示欢迎和感谢！**
打赏![科学空间](https://kexue.fm/usr/themes/geekg/payment/wx.png)
微信打赏![科学空间](https://kexue.fm/usr/themes/geekg/payment/zfb.png)
支付宝打赏因为网站后台对打赏并无记录，因此欢迎在打赏时候备注留言。 你还可以[**点击这里**](http://mail.qq.com/cgi-bin/qm_share?t=qm_mailme&email=tN7d1drY3drrx8H0xcWa19vZ)或在下方评论区留言来告知你的建议或需求。
**如果您需要引用本文，请参考：**
苏剑林. (Mar. 23, 2021). 《Transformer升级之路：2、博采众长的旋转式位置编码 》[Blog post]. Retrieved from[https://kexue.fm/archives/8265](https://kexue.fm/archives/8265)
@online{kexuefm-8265,
title={Transformer升级之路：2、博采众长的旋转式位置编码},
author={苏剑林},
year={2021},
month={Mar},
url={\\url{https://kexue.fm/archives/8265}},
}
分类：[信息时代](https://kexue.fm/category/Big-Data) 标签：[复数](https://kexue.fm/tag/复数/),[语言模型](https://kexue.fm/tag/语言模型/),[attention](https://kexue.fm/tag/attention/),[位置编码](https://kexue.fm/tag/位置编码/),[rope](https://kexue.fm/tag/rope/)[181 评论](https://kexue.fm/archives/8265#comments)
&lt;[WGAN的成功，可能跟Wasserstein距离没啥关系](https://kexue.fm/archives/8244)|[P-tuning：自动构建模版，释放语言模型潜能](https://kexue.fm/archives/8295)&gt;
### 你也许还对下面的内容感兴趣* [为什么DeltaNet要加L2 Normalize？](https://kexue.fm/archives/11486)
* [低精度Attention可能存在有偏的舍入误差](https://kexue.fm/archives/11371)
* [为什么线性注意力要加Short Conv？](https://kexue.fm/archives/11320)
* [QK-Clip：让Muon在Scaleup之路上更进一步](https://kexue.fm/archives/11126)
* [Transformer升级之路：21、MLA好在哪里?（下）](https://kexue.fm/archives/11111)
* [“对角+低秩”三角阵的高效求逆方法](https://kexue.fm/archives/11072)
* [线性注意力简史：从模仿、创新到反哺](https://kexue.fm/archives/11033)
* [Transformer升级之路：20、MLA好在哪里?（上）](https://kexue.fm/archives/10907)
* [Transformer升级之路：19、第二类旋转位置编码](https://kexue.fm/archives/10862)
* [细水长flow之TARFLOW：流模型满血归来？](https://kexue.fm/archives/10667)
[发表你的看法](#comment_form)
1. [&laquo;](https://kexue.fm/archives/8265/comment-page-7#comments)
2. [1](https://kexue.fm/archives/8265/comment-page-1#comments)
3. ...
4. [5](https://kexue.fm/archives/8265/comment-page-5#comments)
5. [6](https://kexue.fm/archives/8265/comment-page-6#comments)
6. [7](https://kexue.fm/archives/8265/comment-page-7#comments)
7. [8](https://kexue.fm/archives/8265/comment-page-8#comments)
[You could have designed state of the art positional encoding](https://aiupdateshub.com/you-could-have-designed-state-of-the-art-positional-encoding/)
March 23rd, 2025
[...]RoFormer paper (Jianlin Su designed it independently on his blog here and here).[...]
[回复评论](https://kexue.fm/archives/8265/comment-page-8?replyTo=27213#respond-post-8265)
huangbei
May 29th, 2025
rope远程衰减的图像似乎非常震荡？这样的震荡性质会不会对表达能力有一定影响呢？
[回复评论](https://kexue.fm/archives/8265/comment-page-8?replyTo=27731#respond-post-8265)
[苏剑林](https://kexue.fm)发表于 June 3rd, 2025
说不准振荡才是本质？比如十进制编码也是很振荡的（考虑$n \\% 10$，结果是$n$的周期函数）
[回复评论](https://kexue.fm/archives/8265/comment-page-8?replyTo=27748#respond-post-8265)
shixf 发表于June 3rd, 2025
苏神，微信交流群和QQ交流群申请了，可以审批一下不\~
[回复评论](https://kexue.fm/archives/8265/comment-page-8?replyTo=27759#respond-post-8265)
[苏剑林](https://kexue.fm)发表于 June 3rd, 2025
有缘会通过～[回复评论](https://kexue.fm/archives/8265/comment-page-8?replyTo=27762#respond-post-8265)
sk
June 17th, 2025
请问公式14是怎么得出来的？
[回复评论](https://kexue.fm/archives/8265/comment-page-8?replyTo=27906#respond-post-8265)
[盏一](http://hidva.com)发表于 June 18th, 2025
我之前做的笔记:
Q: 公式(14) 的理解.
A: 首先基于[定理 5](https://blog.hidva.com/2024/07/14/Lax-Linear-Algebra-9.5/), [9.3.zy3](https://blog.hidva.com/2024/07/14/Lax-Linear-Algebra-9.3/) 可知矩阵级数$e^A = \\sum\_{k=0}^{\\infty}\\frac{A^k}{k!}$ 绝对收敛, 且Tao 定理8.2.2 关于无限和的富比尼定理对于矩阵级数也成立. 即矩阵级数求和时可以任意交换次序.
之后结合[陶哲轩实分析: 再看三角函数](https://blog.hidva.com/2024/05/26/analysis-ii-tenrece-tao-sin/) 了解三角函数的级数定义:
$$
\\begin{align}
\\cos(z) &= 1 - &= 1 - \\frac{z^2}{2!} + \\frac{z^4}{4!} - \\cdots = \\sum\_{n=0}^{\\infty} \\frac{(-1)^n z^{2n}}{(2n)!} \\\\
\\sin(z) &= z - &= z - \\frac{z^3}{3!} + \\frac{z^5}{5!} - \\cdots = \\sum\_{n=0}^{\\infty} \\frac{(-1)^n z^{2n+1}}{(2n+1)!}
\\end{align}
$$
就可以很容易求出来了.
[回复评论](https://kexue.fm/archives/8265/comment-page-8?replyTo=27911#respond-post-8265)
[苏剑林](https://kexue.fm)发表于 June 20th, 2025
就是将2维的$\\langle\\boldsymbol{q},\\boldsymbol{k}\\rangle=\\text{Re}[\\boldsymbol{q}\\boldsymbol{k}^\*]$平行推广到多维呀。
[回复评论](https://kexue.fm/archives/8265/comment-page-8?replyTo=27936#respond-post-8265)
xing112
July 12th, 2025
你好，请问为何使用$\\frac{1}{d/2} \\sum\_{i=1}^{d/2} |S\_i|$ 作为观察衰减的指标?
[回复评论](https://kexue.fm/archives/8265/comment-page-8?replyTo=28084#respond-post-8265)
[苏剑林](https://kexue.fm)发表于 July 14th, 2025
只是表明它会被一个具有衰减趋势的上界给bound住，实际上这个上界也很粗糙。
[回复评论](https://kexue.fm/archives/8265/comment-page-8?replyTo=28101#respond-post-8265)
xing112 发表于July 14th, 2025
好的，谢谢[回复评论](https://kexue.fm/archives/8265/comment-page-8?replyTo=28130#respond-post-8265)
王协August 12th, 2025
苏神，我在推导2维情况下RoPE对自注意力计算结果的影响时，推导出RoPE使用cos((m−n)θ)
引入相对位置信息，对原始注意力做了缩放.同时引入了反对称项A，且A受与相对位置有关的sin((m−n)θ)
调节。这个反对称项A该如何理解？注意力计算之后多出的Asin((m−n)θ)这一偏置项对模型训练有何影响？
我的推导过程如下：引入位置信息后，我们在计算$x\_m,x\_n$的内积时是在计算:
$$(x\_mp\_m)^\\top x\_np\_n=x\_m^\\top x\_np\_m^\\top p\_n$$
为简化推理，我们将$m\\theta,n\\theta$分别记为$\\alpha ,\\beta$:
$$
p\_m =
\\begin{bmatrix}
cos\\alpha& -sin& -sin\\alpha\\\\
sin\\alpha& cos\& cos\alpha
\\end{bmatrix},
p\_n =
\\begin{bmatrix}
cos\\beta& -sin& -sin\\beta\\\\
sin\\beta& cos\& cos\beta
\\end{bmatrix},
p\_m^\\top =
\\begin{bmatrix}
cos\\alpha& sin\& sin\alpha\\\\
-sin\\alpha& cos\& cos\alpha
\\end{bmatrix}
$$
$$
p\_m^\\top p\_n =
\\begin{bmatrix}
cos\\alpha cos\\beta+sin\\alpha sin\\beta & -cos\& -cos\alpha sin\\beta +sin\\alpha cos\\beta \\\\
-sin\\alpha cos\\beta + cos\\alpha sin\\beta & sin\a& sin\alpha sin\\beta+cos\\alpha cos\\beta
\\end{bmatrix}
$$
可化简为：$$
p\_m^\\top p\_n =
\\begin{bmatrix}
\\cos(\\alpha - \\beta) & \sin(& \sin(\\alpha - \\beta) \\\\
-\\sin(\\alpha - \\beta) & \cos(& \cos(\\alpha - \\beta)
\\end{bmatrix}
$$
将$\\alpha - \\beta$记为$\\delta $，并将上述矩阵代入回内积计算公式：
$$
x\_m^\\top
\\begin{bmatrix}
\\cos\\delta & \sin\& \sin\delta \\\\
-\\sin\\delta & \cos\& \cos\delta
\\end{bmatrix} x\_n =
\\begin{bmatrix} x\_{m,0} & x_{m,& x_{m,1} \\end{bmatrix}
\\begin{bmatrix}
\\cos\\delta & \sin\& \sin\delta \\\\
-\\sin\\delta & \cos\& \cos\delta
\\end{bmatrix}
\\begin{bmatrix} x\_{n,0} \\\\ x\_{n,1} \\end{bmatrix}
$$
先计算中间步骤：$$
\\begin{bmatrix}
\\cos\\delta & \sin\& \sin\delta \\\\
-\\sin\\delta & \cos\& \cos\delta
\\end{bmatrix}
\\begin{bmatrix} x\_{n,0} \\\\ x\_{n,1} \\end{bmatrix} =
\\begin{bmatrix}
x\_{n,0} \\cos\\delta + x\_{n,1} \\sin\\delta \\\\
-x\_{n,0} \\sin\\delta + x\_{n,1} \\cos\\delta
\\end{bmatrix}
$$
然后点积：$$
\\begin{bmatrix} x\_{m,0} & x_{m,& x_{m,1} \\end{bmatrix}
\\begin{bmatrix}
x\_{n,0} \\cos\\delta + x\_{n,1} \\sin\\delta \\\\
-x\_{n,0} \\sin\\delta + x\_{n,1} \\cos\\delta
\\end{bmatrix} = x\_{m,0} (x\_{n,0} \\cos\\delta + x\_{n,1} \\sin\\delta) + x\_{m,1} (-x\_{n,0} \\sin\\delta + x\_{n,1} \\cos\\delta)
$$
简化：$$
= x\_{m,0} x\_{n,0} \\cos\\delta + x\_{m,0} x\_{n,1} \\sin\\delta - x\_{m,1} x\_{n,0} \\sin\\delta + x\_{m,1} x\_{n,1} \\cos\\delta
$$
$$
= (x\_{m,0} x\_{n,0} + x\_{m,1} x\_{n,1}) \\cos\\delta + (x\_{m,0} x\_{n,1} - x\_{m,1} x\_{n,0}) \\sin\\delta
$$
得到：- 原始内积（语义相似度）：$\\langle x\_m, x\_n \\rangle = x\_{m,0} x\_{n,0} + x\_{m,1} x\_{n,1}$
- 一个反对称项：$A = x\_{m,0} x\_{n,1} - x\_{m,1} x\_{n,0}$
最终内积为：$$
\\langle x\_mp\_m, x\_np\_n \\rangle = \\langle x\_m, x\_n \\rangle \\cos((m - n)\\theta) + A \\sin((m - n)\\theta)
$$
使用$cos((m - n)\\theta$引入位置信息，对原始注意力做了缩放，同时引入了反对称项A，且A受与相对位置有关的$\\sin((m - n)\\theta$调节
[回复评论](https://kexue.fm/archives/8265/comment-page-8?replyTo=28360#respond-post-8265)
[苏剑林](https://kexue.fm)发表于 August 12th, 2025
1、你这推导有点繁琐了，其实就是$\\langle\\boldsymbol{q} e^{\\text{i}\\alpha},\\boldsymbol{k}e^{\\text{i}\\beta}\\rangle=\\text{Re}[\\boldsymbol{q}\\boldsymbol{k}^\* e^{\\text{i}(\\alpha - \\beta)}]$，然后保留实部就行；
2、对于我个人来说，其实不大能接受这种分解的诠释，所以我也没法谈太多，不过你这个角度，让我想起了[https://kexue.fm/archives/10122](https://kexue.fm/archives/10122)，你可以看看有没有参考价值。
[回复评论](https://kexue.fm/archives/8265/comment-page-8?replyTo=28377#respond-post-8265)
Eliot
September 11th, 2025
苏神，关于从式子(10)到(11)的理解；二维度向量，式子(10)的旋转很好理解；
对于高维变量下，式子(11)是一个分组 旋转再合并的过程，(10)可以作为一个二维向量的旋转表达，但是(11)是不是不能直接等价于高维度向量的空间旋转(数学上我不是很明确这点，希望苏神可以解答)；
然后如果(11)不等价于高维变量的空间旋转，那么(11)的理解 我和gemini-2.5 brainstrom了一下，是否可以理解为这里和通信领域的psk编码有一定共同之处，一个多bit信号被分解到多个窄带然后调制；gemini还给了一些其他的理解分析，但psk个人觉得是有比较多共同之处的，位置编码暗合通信编码
算法/概念 核心思想与RoPE的关联
:--- :--- :---
\*\*正弦位置编码\*\* 使用不同频率的sin/cos对来表示位置，然后与词向量相加。 \*\*直接的祖先\*\*。RoPE将其从“加法”模型升级为更优越的“旋转（乘法）”模型。
\*\*相移键控 (PSK)\*\* 通过改变载波的相位来编码离散的数字信息。\*\*物理/工程上的完美类比\*\*。RoPE的每个二维旋转就是在用相位编码位置 `m`。
\*\*小波变换\*\* 使用不同尺度的小波来同时分析信号的频率和位置信息。\*\*数学/功能上的深刻类比\*\*。两者都体现了强大的“多尺度/多分辨率”思想。
[回复评论](https://kexue.fm/archives/8265/comment-page-8?replyTo=28541#respond-post-8265)
[苏剑林](https://kexue.fm)发表于 September 13th, 2025
第一个问题：确实不能，公式$(11)$的结果并不是原始高维向量在高维空间中的旋转。
第二个问题：psk编码我不了解，去查了一下，单看形式好像真有点相似。
至于用相位编码$m$这个理解没啥问题的，你可以看看[https://kexue.fm/archives/9675](https://kexue.fm/archives/9675)，可能会有所收获。一个更直接地理解是，RoPE是用$m$的$\\beta$进制编码的多个数字（而不是$m$这个数字本身）来编码。
[回复评论](https://kexue.fm/archives/8265/comment-page-8?replyTo=28553#respond-post-8265)
Eliot 发表于September 15th, 2025
谢谢苏神回复：beta编码那个文章发完评论翻其他评论之后看到了，也再学习下的；本科学的是EE，统计信号处理里面有不少对于信道非平稳平稳状况下如何有效传递信息的编码理论，我再综合理解下的。
[回复评论](https://kexue.fm/archives/8265/comment-page-8?replyTo=28559#respond-post-8265)
[Minimind-一个开源LLM项目的代码分析1：模型结构 | 呱唧呱唧网](https://www.itfaba.com/jishufenxian/232846.html)
September 21st, 2025
[...]Rope（Su. et al.）：[https://spaces.ac.cn/archives/8265](https://spaces.ac.cn/archives/8265)[...]
[回复评论](https://kexue.fm/archives/8265/comment-page-8?replyTo=28575#respond-post-8265)
pb
September 23rd, 2025
苏神您好，这两天读到“远程衰减”的部分时觉得有些奇怪，还请指正。文中的upperbound 似乎并不比trivial bound 好：1. 通过Abel 变换后得到的upperbound 形如$ h \\cdot \\sum\_{i=0}^{k/2-1} S\_i $ ，并通过模拟得出$\\sum\_{i=0}^{k/1-2} S\_i $ 会衰减到$\\Theta(k)$ 级别，因此整体的bound 是$\\Theta(k\\cdot h )$ ；2. 原式形如$\\sum\_{i=0}^{k/2-1} h\_i \\cdot e^{i (m-n) \\theta\_i}$ ，且$e^{i (m-n) \\theta\_i}$ 是unit vector ，因此直接对每一项分别bound ，即可得到$\\Theta(k\\cdot h )$ 。事实上，因为每个$e^{i(m-n)\\theta\_i}$ 都是unit vector ，所以至少会有一半的$S\_i$ 长度在$\\Omega(1)$ 级别，用Abel 并不会得到更好的上界。[回复评论](https://kexue.fm/archives/8265/comment-page-8?replyTo=28586#respond-post-8265)
[苏剑林](https://kexue.fm)发表于 October 3rd, 2025
你说的没错，这个上界实际上非常粗糙。所以这个远程衰减更多是想表达两个事情，第一是它能被一个有远程衰减趋势的界给bound住，第二是它有潜力实现（而不是已经实现）远程衰减。
[回复评论](https://kexue.fm/archives/8265/comment-page-8?replyTo=28605#respond-post-8265)
pb 发表于October 5th, 2025
感谢回复。个人认为给两个完全没有限制的向量分析上界的意义不大，因为对于任意的$m,n$ 都存在两个向量使得它们旋转之后取到trivial bound $k\\cdot h$ 。我想用我目前对RoPE 的远程衰减的理解抛砖引玉：我们对每个feature vector $x$ 分开考虑，因为在高维空间里$\\Theta(k)$ 个random vector 大概率是近似正交的，不同的feature 基本不会互相影响（即使经过RoPE 的旋转）。如果$q\_i$ 和$k\_j$ 同时包含$x$ 的分量，比如$\\langle q\_i,x\\rangle = 0.5, \\langle k\_j,x\\rangle = 0.7$ ，那么在不加入位置编码时$x$ 就会给$(i,j)$ 的attention 强度贡献$0.35$ 。加入位置编码后，两边的$x$ 会先经过旋转再做点积，旋转角度相差越大，点积就越小，因此attention 强度会随着$ i-j $ 衰减，衰减的速度由$x$ 的位置是高频还是低频区域决定。请问这样理解有道理吗？[回复评论](https://kexue.fm/archives/8265/comment-page-8?replyTo=28624#respond-post-8265)
[苏剑林](https://kexue.fm)发表于 October 6th, 2025
对，事后来看，这个上界的衰减性意义不大。不过这个上界是有机会达到的（虽然概率很小），所以算是给出了远程衰减的一个具体实现模式吧。您后面的解释，个人感觉不完全对，因为$\\cos,\\sin$都不是单调递增/递减函数，它们是周期函数，不管是增减都只是局部成立的，真正实现远程衰减效应的，需要多个不同的、连续的频率的周期函数进行叠加，具体例子正好可以参考上界的样子。
[回复评论](https://kexue.fm/archives/8265/comment-page-8?replyTo=28628#respond-post-8265)
pb 发表于October 6th, 2025
谢谢，我也自己画了画图，的确如您所说，多个频率叠加可以得到比较有趣的衰减效应。[回复评论](https://kexue.fm/archives/8265/comment-page-8?replyTo=28631#respond-post-8265)
tianshu\_wu
November 11th, 2025
我跟着博客内容，学习推导了Sinusoidal和Rope两个方法。
我有一个感悟，不知道对不对：Rope是和Sinusoidal一样，每个embedding内部，每两个元素一个旋转频率，应对不同token长度。
长句子靠后位置token，embedding后边元素旋转慢。
句子内前部位置token，embedding前边元素旋转速度快。
这样可以避免远距离因为周期性，被套圈，导致分不清距离。被旋转后向量再做内积，旋转后模长不变，内积就近似于求两个向量夹角。这个夹角就是相对位置。embedding内不能有效区分距离这些编码对，约等于共线平行，内积或者夹角就会很小。
被区分出距离那部分编码对，做内积，内积夹角大，就体现出了相对位置。是不是从向量内积找夹角就是相对位置，也理解Rope。
Sinusoidal编码等同于在内积这步没有Rope更明显体现相对位置，因为编码加到了embedding上。
感谢您的博客，期待您的回复，谢谢。[回复评论](https://kexue.fm/archives/8265/comment-page-8?replyTo=28792#respond-post-8265)
tianshu\_wu 发表于November 11th, 2025
Sorry，写错了，如果向量倾向于共线，应该是内积大，倾向于正交时内积小
[回复评论](https://kexue.fm/archives/8265/comment-page-8?replyTo=28793#respond-post-8265)
[苏剑林](https://kexue.fm)发表于 November 18th, 2025
对的。通过不同周期的三角函数混合，来保持整体的有界性，同时突破周期性的限制，这是位置编码的重要特点。RoPE比Sinusoidal更突出相对位置，这个也没问题。
[回复评论](https://kexue.fm/archives/8265/comment-page-8?replyTo=28827#respond-post-8265)
Cuddle
November 13th, 2025
苏神，在linear-attention的应用场景中，如果去掉分母中的 rope好像需要多一部分计算；不然的话可以参考sana直接在 value 后pad 一个1，QKV计算完后再把pad的部分取出来就可以直接当做分母
[回复评论](https://kexue.fm/archives/8265/comment-page-8?replyTo=28812#respond-post-8265)
[苏剑林](https://kexue.fm)发表于 November 18th, 2025
都可以，不过事实上现在linear已经不流行除分母了，参考：[https://kexue.fm/archives/11033](https://kexue.fm/archives/11033)
[回复评论](https://kexue.fm/archives/8265/comment-page-8?replyTo=28837#respond-post-8265)
1. [&laquo;](https://kexue.fm/archives/8265/comment-page-7#comments)
2. [1](https://kexue.fm/archives/8265/comment-page-1#comments)
3. ...
4. [5](https://kexue.fm/archives/8265/comment-page-5#comments)
5. [6](https://kexue.fm/archives/8265/comment-page-6#comments)
6. [7](https://kexue.fm/archives/8265/comment-page-7#comments)
7. [8](https://kexue.fm/archives/8265/comment-page-8#comments)
[取消回复](https://kexue.fm/archives/8265#respond-post-8265)
你的大名电子邮箱个人网站（选填）1. 可以使用LaTeX代码，点击“预览效果”可查看效果；
2. 可以通过点击评论楼层编号来引用该楼层；3. 网站可能会有点卡，如非确认评论失败，请**不要重复点击提交**。
********************
### 内容速览8. [基本思路](#基本思路)
9. [求解过程](#求解过程)
10. [编码形式](#编码形式)
11. [远程衰减](#远程衰减)
12. [线性场景](#线性场景)
13. [模型开源](#模型开源)
14. [文章小结](#文章小结)
********************
### 智能搜索支持整句搜索！网站自动使用[结巴分词](https://github.com/fxsjy/jieba)进行分词，并结合ngrams排序算法给出合理的搜索结果。
********************
### 热门标签[生成模型](https://kexue.fm/tag/生成模型/)[attention](https://kexue.fm/tag/attention/)[优化](https://kexue.fm/tag/优化/)[语言模型](https://kexue.fm/tag/语言模型/)[模型](https://kexue.fm/tag/模型/)[网站](https://kexue.fm/tag/网站/)[梯度](https://kexue.fm/tag/梯度/)[概率](https://kexue.fm/tag/概率/)[矩阵](https://kexue.fm/tag/矩阵/)[优化器](https://kexue.fm/tag/优化器/)[转载](https://kexue.fm/tag/转载/)[微分方程](https://kexue.fm/tag/微分方程/)[分析](https://kexue.fm/tag/分析/)[天象](https://kexue.fm/tag/天象/)[深度学习](https://kexue.fm/tag/深度学习/)[积分](https://kexue.fm/tag/积分/)[python](https://kexue.fm/tag/python/)[扩散](https://kexue.fm/tag/扩散/)[力学](https://kexue.fm/tag/力学/)[无监督](https://kexue.fm/tag/无监督/)[几何](https://kexue.fm/tag/几何/)[节日](https://kexue.fm/tag/节日/)[生活](https://kexue.fm/tag/生活/)[文本生成](https://kexue.fm/tag/文本生成/)[数论](https://kexue.fm/tag/数论/)
********************
********************
### 随机文章* [关于自由落体公式的简单修正](https://kexue.fm/archives/584)
* [通向最优分布之路：概率空间的最小化](https://kexue.fm/archives/10289)
* [三连杆装置曲线方程](https://kexue.fm/archives/1168)
* [RoFormerV2：自然语言理解的极限探索](https://kexue.fm/archives/8998)
* [[更新]将向量乘法“退化”到复数](https://kexue.fm/archives/1188)
* [一维弹簧的运动（下）](https://kexue.fm/archives/2434)
* [网站PR升到3了！](https://kexue.fm/archives/335)
* [施密特系统的校正镜方程求解](https://kexue.fm/archives/1257)
* [SimBERTv2来了！融合检索和生成的RoFormer-Sim模型](https://kexue.fm/archives/8454)
* [《重逢》——最终亚运会会歌](https://kexue.fm/archives/960)
********************
********************
### 最近评论* [且寻](https://kexue.fm/archives/11480/comment-page-1#comment-29057): same。本来想转成二重积分的，但想半天整不出来，搞出来这个半成品...
* [苏剑林](https://kexue.fm/archives/11267/comment-page-1#comment-29056): 是梯度均值为零的假设。这个问题不是在“数值模拟”一节讨论过了吗？而且最终结果跟实际观测是接近的...
* [苏剑林](https://kexue.fm/archives/7180/comment-page-2#comment-29055): X与单位阵的平均平方误差(mse)，作为它跟单位阵的差距，有什么问题？当然取误差最大值也是一个...
* [苏剑林](https://kexue.fm/archives/9209/comment-page-8#comment-29054): 从公式$(12)$到公式$(15)$，都在推导和解释你说的这个事啊，以$\\mathbb{E}\_...
* [苏剑林](https://kexue.fm/archives/8130/comment-page-7#comment-29053): 相对位置编码，似乎没有太多选择了，要不RoPE这种，算是乘性了，要不Alibi这种加性。你还想...
* [苏剑林](https://kexue.fm/archives/11480/comment-page-1#comment-29052): 学习了一下，感觉这个更多是证明，而不是理解？而且这个证明也没有比原始证明简单。我说的缺乏直观理...
* [苏剑林](https://kexue.fm/archives/11320/comment-page-1#comment-29051): 那还不如直接softmax attention？
* [苏剑林](https://kexue.fm/archives/11486/comment-page-1#comment-29050): 可以啊，L1 Norm一定大于等于L2 Norm，所以L1 Norm归一化后模长小于等于1
* [苏剑林](https://kexue.fm/archives/9119/comment-page-14#comment-29049): 你本来要用数值模拟算两重积分，我现在帮你把一重积分算出来了，你只需要算一重积分了，怎么会更难呢...
* [苏剑林](https://kexue.fm/archives/11158/comment-page-1#comment-29048): learned, thanks
********************
********************
### 友情链接* [Cool Papers](https://papers.cool)
* [数学研发](https://bbs.emath.ac.cn)
* [Seatop](http://www.seatop.com.cn/)
* [Xiaoxia](https://xiaoxia.org/)
* [积分表-网络版](https://kexue.fm/sci/integral/index.html)
* [丝路博傲](http://blog.dvxj.com/)
* [数学之家](http://www.2math.cn/)
* [有趣天文奇观](http://interesting-sky.china-vo.org/)
* [TwistedW](http://www.twistedwg.com/)
* [godweiyang](https://godweiyang.com/)
* [AI柠檬](https://blog.ailemon.net/)
* [王登科-DK博客](https://greatdk.com)
* [ESON](https://blog.eson.org/)
* [枫之羽](https://fzhiy.net/)
* [coding-zuo](https://coding-zuo.github.io/)
* [博科园](https://www.bokeyuan.net/)
* [孔皮皮的博客](https://www.kppkkp.top/)
* [运鹏的博客](https://yunpengtai.top/)
* [jiming.site](https://jiming.site/)
* [OmegaXYZ](https://www.omegaxyz.com/)
* [EAI猩球](https://www.robotech.ink/)
* [文举的博客](https://liwenju0.com/)
* [申请链接](https://kexue.fm/links.html)
********************
[![署名-非商业用途-保持一致](https://kexue.fm/usr/themes/geekg/images/cc.gif)](http://creativecommons.org/licenses/by-nc-nd/2.5/cn/)本站采用创作共用版权协议，要求署名、非商业用途和保持一致。转载本站内容必须也遵循“[署名-非商业用途-保持一致](http://creativecommons.org/licenses/by-nc-nd/2.5/cn/)”的创作共用协议。
©2009-2025 Scientific Spaces. All rights reserved. Theme by[laogui](http://www.laogui.com). Powered by[Typecho](http://typecho.org). 备案号:[粤ICP备09093259号-1/2](https://beian.miit.gov.cn/)。