Loading \[MathJax\]/extensions/TeX/boldsymbol.js

![MobileSideBar](https://kexue.fm/usr/themes/geekg/images/slide-button.png)

## SEARCH

## MENU

- [打赏](https://kexue.fm/reward.html)
- [公式](https://kexue.fm/latex.html)
- [天象](https://kexue.fm/ac.html)
- [链接](https://kexue.fm/links.html)
- [时光](https://kexue.fm/me.html)
- [博览](https://kexue.fm/science.html)
- [归档](https://kexue.fm/content.html)

## CATEGORIES

- [千奇百怪](https://kexue.fm/category/Everything)
- [天文探索](https://kexue.fm/category/Astronomy)
- [数学研究](https://kexue.fm/category/Mathematics)
- [物理化学](https://kexue.fm/category/Phy-chem)
- [信息时代](https://kexue.fm/category/Big-Data)
- [生物自然](https://kexue.fm/category/Biology)
- [图片摄影](https://kexue.fm/category/Photograph)
- [问题百科](https://kexue.fm/category/Questions)
- [生活/情感](https://kexue.fm/category/Life-Feeling)
- [资源共享](https://kexue.fm/category/Resources)

## NEWPOSTS

- [让炼丹更科学一些（五）：基于梯度精...](https://kexue.fm/archives/11530)
- [让炼丹更科学一些（四）：新恒等式，...](https://kexue.fm/archives/11494)
- [为什么DeltaNet要加L2 N...](https://kexue.fm/archives/11486)
- [让炼丹更科学一些（三）：SGD的终...](https://kexue.fm/archives/11480)
- [让炼丹更科学一些（二）：将结论推广...](https://kexue.fm/archives/11469)
- [滑动平均视角下的权重衰减和学习率](https://kexue.fm/archives/11459)
- [生成扩散模型漫谈（三十一）：预测数...](https://kexue.fm/archives/11428)
- [Muon优化器指南：快速上手与关键细节](https://kexue.fm/archives/11416)
- [AdamW的Weight RMS的...](https://kexue.fm/archives/11404)
- [n个正态随机数的最大值的渐近估计](https://kexue.fm/archives/11390)

## COMMENTS

- [Bin: 今天偶然从某个论坛看到有人推荐您的博客，定睛一看竟然是华师同院...](https://kexue.fm/archives/1990/comment-page-2#comment-29105)
- [Rapture D: 我有一个问题，为什么不考虑亥姆霍兹定理和斯托克斯公式。](https://kexue.fm/archives/11530/comment-page-1#comment-29104)
- [mofheka: 苏神是还在用jax是么？最近在做基于Google Pathwa...](https://kexue.fm/archives/11390/comment-page-1#comment-29103)
- [长琴: 看懂这篇博客也不是一件容易的事情。](https://kexue.fm/archives/11530/comment-page-1#comment-29102)
- [AlexLi: 苏老师，请教一下(7)式中将 μ(xt) 传给 $p...](https://kexue.fm/archives/9257/comment-page-4#comment-29101)
- [tyler\_zxc: "Performer的思想是将标准的Attention线性化，...](https://kexue.fm/archives/7921/comment-page-2#comment-29100)
- [我: 似乎并非mHC提出矩阵的思想？之前hyper connecti...](https://kexue.fm/archives/11494/comment-page-1#comment-29099)
- [winter: 苏神您好，假如对于比较均匀的attention weightP...](https://kexue.fm/archives/10847/comment-page-1#comment-29098)
- [苏剑林: KL散度、JS散度、W距离啥的，都行啊，看你喜欢哪个](https://kexue.fm/archives/8512/comment-page-2#comment-29097)
- [苏剑林: 没有绝对公平的对比方法，主要看你关心什么。比如，如果只关心推理...](https://kexue.fm/archives/9119/comment-page-14#comment-29096)

## USERLOGIN

- [登录](https://kexue.fm/admin/login.php)

[科学空间\|Scientific Spaces](https://kexue.fm/)

- [登录](https://kexue.fm/admin/login.php)
- [打赏](https://kexue.fm/reward.html)
- [公式](https://kexue.fm/latex.html)
- [天象](https://kexue.fm/ac.html)
- [链接](https://kexue.fm/links.html)
- [时光](https://kexue.fm/me.html)
- [博览](https://kexue.fm/science.html)
- [归档](https://kexue.fm/content.html)

渴望成为一个小飞侠

- [![](https://kexue.fm/usr/themes/geekg/images/rss.png)\\
\\
欢迎订阅](https://kexue.fm/feed)
- [![](https://kexue.fm/usr/themes/geekg/images/mail.png)\\
\\
个性邮箱](https://kexue.fm/archives/119)
- [![](https://kexue.fm/usr/themes/geekg/images/Saturn.png)\\
\\
天象信息](https://kexue.fm/ac.html)
- [![](https://kexue.fm/usr/themes/geekg/images/iss.png)\\
\\
观测ISS](https://kexue.fm/archives/41)
- [![](https://kexue.fm/usr/themes/geekg/images/pi.png)\\
\\
LaTeX](https://kexue.fm/latex.html)
- [![](https://kexue.fm/usr/themes/geekg/images/mlogo.png)\\
\\
关于博主](https://kexue.fm/me.html)

欢迎访问“科学空间”，这里将与您共同探讨自然科学，回味人生百态；也期待大家的分享～

- [**千奇百怪** Everything](https://kexue.fm/category/Everything)
- [**天文探索** Astronomy](https://kexue.fm/category/Astronomy)
- [**数学研究** Mathematics](https://kexue.fm/category/Mathematics)
- [**物理化学** Phy-chem](https://kexue.fm/category/Phy-chem)
- [**信息时代** Big-Data](https://kexue.fm/category/Big-Data)
- [**生物自然** Biology](https://kexue.fm/category/Biology)
- [**图片摄影** Photograph](https://kexue.fm/category/Photograph)
- [**问题百科** Questions](https://kexue.fm/category/Questions)
- [**生活/情感** Life-Feeling](https://kexue.fm/category/Life-Feeling)
- [**资源共享** Resources](https://kexue.fm/category/Resources)

- [**千奇百怪**](https://kexue.fm/category/Everything)
- [**天文探索**](https://kexue.fm/category/Astronomy)
- [**数学研究**](https://kexue.fm/category/Mathematics)
- [**物理化学**](https://kexue.fm/category/Phy-chem)
- [**信息时代**](https://kexue.fm/category/Big-Data)
- [**生物自然**](https://kexue.fm/category/Biology)
- [**图片摄影**](https://kexue.fm/category/Photograph)
- [**问题百科**](https://kexue.fm/category/Questions)
- [**生活/情感**](https://kexue.fm/category/Life-Feeling)
- [**资源共享**](https://kexue.fm/category/Resources)

[首页](https://kexue.fm/) [数学研究](https://kexue.fm/category/Mathematics) 你可能不需要BERT-flow：一个线性变换媲美BERT-flow

11Jan

# [你可能不需要BERT-flow：一个线性变换媲美BERT-flow](https://kexue.fm/archives/8069)

By 苏剑林 \|
2021-01-11 \|
328888位读者 \|

BERT-flow来自论文 [《On the Sentence Embeddings from Pre-trained Language Models》](https://papers.cool/arxiv/2011.05864)，中了EMNLP 2020，主要是用flow模型校正了BERT出来的句向量的分布，从而使得计算出来的cos相似度更为合理一些。由于笔者定时刷Arixv的习惯，早在它放到Arxiv时笔者就看到了它，但并没有什么兴趣，想不到前段时间小火了一把，短时间内公众号、知乎等地出现了不少的解读，相信读者们多多少少都被它刷屏了一下。

从实验结果来看，BERT-flow确实是达到了一个新SOTA，但对于这一结果，笔者的第一感觉是：不大对劲！当然，不是说结果有问题，而是根据笔者的理解，flow模型不大可能发挥关键作用。带着这个直觉，笔者做了一些分析，果不其然，笔者发现尽管BERT-flow的思路没有问题，但只要一个线性变换就可以达到相近的效果，flow模型并不是十分关键。

## 余弦相似度的假设 [\#](https://kexue.fm/archives/8069\#%E4%BD%99%E5%BC%A6%E7%9B%B8%E4%BC%BC%E5%BA%A6%E7%9A%84%E5%81%87%E8%AE%BE)

一般来说，我们语义相似度比较或检索，都是给每个句子算出一个句向量来，然后算它们的夹角余弦来比较或者排序。那么，我们有没有思考过这样的一个问题：余弦相似度对所输入的向量提出了什么假设呢？或者说，满足什么条件的向量用余弦相似度做比较效果会更好呢？

我们知道，两个向量\\boldsymbol{x},\\boldsymbol{y}的内积的几何意义就是“各自的模长乘以它们的夹角余弦”，所以余弦相似度就是两个向量的内积并除以各自的模长，对应的坐标计算公式是

\\begin{equation}\\cos(\\boldsymbol{x},\\boldsymbol{y}) = \\frac{\\sum\\limits\_{i=1}^d x\_i y\_i}{\\sqrt{\\sum\\limits\_{i=1}^d x\_i^2} \\sqrt{\\sum\\limits\_{i=1}^d y\_i^2}}\\label{eq:cos}\\end{equation}

然而，别忘了一件事情，上述等号只在“标准正交基”下成立。换句话说，向量的“夹角余弦”本身是具有鲜明的几何意义的，但上式右端只是坐标的运算，坐标依赖于所选取的坐标基，基底不同，内积对应的坐标公式就不一样，从而余弦值的坐标公式也不一样。

因此，假定BERT句向量已经包含了足够的语义（比如可以重构出原句子），那么如果它用公式\\eqref{eq:cos}算余弦值来比较句子相似度时表现不好，那么原因可能就是此时的句向量所属的坐标系并非标准正交基。那么，我们怎么知道它具体用了哪种基底呢？原则上没法知道，但是我们可以去猜。猜测的依据是我们在给向量集合选择基底时，会尽量地平均地用好每一个基向量，从统计学的角度看，这就体现为每个分量的使用都是独立的、均匀的，如果这组基是标准正交基，那么对应的向量集应该表现出“各向同性”来。

当然，这不算是什么推导，只是一个启发式引导，它告诉我们如果一个向量的集合满足各向同性，那么我们可以认为它源于标准正交基，此时可以考虑用式\\eqref{eq:cos}算相似度；反之，如果它并不满足各向同性，那么可以想办法让它变得更加各向同性一些，然后再用式\\eqref{eq:cos}算相似度，而BERT-flow正是想到了“flow模型”这个办法。

## flow模型的碎碎念 [\#](https://kexue.fm/archives/8069\#flow%E6%A8%A1%E5%9E%8B%E7%9A%84%E7%A2%8E%E7%A2%8E%E5%BF%B5)

依笔者来看，flow模型真的是一种让人觉得一言难尽的模型了，关于它的碎碎念又可以写上几页纸，这里尽量长话短说。2018年中，OpenAI发布了Glow模型，效果看起来很不错，这吸引了笔者进一步去学习flow模型，甚至还去复现了一把Glow模型，相关工作记录在 [《细水长flow之NICE：流模型的基本概念与实现》](https://kexue.fm/archives/5776) 和 [《细水长flow之RealNVP与Glow：流模型的传承与升华》](https://kexue.fm/archives/5807) 中，如果还不了解flow模型的，欢迎去看看这两篇博客。简单来说，flow模型是一个向量变换模型，它可以将输入数据的分布转化为标准正态分布，而显然标准正态分布是各向同性的，所以BERT-flow就选择了flow模型。

那么flow模型有什么毛病吗？其实之前在文章 [《细水长flow之可逆ResNet：极致的暴力美学》](https://kexue.fm/archives/6482) 就已经吐槽过了，这里重复一下：

> （flow模型）通过比较巧妙的设计，使得模型每一层的逆变换比较简单，而且雅可比矩阵是一个三角阵，从而雅可比行列式很容易计算。这样的模型在理论上很优雅漂亮，但是有一个很严重的问题：由于必须保证逆变换简单和雅可比行列式容易计算，那么每一层的非线性变换能力都很弱。事实上像Glow这样的模型，每一层只有一半的变量被变换，所以为了保证充分的拟合能力，模型就必须堆得非常深（比如256的人脸生成，Glow模型堆了大概600个卷积层，两亿参数量），计算量非常大。

看到这里，读者就能理解为什么笔者开头说看到BERT-flow的第一感觉就是“不对劲”了。上述吐槽告诉我们，flow模型其实是很弱的；然后BERT-flow里边所用的flow模型是多大呢？是一个level=2、depth=3的Glow模型，这两个参数大家可能没什么概念，反正就是很小，以至于整个模型并没有增加什么计算量。所以，笔者的“不对劲”直觉就是：

> flow模型本身很弱，BERT-flow里边使用的flow模型更弱，所以flow模型不大可能在BERT-flow中发挥至关重要的作用。反过来想，那就是也许我们可以找到更简单直接的方法达到BERT-flow的效果。

## 标准化协方差矩阵 [\#](https://kexue.fm/archives/8069\#%E6%A0%87%E5%87%86%E5%8C%96%E5%8D%8F%E6%96%B9%E5%B7%AE%E7%9F%A9%E9%98%B5)

经过探索，笔者还真找到了这样的方法，正如本文标题所说，它只是一个线性变换。

其实思想很简单，我们知道标准正态分布的均值为0、协方差矩阵为单位阵，那么我们不妨将句向量的均值变换为0、协方差矩阵变换为单位阵试试看？假设（行）向量集合为\\{\\boldsymbol{x}\_i\\}\_{i=1}^N，我们执行变换

\\begin{equation}\\tilde{\\boldsymbol{x}}\_i = (\\boldsymbol{x}\_i - \\boldsymbol{\\mu})\\boldsymbol{W}
\\end{equation}

使得\\{\\tilde{\\boldsymbol{x}}\_i\\}\_{i=1}^N的均值为0、协方差矩阵为单位阵。了解传统数据挖掘的读者可能知道，这实际上就相当于传统数据挖掘中的白化操作（Whitening），所以该方法笔者称之为BERT-whitening。

均值为0很简单，让\\boldsymbol{\\mu}=\\frac{1}{N}\\sum\\limits\_{i=1}^N \\boldsymbol{x}\_i即可，有点难度的是\\boldsymbol{W}矩阵的求解。将原始数据的协方差矩阵记为

\\begin{equation}\\boldsymbol{\\Sigma}=\\frac{1}{N}\\sum\\limits\_{i=1}^N (\\boldsymbol{x}\_i - \\boldsymbol{\\mu})^{\\top}(\\boldsymbol{x}\_i - \\boldsymbol{\\mu})=\\left(\\frac{1}{N}\\sum\\limits\_{i=1}^N \\boldsymbol{x}\_i^{\\top}\\boldsymbol{x}\_i\\right) - \\boldsymbol{\\mu}^{\\top}\\boldsymbol{\\mu}\\end{equation}

那么不难得到变换后的数据协方差矩阵为\\tilde{\\boldsymbol{\\Sigma}}=\\boldsymbol{W}^{\\top}\\boldsymbol{\\Sigma}\\boldsymbol{W}，所以我们实际上要解方程

\\begin{equation}\\boldsymbol{W}^{\\top}\\boldsymbol{\\Sigma}\\boldsymbol{W}=\\boldsymbol{I}\\quad\\Rightarrow \\quad \\boldsymbol{\\Sigma} = \\left(\\boldsymbol{W}^{\\top}\\right)^{-1}\\boldsymbol{W}^{-1} = \\left(\\boldsymbol{W}^{-1}\\right)^{\\top}\\boldsymbol{W}^{-1}\\end{equation}

我们知道协方差矩阵\\boldsymbol{\\Sigma}是一个半正定对称矩阵，且数据够多时它通常都是正定的，具有如下形式的SVD分解

\\begin{equation}\\boldsymbol{\\Sigma} = \\boldsymbol{U}\\boldsymbol{\\Lambda}\\boldsymbol{U}^{\\top}\\end{equation}

其中\\boldsymbol{U}是一个正交矩阵，而\\boldsymbol{\\Lambda}是一个对角阵，并且对角线元素都是正的，因此直接让\\boldsymbol{W}^{-1}=\\sqrt{\\boldsymbol{\\Lambda}}\\boldsymbol{U}^{\\top}就可以完成求解：

\\begin{equation}\\boldsymbol{W} = \\boldsymbol{U}\\sqrt{\\boldsymbol{\\Lambda}^{-1}}\\end{equation}

Numpy的参考代码为：

```python

```

可能会有人问答大语料怎么办的问题。首先，上述算法只需要知道全体句向量的均值向量\\boldsymbol{\\mu}\\in\\mathbb{R}^{d}和协方差矩阵\\boldsymbol{\\Sigma}\\in\\mathbb{R}^{d\\times d}（d是词向量维度），\\boldsymbol{\\mu}是全体句向量\\boldsymbol{x}\_i的均值，均值是可以递归计算的：

\\begin{equation}\\boldsymbol{\\mu}\_{n+1} = \\frac{n}{n+1}\\boldsymbol{\\mu}\_{n} + \\frac{1}{n+1}\\boldsymbol{x}\_{n+1}\\end{equation}

同理，协方差矩阵\\boldsymbol{\\Sigma}也只不过是全体\\boldsymbol{x}\_i^{\\top}\\boldsymbol{x}\_i的均值再减去\\boldsymbol{\\mu}^{\\top}\\boldsymbol{\\mu}，自然也是可以递归计算的：

\\begin{equation}\\boldsymbol{\\Sigma}\_{n+1} = \\frac{n}{n+1}\\left(\\boldsymbol{\\Sigma}\_{n}+\\boldsymbol{\\mu}\_{n}^{\\top}\\boldsymbol{\\mu}\_{n}\\right) + \\frac{1}{n+1}\\boldsymbol{x}\_{n+1}^{\\top}\\boldsymbol{x}\_{n+1}-\\boldsymbol{\\mu}\_{n+1}^{\\top}\\boldsymbol{\\mu}\_{n+1}\\end{equation}

既然可以递归，那么就意味着我们是可以在有限内存下计算\\boldsymbol{\\mu},\\boldsymbol{\\Sigma}的，因此对于大语料来说BERT-whitening也不成问题的。

## 相比于BERT-flow [\#](https://kexue.fm/archives/8069\#%E7%9B%B8%E6%AF%94%E4%BA%8EBERT-flow)

现在，我们就可以测试一下上述BERT-whitening的效果了。为了跟BERT-flow对比，笔者用bert4keras在STS-B任务上进行了测试，参考脚本在：

> **Github链接： [https://github.com/bojone/BERT-whitening](https://github.com/bojone/BERT-whitening)**

效果比较如下：

\\begin{array}{l\|c}
\\hline
& \\,\\,\\text{STS-B}\\,\\, \\\
\\hline
\\text{BERT}\_{\\text{base}}\\text{-last2avg}\\,(\\text{论文结果}) & 59.04 \\\
\\text{BERT}\_{\\text{base}}\\text{-flow}\\,(\\text{target, 论文结果}) & 70.72 \\\
\\text{BERT}\_{\\text{base}}\\text{-last2avg}\\,(\\text{个人复现}) & 59.04 \\\
\\text{BERT}\_{\\text{base}}\\text{-whitening}\\,(\\text{target, 个人实现}) & 71.20 \\\
\\hline
\\text{BERT}\_{\\text{large}}\\text{-last2avg}\\,(\\text{论文结果}) & 59.56 \\\
\\text{BERT}\_{\\text{large}}\\text{-flow}\\,(\\text{target, 论文结果}) & 72.26 \\\
\\text{BERT}\_{\\text{large}}\\text{-last2avg}\\,(\\text{个人复现}) & 59.59 \\\
\\text{BERT}\_{\\text{large}}\\text{-whitening}\\,(\\text{target, 个人实现}) & 71.98 \\\
\\hline
\\end{array}

可以看到，简单的BERT-whitening确实能取得跟BERT-flow媲美的结果。除了STS-B之外，笔者的同事在中文业务数据内做了类似的比较，结果都表明BERT-flow带来的提升跟BERT-whitening是相近的，这表明，flow模型的引入可能没那么必要了，因为flow模型的层并非常见的层，它需要专门的实现，并且训练起来也有一定的工作量，而BERT-whitening的实现很简单，就一个线性变换，可以轻松套到任意的句向量模型中。（当然，非要辩的话，也可以说whitening是用线性变换实现的flow模型...）

> 注：这里顺便补充一句，BERT-flow论文里边说的last2avg，本来含义是最后两层输出的平均向量，但它的代码实际上是“第一层+最后一层”输出的平均向量，相关讨论参考 [ISSUE](https://github.com/bohanli/BERT-flow/issues/11)。

## 降维效果还能更好 [\#](https://kexue.fm/archives/8069\#%E9%99%8D%E7%BB%B4%E6%95%88%E6%9E%9C%E8%BF%98%E8%83%BD%E6%9B%B4%E5%A5%BD)

现在我们知道BERT-whitening的变换矩阵\\boldsymbol{W} = \\boldsymbol{U}\\sqrt{\\boldsymbol{\\Lambda}^{-1}}可以将数据的协方差矩阵变换成单位阵，如果我们不考虑\\sqrt{\\boldsymbol{\\Lambda}^{-1}}，直接用\\boldsymbol{U}来变换，结果如何呢？不难得出，如果只用\\boldsymbol{U}来变换，那么数据的协方差矩阵就变成了\\boldsymbol{\\Lambda}，它是个对角阵。

前面说了，\\boldsymbol{U}是一个正交矩阵，它相当于只是旋转了一下整体数据，不改变样本之间的相对位置，换句话说它是完全“保真”的变换。而\\boldsymbol{\\Lambda}的每个对角线元素，则衡量了它所在的那一维数据的变化幅度。如果它的值很小，说明这一维特征的变化很小，接近一个常数，那么就意味着原来句向量所在可能只是一个更低维的空间，我们就可以去掉这一维特征，在降维的同时还可以使得余弦相似度的结果更为合理。

事实上，SVD出来的对角矩阵\\boldsymbol{\\Lambda}已经从大到小排好序了，所以我们只需要保留前面若干维，就可以到达这个降维效果。熟悉线性代数的读者应该清楚，这个操作其实就是PCA！而代码只需要修改一行：

```python

```

效果如下：

\\begin{array}{l\|c}
\\hline
& \\,\\,\\text{STS-B}\\,\\, \\\
\\hline
\\text{BERT}\_{\\text{base}}\\text{-last2avg}\\,(\\text{论文结果}) & 59.04 \\\
\\text{BERT}\_{\\text{base}}\\text{-flow}\\,(\\text{target, 论文结果}) & 70.72 \\\
\\text{BERT}\_{\\text{base}}\\text{-last2avg}\\,(\\text{个人复现}) & 59.04 \\\
\\text{BERT}\_{\\text{base}}\\text{-whitening}\\,(\\text{target, 个人实现}) & 71.20 \\\
\\text{BERT}\_{\\text{base}}\\text{-whitening-256}\\,(\\text{target, 个人实现}) & 71.42 \\\
\\hline
\\text{BERT}\_{\\text{large}}\\text{-last2avg}\\,(\\text{论文结果}) & 59.56 \\\
\\text{BERT}\_{\\text{large}}\\text{-flow}\\,(\\text{target, 论文结果}) & 72.26 \\\
\\text{BERT}\_{\\text{large}}\\text{-last2avg}\\,(\\text{个人复现}) & 59.59 \\\
\\text{BERT}\_{\\text{large}}\\text{-whitening}\\,(\\text{target, 个人实现}) & 71.98 \\\
\\text{BERT}\_{\\text{large}}\\text{-whitening-384}\\,(\\text{target, 个人实现}) & 72.66 \\\
\\hline
\\end{array}

从上表可以看出，我们将base版本的768维只保留前256维，那么效果还有所提升，并且由于降维了，向量检索速度肯定也能大大加快；类似地，将large版的1024维只保留前384维，那么降维的同时也提升了效果。这个结果表明，无监督训练出来的句向量其实是“通用型”的，对于特定领域内的应用，里边有很多特征是冗余的，剔除这些冗余特征，往往能达到提速又提效的效果。

相比之下，flow模型是可逆的、不降维的，这在某些场景下是好处，但在不少场景下也是缺点，因为它无法剔除冗余维度，限制了性能，比如GAN的研究表明，通过一个256维的高斯向量就可以随机生成1024\\times 1024的人脸图，这表明这些人脸图其实只是构成了一个相当低维的流形，但是如果用flow模型来做，因为要保证可逆性，就得强行用1024\\times 1024\\times 3那么多维的高斯向量来随机生成，计算成本大大增加，而且效果还上不去。

（注：后续实验结果，请看 [《无监督语义相似度哪家强？我们做了个比较全面的评测》](https://kexue.fm/archives/8321)。）

## 所以最终结论就是 [\#](https://kexue.fm/archives/8069\#%E6%89%80%E4%BB%A5%E6%9C%80%E7%BB%88%E7%BB%93%E8%AE%BA%E5%B0%B1%E6%98%AF)

所以，目前的结果就是：笔者的若干实验表明，通过简单的线性变换（BERT-whitening）操作，效果基本上能媲美BERT-flow模型，这表明往句向量模型里边引入flow模型可能并非那么关键，它对分布的校正可能仅仅是浅层的，而通过线性变换直接校正句向量的协方差矩阵就能达到相近的效果。同时，BERT-whitening还支持降维操作，能达到提速又提效的效果。

_**转载到请包括本文地址：** [https://kexue.fm/archives/8069](https://kexue.fm/archives/8069 "你可能不需要BERT-flow：一个线性变换媲美BERT-flow")_

_**更详细的转载事宜请参考：**_ [《科学空间FAQ》](https://kexue.fm/archives/6508#%E6%96%87%E7%AB%A0%E5%A6%82%E4%BD%95%E8%BD%AC%E8%BD%BD/%E5%BC%95%E7%94%A8 "《科学空间FAQ》")

**如果您还有什么疑惑或建议，欢迎在下方评论区继续讨论。**

**如果您觉得本文还不错，欢迎 [分享](https://kexue.fm/archives/8069#share)/ [打赏](https://kexue.fm/archives/8069#pay) 本文。打赏并非要从中获得收益，而是希望知道科学空间获得了多少读者的真心关注。当然，如果你无视它，也不会影响你的阅读。再次表示欢迎和感谢！**

打赏

![科学空间](https://kexue.fm/usr/themes/geekg/payment/wx.png)

微信打赏

![科学空间](https://kexue.fm/usr/themes/geekg/payment/zfb.png)

支付宝打赏

因为网站后台对打赏并无记录，因此欢迎在打赏时候备注留言。

你还可以 [**点击这里**](http://mail.qq.com/cgi-bin/qm_share?t=qm_mailme&email=tN7d1drY3drrx8H0xcWa19vZ) 或在下方评论区留言来告知你的建议或需求。

**如果您需要引用本文，请参考：**

苏剑林. (Jan. 11, 2021). 《你可能不需要BERT-flow：一个线性变换媲美BERT-flow 》\[Blog post\]. Retrieved from [https://kexue.fm/archives/8069](https://kexue.fm/archives/8069)

@online{kexuefm-8069,

         title={你可能不需要BERT-flow：一个线性变换媲美BERT-flow},

         author={苏剑林},

         year={2021},

         month={Jan},

         url={\\url{https://kexue.fm/archives/8069}},

}


分类： [数学研究](https://kexue.fm/category/Mathematics)    标签： [语言模型](https://kexue.fm/tag/%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/), [语义](https://kexue.fm/tag/%E8%AF%AD%E4%B9%89/), [flow](https://kexue.fm/tag/flow/), [语义相似度](https://kexue.fm/tag/%E8%AF%AD%E4%B9%89%E7%9B%B8%E4%BC%BC%E5%BA%A6/)[140 评论](https://kexue.fm/archives/8069#comments)

< [【搜出来的文本】⋅（一）从文本生成到搜索采样](https://kexue.fm/archives/8062 "【搜出来的文本】⋅（一）从文本生成到搜索采样") \| [【搜出来的文本】⋅（二）从MCMC到模拟退火](https://kexue.fm/archives/8084 "【搜出来的文本】⋅（二）从MCMC到模拟退火") >

### 你也许还对下面的内容感兴趣

- [Transformer升级之路：21、MLA好在哪里?（下）](https://kexue.fm/archives/11111 "Transformer升级之路：21、MLA好在哪里?（下）")
- [Transformer升级之路：20、MLA好在哪里?（上）](https://kexue.fm/archives/10907 "Transformer升级之路：20、MLA好在哪里?（上）")
- [Transformer升级之路：19、第二类旋转位置编码](https://kexue.fm/archives/10862 "Transformer升级之路：19、第二类旋转位置编码")
- [细水长flow之TARFLOW：流模型满血归来？](https://kexue.fm/archives/10667 "细水长flow之TARFLOW：流模型满血归来？")
- [Decoder-only的LLM为什么需要位置编码？](https://kexue.fm/archives/10347 "Decoder-only的LLM为什么需要位置编码？")
- [Monarch矩阵：计算高效的稀疏型矩阵分解](https://kexue.fm/archives/10249 "Monarch矩阵：计算高效的稀疏型矩阵分解")
- [缓存与效果的极限拉扯：从MHA、MQA、GQA到MLA](https://kexue.fm/archives/10091 "缓存与效果的极限拉扯：从MHA、MQA、GQA到MLA")
- [时空之章：将Attention视为平方复杂度的RNN](https://kexue.fm/archives/10017 "时空之章：将Attention视为平方复杂度的RNN")
- [我在Performer中发现了Transformer-VQ的踪迹](https://kexue.fm/archives/9862 "我在Performer中发现了Transformer-VQ的踪迹")
- [预训练一下，Transformer的长序列成绩还能涨不少！](https://kexue.fm/archives/9787 "预训练一下，Transformer的长序列成绩还能涨不少！")

[发表你的看法](https://kexue.fm/archives/8069#comment_form)

1. [«](https://kexue.fm/archives/8069/comment-page-6#comments)
2. [1](https://kexue.fm/archives/8069/comment-page-1#comments)
3. ...
4. [4](https://kexue.fm/archives/8069/comment-page-4#comments)
5. [5](https://kexue.fm/archives/8069/comment-page-5#comments)
6. [6](https://kexue.fm/archives/8069/comment-page-6#comments)
7. [7](https://kexue.fm/archives/8069/comment-page-7#comments)

Jinjin

November 15th, 2023

苏神，请问这个白化是不是对于单个单词的相似度比较来说不太适用呀，有什么办法可以提高bert在单个单词比较相似度的准确率嘛

[回复评论](https://kexue.fm/archives/8069/comment-page-7?replyTo=23079#respond-post-8069)

[苏剑林](https://kexue.fm/) 发表于
November 20th, 2023

没测过word embedding的？你是怎么得到的word embedding？word embedding一般各向同性都比较好了，刻意whitening可能会有负作用。

[回复评论](https://kexue.fm/archives/8069/comment-page-7?replyTo=23114#respond-post-8069)

不知黑夜是晴天

February 2nd, 2024

您好，我用codebert-base模型在bigclonebench(比较两段代码是否相似)数据集上做了个降维维数实验，维数从2维到768维，指标采用AUC。发现AUC指标随着维数的增大先升后降，降维到10维的时候，AUC最大。这是否说明codebert-base得到的cls特征只有10维是有效的，其他大部分都是冗余的呢？

[回复评论](https://kexue.fm/archives/8069/comment-page-7?replyTo=23660#respond-post-8069)

[苏剑林](https://kexue.fm/) 发表于
February 6th, 2024

没毛病呀，可以这样理解（对于你给定的指标和余弦相似度来说）

当然，whitening并不总是有提升的，你可以进一步尝试 [https://kexue.fm/archives/9079](https://kexue.fm/archives/9079)

[回复评论](https://kexue.fm/archives/8069/comment-page-7?replyTo=23678#respond-post-8069)

xychen\_cxy

January 8th, 2025

苏神你好，我们在复现实验的时候，尝试通过计算白话前后bert向量的cosine相似度，发现数据量增加白话后的相似度反而远远低于白话前相似度，请问这样的现象是正常的嘛？

具体的，我们从ATEC数据集中分别取前100和1000条数据

n=100，白话前cosine\_similarity=0.8096， 白话后cosine\_similarity=0.9729

n=1000，白话前cosine\_similarity=0.8136，白话后cosine\_similarity=0.0033

[回复评论](https://kexue.fm/archives/8069/comment-page-7?replyTo=26206#respond-post-8069)

[苏剑林](https://kexue.fm/) 发表于
January 13th, 2025

这个cosine\_similarity是怎么算的？直接相加取平均？会不会正负样本采样不均匀？

[回复评论](https://kexue.fm/archives/8069/comment-page-7?replyTo=26248#respond-post-8069)

1. [«](https://kexue.fm/archives/8069/comment-page-6#comments)
2. [1](https://kexue.fm/archives/8069/comment-page-1#comments)
3. ...
4. [4](https://kexue.fm/archives/8069/comment-page-4#comments)
5. [5](https://kexue.fm/archives/8069/comment-page-5#comments)
6. [6](https://kexue.fm/archives/8069/comment-page-6#comments)
7. [7](https://kexue.fm/archives/8069/comment-page-7#comments)

[取消回复](https://kexue.fm/archives/8069#respond-post-8069)

你的大名

电子邮箱

个人网站（选填）

1\. 可以使用LaTeX代码，点击“预览效果”可查看效果；

2\. 可以通过点击评论楼层编号来引用该楼层；

3\. 网站可能会有点卡，如非确认评论失败，请 **不要重复点击提交**。

### 内容速览

[余弦相似度的假设](https://kexue.fm/archives/8069#%E4%BD%99%E5%BC%A6%E7%9B%B8%E4%BC%BC%E5%BA%A6%E7%9A%84%E5%81%87%E8%AE%BE)
[flow模型的碎碎念](https://kexue.fm/archives/8069#flow%E6%A8%A1%E5%9E%8B%E7%9A%84%E7%A2%8E%E7%A2%8E%E5%BF%B5)
[标准化协方差矩阵](https://kexue.fm/archives/8069#%E6%A0%87%E5%87%86%E5%8C%96%E5%8D%8F%E6%96%B9%E5%B7%AE%E7%9F%A9%E9%98%B5)
[相比于BERT-flow](https://kexue.fm/archives/8069#%E7%9B%B8%E6%AF%94%E4%BA%8EBERT-flow)
[降维效果还能更好](https://kexue.fm/archives/8069#%E9%99%8D%E7%BB%B4%E6%95%88%E6%9E%9C%E8%BF%98%E8%83%BD%E6%9B%B4%E5%A5%BD)
[所以最终结论就是](https://kexue.fm/archives/8069#%E6%89%80%E4%BB%A5%E6%9C%80%E7%BB%88%E7%BB%93%E8%AE%BA%E5%B0%B1%E6%98%AF)

### 智能搜索

支持整句搜索！网站自动使用 [结巴分词](https://github.com/fxsjy/jieba) 进行分词，并结合ngrams排序算法给出合理的搜索结果。

### 热门标签

[生成模型](https://kexue.fm/tag/%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B/) [attention](https://kexue.fm/tag/attention/) [优化](https://kexue.fm/tag/%E4%BC%98%E5%8C%96/) [语言模型](https://kexue.fm/tag/%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/) [模型](https://kexue.fm/tag/%E6%A8%A1%E5%9E%8B/) [梯度](https://kexue.fm/tag/%E6%A2%AF%E5%BA%A6/) [网站](https://kexue.fm/tag/%E7%BD%91%E7%AB%99/) [概率](https://kexue.fm/tag/%E6%A6%82%E7%8E%87/) [矩阵](https://kexue.fm/tag/%E7%9F%A9%E9%98%B5/) [优化器](https://kexue.fm/tag/%E4%BC%98%E5%8C%96%E5%99%A8/) [转载](https://kexue.fm/tag/%E8%BD%AC%E8%BD%BD/) [微分方程](https://kexue.fm/tag/%E5%BE%AE%E5%88%86%E6%96%B9%E7%A8%8B/) [分析](https://kexue.fm/tag/%E5%88%86%E6%9E%90/) [天象](https://kexue.fm/tag/%E5%A4%A9%E8%B1%A1/) [深度学习](https://kexue.fm/tag/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/) [积分](https://kexue.fm/tag/%E7%A7%AF%E5%88%86/) [python](https://kexue.fm/tag/python/) [扩散](https://kexue.fm/tag/%E6%89%A9%E6%95%A3/) [力学](https://kexue.fm/tag/%E5%8A%9B%E5%AD%A6/) [无监督](https://kexue.fm/tag/%E6%97%A0%E7%9B%91%E7%9D%A3/) [几何](https://kexue.fm/tag/%E5%87%A0%E4%BD%95/) [节日](https://kexue.fm/tag/%E8%8A%82%E6%97%A5/) [生活](https://kexue.fm/tag/%E7%94%9F%E6%B4%BB/) [文本生成](https://kexue.fm/tag/%E6%96%87%E6%9C%AC%E7%94%9F%E6%88%90/) [数论](https://kexue.fm/tag/%E6%95%B0%E8%AE%BA/)

### 随机文章

- [SVD分解(二)：为什么SVD意味着聚类？](https://kexue.fm/archives/4216)
- [纠缠的时空（一）：洛仑兹变换的矩阵](https://kexue.fm/archives/1889)
- [《虚拟的实在(2)》——为什么引力如此复杂？](https://kexue.fm/archives/2004)
- [UniVAE：基于Transformer的单模型、多尺度的VAE模型](https://kexue.fm/archives/8475)
- [旋转的弹簧将如何伸长(2)？](https://kexue.fm/archives/826)
- [“对角+低秩”三角阵的高效求逆方法](https://kexue.fm/archives/11072)
- [【备忘】Python中断多重循环的几种思路](https://kexue.fm/archives/4159)
- [生成扩散模型漫谈（二）：DDPM = 自回归式VAE](https://kexue.fm/archives/9152)
- [【外微分浅谈】7\. 有力的计算](https://kexue.fm/archives/4076)
- [恒等式 det(exp(A)) = exp(Tr(A)) 赏析](https://kexue.fm/archives/6377)

### 最近评论

- [Bin](https://kexue.fm/archives/1990/comment-page-2#comment-29105): 今天偶然从某个论坛看到有人推荐您的博客，定睛一看竟然是华师同院的往届师兄！看到这篇2013年的...
- [Rapture D](https://kexue.fm/archives/11530/comment-page-1#comment-29104): 我有一个问题，为什么不考虑亥姆霍兹定理和斯托克斯公式。
- [mofheka](https://kexue.fm/archives/11390/comment-page-1#comment-29103): 苏神是还在用jax是么？最近在做基于Google Pathway的理念做一个动态版的MPMD框...
- [长琴](https://kexue.fm/archives/11530/comment-page-1#comment-29102): 看懂这篇博客也不是一件容易的事情。
- [AlexLi](https://kexue.fm/archives/9257/comment-page-4#comment-29101): 苏老师，请教一下(7)式中将 \\mu(x\_t) 传给 p\_o 进行推理的操作。 $x\_...
- [tyler\_zxc](https://kexue.fm/archives/7921/comment-page-2#comment-29100): "Performer的思想是将标准的Attention线性化，所以为什么不干脆直接训练一个线性...
- [我](https://kexue.fm/archives/11494/comment-page-1#comment-29099): 似乎并非mHC提出矩阵的思想？之前hyper connection就是了
- [winter](https://kexue.fm/archives/10847/comment-page-1#comment-29098): 苏神您好，假如对于比较均匀的attention weightP，往往呈现long tail分布...
- [苏剑林](https://kexue.fm/archives/8512/comment-page-2#comment-29097): KL散度、JS散度、W距离啥的，都行啊，看你喜欢哪个
- [苏剑林](https://kexue.fm/archives/9119/comment-page-14#comment-29096): 没有绝对公平的对比方法，主要看你关心什么。比如，如果只关心推理成本和推理效果，那么有的方法可以...

### 友情链接

- [Cool Papers](https://papers.cool/)
- [数学研发](https://bbs.emath.ac.cn/)
- [Seatop](http://www.seatop.com.cn/)
- [Xiaoxia](https://xiaoxia.org/)
- [积分表-网络版](https://kexue.fm/sci/integral/index.html)
- [丝路博傲](http://blog.dvxj.com/)
- [数学之家](http://www.2math.cn/)
- [有趣天文奇观](http://interesting-sky.china-vo.org/)
- [TwistedW](http://www.twistedwg.com/)
- [godweiyang](https://godweiyang.com/)
- [AI柠檬](https://blog.ailemon.net/)
- [王登科-DK博客](https://greatdk.com/)
- [ESON](https://blog.eson.org/)
- [枫之羽](https://fzhiy.net/)
- [coding-zuo](https://coding-zuo.github.io/)
- [博科园](https://www.bokeyuan.net/)
- [孔皮皮的博客](https://www.kppkkp.top/)
- [运鹏的博客](https://yunpengtai.top/)
- [jiming.site](https://jiming.site/)
- [OmegaXYZ](https://www.omegaxyz.com/)
- [EAI猩球](https://www.robotech.ink/)
- [文举的博客](https://liwenju0.com/)
- [申请链接](https://kexue.fm/links.html)

[![署名-非商业用途-保持一致](https://kexue.fm/usr/themes/geekg/images/cc.gif)](http://creativecommons.org/licenses/by-nc-nd/2.5/cn/) 本站采用创作共用版权协议，要求署名、非商业用途和保持一致。转载本站内容必须也遵循“ [署名-非商业用途-保持一致](http://creativecommons.org/licenses/by-nc-nd/2.5/cn/)”的创作共用协议。



© 2009-2026 Scientific Spaces. All rights reserved. Theme by [laogui](http://www.laogui.com/). Powered by [Typecho](http://typecho.org/). 备案号: [粤ICP备09093259号-1/2](https://beian.miit.gov.cn/ "粤ICP备09093259号")。