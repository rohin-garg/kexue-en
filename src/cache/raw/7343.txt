Processing math: 0%

![MobileSideBar](https://kexue.fm/usr/themes/geekg/images/slide-button.png)

## SEARCH

## MENU

- [打赏](https://kexue.fm/reward.html)
- [公式](https://kexue.fm/latex.html)
- [天象](https://kexue.fm/ac.html)
- [链接](https://kexue.fm/links.html)
- [时光](https://kexue.fm/me.html)
- [博览](https://kexue.fm/science.html)
- [归档](https://kexue.fm/content.html)

## CATEGORIES

- [千奇百怪](https://kexue.fm/category/Everything)
- [天文探索](https://kexue.fm/category/Astronomy)
- [数学研究](https://kexue.fm/category/Mathematics)
- [物理化学](https://kexue.fm/category/Phy-chem)
- [信息时代](https://kexue.fm/category/Big-Data)
- [生物自然](https://kexue.fm/category/Biology)
- [图片摄影](https://kexue.fm/category/Photograph)
- [问题百科](https://kexue.fm/category/Questions)
- [生活/情感](https://kexue.fm/category/Life-Feeling)
- [资源共享](https://kexue.fm/category/Resources)

## NEWPOSTS

- [让炼丹更科学一些（五）：基于梯度精...](https://kexue.fm/archives/11530)
- [让炼丹更科学一些（四）：新恒等式，...](https://kexue.fm/archives/11494)
- [为什么DeltaNet要加L2 N...](https://kexue.fm/archives/11486)
- [让炼丹更科学一些（三）：SGD的终...](https://kexue.fm/archives/11480)
- [让炼丹更科学一些（二）：将结论推广...](https://kexue.fm/archives/11469)
- [滑动平均视角下的权重衰减和学习率](https://kexue.fm/archives/11459)
- [生成扩散模型漫谈（三十一）：预测数...](https://kexue.fm/archives/11428)
- [Muon优化器指南：快速上手与关键细节](https://kexue.fm/archives/11416)
- [AdamW的Weight RMS的...](https://kexue.fm/archives/11404)
- [n个正态随机数的最大值的渐近估计](https://kexue.fm/archives/11390)

## COMMENTS

- [Bin: 今天偶然从某个论坛看到有人推荐您的博客，定睛一看竟然是华师同院...](https://kexue.fm/archives/1990/comment-page-2#comment-29105)
- [Rapture D: 我有一个问题，为什么不考虑亥姆霍兹定理和斯托克斯公式。](https://kexue.fm/archives/11530/comment-page-1#comment-29104)
- [mofheka: 苏神是还在用jax是么？最近在做基于Google Pathwa...](https://kexue.fm/archives/11390/comment-page-1#comment-29103)
- [长琴: 看懂这篇博客也不是一件容易的事情。](https://kexue.fm/archives/11530/comment-page-1#comment-29102)
- [AlexLi: 苏老师，请教一下(7)式中将 \\mu(x\_t) 传给 $p...](https://kexue.fm/archives/9257/comment-page-4#comment-29101)
- [tyler\_zxc: "Performer的思想是将标准的Attention线性化，...](https://kexue.fm/archives/7921/comment-page-2#comment-29100)
- [我: 似乎并非mHC提出矩阵的思想？之前hyper connecti...](https://kexue.fm/archives/11494/comment-page-1#comment-29099)
- [winter: 苏神您好，假如对于比较均匀的attention weightP...](https://kexue.fm/archives/10847/comment-page-1#comment-29098)
- [苏剑林: KL散度、JS散度、W距离啥的，都行啊，看你喜欢哪个](https://kexue.fm/archives/8512/comment-page-2#comment-29097)
- [苏剑林: 没有绝对公平的对比方法，主要看你关心什么。比如，如果只关心推理...](https://kexue.fm/archives/9119/comment-page-14#comment-29096)

## USERLOGIN

- [登录](https://kexue.fm/admin/login.php)

[科学空间\|Scientific Spaces](https://kexue.fm/)

- [登录](https://kexue.fm/admin/login.php)
- [打赏](https://kexue.fm/reward.html)
- [公式](https://kexue.fm/latex.html)
- [天象](https://kexue.fm/ac.html)
- [链接](https://kexue.fm/links.html)
- [时光](https://kexue.fm/me.html)
- [博览](https://kexue.fm/science.html)
- [归档](https://kexue.fm/content.html)

渴望成为一个小飞侠

- [![](https://kexue.fm/usr/themes/geekg/images/rss.png)\\
\\
欢迎订阅](https://kexue.fm/feed)
- [![](https://kexue.fm/usr/themes/geekg/images/mail.png)\\
\\
个性邮箱](https://kexue.fm/archives/119)
- [![](https://kexue.fm/usr/themes/geekg/images/Saturn.png)\\
\\
天象信息](https://kexue.fm/ac.html)
- [![](https://kexue.fm/usr/themes/geekg/images/iss.png)\\
\\
观测ISS](https://kexue.fm/archives/41)
- [![](https://kexue.fm/usr/themes/geekg/images/pi.png)\\
\\
LaTeX](https://kexue.fm/latex.html)
- [![](https://kexue.fm/usr/themes/geekg/images/mlogo.png)\\
\\
关于博主](https://kexue.fm/me.html)

欢迎访问“科学空间”，这里将与您共同探讨自然科学，回味人生百态；也期待大家的分享～

- [**千奇百怪** Everything](https://kexue.fm/category/Everything)
- [**天文探索** Astronomy](https://kexue.fm/category/Astronomy)
- [**数学研究** Mathematics](https://kexue.fm/category/Mathematics)
- [**物理化学** Phy-chem](https://kexue.fm/category/Phy-chem)
- [**信息时代** Big-Data](https://kexue.fm/category/Big-Data)
- [**生物自然** Biology](https://kexue.fm/category/Biology)
- [**图片摄影** Photograph](https://kexue.fm/category/Photograph)
- [**问题百科** Questions](https://kexue.fm/category/Questions)
- [**生活/情感** Life-Feeling](https://kexue.fm/category/Life-Feeling)
- [**资源共享** Resources](https://kexue.fm/category/Resources)

- [**千奇百怪**](https://kexue.fm/category/Everything)
- [**天文探索**](https://kexue.fm/category/Astronomy)
- [**数学研究**](https://kexue.fm/category/Mathematics)
- [**物理化学**](https://kexue.fm/category/Phy-chem)
- [**信息时代**](https://kexue.fm/category/Big-Data)
- [**生物自然**](https://kexue.fm/category/Biology)
- [**图片摄影**](https://kexue.fm/category/Photograph)
- [**问题百科**](https://kexue.fm/category/Questions)
- [**生活/情感**](https://kexue.fm/category/Life-Feeling)
- [**资源共享**](https://kexue.fm/category/Resources)

[首页](https://kexue.fm/) [信息时代](https://kexue.fm/category/Big-Data) EAE：自编码器 + BN + 最大熵 = 生成模型

20Apr

# [EAE：自编码器 + BN + 最大熵 = 生成模型](https://kexue.fm/archives/7343)

By 苏剑林 \|
2020-04-20 \|
81650位读者 \|

生成模型一直是笔者比较关注的主题，不管是NLP和CV的生成模型都是如此。这篇文章里，我们介绍一个新颖的生成模型，来自论文 [《Batch norm with entropic regularization turns deterministic autoencoders into generative models》](https://papers.cool/arxiv/2002.10631)，论文中称之为 **EAE**（Entropic AutoEncoder）。它要做的事情给变分自编码器（VAE）基本一致，最终效果其实也差不多（略优），说它新颖并不是它生成效果有多好，而是思路上的新奇，颇有别致感。此外，借着这个机会，我们还将学习一种统计量的估计方法——k邻近方法，这是一种很有用的非参数估计方法。

## 自编码器vs生成模型 [\#](https://kexue.fm/archives/7343\#%E8%87%AA%E7%BC%96%E7%A0%81%E5%99%A8vs%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B)

普通的自编码器是一个“编码-解码”的重构过程，如下图所示：

[![典型自编码器示意图](https://kexue.fm/usr/uploads/2020/04/2128638498.png)](https://kexue.fm/usr/uploads/2020/04/2128638498.png "点击查看原图")

典型自编码器示意图

其loss一般为

\\begin{equation}L\_{AE} = \\mathbb{E}\_{x\\sim \\tilde{p}(x)}\\left\[\\left\\Vert x - \\hat{x}\\right\\Vert^2\\right\] = \\mathbb{E}\_{x\\sim \\tilde{p}(x)}\\left\[\\left\\Vert x - D(E(x))\\right\\Vert^2\\right\]\\end{equation}

当训练完成后，我们自然可以针对每一幅图像x，得到它的编码z=E(x)以及重构图\\hat{x}=D(z)，而当x与\\hat{x}足够接近时，我们就可以认为z是x的有效表征，它已经充分包含了x的信息。

那么，生成模型又是什么情况呢？“生成”指的是随机生成，也就是说允许我们能随机构建一幅图像来，对于自编码器的解码器D(z)，并不是每一个z解码出来的D(z)都是一幅有意义的图像，因此普通的自编码器并不能视为生成模型。如果我们能够事先知道所有的x编码出来的z=E(x)所构成的分布，并且这个分布是一个易于采样分布，那么就可以实现随机采样生成了。

所以，从自编码器到生成模型，缺的那一步就是确定隐变量z的分布，更准确来说，是迫使隐变量z服从一个易于采样的简单分布，比如标准正态分布。VAE通过引入KL散度项来达到这一点，那么EAE又是怎么实现的呢？

## 正态分布与最大熵 [\#](https://kexue.fm/archives/7343\#%E6%AD%A3%E6%80%81%E5%88%86%E5%B8%83%E4%B8%8E%E6%9C%80%E5%A4%A7%E7%86%B5)

我们知道，最大熵原理是一个相当普适的原理，它代表着我们对未知事件的最客观认知。最大熵原理的一个结论是：

> 在所有均值为0、方差为1的分布中，标准正态分布的熵最大。

如果读者还不了解最大熵的相关内容，可以参考旧作 [《“熵”不起：从熵、最大熵原理到最大熵模型（二）》](https://kexue.fm/archives/3552)。上述结论告诉我们，如果我们能有某种手段保证隐变量的均值为0和方差为1，那么我们只需要同时最大化隐变量的熵，就可以得到“隐变量服从标准正态分布”这个目的了，即

\\begin{equation}\\begin{aligned}&L\_{EAE} = \\mathbb{E}\_{x\\sim \\tilde{p}(x)}\\left\[\\left\\Vert x - D(E(x))\\right\\Vert^2\
\\right\] - \\lambda H(Z)\\\
&\\text{s.t.}\\,\\,\\text{avg}(E(x))=0,\\,\\text{std}(E(x))=1
\\end{aligned}\\end{equation}

其中\\lambda > 0是超参数，而

\\begin{equation}H(Z)=\\mathbb{E}\_{z\\sim p(z)}\[-\\log p(z)\]\\end{equation}

是隐变量z=E(x)对应的熵，最小化\- \\lambda H(Z)意味着最大化\\lambda H(Z)，即最大熵。

问题是如何保证这两个约束呢？如果计算隐变量的熵呢？

## 均值方差约束与BN [\#](https://kexue.fm/archives/7343\#%E5%9D%87%E5%80%BC%E6%96%B9%E5%B7%AE%E7%BA%A6%E6%9D%9F%E4%B8%8EBN)

先来解决第一个问题：如何达到——至少近似地达到——“隐变量的均值为0、方差为1”这个约束？因为只有满足这个约束的前提下，最大熵的分布才是标准正态的。解决这个问题的办法是我们熟悉的批归一化，也就是BN（Batch Normalization）。

在BN的训练阶段，我们会直接对每个变量减去其batch内的均值并且除以batch内标准差，这保证了训练阶段每个batch的变量均值确实为0，方差确实为1；然后，它会将每个batch内的均值方差滑动平均并缓存下来，用于推断阶段的预测。总而言之，就是将BN应用于隐变量，就可以使得隐变量（近似地）满足相应的均值方差约束。对了，要说明的是，本文说的BN层，不包括\\beta,\\gamma这两个可训练参数，如果在Keras中，则是要在初始化BN层时传入参数`scale=False, center=False`。

此时，我们就得到

\\begin{equation}L\_{EAE} = \\mathbb{E}\_{x\\sim \\tilde{p}(x)}\\left\[\\left\\Vert x - D(\\mathcal{N}(E(x)))\\right\\Vert^2\\right\] - \\lambda H(Z)\\label{eq:eae}\\end{equation}

这里的\\mathcal{N}(\\cdot)代表BN层。

## 熵的采样邻近估计 [\#](https://kexue.fm/archives/7343\#%E7%86%B5%E7%9A%84%E9%87%87%E6%A0%B7%E9%82%BB%E8%BF%91%E4%BC%B0%E8%AE%A1)

现在，来到了整个EAE模型的最后一部分、同时也是最硬核的一部分了，也就是如何估计熵H(Z)。理论上来说，为了算H(Z)我们需要知道p(z)，但我们现在只有样本z\_1, z\_2, \\dots, z\_n而不知道p(z)的表达式，在这种前提下对H(Z)做的估计叫做非参数估计。

先给结论：

> **熵的最邻近估计** 设z\_1,z\_2,\\dots,z\_n\\in\\mathbb{R}^d是从p(z)采样出来的n个样本，记\\varepsilon(i)为z\_i到它最邻近的样本的距离，即\\varepsilon(i) = \\min\\limits\_{j\\neq i} \\Vert z\_i - z\_j \\Vert，B\_d是d维单位球的体积，\\gamma=0.5772\\dots是 [欧拉常数](https://en.wikipedia.org/wiki/Euler%E2%80%93Mascheroni_constant)，则
> \\begin{equation}H(Z)\\approx \\frac{d}{n}\\sum\_{i=1}^n \\log \\varepsilon(i) + \\log B\_d + \\log (n - 1) + \\gamma \\label{eq:1nn}\\end{equation}

抛开跟优化不相关的常数，上述结论实际上就是说H(Z)\\sim \\sum\\limits\_{i=1}^n \\log \\varepsilon(i)，这就是我们需要添加到loss的项。

> 这个看上去很奇怪、实际上确实也不容易理解的结果是怎么得来的呢？事实上，它是一种重要的估计方法——k邻近方法——的经典例子。下面将会给出它的推导过程，该过程参考自论文 [《A non-parametric k-nearest neighbour entropy estimator》](https://papers.cool/arxiv/1506.06501)。
>
> 让我们考虑特定的样本z\_i，设z\_{i(k)}是它的第k个最邻近的样本，即将所有的z\_j (j\\neq i)按照\\Vert z\_j - z\_i\\Vert从小到大排列，第k个就是z\_{i(k)}，记\\varepsilon\_k(i) = \\left\\Vert z\_i - z\_{i(k)}\\right\\Vert，我们现在考虑\\varepsilon\_k(i)的概率分布。
>
> 假设\\varepsilon \\leq \\varepsilon\_k(i) \\leq \\varepsilon + d\\varepsilon，那么就意味着剩下的n-1个样本之中，有k-1个落在了“以z\_i为球心、以\\varepsilon为半径”的球内，有n-k-1个落在了“以z\_i为球心、以\\varepsilon+d\\varepsilon为半径”的球外，剩下一个夹在两球之间，不难得到这种情况发生的概率是
>
> \\begin{equation}\\binom{n-1}{1}\\binom{n-2}{k-1}P\_i(\\varepsilon)^{k-1}(1 - P\_i(\\varepsilon + d\\varepsilon))^{n-k-1}(P\_i(\\varepsilon + d\\varepsilon) - P\_i(\\varepsilon))\\label{eq:dp-1}\\end{equation}
>
> 其中\\binom{n-1}{1}代表着从n-1个样本中挑出1个样本夹在两球之间的组合数，而\\binom{n-2}{k-1}则是从剩下的n-2个样本中挑出k-1个样本放到球内的组合数（剩下的n-k-1个自动就在球外了）；P\_i(\\varepsilon)是单个样本位于球内的概率，即
>
> \\begin{equation}P\_i(\\varepsilon) = \\int\_{\\Vert z - z\_i\\Vert \\leq \\varepsilon} p(z)dz\\end{equation}
>
> 所以P\_i(\\varepsilon)^{k-1}是挑出来的k-1个样本都在球内的概率，(1 - P\_i(\\varepsilon + d\\varepsilon))^{n-k-1}是n-k-1个样本都在球外的概率，P\_i(\\varepsilon + d\\varepsilon) - P\_i(\\varepsilon)则是一个样本在球间的概率，所有项乘起来就是式\\eqref{eq:dp-1}，而展开并只保留一阶项得到近似式：
>
> \\begin{equation}\\binom{n-1}{1}\\binom{n-2}{k-1}P\_i(\\varepsilon)^{k-1}(1 - P\_i(\\varepsilon))^{n-k-1}dP\_i(\\varepsilon)\\label{eq:dp-2}\\end{equation}
>
> 注意上式描述了一个合理的概率分布，因此它的积分必然为1。
>
> 现在我们可以做个 **近似假设**，值得注意的是，这是整个推导过程的唯一假设，而最终结果的可靠程度也取决于这个假设的成立程度：
>
> \\begin{equation}p(z\_i) \\approx \\frac{1}{B\_d \\varepsilon\_k(i)^d}\\int\_{\\Vert z - z\_i\\Vert \\leq \\varepsilon} p(z)dz = \\frac{P\_i(\\varepsilon)}{B\_d \\varepsilon\_k(i)^d}\\end{equation}
>
> 其中B\_d \\varepsilon\_k(i)^d就是半径为\\varepsilon\_k(i)的球的体积。根据这个近似我们有p(z\_i) B\_d \\varepsilon\_k(i)^d \\approx P\_i(\\varepsilon)，看上去不合理，因为左端相当于一个常数，右端则是关于\\varepsilon的函数，两者怎么能一直保持近似？事实上，当n足够大的时候，采样出来的样本足够稠密，这时候\\varepsilon会集中在一个比较小的范围内，而\\varepsilon\_k(i)是\\varepsilon某次采样的值，所以我们可以认为\\varepsilon会集中在\\varepsilon\_k(i)附近。虽然P\_i(\\varepsilon)关于\\varepsilon是变化的，但下面我们还要对\\varepsilon进行积分，所以我们只需要在\\varepsilon\_k(i)附近对P\_i(\\varepsilon)做好近似进行，不需要整体都可以很好地近似（即\\varepsilon \\gg \\varepsilon\_k(i)时，\\eqref{eq:dp-2}几乎为0）。而在\\varepsilon\_k(i)附近我们可以认为概率变化是平缓的，所以p(z\_i) B\_d \\varepsilon\_k(i)^d \\approx P\_i(\\varepsilon)。
>
> 现在我们可以写出
>
> \\begin{equation}\\log p(z\_i) \\approx \\log P\_i(\\varepsilon) - \\log B\_d - d \\log \\varepsilon\_k(i) \\end{equation}
>
> 用\\eqref{eq:dp-2}乘以上式两端，并对\\varepsilon积分（积分区间为\[0,+\\infty)，或者等价于对P\_i在\[0,1\]积分）。除\\log P\_i(\\varepsilon)外，其余几项都是跟\\varepsilon无关，所以积分后依然等于自身，而\
>\
> \\begin{equation}\\begin{aligned}&\\int\_0^1 \\binom{n-1}{1}\\binom{n-2}{k-1}P\_i(\\varepsilon)^{k-1}(1 - P\_i(\\varepsilon))^{n-k-1} \\log P\_i(\\varepsilon) d P\_i(\\varepsilon) \\\\
> =&\\psi(k)-\\psi(n)\\end{aligned}\\end{equation}\
>\
> 其中\\psi代表着 [双伽马函数](https://en.wikipedia.org/wiki/Digamma_function)。（别问我这些积分是怎么算出来的，我也不知道，但我知道用Mathematica软件能把它都算出来～）\
>\
> 于是我们得到近似\
>\
> \\begin{equation}\\log p(z\_i) \\approx \\psi(k)-\\psi(n) - \\log B\_d - d \\log \\varepsilon\_k(i) \\end{equation}\
>\
> 所以最终熵的近似为：\
>\
> \\begin{equation}\\begin{aligned}H(Z)=&\\, \\mathbb{E}\_{z\\sim p(z)}\[-\\log p(z)\]\\\\
> \\approx&\\, -\\frac{1}{n}\\sum\_{i=1}^n \\log p(z\_i)\\\\
> \\approx&\\, \\frac{d}{n}\\sum\_{i=1}^n \\log \\varepsilon\_k(i) + \\log B\_d + \\psi(n)-\\psi(k)\
> \\end{aligned}\\label{eq:knn}\\end{equation}\
>\
> 这是比式\\eqref{eq:1nn}更一般的结果。事实上式\\eqref{eq:1nn}是上式k=1时的特例，因为\\psi(1)=-\\gamma，而\\psi(n)= \\sum\\limits\_{m=1}^{n-1}\\frac{1}{m}-\\gamma\\approx \\log(n-1)，这些变换公式都可以在维基百科上找到。\
>\
> 开头就已经提到过，k邻近方法是一种很有用的非参数估计方法，它还跟笔者之前介绍过的 [IMLE模型](https://kexue.fm/archives/6394) 有关。但笔者本身也不熟悉k邻近方法，还需要多多学习，目前找到的资料是 [《Lectures on the Nearest Neighbor Method》](https://link.springer.com/book/10.1007/978-3-319-25388-6)。此外，关于熵的估计，还可以参考斯坦福的资料 [《Theory and Practice of Differential Entropy Estimation》](https://web.stanford.edu/~yjhan/diff_entropy.pdf)。\
\
## 进一步思考与分析 [\#](https://kexue.fm/archives/7343\#%E8%BF%9B%E4%B8%80%E6%AD%A5%E6%80%9D%E8%80%83%E4%B8%8E%E5%88%86%E6%9E%90)\
\
有了\\eqref{eq:1nn}或\\eqref{eq:knn}，式\\eqref{eq:eae}所描述的EAE的loss就完成了，所以EAE模型也就介绍完毕了。剩下的是实验结果，就不详细介绍了，反正就是感觉生成的图像跟VAE差不多，但指标上更优一些。\
\
[![来自EAE论文的实验对比](https://kexue.fm/usr/uploads/2020/04/3907052608.jpg)](https://kexue.fm/usr/uploads/2020/04/3907052608.jpg "点击查看原图")\
\
来自EAE论文的实验对比\
\
[![来自EAE论文的效果图示](https://kexue.fm/usr/uploads/2020/04/2546475316.jpg)](https://kexue.fm/usr/uploads/2020/04/2546475316.jpg "点击查看原图")\
\
来自EAE论文的效果图示\
\
那EAE相比VAE的好处在哪呢？在VAE中，比较关键的一步是重参数（可以参考笔者的 [《变分自编码器（一）：原来是这么一回事》](https://kexue.fm/archives/5253)），就是这一步降低了模型训练的方差（相比REINFORCE方差更小，可以参考笔者的 [《漫谈重参数：从正态分布到Gumbel Softmax》](https://kexue.fm/archives/6705)），从而使得VAE可以有效地训练下去。然而，虽然重参数降低了方差，但事实上方差依然不小，简单来说就是重参数这一步带来较大的噪声（尤其是训练早期），导致decoder无法很好地利用encoder的信息，典型的例子就是将VAE用在NLP时的“KL散度消失”现象。\
\
但是EAE基本上不存在这个问题，因为EAE基本上就是普通的自编码器，多加的BN不会对自编码性能有什么影响，而多加的熵正则项原则上也只是增加隐变量的多样性，不会给编码信息的利用与重构带来明显困难。笔者认为，这就是EAE相对于VAE的优势所在。当然，笔者目前还没有对EAE进行太多实验，上述分析多为主观推断，请读者自行甄别。如果笔者有进一步的实验结论，到时会继续在博客与大家分享。\
\
## 最后补上一个小结 [\#](https://kexue.fm/archives/7343\#%E6%9C%80%E5%90%8E%E8%A1%A5%E4%B8%8A%E4%B8%80%E4%B8%AA%E5%B0%8F%E7%BB%93)\
\
本文介绍了一个称之为EAE的模型，主要是把BN层和最大熵塞进了普通的自编码器中，使得它具有生成模型的能力。原论文做的不少实验显示EAE比VAE效果更好，所以应该是一个值得学习和试用的模型。此外，EAE的关键部分是通过k邻近方法来估计熵，这部分比较硬核，但事实上也很有价值，值得对统计估计感兴趣的读者细细阅读。\
\
_**转载到请包括本文地址：** [https://kexue.fm/archives/7343](https://kexue.fm/archives/7343 "EAE：自编码器 + BN + 最大熵  = 生成模型")_\
\
_**更详细的转载事宜请参考：**_ [《科学空间FAQ》](https://kexue.fm/archives/6508#%E6%96%87%E7%AB%A0%E5%A6%82%E4%BD%95%E8%BD%AC%E8%BD%BD/%E5%BC%95%E7%94%A8 "《科学空间FAQ》")\
\
**如果您还有什么疑惑或建议，欢迎在下方评论区继续讨论。**\
\
**如果您觉得本文还不错，欢迎 [分享](https://kexue.fm/archives/7343#share)/ [打赏](https://kexue.fm/archives/7343#pay) 本文。打赏并非要从中获得收益，而是希望知道科学空间获得了多少读者的真心关注。当然，如果你无视它，也不会影响你的阅读。再次表示欢迎和感谢！**\
\
打赏\
\
![科学空间](https://kexue.fm/usr/themes/geekg/payment/wx.png)\
\
微信打赏\
\
![科学空间](https://kexue.fm/usr/themes/geekg/payment/zfb.png)\
\
支付宝打赏\
\
因为网站后台对打赏并无记录，因此欢迎在打赏时候备注留言。\
\
你还可以 [**点击这里**](http://mail.qq.com/cgi-bin/qm_share?t=qm_mailme&email=tN7d1drY3drrx8H0xcWa19vZ) 或在下方评论区留言来告知你的建议或需求。\
\
**如果您需要引用本文，请参考：**\
\
苏剑林. (Apr. 20, 2020). 《EAE：自编码器 + BN + 最大熵 = 生成模型 》\[Blog post\]. Retrieved from [https://kexue.fm/archives/7343](https://kexue.fm/archives/7343)\
\
@online{kexuefm-7343,\
\
         title={EAE：自编码器 + BN + 最大熵 = 生成模型},\
\
         author={苏剑林},\
\
         year={2020},\
\
         month={Apr},\
\
         url={\\url{https://kexue.fm/archives/7343}},\
\
}\
\
\
分类： [信息时代](https://kexue.fm/category/Big-Data)    标签： [最大熵](https://kexue.fm/tag/%E6%9C%80%E5%A4%A7%E7%86%B5/), [熵](https://kexue.fm/tag/%E7%86%B5/), [无监督](https://kexue.fm/tag/%E6%97%A0%E7%9B%91%E7%9D%A3/), [生成模型](https://kexue.fm/tag/%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B/)[21 评论](https://kexue.fm/archives/7343#comments)\
\
< [突破瓶颈，打造更强大的Transformer](https://kexue.fm/archives/7325 "突破瓶颈，打造更强大的Transformer") \| [将“Softmax+交叉熵”推广到多标签分类问题](https://kexue.fm/archives/7359 "将“Softmax+交叉熵”推广到多标签分类问题") >\
\
### 你也许还对下面的内容感兴趣\
\
- [生成扩散模型漫谈（三十一）：预测数据而非噪声](https://kexue.fm/archives/11428 "生成扩散模型漫谈（三十一）：预测数据而非噪声")\
- [DiVeQ：一种非常简洁的VQ训练方案](https://kexue.fm/archives/11328 "DiVeQ：一种非常简洁的VQ训练方案")\
- [为什么线性注意力要加Short Conv？](https://kexue.fm/archives/11320 "为什么线性注意力要加Short Conv？")\
- [Transformer升级之路：21、MLA好在哪里?（下）](https://kexue.fm/archives/11111 "Transformer升级之路：21、MLA好在哪里?（下）")\
- [线性注意力简史：从模仿、创新到反哺](https://kexue.fm/archives/11033 "线性注意力简史：从模仿、创新到反哺")\
- [生成扩散模型漫谈（三十）：从瞬时速度到平均速度](https://kexue.fm/archives/10958 "生成扩散模型漫谈（三十）：从瞬时速度到平均速度")\
- [Transformer升级之路：20、MLA好在哪里?（上）](https://kexue.fm/archives/10907 "Transformer升级之路：20、MLA好在哪里?（上）")\
- [矩阵的有效秩（Effective Rank）](https://kexue.fm/archives/10847 "矩阵的有效秩（Effective Rank）")\
- [生成扩散模型漫谈（二十九）：用DDPM来离散编码](https://kexue.fm/archives/10711 "生成扩散模型漫谈（二十九）：用DDPM来离散编码")\
- [细水长flow之TARFLOW：流模型满血归来？](https://kexue.fm/archives/10667 "细水长flow之TARFLOW：流模型满血归来？")\
\
[发表你的看法](https://kexue.fm/archives/7343#comment_form)\
\
李炜\
\
October 1st, 2021\
\
苏神，EAE的loss中加入的熵正则项在代码层面怎么实现啊？\
\
[回复评论](https://kexue.fm/archives/7343/comment-page-1?replyTo=17470#respond-post-7343)\
\
[苏剑林](https://kexue.fm/) 发表于\
October 4th, 2021\
\
就直接按照公式实现呀，哪部分有难度呢？\
\
[回复评论](https://kexue.fm/archives/7343/comment-page-1?replyTo=17482#respond-post-7343)\
\
a11en0\
\
November 19th, 2021\
\
直接使用KL散度不也可以实现“隐变量的均值为0、方差为1”这个约束?为什么还要用BN的方式？\
\
[回复评论](https://kexue.fm/archives/7343/comment-page-1?replyTo=17846#respond-post-7343)\
\
[苏剑林](https://kexue.fm/) 发表于\
November 19th, 2021\
\
如何实现？这里要求的是显式约束。\
\
[回复评论](https://kexue.fm/archives/7343/comment-page-1?replyTo=17852#respond-post-7343)\
\
a11en0 发表于\
November 21st, 2021\
\
我想的是像VAE那样直接用KL将后验约束到先验正态上面去，不过貌似这里是用另一种更复杂的手段实现这件事情？\
\
[回复评论](https://kexue.fm/archives/7343/comment-page-1?replyTo=17858#respond-post-7343)\
\
[苏剑林](https://kexue.fm/) 发表于\
November 22nd, 2021\
\
KL项那叫做惩罚项，不叫做约束，约束是指天然满足的，比如向量\\boldsymbol{u}=\\boldsymbol{x}/\\Vert \\boldsymbol{x}\\Vert天然满足\\Vert \\boldsymbol{u}\\Vert=1。\
\
[回复评论](https://kexue.fm/archives/7343/comment-page-1?replyTo=17865#respond-post-7343)\
\
a11en0 发表于\
November 23rd, 2021\
\
好的，感谢回答！不过不管它叫什么，KL起到的作用总是将“后验约束到正态”上面吧？\
\
[回复评论](https://kexue.fm/archives/7343/comment-page-1?replyTo=17872#respond-post-7343)\
\
[苏剑林](https://kexue.fm/) 发表于\
November 23rd, 2021\
\
1、这不是“叫什么”的问题，这是“对与错”、“是与否”的问题，EAE要做的是带约束优化，不是带惩罚优化，这两个不能互相替换；\
\
2、单独的KL没什么作用，它只是与重构项构成VAE的loss，没有重构项它什么也不是。\
\
[回复评论](https://kexue.fm/archives/7343/comment-page-1?replyTo=17875#respond-post-7343)\
\
greg\_yang\
\
November 7th, 2022\
\
苏老师好，想问一下，式子（5）里的d是什么的维度呢，和什么参数有关吗还是说随便定一个？\
\
[回复评论](https://kexue.fm/archives/7343/comment-page-1?replyTo=20306#respond-post-7343)\
\
[苏剑林](https://kexue.fm/) 发表于\
November 10th, 2022\
\
d就是你本身的数据维度啊，已经说得很清楚了，z\_1,z\_2,\\dots,z\_n\\in\\mathbb{R}^d。\
\
[回复评论](https://kexue.fm/archives/7343/comment-page-1?replyTo=20335#respond-post-7343)\
\
greg\_yang 发表于\
November 30th, 2022\
\
我在vae的网络基础上改了一下，效果很差，甚至没有还原训练集的能力，也找不到eae相关代码 T\_T ,这篇论文我找不到相关参考代码，苏老师有复现结果吗\
\
[回复评论](https://kexue.fm/archives/7343/comment-page-1?replyTo=20494#respond-post-7343)\
\
[苏剑林](https://kexue.fm/) 发表于\
December 4th, 2022\
\
我没复现过。你可以尝试先将最大熵的正则项设为零，看看能不能正常重构，然后慢慢增大权重。\
\
[回复评论](https://kexue.fm/archives/7343/comment-page-1?replyTo=20513#respond-post-7343)\
\
greg\_yang 发表于\
December 7th, 2022\
\
感谢苏老师，实验结果问题主要出现在bn层上，不加最大熵和bn层就是最基础的Autoencoder，这个重构是没问题的，但是将bn层加在z上后，AE也不能重构了，苏老师有什么建议吗，是我bn层加的不对吗\
\
[回复评论](https://kexue.fm/archives/7343/comment-page-1?replyTo=20529#respond-post-7343)\
\
[苏剑林](https://kexue.fm/) 发表于\
December 7th, 2022\
\
大概率是你BN加得不对吧。EAE我没实现过，但是我实验过 [https://kexue.fm/archives/7381](https://kexue.fm/archives/7381) 的方法，它也是加了BN，并且保留了正则项，在这种情况下，模型（就是一个VAE）都能完成重构，更不用说AE了。单纯加上BN，基本来说对重构是更有利的。\
\
[回复评论](https://kexue.fm/archives/7343/comment-page-1?replyTo=20534#respond-post-7343)\
\
greg\_yang 发表于\
December 9th, 2022\
\
谢谢苏老师耐心解答，成功复现了，重构结果还可以，不过对其latent随机生成（randn）后直接decoder测试，EAE的生成能力就远不如VAE了\
\
[苏剑林](https://kexue.fm/) 发表于\
December 16th, 2022\
\
非主流模型，可能对训练细节的要求很高吧～\
\
master\_ge\
\
January 16th, 2023\
\
苏神我想我问一下，有H(z)的表达了，为什么不直接进行采样计算，而需要用熵的采样临近估计呢\
\
[回复评论](https://kexue.fm/archives/7343/comment-page-1?replyTo=20757#respond-post-7343)\
\
[苏剑林](https://kexue.fm/) 发表于\
January 19th, 2023\
\
哪里有H(z)的表达式？\
\
[回复评论](https://kexue.fm/archives/7343/comment-page-1?replyTo=20779#respond-post-7343)\
\
灰羽 发表于\
April 26th, 2023\
\
他可能是说（3）式得到的概率分布可以用采样的方法近似p(x)来求吗\
\
[回复评论](https://kexue.fm/archives/7343/comment-page-1?replyTo=21484#respond-post-7343)\
\
ningmang208\
\
December 26th, 2023\
\
苏神，有个小疑问，BN 后的 z 不是已经是接近正态分布了吗\
\
[回复评论](https://kexue.fm/archives/7343/comment-page-1?replyTo=23363#respond-post-7343)\
\
[苏剑林](https://kexue.fm/) 发表于\
December 30th, 2023\
\
BN是稍微向标准正态分布靠近了一点，但离标准正态分布还差得远。\
\
[回复评论](https://kexue.fm/archives/7343/comment-page-1?replyTo=23403#respond-post-7343)\
\
[取消回复](https://kexue.fm/archives/7343#respond-post-7343)\
\
你的大名\
\
电子邮箱\
\
个人网站（选填）\
\
1\. 可以使用LaTeX代码，点击“预览效果”可查看效果；\
\
2\. 可以通过点击评论楼层编号来引用该楼层；\
\
3\. 网站可能会有点卡，如非确认评论失败，请 **不要重复点击提交**。\
\
### 内容速览\
\
[自编码器vs生成模型](https://kexue.fm/archives/7343#%E8%87%AA%E7%BC%96%E7%A0%81%E5%99%A8vs%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B)\
[正态分布与最大熵](https://kexue.fm/archives/7343#%E6%AD%A3%E6%80%81%E5%88%86%E5%B8%83%E4%B8%8E%E6%9C%80%E5%A4%A7%E7%86%B5)\
[均值方差约束与BN](https://kexue.fm/archives/7343#%E5%9D%87%E5%80%BC%E6%96%B9%E5%B7%AE%E7%BA%A6%E6%9D%9F%E4%B8%8EBN)\
[熵的采样邻近估计](https://kexue.fm/archives/7343#%E7%86%B5%E7%9A%84%E9%87%87%E6%A0%B7%E9%82%BB%E8%BF%91%E4%BC%B0%E8%AE%A1)\
[进一步思考与分析](https://kexue.fm/archives/7343#%E8%BF%9B%E4%B8%80%E6%AD%A5%E6%80%9D%E8%80%83%E4%B8%8E%E5%88%86%E6%9E%90)\
[最后补上一个小结](https://kexue.fm/archives/7343#%E6%9C%80%E5%90%8E%E8%A1%A5%E4%B8%8A%E4%B8%80%E4%B8%AA%E5%B0%8F%E7%BB%93)\
\
### 智能搜索\
\
支持整句搜索！网站自动使用 [结巴分词](https://github.com/fxsjy/jieba) 进行分词，并结合ngrams排序算法给出合理的搜索结果。\
\
### 热门标签\
\
[生成模型](https://kexue.fm/tag/%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B/) [attention](https://kexue.fm/tag/attention/) [优化](https://kexue.fm/tag/%E4%BC%98%E5%8C%96/) [语言模型](https://kexue.fm/tag/%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/) [模型](https://kexue.fm/tag/%E6%A8%A1%E5%9E%8B/) [梯度](https://kexue.fm/tag/%E6%A2%AF%E5%BA%A6/) [网站](https://kexue.fm/tag/%E7%BD%91%E7%AB%99/) [概率](https://kexue.fm/tag/%E6%A6%82%E7%8E%87/) [矩阵](https://kexue.fm/tag/%E7%9F%A9%E9%98%B5/) [优化器](https://kexue.fm/tag/%E4%BC%98%E5%8C%96%E5%99%A8/) [转载](https://kexue.fm/tag/%E8%BD%AC%E8%BD%BD/) [微分方程](https://kexue.fm/tag/%E5%BE%AE%E5%88%86%E6%96%B9%E7%A8%8B/) [分析](https://kexue.fm/tag/%E5%88%86%E6%9E%90/) [天象](https://kexue.fm/tag/%E5%A4%A9%E8%B1%A1/) [深度学习](https://kexue.fm/tag/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/) [积分](https://kexue.fm/tag/%E7%A7%AF%E5%88%86/) [python](https://kexue.fm/tag/python/) [扩散](https://kexue.fm/tag/%E6%89%A9%E6%95%A3/) [力学](https://kexue.fm/tag/%E5%8A%9B%E5%AD%A6/) [无监督](https://kexue.fm/tag/%E6%97%A0%E7%9B%91%E7%9D%A3/) [几何](https://kexue.fm/tag/%E5%87%A0%E4%BD%95/) [节日](https://kexue.fm/tag/%E8%8A%82%E6%97%A5/) [生活](https://kexue.fm/tag/%E7%94%9F%E6%B4%BB/) [文本生成](https://kexue.fm/tag/%E6%96%87%E6%9C%AC%E7%94%9F%E6%88%90/) [数论](https://kexue.fm/tag/%E6%95%B0%E8%AE%BA/)\
\
### 随机文章\
\
- [【NASA每日一图】牧羊卫星Prometheus](https://kexue.fm/archives/391)\
- [【备忘】谈谈dropout](https://kexue.fm/archives/4521)\
- [《当彩色的声音尝起来是甜的》电子版](https://kexue.fm/archives/280)\
- [月全食刚过...](https://kexue.fm/archives/1521)\
- [端午&高考乱弹：怀念的，也许只是怀念本身](https://kexue.fm/archives/6704)\
- [纠缠的时空（二）：洛仑兹变换的矩阵(续)](https://kexue.fm/archives/1923)\
- [2009年中秋手机拍摄月亮](https://kexue.fm/archives/157)\
- [祝大家国庆快乐！](https://kexue.fm/archives/140)\
- [【NASA每日一图】天文城的夏季星空](https://kexue.fm/archives/120)\
- [最近的那些事儿...](https://kexue.fm/archives/1388)\
\
### 最近评论\
\
- [Bin](https://kexue.fm/archives/1990/comment-page-2#comment-29105): 今天偶然从某个论坛看到有人推荐您的博客，定睛一看竟然是华师同院的往届师兄！看到这篇2013年的...\
- [Rapture D](https://kexue.fm/archives/11530/comment-page-1#comment-29104): 我有一个问题，为什么不考虑亥姆霍兹定理和斯托克斯公式。\
- [mofheka](https://kexue.fm/archives/11390/comment-page-1#comment-29103): 苏神是还在用jax是么？最近在做基于Google Pathway的理念做一个动态版的MPMD框...\
- [长琴](https://kexue.fm/archives/11530/comment-page-1#comment-29102): 看懂这篇博客也不是一件容易的事情。\
- [AlexLi](https://kexue.fm/archives/9257/comment-page-4#comment-29101): 苏老师，请教一下(7)式中将 \\mu(x\_t) 传给 p\_o 进行推理的操作。 $x\_...\
- [tyler\_zxc](https://kexue.fm/archives/7921/comment-page-2#comment-29100): "Performer的思想是将标准的Attention线性化，所以为什么不干脆直接训练一个线性...\
- [我](https://kexue.fm/archives/11494/comment-page-1#comment-29099): 似乎并非mHC提出矩阵的思想？之前hyper connection就是了\
- [winter](https://kexue.fm/archives/10847/comment-page-1#comment-29098): 苏神您好，假如对于比较均匀的attention weightP，往往呈现long tail分布...\
- [苏剑林](https://kexue.fm/archives/8512/comment-page-2#comment-29097): KL散度、JS散度、W距离啥的，都行啊，看你喜欢哪个\
- [苏剑林](https://kexue.fm/archives/9119/comment-page-14#comment-29096): 没有绝对公平的对比方法，主要看你关心什么。比如，如果只关心推理成本和推理效果，那么有的方法可以...\
\
### 友情链接\
\
- [Cool Papers](https://papers.cool/)\
- [数学研发](https://bbs.emath.ac.cn/)\
- [Seatop](http://www.seatop.com.cn/)\
- [Xiaoxia](https://xiaoxia.org/)\
- [积分表-网络版](https://kexue.fm/sci/integral/index.html)\
- [丝路博傲](http://blog.dvxj.com/)\
- [数学之家](http://www.2math.cn/)\
- [有趣天文奇观](http://interesting-sky.china-vo.org/)\
- [TwistedW](http://www.twistedwg.com/)\
- [godweiyang](https://godweiyang.com/)\
- [AI柠檬](https://blog.ailemon.net/)\
- [王登科-DK博客](https://greatdk.com/)\
- [ESON](https://blog.eson.org/)\
- [枫之羽](https://fzhiy.net/)\
- [coding-zuo](https://coding-zuo.github.io/)\
- [博科园](https://www.bokeyuan.net/)\
- [孔皮皮的博客](https://www.kppkkp.top/)\
- [运鹏的博客](https://yunpengtai.top/)\
- [jiming.site](https://jiming.site/)\
- [OmegaXYZ](https://www.omegaxyz.com/)\
- [EAI猩球](https://www.robotech.ink/)\
- [文举的博客](https://liwenju0.com/)\
- [申请链接](https://kexue.fm/links.html)\
\
[![署名-非商业用途-保持一致](https://kexue.fm/usr/themes/geekg/images/cc.gif)](http://creativecommons.org/licenses/by-nc-nd/2.5/cn/) 本站采用创作共用版权协议，要求署名、非商业用途和保持一致。转载本站内容必须也遵循“ [署名-非商业用途-保持一致](http://creativecommons.org/licenses/by-nc-nd/2.5/cn/)”的创作共用协议。\
\
\
\
© 2009-2026 Scientific Spaces. All rights reserved. Theme by [laogui](http://www.laogui.com/). Powered by [Typecho](http://typecho.org/). 备案号: [粤ICP备09093259号-1/2](https://beian.miit.gov.cn/ "粤ICP备09093259号")。