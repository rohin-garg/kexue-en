## SEARCH

## MENU

- [打赏](https://kexue.fm/reward.html)
- [公式](https://kexue.fm/latex.html)
- [天象](https://kexue.fm/ac.html)
- [链接](https://kexue.fm/links.html)
- [时光](https://kexue.fm/me.html)
- [博览](https://kexue.fm/science.html)
- [归档](https://kexue.fm/content.html)

## CATEGORIES

- [千奇百怪](https://kexue.fm/category/Everything)
- [天文探索](https://kexue.fm/category/Astronomy)
- [数学研究](https://kexue.fm/category/Mathematics)
- [物理化学](https://kexue.fm/category/Phy-chem)
- [信息时代](https://kexue.fm/category/Big-Data)
- [生物自然](https://kexue.fm/category/Biology)
- [图片摄影](https://kexue.fm/category/Photograph)
- [问题百科](https://kexue.fm/category/Questions)
- [生活/情感](https://kexue.fm/category/Life-Feeling)
- [资源共享](https://kexue.fm/category/Resources)

## NEWPOSTS

- [从无穷范数求导到等值振荡定理](https://kexue.fm/archives/10972)
- [生成扩散模型漫谈（三十）：从瞬时速...](https://kexue.fm/archives/10958)
- [MoE环游记：5、均匀分布的反思](https://kexue.fm/archives/10945)
- [msign算子的Newton-Sc...](https://kexue.fm/archives/10922)
- [Transformer升级之路：2...](https://kexue.fm/archives/10907)
- [一道概率不等式：盯着它到显然成立为止！](https://kexue.fm/archives/10902)
- [SVD的导数](https://kexue.fm/archives/10878)
- [智能家居之手搓一套能接入米家的零冷水装置](https://kexue.fm/archives/10869)
- [Transformer升级之路：1...](https://kexue.fm/archives/10862)
- [矩阵的有效秩（Effective ...](https://kexue.fm/archives/10847)

## COMMENTS

- [OceanYU: 您好，关于由式（7）推导出高斯分布，我这里有一点问题，式（7）...](https://kexue.fm/archives/9164/comment-page-4#comment-27801)
- [jorjiang: 训练和prefill这个compute-bound阶段不做矩阵...](https://kexue.fm/archives/10907/comment-page-2#comment-27800)
- [amy: 苏老师，您有关注傅里叶旋转位置编码这篇工作吗，想知道您对这篇工...](https://kexue.fm/archives/10907/comment-page-2#comment-27799)
- [jiurizz: 在2\*shared experts + 160\*routed ...](https://kexue.fm/archives/10945/comment-page-1#comment-27798)
- [开水: 感谢苏老师回复，论文appendix里面写了实验是load b...](https://kexue.fm/archives/10945/comment-page-1#comment-27797)
- [苏剑林: 欢迎作者，这篇文章确实在收藏夹了，结果还没来得及看，抱歉哈，马...](https://kexue.fm/archives/10958/comment-page-1#comment-27796)
- [苏剑林: “将存储kv cache改为存储降维后的Embedding X...](https://kexue.fm/archives/10907/comment-page-2#comment-27795)
- [苏剑林: 呃，是这样的，Moonlight-16B-A3B实际上对应的是...](https://kexue.fm/archives/10945/comment-page-1#comment-27794)
- [Jiaming Song: 推销一下我们前一段时间的工作，这个其实已经达到了你说的三个标准...](https://kexue.fm/archives/10958/comment-page-1#comment-27793)
- [ZhouTimeMachine: 感谢您的回复！这么一看，我才注意到能从 $p(x\_0)$ 变换...](https://kexue.fm/archives/9497/comment-page-2#comment-27792)

## USERLOGIN

- [登录](https://kexue.fm/admin/login.php)

[科学空间\|Scientific Spaces](https://kexue.fm)

- [登录](https://kexue.fm/admin/login.php)
- [打赏](https://kexue.fm/reward.html)
- [公式](https://kexue.fm/latex.html)
- [天象](https://kexue.fm/ac.html)
- [链接](https://kexue.fm/links.html)
- [时光](https://kexue.fm/me.html)
- [博览](https://kexue.fm/science.html)
- [归档](https://kexue.fm/content.html)

渴望成为一个小飞侠

- [欢迎订阅](https://kexue.fm/feed)
- [个性邮箱](https://kexue.fm/archives/119)
- [天象信息](https://kexue.fm/ac.html)
- [观测ISS](https://kexue.fm/archives/41)
- [LaTeX](https://kexue.fm/latex.html)
- [关于博主](https://kexue.fm/me.html)

欢迎访问“科学空间”，这里将与您共同探讨自然科学，回味人生百态；也期待大家的分享～

- [**千奇百怪** Everything](https://kexue.fm/category/Everything)
- [**天文探索** Astronomy](https://kexue.fm/category/Astronomy)
- [**数学研究** Mathematics](https://kexue.fm/category/Mathematics)
- [**物理化学** Phy-chem](https://kexue.fm/category/Phy-chem)
- [**信息时代** Big-Data](https://kexue.fm/category/Big-Data)
- [**生物自然** Biology](https://kexue.fm/category/Biology)
- [**图片摄影** Photograph](https://kexue.fm/category/Photograph)
- [**问题百科** Questions](https://kexue.fm/category/Questions)
- [**生活/情感** Life-Feeling](https://kexue.fm/category/Life-Feeling)
- [**资源共享** Resources](https://kexue.fm/category/Resources)

- [**千奇百怪**](https://kexue.fm/category/Everything)
- [**天文探索**](https://kexue.fm/category/Astronomy)
- [**数学研究**](https://kexue.fm/category/Mathematics)
- [**物理化学**](https://kexue.fm/category/Phy-chem)
- [**信息时代**](https://kexue.fm/category/Big-Data)
- [**生物自然**](https://kexue.fm/category/Biology)
- [**图片摄影**](https://kexue.fm/category/Photograph)
- [**问题百科**](https://kexue.fm/category/Questions)
- [**生活/情感**](https://kexue.fm/category/Life-Feeling)
- [**资源共享**](https://kexue.fm/category/Resources)

[首页](https://kexue.fm) [信息时代](https://kexue.fm/category/Big-Data) AdaFactor优化器浅析（附开源实现）

23Mar

# [AdaFactor优化器浅析（附开源实现）](https://kexue.fm/archives/7302)

By 苏剑林 \|
2020-03-23 \|
97193位读者\|

自从GPT、BERT等预训练模型流行起来后，其中一个明显的趋势是模型越做越大，因为更大的模型配合更充分的预训练通常能更有效地刷榜。不过，理想可以无限远，现实通常很局促，有时候模型太大了，大到哪怕你拥有了大显存的GPU甚至TPU，依然会感到很绝望。比如GPT2最大的版本有15亿参数，最大版本的T5模型参数量甚至去到了110亿，这等规模的模型，哪怕在TPU集群上也没法跑到多大的batch size。

这时候通常要往优化过程着手，比如使用混合精度训练（tensorflow下还可以使用一种叫做bfloat16的新型浮点格式），即省显存又加速训练；又或者使用更省显存的优化器，比如RMSProp就比Adam更省显存。本文则介绍 **AdaFactor**，一个由Google提出来的新型优化器，首发论文为 [《Adafactor: Adaptive Learning Rates with Sublinear Memory Cost》](https://papers.cool/arxiv/1804.04235)。AdaFactor具有自适应学习率的特性，但比RMSProp还要省显存，并且还针对性地解决了Adam的一些缺陷。

## Adam [\#](https://kexue.fm/archives/7302\#Adam)

首先我们来回顾一下常用的Adam优化器的更新过程。设$t$为迭代步数，$\\alpha\_t$为当前学习率，$L(\\theta)$是损失函数，$\\theta$是待优化参数，$\\epsilon$则是防止溢出的小正数，那么Adam的更新过程为
\\begin{equation}\\left\\{\\begin{aligned}&g\_t = \\nabla\_{\\theta} L(\\theta\_{t-1})\\\
&m\_t = \\beta\_1 m\_{t-1} + \\left(1 - \\beta\_1\\right) g\_t\\\
&v\_t = \\beta\_2 v\_{t-1} + \\left(1 - \\beta\_2\\right) g\_t^2\\\
&\\hat{m}\_t = m\_t\\left/\\left(1 - \\beta\_1^t\\right)\\right.\\\
&\\hat{v}\_t = v\_t\\left/\\left(1 - \\beta\_2^t\\right)\\right.\\\
&\\theta\_t = \\theta\_{t-1} - \\alpha\_t \\hat{m}\_t\\left/\\left(\\sqrt{\\hat{v}\_t} + \\epsilon\\right)\\right.
\\end{aligned}\\right.\\end{equation}

要省显存，就首先得知道显存花在哪里的。首先，计算量和显存的大头肯定都是$\\nabla\_{\\theta} L(\\theta\_{t-1})$，也就是说，计算梯度是很费资源的，这也是为啥“ALBERT相比BERT参数量虽然少了那么多，但训练速度也没见快多少”的原因了；除此之外，显存的消耗主要是$m,v$了，我们要维护两组缓存变量，来滑动计算梯度的前两阶矩（也就是$m$和$v$），用以计算参数的更新量。这两组变量每一组都跟训练参数本身一样大，因此对于参数比较多的模型，两组缓存变量所消耗的显存也不少。

## AdaFactor [\#](https://kexue.fm/archives/7302\#AdaFactor)

在这一节中，我们会相对详细地介绍一些AdaFactor优化器，介绍中会设计比较多的公式和推导。如果只求一个大致了解的读者，可以自行跳过部分数学内容～

### 抛弃动量 [\#](https://kexue.fm/archives/7302\#%E6%8A%9B%E5%BC%83%E5%8A%A8%E9%87%8F)

我们知道，CV模型很多时候要靠“SGD+动量”来炼出最优效果来，自适应学习率优化器通常训练不出最好的效果。但对于NLP模型来说，情况有点相反，自适应学习率显得更重要一些，很少听到由纯靠SGD调NLP模型的案例。因此，作为省显存的第一步，我们可以抛弃Adam里边的动量，这样就少一组缓存参数了，自然也就省了显存：
\\begin{equation}\\left\\{\\begin{aligned}&g\_t = \\nabla\_{\\theta} L(\\theta\_{t-1})\\\
&v\_t = \\beta\_2 v\_{t-1} + \\left(1 - \\beta\_2\\right) g\_t^2\\\
&\\hat{v}\_t = v\_t\\left/\\left(1 - \\beta\_2^t\\right)\\right.\\\
&\\theta\_t = \\theta\_{t-1} - \\alpha\_t g\_t\\left/\\sqrt{\\hat{v}\_t + \\epsilon}\\right.
\\end{aligned}\\right.\\end{equation}
这其实就是RMSProp的变种，比RMSProp多了$\\hat{v}\_t = v\_t\\left/\\left(1 - \\beta\_2^t\\right)\\right.$这一步。

### 低秩分解 [\#](https://kexue.fm/archives/7302\#%E4%BD%8E%E7%A7%A9%E5%88%86%E8%A7%A3)

去掉$m$之后，缓存变量直接减少了一半，但AdaFactor还不满意，它希望保留自适应学习率功能，但把缓存变量$v$的参数量再压一压。这一次，它用到了矩阵的低秩分解。

#### 广义KL散度 [\#](https://kexue.fm/archives/7302\#%E5%B9%BF%E4%B9%89KL%E6%95%A3%E5%BA%A6)

在SGD中，所有参数都是共用一个标量学习率；在Adam中，则是每一个参数都有自己的学习率$\\alpha\_t\\left/\\sqrt{\\hat{v}\_t + \\epsilon}\\right.$。我们知道通过精调学习率，SGD其实也能有不错的效果，这表明“每一个参数都有自己的学习率”这件事情都不是特别重要，或者换一种说法，就是“精调每一个参数自己的学习率”并不是特别重要。

这启发我们，将$\\hat{v}\_t$换一种参数更少的近似可能也就足够了。而“参数更少的近似”，我们就不难想到低秩分解了。对于$m\\times n$的矩阵$C$，我们希望找到$m\\times k$的矩阵$A$和$k\\times n$的矩阵$B$，使得
\\begin{equation}AB \\approx C\\end{equation}
当$k$足够小时，$A$、$B$的参数总量就小于$C$的参数量。为了“省”到极致，AdaFactor直接让$k=1$，即寻找$\\{a\_i\\}\_{i=1}^m$和$\\{b\_j\\}\_{j=1}^n$，使得
\\begin{equation}a\_i b\_j \\approx c\_{i,j}\\end{equation}
既然要近似，就要有一个度量的标准。很容易想到的标准是欧氏距离，即
\\begin{equation}\\sum\_{i,j} (a\_i b\_j - c\_{i,j})^2\\end{equation}
但在这个距离之下，$a\_i,b\_j$并没有解析解；此外，在优化过程中$c\_{i,j}$（即$\\hat{v}\_t$）是非负的，而通过上述目标优化出来的$a\_i b\_j$无法保证非负，因此很可能扰乱优化过程。

原论文的作者们很机智地换了一个度量标准，使得$a\_i,b\_j$有解析解。具体来说，它使用了“广义KL散度”，又称“I散度（ [I-Divergence](https://arxiv.org/abs/math/0412070)）”，其形式为：
\\begin{equation}l = \\sum\_{i,j} c\_{i,j}\\log \\frac{c\_{i,j}}{a\_i b\_j} - c\_{i,j} + a\_i b\_j \\label{eq:i-div}\\end{equation}
这个度量源自不等式$x\\log x\\geq x - 1(\\forall x > 0)$，当且仅当$x=1$时等号成立。所以代入$x = p / q\\,(p,q > 0)$，然后两端乘以$q$，我们有
\\begin{equation}p\\log \\frac{p}{q} - p + q \\geq 0\\end{equation}
当且仅当$p=q$成立，如果$p,q$有多个分量，那么对多个分量的结果求和即可，这就得到了度量$\\eqref{eq:i-div}$

显然，广义KL散度是概率的KL散度的自然推广，但它不要求$c\_{i,j}$和$a\_i b\_j$满足归一化，只要求它们非负，这正好对应了AdaFactor的场景。而且巧妙的是，这种情形配上这个目标，刚好有解析解：
\\begin{equation}a\_i = \\sum\\limits\_{j}c\_{i,j},\\quad b\_j = \\frac{\\sum\\limits\_{i}c\_{i,j}}{\\sum\\limits\_{i,j}c\_{i,j}}\\label{eq:aibj}\\end{equation}
其实这个解析解也很形象，就是行、列分别求和，然后相乘，再除以全体的和。

#### 推导过程 [\#](https://kexue.fm/archives/7302\#%E6%8E%A8%E5%AF%BC%E8%BF%87%E7%A8%8B)

直接对$\\eqref{eq:i-div}$求偏导数并让偏导数等于0，得
\\begin{equation}\\left\\{\\begin{aligned}
&\\frac{\\partial l}{\\partial a\_i}=\\sum\_j -\\frac{c\_{i,j}}{a\_i} + b\_j = 0\\\
&\\frac{\\partial l}{\\partial b\_j}=\\sum\_i -\\frac{c\_{i,j}}{b\_j} + a\_i = 0
\\end{aligned}\\right.\\end{equation}
整理得
\\begin{equation}\\left\\{\\begin{aligned}
&a\_i \\sum\_{j} b\_j = \\sum\_j c\_{i,j}\\\
&b\_j \\sum\_{i} a\_i = \\sum\_i c\_{i,j}
\\end{aligned}\\right.\\end{equation}
注意到如果$(a\_i,b\_j)$是一组最优解，那么$(\\lambda a\_i,b\_j/\\lambda)$也是，说白了，所有的$a\_i$乘以一个常数，所有的$b\_j$也除以这个常数，$a\_i b\_j$是不变的。那么我们就可以随意指定$\\sum\\limits\_{i} a\_i$或$\\sum\\limits\_{j} b\_j$，因为它们就只是一个缩放标量而已。不失一般性，我们指定$\\sum\\limits\_{j} b\_j=1$，那么就解得$\\eqref{eq:aibj}$。

#### 直观理解 [\#](https://kexue.fm/archives/7302\#%E7%9B%B4%E8%A7%82%E7%90%86%E8%A7%A3)

我们也可以从另一个角度理解结果$\\eqref{eq:aibj}$。由于$c\_{i,j}$是非负的，我们可以将它归一化，变成具有概率分布的特性，即$\\hat{c}\_{i,j}=\\frac{c\_{i,j}}{\\sum\\limits\_{i,j}c\_{i,j}}$，然后我们试图完成分解$\\hat{c}\_{i,j}\\approx \\hat{a}\_i \\hat{b}\_j$，由于$\\hat{c}\_{i,j}$现在相当于一个二元联合概率分布，那么$\\hat{a}\_i,\\hat{b}\_j$就相当于它们的边缘分布，即
\\begin{equation}\\hat{a}\_i = \\sum\_j \\hat{c}\_{i,j} = \\frac{\\sum\\limits\_{j}c\_{i,j}}{\\sum\\limits\_{i,j} c\_{i,j}},\\quad \\hat{b}\_j = \\sum\_i \\hat{c}\_{i,j} = \\frac{\\sum\\limits\_{i}c\_{i,j}}{\\sum\\limits\_{i,j}c\_{i,j}}\\end{equation}
现在$\\hat{c}\_{i,j}$到$c\_{i,j}$还需要乘上一个$\\sum\\limits\_{i,j}c\_{i,j}$，我们可以把它乘到$\\hat{a}\_i$或$\\hat{b}\_j$中，不失一般性，我们假设乘到$\\hat{a}\_i$上，那么就得到$\\eqref{eq:aibj}$。

#### AdaFactor雏形 [\#](https://kexue.fm/archives/7302\#AdaFactor%E9%9B%8F%E5%BD%A2)

有了结果$\\eqref{eq:aibj}$后，我们就可以用它来构建更省内存的优化器了，这就是AdaFactor的雏形。简单来说，当参数$\\theta$是普通一维向量时，优化过程保持不变；但$\\theta$是$m\\times n$的矩阵时，算出来的梯度$g\_t$也是矩阵，从而$g\_t^2$也是矩阵，这时候我们对$g\_t^2$做低秩分解，然后维护两组缓存变量$v^{(r)}\_t\\in \\mathbb{R}^m,v^{(c)}\_t\\in\\mathbb{R}^n$，分别滑动平均低秩分解后的结果，最后用$v^{(r)}\_t,v^{(c)}\_t$共同调整学习率：
\\begin{equation}\\left\\{\\begin{aligned}&g\_{i,j;t} = \\nabla\_{\\theta} L(\\theta\_{i,j;t-1})\\\
&v^{(r)}\_{i;t} = \\beta\_2 v^{(r)}\_{t-1;i} + \\left(1 - \\beta\_2\\right) \\sum\\limits\_{j}\\left(g\_{i,j;t}^2+\\epsilon\\right)\\\
&v^{(c)}\_{j;t} = \\beta\_2 v^{(c)}\_{t-1;j} + \\left(1 - \\beta\_2\\right) \\sum\\limits\_{i}\\left(g\_{i,j;t}^2+\\epsilon\\right)\\\
&v\_{i,j;t} = v^{(r)}\_{i;t} v^{(c)}\_{j;t}\\left/\\sum\\limits\_{j}v^{(c)}\_{j;t}\\right.\\\
&\\hat{v}\_t = v\_t\\left/\\left(1 - \\beta\_2^t\\right)\\right.\\\
&\\theta\_t = \\theta\_{t-1} - \\alpha\_t g\_t\\left/\\sqrt{\\hat{v}\_t}\\right.
\\end{aligned}\\right.\\end{equation}
（把$\\epsilon$加到$g\_t^2$上去而不是$\\hat{v}\_t$上去，这是AdaFactor整出来的形式，不是笔者的锅～）

### 滑动权重 [\#](https://kexue.fm/archives/7302\#%E6%BB%91%E5%8A%A8%E6%9D%83%E9%87%8D)

在Adam以及上述AdaFactor雏形中，滑动权重$\\beta\_2$都是恒为常数，AdaFactor指出这是不科学的，并提出新的策略。

#### 等价形式 [\#](https://kexue.fm/archives/7302\#%E7%AD%89%E4%BB%B7%E5%BD%A2%E5%BC%8F)

为了认识到这一点，我们重写一下Adam的$\\hat{v}\_t$的更新过程：
\\begin{equation}\\begin{aligned}
\\hat{v}\_t =& v\_t\\left/\\left(1 - \\beta\_2^t\\right)\\right.\\\
=&\\frac{\\beta\_2 v\_{t-1} + (1-\\beta\_2) g\_t^2}{1 - \\beta\_2^t}\\\
=&\\frac{\\beta\_2 \\hat{v}\_{t-1}\\left(1 - \\beta\_2^{t-1}\\right) + (1-\\beta\_2) g\_t^2}{1 - \\beta\_2^t}\\\
=&\\beta\_2\\frac{1 - \\beta\_2^{t-1}}{1 - \\beta\_2^t}\\hat{v}\_{t-1} + \\left(1 - \\beta\_2\\frac{1 - \\beta\_2^{t-1}}{1 - \\beta\_2^t}\\right)g\_t^2
\\end{aligned}\\end{equation}
所以如果设$\\hat{\\beta}\_{2,t}=\\beta\_2\\frac{1 - \\beta\_2^{t-1}}{1 - \\beta\_2^t}$，那么更新公式就是
\\begin{equation}\\hat{v}\_t =\\hat{\\beta}\_{2,t}\\hat{v}\_{t-1} + \\left(1 - \\hat{\\beta}\_{2,t}\\right)g\_t^2\\end{equation}
问题是这个$\\hat{\\beta}\_{2,t}$够不够合理呢？答案是可能不大够。当$t=1$时$\\hat{\\beta}\_{2,t}=0$，这时候$\\hat{v}\_t$就是$g\_t^2$，也就是用实时梯度来校正学习率，这时候校正力度最大；当$t\\to\\infty$时，$\\hat{\\beta}\_{2,t}\\to \\beta\_2$，这时候$v\_t$是累积梯度平方与当前梯度平方的加权平均，由于$\\beta\_2 < 1$，所以意味着当前梯度的权重$1 - \\beta\_2$不为0，这可能导致训练不稳定，因为训练后期梯度变小，训练本身趋于稳定，校正学习率的意义就不大了，因此学习率的校正力度应该变小，并且$t\\to\\infty$，学习率最好恒定为常数（这时候相当于退化为SGD），这就要求$t\\to\\infty$时，$\\hat{\\beta}\_{2,t}\\to 1$。

#### 新的衰减策略 [\#](https://kexue.fm/archives/7302\#%E6%96%B0%E7%9A%84%E8%A1%B0%E5%87%8F%E7%AD%96%E7%95%A5)

为了达到这个目的，AdaFactor采用如下的衰减策略
\\begin{equation}\\hat{\\beta}\_{2,t} =1 - \\frac{1}{t^c}\\label{eq:beta2}\\end{equation}
它满足$\\hat{\\beta}\_{2,1}=0,\\lim\\limits\_{t\\to\\infty} \\hat{\\beta}\_{2,t}=1$。但即便如此，也不是任何$c$都适合，必须有$0 < c <1$。$c > 0$好理解，那为什么要$c < 1$呢？原论文包含了对它的分析，大家可以去读读，但笔者觉得原论文的推导过于晦涩，所以这里给出自己的理解。

首先，对于$\\hat{v}\_t$来说，一个很容易想到的方案是所有梯度平方的平均，即：
\\begin{equation}\\hat{v}\_t = \\frac{1}{t}\\sum\_{i=1}^t g\_i^2=\\frac{t-1}{t}\\hat{v}\_{t-1} + \\frac{1}{t}g\_t^2\\end{equation}
所以这等价于让$\\hat{\\beta}\_{2,t} =1 - \\frac{1}{t}$。这个方案美中不足的一点是，每一步梯度都是平权的，这不符合直觉，因为正常来说越久远的梯度应该越不重要才对，所以应该适当降低历史部分权重，而当$c < 1$时，$1 - \\frac{1}{t^c} < 1 - \\frac{1}{t}$，因此一个简洁的方案是在式$\\eqref{eq:beta2}$中取$c < 1$，AdaFactor默认的$c$是$0.8$。

### 层自适应 [\#](https://kexue.fm/archives/7302\#%E5%B1%82%E8%87%AA%E9%80%82%E5%BA%94)

最后，我们还可以进一步根据参数的模长来校正更新量，这个思路来自 [LAMB优化器](https://papers.cool/arxiv/1904.00962)，在之前的文章 [《6个派生优化器的简单介绍及其实现》](https://kexue.fm/archives/7094#%E5%B1%82%E8%87%AA%E9%80%82%E5%BA%94) 中也介绍过。简单来说，它就是将最后的更新量标准化，然后乘以参数的模长，说白了，就是不管你怎么折腾，最后的更新量我只要你的方向，而大小由参数本身的模长和预先设置学习率共同决定，使得所有层所有参数的相对变化程度保持一致。

### AdaFactor完整版 [\#](https://kexue.fm/archives/7302\#AdaFactor%E5%AE%8C%E6%95%B4%E7%89%88)

至此，我们终于可以写出完整版AdaFactor的更新过程了：
\\begin{equation}\\left\\{\\begin{aligned}&g\_{i,j;t} = \\nabla\_{\\theta} L(\\theta\_{i,j;t-1})\\\
&\\hat{\\beta}\_{2,t} =1 - t^{-c}\\\
&v^{(r)}\_{i;t} = \\hat{\\beta}\_{2,t} v^{(r)}\_{t-1;i} + \\left(1 - \\hat{\\beta}\_{2,t}\\right) \\sum\\limits\_{j}\\left(g\_{i,j;t}^2+\\epsilon\_1\\right)\\\
&v^{(c)}\_{j;t} = \\hat{\\beta}\_{2,t} v^{(c)}\_{t-1;j} + \\left(1 - \\hat{\\beta}\_{2,t}\\right) \\sum\\limits\_{i}\\left(g\_{i,j;t}^2+\\epsilon\_1\\right)\\\
&\\hat{v}\_{i,j;t} = v^{(r)}\_{i;t} v^{(c)}\_{j;t}\\left/\\sum\\limits\_{j}v^{(c)}\_{j;t}\\right.\\\
&u\_t = g\_t\\left/\\sqrt{\\hat{v}\_t}\\right.\\\
&\\hat{u}\_t = u\_t \\left/\\max\\left(1, \\left. RMS(u\_t)\\right/d\\right)\\right.\\times \\max\\left(\\epsilon\_2, RMS(\\theta\_{t-1})\\right)\\\
&\\theta\_t = \\theta\_{t-1} - \\alpha\_t \\hat{u}\_t
\\end{aligned}\\right.\\end{equation}
其中$RMS(x)=\\sqrt{\\frac{1}{n}\\sum\\limits\_{i=1}^n x\_i^2}$是模长的变种，$\\max\\left(1, \\left. RMS(u\_t)\\right/d\\right)$这一步相当于做了个截断，即$RMS(u\_t) > d$时才执行归一化。原论文中的默认参数为
$$\\begin{array}{c\|c}
\\hline
\\epsilon\_1 & 10^{-30}\\\
\\hline
\\epsilon\_2 & 10^{-3}\\\
\\hline
d & 1\\\
\\hline
\\hat{\\beta}\_{2,t} & 1 - t^{-0.8}\\\
\\hline
\\end{array}$$
如果参数是一维向量而不是矩阵，那么$\\hat{v}\_t$使用普通的更新公式$\\hat{v}\_t = \\hat{\\beta}\_{2,t} v\_{t-1} + \\left(1 - \\hat{\\beta}\_{2,t}\\right) \\left(g\_t^2+\\epsilon\_1\\right)$就行了。此外，论文还提出如果没有传入学习率，那么可以使用$a\_t = \\min\\left(10^{-2},\\frac{1}{\\sqrt{t}}\\right)$为默认学习率，但笔者看源码的时候发现这个默认学习率很少使用，基本上还是需要自己传入学习率的。

## 开源实现 [\#](https://kexue.fm/archives/7302\#%E5%BC%80%E6%BA%90%E5%AE%9E%E7%8E%B0)

为了方便大家使用，笔者开源了自己实现的AdaFactor：

> **Github地址： [https://github.com/bojone/adafactor](https://github.com/bojone/adafactor)**

开源包括纯keras版和tf.keras版，使用方法跟普通keras优化器一样，tf.keras版也可以当做一个普通的tensorflow优化器使用。开源实现参考了 [mesh\_tensorflow版的源码](https://github.com/tensorflow/mesh/blob/63754cf4524cb96282ac0dfe453a15076a76589f/mesh_tensorflow/optimize.py#L204)，在此表示感谢。优化器也已经内置在 [bert4keras](https://github.com/bojone/bert4keras/blob/master/bert4keras/optimizers.py) 中，方便大家调用。

需要提醒的是，用AdaFactor的时候，batch\_size最好大一些，因为本身低秩分解会带来误差，而如果batch\_size过小，那么梯度估算本身也带来较大的误差，两者叠加优化过程可能还不收敛。对于预训练模型来说，batch\_size通常还是很大的，所以现在不少预训练模型开始用AdaFactor优化器了；对于普通的下游任务来说，AdaFactor也可以尝试，但可能需要多炼炼丹，才能搞出优于无脑Adam的效果。对了，还要提醒一下，用AdaFactor的时候，学习率要设大一点，大概是$10^{-3}$级别为好，哪怕是finetune阶段也是如此。

## 文章小结 [\#](https://kexue.fm/archives/7302\#%E6%96%87%E7%AB%A0%E5%B0%8F%E7%BB%93)

本文介绍了Google提出来的AdaFactor优化器，一个旨在减少显存占用的优化器，并且针对性地分析并解决了Adam的一些缺陷。笔者认为，AdaFactor针对Adam所做的分析相当经典，值得我们认真琢磨体味，对有兴趣研究优化问题的读者来说，更是一个不可多得的分析案例。

当然，没有什么绝对能有效的方法，有的只是

> 方法虽好，要想实际有效，依然要用心炼丹。

_**转载到请包括本文地址：** [https://kexue.fm/archives/7302](https://kexue.fm/archives/7302)_

_**更详细的转载事宜请参考：**_ [《科学空间FAQ》](https://kexue.fm/archives/6508#%E6%96%87%E7%AB%A0%E5%A6%82%E4%BD%95%E8%BD%AC%E8%BD%BD/%E5%BC%95%E7%94%A8)

**如果您还有什么疑惑或建议，欢迎在下方评论区继续讨论。**

**如果您觉得本文还不错，欢迎 [分享](https://kexue.fm/archives/7302#share)/ [打赏](https://kexue.fm/archives/7302#pay) 本文。打赏并非要从中获得收益，而是希望知道科学空间获得了多少读者的真心关注。当然，如果你无视它，也不会影响你的阅读。再次表示欢迎和感谢！**

打赏

微信打赏

支付宝打赏

因为网站后台对打赏并无记录，因此欢迎在打赏时候备注留言。你还可以 [**点击这里**](http://mail.qq.com/cgi-bin/qm_share?t=qm_mailme&email=tN7d1drY3drrx8H0xcWa19vZ) 或在下方评论区留言来告知你的建议或需求。

**如果您需要引用本文，请参考：**

苏剑林. (Mar. 23, 2020). 《AdaFactor优化器浅析（附开源实现） 》\[Blog post\]. Retrieved from [https://kexue.fm/archives/7302](https://kexue.fm/archives/7302)

@online{kexuefm-7302,
        title={AdaFactor优化器浅析（附开源实现）},
        author={苏剑林},
        year={2020},
        month={Mar},
        url={\\url{https://kexue.fm/archives/7302}},
}

分类： [信息时代](https://kexue.fm/category/Big-Data)    标签： [分析](https://kexue.fm/tag/%E5%88%86%E6%9E%90/), [keras](https://kexue.fm/tag/keras/), [优化器](https://kexue.fm/tag/%E4%BC%98%E5%8C%96%E5%99%A8/)[20 评论](https://kexue.fm/archives/7302#comments)

< [现在可以用Keras玩中文GPT2了（GPT2\_ML）](https://kexue.fm/archives/7292) \| [GELU的两个初等函数近似是怎么来的](https://kexue.fm/archives/7309) >

### 你也许还对下面的内容感兴趣

- [从无穷范数求导到等值振荡定理](https://kexue.fm/archives/10972)
- [msign算子的Newton-Schulz迭代（上）](https://kexue.fm/archives/10922)
- [SVD的导数](https://kexue.fm/archives/10878)
- [通过梯度近似寻找Normalization的替代品](https://kexue.fm/archives/10831)
- [高阶muP：更简明但更高明的谱条件缩放](https://kexue.fm/archives/10795)
- [初探muP：超参数的跨模型尺度迁移规律](https://kexue.fm/archives/10770)
- [Muon续集：为什么我们选择尝试Muon？](https://kexue.fm/archives/10739)
- [为什么梯度裁剪的默认模长是1？](https://kexue.fm/archives/10657)
- [从谱范数梯度到新式权重衰减的思考](https://kexue.fm/archives/10648)
- [Muon优化器赏析：从向量到矩阵的本质跨越](https://kexue.fm/archives/10592)

[发表你的看法](https://kexue.fm/archives/7302#comment_form)

jjjjohnson

April 2nd, 2020

大佬用的keras是什么版本的？ 我跑了会有这个报错：

AttributeError: module 'keras.backend' has no attribute 'symbolic'

我的版本：2.2.5

多谢！

[回复评论](https://kexue.fm/archives/7302/comment-page-1?replyTo=13126#respond-post-7302)

[苏剑林](https://kexue.fm) 发表于
April 2nd, 2020

2.3.1。如果是2.2.x的话，去掉@K.symbolic那一行就行了。

[回复评论](https://kexue.fm/archives/7302/comment-page-1?replyTo=13128#respond-post-7302)

[太仓人才网](https://www.tcrcw.com)

April 2nd, 2020

完全懵的状态

[回复评论](https://kexue.fm/archives/7302/comment-page-1?replyTo=13129#respond-post-7302)

Zessay

April 26th, 2020

苏神，公式（8）的$a\_i$，分母是不是也应该要除以所有元素的和？

[回复评论](https://kexue.fm/archives/7302/comment-page-1?replyTo=13224#respond-post-7302)

Zessay 发表于
April 26th, 2020

算了一下，确实不需要，不好意思！

[回复评论](https://kexue.fm/archives/7302/comment-page-1?replyTo=13225#respond-post-7302)

fengfei91

May 18th, 2020

苏神，今天我用了bert4keras 和adafactor训练了个bert模型，用model.save保存了下来。然后我再另一个app里面用
from bert4keras.layers import \*
model = keras.models.load\_model(bert\_model.h5)

结果报错：找不到adafactor这个类，在反序列化的时候。在keras的optimizers.py的方法
def deserialize(config, custom\_objects=None):

all\_classes = {
'sgd': SGD,
'rmsprop': RMSprop,
'adagrad': Adagrad,
'adadelta': Adadelta,
'adam': Adam,
'adamax': Adamax,
'nadam': Nadam,
'tfoptimizer': TFOptimizer,
}

这里面没有adafactor这个类。

[回复评论](https://kexue.fm/archives/7302/comment-page-1?replyTo=13381#respond-post-7302)

[苏剑林](https://kexue.fm) 发表于
May 18th, 2020

加一句
from bert4keras.optimizers import \*

AdaFactor是自己定义的优化器，定义在bert4keras/optimizers.py中

[回复评论](https://kexue.fm/archives/7302/comment-page-1?replyTo=13382#respond-post-7302)

fengfei91 发表于
May 19th, 2020

还是不行，加了这句。

我最后是custom\_objects = {
'AdaFactorV1': AdaFactor,
}
然后load\_model的时候，把custom objects传进去好了。

[回复评论](https://kexue.fm/archives/7302/comment-page-1?replyTo=13384#respond-post-7302)

[苏剑林](https://kexue.fm) 发表于
May 19th, 2020

是我的疏忽，github上已经调整过来了～

[回复评论](https://kexue.fm/archives/7302/comment-page-1?replyTo=13385#respond-post-7302)

zbh

July 9th, 2020

代码中，
`if indices[-2] < self.min_dim_size_to_factor:`
似乎应该改为
`if shape[indices[-2]] < self.min_dim_size_to_factor:`

[回复评论](https://kexue.fm/archives/7302/comment-page-1?replyTo=13783#respond-post-7302)

[苏剑林](https://kexue.fm) 发表于
July 10th, 2020

是的是的，感谢你的纠正。

[回复评论](https://kexue.fm/archives/7302/comment-page-1?replyTo=13791#respond-post-7302)

神马呵呵哒

August 13th, 2021

您好，想请教一下公式（1）的第三行和第四行中的β1t和β2t是怎么计算的呢？因为torch调包的时候只有 learning\_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-8这几个参数，如果不加第三行和第四行有影响吗？因为我理解的Adam是加了动量的RMSpro,而不管是动还是RMSPro好像都没有出现修正的情况

[回复评论](https://kexue.fm/archives/7302/comment-page-1?replyTo=17130#respond-post-7302)

[苏剑林](https://kexue.fm) 发表于
August 14th, 2021

正常来说bias correction都会自带在Adam的内置实现中，不需要刻意传参。

[回复评论](https://kexue.fm/archives/7302/comment-page-1?replyTo=17136#respond-post-7302)

神马呵呵哒 发表于
August 14th, 2021

这个修正的作用主要是什么呢？因为看的SGD加动量是没有修正的，RMSPro好像也没有修正

[回复评论](https://kexue.fm/archives/7302/comment-page-1?replyTo=17139#respond-post-7302)

[苏剑林](https://kexue.fm) 发表于
August 15th, 2021

修正估计结果，使得一开始就以均值的量级进行更新。

不过你也可以理解为加上bias correction就是正常的Adam，去掉bias correction就相当于warmup，这个具体有多大作用，我也不清楚。

[回复评论](https://kexue.fm/archives/7302/comment-page-1?replyTo=17143#respond-post-7302)

ddf13

August 17th, 2021

公式10 ，为什么bj也是累加的，感觉应该不累加吧？

[回复评论](https://kexue.fm/archives/7302/comment-page-1?replyTo=17147#respond-post-7302)

[苏剑林](https://kexue.fm) 发表于
August 17th, 2021

那就说明你的感觉错误，公式$(10)$是没错的。

[回复评论](https://kexue.fm/archives/7302/comment-page-1?replyTo=17148#respond-post-7302)

list

March 28th, 2022

苏神，“吹毛求疵”一下。
Adam 公式，最后参数更新的时候，$\\epsilon$ 似乎应该写在根号外面。
$$\\theta\_t = \\theta\_{t-1} - \\alpha\_t \\hat{m\_t}\\bigg{/} \\left(\\sqrt{\\hat{v\_t}} + \\epsilon\\right) $$
原论文和您的实现也都是先开方再加 $\\epsilon$。
沐神在《动手学深度学习》Adam 算法中，也简单提了下这个地方。

[回复评论](https://kexue.fm/archives/7302/comment-page-1?replyTo=18793#respond-post-7302)

list 发表于
March 28th, 2022

$$\\theta\_t = \\theta\_{t-1} - \\alpha\_t \\hat{m}\_t \\bigg/\\left( \\sqrt{\\hat{v}\_t} + \\epsilon\\right)$$

[回复评论](https://kexue.fm/archives/7302/comment-page-1?replyTo=18794#respond-post-7302)

[苏剑林](https://kexue.fm) 发表于
March 29th, 2022

好的，接受批评指正哈，已修正，谢谢

[回复评论](https://kexue.fm/archives/7302/comment-page-1?replyTo=18803#respond-post-7302)

[取消回复](https://kexue.fm/archives/7302#respond-post-7302)

你的大名

电子邮箱

个人网站（选填）

1\. 可以使用LaTeX代码，点击“预览效果”可查看效果；2. 可以通过点击评论楼层编号来引用该楼层；3. 网站可能会有点卡，如非确认评论失败，请不要重复点击提交。

### 内容速览

[Adam](https://kexue.fm/archives/7302#Adam)
[AdaFactor](https://kexue.fm/archives/7302#AdaFactor)
[抛弃动量](https://kexue.fm/archives/7302#%E6%8A%9B%E5%BC%83%E5%8A%A8%E9%87%8F)
[低秩分解](https://kexue.fm/archives/7302#%E4%BD%8E%E7%A7%A9%E5%88%86%E8%A7%A3)
[广义KL散度](https://kexue.fm/archives/7302#%E5%B9%BF%E4%B9%89KL%E6%95%A3%E5%BA%A6)
[推导过程](https://kexue.fm/archives/7302#%E6%8E%A8%E5%AF%BC%E8%BF%87%E7%A8%8B)
[直观理解](https://kexue.fm/archives/7302#%E7%9B%B4%E8%A7%82%E7%90%86%E8%A7%A3)
[AdaFactor雏形](https://kexue.fm/archives/7302#AdaFactor%E9%9B%8F%E5%BD%A2)
[滑动权重](https://kexue.fm/archives/7302#%E6%BB%91%E5%8A%A8%E6%9D%83%E9%87%8D)
[等价形式](https://kexue.fm/archives/7302#%E7%AD%89%E4%BB%B7%E5%BD%A2%E5%BC%8F)
[新的衰减策略](https://kexue.fm/archives/7302#%E6%96%B0%E7%9A%84%E8%A1%B0%E5%87%8F%E7%AD%96%E7%95%A5)
[层自适应](https://kexue.fm/archives/7302#%E5%B1%82%E8%87%AA%E9%80%82%E5%BA%94)
[AdaFactor完整版](https://kexue.fm/archives/7302#AdaFactor%E5%AE%8C%E6%95%B4%E7%89%88)
[开源实现](https://kexue.fm/archives/7302#%E5%BC%80%E6%BA%90%E5%AE%9E%E7%8E%B0)
[文章小结](https://kexue.fm/archives/7302#%E6%96%87%E7%AB%A0%E5%B0%8F%E7%BB%93)

### 智能搜索

支持整句搜索！网站自动使用 [结巴分词](https://github.com/fxsjy/jieba) 进行分词，并结合ngrams排序算法给出合理的搜索结果。

### 热门标签

[生成模型](https://kexue.fm/tag/%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B/) [attention](https://kexue.fm/tag/attention/) [优化](https://kexue.fm/tag/%E4%BC%98%E5%8C%96/) [语言模型](https://kexue.fm/tag/%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/) [模型](https://kexue.fm/tag/%E6%A8%A1%E5%9E%8B/) [网站](https://kexue.fm/tag/%E7%BD%91%E7%AB%99/) [概率](https://kexue.fm/tag/%E6%A6%82%E7%8E%87/) [梯度](https://kexue.fm/tag/%E6%A2%AF%E5%BA%A6/) [转载](https://kexue.fm/tag/%E8%BD%AC%E8%BD%BD/) [微分方程](https://kexue.fm/tag/%E5%BE%AE%E5%88%86%E6%96%B9%E7%A8%8B/) [天象](https://kexue.fm/tag/%E5%A4%A9%E8%B1%A1/) [矩阵](https://kexue.fm/tag/%E7%9F%A9%E9%98%B5/) [分析](https://kexue.fm/tag/%E5%88%86%E6%9E%90/) [深度学习](https://kexue.fm/tag/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/) [积分](https://kexue.fm/tag/%E7%A7%AF%E5%88%86/) [python](https://kexue.fm/tag/python/) [力学](https://kexue.fm/tag/%E5%8A%9B%E5%AD%A6/) [无监督](https://kexue.fm/tag/%E6%97%A0%E7%9B%91%E7%9D%A3/) [优化器](https://kexue.fm/tag/%E4%BC%98%E5%8C%96%E5%99%A8/) [扩散](https://kexue.fm/tag/%E6%89%A9%E6%95%A3/) [几何](https://kexue.fm/tag/%E5%87%A0%E4%BD%95/) [节日](https://kexue.fm/tag/%E8%8A%82%E6%97%A5/) [生活](https://kexue.fm/tag/%E7%94%9F%E6%B4%BB/) [文本生成](https://kexue.fm/tag/%E6%96%87%E6%9C%AC%E7%94%9F%E6%88%90/) [数论](https://kexue.fm/tag/%E6%95%B0%E8%AE%BA/)

### 随机文章

- [《方程与宇宙》:二体问题的来来去去(一)](https://kexue.fm/archives/549)
- [科学空间：2009年11月重要天象](https://kexue.fm/archives/204)
- [首次报名参加天文竞赛，期待中...](https://kexue.fm/archives/277)
- [《新理解矩阵6》：为什么只有方阵有行列式？](https://kexue.fm/archives/2757)
- [古老的火山爆发造成地球冰期？](https://kexue.fm/archives/13)
- [《新理解矩阵4》：相似矩阵的那些事儿](https://kexue.fm/archives/1777)
- [Naive Bayes is all you need ?](https://kexue.fm/archives/9648)
- [矩阵化简二次型（无穷小近似处理抛物型）](https://kexue.fm/archives/1841)
- [新年新天象：2010年1月重要天象](https://kexue.fm/archives/316)
- [从最大似然到EM算法：一致的理解方式](https://kexue.fm/archives/5239)

### 最近评论

- [OceanYU](https://kexue.fm/archives/9164/comment-page-4#comment-27801): 您好，关于由式（7）推导出高斯分布，我这里有一点问题，式（7）只能保证关于x\_t-1是二次函数...
- [jorjiang](https://kexue.fm/archives/10907/comment-page-2#comment-27800): 训练和prefill这个compute-bound阶段不做矩阵吸收，这个用我这个解释更好理解了...
- [amy](https://kexue.fm/archives/10907/comment-page-2#comment-27799): 苏老师，您有关注傅里叶旋转位置编码这篇工作吗，想知道您对这篇工作的看法是什么，这篇工作可以wo...
- [jiurizz](https://kexue.fm/archives/10945/comment-page-1#comment-27798): 在2\*shared experts + 160\*routed expert + top6的配置...
- [开水](https://kexue.fm/archives/10945/comment-page-1#comment-27797): 感谢苏老师回复，论文appendix里面写了实验是load balance loss，正交lo...
- [苏剑林](https://kexue.fm/archives/10958/comment-page-1#comment-27796): 欢迎作者，这篇文章确实在收藏夹了，结果还没来得及看，抱歉哈，马上学习。
- [苏剑林](https://kexue.fm/archives/10907/comment-page-2#comment-27795): “将存储kv cache改为存储降维后的Embedding X”这句话没错，但是按照我的理解，...
- [苏剑林](https://kexue.fm/archives/10945/comment-page-1#comment-27794): 呃，是这样的，Moonlight-16B-A3B实际上对应的是 scaling\_factor(...
- [Jiaming Song](https://kexue.fm/archives/10958/comment-page-1#comment-27793): 推销一下我们前一段时间的工作，这个其实已经达到了你说的三个标准，并且不需要stop\_grad或...
- [ZhouTimeMachine](https://kexue.fm/archives/9497/comment-page-2#comment-27792): 感谢您的回复！这么一看，我才注意到能从 $p(x\_0)$ 变换到 $p(x\_1)$ 实际上是每...

### 友情链接

- [Cool Papers](https://papers.cool)
- [数学研发](https://bbs.emath.ac.cn)
- [Seatop](http://www.seatop.com.cn/)
- [Xiaoxia](https://xiaoxia.org/)
- [积分表-网络版](https://kexue.fm/sci/integral/index.html)
- [丝路博傲](http://blog.dvxj.com/)
- [ph4ntasy 饭特稀](http://www.ph4ntasy.com/)
- [数学之家](http://www.2math.cn/)
- [有趣天文奇观](http://interesting-sky.china-vo.org/)
- [TwistedW](http://www.twistedwg.com/)
- [godweiyang](https://godweiyang.com/)
- [AI柠檬](https://blog.ailemon.net/)
- [王登科-DK博客](https://greatdk.com)
- [ESON](https://blog.eson.org/)
- [枫之羽](https://fzhiy.net/)
- [Mathor's blog](https://wmathor.com/)
- [coding-zuo](https://coding-zuo.github.io/)
- [博科园](https://www.bokeyuan.net/)
- [孔皮皮的博客](https://www.kppkkp.top/)
- [运鹏的博客](https://yunpengtai.top/)
- [jiming.site](https://jiming.site/)
- [OmegaXYZ](https://www.omegaxyz.com/)
- [Blog by Eacls](https://www.eacls.top/)
- [EAI猩球](https://www.robotech.ink/)
- [文举的博客](https://liwenju0.com/)
- [用代码打点酱油](https://bruceyuan.com/)
- [申请链接](https://kexue.fm/links.html)

本站采用创作共用版权协议，要求署名、非商业用途和保持一致。转载本站内容必须也遵循“ [署名-非商业用途-保持一致](http://creativecommons.org/licenses/by-nc-nd/2.5/cn/)”的创作共用协议。
© 2009-2025 Scientific Spaces. All rights reserved. Theme by [laogui](http://www.laogui.com). Powered by [Typecho](http://typecho.org). 备案号: [粤ICP备09093259号-1/2](https://beian.miit.gov.cn/)。