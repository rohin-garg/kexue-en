Processing math: 0%

![MobileSideBar](https://kexue.fm/usr/themes/geekg/images/slide-button.png)

## SEARCH

## MENU

- [打赏](https://kexue.fm/reward.html)
- [公式](https://kexue.fm/latex.html)
- [天象](https://kexue.fm/ac.html)
- [链接](https://kexue.fm/links.html)
- [时光](https://kexue.fm/me.html)
- [博览](https://kexue.fm/science.html)
- [归档](https://kexue.fm/content.html)

## CATEGORIES

- [千奇百怪](https://kexue.fm/category/Everything)
- [天文探索](https://kexue.fm/category/Astronomy)
- [数学研究](https://kexue.fm/category/Mathematics)
- [物理化学](https://kexue.fm/category/Phy-chem)
- [信息时代](https://kexue.fm/category/Big-Data)
- [生物自然](https://kexue.fm/category/Biology)
- [图片摄影](https://kexue.fm/category/Photograph)
- [问题百科](https://kexue.fm/category/Questions)
- [生活/情感](https://kexue.fm/category/Life-Feeling)
- [资源共享](https://kexue.fm/category/Resources)

## NEWPOSTS

- [让炼丹更科学一些（五）：基于梯度精...](https://kexue.fm/archives/11530)
- [让炼丹更科学一些（四）：新恒等式，...](https://kexue.fm/archives/11494)
- [为什么DeltaNet要加L2 N...](https://kexue.fm/archives/11486)
- [让炼丹更科学一些（三）：SGD的终...](https://kexue.fm/archives/11480)
- [让炼丹更科学一些（二）：将结论推广...](https://kexue.fm/archives/11469)
- [滑动平均视角下的权重衰减和学习率](https://kexue.fm/archives/11459)
- [生成扩散模型漫谈（三十一）：预测数...](https://kexue.fm/archives/11428)
- [Muon优化器指南：快速上手与关键细节](https://kexue.fm/archives/11416)
- [AdamW的Weight RMS的...](https://kexue.fm/archives/11404)
- [n个正态随机数的最大值的渐近估计](https://kexue.fm/archives/11390)

## COMMENTS

- [Bin: 今天偶然从某个论坛看到有人推荐您的博客，定睛一看竟然是华师同院...](https://kexue.fm/archives/1990/comment-page-2#comment-29105)
- [Rapture D: 我有一个问题，为什么不考虑亥姆霍兹定理和斯托克斯公式。](https://kexue.fm/archives/11530/comment-page-1#comment-29104)
- [mofheka: 苏神是还在用jax是么？最近在做基于Google Pathwa...](https://kexue.fm/archives/11390/comment-page-1#comment-29103)
- [长琴: 看懂这篇博客也不是一件容易的事情。](https://kexue.fm/archives/11530/comment-page-1#comment-29102)
- [AlexLi: 苏老师，请教一下(7)式中将 \\mu(x\_t) 传给 $p...](https://kexue.fm/archives/9257/comment-page-4#comment-29101)
- [tyler\_zxc: "Performer的思想是将标准的Attention线性化，...](https://kexue.fm/archives/7921/comment-page-2#comment-29100)
- [我: 似乎并非mHC提出矩阵的思想？之前hyper connecti...](https://kexue.fm/archives/11494/comment-page-1#comment-29099)
- [winter: 苏神您好，假如对于比较均匀的attention weightP...](https://kexue.fm/archives/10847/comment-page-1#comment-29098)
- [苏剑林: KL散度、JS散度、W距离啥的，都行啊，看你喜欢哪个](https://kexue.fm/archives/8512/comment-page-2#comment-29097)
- [苏剑林: 没有绝对公平的对比方法，主要看你关心什么。比如，如果只关心推理...](https://kexue.fm/archives/9119/comment-page-14#comment-29096)

## USERLOGIN

- [登录](https://kexue.fm/admin/login.php)

[科学空间\|Scientific Spaces](https://kexue.fm/)

- [登录](https://kexue.fm/admin/login.php)
- [打赏](https://kexue.fm/reward.html)
- [公式](https://kexue.fm/latex.html)
- [天象](https://kexue.fm/ac.html)
- [链接](https://kexue.fm/links.html)
- [时光](https://kexue.fm/me.html)
- [博览](https://kexue.fm/science.html)
- [归档](https://kexue.fm/content.html)

渴望成为一个小飞侠

- [![](https://kexue.fm/usr/themes/geekg/images/rss.png)\\
\\
欢迎订阅](https://kexue.fm/feed)
- [![](https://kexue.fm/usr/themes/geekg/images/mail.png)\\
\\
个性邮箱](https://kexue.fm/archives/119)
- [![](https://kexue.fm/usr/themes/geekg/images/Saturn.png)\\
\\
天象信息](https://kexue.fm/ac.html)
- [![](https://kexue.fm/usr/themes/geekg/images/iss.png)\\
\\
观测ISS](https://kexue.fm/archives/41)
- [![](https://kexue.fm/usr/themes/geekg/images/pi.png)\\
\\
LaTeX](https://kexue.fm/latex.html)
- [![](https://kexue.fm/usr/themes/geekg/images/mlogo.png)\\
\\
关于博主](https://kexue.fm/me.html)

欢迎访问“科学空间”，这里将与您共同探讨自然科学，回味人生百态；也期待大家的分享～

- [**千奇百怪** Everything](https://kexue.fm/category/Everything)
- [**天文探索** Astronomy](https://kexue.fm/category/Astronomy)
- [**数学研究** Mathematics](https://kexue.fm/category/Mathematics)
- [**物理化学** Phy-chem](https://kexue.fm/category/Phy-chem)
- [**信息时代** Big-Data](https://kexue.fm/category/Big-Data)
- [**生物自然** Biology](https://kexue.fm/category/Biology)
- [**图片摄影** Photograph](https://kexue.fm/category/Photograph)
- [**问题百科** Questions](https://kexue.fm/category/Questions)
- [**生活/情感** Life-Feeling](https://kexue.fm/category/Life-Feeling)
- [**资源共享** Resources](https://kexue.fm/category/Resources)

- [**千奇百怪**](https://kexue.fm/category/Everything)
- [**天文探索**](https://kexue.fm/category/Astronomy)
- [**数学研究**](https://kexue.fm/category/Mathematics)
- [**物理化学**](https://kexue.fm/category/Phy-chem)
- [**信息时代**](https://kexue.fm/category/Big-Data)
- [**生物自然**](https://kexue.fm/category/Biology)
- [**图片摄影**](https://kexue.fm/category/Photograph)
- [**问题百科**](https://kexue.fm/category/Questions)
- [**生活/情感**](https://kexue.fm/category/Life-Feeling)
- [**资源共享**](https://kexue.fm/category/Resources)

[首页](https://kexue.fm/) [信息时代](https://kexue.fm/category/Big-Data) 生成扩散模型漫谈（二十七）：将步长作为条件输入

15Dec

# [生成扩散模型漫谈（二十七）：将步长作为条件输入](https://kexue.fm/archives/10617)

By 苏剑林 \|
2024-12-15 \|
66683位读者 \|

这篇文章我们再次聚焦于扩散模型的采样加速。众所周知，扩散模型的采样加速主要有两种思路，一是开发更高效的求解器，二是事后蒸馏。然而，据笔者观察，除了上两篇文章介绍过的 [SiD](https://kexue.fm/archives/10085) 外，这两种方案都鲜有能将生成步数降低到一步的结果。虽然SiD能做到单步生成，但它需要额外的蒸馏成本，并且蒸馏过程中用到了类似GAN的交替训练过程，总让人感觉差点意思。

本文要介绍的是 [《One Step Diffusion via Shortcut Models》](https://papers.cool/arxiv/2410.12557)，其突破性思想是将生成步长也作为扩散模型的条件输入，然后往训练目标中加入了一个直观的正则项，这样就能直接稳定训练出可以单步生成模型，可谓简单有效的经典之作。

## ODE扩散 [\#](https://kexue.fm/archives/10617\#ODE%E6%89%A9%E6%95%A3)

原论文的结论是基于ODE式扩散模型的，而对于ODE式扩散的理论基础，我们在本系列的 [（六）](https://kexue.fm/archives/9228)、 [（十二）](https://kexue.fm/archives/9280)、 [（十四）](https://kexue.fm/archives/9370)、 [（十五）](https://kexue.fm/archives/9379)、 [（十七）](https://kexue.fm/archives/9497) 等博客中已经多次介绍，其中最简单的一种理解方式大概是 [（十七）](https://kexue.fm/archives/9497) 中的ReFlow视角，下面我们简单重复一下。

假设\\boldsymbol{x}\_0\\sim p\_0(\\boldsymbol{x}\_0)是先验分布采样的随机噪声，\\boldsymbol{x}\_1\\sim p\_1(\\boldsymbol{x}\_1)是目标分布采样的真实样本（注：前面的文章中，普通都是\\boldsymbol{x}\_T是噪声、\\boldsymbol{x}\_0是目标样本，这里方便起见反过来了），ReFlow允许我们指定任意从\\boldsymbol{x}\_0到\\boldsymbol{x}\_1的运动轨迹，最简单的轨迹自然是直线：

\\begin{equation}\\boldsymbol{x}\_t = (1-t)\\boldsymbol{x}\_0 + t \\boldsymbol{x}\_1\\label{eq:line}\\end{equation}

两边求导，就可以得到它满足的ODE（常微分方程）：

\\begin{equation}\\frac{d\\boldsymbol{x}\_t}{dt} = \\boldsymbol{x}\_1 - \\boldsymbol{x}\_0\\end{equation}

这个ODE很简单，但实际上没用，因为我们想要的是通过ODE由\\boldsymbol{x}\_0生成\\boldsymbol{x}\_1，而上述ODE却显式地依赖\\boldsymbol{x}\_1。为了解决这个问题，一个很简单的想法是“学一个\\boldsymbol{x}\_t的函数去逼近\\boldsymbol{x}\_1 - \\boldsymbol{x}\_0”，学完之后就用它来取代\\boldsymbol{x}\_1 - \\boldsymbol{x}\_0，即

\\begin{equation}\\boldsymbol{\\theta}^\* = \\mathop{\\text{argmin}}\_{\\boldsymbol{\\theta}} \\mathbb{E}\_{\\boldsymbol{x}\_0\\sim p\_0(\\boldsymbol{x}\_0),\\boldsymbol{x}\_1\\sim p\_1(\\boldsymbol{x}\_1)}\\left\[\\Vert\\boldsymbol{v}\_{\\boldsymbol{\\theta}}(\\boldsymbol{x}\_t, t) - (\\boldsymbol{x}\_1 - \\boldsymbol{x}\_0)\\Vert^2\\right\]\\label{eq:loss}\\end{equation}

以及

\\begin{equation}\\frac{d\\boldsymbol{x}\_t}{dt} = \\boldsymbol{x}\_1 - \\boldsymbol{x}\_0\\quad\\Rightarrow\\quad\\frac{d\\boldsymbol{x}\_t}{dt} = \\boldsymbol{v}\_{\\boldsymbol{\\theta}^\*}(\\boldsymbol{x}\_t, t)\\label{eq:ode-core}\\end{equation}

这就是ReFlow。当然这里边还欠缺了一个理论证明，就是通过平方误差来拟合\\boldsymbol{v}\_{\\boldsymbol{\\theta}}(\\boldsymbol{x}\_t, t)所得到的ODE确实能生成我们期望的分布，这部分大家自行看 [《生成扩散模型漫谈（十七）：构建ODE的一般步骤（下）》](https://kexue.fm/archives/9497) 就好。

## 步长自洽 [\#](https://kexue.fm/archives/10617\#%E6%AD%A5%E9%95%BF%E8%87%AA%E6%B4%BD)

假设我们已经有了\\boldsymbol{v}\_{\\boldsymbol{\\theta}}(\\boldsymbol{x}\_t, t)，那么通过求解微分方程\\frac{d\\boldsymbol{x}\_t}{dt} = \\boldsymbol{v}\_{\\boldsymbol{\\theta}}(\\boldsymbol{x}\_t, t)就可以实现从\\boldsymbol{x}\_0到\\boldsymbol{x}\_1的变换。划重点，是“微分方程”，但实际上我们没法真的去数值计算微分方程，而是只能算“差分方程”：

\\begin{equation}\\boldsymbol{x}\_{t + \\epsilon} - \\boldsymbol{x}\_t = \\boldsymbol{v}\_{\\boldsymbol{\\theta}}(\\boldsymbol{x}\_t, t) \\epsilon\\label{eq:de}\\end{equation}

这个差分方程是原始ODE的“欧拉近似”，近似程度取决于步长\\epsilon的大小，当\\epsilon\\to 0时就精确等于原始ODE，换言之步长越小越精确。然而，生成步数等于1/\\epsilon，我们希望生成步数越少越好，这意味着不能用太大的步长，最好\\epsilon可以等于1，这样\\boldsymbol{x}\_1 = \\boldsymbol{x}\_0 + \\boldsymbol{v}\_{\\boldsymbol{\\theta}}(\\boldsymbol{x}\_0, 0)，一步就可以完成生成。

问题是，如果直接用大步长代入上式，最终所算得的\\boldsymbol{x}\_1必然会严重偏离精确解。这时候原论文（下称“Shortcut模型”）的巧妙构思就登场了：它认为模型\\boldsymbol{v}\_{\\boldsymbol{\\theta}}(\\boldsymbol{x}\_t, t)不应该只是\\boldsymbol{x}\_t和t的函数，还应该是步长\\epsilon的函数，这样差分方程\\eqref{eq:de}就可以自行适应步长：

\\begin{equation}\\boldsymbol{x}\_{t + \\epsilon} - \\boldsymbol{x}\_t = \\boldsymbol{v}\_{\\boldsymbol{\\theta}}(\\boldsymbol{x}\_t, t, \\epsilon) \\epsilon\\end{equation}

目标\\eqref{eq:loss}训练的是精确的ODE模型，所以它训练的是\\epsilon=0的模型：

\\begin{equation}\\mathcal{L}\_1 = \\mathbb{E}\_{\\boldsymbol{x}\_0\\sim p\_0(\\boldsymbol{x}\_0),\\boldsymbol{x}\_1\\sim p\_1(\\boldsymbol{x}\_1)}\\left\[\\frac{1}{2}\\Vert\\boldsymbol{v}\_{\\boldsymbol{\\theta}}(\\boldsymbol{x}\_t, t, 0) - (\\boldsymbol{x}\_1 - \\boldsymbol{x}\_0)\\Vert^2\\right\]\\end{equation}

那\\epsilon > 0的部分又怎么训练呢？我们的目标是生成步数越少越好，这等价于说希望“两倍的步长走1步等于单倍的步长走2步”：

\\begin{equation}\\boldsymbol{x}\_t + \\boldsymbol{v}\_{\\boldsymbol{\\theta}}(\\boldsymbol{x}\_t, t, 2\\epsilon) 2\\epsilon = \\color{green}{\\underbrace{\\boldsymbol{x}\_t + \\boldsymbol{v}\_{\\boldsymbol{\\theta}}(\\boldsymbol{x}\_t, t, \\epsilon) \\epsilon}\_{\\tilde{\\boldsymbol{x}}\_{t+\\epsilon}}} + \\boldsymbol{v}\_{\\boldsymbol{\\theta}}\\big(\\color{green}{\\underbrace{\\boldsymbol{x}\_t + \\boldsymbol{v}\_{\\boldsymbol{\\theta}}(\\boldsymbol{x}\_t, t, \\epsilon) \\epsilon}\_{\\tilde{\\boldsymbol{x}}\_{t+\\epsilon}}}, t+\\epsilon, \\epsilon\\big) \\epsilon\\label{eq:cond}\\end{equation}

即\\boldsymbol{v}\_{\\boldsymbol{\\theta}}(\\boldsymbol{x}\_t, t, 2\\epsilon) = \[\\boldsymbol{v}\_{\\boldsymbol{\\theta}}(\\boldsymbol{x}\_t, t, \\epsilon) + \\boldsymbol{v}\_{\\boldsymbol{\\theta}}(\\color{green}{\\tilde{\\boldsymbol{x}}\_{t+\\epsilon}}, t+\\epsilon, \\epsilon)\] /2。为了达到这个目标，我们补充一项自洽性损失函数

\\begin{equation}\\mathcal{L}\_2 = \\mathbb{E}\_{\\boldsymbol{x}\_0\\sim p\_0(\\boldsymbol{x}\_0),\\boldsymbol{x}\_1\\sim p\_1(\\boldsymbol{x}\_1)}\\left\[\\Vert\\boldsymbol{v}\_{\\boldsymbol{\\theta}}(\\boldsymbol{x}\_t, t, 2\\epsilon) - \[\\boldsymbol{v}\_{\\boldsymbol{\\theta}}(\\boldsymbol{x}\_t, t, \\epsilon)+ \\boldsymbol{v}\_{\\boldsymbol{\\theta}}(\\color{green}{\\tilde{\\boldsymbol{x}}\_{t+\\epsilon}}, t+\\epsilon, \\epsilon) \]/2\\Vert^2\\right\]\\end{equation}

\\mathcal{L}\_1与\\mathcal{L}\_2相加，就构成了Shortcut模型的损失函数。

（注：有读者指出，更早的 [《Consistency Trajectory Models: Learning Probability Flow ODE Trajectory of Diffusion》](https://papers.cool/arxiv/2310.02279) 提出过以离散化时间的起点和终点作为条件输入的做法，指定起点和终点后步长其实也就确定了，所以Shortcut以步长为输入的做法并不算完全创新。）

## 模型细节 [\#](https://kexue.fm/archives/10617\#%E6%A8%A1%E5%9E%8B%E7%BB%86%E8%8A%82)

以上基本就是Shortcut模型的全部理论内容，非常精巧且简明，但从理论到实验，还需要一些细节，比如步长\\epsilon如何融入到模型中去。

首先，在训练\\mathcal{L}\_2时，Shortcut并没有均匀地从\[0,1\]采样\\epsilon，而是设置了一个最小步长2^{-7}，然后将它们倍增至1，即所有的非零步长只有\\{2^{-7},2^{-6},2^{-5},2^{-4},2^{-3},2^{-2},2^{-1},1\\}这8个值，从前7个中均匀采样来训练\\mathcal{L}\_2。这样一来，\\epsilon的取值就是有限的，算上0一共就只有9个，所以Shortcut模型直接以Embedding的方式来输入\\epsilon，将它跟t的Embedding加在一起。

其次，注意到\\mathcal{L}\_2的计算量是比\\mathcal{L}\_1大的，因为\\boldsymbol{v}\_{\\boldsymbol{\\theta}}(\\tilde{\\boldsymbol{x}}\_{t+\\epsilon}, t, \\epsilon)这一项需要两次前向传播，所以论文的做法是每个batch中3/4的样本都用来计算\\mathcal{L}\_1，剩下的1/4样本才用来算\\mathcal{L}\_2。该操作不仅是为了节省计算量，实际上还调节了\\mathcal{L}\_1,\\mathcal{L}\_2的权重，因为\\mathcal{L}\_2比\\mathcal{L}\_1更好训练，所以它的训练样本可以适当少些。

除此之外，论文在实践的时候还对\\mathcal{L}\_2做了微调，多加了个stop gradient算子：

\\begin{equation}\\mathcal{L}\_2 = \\mathbb{E}\_{\\boldsymbol{x}\_0\\sim p\_0(\\boldsymbol{x}\_0),\\boldsymbol{x}\_1\\sim p\_1(\\boldsymbol{x}\_1)}\\left\[\\Vert\\boldsymbol{v}\_{\\boldsymbol{\\theta}}(\\boldsymbol{x}\_t, t, 2\\epsilon) - \\color{skyblue}{\\text{sg}\[}\\boldsymbol{v}\_{\\boldsymbol{\\theta}}(\\boldsymbol{x}\_t, t, \\epsilon)+ \\boldsymbol{v}\_{\\boldsymbol{\\theta}}(\\color{green}{\\tilde{\\boldsymbol{x}}\_{t+\\epsilon}}, t+\\epsilon, \\epsilon) \\color{skyblue}{\]}/2\\Vert^2\\right\]\\end{equation}

为什么要这样做呢？按照作者的 [回复](https://openreview.net/forum?id=OlzB6LnXcS&noteId=k4If3csXST)，这是自引导学习的常见做法，被stop gradient的部分属于目标，不应该有梯度，跟 [BYOL](https://papers.cool/arxiv/2006.07733)、 [SimSiam](https://papers.cool/arxiv/2011.10566) 等无监督学习方案类似。不过照笔者看来，这个操作最大的价值还是节省训练成本，因为\\boldsymbol{v}\_{\\boldsymbol{\\theta}}(\\tilde{\\boldsymbol{x}}\_{t+\\epsilon}, t, \\epsilon)这一项做了两次前向传播，如果要对它反向传播，计算量也要翻倍。

## 实验效果 [\#](https://kexue.fm/archives/10617\#%E5%AE%9E%E9%AA%8C%E6%95%88%E6%9E%9C)

现在我们来看Shortcut模型的实验效果，看起来它是目前单步生成效果最好的、单阶段训练的扩散模型：

[![各种扩散模型的生成质量评估](https://kexue.fm/usr/uploads/2024/12/2449555374.png)](https://kexue.fm/usr/uploads/2024/12/2449555374.png "点击查看原图")

各种扩散模型的生成质量评估

这是它的实际采样效果图：

[![Flow Matching与Shortcut Model的实际采样效果对比](https://kexue.fm/usr/uploads/2024/12/3428964799.jpg)](https://kexue.fm/usr/uploads/2024/12/3428964799.jpg "点击查看原图")

Flow Matching与Shortcut Model的实际采样效果对比

不过仔细观察单步生成的样本就会发现，其实还有明显的瑕疵，所以说虽然Shortcut模型相比于之前的单阶段训练方案来说已经取得了较大的进步，但还有明显的提升空间。

作者已经将Shortcut模型的代码开源，Github链接是：

> **[https://github.com/kvfrans/shortcut-models](https://github.com/kvfrans/shortcut-models)**

顺便说，Shortcut模型投到了ICLR 2025上，获得了reviewer的一致好评（全8分）。

## 延伸思考 [\#](https://kexue.fm/archives/10617\#%E5%BB%B6%E4%BC%B8%E6%80%9D%E8%80%83)

看到Shortcut模型，不知道大家想到了哪些相关工作？笔者想到了一个可能大家都意想不到的，那就是我们在 [《生成扩散模型漫谈（二十一）：中值定理加速ODE采样》](https://kexue.fm/archives/9881) 介绍过的AMED。

Shortcut模型与AMED的底层思想是相通的，它们都已经发现，单靠研究复杂的高阶求解器，将生成的NFE（模型的运行次数）降低到个位数就已经很简单了，更不用说做单步生成了。所以它们一致认为，真正要变的并不是求解器，而是模型。该怎么变呢？AMED想到的是“中值定理”：对ODE两端积分，我们有精确的

\\begin{equation}\\boldsymbol{x}\_{t + \\epsilon} - \\boldsymbol{x}\_t = \\int\_t^{t + \\epsilon}\\boldsymbol{v}\_{\\boldsymbol{\\theta}}(\\boldsymbol{x}\_{\\tau}, \\tau) d\\tau\\end{equation}

类比“ [积分中值定理](https://en.wikipedia.org/wiki/Mean_value_theorem#Mean_value_theorems_for_definite_integrals)”，我们能找到一个s\\in\[t, t + \\epsilon\]，成立

\\begin{equation}\\frac{1}{\\epsilon}\\int\_t^{t + \\epsilon}\\boldsymbol{v}\_{\\boldsymbol{\\theta}}(\\boldsymbol{x}\_{\\tau}, \\tau) d\\tau = \\boldsymbol{v}\_{\\boldsymbol{\\theta}}(\\boldsymbol{x}\_s, s)\\end{equation}

于是我们得到

\\begin{equation}\\boldsymbol{x}\_{t + \\epsilon} - \\boldsymbol{x}\_t = \\boldsymbol{v}\_{\\boldsymbol{\\theta}}(\\boldsymbol{x}\_s, s) \\epsilon\\end{equation}

当然，积分中值定理实际上只对标量函数成立，对向量函数是不保证成立的，所以说是“类比”。现在的问题是并不知道s的值，所以AMED的后续做法是用一个非常小的（计算量几乎可以忽略的）模型去预测s。

AMED是基于现成扩散模型的事后修正方法，因此它的效果取决于中值定理对\\boldsymbol{v}\_{\\boldsymbol{\\theta}}(\\boldsymbol{x}\_t, t)模型的成立程度，这显得有些“运气成分”，并且AMED需要先用欧拉格式预估一下\\boldsymbol{x}\_s，所以它的NFE最少是2，不能做到单步生成。相比之下，Shortcut模型更“激进”，它直接把步长作为条件输入，将加速生成的条件\\eqref{eq:cond}作为损失函数，这样一来不仅避免了“中值定理”近似的可行性讨论，还使得最少NFE可以降低到1。

更巧妙的是，细思之下我们会发现两者的做法其实也有些共性，前面我们说了Shortcut是直接将\\epsilon转成Embedding加到t的Embeddding上的，这不相当于跟AMED一样都是修改t嘛！只不过AMED是直接修改t的数值，而Shortcut修改的是t的Embedding。

## 文章小结 [\#](https://kexue.fm/archives/10617\#%E6%96%87%E7%AB%A0%E5%B0%8F%E7%BB%93)

本文介绍了一个单阶段训练就可以实现单步生成的扩散模型新工作，它的突破思想是将步长也当成条件输入到扩散模型中，并配以一个直观的正则项，这样只通过单阶段训练就可以得到单步生成的扩散模型。

_**转载到请包括本文地址：** [https://kexue.fm/archives/10617](https://kexue.fm/archives/10617 "生成扩散模型漫谈（二十七）：将步长作为条件输入")_

_**更详细的转载事宜请参考：**_ [《科学空间FAQ》](https://kexue.fm/archives/6508#%E6%96%87%E7%AB%A0%E5%A6%82%E4%BD%95%E8%BD%AC%E8%BD%BD/%E5%BC%95%E7%94%A8 "《科学空间FAQ》")

**如果您还有什么疑惑或建议，欢迎在下方评论区继续讨论。**

**如果您觉得本文还不错，欢迎 [分享](https://kexue.fm/archives/10617#share)/ [打赏](https://kexue.fm/archives/10617#pay) 本文。打赏并非要从中获得收益，而是希望知道科学空间获得了多少读者的真心关注。当然，如果你无视它，也不会影响你的阅读。再次表示欢迎和感谢！**

打赏

![科学空间](https://kexue.fm/usr/themes/geekg/payment/wx.png)

微信打赏

![科学空间](https://kexue.fm/usr/themes/geekg/payment/zfb.png)

支付宝打赏

因为网站后台对打赏并无记录，因此欢迎在打赏时候备注留言。

你还可以 [**点击这里**](http://mail.qq.com/cgi-bin/qm_share?t=qm_mailme&email=tN7d1drY3drrx8H0xcWa19vZ) 或在下方评论区留言来告知你的建议或需求。

**如果您需要引用本文，请参考：**

苏剑林. (Dec. 15, 2024). 《生成扩散模型漫谈（二十七）：将步长作为条件输入 》\[Blog post\]. Retrieved from [https://kexue.fm/archives/10617](https://kexue.fm/archives/10617)

@online{kexuefm-10617,

         title={生成扩散模型漫谈（二十七）：将步长作为条件输入},

         author={苏剑林},

         year={2024},

         month={Dec},

         url={\\url{https://kexue.fm/archives/10617}},

}


分类： [信息时代](https://kexue.fm/category/Big-Data)    标签： [微分方程](https://kexue.fm/tag/%E5%BE%AE%E5%88%86%E6%96%B9%E7%A8%8B/), [生成模型](https://kexue.fm/tag/%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B/), [采样](https://kexue.fm/tag/%E9%87%87%E6%A0%B7/), [扩散](https://kexue.fm/tag/%E6%89%A9%E6%95%A3/)[24 评论](https://kexue.fm/archives/10617#comments)

< [Muon优化器赏析：从向量到矩阵的本质跨越](https://kexue.fm/archives/10592 "Muon优化器赏析：从向量到矩阵的本质跨越") \| [生成扩散模型漫谈（二十八）：分步理解一致性模型](https://kexue.fm/archives/10633 "生成扩散模型漫谈（二十八）：分步理解一致性模型") >

### 你也许还对下面的内容感兴趣

- [为什么DeltaNet要加L2 Normalize？](https://kexue.fm/archives/11486 "为什么DeltaNet要加L2 Normalize？")
- [生成扩散模型漫谈（三十一）：预测数据而非噪声](https://kexue.fm/archives/11428 "生成扩散模型漫谈（三十一）：预测数据而非噪声")
- [AdamW的Weight RMS的渐近估计（下）](https://kexue.fm/archives/11404 "AdamW的Weight RMS的渐近估计（下）")
- [DiVeQ：一种非常简洁的VQ训练方案](https://kexue.fm/archives/11328 "DiVeQ：一种非常简洁的VQ训练方案")
- [为什么线性注意力要加Short Conv？](https://kexue.fm/archives/11320 "为什么线性注意力要加Short Conv？")
- [Transformer升级之路：21、MLA好在哪里?（下）](https://kexue.fm/archives/11111 "Transformer升级之路：21、MLA好在哪里?（下）")
- [线性注意力简史：从模仿、创新到反哺](https://kexue.fm/archives/11033 "线性注意力简史：从模仿、创新到反哺")
- [生成扩散模型漫谈（三十）：从瞬时速度到平均速度](https://kexue.fm/archives/10958 "生成扩散模型漫谈（三十）：从瞬时速度到平均速度")
- [Transformer升级之路：20、MLA好在哪里?（上）](https://kexue.fm/archives/10907 "Transformer升级之路：20、MLA好在哪里?（上）")
- [生成扩散模型漫谈（二十九）：用DDPM来离散编码](https://kexue.fm/archives/10711 "生成扩散模型漫谈（二十九）：用DDPM来离散编码")

[发表你的看法](https://kexue.fm/archives/10617#comment_form)

sky

December 15th, 2024

您好，请问（8）式右边第三项中,v\_\\theta的输入为什么是t而不是t+\\epsilon呀？原文也是t，感到疑惑。

另：（8）式左边第二项v\_\\theta的输入应该是2\\epsilon。

谢谢。

[回复评论](https://kexue.fm/archives/10617/comment-page-1?replyTo=25985#respond-post-10617)

[苏剑林](https://kexue.fm/) 发表于
December 16th, 2024

左边的笔误已修正，右端我也同意用t+\\epsilon更合理，按照这个方式来写了。谢谢指出。

[回复评论](https://kexue.fm/archives/10617/comment-page-1?replyTo=25996#respond-post-10617)

Megatron

December 18th, 2024

看了原文，似乎也可以理解为一种一边训练diffusion一边distill的过程

v\_\\theta(x\_t, t,0)的训练就是原本的diffusion，v\_\\theta(x\_t, t,d)在训练过程中不断地吸收d=0的trajectories

v\_\\theta(x\_t, t,d)的训练也完全不影响v\_\\theta(x\_t, t,0)

如果用另一个网络u来做非零步长d也似乎可行，比如u\_\\theta(x\_t, t,d)，说不定u还可以再简化

[回复评论](https://kexue.fm/archives/10617/comment-page-1?replyTo=26004#respond-post-10617)

[苏剑林](https://kexue.fm/) 发表于
December 20th, 2024

可以这么理解。

但用另一个模型来做v\_\\theta(x\_t, t,d)简化不了吧？除非预测v\_\\theta(x\_t, t,d)与v\_\\theta(x\_t, t,0)的残差，可能还有机会。

[回复评论](https://kexue.fm/archives/10617/comment-page-1?replyTo=26039#respond-post-10617)

huchenz1

December 19th, 2024

感觉有些像Progressive Distillation，相当于在finetune teacher model

[回复评论](https://kexue.fm/archives/10617/comment-page-1?replyTo=26032#respond-post-10617)

ningmang208 发表于
March 2nd, 2025

是的，同感，我感觉 training cost 层面想要得到一个 one-step sampling 的扩散模型，是没有 free lunch 的。shortcut 模型学完，像是集成了 progressive distialltion 的多个学生模型

[回复评论](https://kexue.fm/archives/10617/comment-page-1?replyTo=26899#respond-post-10617)

[苏剑林](https://kexue.fm/) 发表于
March 5th, 2025

问题来了，如果是one-step sampling还能不能叫扩散模型呢

[回复评论](https://kexue.fm/archives/10617/comment-page-1?replyTo=26982#respond-post-10617)

梁宇辰

December 30th, 2024

感觉和这篇很像Catch-Up Distillation: You Only Need to Train Once for Accelerating Sampling然而这篇没中哈哈。不知道和ECT这种改进的CT比怎么样，之前实验跑过类似的，前传的成本其实也是有一些的。

[回复评论](https://kexue.fm/archives/10617/comment-page-1?replyTo=26125#respond-post-10617)

[苏剑林](https://kexue.fm/) 发表于
January 7th, 2025

Catch-Up Distillation相对来说还是复杂一些。ECT大致上相当于CT的EMA衰减系数设为0？我对CT这套方案其实兴趣不大，没有试验过，所以也不好判断优劣。当然本文的模型也未必是SOTA，只是学习到了“步长作为条件输入”这个思想，跟大家分享一下。

[回复评论](https://kexue.fm/archives/10617/comment-page-1?replyTo=26178#respond-post-10617)

Qiang Sun 发表于
January 19th, 2025

ECT 和CT分别是哪两片？

[回复评论](https://kexue.fm/archives/10617/comment-page-1?replyTo=26298#respond-post-10617)

[苏剑林](https://kexue.fm/) 发表于
January 20th, 2025

CT是https://arxiv.org/abs/2303.01469；ECT是https://arxiv.org/abs/2406.14548

[回复评论](https://kexue.fm/archives/10617/comment-page-1?replyTo=26346#respond-post-10617)

[Chang Li](https://github.com/ivcylc)

February 8th, 2025

shortcut这类直接end-to-end的工作是不是暂时还比不过SiD这类后处理蒸馏

[回复评论](https://kexue.fm/archives/10617/comment-page-1?replyTo=26491#respond-post-10617)

[苏剑林](https://kexue.fm/) 发表于
February 9th, 2025

暂时是的

[回复评论](https://kexue.fm/archives/10617/comment-page-1?replyTo=26538#respond-post-10617)

lic 发表于
February 11th, 2025

如果网络的预测目标不是速度而是x0的话（对应地不能用简单的一阶Euler采样了），这个时候在速度上约束这样的一致性还有用吗？

还是说在x0空间也可以推出相似的结论呢

[回复评论](https://kexue.fm/archives/10617/comment-page-1?replyTo=26571#respond-post-10617)

[苏剑林](https://kexue.fm/) 发表于
February 15th, 2025

预测速度跟预测\\boldsymbol{x}\_0本质上是等价的呀，而且最终生成过程也是一个ODE吧？

[回复评论](https://kexue.fm/archives/10617/comment-page-1?replyTo=26619#respond-post-10617)

Senmao

April 10th, 2025

感觉Shortcut Models可以看作基于Flow-Macting的CMs?

[回复评论](https://kexue.fm/archives/10617/comment-page-1?replyTo=27349#respond-post-10617)

[苏剑林](https://kexue.fm/) 发表于
April 13th, 2025

翻了翻我对一致性模型的理解（ [https://kexue.fm/archives/10633](https://kexue.fm/archives/10633) ），好像没法对得上。

[回复评论](https://kexue.fm/archives/10617/comment-page-1?replyTo=27372#respond-post-10617)

Dongraemon

May 7th, 2025

（9）中如果忽略步长条件的作用，是不是就跟CM的损失函数等价了。这样看的话，步长嵌入的作用存疑

[回复评论](https://kexue.fm/archives/10617/comment-page-1?replyTo=27543#respond-post-10617)

Xuancx

May 23rd, 2025

苏神可以看看最近的《Mean Flows for One-step Generative Modeling》吗？从Flow Mathcing角度做One Step的，看起来效果比ShortCut好多了。

[回复评论](https://kexue.fm/archives/10617/comment-page-1?replyTo=27664#respond-post-10617)

[苏剑林](https://kexue.fm/) 发表于
May 28th, 2025

[https://kexue.fm/archives/10958](https://kexue.fm/archives/10958) 新鲜出炉

[回复评论](https://kexue.fm/archives/10617/comment-page-1?replyTo=27708#respond-post-10617)

nihaowhut

July 3rd, 2025

请问ϵ为什么要限制到2^-7, 2^-6, .. 2^-1, 2^0这8个数，感觉没有必要

[回复评论](https://kexue.fm/archives/10617/comment-page-1?replyTo=28042#respond-post-10617)

[苏剑林](https://kexue.fm/) 发表于
July 11th, 2025

从shortcut的整篇文风和结果来看，作者的原则是“够用即可”，所以步长假设这8个数确实很符合这个风格。

[回复评论](https://kexue.fm/archives/10617/comment-page-1?replyTo=28068#respond-post-10617)

[Haoyu](https://cintellifusion.github.io/)

August 27th, 2025

从模型角度来说model(xt,t,d)和meanflow的model(xt,t1,t2)是等价的对吧

[回复评论](https://kexue.fm/archives/10617/comment-page-1?replyTo=28437#respond-post-10617)

[苏剑林](https://kexue.fm/) 发表于
August 31st, 2025

从理论能力来说是等价的，不过实践表现可能会有差距。

[回复评论](https://kexue.fm/archives/10617/comment-page-1?replyTo=28460#respond-post-10617)

[取消回复](https://kexue.fm/archives/10617#respond-post-10617)

你的大名

电子邮箱

个人网站（选填）

1\. 可以使用LaTeX代码，点击“预览效果”可查看效果；

2\. 可以通过点击评论楼层编号来引用该楼层；

3\. 网站可能会有点卡，如非确认评论失败，请 **不要重复点击提交**。

### 内容速览

[ODE扩散](https://kexue.fm/archives/10617#ODE%E6%89%A9%E6%95%A3)
[步长自洽](https://kexue.fm/archives/10617#%E6%AD%A5%E9%95%BF%E8%87%AA%E6%B4%BD)
[模型细节](https://kexue.fm/archives/10617#%E6%A8%A1%E5%9E%8B%E7%BB%86%E8%8A%82)
[实验效果](https://kexue.fm/archives/10617#%E5%AE%9E%E9%AA%8C%E6%95%88%E6%9E%9C)
[延伸思考](https://kexue.fm/archives/10617#%E5%BB%B6%E4%BC%B8%E6%80%9D%E8%80%83)
[文章小结](https://kexue.fm/archives/10617#%E6%96%87%E7%AB%A0%E5%B0%8F%E7%BB%93)

### 智能搜索

支持整句搜索！网站自动使用 [结巴分词](https://github.com/fxsjy/jieba) 进行分词，并结合ngrams排序算法给出合理的搜索结果。

### 热门标签

[生成模型](https://kexue.fm/tag/%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B/) [attention](https://kexue.fm/tag/attention/) [优化](https://kexue.fm/tag/%E4%BC%98%E5%8C%96/) [语言模型](https://kexue.fm/tag/%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/) [模型](https://kexue.fm/tag/%E6%A8%A1%E5%9E%8B/) [梯度](https://kexue.fm/tag/%E6%A2%AF%E5%BA%A6/) [网站](https://kexue.fm/tag/%E7%BD%91%E7%AB%99/) [概率](https://kexue.fm/tag/%E6%A6%82%E7%8E%87/) [矩阵](https://kexue.fm/tag/%E7%9F%A9%E9%98%B5/) [优化器](https://kexue.fm/tag/%E4%BC%98%E5%8C%96%E5%99%A8/) [转载](https://kexue.fm/tag/%E8%BD%AC%E8%BD%BD/) [微分方程](https://kexue.fm/tag/%E5%BE%AE%E5%88%86%E6%96%B9%E7%A8%8B/) [分析](https://kexue.fm/tag/%E5%88%86%E6%9E%90/) [天象](https://kexue.fm/tag/%E5%A4%A9%E8%B1%A1/) [深度学习](https://kexue.fm/tag/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/) [积分](https://kexue.fm/tag/%E7%A7%AF%E5%88%86/) [python](https://kexue.fm/tag/python/) [扩散](https://kexue.fm/tag/%E6%89%A9%E6%95%A3/) [力学](https://kexue.fm/tag/%E5%8A%9B%E5%AD%A6/) [无监督](https://kexue.fm/tag/%E6%97%A0%E7%9B%91%E7%9D%A3/) [几何](https://kexue.fm/tag/%E5%87%A0%E4%BD%95/) [节日](https://kexue.fm/tag/%E8%8A%82%E6%97%A5/) [生活](https://kexue.fm/tag/%E7%94%9F%E6%B4%BB/) [文本生成](https://kexue.fm/tag/%E6%96%87%E6%9C%AC%E7%94%9F%E6%88%90/) [数论](https://kexue.fm/tag/%E6%95%B0%E8%AE%BA/)

### 随机文章

- [科学空间：2010年7月重要天象](https://kexue.fm/archives/704)
- [暑假结束了，上学去~](https://kexue.fm/archives/2056)
- [【NASA每日一图】撞击目标：凯布斯月球坑](https://kexue.fm/archives/178)
- [更别致的词向量模型(二)：对语言进行建模](https://kexue.fm/archives/4669)
- [“闭门造车”之多模态思路浅谈（三）：位置编码](https://kexue.fm/archives/10352)
- [开始学习数学软件Scilab](https://kexue.fm/archives/1720)
- [基于Amos优化器思想推导出来的一些“炼丹策略”](https://kexue.fm/archives/9344)
- [行动起来！共同应对全球气候变暖](https://kexue.fm/archives/107)
- [OCR技术浅探：7. 语言模型](https://kexue.fm/archives/3842)
- [将多项式分解为两个不可约多项式之和](https://kexue.fm/archives/3150)

### 最近评论

- [Bin](https://kexue.fm/archives/1990/comment-page-2#comment-29105): 今天偶然从某个论坛看到有人推荐您的博客，定睛一看竟然是华师同院的往届师兄！看到这篇2013年的...
- [Rapture D](https://kexue.fm/archives/11530/comment-page-1#comment-29104): 我有一个问题，为什么不考虑亥姆霍兹定理和斯托克斯公式。
- [mofheka](https://kexue.fm/archives/11390/comment-page-1#comment-29103): 苏神是还在用jax是么？最近在做基于Google Pathway的理念做一个动态版的MPMD框...
- [长琴](https://kexue.fm/archives/11530/comment-page-1#comment-29102): 看懂这篇博客也不是一件容易的事情。
- [AlexLi](https://kexue.fm/archives/9257/comment-page-4#comment-29101): 苏老师，请教一下(7)式中将 \\mu(x\_t) 传给 p\_o 进行推理的操作。 $x\_...
- [tyler\_zxc](https://kexue.fm/archives/7921/comment-page-2#comment-29100): "Performer的思想是将标准的Attention线性化，所以为什么不干脆直接训练一个线性...
- [我](https://kexue.fm/archives/11494/comment-page-1#comment-29099): 似乎并非mHC提出矩阵的思想？之前hyper connection就是了
- [winter](https://kexue.fm/archives/10847/comment-page-1#comment-29098): 苏神您好，假如对于比较均匀的attention weightP，往往呈现long tail分布...
- [苏剑林](https://kexue.fm/archives/8512/comment-page-2#comment-29097): KL散度、JS散度、W距离啥的，都行啊，看你喜欢哪个
- [苏剑林](https://kexue.fm/archives/9119/comment-page-14#comment-29096): 没有绝对公平的对比方法，主要看你关心什么。比如，如果只关心推理成本和推理效果，那么有的方法可以...

### 友情链接

- [Cool Papers](https://papers.cool/)
- [数学研发](https://bbs.emath.ac.cn/)
- [Seatop](http://www.seatop.com.cn/)
- [Xiaoxia](https://xiaoxia.org/)
- [积分表-网络版](https://kexue.fm/sci/integral/index.html)
- [丝路博傲](http://blog.dvxj.com/)
- [数学之家](http://www.2math.cn/)
- [有趣天文奇观](http://interesting-sky.china-vo.org/)
- [TwistedW](http://www.twistedwg.com/)
- [godweiyang](https://godweiyang.com/)
- [AI柠檬](https://blog.ailemon.net/)
- [王登科-DK博客](https://greatdk.com/)
- [ESON](https://blog.eson.org/)
- [枫之羽](https://fzhiy.net/)
- [coding-zuo](https://coding-zuo.github.io/)
- [博科园](https://www.bokeyuan.net/)
- [孔皮皮的博客](https://www.kppkkp.top/)
- [运鹏的博客](https://yunpengtai.top/)
- [jiming.site](https://jiming.site/)
- [OmegaXYZ](https://www.omegaxyz.com/)
- [EAI猩球](https://www.robotech.ink/)
- [文举的博客](https://liwenju0.com/)
- [申请链接](https://kexue.fm/links.html)

[![署名-非商业用途-保持一致](https://kexue.fm/usr/themes/geekg/images/cc.gif)](http://creativecommons.org/licenses/by-nc-nd/2.5/cn/) 本站采用创作共用版权协议，要求署名、非商业用途和保持一致。转载本站内容必须也遵循“ [署名-非商业用途-保持一致](http://creativecommons.org/licenses/by-nc-nd/2.5/cn/)”的创作共用协议。



© 2009-2026 Scientific Spaces. All rights reserved. Theme by [laogui](http://www.laogui.com/). Powered by [Typecho](http://typecho.org/). 备案号: [粤ICP备09093259号-1/2](https://beian.miit.gov.cn/ "粤ICP备09093259号")。