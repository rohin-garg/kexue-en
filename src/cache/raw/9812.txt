## SEARCH

## MENU

- [打赏](https://kexue.fm/reward.html)
- [公式](https://kexue.fm/latex.html)
- [天象](https://kexue.fm/ac.html)
- [链接](https://kexue.fm/links.html)
- [时光](https://kexue.fm/me.html)
- [博览](https://kexue.fm/science.html)
- [归档](https://kexue.fm/content.html)

## CATEGORIES

- [千奇百怪](https://kexue.fm/category/Everything)
- [天文探索](https://kexue.fm/category/Astronomy)
- [数学研究](https://kexue.fm/category/Mathematics)
- [物理化学](https://kexue.fm/category/Phy-chem)
- [信息时代](https://kexue.fm/category/Big-Data)
- [生物自然](https://kexue.fm/category/Biology)
- [图片摄影](https://kexue.fm/category/Photograph)
- [问题百科](https://kexue.fm/category/Questions)
- [生活/情感](https://kexue.fm/category/Life-Feeling)
- [资源共享](https://kexue.fm/category/Resources)

## NEWPOSTS

- [通过msign来计算奇异值裁剪mc...](https://kexue.fm/archives/11059)
- [矩阵符号函数mcsgn能计算什么？](https://kexue.fm/archives/11056)
- [线性注意力简史：从模仿、创新到反哺](https://kexue.fm/archives/11033)
- [msign的导数](https://kexue.fm/archives/11025)
- [通过msign来计算奇异值裁剪mc...](https://kexue.fm/archives/11006)
- [msign算子的Newton-Sc...](https://kexue.fm/archives/10996)
- [等值振荡定理：最优多项式逼近的充要条件](https://kexue.fm/archives/10972)
- [生成扩散模型漫谈（三十）：从瞬时速...](https://kexue.fm/archives/10958)
- [MoE环游记：5、均匀分布的反思](https://kexue.fm/archives/10945)
- [msign算子的Newton-Sc...](https://kexue.fm/archives/10922)

## COMMENTS

- [百万光年: 请问“迹技巧”的结论部分是否有笔误？应该是$\\boldsymb...](https://kexue.fm/archives/11025/comment-page-1#comment-27967)
- [wine: 这里我可以给一个角度，很简单，就是因为 $q\_{rope}$ ...](https://kexue.fm/archives/10907/comment-page-1#comment-27966)
- [zheng bo: 请问苏神，虽然我们辅助损失的目标分布都假设均匀分布，那么实际上...](https://kexue.fm/archives/10945/comment-page-1#comment-27965)
- [李润中: 我想问一下，这里提到attention的归一化，如果用了O N...](https://kexue.fm/archives/11033/comment-page-1#comment-27964)
- [行云流水0: 各路大神，有使用meanflow在工业级的复杂任务上做出效果的...](https://kexue.fm/archives/10958/comment-page-2#comment-27961)
- [无敌大铁锤: 苏神，如果去掉Causal Mask $M$,\
$\\exp(\\...](https://kexue.fm/archives/11033/comment-page-1#comment-27960)
- [Kuo: rmsnorm梯度对角线是I,各分量变化速度基本没有区别，而s...](https://kexue.fm/archives/10831/comment-page-1#comment-27959)
- [Kuo: rmsnorm的梯度形式看起来跟softmax一致，是不是有n...](https://kexue.fm/archives/10831/comment-page-1#comment-27958)
- [mona: 苏神，想请教一下，关于“推理阶段可以事先预估Routed Ex...](https://kexue.fm/archives/10945/comment-page-1#comment-27956)
- [石子131: 也许可以尝试把热水管的回水管的开关阀做成用户的手动阀，在热水管...](https://kexue.fm/archives/9405/comment-page-2#comment-27955)

## USERLOGIN

- [登录](https://kexue.fm/admin/login.php)

[科学空间\|Scientific Spaces](https://kexue.fm)

- [登录](https://kexue.fm/admin/login.php)
- [打赏](https://kexue.fm/reward.html)
- [公式](https://kexue.fm/latex.html)
- [天象](https://kexue.fm/ac.html)
- [链接](https://kexue.fm/links.html)
- [时光](https://kexue.fm/me.html)
- [博览](https://kexue.fm/science.html)
- [归档](https://kexue.fm/content.html)

渴望成为一个小飞侠

- [欢迎订阅](https://kexue.fm/feed)
- [个性邮箱](https://kexue.fm/archives/119)
- [天象信息](https://kexue.fm/ac.html)
- [观测ISS](https://kexue.fm/archives/41)
- [LaTeX](https://kexue.fm/latex.html)
- [关于博主](https://kexue.fm/me.html)

欢迎访问“科学空间”，这里将与您共同探讨自然科学，回味人生百态；也期待大家的分享～

- [**千奇百怪** Everything](https://kexue.fm/category/Everything)
- [**天文探索** Astronomy](https://kexue.fm/category/Astronomy)
- [**数学研究** Mathematics](https://kexue.fm/category/Mathematics)
- [**物理化学** Phy-chem](https://kexue.fm/category/Phy-chem)
- [**信息时代** Big-Data](https://kexue.fm/category/Big-Data)
- [**生物自然** Biology](https://kexue.fm/category/Biology)
- [**图片摄影** Photograph](https://kexue.fm/category/Photograph)
- [**问题百科** Questions](https://kexue.fm/category/Questions)
- [**生活/情感** Life-Feeling](https://kexue.fm/category/Life-Feeling)
- [**资源共享** Resources](https://kexue.fm/category/Resources)

- [**千奇百怪**](https://kexue.fm/category/Everything)
- [**天文探索**](https://kexue.fm/category/Astronomy)
- [**数学研究**](https://kexue.fm/category/Mathematics)
- [**物理化学**](https://kexue.fm/category/Phy-chem)
- [**信息时代**](https://kexue.fm/category/Big-Data)
- [**生物自然**](https://kexue.fm/category/Biology)
- [**图片摄影**](https://kexue.fm/category/Photograph)
- [**问题百科**](https://kexue.fm/category/Questions)
- [**生活/情感**](https://kexue.fm/category/Life-Feeling)
- [**资源共享**](https://kexue.fm/category/Resources)

[首页](https://kexue.fm) [数学研究](https://kexue.fm/category/Mathematics) [信息时代](https://kexue.fm/category/Big-Data) 从梯度最大化看Attention的Scale操作

22Oct

# [从梯度最大化看Attention的Scale操作](https://kexue.fm/archives/9812)

By 苏剑林 \|
2023-10-22 \|
91756位读者\|

我们知道， [Scaled Dot-Product Attention](https://kexue.fm/archives/4765) 的Scale因子是$\\frac{1}{\\sqrt{d}}$，其中$d$是$\\boldsymbol{q},\\boldsymbol{k}$的维度。这个Scale因子的一般解释是：如果不除以$\\sqrt{d}$，那么初始的Attention就会很接近one hot分布，这会造成梯度消失，导致模型训练不起来。然而，可以证明的是，当Scale等于0时同样也会有梯度消失问题，这也就是说Scale太大太小都不行。

那么多大的Scale才适合呢？$\\frac{1}{\\sqrt{d}}$是最佳的Scale了吗？本文试图从梯度角度来回答这个问题。

## 已有结果 [\#](https://kexue.fm/archives/9812\#%E5%B7%B2%E6%9C%89%E7%BB%93%E6%9E%9C)

在 [《浅谈Transformer的初始化、参数化与标准化》](https://kexue.fm/archives/8620#NTK%E5%8F%82%E6%95%B0%E5%8C%96) 中，我们已经推导过标准的Scale因子$\\frac{1}{\\sqrt{d}}$，推导的思路很简单，假设初始阶段$\\boldsymbol{q},\\boldsymbol{k}\\in\\mathbb{R}^d$都采样自“均值为0、方差为1”的分布，那么可以算得
\\begin{equation}\\mathbb{V}ar\[\\boldsymbol{q}\\cdot\\boldsymbol{k}\] = d\\end{equation}
于是我们将$\\boldsymbol{q}\\cdot\\boldsymbol{k}$除以$\\sqrt{d}$，将Attention Score的方差变为1。也就是说，之前的推导纯粹是基于“均值为0、方差为1”就会更好的 **信仰** 来得到的结果，但没有解释让Attention Score的方差为1，也没有评估$\\frac{1}{\\sqrt{d}}$是否真的就解决了梯度消失问题。

当然，从已有的实验来看，$\\frac{1}{\\sqrt{d}}$至少一定程度上是缓解了这个问题，但这毕竟是实验结果，我们还是希望能从理论上知道“一定程度”究竟是多少。

## 计算梯度 [\#](https://kexue.fm/archives/9812\#%E8%AE%A1%E7%AE%97%E6%A2%AF%E5%BA%A6)

既然涉及到了梯度，那么最好的办法就是把梯度算出来，然后定一个优化目标。设$p\_i = e^{\\alpha s\_i}/Z$，$i \\in \\{1,2,...,n\\}$，$Z=\\sum\_i e^{\\alpha s\_i}$是归一化因子，那么可以直接算得：
\\begin{equation}\\frac{\\partial p\_i}{\\partial s\_j} = \\left\\{\\begin{aligned}
\\alpha(p\_i - p\_i^2),&\\quad i=j\\\
-\\alpha p\_i p\_j,&\\quad i\\neq j
\\end{aligned}\\right.\\end{equation}
或者可以简写成$\\partial p\_i/\\partial s\_j = \\alpha(p\_i\\delta\_{i,j} - p\_i p\_j)$。很明显，当$\\alpha\\to 0$时梯度为0；当$\\alpha\\to\\infty$时，$p\_i$之中只有一个1、其余都是0（假设$s\_i$中只有唯一的最大值），梯度也是0。

为了更有利于优化，我们应该选取$\\alpha$使得梯度尽可能最大化。为此，我们以L1范数作为梯度大小的度量：
\\begin{equation}\\frac{1}{2}\\left\\Vert\\frac{\\partial p}{\\partial s}\\right\\Vert\_1=\\frac{1}{2}\\sum\_{i,j}\\left\|\\frac{\\partial p\_i}{\\partial s\_j}\\right\|=\\frac{1}{2}\\sum\_i \\alpha(p\_i - p\_i^2) + \\frac{1}{2}\\sum\_{i\\neq j} \\alpha p\_i p\_j = \\alpha\\left(1 - \\sum\_i p\_i^2\\right)\\label{eq:target}\\end{equation}
从最后的结果不难猜到，之所以选择L1而不是其他的根本原因是因为L1范数的计算结果足够简单。值得指出的是，这里出现了$\\sum\_i p\_i^2$，它本质上就是我们在 [《如何度量数据的稀疏程度？》](https://kexue.fm/archives/9595#%E7%86%B5%E7%9A%84%E8%81%94%E7%B3%BB) 介绍过的“Rényi熵”，跟信息熵类似，它也是不确定性的一种度量。

有了优化目标后，我们就可以着手进行最大化了。注意$p\_i$的定义里边也包含$\\alpha$，所以这是一个关于$\\alpha$复杂的非线性目标，看上去求解析解是不可能的，但我们可以针对一些特殊例子求近似解。

## 正态分布 [\#](https://kexue.fm/archives/9812\#%E6%AD%A3%E6%80%81%E5%88%86%E5%B8%83)

首先，我们可以接着前面的结果来做，当我们通过除以$\\sqrt{d}$使得Attention Score的均值为0、方差为1后，我们就可以近似假设$s\_i\\sim\\mathcal{N}(0,1)$，然后再求$\\alpha$的最优解，如果$\\alpha=1$，那么就意味着原来的$\\frac{1}{\\sqrt{d}}$就是最优的Scale比例了，否则$\\frac{\\alpha}{\\sqrt{d}}$才是最佳的Scale比例。

我们用期望去估计求和
\\begin{equation}\\sum\_i p\_i^2 = \\frac{\\sum\_i e^{2\\alpha s\_i}}{\\left(\\sum\_i e^{\\alpha s\_i}\\right)^2} = \\frac{\\frac{1}{n}\\sum\_i e^{2\\alpha s\_i}}{n\\left(\\frac{1}{n}\\sum\_i e^{\\alpha s\_i}\\right)^2} \\approx \\frac{\\mathbb{E}\_s\[e^{2\\alpha s}\]}{n\\left(\\mathbb{E}\_s\[e^{\\alpha s}\]\\right)^2}\\label{eq:approx}\\end{equation}
对于服从标准正态分布的$s$，我们有
\\begin{equation}\\mathbb{E}\_s\[e^{\\alpha s}\] = \\int \\frac{1}{\\sqrt{2\\pi}}e^{-s^2/2}e^{\\alpha s} ds = e^{\\alpha^2 / 2}\\label{eq:normal}\\end{equation}
代入上式，然后代入式$\\eqref{eq:target}$，得到
\\begin{equation}\\alpha\\left(1 - \\sum\_i p\_i^2\\right)\\approx\\alpha\\left(1 - \\frac{e^{\\alpha^2}}{n}\\right)\\end{equation}
最后的近似，虽然已经足够简化了，但其实也不容易求出最大值来。不过无妨，我们可以遍历一些$n$，然后数值求解出取最大值时的$\\alpha^\*$，这样我们就大致能看到$\\alpha^\*$与$n$的关系了，Mathematica的参考代码如下：

```
(*定义函数*)
f[a_, n_] := a*(1 - Exp[a^2]/n)
(*找到函数的最大点对应的a*)
FindArg[n_] :=
 Module[{a}, a = a /. Last@NMaximize[{f[a, n], a > 0}, a][[2]]; a]
(*给定n的范围*)
nRange = 40*Range[1, 500];
(*求出每个n对应的a*)
args = FindArg /@ nRange;
(*画出a与n的函数图像*)
ListLinePlot[{args, 0.84*Log[nRange]^0.5},
 DataRange -> {40, 20000}, AxesLabel -> {"n", "a"},
 PlotLegends -> {Row[{"a", Superscript["", "*"]}],
 TraditionalForm[HoldForm[0.84*Sqrt[Log[n]]]]}]
```

经过拟合，笔者发现一定范围内最优点$\\alpha^\*$与$n$大致满足$\\alpha\\approx 0.84\\sqrt{\\log n}$的关系，所以也已经将对应的近似函数一并画在一起：

标准正态分布的最优alpha与n关系

可以看到，在相当大的一个范围内，$\\alpha^\*$的最优值都在$2\\sim 3$之间，所以折中一下的话，盲取$\\frac{2.5}{\\sqrt{d}}$作为Attention的Scale因子理论上更有利于优化。

## 余弦分布 [\#](https://kexue.fm/archives/9812\#%E4%BD%99%E5%BC%A6%E5%88%86%E5%B8%83)

现在我们考虑另一个不那么常见的例子：当我们对$\\boldsymbol{q},\\boldsymbol{k}$都做$l\_2$归一化变成单位向量后，它们的内积就变成了夹角余弦，即$s\_i$近似服从$d$维空间中的两个随机向量的夹角余弦分布。这个分布可能有些读者并不熟悉，但之前我们在 [《n维空间下两个随机向量的夹角分布》](https://kexue.fm/archives/7076) 已经探讨过，它的概率密度具有形式
\\begin{equation}p(s)\\propto (1-s^2)^{(d-3)/2}\\end{equation}

看上去并不复杂，但事实上这个形式比正态分布难处理得多，主要是$\\mathbb{E}\_s\[e^{\\alpha s}\]$已经不像式$\\eqref{eq:normal}$那样可以用初等函数表达出来了，不过对于Mathematica数值求解来说问题不大。跟上一节同样的思路，近似式$\\eqref{eq:approx}$也同样适用，先数值求解最大值，然后再拟合，结果如下（图中$d=128$，$\\alpha^\*$跟$d$相关）：

余弦分布的最优alpha与n关系

可以看到，$\\alpha^\*$与$3.5\\log n$拟合得也不错（换一个$d$的话，$3.5$这个系数会变化）。可以看到，在一个相当大的范围内，$\\alpha^\*$都是$25\\sim 35$之间，所以如果用$\\cos$值作为Attention Score的话，就需要乘以一个$25\\sim 35$之间的Scale，才能使得模型比较容易训下去。这同时也解释了为什么我们在用$\\cos$值构建Softmax分布（比如 [AM-Softmax](https://kexue.fm/archives/5743#am-softmax)、 [SimCSE](https://kexue.fm/archives/8348) 等）时，需要在$\\cos$之后乘上一个30左右的Scale了，因为不乘是很难训得动模型的。

对于不同的$d$和$n$，读者可以自行修改下面的代码计算最优$\\alpha$：

```
(*定义函数*)
h[a_] :=
 Integrate[Exp[a*s]*(1 - s^2)^((d - 3)/2), {s, -1, 1},
 Assumptions -> {d > 10}]
g[a_] = h[a]/h[0] // FullSimplify;
f[a_, n_] := a (1 - g[2*a]/g[a]^2/n) /. {d -> 128}
(*找到函数的最大点对应的a*)
FindArg[n_] :=
 Module[{a}, a = a /. Last@NMaximize[{f[a, n], a > 0}, a][[2]]; a]
(*给定n的范围*)
nRange = 40*Range[1, 500];
(*求出每个n对应的a*)
args = FindArg /@ nRange;
(*画出a与n的函数图像*)
ListLinePlot[{args, 3.5*Log[nRange]},
 DataRange -> {40, 20000}, AxesLabel -> {"n", "a"},
 PlotLegends -> {Row[{"a", Superscript["", "*"]}],
 TraditionalForm[HoldForm[3.5*Log[n]]]}]
```

## 相关思考 [\#](https://kexue.fm/archives/9812\#%E7%9B%B8%E5%85%B3%E6%80%9D%E8%80%83)

本文的标题和结果，尤其是余弦分布中$\\alpha$近似正比于$\\log n$的结果，很容易让我们联想到另一篇讨论Attention Scale的文章 [《从熵不变性看Attention的Scale操作》](https://kexue.fm/archives/8823)。事实上，两篇文章的联系确实存在，本文的优化目标$\\eqref{eq:target}$出现了“Rényi熵”，而“熵不变性”的熵指的是香侬信息熵，两者的性质很大程度上是一致的。最大化式$\\eqref{eq:target}$使得它进入了一个“缓变”的区域，这意味着“Rényi熵”关于$n$的变化是很慢的，也意味着信息熵关于$n$的变化是很慢的，这就约等于熵不变性。

此外，对于双向Attention（Encoder）来说，假设训练样本长度相同，那么$n$就是一个常数，我们可以根据$n$算得相应的最优$\\alpha$，然后固定在模型中即可；但是对于单向Attention（Decoder）来说，每个token的$n$实际上都不一样（位置id加1），所以理论上无法做到对所有token都最大化式$\\eqref{eq:target}$，不过由于$\\alpha^\*$关于$n$的变化较慢，所以取一个差不多的值就行了，比如可以取$n=L\_{\\max} / 2$，这样对大部分token的梯度都比较友好了。

## 文章小结 [\#](https://kexue.fm/archives/9812\#%E6%96%87%E7%AB%A0%E5%B0%8F%E7%BB%93)

本文从梯度的角度探讨了Attention Scale因子的选择问题。众所周知，关于这个Scale因子的“标准答案”是$\\frac{1}{\\sqrt{d}}$，但其推导过程中并没有讨论到它的最优性问题，所以笔者定义了一个Softmax梯度的优化目标，从最大化该目标的角度探讨了Scale因子的最优值。相关结果既可以用来改进Attention的Scale因子，也可以用来解释$\\cos$相似度的对比学习的温度参数。

_**转载到请包括本文地址：** [https://kexue.fm/archives/9812](https://kexue.fm/archives/9812)_

_**更详细的转载事宜请参考：**_ [《科学空间FAQ》](https://kexue.fm/archives/6508#%E6%96%87%E7%AB%A0%E5%A6%82%E4%BD%95%E8%BD%AC%E8%BD%BD/%E5%BC%95%E7%94%A8)

**如果您还有什么疑惑或建议，欢迎在下方评论区继续讨论。**

**如果您觉得本文还不错，欢迎 [分享](https://kexue.fm/archives/9812#share)/ [打赏](https://kexue.fm/archives/9812#pay) 本文。打赏并非要从中获得收益，而是希望知道科学空间获得了多少读者的真心关注。当然，如果你无视它，也不会影响你的阅读。再次表示欢迎和感谢！**

打赏

微信打赏

支付宝打赏

因为网站后台对打赏并无记录，因此欢迎在打赏时候备注留言。你还可以 [**点击这里**](http://mail.qq.com/cgi-bin/qm_share?t=qm_mailme&email=tN7d1drY3drrx8H0xcWa19vZ) 或在下方评论区留言来告知你的建议或需求。

**如果您需要引用本文，请参考：**

苏剑林. (Oct. 22, 2023). 《从梯度最大化看Attention的Scale操作 》\[Blog post\]. Retrieved from [https://kexue.fm/archives/9812](https://kexue.fm/archives/9812)

@online{kexuefm-9812,
        title={从梯度最大化看Attention的Scale操作},
        author={苏剑林},
        year={2023},
        month={Oct},
        url={\\url{https://kexue.fm/archives/9812}},
}

分类： [数学研究](https://kexue.fm/category/Mathematics), [信息时代](https://kexue.fm/category/Big-Data)    标签： [优化](https://kexue.fm/tag/%E4%BC%98%E5%8C%96/), [梯度](https://kexue.fm/tag/%E6%A2%AF%E5%BA%A6/), [attention](https://kexue.fm/tag/attention/)[30 评论](https://kexue.fm/archives/9812#comments)

< [随机分词再探：从Viterbi Sampling到完美采样算法](https://kexue.fm/archives/9811) \| [简单得令人尴尬的FSQ：“四舍五入”超越了VQ-VAE](https://kexue.fm/archives/9826) >

### 你也许还对下面的内容感兴趣

- [线性注意力简史：从模仿、创新到反哺](https://kexue.fm/archives/11033)
- [msign的导数](https://kexue.fm/archives/11025)
- [MoE环游记：5、均匀分布的反思](https://kexue.fm/archives/10945)
- [Transformer升级之路：20、MLA究竟好在哪里？](https://kexue.fm/archives/10907)
- [SVD的导数](https://kexue.fm/archives/10878)
- [Transformer升级之路：19、第二类旋转位置编码](https://kexue.fm/archives/10862)
- [通过梯度近似寻找Normalization的替代品](https://kexue.fm/archives/10831)
- [MoE环游记：4、难处应当多投入](https://kexue.fm/archives/10815)
- [高阶muP：更简明但更高明的谱条件缩放](https://kexue.fm/archives/10795)
- [初探muP：超参数的跨模型尺度迁移规律](https://kexue.fm/archives/10770)

[发表你的看法](https://kexue.fm/archives/9812#comment_form)

1. [«](https://kexue.fm/archives/9812/comment-page-1#comments)
2. [1](https://kexue.fm/archives/9812/comment-page-1#comments)
3. [2](https://kexue.fm/archives/9812/comment-page-2#comments)

aic

January 10th, 2025

公式3里是不是应该减号

[回复评论](https://kexue.fm/archives/9812/comment-page-2?replyTo=26222#respond-post-9812)

aic 发表于
January 10th, 2025

哦，没错

[回复评论](https://kexue.fm/archives/9812/comment-page-2?replyTo=26223#respond-post-9812)

1. [«](https://kexue.fm/archives/9812/comment-page-1#comments)
2. [1](https://kexue.fm/archives/9812/comment-page-1#comments)
3. [2](https://kexue.fm/archives/9812/comment-page-2#comments)

[取消回复](https://kexue.fm/archives/9812#respond-post-9812)

你的大名

电子邮箱

个人网站（选填）

1\. 可以使用LaTeX代码，点击“预览效果”可查看效果；2. 可以通过点击评论楼层编号来引用该楼层；3. 网站可能会有点卡，如非确认评论失败，请 **不要重复点击提交**。

### 内容速览

[已有结果](https://kexue.fm/archives/9812#%E5%B7%B2%E6%9C%89%E7%BB%93%E6%9E%9C)
[计算梯度](https://kexue.fm/archives/9812#%E8%AE%A1%E7%AE%97%E6%A2%AF%E5%BA%A6)
[正态分布](https://kexue.fm/archives/9812#%E6%AD%A3%E6%80%81%E5%88%86%E5%B8%83)
[余弦分布](https://kexue.fm/archives/9812#%E4%BD%99%E5%BC%A6%E5%88%86%E5%B8%83)
[相关思考](https://kexue.fm/archives/9812#%E7%9B%B8%E5%85%B3%E6%80%9D%E8%80%83)
[文章小结](https://kexue.fm/archives/9812#%E6%96%87%E7%AB%A0%E5%B0%8F%E7%BB%93)

### 智能搜索

支持整句搜索！网站自动使用 [结巴分词](https://github.com/fxsjy/jieba) 进行分词，并结合ngrams排序算法给出合理的搜索结果。

### 热门标签

[生成模型](https://kexue.fm/tag/%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B/) [attention](https://kexue.fm/tag/attention/) [优化](https://kexue.fm/tag/%E4%BC%98%E5%8C%96/) [语言模型](https://kexue.fm/tag/%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/) [模型](https://kexue.fm/tag/%E6%A8%A1%E5%9E%8B/) [网站](https://kexue.fm/tag/%E7%BD%91%E7%AB%99/) [概率](https://kexue.fm/tag/%E6%A6%82%E7%8E%87/) [梯度](https://kexue.fm/tag/%E6%A2%AF%E5%BA%A6/) [转载](https://kexue.fm/tag/%E8%BD%AC%E8%BD%BD/) [微分方程](https://kexue.fm/tag/%E5%BE%AE%E5%88%86%E6%96%B9%E7%A8%8B/) [矩阵](https://kexue.fm/tag/%E7%9F%A9%E9%98%B5/) [天象](https://kexue.fm/tag/%E5%A4%A9%E8%B1%A1/) [分析](https://kexue.fm/tag/%E5%88%86%E6%9E%90/) [深度学习](https://kexue.fm/tag/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/) [积分](https://kexue.fm/tag/%E7%A7%AF%E5%88%86/) [python](https://kexue.fm/tag/python/) [优化器](https://kexue.fm/tag/%E4%BC%98%E5%8C%96%E5%99%A8/) [力学](https://kexue.fm/tag/%E5%8A%9B%E5%AD%A6/) [无监督](https://kexue.fm/tag/%E6%97%A0%E7%9B%91%E7%9D%A3/) [扩散](https://kexue.fm/tag/%E6%89%A9%E6%95%A3/) [几何](https://kexue.fm/tag/%E5%87%A0%E4%BD%95/) [节日](https://kexue.fm/tag/%E8%8A%82%E6%97%A5/) [生活](https://kexue.fm/tag/%E7%94%9F%E6%B4%BB/) [文本生成](https://kexue.fm/tag/%E6%96%87%E6%9C%AC%E7%94%9F%E6%88%90/) [数论](https://kexue.fm/tag/%E6%95%B0%E8%AE%BA/)

### 随机文章

- [力的无穷分解与格林函数法](https://kexue.fm/archives/3092)
- [2016年全年天象](https://kexue.fm/archives/4171)
- [你的语言模型有没有“无法预测的词”？](https://kexue.fm/archives/9046)
- [太空中的巨影——日食间的月球影子](https://kexue.fm/archives/32)
- [为什么是抛物线？——聚光面研究](https://kexue.fm/archives/1055)
- [庆祝圆周率(π)节！](https://kexue.fm/archives/524)
- [修改了一下公式的显示方式（移动端）](https://kexue.fm/archives/3576)
- [提速不掉点：基于词颗粒度的中文WoBERT](https://kexue.fm/archives/7758)
- [进驻中山大学南校区，折腾校园网](https://kexue.fm/archives/3936)
- [校外通过VPN通道访问华师资源](https://kexue.fm/archives/1886)

### 最近评论

- [百万光年](https://kexue.fm/archives/11025/comment-page-1#comment-27967): 请问“迹技巧”的结论部分是否有笔误？应该是$\\boldsymbol{G} = \\nabla\_{...
- [wine](https://kexue.fm/archives/10907/comment-page-1#comment-27966): 这里我可以给一个角度，很简单，就是因为 $q\_{rope}$ 是多头的，相当于需要输出的维度更...
- [zheng bo](https://kexue.fm/archives/10945/comment-page-1#comment-27965): 请问苏神，虽然我们辅助损失的目标分布都假设均匀分布，那么实际上训练算法得到的MOE在推理时，其...
- [李润中](https://kexue.fm/archives/11033/comment-page-1#comment-27964): 我想问一下，这里提到attention的归一化，如果用了O Norm的话，是可以去掉的，请问在...
- [行云流水0](https://kexue.fm/archives/10958/comment-page-2#comment-27961): 各路大神，有使用meanflow在工业级的复杂任务上做出效果的吗？是在什么样的场景中做的？模型...
- [无敌大铁锤](https://kexue.fm/archives/11033/comment-page-1#comment-27960): 苏神，如果去掉Causal Mask $M$,
$\\exp(\\boldsymbol{Q}\\bo...
- [Kuo](https://kexue.fm/archives/10831/comment-page-1#comment-27959): rmsnorm梯度对角线是I,各分量变化速度基本没有区别，而softmax对角线是当下权重分量...
- [Kuo](https://kexue.fm/archives/10831/comment-page-1#comment-27958): rmsnorm的梯度形式看起来跟softmax一致，是不是有normalization的都这样？
- [mona](https://kexue.fm/archives/10945/comment-page-1#comment-27956): 苏神，想请教一下，关于“推理阶段可以事先预估Routed Expert的实际分布，只要细致地进...
- [石子131](https://kexue.fm/archives/9405/comment-page-2#comment-27955): 也许可以尝试把热水管的回水管的开关阀做成用户的手动阀，在热水管临近回水管、手动阀靠近热水管侧加...

### 友情链接

- [Cool Papers](https://papers.cool)
- [数学研发](https://bbs.emath.ac.cn)
- [Seatop](http://www.seatop.com.cn/)
- [Xiaoxia](https://xiaoxia.org/)
- [积分表-网络版](https://kexue.fm/sci/integral/index.html)
- [丝路博傲](http://blog.dvxj.com/)
- [ph4ntasy 饭特稀](http://www.ph4ntasy.com/)
- [数学之家](http://www.2math.cn/)
- [有趣天文奇观](http://interesting-sky.china-vo.org/)
- [TwistedW](http://www.twistedwg.com/)
- [godweiyang](https://godweiyang.com/)
- [AI柠檬](https://blog.ailemon.net/)
- [王登科-DK博客](https://greatdk.com)
- [ESON](https://blog.eson.org/)
- [枫之羽](https://fzhiy.net/)
- [Mathor's blog](https://wmathor.com/)
- [coding-zuo](https://coding-zuo.github.io/)
- [博科园](https://www.bokeyuan.net/)
- [孔皮皮的博客](https://www.kppkkp.top/)
- [运鹏的博客](https://yunpengtai.top/)
- [jiming.site](https://jiming.site/)
- [OmegaXYZ](https://www.omegaxyz.com/)
- [Blog by Eacls](https://www.eacls.top/)
- [EAI猩球](https://www.robotech.ink/)
- [文举的博客](https://liwenju0.com/)
- [用代码打点酱油](https://bruceyuan.com/)
- [申请链接](https://kexue.fm/links.html)

本站采用创作共用版权协议，要求署名、非商业用途和保持一致。转载本站内容必须也遵循“ [署名-非商业用途-保持一致](http://creativecommons.org/licenses/by-nc-nd/2.5/cn/)”的创作共用协议。
© 2009-2025 Scientific Spaces. All rights reserved. Theme by [laogui](http://www.laogui.com). Powered by [Typecho](http://typecho.org). 备案号: [粤ICP备09093259号-1/2](https://beian.miit.gov.cn/)。