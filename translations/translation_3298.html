
    <style type="text/css">

    body {
    margin: 48px auto;
    max-width: 68ch;              /* character-based width reads better */
    padding: 0 16px;

    font-family: -apple-system, BlinkMacSystemFont, "Segoe UI",
                Roboto, "Helvetica Neue", Arial, sans-serif;
    font-size: 18px;
    line-height: 1.65;
    color: #333;
    background: #fafafa;
    }

    h1, h2, h3, h4 {
    line-height: 1.25;
    margin-top: 2.2em;
    margin-bottom: 0.6em;
    font-weight: 600;
    }

    h1 {
    font-size: 2.1em;
    margin-top: 0;
    }

    h2 {
    font-size: 1.6em;
    border-bottom: 1px solid #e5e5e5;
    padding-bottom: 0.3em;
    }

    h3 {
    font-size: 1.25em;
    }

    h4 {
    font-size: 1.05em;
    color: #555;
    }

    /* Paragraphs and lists */
    p {
    margin: 1em 0;
    }

    ul, ol {
    margin: 1em 0 1em 1.5em;
    }

    li {
    margin: 0.4em 0;
    }

    a {
    color: #005fcc;
    text-decoration: none;
    }

    a:hover {
    text-decoration: underline;
    }

    code {
    font-family: ui-monospace, SFMono-Regular, Menlo, Consolas, monospace;
    font-size: 0.95em;
    background: #f2f2f2;
    padding: 0.15em 0.35em;
    border-radius: 4px;
    }

    pre {
    font-family: ui-monospace, SFMono-Regular, Menlo, Consolas, monospace;
    font-size: 0.9em;
    background: #f5f5f5;
    padding: 1em 1.2em;
    overflow-x: auto;
    border-radius: 6px;
    line-height: 1.45;
    }

    pre code {
    background: none;
    padding: 0;
    }

    blockquote {
    margin: 1.5em 0;
    padding-left: 1em;
    border-left: 4px solid #ddd;
    color: #555;
    }

    hr {
    border: none;
    border-top: 1px solid #e0e0e0;
    margin: 3em 0;
    }

    table {
    border-collapse: collapse;
    margin: 1.5em 0;
    width: 100%;
    font-size: 0.95em;
    }

    th, td {
    padding: 0.5em 0.7em;
    border-bottom: 1px solid #e5e5e5;
    text-align: left;
    }

    th {
    font-weight: 600;
    }

    img {
    max-width: 100%;
    display: block;
    margin: 1.5em auto;
    }

    small {
    color: #666;
    }

    mjx-container {
    margin: 1em 0;
    }

    ::selection {
    background: #cce2ff;
    }

    </style>
    

<script>
  window.MathJax = {
    tex: {
      inlineMath: [['$', '$'], ['\\(', '\\)']],
      displayMath: [['$$', '$$'], ['\\[', '\\]']],
      processEscapes: true,
      tags: 'ams'
    },
    options: {
      renderActions: {
        findScript: [10, function (doc) {
          for (const node of document.querySelectorAll('script[type^="math/tex"]')) {
            const display = !!node.type.match(/; *mode=display/);
            const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
            const text = document.createTextNode('');
            node.parentNode.replaceChild(text, node);
            math.start = {node: text, delim: '', n: 0};
            math.end = {node: text, delim: '', n: 0};
            doc.math.push(math);
          }
        }, '']
      }
    }
  };
</script>
<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

<article>
    <h1><a href="https://kexue.fm/archives/3298">Recording a Process of Crawling Taobao/Tmall Review Data</a></h1>
    <p>By 苏剑林 | May 06, 2015</p>

    <p>Recently, I have become fascinated with data mining and machine learning. To perform data analysis, one must first have data. For commoners like us, the cheapest way to acquire data is likely using a crawler to scrape it from the web. This article records the entire process of crawling a certain product on Tmall. The approach for Taobao stores is similar, so I will not elaborate further. <strong>The focus is on analyzing the page and implementing a simple and convenient crawl using Python.</strong></p>

    <p>The tools I used are as follows:</p>

    <blockquote>
        <p><strong>Python 3</strong>—An extremely convenient programming language. Version 3.x was chosen because it handles Chinese characters more gracefully.</p>
        <p><strong>Pandas</strong>—An additional Python library used for data organization.</p>
        <p><strong>IE 11</strong>—Used to analyze the page request process (any other similar traffic monitoring tool will also work).</p>
        <p>The remaining libraries include <strong>requests</strong> and <strong>re</strong>, which come with Python or are easily installed.</p>
    </blockquote>

    <p>Example page (a Midea water heater): <a href="http://detail.tmall.com/item.htm?id=41464129793">http://detail.tmall.com/item.htm?id=41464129793</a></p>

    <h3 id="评论在哪里？">Where are the reviews? <a href="https://kexue.fm/archives/3298#%E8%AF%84%E8%AE%BA%E5%9C%A8%E5%93%AA%E9%87%8C%EF%BC%9F">#</a></h3>

    <p>To crawl review data, you must first find where the reviews actually reside. Open the URL above and view the source code; you will find that there is no review content inside! So, where is the review data? It turns out Tmall uses AJAX encryption; it loads review data from a separate page.</p>

    <p>This is where IE 11 comes in handy (of course, you can use other traffic monitoring tools). Before using it, open the URL above. Once the page is loaded, clear the IE 11 cache and history files, then press <strong>F12</strong>. The following interface will appear:</p>

    <p><img src="https://kexue.fm/usr/uploads/2015/05/1857850220.png" alt="F12"></p>

    <p>At this point, click the green triangle button to start capturing network traffic (or simply press F5), and then click "Cumulative Reviews" on the Tmall page:</p>

    <p><img src="https://kexue.fm/usr/uploads/2015/05/182650059.png" alt="Capture"></p>

    <p>The following results appear:</p>

    <p><img src="https://kexue.fm/usr/uploads/2015/05/2767078330.png" alt="Capture results"></p>

    <p>Many URLs appear under the URL column, and the review data is hidden among them! We mainly look for URLs with the type "text/html" or "application/json". After testing, it was found that Tmall's reviews are contained within the following URL:</p>

    <blockquote>
        <p>http://rate.tmall.com/list_detail_rate.htm?itemId=41464129793&spuId=296980116&sellerId=1652490016&order=3&currentPage=1&append=0&content=1&tagId=&posi=&picture=&ua=166UW5TcyMNYQwiAiwVQX1EeUR5RH5Cd0xiNGI%3D%7CUm5Ockt1SHxBe0B0SXNOdCI%3D%7CU2xMHDJxPk82UjVOI1h2VngRd1snQSJEI107F2gFfgRlAmRKakQYeR9zFGoQPmg%2B%7CVGhXd1llXGJfa1ZsV2NeZFljVGlLdUt2TXFOc0tyT3pHe0Z6QHlXAQ%3D%3D%7CVWldfS0SMgo3FysUNBonHyMdNwI4HStHNkVrPWs%3D%7CVmhIGCIWNgsrFykQJAQ6DzQAIBwiGSICOAM2FioULxQ0DjEEUgQ%3D%7CV25OHjAePgA0DCwQKRYsDDgHPAdRBw%3D%3D%7CWGFBET8RMQ04ACAcJR0iAjYDNwtdCw%3D%3D%7CWWBAED5%2BKmIZcBZ6MUwxSmREfUl2VmpSbVR0SHVLcU4YTg%3D%3D%7CWmFBET9aIgwsECoKNxcrFysSL3kv%7CW2BAED5bIw0tESQEOBgkGCEfI3Uj%7CXGVFFTsVNQw2AiIeJxMoCDQIMwg9az0%3D%7CXWZGFjhdJQsrECgINhYqFiwRL3kv%7CXmdHFzkXNws3DS0RLxciAj4BPAY%2BaD4%3D%7CX2ZGFjgWNgo1ASEdIxsjAz8ANQE1YzU%3D%7CQHtbCyVAOBY2Aj4eIwM%2FAToONGI0%7CQXhYCCYIKBMqFzcLMwY%2FHyMdKRItey0%3D%7CQntbCyULKxQgGDgEPQg8HCAZIxoveS8%3D%7CQ3paCiQKKhYoFDQIMggwEC8SJh8idCI%3D%7CRH1dDSMNLRIrFTUJMw82FikWKxUueC4%3D%7CRX5eDiAOLhItEzMOLhIuFy4VKH4o%7CRn5eDiAOLn5GeEdnW2VeYjQUKQknCSkQKRIrFyN1Iw%3D%3D%7CR35Dfl5jQ3xcYFllRXtDeVlgQHxBYVV1QGBfZUV6QWFZeUZ%2FX2FBfl5hXX1AYEF9XXxDY0J8XGBbe0IU&isg=B2E8ACFC7C2F2CB185668041148A7DAA&_ksTS=1430908138129_1993&callback=jsonp1994</p>
    </blockquote>

    <p>Does it look long enough to make you dizzy? Don't worry, with a little analysis, you'll find it can be boiled down to the following:</p>

    <blockquote>
        <p>http://rate.tmall.com/list_detail_rate.htm?itemId=41464129793&sellerId=1652490016&currentPage=1</p>
    </blockquote>

    <p>We find that Tmall is quite generous; the review page addresses are very regular (unlike JD, which is completely irregular and randomly generated). In this URL, <code>itemId</code> is the product ID, <code>sellerId</code> is the seller ID, and <code>currentPage</code> is the page number.</p>

    <h3 id="怎么爬取？">How to crawl? <a href="https://kexue.fm/archives/3298#%E6%80%8E%E4%B9%88%E7%88%AC%E5%8F%96%EF%BC%9F">#</a></h3>

    <p>After some effort, we finally found where the reviews are. Next is the crawling. How to crawl? First, analyze the page format.</p>

    <p><img src="https://kexue.fm/usr/uploads/2015/05/1815330310.png" alt="Page format"></p>

    <p>We see that the page data is very standardized. In fact, it is a lightweight data exchange format called JSON (you can search for JSON). However, it isn't strictly standard JSON; actually, the content inside the square brackets <code>[]</code> is the valid JSON-compliant text.</p>

    <p>Now let's start our crawl. I use the <code>requests</code> library in Python. Enter the following in Python:</p>

<pre><code>import requests as rq
url = 'http://rate.tmall.com/list_detail_rate.htm?itemId=41464129793&sellerId=1652490016&currentPage=1'
myweb = rq.get(url)</code></pre>

    <p>The content of the page is now saved in the <code>myweb</code> variable. You can view the text content using <code>myweb.text</code>.</p>

    <p>The next step is to keep only the part inside the brackets. This requires regular expressions, using the <code>re</code> module.</p>

<pre><code>import re
myjson = re.findall('\"rateList\":(\[.*?\])\,\"tags\"', myweb.text)[0]</code></pre>

    <p>Uh, what does this line of code mean? Readers familiar with Python can probably understand it. If not, please read a tutorial on regular expressions. The code above means: find the following tags in the text:</p>

    <blockquote>
        "rateList":[...],"tags"
    </blockquote>

    <p>Once found, keep the brackets and the content inside them. Why not just use the brackets as the tag? It is to prevent errors in case a user's comment contains brackets.</p>

    <p>Now we have <code>myjson</code>, which is standard JSON text. How to read JSON? It's simple—just use Pandas. This is a powerful data analysis tool in Python that can read JSON directly. Of course, if it were only for reading JSON, there would be no need for it; however, we also need to consider merging the review data from each page into a single table and perform preprocessing. Pandas is extremely convenient for this.</p>

<pre><code>import pandas as pd
mytable = pd.read_json(myjson)</code></pre>

    <p>Now <code>mytable</code> is a standardized Pandas DataFrame:</p>

    <p><img src="https://kexue.fm/usr/uploads/2015/05/2034938637.png" alt="mytable1"></p>
    <p><img src="https://kexue.fm/usr/uploads/2015/05/444778156.png" alt="mytable2"></p>

    <p>If you have two tables, <code>mytable1</code> and <code>mytable2</code>, that need to be merged, you simply do:</p>

<pre><code>pd.concat([mytable1, mytable2], ignore_index=True)</code></pre>

    <p>And so on. For more operations, please refer to Pandas tutorials.</p>

    <p>Finally, save the reviews as a .txt or Excel file (due to Chinese encoding issues, saving as .txt might lead to errors, so saving as Excel is a good alternative; Pandas can also read/write Excel files):</p>

<pre><code>mytable.to_csv('mytable.txt')
mytable.to_excel('mytable.xls')</code></pre>

    <h3 id="一点点结论">A few conclusions <a href="https://kexue.fm/archives/3298#%E1%B8%80%E7%82%B9%E7%82%B9%E7%BB%93%E8%AE%BA">#</a></h3>

    <p>Let's see how many lines of code we used in total?</p>

<pre><code>import requests as rq
import re
import pandas as pd
url = 'http://rate.tmall.com/list_detail_rate.htm?itemId=41464129793&sellerId=1652490016&currentPage=1'
myweb = rq.get(url)
myjson = re.findall('\"rateList\":(\[.*?\])\,\"tags\"', myweb.text)[0]
mytable = pd.read_json(myjson)
mytable.to_csv('mytable.txt')
mytable.to_excel('mytable.xls')</code></pre>

    <p><strong>Nine lines! In fewer than ten lines, we completed a simple crawler program and successfully scraped data from Tmall! Are you eager to try it out?</strong></p>

    <p>Of course, this is just a simple example. For practical use, some features should be added, such as finding out how many pages of reviews there are in total and reading them page by page. Additionally, batch acquisition of product IDs should be implemented. These are left to your own creativity; they are not difficult problems. This article merely hopes to serve as a starting point, providing a simple guide for readers who need to crawl data.</p>

    <p>The most difficult problem among these is likely that after mass collection, you might be detected by Tmall's own anti-crawling system, which may require you to enter a captcha to continue. This is much more complex. Solutions include using proxies, using longer intervals between crawls, or using OCR systems to recognize captchas. I do not have a perfect solution for this either.</p>

    <p><em><strong>When reposting, please include the original address of this article:</strong> <a href="translation_3298.html">https://kexue.fm/archives/3298</a></em></p>
</article>
<hr>
<footer style="margin-top: 3em; padding: 1.5em; background: #f5f5f5; border-radius: 8px; font-size: 0.9em; color: #555;">
    <p style="margin: 0 0 0.5em 0;"><strong>Citation</strong></p>
    <p style="margin: 0 0 0.5em 0;">
        This is a machine translation of the original Chinese article:<br>
        <a href="translation_3298.html" style="color: #005fcc;">https://kexue.fm/archives/3298</a>
    </p>
    <p style="margin: 0 0 0.5em 0;">
        Original author: 苏剑林 (Su Jianlin)<br>
        Original publication: <a href="https://kexue.fm" style="color: #005fcc;">科学空间 (Scientific Spaces)</a>
    </p>
    <p style="margin: 0; font-style: italic;">
        Translated using Gemini 3 Flash. Please refer to the original for authoritative content.
    </p>
</footer>
