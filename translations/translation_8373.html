
    <style type="text/css">

    body {
    margin: 48px auto;
    max-width: 68ch;              /* character-based width reads better */
    padding: 0 16px;

    font-family: -apple-system, BlinkMacSystemFont, "Segoe UI",
                Roboto, "Helvetica Neue", Arial, sans-serif;
    font-size: 18px;
    line-height: 1.65;
    color: #333;
    background: #fafafa;
    }

    h1, h2, h3, h4 {
    line-height: 1.25;
    margin-top: 2.2em;
    margin-bottom: 0.6em;
    font-weight: 600;
    }

    h1 {
    font-size: 2.1em;
    margin-top: 0;
    }

    h2 {
    font-size: 1.6em;
    border-bottom: 1px solid #e5e5e5;
    padding-bottom: 0.3em;
    }

    h3 {
    font-size: 1.25em;
    }

    h4 {
    font-size: 1.05em;
    color: #555;
    }

    /* Paragraphs and lists */
    p {
    margin: 1em 0;
    }

    ul, ol {
    margin: 1em 0 1em 1.5em;
    }

    li {
    margin: 0.4em 0;
    }

    a {
    color: #005fcc;
    text-decoration: none;
    }

    a:hover {
    text-decoration: underline;
    }

    code {
    font-family: ui-monospace, SFMono-Regular, Menlo, Consolas, monospace;
    font-size: 0.95em;
    background: #f2f2f2;
    padding: 0.15em 0.35em;
    border-radius: 4px;
    }

    pre {
    font-family: ui-monospace, SFMono-Regular, Menlo, Consolas, monospace;
    font-size: 0.9em;
    background: #f5f5f5;
    padding: 1em 1.2em;
    overflow-x: auto;
    border-radius: 6px;
    line-height: 1.45;
    }

    pre code {
    background: none;
    padding: 0;
    }

    blockquote {
    margin: 1.5em 0;
    padding-left: 1em;
    border-left: 4px solid #ddd;
    color: #555;
    }

    hr {
    border: none;
    border-top: 1px solid #e0e0e0;
    margin: 3em 0;
    }

    table {
    border-collapse: collapse;
    margin: 1.5em 0;
    width: 100%;
    font-size: 0.95em;
    }

    th, td {
    padding: 0.5em 0.7em;
    border-bottom: 1px solid #e5e5e5;
    text-align: left;
    }

    th {
    font-weight: 600;
    }

    img {
    max-width: 100%;
    display: block;
    margin: 1.5em auto;
    }

    small {
    color: #666;
    }

    mjx-container {
    margin: 1em 0;
    }

    ::selection {
    background: #cce2ff;
    }

    </style>
    

<script>
window.MathJax = {
  tex: {
    inlineMath: [['$', '$'], ['\\(', '\\)']],
    displayMath: [['$$', '$$'], ['\\[', '\\]']],
    processEscapes: true,
    tags: 'ams'
  }
};
</script>
<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

<article>
    <h1><a href="https://kexue.fm/archives/8373">GlobalPointer: A Unified Manner to Handle Nested and Non-nested NER</a></h1>
    <p>By 苏剑林 | May 01, 2021</p>

    <p><strong>(Note: The relevant content of this article has been organized into the paper <a href="https://papers.cool/arxiv/2208.03054">《Global Pointer: Novel Efficient Span-based Approach for Named Entity Recognition》</a>. If you need to cite it, you can cite the English paper directly. Thank you.)</strong></p>

    <p>This article will introduce a design called GlobalPointer, which uses the idea of global normalization to perform Named Entity Recognition (NER). It can identify both nested and non-nested entities without distinction. In the case of non-nested (Flat NER), it achieves results comparable to CRF, while in nested (Nested NER) scenarios, it also performs well. Furthermore, theoretically, the design philosophy of GlobalPointer is more reasonable than CRF; and in practice, it does not require recursive denominator calculation during training like CRF, nor does it require dynamic programming during prediction—it is completely parallel. In ideal conditions, the time complexity is $\mathcal{O}(1)$!</p>

    <p>In short: more elegant, faster, and more powerful! Is there really such a good design? Let's take a look.</p>

    <figure>
        <img src="https://kexue.fm/usr/uploads/2021/05/2916450638.png" alt="GlobalPointer Multi-head Nested Entity Recognition Diagram">
        <figcaption>GlobalPointer multi-head recognition of nested entities diagram</figcaption>
    </figure>

    <h2 id="GlobalPointer">GlobalPointer</h2>
    <p>Conventional Pointer Network designs for entity recognition or reading comprehension usually use two modules to identify the start and end of an entity separately, which causes inconsistency between training and prediction. GlobalPointer is designed specifically to address this inconsistency. It treats the head and tail as a unified whole for discrimination, thus having a more "global view" (more Global).</p>

    <h3 id="基本思路">Basic Idea</h3>
    <p>Specifically, suppose the input text sequence length is $n$. For simplicity, assume there is only one type of entity to be recognized, and assume each entity is a continuous segment of the sequence with no length limit, and they can be nested (overlapping entities). How many "candidate entities" are there in such a sequence? It is not difficult to deduce that the answer is $n(n+1)/2$, which is the number of all possible continuous subsequences. Our task is to pick the true entities from these $n(n+1)/2$ candidates, which is essentially a "k-out-of-$n(n+1)/2$" multi-label classification problem. If there are $m$ entity types, we carry out $m$ such multi-label classification problems. This is the basic idea of GlobalPointer: recognizing entities as the basic unit, as shown in the image at the beginning of this article.</p>

    <p>Some readers might ask: Isn't the complexity of this design $\mathcal{O}(n^2)$? Won't it be very slow? In the era of RNNs/CNNs, it might have been perceived as slow. But now, in the era of Transformers, every layer of a Transformer already has $\mathcal{O}(n^2)$ complexity. Adding one layer of GlobalPointer or removing one doesn't make much difference. Crucially, $\mathcal{O}(n^2)$ is only the spatial complexity. If parallel performance is good, the time complexity can even drop to $\mathcal{O}(1)$, so there is no noticeable delay.</p>

    <h3 id="数学形式">Mathematical Form</h3>
    <p>Let the input $t$ of length $n$ be encoded into a vector sequence $[\boldsymbol{h}_1,\boldsymbol{h}_2,\cdots,\boldsymbol{h}_n]$. Through transformations $\boldsymbol{q}_{i,\alpha}=\boldsymbol{W}_{q,\alpha}\boldsymbol{h}_i+\boldsymbol{b}_{q,\alpha}$ and $\boldsymbol{k}_{i,\alpha}=\boldsymbol{W}_{k,\alpha}\boldsymbol{h}_i+\boldsymbol{b}_{k,\alpha}$, we obtain vector sequences $[\boldsymbol{q}_{1,\alpha},\boldsymbol{q}_{2,\alpha},\cdots,\boldsymbol{q}_{n,\alpha}]$ and $[\boldsymbol{k}_{1,\alpha},\boldsymbol{k}_{2,\alpha},\cdots,\boldsymbol{k}_{n,\alpha}]$, which are used to identify entities of type $\alpha$. We can then define:
    \begin{equation}s_{\alpha}(i,j) = \boldsymbol{q}_{i,\alpha}^{\top}\boldsymbol{k}_{j,\alpha}\label{eq:s}\end{equation}
    as the score for the continuous segment from $i$ to $j$ being an entity of type $\alpha$. That is, the inner product of $\boldsymbol{q}_{i,\alpha}$ and $\boldsymbol{k}_{j,\alpha}$ is used as the logit for the segment $t_{[i:j]}$ being type $\alpha$. Under this design, GlobalPointer is actually a simplified version of Multi-Head Attention, where each entity type corresponds to one head, omitting calculations related to $\boldsymbol{V}$.</p>

    <h3 id="相对位置">Relative Position</h3>
    <p>Theoretically, the design in Eq. $\eqref{eq:s}$ is sufficient. However, in practice, when training data is limited, its performance is often subpar because it does not explicitly contain relative position information. In the experiments later, we will see that the presence or absence of relative position information can make a difference of over 30 percentage points!</p>

    <p>For example, if we want to identify place names in weather forecast text like "Beijing: 21 degrees; Shanghai: 22 degrees; Hangzhou: 23 degrees...", there are many entities to identify. Without relative position information, GlobalPointer is not very sensitive to the length and span of entities. Therefore, it easily predicts any combination of a start from one entity and an end from another (e.g., predicting "Beijing: 21 degrees; Shanghai" as an entity). Conversely, with relative position information, GlobalPointer becomes sensitive to entity spans and can better distinguish true entities.</p>

    <p>Which relative position encoding should be used? Theoretically, any relative position encoding used in Transformers can be considered (refer to <a href="translation_8130.html">"Transformer Position Encodings that Rack Researchers' Brains"</a>). However, in practice, most relative position encodings truncate the relative position. While this truncation range is usually enough for entity recognition, it lacks elegance. Without truncation, one faces the problem of too many learnable parameters. After consideration, I found the Rotary Positional Embedding (RoPE) I previously conceived to be suitable.</p>

    <p>The introduction to RoPE can be found in <a href="translation_8265.html">"Transformer Upgrade Road: 2. Rotary Positional Embedding"</a>. It is essentially a transformation matrix $\boldsymbol{\mathcal{R}}_i$ satisfying the relationship $\boldsymbol{\mathcal{R}}_i^{\top}\boldsymbol{\mathcal{R}}_j = \boldsymbol{\mathcal{R}}_{j-i}$. Applying this to $\boldsymbol{q}$ and $\boldsymbol{k}$ respectively, we have:
    \begin{equation}s_{\alpha}(i,j) = (\boldsymbol{\mathcal{R}}_i\boldsymbol{q}_{i,\alpha})^{\top}(\boldsymbol{\mathcal{R}}_j\boldsymbol{k}_{j,\alpha}) = \boldsymbol{q}_{i,\alpha}^{\top} \boldsymbol{\mathcal{R}}_i^{\top}\boldsymbol{\mathcal{R}}_j\boldsymbol{k}_{j,\alpha} = \boldsymbol{q}_{i,\alpha}^{\top} \boldsymbol{\mathcal{R}}_{j-i}\boldsymbol{k}_{j,\alpha}\end{equation}
    This explicitly injects relative position information into the score $s_{\alpha}(i,j)$.</p>

    <h2 id="优化细节">Optimization Details</h2>
    <p>In this section, we discuss details during the training of GlobalPointer, including the choice of loss function and the calculation and optimization of evaluation metrics. We can see that GlobalPointer's entity-based design offers many elegant conveniences.</p>

    <h3 id="损失函数">Loss Function</h3>
    <p>So far, we have designed the score $s_{\alpha}(i,j)$. Identifying entities of a specific category $\alpha$ has become a multi-label classification problem with $n(n+1)/2$ classes. The next key is the design of the loss function. The most naive approach is to treat it as $n(n+1)/2$ binary classifications. However, in practice, $n$ is not small, $n(n+1)/2$ is even larger, while the number of entities in each sentence is small (usually in the single digits). Therefore, treating it as $n(n+1)/2$ binary classifications would lead to an extremely severe class imbalance problem.</p>

    <p>This is where our previous research <a href="translation_7359.html">"Generalizing Softmax+CrossEntropy to Multi-label Classification"</a> comes in handy. Simply put, this is a loss function for multi-label classification that is a generalization of single-target multi-class cross-entropy, particularly suitable for scenarios where the total number of categories is large and the number of target categories is small. Its form is not complex; for the GlobalPointer scenario, it is:
    \begin{equation}\log \left(1 + \sum\limits_{(i,j)\in P_{\alpha}} e^{-s_{\alpha}(i,j)}\right) + \log \left(1 + \sum\limits_{(i,j)\in Q_{\alpha}} e^{s_{\alpha}(i,j)}\right)\end{equation}
    where $P_{\alpha}$ is the set of (start, end) indices of all entities of type $\alpha$ in the sample, and $Q_{\alpha}$ is the set of indices for all non-entities or entities not of type $\alpha$. Note that we only need to consider combinations where $i\leq j$:
    \begin{equation}\begin{aligned}
    \Omega=&\, \big\{(i,j) \,\big\|\, 1\leq i\leq j\leq n\big\}\\
    P_{\alpha}=&\, \big\{(i,j) \,\big\|\, t_{[i:j]} \text{ is an entity of type } \alpha\big\}\\
    Q_{\alpha}=&\, \Omega - P_{\alpha}
    \end{aligned}\end{equation}
    In the decoding phase, all segments $t_{[i:j]}$ satisfying $s_{\alpha}(i,j) > 0$ are treated as entities of type $\alpha$. As can be seen, the decoding process is extremely simple, and under full parallelism, the decoding efficiency is $\mathcal{O}(1)$!</p>

    <h3 id="评价指标">Evaluation Metrics</h3>
    <p>The common evaluation metric for NER is F1. Note that this is entity-level F1, not labeling-tag-level F1. Under traditional Pointer Network or CRF designs, calculating entity-level F1 during training is difficult. However, under the GlobalPointer design, calculating entity-level F1 or accuracy is very easy. For example, the F1 calculation is as follows:</p>

<pre><code>def global_pointer_f1_score(y_true, y_pred):
    """F1 designed for GlobalPointer
    """
    y_pred = K.cast(K.greater(y_pred, 0), K.floatx())
    return 2 * K.sum(y_true * y_pred) / K.sum(y_true + y_pred)
</code></pre>

    <p>It is this simple primarily due to GlobalPointer being "Global". Both <code>y_true</code> and <code>y_pred</code> are already at the entity level. By checking <code>y_pred > 0</code>, we know which entities were extracted, and then by matching them, we can calculate various entity-level metrics, achieving consistency between training, evaluation, and prediction.</p>

    <h3 id="优化F1值">Optimizing F1 Value</h3>
    <p>Another benefit of GlobalPointer's "Global" nature is that if we use it for Machine Reading Comprehension (MRC), it can directly optimize the MRC F1 metric! The F1 for MRC differs from NER F1 as it is a fuzzy match score of the answer. Directly optimizing F1 may be more beneficial for improving the final score. Using GlobalPointer for MRC is equivalent to NER with only one entity type. We define:
    \begin{equation}p(i,j) = \frac{e^{s(i,j)}}{\sum\limits_{i \leq j} e^{s(i,j)}}\end{equation}
    With $p(i,j)$ defined, and following reinforcement learning ideas (refer to <a href="translation_7737.html">"Policy Gradient and Zero-order Optimization Reaching the Same Goal"</a>), optimizing F1 can be done using the following loss:
    \begin{equation}-\sum_{i\leq j} p(i,j) f_1(i,j) + \lambda \sum_{i\leq j}p(i,j)\log p(i,j)\end{equation}
    where $f_1(i,j)$ is the pre-calculated F1 similarity between the segment $t_{[i:j]}$ and the ground truth, and $\lambda$ is a hyperparameter. Calculating all $f_1(i,j)$ might be costly, but it is a one-time operation and can be optimized (e.g., setting to zero if the start and end are too far apart). Overall, it's within an acceptable range. If the goal is to improve the final F1 of MRC, this is a direct scheme to try. (I tried this on the Baidu LIC2021 MRC track this year and it indeed showed some effect.)</p>

    <h2 id="实验结果">Experimental Results</h2>
    <p>Now that everything is ready, let's start the experiments. The experiment code is organized as follows:</p>

    <blockquote>
        <p><strong>Open source address: <a href="https://github.com/bojone/GlobalPointer">https://github.com/bojone/GlobalPointer</a></strong></p>
    </blockquote>

    <p>Currently, GlobalPointer is built into bert4keras >= 0.10.6. The three experimental tasks are all Chinese NER tasks. The first two are non-nested, and the third is nested NER. The sequence length statistics for their training sets are:</p>

    <table border="1" cellpadding="5" cellspacing="0" style="text-align: center; width: 100%;">
        <thead>
            <tr>
                <th>Dataset</th>
                <th>Avg. Length (Chars)</th>
                <th>Std Dev</th>
            </tr>
        </thead>
        <tbody>
            <tr>
                <td>People's Daily NER</td>
                <td>46.93</td>
                <td>30.08</td>
            </tr>
            <tr>
                <td>CLUENER</td>
                <td>37.38</td>
                <td>10.71</td>
            </tr>
            <tr>
                <td>CMeEE</td>
                <td>54.15</td>
                <td>80.27</td>
            </tr>
        </tbody>
    </table>

    <h3 id="人民日报">People's Daily</h3>
    <p>First, we verify whether GlobalPointer can replace CRF in non-nested scenarios using the classic People's Daily corpus. The baseline is the BERT+CRF combination, compared against BERT+GlobalPointer. The results are as follows:</p>

    <table border="1" cellpadding="5" cellspacing="0" style="text-align: center; width: 100%;">
        <caption>People's Daily NER Results</caption>
        <thead>
            <tr>
                <th>Model</th>
                <th>Val F1</th>
                <th>Test F1</th>
                <th>Training Speed</th>
                <th>Inference Speed</th>
            </tr>
        </thead>
        <tbody>
            <tr>
                <td>CRF</td>
                <td>96.39%</td>
                <td>95.46%</td>
                <td>1x</td>
                <td>1x</td>
            </tr>
            <tr>
                <td>GlobalPointer (w/o RoPE)</td>
                <td>54.35%</td>
                <td>62.59%</td>
                <td>1.61x</td>
                <td>1.13x</td>
            </tr>
            <tr>
                <td>GlobalPointer (w/ RoPE)</td>
                <td>96.25%</td>
                <td>95.51%</td>
                <td>1.56x</td>
                <td>1.11x</td>
            </tr>
        </tbody>
    </table>

    <p>Firstly, the most striking visual impact in the table is the gap between GlobalPointer with and without RoPE, which reaches over 30 points! This demonstrates the importance of explicitly adding relative position information to GlobalPointer. In subsequent experiments, we will no longer verify the version without RoPE and assume it is added by default.</p>

    <p>From the table, it is also evident that in classic non-nested NER tasks, GlobalPointer matches CRF in performance and even slightly surpasses it in speed. It can be described as both fast and effective.</p>

    <h3 id="CLUENER">CLUENER</h3>
    <p>Since the starting point for People's Daily is already quite high, the gap might not be obvious. To further test, we use the newer <a href="https://github.com/CLUEbenchmark/CLUENER2020">CLUENER</a> dataset, which is also non-nested. The current SOTA F1 is around 81%. The comparison between BERT+CRF and BERT+GlobalPointer is as follows:</p>

    <table border="1" cellpadding="5" cellspacing="0" style="text-align: center; width: 100%;">
        <caption>CLUENER Experimental Results</caption>
        <thead>
            <tr>
                <th>Model</th>
                <th>Val F1</th>
                <th>Test F1</th>
                <th>Training Speed</th>
                <th>Inference Speed</th>
            </tr>
        </thead>
        <tbody>
            <tr>
                <td>CRF</td>
                <td>79.51%</td>
                <td>78.70%</td>
                <td>1x</td>
                <td>1x</td>
            </tr>
            <tr>
                <td>GlobalPointer</td>
                <td>80.03%</td>
                <td>79.44%</td>
                <td>1.22x</td>
                <td>1x</td>
            </tr>
        </tbody>
    </table>

    <p>This result shows that as NER difficulty increases, even in non-nested scenarios, GlobalPointer's effect can outperform CRF. This indicates that for NER scenarios, GlobalPointer is actually more useful than CRF. We will later provide a simple theoretical analysis to further explain why GlobalPointer is theoretically more reasonable than CRF.</p>

    <p>Regarding speed, since text lengths in this task are generally short, the speed increase for GlobalPointer is not very significant.</p>

    <h3 id="CMeEE">CMeEE</h3>
    <p>Finally, we test a nested task: CMeEE. It was the "Chinese Medical Named Entity Recognition" competition on biendata last year and is Task 1 of this year's "Chinese Medical Information Processing Challenge (CBLUE)". It is essentially medical NER with nested entities. Again, comparing CRF and GlobalPointer:</p>

    <table border="1" cellpadding="5" cellspacing="0" style="text-align: center; width: 100%;">
        <caption>CMeEE Experimental Results</caption>
        <thead>
            <tr>
                <th>Model</th>
                <th>Val F1</th>
                <th>Test F1</th>
                <th>Training Speed</th>
                <th>Inference Speed</th>
            </tr>
        </thead>
        <tbody>
            <tr>
                <td>CRF</td>
                <td>63.81%</td>
                <td>64.39%</td>
                <td>1x</td>
                <td>1x</td>
            </tr>
            <tr>
                <td>GlobalPointer</td>
                <td>64.84%</td>
                <td>65.98%</td>
                <td>1.52x</td>
                <td>1.13x</td>
            </tr>
        </tbody>
    </table>

    <p>It can be seen that GlobalPointer clearly outperforms CRF in performance. Regarding speed, combined with the results of the three tasks, generally, the longer the text in the task, the more obvious the training acceleration of GlobalPointer. Prediction speed usually also sees a slight improvement, though not as large as the increase in the training phase. Later, I tinkered with RoBERTa-large as the encoder and found it (not too difficultly) reached over 67% on the online test set, proving GlobalPointer is a "competent" design.</p>

    <p>Of course, some readers might complain: Using a non-nested CRF for nested NER makes the comparison with GlobalPointer unfair. True, to some extent. But it's not a major issue; on one hand, the F1 for CMeEE is still relatively low, and nested entities are not that numerous. Even ignoring the nested parts and treating them as non-nested wouldn't have a massive impact. On the other hand, for nested NER, I haven't found a simple and clear design that could easily serve as a baseline, so I ran CRF for a benchmark. Readers are welcome to report results from other designs.</p>

    <h2 id="思考拓展">Reflections and Extensions</h2>
    <p>In this section, we further compare CRF and GlobalPointer theoretically and introduce some works related to GlobalPointer to help readers better understand its positioning.</p>

    <h3 id="相比CRF">Compared to CRF</h3>
    <p>CRF (Conditional Random Field) is a classic design for sequence labeling. Since most NER can be transformed into sequence labeling, CRF is a classic method for NER. I have written articles like <a href="translation_5542.html">"A Concise Introduction to Conditional Random Field CRF (with Pure Keras Implementation)"</a> and <a href="translation_7196.html">"Your CRF Layer's Learning Rate Might not be Big Enough"</a>. In previous introductions, we noted that if the number of sequence labels is $k$, the difference between frame-wise softmax and CRF is:</p>

    <blockquote>
        <p>The former views sequence labeling as $n$ independent $k$-classification problems, while the latter views sequence labeling as one single $k^n$-classification problem.</p>
    </blockquote>

    <p>This statement actually points out the theoretical shortcomings of using frame-wise softmax and CRF for NER. How so? Frame-wise softmax is too lenient because even if every label is predicted correctly, it doesn't mean the entity is correctly extracted—entities require all labels in a segment to be correct. Conversely, CRF viewing sequence labeling as one $k^n$-classification problem is too strict, meaning it requires every entity in the sequence to be predicted correctly to get a "correct" score; partial entities don't earn credit. Although CRF can produce partially correct results in practice, that's more a testament to the model's generalization; the CRF design itself implies a "full correctness or nothing" approach.</p>

    <p>Therefore, CRF has theoretical aspects that are not entirely reasonable. In contrast, GlobalPointer is more aligned with application and evaluation scenarios: it is entity-based and designed as a "multi-label classification" problem. Consequently, its loss function and evaluation metrics are at the entity granularity, granting reasonable scores even if only some entities are correct. Therefore, it is "within reason" that GlobalPointer can achieve better results than CRF even in non-nested NER scenarios.</p>

    <h3 id="相关工作">Related Work</h3>
    <p>If readers follow progress in entity recognition and information extraction, they might notice that GlobalPointer is very similar to <a href="https://papers.cool/arxiv/2010.13415">TPLinker</a>, a recent design for relation extraction. However, this idea of global normalization can be traced back even further.</p>

    <p>I first encountered this idea in the paper <a href="https://papers.cool/arxiv/1709.02828">《Globally Normalized Reader》</a> published by Baidu in 2017, which proposed a Global Normalization Reader (GNR) for MRC. It not only treats (start, end) as a whole but (sentence, start, end) as a whole (following a flow of selecting a sentence first, then start/end), resulting in many combinations. It used ideas from <a href="https://papers.cool/arxiv/1606.02960">《Sequence-to-Sequence Learning as Beam-Search Optimization》</a> to reduce computation.</p>

    <p>With GNR as a foundation, GlobalPointer is a quite natural expansion. In fact, back in 2019 while participating in the LIC2019 relation extraction track, similar ideas occurred to me, but several issues were unresolved at the time.</p>

    <p>First, Transformer wasn't as prevalent, and $\mathcal{O}(n^2)$ complexity seemed daunting. Second, <a href="translation_7359.html">"Generalizing Softmax+CrossEntropy to Multi-label Classification"</a> hadn't been conceived, so there was no good solution for class imbalance in multi-label classification. Third, my understanding of NLP was shallower, and bert4keras wasn't developed, making experiments difficult—if I had omitted RoPE back then and seen a 30-point drop, I wouldn't have known how to tune it.</p>

    <p>Thus, GlobalPointer is a culmination of accumulation over the last two years—a bit "coincidental" but also "water reaching its course." As for TPLinker, it has no direct connection to the origin of GlobalPointer. Formally, GlobalPointer is indeed similar to TPLinker. In fact, TPLinker can be traced back further to <a href="https://www.sciencedirect.com/science/article/abs/pii/S095741741830455X">《Joint entity recognition and relation extraction as a multi-head selection problem》</a>, but those works mainly applied Global ideas to relation extraction and did not optimize specifically for NER.</p>

    <h3 id="加性乘性">Additive vs Multiplicative</h3>
    <p>In implementation, a major difference between TPLinker and GlobalPointer is that for Multi-Head, TPLinker uses additive Attention:
    \begin{equation}s_{\alpha}(i,j) = \boldsymbol{W}_{o,\alpha}\tanh\left(\boldsymbol{W}_{h,\alpha}[\boldsymbol{h}_{i},\boldsymbol{h}_{j}]+\boldsymbol{b}_{h,\alpha}\right)+\boldsymbol{b}_{o,\alpha}
    \end{equation}
    It is currently unclear how much effect difference there is between this choice and Eq. $\eqref{eq:s}$. However, compared to the multiplicative Attention in Eq. $\eqref{eq:s}$, while their theoretical complexity is similar, the actual implementation of additive Attention is much more costly, especially in terms of space (video memory).</p>

    <p>Therefore, I believe even if additive performance is slightly better, one should optimize based on the multiplicative approach, as additive efficiency is truly lacking. Additionally, papers like TPLinker did not report the importance of relative position information. Is relative position less important in additive Attention? This is currently unknown.</p>

    <h2 id="本文小结">Conclusion</h2>
    <p>This article introduced GlobalPointer, a new design for NER based on the idea of global pointers. It integrates several of my previous research results to implement an "ideal design" for handling both nested and non-nested NER in a unified manner. Experimental results show that in non-nested scenarios, it achieves results comparable to CRF, and it also performs well in nested scenarios.</p>
</article>
<hr>
<footer style="margin-top: 3em; padding: 1.5em; background: #f5f5f5; border-radius: 8px; font-size: 0.9em; color: #555;">
    <p style="margin: 0 0 0.5em 0;"><strong>Citation</strong></p>
    <p style="margin: 0 0 0.5em 0;">
        This is a machine translation of the original Chinese article:<br>
        <a href="translation_8373.html" style="color: #005fcc;">https://kexue.fm/archives/8373</a>
    </p>
    <p style="margin: 0 0 0.5em 0;">
        Original author: 苏剑林 (Su Jianlin)<br>
        Original publication: <a href="https://kexue.fm" style="color: #005fcc;">科学空间 (Scientific Spaces)</a>
    </p>
    <p style="margin: 0; font-style: italic;">
        Translated using Gemini 3 Flash. Please refer to the original for authoritative content.
    </p>
</footer>
