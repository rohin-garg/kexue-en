
    <style type="text/css">

    body {
    margin: 48px auto;
    max-width: 68ch;              /* character-based width reads better */
    padding: 0 16px;

    font-family: -apple-system, BlinkMacSystemFont, "Segoe UI",
                Roboto, "Helvetica Neue", Arial, sans-serif;
    font-size: 18px;
    line-height: 1.65;
    color: #333;
    background: #fafafa;
    }

    h1, h2, h3, h4 {
    line-height: 1.25;
    margin-top: 2.2em;
    margin-bottom: 0.6em;
    font-weight: 600;
    }

    h1 {
    font-size: 2.1em;
    margin-top: 0;
    }

    h2 {
    font-size: 1.6em;
    border-bottom: 1px solid #e5e5e5;
    padding-bottom: 0.3em;
    }

    h3 {
    font-size: 1.25em;
    }

    h4 {
    font-size: 1.05em;
    color: #555;
    }

    /* Paragraphs and lists */
    p {
    margin: 1em 0;
    }

    ul, ol {
    margin: 1em 0 1em 1.5em;
    }

    li {
    margin: 0.4em 0;
    }

    a {
    color: #005fcc;
    text-decoration: none;
    }

    a:hover {
    text-decoration: underline;
    }

    code {
    font-family: ui-monospace, SFMono-Regular, Menlo, Consolas, monospace;
    font-size: 0.95em;
    background: #f2f2f2;
    padding: 0.15em 0.35em;
    border-radius: 4px;
    }

    pre {
    font-family: ui-monospace, SFMono-Regular, Menlo, Consolas, monospace;
    font-size: 0.9em;
    background: #f5f5f5;
    padding: 1em 1.2em;
    overflow-x: auto;
    border-radius: 6px;
    line-height: 1.45;
    }

    pre code {
    background: none;
    padding: 0;
    }

    blockquote {
    margin: 1.5em 0;
    padding-left: 1em;
    border-left: 4px solid #ddd;
    color: #555;
    }

    hr {
    border: none;
    border-top: 1px solid #e0e0e0;
    margin: 3em 0;
    }

    table {
    border-collapse: collapse;
    margin: 1.5em 0;
    width: 100%;
    font-size: 0.95em;
    }

    th, td {
    padding: 0.5em 0.7em;
    border-bottom: 1px solid #e5e5e5;
    text-align: left;
    }

    th {
    font-weight: 600;
    }

    img {
    max-width: 100%;
    display: block;
    margin: 1.5em auto;
    }

    small {
    color: #666;
    }

    mjx-container {
    margin: 1em 0;
    }

    ::selection {
    background: #cce2ff;
    }

    </style>
    

<script>
window.MathJax = {
  tex: {
    inlineMath: [['$', '$'], ['\\(', '\\)']],
    displayMath: [['$$', '$$'], ['\\[', '\\]']],
    processEscapes: true,
    tags: 'ams',
    packages: {'[+]': ['ams']}
  },
  options: {
    renderActions: {
      findScript: [10, function (doc) {
        for (const node of document.querySelectorAll('script[type^="math/tex"]')) {
          const display = !!node.type.match(/; *mode=display/);
          const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
          const text = document.createTextNode('');
          node.parentNode.replaceChild(text, node);
          math.start = {node: text, delim: '', n: 0};
          math.end = {node: text, delim: '', n: 0};
          doc.math.push(math);
        }
      }, '']
    }
  }
};
</script>
<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

<nav style="margin-bottom: 1.5em;">
    <a href="../index.html" style="display: inline-flex; align-items: center; color: #555; text-decoration: none; font-size: 0.95em;">
        <span style="margin-right: 0.3em;">&larr;</span> Back to Index
    </a>
</nav>

<h1><a href="https://kexue.fm/archives/6096">Rethinking the Determinant of Non-Square Matrices</a></h1>

<p>By 苏剑林 | Oct 16, 2018</p>

<p>A few years ago, the author wrote an "Understanding Matrices" series based on his own modest understanding, including an article <a href="translation_2757.html">"Why Do Only Square Matrices Have Determinants?"</a> which discussed the issue of determinants for non-square matrices. It presented views like "determinants of non-square matrices are not elegant" and "square matrix determinants are sufficient." This article revisits this question.</p>

<p>First, recall the determinant of a square matrix. Its most important value lies in its geometric meaning:</p>

<blockquote>The absolute value of the determinant of an $n$-dimensional square matrix is equal to the hypervolume of the $n$-dimensional solid spanned by its row (or column) vectors.</blockquote>

<p>This geometric meaning is the source of all the importance of determinants; related discussions can be found in <a href="translation_1770.html">"Bits and Pieces of Determinants."</a> It is also the basis for our discussion of non-square matrix determinants.</p>

<h2>Analysis</h2>

<p>For a square matrix $\boldsymbol{A}_{n \times n}$, it can be viewed as a combination of $n$ row vectors or $n$ column vectors. In either case, the absolute value of the determinant equals the hypervolume of the $n$-dimensional solid spanned by these $n$ vectors. In other words, for square matrices, the distinction between row and column vectors does not change the determinant.</p>

<p>For a non-square matrix $\boldsymbol{B}_{n \times k}$, it is different. Without loss of generality, assume $n > k$. We can view it as a combination of $n$ $k$-dimensional row vectors or $k$ $n$-dimensional column vectors. The determinant of a non-square matrix should also possess the same meaning, namely, the hypervolume of the solid they span.</p>

<p>Consider the first case: if viewed as $n$ $k$-dimensional row vectors, we must consider the hypervolume of the $n$-dimensional solid spanned by these $n$ vectors. However, since $n > k$, these $n$ vectors are necessarily linearly dependent, so they cannot span an $n$-dimensional solid; it might be an $(n-1)$-dimensional solid or lower. Consequently, its $n$-dimensional hypervolume is naturally 0.</p>

<p>However, the second case is not so trivial. If viewed as $k$ $n$-dimensional column vectors, although they are $n$-dimensional, they span a $k$-dimensional solid. The hypervolume of this $k$-dimensional solid is not necessarily 0. Let's use this non-trivial volume as the definition for the determinant of a non-square matrix.</p>

<h2>Definition</h2>

<p>For the second case, there is a very clever definition that can leverage square matrix determinants:</p>

\begin{equation}\|\det \boldsymbol{B}\| = \sqrt{\det (\boldsymbol{B}^{\top}\boldsymbol{B})}\label{eq:dingyi}\end{equation}

<p>Of course, this only defines the absolute value of the determinant, but that is often sufficient. In most cases, we only use the absolute value.</p>

<p>It can be observed that this definition is compatible with square matrix determinants, and we will further prove that this definition indeed preserves the geometric meaning of the determinant.</p>

<p>Let's calculate two examples. The first example considers an $n \times 1$ matrix:</p>

\begin{equation}\boldsymbol{X} = \begin{pmatrix}x_1 \\ x_2 \\ \vdots \\ x_n\end{pmatrix}\end{equation}

<p>According to definition $\eqref{eq:dingyi}$, we calculate:</p>

\begin{equation}\|\det \boldsymbol{X}\| = \sqrt{x_1^2 + x_2^2 + \dots + x_n^2}\end{equation}

<p>According to our definition, it should represent the "1-dimensional volume" of one $n$-dimensional column vector. By analogy, "1-dimensional volume" is length, and the formula above is exactly the vector norm formula. That is to say, in the $n \times 1$ case, definition $\eqref{eq:dingyi}$ is compatible with our expectation.</p>

<p>The second example is an $n \times 2$ matrix:</p>

\begin{equation}\boldsymbol{Z} = \begin{pmatrix}x_1 & y_1 \\ x_2 & y_2 \\ \vdots & \vdots \\ x_n & y_n\end{pmatrix}=(\boldsymbol{x}, \boldsymbol{y})\end{equation}

<p>Calculating according to definition $\eqref{eq:dingyi}$, we obtain the final result as:</p>

\begin{equation}\|\det \boldsymbol{Z}\| = \sqrt{\boldsymbol{x}^{\top}\boldsymbol{x}\boldsymbol{y}^{\top}\boldsymbol{y} - (\boldsymbol{x}^{\top}\boldsymbol{y})^2}\end{equation}

<p>It is not difficult to find that this result is exactly the square of the area of the parallelogram spanned by $\boldsymbol{x}, \boldsymbol{y}$, because the area of the parallelogram calculated according to the definition should be:</p>

\begin{equation}\begin{aligned}\|\boldsymbol{x}\|\cdot\|\boldsymbol{y}\|\cdot\sin\theta =& \|\boldsymbol{x}\|\cdot\|\boldsymbol{y}\|\cdot\sqrt{1-\cos^2\theta}\\
=&\|\boldsymbol{x}\|\cdot\|\boldsymbol{y}\|\cdot\sqrt{1-\left(\frac{\boldsymbol{x}^{\top}\boldsymbol{y}}{\|\boldsymbol{x}\|\cdot\|\boldsymbol{y}\|}\right)^2}
\end{aligned}\end{equation}

<p>In other words, for an $n \times 2$ matrix, definition $\eqref{eq:dingyi}$ is also consistent with our expectation.</p>

<h2>Proof</h2>

<p>Now consider the general proof. For an $n \times k$ matrix $\boldsymbol{B}$:</p>

\begin{equation}\boldsymbol{B}_{n \times k} = \begin{pmatrix}b_{11} & \dots & b_{1k}\\ b_{21} & \dots & b_{2k}\\
\vdots & \ddots & \vdots\\
b_{n1} & \dots & b_{nk}\end{pmatrix} = (\boldsymbol{b}_1,\dots,\boldsymbol{b}_k)\end{equation}

<p>and $n > k$. First, from the Gram–Schmidt orthogonalization process we are familiar with, we know there exists an $n \times k$ semi-orthogonal matrix $\boldsymbol{U}_{n \times k}$ ($k$ mutually orthogonal $n$-dimensional unit column vectors) and a $k \times k$ triangular matrix $\boldsymbol{C}_{k \times k}$, such that:</p>

\begin{equation}\boldsymbol{B}_{n \times k}=\boldsymbol{U}_{n\times k}\boldsymbol{C}_{k\times k}\end{equation}

<p>This is mathematically known as "QR decomposition." We know that orthogonal transformations do not change any geometric properties, so the determinant of $\boldsymbol{B}_{n \times k}$ should equal the determinant of $\boldsymbol{C}_{k \times k}$ (absolute value), which is $\|\det \boldsymbol{C}_{k\times k}\|$.</p>

<p>Thus, we have:</p>

\begin{equation}\begin{aligned}\|\det \boldsymbol{B}_{n\times k}\| =& \|\det \boldsymbol{C}_{k\times k}\|\\
=& \sqrt{\det\left(\boldsymbol{C}_{k\times k}^{\top}\boldsymbol{C}_{k\times k}\right)}\\
=& \sqrt{\det\left[\left(\boldsymbol{U}_{n\times k}^{\top}\boldsymbol{B}_{n \times k}\right)^{\top}\left(\boldsymbol{U}_{n\times k}^{\top}\boldsymbol{B}_{n \times k}\right)\right]}\\
=& \sqrt{\det\left(\boldsymbol{B}_{n \times k}^{\top}\boldsymbol{B}_{n \times k}\right)}
\end{aligned}\end{equation}

<p>Therefore, for an $n \times k$ matrix $\boldsymbol{B}$ where $n > k$, a non-trivial and reasonable matrix definition is $\sqrt{\det (\boldsymbol{B}^{\top}\boldsymbol{B})}$. Obviously, if $n < k$, the definition is $\sqrt{\det (\boldsymbol{B}\boldsymbol{B}^{\top})}$.</p>

<h2>Conclusion</h2>

<p>We started from geometric meaning to discuss the problem of determinants for non-square matrices, finally showing that formula $\eqref{eq:dingyi}$ can serve as a relatively reasonable definition for non-square determinants. Although theoretically $\eqref{eq:dingyi}$ only defines the absolute value of the determinant, it is sufficient for most scenarios.</p>

<p>As for applications of non-square determinants, we know that when performing coordinate transformations for integrals, we have a Jacobian determinant to ensure the non-triviality of the transformation. Similarly, it might be possible to use non-square determinants to ensure the non-triviality of upsampling or downsampling transformations. Of course, this is a conceptual idea; the author is currently reflecting on these types of problems and welcomes interested readers to discuss.</p>
<hr>
<footer style="margin-top: 3em; padding: 1.5em; background: #f5f5f5; border-radius: 8px; font-size: 0.9em; color: #555;">
    <p style="margin: 0 0 0.5em 0;"><strong>Citation</strong></p>
    <p style="margin: 0 0 0.5em 0;">
        This is a machine translation of the original Chinese article:<br>
        <a href="https://kexue.fm/archives/6096" style="color: #005fcc;">https://kexue.fm/archives/6096</a>
    </p>
    <p style="margin: 0 0 0.5em 0;">
        Original author: 苏剑林 (Su Jianlin)<br>
        Original publication: <a href="https://kexue.fm" style="color: #005fcc;">科学空间 (Scientific Spaces)</a>
    </p>
    <p style="margin: 0; font-style: italic;">
        Translated using Gemini 3 Flash. Please refer to the original for authoritative content.
    </p>
</footer>
