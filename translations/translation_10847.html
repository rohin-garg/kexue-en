
    <style type="text/css">

    body {
    margin: 48px auto;
    max-width: 68ch;              /* character-based width reads better */
    padding: 0 16px;

    font-family: -apple-system, BlinkMacSystemFont, "Segoe UI",
                Roboto, "Helvetica Neue", Arial, sans-serif;
    font-size: 18px;
    line-height: 1.65;
    color: #333;
    background: #fafafa;
    }

    h1, h2, h3, h4 {
    line-height: 1.25;
    margin-top: 2.2em;
    margin-bottom: 0.6em;
    font-weight: 600;
    }

    h1 {
    font-size: 2.1em;
    margin-top: 0;
    }

    h2 {
    font-size: 1.6em;
    border-bottom: 1px solid #e5e5e5;
    padding-bottom: 0.3em;
    }

    h3 {
    font-size: 1.25em;
    }

    h4 {
    font-size: 1.05em;
    color: #555;
    }

    /* Paragraphs and lists */
    p {
    margin: 1em 0;
    }

    ul, ol {
    margin: 1em 0 1em 1.5em;
    }

    li {
    margin: 0.4em 0;
    }

    a {
    color: #005fcc;
    text-decoration: none;
    }

    a:hover {
    text-decoration: underline;
    }

    code {
    font-family: ui-monospace, SFMono-Regular, Menlo, Consolas, monospace;
    font-size: 0.95em;
    background: #f2f2f2;
    padding: 0.15em 0.35em;
    border-radius: 4px;
    }

    pre {
    font-family: ui-monospace, SFMono-Regular, Menlo, Consolas, monospace;
    font-size: 0.9em;
    background: #f5f5f5;
    padding: 1em 1.2em;
    overflow-x: auto;
    border-radius: 6px;
    line-height: 1.45;
    }

    pre code {
    background: none;
    padding: 0;
    }

    blockquote {
    margin: 1.5em 0;
    padding-left: 1em;
    border-left: 4px solid #ddd;
    color: #555;
    }

    hr {
    border: none;
    border-top: 1px solid #e0e0e0;
    margin: 3em 0;
    }

    table {
    border-collapse: collapse;
    margin: 1.5em 0;
    width: 100%;
    font-size: 0.95em;
    }

    th, td {
    padding: 0.5em 0.7em;
    border-bottom: 1px solid #e5e5e5;
    text-align: left;
    }

    th {
    font-weight: 600;
    }

    img {
    max-width: 100%;
    display: block;
    margin: 1.5em auto;
    }

    small {
    color: #666;
    }

    mjx-container {
    margin: 1em 0;
    }

    ::selection {
    background: #cce2ff;
    }

    </style>
    

<script>
window.MathJax = {
  tex: {
    inlineMath: [['$', '$'], ['\\(', '\\)']],
    displayMath: [['$$', '$$'], ['\\[', '\\]']],
    processEscapes: true,
    packages: {'[+]': ['ams']},
    tags: 'ams'
  }
};
</script>
<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

<h1><a href="https://kexue.fm/archives/10847">Matrix Effective Rank</a></h1>

<p>By 苏剑林 | Apr 10, 2025</p>

<p>Rank is an important concept in linear algebra, representing the inherent dimensionality of a matrix. However, the strict mathematical definition of rank is often not fully applicable to numerical computation scenarios. This is because rank equals the number of non-zero singular values, and the mathematical understanding of "equal to zero" differs from numerical computation. In mathematics, "equal to zero" means absolutely and strictly zero—even $10^{-100}$ is not zero. But numerical computation is different; in many cases, $10^{-10}$ can be treated as zero.</p>

<p>Therefore, we hope to generalize the concept of rank to a form that better fits the characteristics of numerical computation. This is the origin of the concept of Effective Rank.</p>

<h2>Error Truncation</h2>

<p>It should be noted that there is currently no unified definition of effective rank in academic circles. In the following, we introduce several approaches to defining effective rank from different perspectives. For practical problems, readers can choose the definition that best suits their needs.</p>

<p>We primarily consider rank from the perspective of singular values. For a matrix $\boldsymbol{M}\in\mathbb{R}^{n\times m}$, let $n\leq m$ without loss of generality. Its standard rank is</p>

\begin{equation}\mathop{\text{rank}}(\boldsymbol{M}) \triangleq \max\{i\,|\,\sigma_i > 0\}\leq n\end{equation}

<p>where $\sigma_1\geq \sigma_2\geq \cdots\geq\sigma_n \geq 0$ are the singular values of $\boldsymbol{M}$. Intuitively, the concept of effective rank is intended to treat singular values close to zero as zero. Therefore, a basic property of effective rank is that it does not exceed the standard rank, and in special cases, it can degenerate into the standard rank. A simple definition satisfying this property is</p>

\begin{equation}\mathop{\text{erank}}(\boldsymbol{M},\epsilon) \triangleq \max\{i\,|\,\sigma_i > \epsilon\}\end{equation}

<p>However, the concepts of "large" and "small" should be relative. 1 is large compared to 0.01, but small compared to 100. Thus, it seems more scientific to standardize by dividing by $\sigma_1$:</p>

\begin{equation}\mathop{\text{erank}}(\boldsymbol{M},\epsilon) \triangleq \max\big\{i\,\big\|\,\sigma_i/\sigma_1 > \epsilon\big\}\end{equation}

<p>In addition to directly truncating singular values close to zero, we can also approach this from the perspective of low-rank approximation. In <a href="translation_10407.html">"The Path to Low-Rank Approximation (II): SVD"</a>, we proved that the optimal $r$-rank approximation of a matrix is the SVD result obtained by retaining only the largest $r$ singular values. Conversely, we can specify a relative error $\epsilon$ and define the effective rank as the minimum rank required to achieve this relative error:</p>

\begin{equation}\mathop{\text{erank}}(\boldsymbol{M},\epsilon) \triangleq \min\left\{ i\,\,\left\|\,\,\sqrt{\left(\sum_{i=1}^r \sigma_i^2\right)\left/\left(\sum_{i=1}^n \sigma_i^2\right.\right)} \geq 1-\epsilon\right.\right\}\end{equation}

<p>The numerical significance of this definition is clearer, but since it only considers the overall error, some examples are not very elegant. For instance, for an $n\times n$ identity matrix, we expect its effective rank to always be $n$, because every singular value is identically 1, and no truncation should occur. However, with the above definition, if $n$ is large enough ($1/n < \epsilon$), the effective rank will be less than $n$.</p>

<h2>Norm Ratios</h2>

<p>Although the definitions of effective rank in the previous section are intuitive, they all rely on a hyperparameter $\epsilon$, which is ultimately not concise enough. Our basic understanding now is that the concept of effective rank should only depend on relative magnitudes, so the problem is equivalent to constructing an effective rank from $1\geq \sigma_2/ \sigma_1\geq\cdots\geq\sigma_n/\sigma_1\geq 0$. Given that these values are within $[0,1]$, a clever idea is to simply sum them:</p>

\begin{equation}\mathop{\text{erank}}(\boldsymbol{M}) \triangleq \sum_{i=1}^n\frac{\sigma_i}{\sigma_1}\end{equation}

<p>From <a href="translation_10407.html">"The Path to Low-Rank Approximation (II): SVD"</a>, we know that the largest singular value $\sigma_1$ is the matrix's <a href="https://en.wikipedia.org/wiki/Matrix_norm#Spectral_norm_(p_=_2)">Spectral Norm</a>, denoted as $\Vert\boldsymbol{M}\Vert_2$, and the sum of all singular values is actually also a matrix norm, called the <a href="https://en.wikipedia.org/wiki/Matrix_norm#Schatten_norms">"Nuclear Norm"</a>, usually denoted as $\Vert\boldsymbol{M}\Vert_*$. Thus, the above expression can be simplified as</p>

\begin{equation}\mathop{\text{erank}}(\boldsymbol{M}) \triangleq \frac{\Vert\boldsymbol{M}\Vert_*}{\Vert\boldsymbol{M}\Vert_2}\label{eq:n-2}\end{equation}

<p>The earliest source for this might be <a href="https://papers.cool/arxiv/1501.01571">"An Introduction to Matrix Concentration Inequalities"</a>, where it is called the "Intrinsic Dimension," though related properties were already explored in <a href="https://papers.cool/arxiv/0706.4138">"Guaranteed Minimum-Rank Solutions of Linear Matrix Equations via Nuclear Norm Minimization"</a>.</p>

<p>Similarly, we can construct the effective rank through a sum of squares:</p>

\begin{equation}\mathop{\text{erank}}(\boldsymbol{M}) \triangleq \sum_{i=1}^n\frac{\sigma_i^2}{\sigma_1^2} = \frac{\Vert\boldsymbol{M}\Vert_F^2}{\Vert\boldsymbol{M}\Vert_2^2}\label{eq:f-2}\end{equation}

<p>Here $\Vert\cdot\Vert_F$ is the $F$-norm (Frobenius norm). This definition likely stems from <a href="https://arxiv.org/abs/math/0503442">"Sampling from large matrices: an approach through geometric functional analysis"</a>, where it was called "Numerical Rank"; today it is more commonly known as "Stable Rank" and is one of the popular concepts of effective rank.</p>

<p>From the perspective of computational complexity, Eq. $\eqref{eq:f-2}$ is less costly than Eq. $\eqref{eq:n-2}$. Calculating the nuclear norm requires all singular values, which implies a full SVD; however, $\Vert\boldsymbol{M}\Vert_F^2$ is equal to the sum of the squares of all matrix elements. Therefore, the main computational cost for Eq. $\eqref{eq:f-2}$ is the maximum singular value, which is significantly cheaper than computing all singular values. If desired, one could also generalize Eq. $\eqref{eq:n-2}$ and Eq. $\eqref{eq:f-2}$ to a sum of $k$-th powers, where the result is actually a ratio of a more general <a href="https://en.wikipedia.org/wiki/Schatten_norm">Schatten norm</a> to the spectral norm.</p>

<h2>Distribution and Entropy</h2>

<p>If readers search directly for "Effective Rank," they are highly likely to find the paper <a href="https://www.eurasip.org/Proceedings/Eusipco/Eusipco2007/Papers/a5p-h05.pdf">"The Effective Rank: a Measure of Effective Dimensionality"</a>. This is one of the earlier works discussing effective rank, and it proposes a definition based on entropy.</p>

<p>First, since singular values are always non-negative, we can normalize them to obtain a probability distribution:</p>

\begin{equation}p_i = \frac{\sigma_i^{\gamma}}{\sum_{j=1}^n \sigma_j^{\gamma}}\end{equation}

<p>where $\gamma > 0$. According to the literature, both $\gamma=1$ and $\gamma=2$ are frequently used (we used $\gamma=2$ in <a href="translation_10739.html">Moonlight</a>). For the following, we use $\gamma=1$ as an example. Given a probability distribution, we can calculate the information entropy (Shannon entropy):</p>

\begin{equation}H = -\sum_{i=1}^n p_i \log p_i\end{equation}

<p>Recall that the range of entropy is $[0, \log n]$. Taking the exponential gives $e^H \in [1, n]$. When the distribution is One-Hot, $e^H=1$ (only one non-zero singular value); when the distribution is uniform, $e^H=n$ (all singular values are equal). These correspond exactly to the two special cases of standard rank, which inspires us to define the effective rank as</p>

\begin{equation}\mathop{\text{erank}}(\boldsymbol{M}) \triangleq e^H = \exp\left(-\sum_{i=1}^n p_i \log p_i\right)\label{eq:h-erank}\end{equation}

<p>Substituting the definition of $p_i$, we find that it can be further transformed into</p>

\begin{equation}\mathop{\text{erank}}(\boldsymbol{M}) = \exp\left(\log\sum_{i=1}^n \sigma_i -\frac{\sum_{i=1}^n \sigma_i\log\sigma_i}{\sum_{i=1}^n \sigma_i}\right)\end{equation}

<p>Clearly, the first term inside the parentheses after taking the exponential is $\Vert\boldsymbol{M}\Vert_*$; the second term is a weighted average of $\log\sigma_i$ with weights $\sigma_i$. In this case, $\log\sigma_i$ will be approximately equal to the largest $\log\sigma_1$. Taking the exponential then yields $\sigma_1=\Vert\boldsymbol{M}\Vert_2$. Thus, the overall result of this formula will be approximately Eq. $\eqref{eq:n-2}$. This demonstrates that defining effective rank based on entropy, while seemingly a completely different path, is actually similar in essence to the norm ratios discussed in the previous section.</p>

<p>We know that standard rank satisfies the triangle inequality $\mathop{\text{rank}}(\boldsymbol{A}+\boldsymbol{B})\leq \mathop{\text{rank}}(\boldsymbol{A}) + \mathop{\text{rank}}(\boldsymbol{B})$. The original paper proved that for (semi-)positive definite symmetric matrices $\boldsymbol{A}$ and $\boldsymbol{B}$, the effective rank defined by Eq. $\eqref{eq:h-erank}$ satisfies $\mathop{\text{erank}}(\boldsymbol{A}+\boldsymbol{B})\leq \mathop{\text{erank}}(\boldsymbol{A}) + \mathop{\text{erank}}(\boldsymbol{B})$. It remains unclear whether this inequality can be generalized to arbitrary matrices. Currently, proving whether effective rank preserves certain inequalities of standard rank is not an easy task.</p>

<h2>Sparsity Indicators</h2>

<p>From the various definitions of effective rank discussed above—especially the transition from singular values to distributions and then to entropy—some readers may have vaguely realized that effective rank shares clear commonalities with sparsity. In fact, effective rank can be understood as a measure of sparsity for the vector of singular values. Unlike standard sparsity metrics, we align the range to $1\leq \mathop{\text{erank}}(\boldsymbol{M}) \leq \mathop{\text{rank}}(\boldsymbol{M}) \leq n$ to align it with the concept of rank, making the degree of sparsity easier to perceive intuitively.</p>

<p>Regarding sparsity measures, we previously had a systematic discussion in <a href="translation_9595.html">"How to Measure the Sparsity of Data?"</a>. In theory, the results there can be used to construct definitions for effective rank. Indeed, we have done so. In the "Norm Ratios" section, our construction of effective rank based on the ratio of the Schatten norm to the spectral norm only corresponds to formula (1) in that article. We could also use other formulas, such as (16), which is equivalent to defining effective rank as the square of the ratio between the nuclear norm and the Frobenius norm:</p>

\begin{equation}\mathop{\text{erank}}(\boldsymbol{M}) \triangleq \frac{\Vert\boldsymbol{M}\Vert_*^2}{\Vert\boldsymbol{M}\Vert_F^2} = \frac{(\sum_{i=1}^n\sigma_i)^2}{\sum_{i=1}^n\sigma_i^2}\end{equation}

<p>This similarly satisfies $1\leq \mathop{\text{erank}}(\boldsymbol{M}) \leq \mathop{\text{rank}}(\boldsymbol{M})$, making it a valid definition for effective rank.</p>

<p>It is quite remarkable how our understanding of effective rank and sparsity has invisibly formed a closed loop. It must be said that this is a wonderful experience: when I first began studying sparsity measures, I knew nothing about effective rank. While studying effective rank over these past few days, I gradually realized it is essentially connected to sparsity. It seems as if there is a mysterious force quietly linking the knowledge we accumulate across different fields and disciplines, eventually converging in the same correct direction.</p>

<h2>Summary</h2>

<p>This article explores the concept of the effective rank of a matrix. It is an extension of the rank concept in linear algebra for numerical computation scenarios and offers a more effective way to measure the inherent dimensionality of a matrix.</p>

<p><strong>If you have any doubts or suggestions, please feel free to discuss them in the comments section below.</strong></p>

<p><strong>If you find this article helpful, you are welcome to share/support this site. Support is not about profit, but to let me know how many readers are truly following "Scientific Space." Of course, if you choose to ignore it, it will not affect your reading. Thank you again!</strong></p>

<hr>
<footer style="margin-top: 3em; padding: 1.5em; background: #f5f5f5; border-radius: 8px; font-size: 0.9em; color: #555;">
    <p style="margin: 0 0 0.5em 0;"><strong>Citation</strong></p>
    <p style="margin: 0 0 0.5em 0;">
        This is a machine translation of the original Chinese article:<br>
        <a href="https://kexue.fm/archives/10847" style="color: #005fcc;">https://kexue.fm/archives/10847</a>
    </p>
    <p style="margin: 0 0 0.5em 0;">
        Original author: 苏剑林 (Su Jianlin)<br>
        Original publication: <a href="https://kexue.fm" style="color: #005fcc;">科学空间 (Scientific Spaces)</a>
    </p>
    <p style="margin: 0; font-style: italic;">
        Translated using Gemini 3 Flash. Please refer to the original for authoritative content.
    </p>
</footer>
