
    <style type="text/css">

    body {
    margin: 48px auto;
    max-width: 68ch;              /* character-based width reads better */
    padding: 0 16px;

    font-family: -apple-system, BlinkMacSystemFont, "Segoe UI",
                Roboto, "Helvetica Neue", Arial, sans-serif;
    font-size: 18px;
    line-height: 1.65;
    color: #333;
    background: #fafafa;
    }

    h1, h2, h3, h4 {
    line-height: 1.25;
    margin-top: 2.2em;
    margin-bottom: 0.6em;
    font-weight: 600;
    }

    h1 {
    font-size: 2.1em;
    margin-top: 0;
    }

    h2 {
    font-size: 1.6em;
    border-bottom: 1px solid #e5e5e5;
    padding-bottom: 0.3em;
    }

    h3 {
    font-size: 1.25em;
    }

    h4 {
    font-size: 1.05em;
    color: #555;
    }

    /* Paragraphs and lists */
    p {
    margin: 1em 0;
    }

    ul, ol {
    margin: 1em 0 1em 1.5em;
    }

    li {
    margin: 0.4em 0;
    }

    a {
    color: #005fcc;
    text-decoration: none;
    }

    a:hover {
    text-decoration: underline;
    }

    code {
    font-family: ui-monospace, SFMono-Regular, Menlo, Consolas, monospace;
    font-size: 0.95em;
    background: #f2f2f2;
    padding: 0.15em 0.35em;
    border-radius: 4px;
    }

    pre {
    font-family: ui-monospace, SFMono-Regular, Menlo, Consolas, monospace;
    font-size: 0.9em;
    background: #f5f5f5;
    padding: 1em 1.2em;
    overflow-x: auto;
    border-radius: 6px;
    line-height: 1.45;
    }

    pre code {
    background: none;
    padding: 0;
    }

    blockquote {
    margin: 1.5em 0;
    padding-left: 1em;
    border-left: 4px solid #ddd;
    color: #555;
    }

    hr {
    border: none;
    border-top: 1px solid #e0e0e0;
    margin: 3em 0;
    }

    table {
    border-collapse: collapse;
    margin: 1.5em 0;
    width: 100%;
    font-size: 0.95em;
    }

    th, td {
    padding: 0.5em 0.7em;
    border-bottom: 1px solid #e5e5e5;
    text-align: left;
    }

    th {
    font-weight: 600;
    }

    img {
    max-width: 100%;
    display: block;
    margin: 1.5em auto;
    }

    small {
    color: #666;
    }

    mjx-container {
    margin: 1em 0;
    }

    ::selection {
    background: #cce2ff;
    }

    </style>
    

<script>
window.MathJax = {
  tex: {
    inlineMath: [['$', '$'], ['\\(', '\\)']],
    displayMath: [['$$', '$$'], ['\\[', '\\]']],
    processEscapes: true,
    tags: 'ams'
  }
};
</script>
<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

<article>
    <h1><a href="https://kexue.fm/archives/4138">End-to-End Tencent CAPTCHA Recognition (46% Accuracy)</a></h1>
    <p>By 苏剑林 | December 14, 2016</p>

    <p><strong>For the latest results, please refer to: <a href="translation_4503.html">http://kexue.fm/archives/4503/</a></strong></p>

    <p>Some time ago, I was fortunate enough to obtain a batch of labeled Tencent CAPTCHA samples from a netizen (CAPTCHA sample URL: <a href="http://captcha.qq.com/getimage">http://captcha.qq.com/getimage</a>). So, I took some time to test a CAPTCHA recognition model.</p>

    <p>
        <a href="https://kexue.fm/usr/uploads/2016/12/457376744.jpeg" title="Click to view original image">
            <img src="https://kexue.fm/usr/uploads/2016/12/457376744.jpeg" alt="Tencent CAPTCHA" style="max-width:100%;">
        </a>
        <br>
        Tencent CAPTCHA
    </p>

    <h3 id="Samples">Samples</h3>
    <p>This batch of CAPTCHAs is relatively simple, consisting of 4-digit English letters. They include both upper and lower case, but input is case-insensitive. The patterns have some level of confusion, and traditional segmentation-based solutions are likely difficult to implement. The end-to-end solution directly inputs the CAPTCHA, passes it through several convolutional layers, and then connects to several classifiers (26-way classification) to directly output four letter labels. In fact, there isn't much more to say; if you have samples, you can do it. Moreover, this framework is universal; it can be used for case-sensitive scenarios (52-way classification) or alphanumeric mixtures (just add 10 more categories).</p>

    <p>However, there is one thing I find quite difficult to handle: the labels are case-insensitive. In this batch of samples, the labels are all lowercase, but the CAPTCHAs in the images have both uppercase and lowercase letters. Thus, if it's only a 26-way classification, we are forced to treat 'A' and 'a' as the same category. Since 'A' and 'a' look quite different visually, forcing them into one category seems to be "asking too much of the model"... I suspect this is one of the reasons why my model's accuracy isn't higher. But I don't have a better idea yet.</p>

    <h3 id="Code">Code</h3>
    <p>Without further ado, here is the code:</p>

    <p><a href="https://github.com/bojone/n2n-ocr-for-qqcaptcha">https://github.com/bojone/n2n-ocr-for-qqcaptcha</a></p>

    <p>The model is very concise and conventional (it’s just a single file, how complex could it be?).</p>

    <p>Basically, four convolutional layers are used to extract image features, and then these image features are connected to four separate softmax layers, each categorized into 26 classes. Note that you cannot simply use <code>TimeDistributed</code> for convenience here; <code>TimeDistributed</code> shares weights, whereas here we need to output different labels for the same feature. If the weights were the same, wouldn't the results be the same? Also, note that I am using Keras with Theano as the backend, not TensorFlow. The two handle image data differently, so TensorFlow users will need to adjust accordingly.</p>

<pre>
_________________________________________________________________
Layer (type)                 Output Shape              Param #   Connected to                     
=================================================================
input_15 (InputLayer)        (None, 3, 129, 53)        0                                          
_________________________________________________________________
convolution2d_40 (Convolution(None, 32, 127, 51)       896       input_15[0][0]                   
_________________________________________________________________
maxpooling2d_48 (MaxPooling2D(None, 32, 63, 25)        0         convolution2d_40[0][0]           
_________________________________________________________________
convolution2d_41 (Convolution(None, 32, 61, 23)        9248      maxpooling2d_48[0][0]            
_________________________________________________________________
maxpooling2d_49 (MaxPooling2D(None, 32, 30, 11)        0         convolution2d_41[0][0]           
_________________________________________________________________
activation_37 (Activation)   (None, 32, 30, 11)        0         maxpooling2d_49[0][0]            
_________________________________________________________________
convolution2d_42 (Convolution(None, 32, 28, 9)         9248      activation_37[0][0]              
_________________________________________________________________
maxpooling2d_50 (MaxPooling2D(None, 32, 14, 4)         0         convolution2d_42[0][0]           
_________________________________________________________________
activation_38 (Activation)   (None, 32, 14, 4)         0         maxpooling2d_50[0][0]            
_________________________________________________________________
convolution2d_43 (Convolution(None, 32, 12, 2)         9248      activation_38[0][0]              
_________________________________________________________________
maxpooling2d_51 (MaxPooling2D(None, 32, 6, 1)          0         convolution2d_43[0][0]           
_________________________________________________________________
flatten_15 (Flatten)         (None, 192)               0         maxpooling2d_51[0][0]            
_________________________________________________________________
activation_39 (Activation)   (None, 192)               0         flatten_15[0][0]                 
_________________________________________________________________
dense_63 (Dense)             (None, 26)                5018      activation_39[0][0]              
_________________________________________________________________
dense_64 (Dense)             (None, 26)                5018      activation_39[0][0]              
_________________________________________________________________
dense_65 (Dense)             (None, 26)                5018      activation_39[0][0]              
_________________________________________________________________
dense_66 (Dense)             (None, 26)                5018      activation_39[0][0]              
=================================================================
Total params: 48,712
_________________________________________________________________
</pre>

    <p>After several dozen rounds of training, the model obtained recognition accuracies for the 1st, 2nd, 3rd, and 4th characters of 0.89, 0.72, 0.73, and 0.87, respectively. Thus, the accuracy of getting all four correct should be:</p>

    $$0.89 \times 0.72 \times 0.73 \times 0.87 \approx 0.41$$

    <p>That is, there should be about a 41% overall accuracy. After actual testing, the performance was even slightly better, with an overall accuracy of 46%. This means roughly one out of every two images is recognized correctly, which should be practical in many situations. Of course, this accuracy is specific to this batch of samples; the actual accuracy might be lower, but I estimate it should at least be around 10%? ^_^</p>

    <p>It is not convenient to make the training samples public, and the model weights are also not directly public. If you need them, please contact me privately.</p>

    <h3 id="Afterword">Afterword</h3>
    <p>According to the friend who sent me the samples, he is currently using an interface provided by someone else that has an overall accuracy of over 95%. I was instantly filled with respect and really wanted to learn from that person. However, that program is already commercialized, and it’s unlikely I'll be allowed to observe it. I expect they have been focused specifically on Tencent CAPTCHA recognition for a long time, unlike me, who is "broad but not deep."</p>

    <p>Readers are welcome to provide better modeling ideas. Please feel free to give advice. The current model has fewer than 50,000 parameters and might be underfitting. I will try to tune it further when I have time.</p>
</article>
<hr>
<footer style="margin-top: 3em; padding: 1.5em; background: #f5f5f5; border-radius: 8px; font-size: 0.9em; color: #555;">
    <p style="margin: 0 0 0.5em 0;"><strong>Citation</strong></p>
    <p style="margin: 0 0 0.5em 0;">
        This is a machine translation of the original Chinese article:<br>
        <a href="translation_4138.html" style="color: #005fcc;">https://kexue.fm/archives/4138</a>
    </p>
    <p style="margin: 0 0 0.5em 0;">
        Original author: 苏剑林 (Su Jianlin)<br>
        Original publication: <a href="https://kexue.fm" style="color: #005fcc;">科学空间 (Scientific Spaces)</a>
    </p>
    <p style="margin: 0; font-style: italic;">
        Translated using Gemini 3 Flash. Please refer to the original for authoritative content.
    </p>
</footer>
