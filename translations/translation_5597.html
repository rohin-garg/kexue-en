
    <style type="text/css">

    body {
    margin: 48px auto;
    max-width: 68ch;              /* character-based width reads better */
    padding: 0 16px;

    font-family: -apple-system, BlinkMacSystemFont, "Segoe UI",
                Roboto, "Helvetica Neue", Arial, sans-serif;
    font-size: 18px;
    line-height: 1.65;
    color: #333;
    background: #fafafa;
    }

    h1, h2, h3, h4 {
    line-height: 1.25;
    margin-top: 2.2em;
    margin-bottom: 0.6em;
    font-weight: 600;
    }

    h1 {
    font-size: 2.1em;
    margin-top: 0;
    }

    h2 {
    font-size: 1.6em;
    border-bottom: 1px solid #e5e5e5;
    padding-bottom: 0.3em;
    }

    h3 {
    font-size: 1.25em;
    }

    h4 {
    font-size: 1.05em;
    color: #555;
    }

    /* Paragraphs and lists */
    p {
    margin: 1em 0;
    }

    ul, ol {
    margin: 1em 0 1em 1.5em;
    }

    li {
    margin: 0.4em 0;
    }

    a {
    color: #005fcc;
    text-decoration: none;
    }

    a:hover {
    text-decoration: underline;
    }

    code {
    font-family: ui-monospace, SFMono-Regular, Menlo, Consolas, monospace;
    font-size: 0.95em;
    background: #f2f2f2;
    padding: 0.15em 0.35em;
    border-radius: 4px;
    }

    pre {
    font-family: ui-monospace, SFMono-Regular, Menlo, Consolas, monospace;
    font-size: 0.9em;
    background: #f5f5f5;
    padding: 1em 1.2em;
    overflow-x: auto;
    border-radius: 6px;
    line-height: 1.45;
    }

    pre code {
    background: none;
    padding: 0;
    }

    blockquote {
    margin: 1.5em 0;
    padding-left: 1em;
    border-left: 4px solid #ddd;
    color: #555;
    }

    hr {
    border: none;
    border-top: 1px solid #e0e0e0;
    margin: 3em 0;
    }

    table {
    border-collapse: collapse;
    margin: 1.5em 0;
    width: 100%;
    font-size: 0.95em;
    }

    th, td {
    padding: 0.5em 0.7em;
    border-bottom: 1px solid #e5e5e5;
    text-align: left;
    }

    th {
    font-weight: 600;
    }

    img {
    max-width: 100%;
    display: block;
    margin: 1.5em auto;
    }

    small {
    color: #666;
    }

    mjx-container {
    margin: 1em 0;
    }

    ::selection {
    background: #cce2ff;
    }

    </style>
    

<html>
<head>
<script>
window.MathJax = {
  tex: {
    inlineMath: [['$', '$'], ['\\(', '\\)']],
    displayMath: [['$$', '$$'], ['\\[', '\\]']],
    processEscapes: true,
    tags: 'ams',
    packages: {'[+]': ['ams']}
  },
  options: {
    renderActions: {
      findScript: [10, function (doc) {
        for (const node of document.querySelectorAll('script[type^="math/tex"]')) {
          const display = !!node.type.match(/; *mode=display/);
          const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
          const text = document.createTextNode('');
          node.parentNode.replaceChild(text, node);
          math.start = {node: text, delim: '', n: 0};
          math.end = {node: text, delim: '', n: 0};
          doc.math.push(math);
        }
      }, '']
    }
  }
};
</script>
<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>
<body>

<article>
    <h1><a href="https://kexue.fm/archives/5597">NLP Library Based on Minimum Entropy Principle: nlp zero</a></h1>
    <p>By 苏剑林 | May 31, 2018</p>

    <p>I have written several blog posts about the <a href="https://kexue.fm/tag/%E6%9C%80%E5%B0%8F%E7%86%B5/">Minimum Entropy</a> principle, dedicated to some foundational work in unsupervised NLP. For the convenience of experimentation, I have encapsulated the algorithms mentioned in those articles into a library for interested readers to test and use.</p>

    <p>Since it is oriented towards unsupervised NLP scenarios and covers the basic tasks of NLP, it is named <strong>nlp zero</strong>.</p>

    <h2>Address</h2>
    <p>Github: <a href="https://github.com/bojone/nlp-zero">https://github.com/bojone/nlp-zero</a></p>
    <p>Pypi: <a href="https://pypi.org/project/nlp-zero/">https://pypi.org/project/nlp-zero/</a></p>

    <p>It can be installed directly via:</p>
<pre><code>pip install nlp-zero</code></pre>
    <p>The entire library is implemented in pure Python with no third-party dependencies, supporting both Python 2.x and 3.x.</p>

    <h2>Usage</h2>

    <h3>Default Tokenization</h3>
    <p>The library comes with a built-in dictionary that can be used as a simple tokenization tool.</p>
<pre><code>from nlp_zero import *
tokenizer = Tokenizer()
print(' '.join(tokenizer.tokenize(u'今天天气真好')))</code></pre>
    <p>The built-in dictionary includes some new words discovered through a new word discovery algorithm and has been manually optimized, so its quality is relatively high.</p>

    <h3>Lexicon Construction</h3>
    <p>Build a lexicon from a large volume of raw corpora.</p>
    <p>First, we need to write an iterator container so that we don't have to load the entire corpus into memory at once. The way to write the iterator is very flexible. For example, if my data is stored in MongoDB, it would be:</p>
<pre><code>class texts:
    def __iter__(self):
        for i in db.find():
            yield i['text']</code></pre>
    <p>If the data is stored in a text file, it would look something like this:</p>
<pre><code>class texts:
    def __iter__(self):
        for l in open('corpus.txt'):
            yield l.strip()</code></pre>
    <p>Then you can execute:</p>
<pre><code>word_count = WordCount()
word_count.count(texts())
word_count.save_words('words.csv')</code></pre>
    <p>View the results via Pandas:</p>
<pre><code>import pandas as pd
words = pd.read_csv('words.csv', encoding='utf-8', header=None)</code></pre>
    <p>Build a tokenization tool directly using the statistical lexicon:</p>
<pre><code>tokenizer = Tokenizer()
tokenizer.load_words('words.csv')</code></pre>

    <h3>Sentence Template Construction</h3>
    <p>As before, you also need to write an iterator, which I won't repeat here.</p>
    <p>Because building sentence templates is based on word statistics, a tokenization function is also required. You can use the built-in tokenizer or an external one, such as Jieba.</p>
<pre><code>template_count = TemplateCount()
template_count.count(texts(), tokenize=tokenizer.tokenize)
template_count.save_templates('templates.csv')</code></pre>
    <p>View the results via Pandas:</p>
<pre><code>templates = pd.read_csv('templates.csv', encoding='utf-8', header=None)</code></pre>
    <p>Each template has been encapsulated as a class.</p>

    <h3>Hierarchical Decomposition</h3>
    <p>Parsing sentence structures based on sentence templates.</p>
<pre><code>parser = Parser()
parser.load_templates('templates.csv')
tree = parser.parse(u'今天天气真好')
tree.show()</code></pre>
    <p>For convenience in calling and visualising the results, the output has been encapsulated into a <code>SentTree</code> class. This class has three attributes: <code>template</code> (the current main template), <code>content</code> (the string covered by the current main template), and <code>modules</code> (a list of semantic blocks, where each semantic block is also described by a <code>SentTree</code>). Overall, it is designed according to the assumptions about language structure we made in the article <a href="translation_5577.html">"The Principle of Minimum Entropy (III): 'Crossing the River's Elephant' - Sentence Templates and Language Structure"</a>.</p>

    <h2>To be continued</h2>
    <p>If necessary, please read the source code for answers~ Further updates will continue to be demonstrated here.</p>

    <hr />
    <p><em></em></p>

    <p><strong>If you found this article helpful, you are welcome to share or donate to this article. Donations are not for profit, but to let me know how many readers are truly following Scientific Spaces. Of course, if you ignore it, it will not affect your reading. Thank you again for visiting and for your support!</strong></p>

    </article>

<hr>
<footer style="margin-top: 3em; padding: 1.5em; background: #f5f5f5; border-radius: 8px; font-size: 0.9em; color: #555;">
    <p style="margin: 0 0 0.5em 0;"><strong>Citation</strong></p>
    <p style="margin: 0 0 0.5em 0;">
        This is a machine translation of the original Chinese article:<br>
        <a href="https://kexue.fm/archives/5597" style="color: #005fcc;">https://kexue.fm/archives/5597</a>
    </p>
    <p style="margin: 0 0 0.5em 0;">
        Original author: 苏剑林 (Su Jianlin)<br>
        Original publication: <a href="https://kexue.fm" style="color: #005fcc;">科学空间 (Scientific Spaces)</a>
    </p>
    <p style="margin: 0; font-style: italic;">
        Translated using Gemini 3 Flash. Please refer to the original for authoritative content.
    </p>
</footer>

</body>
</html>