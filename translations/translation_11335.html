
    <style type="text/css">

    body {
    margin: 48px auto;
    max-width: 68ch;              /* character-based width reads better */
    padding: 0 16px;

    font-family: -apple-system, BlinkMacSystemFont, "Segoe UI",
                Roboto, "Helvetica Neue", Arial, sans-serif;
    font-size: 18px;
    line-height: 1.65;
    color: #333;
    background: #fafafa;
    }

    h1, h2, h3, h4 {
    line-height: 1.25;
    margin-top: 2.2em;
    margin-bottom: 0.6em;
    font-weight: 600;
    }

    h1 {
    font-size: 2.1em;
    margin-top: 0;
    }

    h2 {
    font-size: 1.6em;
    border-bottom: 1px solid #e5e5e5;
    padding-bottom: 0.3em;
    }

    h3 {
    font-size: 1.25em;
    }

    h4 {
    font-size: 1.05em;
    color: #555;
    }

    /* Paragraphs and lists */
    p {
    margin: 1em 0;
    }

    ul, ol {
    margin: 1em 0 1em 1.5em;
    }

    li {
    margin: 0.4em 0;
    }

    a {
    color: #005fcc;
    text-decoration: none;
    }

    a:hover {
    text-decoration: underline;
    }

    code {
    font-family: ui-monospace, SFMono-Regular, Menlo, Consolas, monospace;
    font-size: 0.95em;
    background: #f2f2f2;
    padding: 0.15em 0.35em;
    border-radius: 4px;
    }

    pre {
    font-family: ui-monospace, SFMono-Regular, Menlo, Consolas, monospace;
    font-size: 0.9em;
    background: #f5f5f5;
    padding: 1em 1.2em;
    overflow-x: auto;
    border-radius: 6px;
    line-height: 1.45;
    }

    pre code {
    background: none;
    padding: 0;
    }

    blockquote {
    margin: 1.5em 0;
    padding-left: 1em;
    border-left: 4px solid #ddd;
    color: #555;
    }

    hr {
    border: none;
    border-top: 1px solid #e0e0e0;
    margin: 3em 0;
    }

    table {
    border-collapse: collapse;
    margin: 1.5em 0;
    width: 100%;
    font-size: 0.95em;
    }

    th, td {
    padding: 0.5em 0.7em;
    border-bottom: 1px solid #e5e5e5;
    text-align: left;
    }

    th {
    font-weight: 600;
    }

    img {
    max-width: 100%;
    display: block;
    margin: 1.5em auto;
    }

    small {
    color: #666;
    }

    mjx-container {
    margin: 1em 0;
    }

    ::selection {
    background: #cce2ff;
    }

    </style>
    

<script>
window.MathJax = {
  tex: {
    inlineMath: [['$', '$'], ['\\(', '\\)']],
    displayMath: [['$$', '$$'], ['\\[', '\\]']],
    processEscapes: true,
    tags: 'ams',
    packages: {'[+]': ['base', 'ams', 'noerrors', 'noundefined']}
  },
  loader: {load: ['[tex]/ams']}
};
</script>
<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

<nav style="margin-bottom: 1.5em;">
    <a href="../index.html" style="display: inline-flex; align-items: center; color: #555; text-decoration: none; font-size: 0.95em;">
        <span style="margin-right: 0.3em;">&larr;</span> Back to Index
    </a>
</nav>

<h1><a href="https://kexue.fm/archives/11335">Fast Estimation of the Spectral Norm of Random Matrices</a></h1>

<p>By 苏剑林 | Oct 12, 2025</p>

<p>In the "Approximate Estimation" section of <a href="translation_10795.html">"Higher-Order MuP: Simpler but Smarter Spectral Condition Scaling"</a>, we "pre-spent" a conclusion: "For an $n\times m$ random matrix whose elements follow a standard normal distribution, its spectral norm is approximately $\sqrt{n}+\sqrt{m}$."</p>

<p>In this article, we supplement the discussion of this conclusion and provide a fast estimation method for the spectral norm of random matrices.</p>

<h2>Random Matrix Theory #</h2>

<p>Suppose we have a random matrix $\boldsymbol{W}\in\mathbb{R}^{n\times m}$, where each element is independently and identically sampled from the standard normal distribution $\mathcal{N}(0,1)$. We are required to estimate the spectral norm of $\boldsymbol{W}$, which is the largest singular value, and we take $\mathbb{E}[\|\boldsymbol{W}\|_2]$ as the final estimation result.</p>

<p>First, it should be pointed out that the characterization analysis of random matrices has formed a specialized branch. Regarding the spectral norm estimation of normal matrices, relevant keywords include "<a href="https://en.wikipedia.org/wiki/Marchenko%E2%80%93Pastur_distribution">Marchenko–Pastur distribution</a>", "Bai-Yin law", "Gordon’s theorem", etc. They contain detailed estimation results concerning the distribution of singular values. However, we do not intend to introduce these contents; instead, we will introduce a method to quickly derive $\sqrt{n}+\sqrt{m}$.</p>

<p>This method was simplified by the author based on Section 5.3.1 of <a href="https://papers.cool/arxiv/1011.3027">"Introduction to the non-asymptotic analysis of random matrices"</a>. In fact, it can only be considered a "popular science" explanation to help everyone quickly understand the origin of $\sqrt{n}+\sqrt{m}$. A rigorous proof would require the addition of many tedious and monotonous details, which we will not expand upon here.</p>

<h2>Spectral Norm Estimation #</h2>

<p>Our starting point is the identity
\begin{equation}\|\boldsymbol{W}\|_2 = \max_{\|\boldsymbol{u}\|=1, \|\boldsymbol{v}\|=1} \boldsymbol{u}^{\top}\boldsymbol{W} \boldsymbol{v}\label{eq:w-uv}\end{equation}
where $\boldsymbol{u}\in\mathbb{R}^n,\boldsymbol{v}\in\mathbb{R}^m$. Directly calculating $\|\boldsymbol{W}\|_2$ is usually not easy, so it is natural to think of looking for some kind of approximation. We consider the following two "half-finished products":
\begin{equation}\max_{\|\boldsymbol{u}\|=1} \boldsymbol{u}^{\top}\boldsymbol{W} \boldsymbol{v}\qquad\qquad \max_{\|\boldsymbol{v}\|=1} \boldsymbol{u}^{\top}\boldsymbol{W} \boldsymbol{v}\end{equation}
Intuitively, compared to Eq. $\eqref{eq:w-uv}$, the above two expressions only complete half of the work. We make a bold assumption: their results are also half of the complete result. Therefore, by superimposing them, we consider it a good approximation of the final result:
\begin{equation}\|\boldsymbol{W}\|_2 = \max_{\|\boldsymbol{u}\|=1, \|\boldsymbol{v}\|=1} \boldsymbol{u}^{\top}\boldsymbol{W} \boldsymbol{v} \approx \max_{\|\boldsymbol{u}\|=1} \boldsymbol{u}^{\top}\boldsymbol{W} \boldsymbol{v} + \max_{\|\boldsymbol{v}\|=1} \boldsymbol{u}^{\top}\boldsymbol{W} \boldsymbol{v} = \|\boldsymbol{W}\boldsymbol{v}\| + \|\boldsymbol{u}^{\top}\boldsymbol{W}\| \label{eq:core-approx}\end{equation}
That is to say, we sample $\boldsymbol{u}$ and $\boldsymbol{v}$ from the unit hyperspheres of $\mathbb{R}^n$ and $\mathbb{R}^m$ respectively, and then obtain an approximation of the spectral norm of $\boldsymbol{W}$ according to the above formula.</p>

<h2>Calculating the Expected Value #</h2>

<p>With this approximation, we can calculate
\begin{equation}\mathbb{E}[\|\boldsymbol{W}\|_2]\approx\mathbb{E}[\|\boldsymbol{W}\boldsymbol{v}\|] + \mathbb{E}[\|\boldsymbol{u}^{\top}\boldsymbol{W}\|] \approx \sqrt{\mathbb{E}[\|\boldsymbol{W}\boldsymbol{v}\|^2]} + \sqrt{\mathbb{E}[\|\boldsymbol{u}^{\top}\boldsymbol{W}\|^2]}\end{equation}
where
\begin{equation}\mathbb{E}[\|\boldsymbol{W}\boldsymbol{v}\|^2] = \mathbb{E}[\boldsymbol{v}^{\top}\boldsymbol{W}^{\top}\boldsymbol{W}\boldsymbol{v}] = \boldsymbol{v}^{\top}\mathbb{E}[\boldsymbol{W}^{\top}\boldsymbol{W}]\boldsymbol{v} = \boldsymbol{v}^{\top}(n\boldsymbol{I}_m)\boldsymbol{v} = n\end{equation}
Similarly, $\mathbb{E}[\|\boldsymbol{u}^{\top}\boldsymbol{W}\|^2]=m$, so
\begin{equation}\mathbb{E}[\|\boldsymbol{W}\|_2]\approx\sqrt{n} + \sqrt{m}\end{equation}
This result is actually very accurate (it can be verified through simulation experiments). Specifically, if $n=ka, m=kb$, where $a, b$ are constants, then
\begin{equation}\lim_{k\to\infty} \frac{\|\boldsymbol{W}\|_2}{\sqrt{n} + \sqrt{m}} = 1,\qquad \boldsymbol{W}\sim\mathcal{N}(0,1)\end{equation}
The reason it is so accurate is that we cheated—the most critical formula $\eqref{eq:core-approx}$ is essentially reverse-engineered from the known correct answer. Besides Eq. $\eqref{eq:core-approx}$, the only conditions we used were $\mathbb{E}[\boldsymbol{W}^{\top}\boldsymbol{W}]=n\boldsymbol{I}_m$ and $\mathbb{E}[\boldsymbol{W}\boldsymbol{W}^{\top}]=m\boldsymbol{I}_n$. Therefore, it can be considered that the above approximation holds for any distribution with zero mean and unit variance.</p>

<h2>Minimum Singular Value #</h2>

<p>The spectral norm is the largest singular value. In fact, we can use the same logic to estimate the minimum singular value. Of course, "minimum" here needs to be defined more strictly. Assuming $n\geq m$, the minimum singular value here refers to the $m$-th singular value of $\boldsymbol{W}$ ordered from largest to smallest, which is equal to
\begin{equation}\sigma_{\min}(\boldsymbol{W}) = \min_{\|\boldsymbol{v}\|=1}\max_{\|\boldsymbol{u}\|=1} \boldsymbol{u}^{\top}\boldsymbol{W} \boldsymbol{v}\end{equation}
Note that the positions and objects of $\min$ and $\max$ here cannot be interchanged. Similarly, we consider the sum of two "half-finished products" of $\min$ and $\max$ respectively as its approximation:
\begin{equation}\sigma_{\min}(\boldsymbol{W}) = \min_{\|\boldsymbol{v}\|=1}\max_{\|\boldsymbol{u}\|=1} \boldsymbol{u}^{\top}\boldsymbol{W} \boldsymbol{v}\approx \min_{\|\boldsymbol{v}\|=1}\boldsymbol{u}^{\top}\boldsymbol{W} \boldsymbol{v} + \max_{\|\boldsymbol{u}\|=1} \boldsymbol{u}^{\top}\boldsymbol{W} \boldsymbol{v} = -\|\boldsymbol{u}^{\top}\boldsymbol{W}\| + \|\boldsymbol{W}\boldsymbol{v}\| \end{equation}
The rest of the process is the same as before, and the result is $\mathbb{E}[\sigma_{\min}(\boldsymbol{W})]\approx\sqrt{n}-\sqrt{m}$.</p>

<h2>Final Notes #</h2>

<p>This article provides a quick heuristic approach to estimating the spectral norm of random matrices, or more strictly, a popular-science-style, heuristic explanation rather than a rigorous and accurate derivation. It has the potential to be made rigorous, but it would require adding many theoretical details, all of which have been skipped here.</p>

<hr>
<footer style="margin-top: 3em; padding: 1.5em; background: #f5f5f5; border-radius: 8px; font-size: 0.9em; color: #555;">
    <p style="margin: 0 0 0.5em 0;"><strong>Citation</strong></p>
    <p style="margin: 0 0 0.5em 0;">
        This is a machine translation of the original Chinese article:<br>
        <a href="https://kexue.fm/archives/11335" style="color: #005fcc;">https://kexue.fm/archives/11335</a>
    </p>
    <p style="margin: 0 0 0.5em 0;">
        Original author: 苏剑林 (Su Jianlin)<br>
        Original publication: <a href="https://kexue.fm" style="color: #005fcc;">科学空间 (Scientific Spaces)</a>
    </p>
    <p style="margin: 0; font-style: italic;">
        Translated using Gemini 3 Flash. Please refer to the original for authoritative content.
    </p>
</footer>
