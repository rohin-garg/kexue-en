
    <style type="text/css">

    body {
    margin: 48px auto;
    max-width: 68ch;              /* character-based width reads better */
    padding: 0 16px;

    font-family: -apple-system, BlinkMacSystemFont, "Segoe UI",
                Roboto, "Helvetica Neue", Arial, sans-serif;
    font-size: 18px;
    line-height: 1.65;
    color: #333;
    background: #fafafa;
    }

    h1, h2, h3, h4 {
    line-height: 1.25;
    margin-top: 2.2em;
    margin-bottom: 0.6em;
    font-weight: 600;
    }

    h1 {
    font-size: 2.1em;
    margin-top: 0;
    }

    h2 {
    font-size: 1.6em;
    border-bottom: 1px solid #e5e5e5;
    padding-bottom: 0.3em;
    }

    h3 {
    font-size: 1.25em;
    }

    h4 {
    font-size: 1.05em;
    color: #555;
    }

    /* Paragraphs and lists */
    p {
    margin: 1em 0;
    }

    ul, ol {
    margin: 1em 0 1em 1.5em;
    }

    li {
    margin: 0.4em 0;
    }

    a {
    color: #005fcc;
    text-decoration: none;
    }

    a:hover {
    text-decoration: underline;
    }

    code {
    font-family: ui-monospace, SFMono-Regular, Menlo, Consolas, monospace;
    font-size: 0.95em;
    background: #f2f2f2;
    padding: 0.15em 0.35em;
    border-radius: 4px;
    }

    pre {
    font-family: ui-monospace, SFMono-Regular, Menlo, Consolas, monospace;
    font-size: 0.9em;
    background: #f5f5f5;
    padding: 1em 1.2em;
    overflow-x: auto;
    border-radius: 6px;
    line-height: 1.45;
    }

    pre code {
    background: none;
    padding: 0;
    }

    blockquote {
    margin: 1.5em 0;
    padding-left: 1em;
    border-left: 4px solid #ddd;
    color: #555;
    }

    hr {
    border: none;
    border-top: 1px solid #e0e0e0;
    margin: 3em 0;
    }

    table {
    border-collapse: collapse;
    margin: 1.5em 0;
    width: 100%;
    font-size: 0.95em;
    }

    th, td {
    padding: 0.5em 0.7em;
    border-bottom: 1px solid #e5e5e5;
    text-align: left;
    }

    th {
    font-weight: 600;
    }

    img {
    max-width: 100%;
    display: block;
    margin: 1.5em auto;
    }

    small {
    color: #666;
    }

    mjx-container {
    margin: 1em 0;
    }

    ::selection {
    background: #cce2ff;
    }

    </style>
    

<script>
window.MathJax = {
  tex: {
    inlineMath: [['$', '$'], ['\\(', '\\)']],
    displayMath: [['$$', '$$'], ['\\[', '\\]']],
    processEscapes: true,
    tags: 'ams',
    packages: {'[+]': ['ams']}
  },
  options: {
    renderActions: {
      findScript: [10, function (doc) {
        for (const node of document.querySelectorAll('script[type^="math/tex"]')) {
          const display = !!node.type.match(/; *mode=display/);
          const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
          const text = document.createTextNode('');
          node.parentNode.replaceChild(text, node);
          math.start = {node: text, delim: '', n: 0};
          math.end = {node: text, delim: '', n: 0};
          doc.math.push(math);
        }
      }, '']
    }
  }
};
</script>
<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

<article>
    <h1><a href="https://kexue.fm/archives/4667">A More Unique Word Vector Model (I): simpler glove</a></h1>

    <p>By 苏剑林 | November 19, 2017</p>

    
    <div class="post-meta">
        By Su Jianlin | November 19, 2017 | 52,909 Readers
    </div>

    <p>If you ask me which is the most convenient and easiest-to-use word vector model, I think it should be word2vec. But if you ask me which is the most beautiful word vector model, I don't know; I feel that every model has its own deficiencies. Setting aside experimental results (which are often just a matter of evaluation metrics), purely from a theoretical perspective, no model can yet be called truly "beautiful."</p>

    <p>This article discusses some common questions regarding word vectors that many people are concerned about. Many conclusions have been discovered primarily through experimentation but lack reasonable explanations, including:</p>

    <blockquote>
        <p>How should one construct a word vector model?</p>
        <p>Why use cosine similarity for synonym search? What does the inner product of vectors represent?</p>
        <p>Does the norm (length) of a word vector have any special meaning?</p>
        <p>Why do word vectors possess word analogy properties? (King - Man + Woman = Queen)</p>
        <p>After obtaining word vectors, how should sentence vectors be constructed? What is the basis for using the sum of word vectors as a simple sentence vector?</p>
    </blockquote>

    <p>These discussions are both specific and general. Some explanations might be directly transferable to the interpretation of word vector properties in the GloVe model and the Skip-gram model. Readers are encouraged to try this for themselves.</p>

    <p>Centered around the discussion of these questions, this article proposes a new GloVe-like word vector model, here referred to as <strong>simpler glove</strong>. Based on modifications to Stanford's GloVe source code, an implementation is provided, with the specific code available on <a href="https://github.com/bojone/simpler_glove">Github</a>.</p>

    <p>Why improve GloVe? Certainly, the ideas behind GloVe are very inspiring. However, although it claims to rival or even surpass word2vec, it is fundamentally a somewhat flawed model (we will explain why it is problematic later). Therefore, there is room for improvement.</p>

    <p><strong>Content Overview:</strong></p>

    <ul>
        <li>1 Modeling Language
            <ul>
                <li>1.1 From Conditional Probability to Mutual Information</li>
                <li>1.2 Additivity of Mutual Information</li>
                <li>1.3 Interlude: Side Story</li>
            </ul>
        </li>
        <li>2 Models Describing Relationships
            <ul>
                <li>2.1 Geometric Word Vectors</li>
                <li>2.2 Airport - Airplane + Train = Train Station</li>
                <li>2.3 Model Form</li>
                <li>2.4 Forgetting Normalization</li>
            </ul>
        </li>
        <li>3 Solving the Model
            <ul>
                <li>3.1 Loss Function</li>
                <li>3.2 Mutual Information Estimation</li>
                <li>3.3 Weighting and Downsampling</li>
                <li>3.4 Adagrad</li>
            </ul>
        </li>
        <li>4 Interesting Results
            <ul>
                <li>4.1 Meaning of the Norm</li>
                <li>4.2 Word Analogy Experiments</li>
                <li>4.3 Ranking Related Words</li>
                <li>4.4 Redefining Similarity</li>
                <li>4.5 Keyword Extraction</li>
                <li>4.6 Sentence Similarity</li>
                <li>4.7 Sentence Vectors</li>
            </ul>
        </li>
    </ul>
</article>
<hr>
<footer style="margin-top: 3em; padding: 1.5em; background: #f5f5f5; border-radius: 8px; font-size: 0.9em; color: #555;">
    <p style="margin: 0 0 0.5em 0;"><strong>Citation</strong></p>
    <p style="margin: 0 0 0.5em 0;">
        This is a machine translation of the original Chinese article:<br>
        <a href="https://kexue.fm/archives/4667" style="color: #005fcc;">https://kexue.fm/archives/4667</a>
    </p>
    <p style="margin: 0 0 0.5em 0;">
        Original author: 苏剑林 (Su Jianlin)<br>
        Original publication: <a href="https://kexue.fm" style="color: #005fcc;">科学空间 (Scientific Spaces)</a>
    </p>
    <p style="margin: 0; font-style: italic;">
        Translated using Gemini 3 Flash. Please refer to the original for authoritative content.
    </p>
</footer>
