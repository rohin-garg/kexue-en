
    <style type="text/css">

    body {
    margin: 48px auto;
    max-width: 68ch;              /* character-based width reads better */
    padding: 0 16px;

    font-family: -apple-system, BlinkMacSystemFont, "Segoe UI",
                Roboto, "Helvetica Neue", Arial, sans-serif;
    font-size: 18px;
    line-height: 1.65;
    color: #333;
    background: #fafafa;
    }

    h1, h2, h3, h4 {
    line-height: 1.25;
    margin-top: 2.2em;
    margin-bottom: 0.6em;
    font-weight: 600;
    }

    h1 {
    font-size: 2.1em;
    margin-top: 0;
    }

    h2 {
    font-size: 1.6em;
    border-bottom: 1px solid #e5e5e5;
    padding-bottom: 0.3em;
    }

    h3 {
    font-size: 1.25em;
    }

    h4 {
    font-size: 1.05em;
    color: #555;
    }

    /* Paragraphs and lists */
    p {
    margin: 1em 0;
    }

    ul, ol {
    margin: 1em 0 1em 1.5em;
    }

    li {
    margin: 0.4em 0;
    }

    a {
    color: #005fcc;
    text-decoration: none;
    }

    a:hover {
    text-decoration: underline;
    }

    code {
    font-family: ui-monospace, SFMono-Regular, Menlo, Consolas, monospace;
    font-size: 0.95em;
    background: #f2f2f2;
    padding: 0.15em 0.35em;
    border-radius: 4px;
    }

    pre {
    font-family: ui-monospace, SFMono-Regular, Menlo, Consolas, monospace;
    font-size: 0.9em;
    background: #f5f5f5;
    padding: 1em 1.2em;
    overflow-x: auto;
    border-radius: 6px;
    line-height: 1.45;
    }

    pre code {
    background: none;
    padding: 0;
    }

    blockquote {
    margin: 1.5em 0;
    padding-left: 1em;
    border-left: 4px solid #ddd;
    color: #555;
    }

    hr {
    border: none;
    border-top: 1px solid #e0e0e0;
    margin: 3em 0;
    }

    table {
    border-collapse: collapse;
    margin: 1.5em 0;
    width: 100%;
    font-size: 0.95em;
    }

    th, td {
    padding: 0.5em 0.7em;
    border-bottom: 1px solid #e5e5e5;
    text-align: left;
    }

    th {
    font-weight: 600;
    }

    img {
    max-width: 100%;
    display: block;
    margin: 1.5em auto;
    }

    small {
    color: #666;
    }

    mjx-container {
    margin: 1em 0;
    }

    ::selection {
    background: #cce2ff;
    }

    </style>
    

<script>
window.MathJax = {
  tex: {
    inlineMath: [['$', '$'], ['\\(', '\\)']],
    displayMath: [['$$', '$$'], ['\\[', '\\]']],
    tags: 'ams',
    packages: {'[+]': ['ams']}
  }
};
</script>
<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

<h1><a href="https://kexue.fm/archives/9147">A Brief Analysis of the "Hubness Phenomenon" in the Curse of Dimensionality</a></h1>

<p>By 苏剑林 | June 28, 2022</p>

<p>In the past few days, I came across the paper <a href="https://papers.cool/arxiv/2206.06014">"Exploring and Exploiting Hubness Priors for High-Quality GAN Latent Sampling"</a> and learned a new term: the "Hubness phenomenon." It refers to an aggregation effect in high-dimensional space, which is essentially one of the manifestations of the "curse of dimensionality." Using the concept of Hubness, the paper derives a scheme to improve the generation quality of GAN models, which seems quite interesting. Therefore, I took the opportunity to learn more about the Hubness phenomenon and recorded my findings here for your reference.</p>

<h2 id="坍缩的球">The Collapsing Sphere</h2>

<p>"Curse of dimensionality" is a very broad concept. Any conclusion in high-dimensional space that differs significantly from its 2D or 3D counterparts can be called a "curse of dimensionality." For example, as introduced in <a href="translation_7076.html">"Distribution of the Angle Between Two Random Vectors in n-Dimensional Space,"</a> "any two vectors in a high-dimensional space are almost always orthogonal." Many manifestations of the curse of dimensionality share the same source: "The ratio of the volume of an $n$-dimensional unit sphere to its circumscribed hypercube collapses to 0 as the dimension increases." This includes the "Hubness phenomenon" discussed in this article.</p>

<p>In <a href="translation_3154.html">"Masterpieces of Nature: Calculating the Volume of an n-Dimensional Sphere,"</a> we derived the volume formula for an $n$-dimensional sphere. From it, we know that the volume of an $n$-dimensional unit sphere is:</p>

\begin{equation}V_n = \frac{\pi^{n/2}}{\Gamma\left(\frac{n}{2}+1\right)}\end{equation}

<p>The side length of the corresponding circumscribed hypercube is $2$, so its volume is naturally $2^n$. Therefore, the ratio of the volumes is $V_n / 2^n$. Its graph is shown below:</p>

<p><img src="https://kexue.fm/usr/uploads/2022/06/1517429447.png" alt="Ratio of the volume of an n-dimensional sphere to its circumscribed hypercube" title="Ratio of the volume of an n-dimensional sphere to its circumscribed hypercube"></p>

<p>As can be seen, as the dimension increases, this ratio quickly tends toward 0. A descriptive way to put this is: "As the dimension increases, the sphere becomes increasingly insignificant." This tells us that if we try to achieve uniform sampling within a sphere using "uniform distribution + rejection sampling," the efficiency in high-dimensional space will be extremely low (the rejection rate will be close to 100%). Another way to understand this is that "most points in a high-dimensional sphere are concentrated near the surface," and the ratio of the region from the center to the surface becomes smaller and smaller.</p>

<h2 id="Hubness现象">The Hubness Phenomenon</h2>

<p>Now, let's turn to the Hubness phenomenon. It states that when a batch of points is randomly selected in a high-dimensional space, "there are always some points that frequently appear in the $k$-nearest neighbors of other points."</p>

<p>How should we understand this? Suppose we have $N$ points $x_1, x_2, \dots, x_N$. For each $x_i$, we can find the $k$ points closest to it; these $k$ points are called the "$k$-neighbors of $x_i$." With this concept, we can count how many times each point appears in the $k$-neighbors of other points. This count is called the "Hub value." The larger the Hub value, the more likely the point is to appear in the $k$-neighbors of other points.</p>

<p>The Hubness phenomenon states: there are always a few points whose Hub values are significantly large. If Hub values represent "wealth," a vivid metaphor would be "80% of the wealth is concentrated in 20% of the people," and as the dimension increases, this "wealth gap" grows larger. If Hub values represent "connections," it can be compared to "there are always a few people in a community who possess extremely extensive social resources."</p>

<p>How does the Hubness phenomenon arise? It is actually related to the collapse of the $n$-dimensional sphere mentioned in the previous section. We know that the point with the smallest sum of squared distances to all points is exactly the mean point:</p>

\begin{equation}\frac{1}{N} \sum_{i=1}^N x_i = c^* = \mathop{\text{argmin}}_c \sum_{i=1}^N \Vert x_i - c\Vert^2\end{equation}

<p>This implies that points near the mean vector have smaller average distances to all points and have a better chance of becoming the $k$-neighbors of more points. The collapse of the $n$-dimensional sphere tells us that the "neighborhood near the mean vector"—a spherical neighborhood centered at the mean—occupies a very small proportion of the space. Consequently, we see the phenomenon where "very few points appear in the $k$-neighbors of many points." Of course, using the mean vector here is a more intuitive way to understand it; for general data points, points closer to the center of density will have larger Hub values.</p>

<h2 id="提升采样">Improving Sampling</h2>

<p>So, what does the GAN generation quality improvement scheme mentioned at the beginning have to do with the Hubness phenomenon? The paper <a href="https://papers.cool/arxiv/2206.06014">"Exploring and Exploiting Hubness Priors for High-Quality GAN Latent Sampling"</a> proposes a prior hypothesis: the larger the Hub value, the better the generation quality of the corresponding point.</p>

<p>Specifically, the general GAN sampling generation process is $z \sim \mathcal{N}(0,1), x=G(z)$. We can first sample $N$ points $z_1, z_2, \dots, z_N$ from $\mathcal{N}(0,1)$, and then calculate the Hub value of each sample point. The original paper found that the Hub value is positively correlated with generation quality, so they only keep sample points with a Hub value greater than or equal to a threshold $t$ for generation. This is an "ex-ante" screening strategy. The reference code is as follows:</p>

<pre><code>def get_z_samples(size, t=50):
    """Filter sampling results via Hub values"""
    Z = np.empty((0, z_dim))
    while len(Z) &lt; size:
        z = np.random.randn(size, z_dim)
        # Calculate Hub values for these sample points; this part is omitted
        hub_values = calculate_hub_values(z)
        z = z[hub_values &gt;= t]
        Z = np.concatenate([Z, z], 0)[:size]
        print('%s / %s' % (len(Z), size))
    return Z</code></pre>

<p>Why filter by Hub values? Based on the previous discussion, a larger Hub value means a point is closer to the sample center—or more accurately, the density center. This implies there are many surrounding neighbor points, making it unlikely to be an outlier that hasn't been adequately trained. Therefore, the sampling quality is relatively higher. Multiple experimental results in the paper confirm this conclusion.</p>

<p><img src="https://kexue.fm/usr/uploads/2022/06/430095810.jpg" alt="Comparison of generation quality based on Hub value filtering" title="Comparison of generation quality based on Hub value filtering"></p>
<p><i>Comparison of generation quality based on Hub value filtering</i></p>

<h2 id="文章小结">Article Summary</h2>

<p>This article briefly introduced the Hubness phenomenon in the "curse of dimensionality" and described its application in improving the generation quality of GANs.</p>
<hr>
<footer style="margin-top: 3em; padding: 1.5em; background: #f5f5f5; border-radius: 8px; font-size: 0.9em; color: #555;">
    <p style="margin: 0 0 0.5em 0;"><strong>Citation</strong></p>
    <p style="margin: 0 0 0.5em 0;">
        This is a machine translation of the original Chinese article:<br>
        <a href="translation_9147.html" style="color: #005fcc;">https://kexue.fm/archives/9147</a>
    </p>
    <p style="margin: 0 0 0.5em 0;">
        Original author: 苏剑林 (Su Jianlin)<br>
        Original publication: <a href="https://kexue.fm" style="color: #005fcc;">科学空间 (Scientific Spaces)</a>
    </p>
    <p style="margin: 0; font-style: italic;">
        Translated using Gemini 3 Flash. Please refer to the original for authoritative content.
    </p>
</footer>
