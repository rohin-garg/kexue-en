
    <style type="text/css">

    body {
    margin: 48px auto;
    max-width: 68ch;              /* character-based width reads better */
    padding: 0 16px;

    font-family: -apple-system, BlinkMacSystemFont, "Segoe UI",
                Roboto, "Helvetica Neue", Arial, sans-serif;
    font-size: 18px;
    line-height: 1.65;
    color: #333;
    background: #fafafa;
    }

    h1, h2, h3, h4 {
    line-height: 1.25;
    margin-top: 2.2em;
    margin-bottom: 0.6em;
    font-weight: 600;
    }

    h1 {
    font-size: 2.1em;
    margin-top: 0;
    }

    h2 {
    font-size: 1.6em;
    border-bottom: 1px solid #e5e5e5;
    padding-bottom: 0.3em;
    }

    h3 {
    font-size: 1.25em;
    }

    h4 {
    font-size: 1.05em;
    color: #555;
    }

    /* Paragraphs and lists */
    p {
    margin: 1em 0;
    }

    ul, ol {
    margin: 1em 0 1em 1.5em;
    }

    li {
    margin: 0.4em 0;
    }

    a {
    color: #005fcc;
    text-decoration: none;
    }

    a:hover {
    text-decoration: underline;
    }

    code {
    font-family: ui-monospace, SFMono-Regular, Menlo, Consolas, monospace;
    font-size: 0.95em;
    background: #f2f2f2;
    padding: 0.15em 0.35em;
    border-radius: 4px;
    }

    pre {
    font-family: ui-monospace, SFMono-Regular, Menlo, Consolas, monospace;
    font-size: 0.9em;
    background: #f5f5f5;
    padding: 1em 1.2em;
    overflow-x: auto;
    border-radius: 6px;
    line-height: 1.45;
    }

    pre code {
    background: none;
    padding: 0;
    }

    blockquote {
    margin: 1.5em 0;
    padding-left: 1em;
    border-left: 4px solid #ddd;
    color: #555;
    }

    hr {
    border: none;
    border-top: 1px solid #e0e0e0;
    margin: 3em 0;
    }

    table {
    border-collapse: collapse;
    margin: 1.5em 0;
    width: 100%;
    font-size: 0.95em;
    }

    th, td {
    padding: 0.5em 0.7em;
    border-bottom: 1px solid #e5e5e5;
    text-align: left;
    }

    th {
    font-weight: 600;
    }

    img {
    max-width: 100%;
    display: block;
    margin: 1.5em auto;
    }

    small {
    color: #666;
    }

    mjx-container {
    margin: 1em 0;
    }

    ::selection {
    background: #cce2ff;
    }

    </style>
    

<script>
window.MathJax = {
  tex: {
    inlineMath: [['$', '$'], ['\\(', '\\)']],
    displayMath: [['$$', '$$'], ['\\[', '\\]']],
    tags: 'ams'
  }
};
</script>
<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

<article>
    <nav style="margin-bottom: 1.5em;">
    <a href="../index.html" style="display: inline-flex; align-items: center; color: #555; text-decoration: none; font-size: 0.95em;">
        <span style="margin-right: 0.3em;">&larr;</span> Back to Index
    </a>
</nav>

    <h1><a href="https://kexue.fm/archives/4176">Acquiring and Processing Chinese Wikipedia Corpus</a></h1>
    <p>By 苏剑林 | January 06, 2017</p>

    <p>Among Chinese corpora, the one that is highest in quality and easiest to obtain is likely the Wikipedia Chinese corpus. Furthermore, Wikipedia is quite generous, packaging all entries every month (Download address here: <a href="https://dumps.wikimedia.org/zhwiki/">https://dumps.wikimedia.org/zhwiki/</a>) for the world to use. This is truly "taken from the people, given back to the people." Regrettably, due to the unreasonable blockade of the Great Firewall, the number of Chinese Wikipedia entries is currently only just over 910,000, while Baidu Baike and Hudong Baike both have tens of millions (the English Wikipedia also has over ten million). Despite this, it has not stopped Chinese Wikipedia from being arguably the highest quality Chinese corpus available. (Baidu Baike and Hudong Baike can only be obtained by crawling, and many records are of quite poor quality, often being fragments of mutual copying or even plagiarism.)</p>

    <h2>Threshold</h2>
    <p>While downloading is easy, there is a certain threshold to using the Wikipedia corpus. The raw material downloaded directly from Wikipedia is a compressed text package containing many HTML and Markdown markers, making it basically unusable directly. Fortunately, enthusiastic experts have already written processing tools, mainly two: 1. <a href="http://medialab.di.unipi.it/wiki/Wikipedia_Extractor">Wikipedia Extractor</a>; 2. gensim's wikicorpus library. Both are based on Python.</p>

    <p>However, neither of these two mainstream processing methods satisfies me. First, the results extracted by Wikipedia Extractor remove content inside <code>{{}}</code> markers, which leads to the following situation:</p>
    <blockquote>
        In Western languages, the word "mathematics" (; ) originates from Ancient Greek ()
    </blockquote>
    <p>This happens because the words inside the parentheses contained <code>{{}}</code> markers and were cleared. Following common online tutorials to use <code>gensim.corpora.wikicorpus.WikiCorpus</code> directly is even more problematic, as it removes all punctuation. For a person with a "quality obsession" pursuing a high-quality corpus, this is unacceptable. Therefore, I wrote a processing script myself by combining gensim.</p>

    <h2>Code</h2>
    <pre><code class="language-python"># (The original post contains a Python script here)
</code></pre>

    <h2>Notes</h2>
    <p>As you can see, the main part of the code consists of regular expressions. First, we use <code>bz2file</code> to read the downloaded corpus without decompressing it, and then use gensim's <code>extract_pages</code> to extract each page. After extraction, we first handle some special non-text markers on the page, then replace some useful <code>{{}}</code> markers with <code>[[]]</code>, because <code>[[]]</code> markers will not be completely cleared (readers should test the specific principle themselves). Then, we use gensim's <code>filter_wiki</code> function for direct cleaning. Next, we address line break issues, and finally, we use <code>opencc</code> to convert Traditional Chinese to Simplified Chinese.</p>

    <p>In the subsequent loop, the condition <code>re.findall('^[a-zA-Z]+:', d[0])</code> is used to remove help pages, and <code>re.findall(u'^#', d[1])</code> is used to remove redirection pages. In the end, approximately 919,000 pages are obtained. <code>tqdm</code> is used to display progress; this is a must-have. The program ran for about 40 minutes on my machine, resulting in a pure text corpus of about 1.5G. Running time is not critical because preprocessing is a one-time task.</p>

    <p>It is worth noting that <code>opencc</code> should not be installed using <code>sudo apt-get install opencc</code>, as the default version is too low. You should install it from source and then use <code>pip install opencc</code> to install the Python interface. If calling <code>opencc</code> in Python results in a "Segmentation fault," you should run:</p>
    <pre><code class="language-bash"># (Original post provides a bash fix here)
</code></pre>

    <h2>Byproduct</h2>
    <p>The redirection mentioned above implies that two words have the same meaning. I have extracted all redirections from the Chinese Wikipedia and created a matching table. This means that both words in each line of the word list have the same meaning. This can be considered a byproduct.</p>

    <p>Synonym table based on Chinese Wikipedia redirections: <a href="https://kexue.fm/usr/uploads/2017/01/4014947738.7z">wiki_cn_mapping.7z</a></p>
</article>
<hr>
<footer style="margin-top: 3em; padding: 1.5em; background: #f5f5f5; border-radius: 8px; font-size: 0.9em; color: #555;">
    <p style="margin: 0 0 0.5em 0;"><strong>Citation</strong></p>
    <p style="margin: 0 0 0.5em 0;">
        This is a machine translation of the original Chinese article:<br>
        <a href="https://kexue.fm/archives/4176" style="color: #005fcc;">https://kexue.fm/archives/4176</a>
    </p>
    <p style="margin: 0 0 0.5em 0;">
        Original author: 苏剑林 (Su Jianlin)<br>
        Original publication: <a href="https://kexue.fm" style="color: #005fcc;">科学空间 (Scientific Spaces)</a>
    </p>
    <p style="margin: 0; font-style: italic;">
        Translated using Gemini 3 Flash. Please refer to the original for authoritative content.
    </p>
</footer>
