
    <style type="text/css">

    body {
    margin: 48px auto;
    max-width: 68ch;              /* character-based width reads better */
    padding: 0 16px;

    font-family: -apple-system, BlinkMacSystemFont, "Segoe UI",
                Roboto, "Helvetica Neue", Arial, sans-serif;
    font-size: 18px;
    line-height: 1.65;
    color: #333;
    background: #fafafa;
    }

    h1, h2, h3, h4 {
    line-height: 1.25;
    margin-top: 2.2em;
    margin-bottom: 0.6em;
    font-weight: 600;
    }

    h1 {
    font-size: 2.1em;
    margin-top: 0;
    }

    h2 {
    font-size: 1.6em;
    border-bottom: 1px solid #e5e5e5;
    padding-bottom: 0.3em;
    }

    h3 {
    font-size: 1.25em;
    }

    h4 {
    font-size: 1.05em;
    color: #555;
    }

    /* Paragraphs and lists */
    p {
    margin: 1em 0;
    }

    ul, ol {
    margin: 1em 0 1em 1.5em;
    }

    li {
    margin: 0.4em 0;
    }

    a {
    color: #005fcc;
    text-decoration: none;
    }

    a:hover {
    text-decoration: underline;
    }

    code {
    font-family: ui-monospace, SFMono-Regular, Menlo, Consolas, monospace;
    font-size: 0.95em;
    background: #f2f2f2;
    padding: 0.15em 0.35em;
    border-radius: 4px;
    }

    pre {
    font-family: ui-monospace, SFMono-Regular, Menlo, Consolas, monospace;
    font-size: 0.9em;
    background: #f5f5f5;
    padding: 1em 1.2em;
    overflow-x: auto;
    border-radius: 6px;
    line-height: 1.45;
    }

    pre code {
    background: none;
    padding: 0;
    }

    blockquote {
    margin: 1.5em 0;
    padding-left: 1em;
    border-left: 4px solid #ddd;
    color: #555;
    }

    hr {
    border: none;
    border-top: 1px solid #e0e0e0;
    margin: 3em 0;
    }

    table {
    border-collapse: collapse;
    margin: 1.5em 0;
    width: 100%;
    font-size: 0.95em;
    }

    th, td {
    padding: 0.5em 0.7em;
    border-bottom: 1px solid #e5e5e5;
    text-align: left;
    }

    th {
    font-weight: 600;
    }

    img {
    max-width: 100%;
    display: block;
    margin: 1.5em auto;
    }

    small {
    color: #666;
    }

    mjx-container {
    margin: 1em 0;
    }

    ::selection {
    background: #cce2ff;
    }

    </style>
    

<script>
  window.MathJax = {
    tex: {
      inlineMath: [['$', '$'], ['\\(', '\\)']],
      displayMath: [['$$', '$$'], ['\\[', '\\]']],
      processEscapes: true,
      tags: 'ams',
      packages: {'[+]': ['ams']}
    },
    options: {
      ignoreHtmlClass: 'tex2jax_ignore',
      processHtmlClass: 'tex2jax_process'
    }
  };
</script>
<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

<article>
    <h1><a href="https://kexue.fm/archives/11056">What can the matrix sign function mcsgn compute?</a></h1>
    <p>By 苏剑林 | June 23, 2025</p>

    <p>In the article <a href="translation_11025.html">"The Derivative of msign"</a>, we formally introduced two matrix sign functions $\newcommand{msign}{\mathop{\text{msign}}}\msign$ and $\newcommand{mcsgn}{\mathop{\text{mcsgn}}}\mcsgn$, where $\msign$ is the core operation of Muon, while $\mcsgn$ is used to solve the <a href="https://en.wikipedia.org/wiki/Sylvester_equation">Sylvester equation</a>. So, what else can $\mcsgn$ do besides solving the Sylvester equation? This article aims to summarize the answers to this question.</p>

    <h2>Two Signs</h2>
    <p>Let the matrix $\boldsymbol{M}\in\mathbb{R}^{n\times m}$. We have two types of matrix sign functions:</p>

    \begin{gather}
    \msign(\boldsymbol{M}) = (\boldsymbol{M}\boldsymbol{M}^{\top})^{-1/2}\boldsymbol{M}= \boldsymbol{M}(\boldsymbol{M}^{\top}\boldsymbol{M})^{-1/2} \\[6pt]
    \mcsgn(\boldsymbol{M}) = (\boldsymbol{M}^2)^{-1/2}\boldsymbol{M}= \boldsymbol{M}(\boldsymbol{M}^2)^{-1/2}
    \end{gather}

    <p>The first type is applicable to matrices of any shape, while the second type is only applicable to square matrices. The exponent $^{-1/2}$ refers to the inverse of the matrix square root; if it is not invertible, it is calculated according to the "<a href="translation_10366.html">Pseudo-inverse</a>". Generally, $\msign$ and $\mcsgn$ yield different results, but they are equal when $\boldsymbol{M}$ is a symmetric matrix.</p>

    <p>The difference between them is: if $\boldsymbol{M}=\boldsymbol{U}\boldsymbol{\Sigma}\boldsymbol{V}^{\top}$ where $\boldsymbol{U},\boldsymbol{V}$ are orthogonal matrices, then $\msign(\boldsymbol{M}) = \boldsymbol{U}\msign(\boldsymbol{\Sigma})\boldsymbol{V}^{\top}$; if $\boldsymbol{M}=\boldsymbol{P}\boldsymbol{\Lambda}\boldsymbol{P}^{-1}$ where $\boldsymbol{P}$ is an invertible matrix, then $\mcsgn(\boldsymbol{M})=\boldsymbol{P}\mcsgn(\boldsymbol{\Lambda})\boldsymbol{P}^{-1}$. Simply put, one possesses orthogonal invariance while the other possesses similarity invariance; one turns all non-zero singular values into 1, while the other turns all non-zero eigenvalues into $\pm 1$.</p>

    <p>Regarding the calculation of $\msign$, you can refer to <a href="translation_10922.html">"Newton-Schulz Iteration for msign Operator (Part 1)"</a> and <a href="translation_10996.html">"Newton-Schulz Iteration for msign Operator (Part 2)"</a>, which is GPU-efficient. As for $\mcsgn$, since eigenvalues can be complex numbers, the general case can be quite complicated. However, when the eigenvalues of $\boldsymbol{M}$ are all real (which is the case in almost all scenarios where $\mcsgn$ is used), the $\msign$ iteration can be reused:</p>

    \begin{equation}\newcommand{tr}{\mathop{\text{tr}}}\boldsymbol{X}_0 = \frac{\boldsymbol{M}}{\sqrt{\tr(\boldsymbol{M}^2)}},\qquad \boldsymbol{X}_{t+1} = a_{t+1}\boldsymbol{X}_t + b_{t+1}\boldsymbol{X}_t^3 + c_{t+1}\boldsymbol{X}_t^5\end{equation}

    <p>We won't expand further on more properties; next, we mainly look at the applications of $\mcsgn$.</p>

    <h2>Block Identities</h2>
    <p>Historically, $\mcsgn$ was introduced to solve equations—not just the Sylvester equation, but also the more general <a href="https://en.wikipedia.org/wiki/Algebraic_Riccati_equation">Algebraic Riccati Equation</a>. The original paper is <a href="https://www.sciencedirect.com/science/article/pii/0024379587902229">"Solving the algebraic Riccati equation with the matrix sign function"</a>.</p>

    <p>Consider the block matrix $\begin{bmatrix}\boldsymbol{X} & -\boldsymbol{I} \\ \boldsymbol{I} & \boldsymbol{0}\end{bmatrix}$. We have $\begin{bmatrix}\boldsymbol{X} & -\boldsymbol{I} \\ \boldsymbol{I} & \boldsymbol{0}\end{bmatrix}^{-1} = \begin{bmatrix}\boldsymbol{0} & \boldsymbol{I} \\ -\boldsymbol{I} & \boldsymbol{X}\end{bmatrix}$. It can be verified that:</p>

    \begin{equation}\begin{bmatrix}\boldsymbol{0} & \boldsymbol{I} \\ -\boldsymbol{I} & \boldsymbol{X}\end{bmatrix}\begin{bmatrix}\boldsymbol{A} & \boldsymbol{C} \\ \boldsymbol{D} & \boldsymbol{B}\end{bmatrix}\begin{bmatrix}\boldsymbol{X} & -\boldsymbol{I} \\ \boldsymbol{I} & \boldsymbol{0}\end{bmatrix}=\begin{bmatrix}\boldsymbol{B} + \boldsymbol{D}\boldsymbol{X} & -\boldsymbol{D} \\ \boldsymbol{X}\boldsymbol{D}\boldsymbol{X} + \boldsymbol{X}\boldsymbol{B} - \boldsymbol{A}\boldsymbol{X} - \boldsymbol{C} & \boldsymbol{A} - \boldsymbol{X}\boldsymbol{D}\end{bmatrix}\end{equation}

    <p>If</p>

    \begin{equation}\boldsymbol{X}\boldsymbol{D}\boldsymbol{X} + \boldsymbol{X}\boldsymbol{B} - \boldsymbol{A}\boldsymbol{X} - \boldsymbol{C} = \boldsymbol{0}\label{eq:riccati}\end{equation}

    <p>then</p>

    \begin{equation}\begin{bmatrix}\boldsymbol{A} & \boldsymbol{C} \\ \boldsymbol{D} & \boldsymbol{B}\end{bmatrix}=\begin{bmatrix}\boldsymbol{X} & -\boldsymbol{I} \\ \boldsymbol{I} & \boldsymbol{0}\end{bmatrix}\begin{bmatrix}\boldsymbol{B} + \boldsymbol{D}\boldsymbol{X} & -\boldsymbol{D} \\ \boldsymbol{0} & \boldsymbol{A} - \boldsymbol{X}\boldsymbol{D}\end{bmatrix}\begin{bmatrix}\boldsymbol{X} & -\boldsymbol{I} \\ \boldsymbol{I} & \boldsymbol{0}\end{bmatrix}^{-1}\end{equation}

    <p>Equation \eqref{eq:riccati} is the Algebraic Riccati Equation. Taking the $\mcsgn$ of both sides, we have the identity:</p>

    \begin{equation}\begin{aligned}
    \mcsgn\left(\begin{bmatrix}\boldsymbol{A} & \boldsymbol{C} \\ \boldsymbol{D} & \boldsymbol{B}\end{bmatrix}\right)=&\,\begin{bmatrix}\boldsymbol{X} & -\boldsymbol{I} \\ \boldsymbol{I} & \boldsymbol{0}\end{bmatrix}\mcsgn\left(\begin{bmatrix}\boldsymbol{B} + \boldsymbol{D}\boldsymbol{X} & -\boldsymbol{D} \\ \boldsymbol{0} & \boldsymbol{A} - \boldsymbol{X}\boldsymbol{D}\end{bmatrix}\right)\begin{bmatrix}\boldsymbol{X} & - \boldsymbol{I} \\ \boldsymbol{I} & \boldsymbol{0}\end{bmatrix}^{-1} \\[6pt]
    =&\,\begin{bmatrix}\boldsymbol{X} & -\boldsymbol{I} \\ \boldsymbol{I} & \boldsymbol{0}\end{bmatrix}\begin{bmatrix}\mcsgn(\boldsymbol{B} + \boldsymbol{D}\boldsymbol{X}) & \boldsymbol{Y} \\ \boldsymbol{0} & \mcsgn(\boldsymbol{A} - \boldsymbol{X}\boldsymbol{D})\end{bmatrix}\begin{bmatrix}\boldsymbol{X} & -\boldsymbol{I} \\ \boldsymbol{I} & \boldsymbol{0}\end{bmatrix}^{-1}
    \end{aligned}\end{equation}

    <p>The second equality utilizes the properties of (block) triangular matrices. The eigenvalues of a triangular matrix are its diagonal elements, so taking the $\mcsgn$ of a triangular matrix also results in a triangular matrix, where the diagonal elements equal the $\mathop{\text{csgn}}$ of the original diagonal elements. This property also holds for block triangular matrices, thus the result takes the form of the second equality, where $\boldsymbol{Y}$ is an undetermined matrix.</p>

    <h2>Some Results</h2>
    <p>Below we further simplify based on specific situations to obtain some results that might be useful.</p>

    <h3>First Example</h3>
    <p>Assume $\boldsymbol{D}=\boldsymbol{0}$, $\boldsymbol{B}$ is positive definite, and $\boldsymbol{A}$ is negative definite. The operation on block diagonal matrices is closed, so $\boldsymbol{Y}=\boldsymbol{0}$. Then:</p>

    \begin{equation}\mcsgn\left(\begin{bmatrix}\boldsymbol{A} & \boldsymbol{C} \\ \boldsymbol{0} & \boldsymbol{B}\end{bmatrix}\right)=\begin{bmatrix}\boldsymbol{X} & -\boldsymbol{I} \\ \boldsymbol{I} & \boldsymbol{0}\end{bmatrix}\begin{bmatrix}\boldsymbol{I} & \boldsymbol{0} \\ \boldsymbol{0} & -\boldsymbol{I}\end{bmatrix}\begin{bmatrix}\boldsymbol{X} & -\boldsymbol{I} \\ \boldsymbol{I} & \boldsymbol{0}\end{bmatrix}^{-1} = \begin{bmatrix}-\boldsymbol{I} & 2\boldsymbol{X} \\ \boldsymbol{0} & \boldsymbol{I}\end{bmatrix}\end{equation}

    <p>This means that the solution to the Sylvester equation $\boldsymbol{X}\boldsymbol{B} - \boldsymbol{A}\boldsymbol{X} = \boldsymbol{C}$ can be read directly from $\mcsgn\left(\begin{bmatrix}\boldsymbol{A} & \boldsymbol{C} \\ \boldsymbol{0} & \boldsymbol{B}\end{bmatrix}\right)$.</p>

    <h3>Second Example</h3>
    <p>Assume $\boldsymbol{A},\boldsymbol{B}=\boldsymbol{0}$, $\boldsymbol{D}=\boldsymbol{I}$, and $\boldsymbol{C}$ is a positive definite matrix. Then the Riccati equation simplifies to $\boldsymbol{X}^2 = \boldsymbol{C}$, i.e., $\boldsymbol{X}=\boldsymbol{C}^{1/2}$. Thus $\mcsgn(\boldsymbol{C}^{1/2})=\boldsymbol{I}$, so:</p>

    \begin{equation}\mcsgn\left(\begin{bmatrix}\boldsymbol{0} & \boldsymbol{C} \\ \boldsymbol{I} & \boldsymbol{0}\end{bmatrix}\right)=\begin{bmatrix}\boldsymbol{X} & -\boldsymbol{I} \\ \boldsymbol{I} & \boldsymbol{0}\end{bmatrix}\begin{bmatrix}\boldsymbol{I} & \boldsymbol{Y} \\ \boldsymbol{0} & - \boldsymbol{I}\end{bmatrix}\begin{bmatrix}\boldsymbol{X} & -\boldsymbol{I} \\ \boldsymbol{I} & \boldsymbol{0}\end{bmatrix}^{-1} = \begin{bmatrix}-\boldsymbol{X}\boldsymbol{Y}-\boldsymbol{I} & 2\boldsymbol{X} + \boldsymbol{X}\boldsymbol{Y}\boldsymbol{X} \\ -\boldsymbol{Y} & \boldsymbol{Y}\boldsymbol{X} + \boldsymbol{I}\end{bmatrix}\end{equation}

    <p>Note that $\mcsgn$ is an odd function; an odd function of an anti-diagonal matrix must also be an anti-diagonal matrix. Therefore $\boldsymbol{Y}\boldsymbol{X} + \boldsymbol{I}=\boldsymbol{0}$, from which we solve $\boldsymbol{Y} = -\boldsymbol{X}^{-1} = -\boldsymbol{C}^{-1/2}$. Substituting this back into the above equation, we get:</p>

    \begin{equation}\mcsgn\left(\begin{bmatrix}\boldsymbol{0} & \boldsymbol{C} \\ \boldsymbol{I} & \boldsymbol{0}\end{bmatrix}\right)=\begin{bmatrix}\boldsymbol{0} & \boldsymbol{C}^{1/2} \\ \boldsymbol{C}^{-1/2} & \boldsymbol{0}\end{bmatrix}\end{equation}

    <p>This indicates that $\mcsgn$ can also be used to calculate the square root and inverse square root of a matrix. More generally, if the eigenvalues of the matrix $\boldsymbol{A}\boldsymbol{B}$ are non-negative, then:</p>

    \begin{equation}\mcsgn\left(\begin{bmatrix}\boldsymbol{0} & \boldsymbol{A} \\ \boldsymbol{B} & \boldsymbol{0}\end{bmatrix}\right)=\begin{bmatrix}\boldsymbol{0} & \boldsymbol{C} \\ \boldsymbol{C}^{-1} & \boldsymbol{0}\end{bmatrix}\end{equation}

    <p>where $\boldsymbol{C}=\boldsymbol{A}(\boldsymbol{B}\boldsymbol{A})^{-1/2}$.</p>

    <h3>Third Example</h3>
    <p>Assume $\boldsymbol{A},\boldsymbol{B}=\boldsymbol{0}$, and $\boldsymbol{D}=\boldsymbol{C}^{\top}$. Then the Riccati equation simplifies to $\boldsymbol{X}\boldsymbol{C}^{\top}\boldsymbol{X} = \boldsymbol{C}$. It is easy to verify that $\boldsymbol{X}=\msign(\boldsymbol{C})$ is indeed its solution. We will only demonstrate the most ideal case where $\boldsymbol{C}$ is a full-rank square matrix. Then $\boldsymbol{C}^{\top}\boldsymbol{X}$ and $\boldsymbol{X}\boldsymbol{C}^{\top}$ are both positive definite, so we have:</p>

    \begin{equation}\mcsgn\left(\begin{bmatrix}\boldsymbol{0} & \boldsymbol{C} \\ \boldsymbol{C}^{\top} & \boldsymbol{0}\end{bmatrix}\right)=\begin{bmatrix}\boldsymbol{X} & -\boldsymbol{I} \\ \boldsymbol{I} & \boldsymbol{0}\end{bmatrix}\begin{bmatrix}\boldsymbol{I} & \boldsymbol{Y} \\ \boldsymbol{0} & -\boldsymbol{I}\end{bmatrix}\begin{bmatrix}\boldsymbol{X} & -\boldsymbol{I} \\ \boldsymbol{I} & \boldsymbol{0}\end{bmatrix}^{-1}=\begin{bmatrix}-\boldsymbol{X}\boldsymbol{Y}-\boldsymbol{I} & 2\boldsymbol{X} + \boldsymbol{X}\boldsymbol{Y}\boldsymbol{X} \\ -\boldsymbol{Y} & \boldsymbol{Y}\boldsymbol{X} + \boldsymbol{I}\end{bmatrix}\end{equation}

    <p>By the same reasoning as in the previous section, $\boldsymbol{Y}\boldsymbol{X} + \boldsymbol{I}=0$, so:</p>

    \begin{equation}\mcsgn\left(\begin{bmatrix}\boldsymbol{0} & \boldsymbol{C} \\ \boldsymbol{C}^{\top} & \boldsymbol{0}\end{bmatrix}\right)=\begin{bmatrix}\boldsymbol{0} & \msign(\boldsymbol{C}) \\ \msign(\boldsymbol{C}^{\top}) & \boldsymbol{0}\end{bmatrix}\end{equation}

    <p>That is, $\mcsgn$ can also be used to calculate $\msign$. In fact, it can be directly proven that this equality holds for any matrix $\boldsymbol{C}$, but proving it from the perspective of solving the Riccati equation here would involve some tedious details, which the reader can supplement independently.</p>

    <h2>Summary</h2>
    <p>This article mainly organizes several identities related to $\mcsgn$ from the perspective of solving the Algebraic Riccati Equation.</p>
</article>
<hr>
<footer style="margin-top: 3em; padding: 1.5em; background: #f5f5f5; border-radius: 8px; font-size: 0.9em; color: #555;">
    <p style="margin: 0 0 0.5em 0;"><strong>Citation</strong></p>
    <p style="margin: 0 0 0.5em 0;">
        This is a machine translation of the original Chinese article:<br>
        <a href="https://kexue.fm/archives/11056" style="color: #005fcc;">https://kexue.fm/archives/11056</a>
    </p>
    <p style="margin: 0 0 0.5em 0;">
        Original author: 苏剑林 (Su Jianlin)<br>
        Original publication: <a href="https://kexue.fm" style="color: #005fcc;">科学空间 (Scientific Spaces)</a>
    </p>
    <p style="margin: 0; font-style: italic;">
        Translated using Gemini 3 Flash. Please refer to the original for authoritative content.
    </p>
</footer>
