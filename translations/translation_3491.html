
    <style type="text/css">

    body {
    margin: 48px auto;
    max-width: 68ch;              /* character-based width reads better */
    padding: 0 16px;

    font-family: -apple-system, BlinkMacSystemFont, "Segoe UI",
                Roboto, "Helvetica Neue", Arial, sans-serif;
    font-size: 18px;
    line-height: 1.65;
    color: #333;
    background: #fafafa;
    }

    h1, h2, h3, h4 {
    line-height: 1.25;
    margin-top: 2.2em;
    margin-bottom: 0.6em;
    font-weight: 600;
    }

    h1 {
    font-size: 2.1em;
    margin-top: 0;
    }

    h2 {
    font-size: 1.6em;
    border-bottom: 1px solid #e5e5e5;
    padding-bottom: 0.3em;
    }

    h3 {
    font-size: 1.25em;
    }

    h4 {
    font-size: 1.05em;
    color: #555;
    }

    /* Paragraphs and lists */
    p {
    margin: 1em 0;
    }

    ul, ol {
    margin: 1em 0 1em 1.5em;
    }

    li {
    margin: 0.4em 0;
    }

    a {
    color: #005fcc;
    text-decoration: none;
    }

    a:hover {
    text-decoration: underline;
    }

    code {
    font-family: ui-monospace, SFMono-Regular, Menlo, Consolas, monospace;
    font-size: 0.95em;
    background: #f2f2f2;
    padding: 0.15em 0.35em;
    border-radius: 4px;
    }

    pre {
    font-family: ui-monospace, SFMono-Regular, Menlo, Consolas, monospace;
    font-size: 0.9em;
    background: #f5f5f5;
    padding: 1em 1.2em;
    overflow-x: auto;
    border-radius: 6px;
    line-height: 1.45;
    }

    pre code {
    background: none;
    padding: 0;
    }

    blockquote {
    margin: 1.5em 0;
    padding-left: 1em;
    border-left: 4px solid #ddd;
    color: #555;
    }

    hr {
    border: none;
    border-top: 1px solid #e0e0e0;
    margin: 3em 0;
    }

    table {
    border-collapse: collapse;
    margin: 1.5em 0;
    width: 100%;
    font-size: 0.95em;
    }

    th, td {
    padding: 0.5em 0.7em;
    border-bottom: 1px solid #e5e5e5;
    text-align: left;
    }

    th {
    font-weight: 600;
    }

    img {
    max-width: 100%;
    display: block;
    margin: 1.5em auto;
    }

    small {
    color: #666;
    }

    mjx-container {
    margin: 1em 0;
    }

    ::selection {
    background: #cce2ff;
    }

    </style>
    

<script>
window.MathJax = {
  tex: {
    inlineMath: [['$', '$'], ['\\(', '\\)']],
    displayMath: [['$$', '$$'], ['\\[', '\\]']],
    tags: 'ams',
    packages: {'[+]': ['ams']}
  }
};
</script>
<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

<article>
    <h1><a href="https://kexue.fm/archives/3491">Information Entropy Method and Implementation for New Word Discovery</a></h1>
    <p>By 苏剑林 | October 26, 2015</p>

    <p>In previous posts on this blog, I have briefly touched upon the issues of Chinese text processing and mining. The most significant difference between Chinese data mining and similar tasks in English is that Chinese lacks spaces. To effectively complete linguistic tasks, word segmentation must be performed first. Currently, popular segmentation methods are dictionary-based. However, a major problem arises: where do these dictionaries come from? While common words can be manually collected into a dictionary, this approach fails to keep up with the constant emergence of new words, especially internet slang—which is often the key to linguistic tasks. Therefore, a core task in Chinese language processing is refining new word discovery algorithms.</p>

    <p>New word discovery refers to automatically identifying potential word-like language fragments from massive corpora without adding any prior material. A few days ago, I visited Xiaoxia's company and tried joining one of their development projects, primarily focused on processing web articles. Consequently, I brushed up on the algorithmic knowledge of new word discovery, referring to the article on Matrix67.com, <a href="http://www.matrix67.com/blog/archives/5044">"Social Linguistics in the Internet Era: Text Data Mining based on SNS"</a>, particularly the information entropy ideas contained therein. Based on his logic, I wrote a simple Python script.</p>

    <p>The program's algorithm is entirely derived from Matrix67.com's article; interested readers can head to his blog for a detailed read, which I believe will be very rewarding. Here, I will briefly discuss the implementation logic of the code. The specific program can be found at the end of the post. To process larger texts, I tried to avoid Python’s built-in loops as much as possible, using functions provided by third-party libraries like Numpy and Pandas. Since a large number of words are involved, indexing work is crucial; for instance, sorting word items beforehand significantly increases retrieval speed.</p>

    <p>Below are the results of the new word discovery (partial) performed by this script on the Century Revised Edition of Jin Yong's novel <i>Demi-Gods and Semi-Devils</i> (2.5M txt file). It took about 20 seconds, and the results feel quite good—the names of the main characters were discovered automatically. Of course, since the code is short and lacks specialized processing, there is still much room for improvement.</p>

    <blockquote>
        Duan Yu, 3535<br>
        Shenme (What), 2537<br>
        Xiao Feng, 1897<br>
        Ziji (Self), 1730<br>
        Xu Zhu, 1671<br>
        Qiao Feng, 1243<br>
        A Zi, 1157<br>
        Wugong (Martial Arts), 1109<br>
        A Zhu, 1106<br>
        Guniang (Miss/Girl), 1047<br>
        Xiaodao (Said with a smile), 992<br>
        Zanmen (We), 832<br>
        Shifu (Master), 805<br>
        Ruhe (How), 771<br>
        Ruci (So/Thus), 682<br>
        Dali, 665<br>
        Gaibang (Beggars' Gang), 645<br>
        Turan (Suddenly), 640<br>
        Wang Yuyan, 920<br>
        Murong Fu, 900<br>
        Duan Zhengchun, 780<br>
        Mu Wanqing, 751<br>
        Jiumozhi, 600<br>
        You Tanzhi, 515<br>
        Ding Chunqiu, 463<br>
        Youshenme (Have what), 460<br>
        Bao Butong, 447<br>
        Shaolin Temple, 379<br>
        Emperor Baoding, 344<br>
        Mrs. Ma, 324<br>
        Duan Yanqing, 302<br>
        Elder Wu, 294<br>
        Buyoude (Can't help but), 275<br>
        Mrs. Wang, 265<br>
        Weishenme (Why), 258<br>
        Zhitingde (Only heard), 255<br>
        Shishenme (What is it), 237<br>
        Yun Zhonghe, 236<br>
        That girl, 234<br>
        Ba Tianshi, 230<br>
        Miss Wang, 227<br>
        Hutingde (Suddenly heard), 221<br>
        Zhong Wanchou, 218<br>
        Shaolin Sect, 216<br>
        Ye Erniang, 216<br>
        Zhu Danchen, 213<br>
        Feng Bo'e, 209<br>
        Khitan people, 208<br>
        Crocodile Deity of the South Sea, 485<br>
        Young Master Murong, 230<br>
        Yelu Hongji, 189<br>
        Six Meridian Divine Sword, 168<br>
        Stood up, 116<br>
        Leading Big Brother, 103<br>
        These few words, 100<br>
        Nodded, 96<br>
        Old Monster of Xingxiu, 92<br>
        Fairy Sister, 90<br>
        Startled, 87<br>
        Greatly startled, 86<br>
        Mr. Murong, 86<br>
        Youshenme (What else), 86
    </blockquote>

    <p><strong>Complete Code (Version 3.x, can be used for 2.x with simple modifications, mainly the output functions):</strong></p>

<pre><code>import numpy as np
import pandas as pd
import re
from numpy import log,min

f = open('data.txt', 'r') # Open article
s = f.read() # Read as a single string

# Define punctuation to be removed
drop_dict = [u'，', u'\n', u'。', u'、', u'：', u'(', u')', u'[', u']', u'.', u',', u' ', u'\u3000', u'”', u'“', u'？', u'?', u'！', u'‘', u'’', u'…']
for i in drop_dict: # Remove punctuation
 s = s.replace(i, '')

# For convenience, define a dictionary for regex patterns
myre = {2:'(..)', 3:'(...)', 4:'(....)', 5:'(.....)', 6:'(......)', 7:'(.......)'}

min_count = 10 # Minimum occurrences for a word
min_support = 30 # Minimum support for a word, 1 represents random combination
min_s = 3 # Minimum information entropy, higher means more likely to be an independent word
max_sep = 4 # Maximum character length for candidate words
t=[] # To save results

t.append(pd.Series(list(s)).value_counts()) # Count bit by bit
tsum = t[0].sum() # Total character count
rt = [] # To save results

for m in range(2, max_sep+1):
 print(u'Generating %s-character words...'%m)
 t.append([])
 for i in range(m): # Generate all possible m-character words
  t[m-1] = t[m-1] + re.findall(myre[m], s[i:])

 t[m-1] = pd.Series(t[m-1]).value_counts() # Count word frequencies
 t[m-1] = t[m-1][t[m-1] > min_count] # Filter by minimum count
 tt = t[m-1][:]
 for k in range(m-1):
  qq = np.array(list(map(lambda ms: tsum*t[m-1][ms]/t[m-2-k][ms[:m-1-k]]/t[k][ms[m-1-k:]], tt.index))) > min_support # Filter by minimum support.
  tt = tt[qq]
 rt.append(tt.index)

def cal_S(sl): # Information entropy calculation function
 return -((sl/sl.sum()).apply(log)*sl/sl.sum()).sum()

for i in range(2, max_sep+1):
 print(u'Performing maximum entropy screening for %s-character words (%s)...'%(i, len(rt[i-2])))
 pp = [] # Save all left and right neighbors
 for j in range(i+2):
  pp = pp + re.findall('(.)%s(.)'%myre[i], s[j:])
 pp = pd.DataFrame(pp).set_index(1).sort_index() # Sort first, which is important for speeding up retrieval
 index = np.sort(np.intersect1d(rt[i-2], pp.index)) # Intersection
 # Left and right neighbor information entropy screening
 index = index[np.array(list(map(lambda s: cal_S(pd.Series(pp[0][s]).value_counts()), index))) > min_s]
 rt[i-2] = index[np.array(list(map(lambda s: cal_S(pd.Series(pp[2][s]).value_counts()), index))) > min_s]

# Pre-processing for output
for i in range(len(rt)):
 t[i+1] = t[i+1][rt[i]]
 t[i+1].sort_values(ascending = False)

# Save result and output
pd.DataFrame(pd.concat(t[1:])).to_csv('result.txt', header = False)
</code></pre>

</article>
<hr>
<footer style="margin-top: 3em; padding: 1.5em; background: #f5f5f5; border-radius: 8px; font-size: 0.9em; color: #555;">
    <p style="margin: 0 0 0.5em 0;"><strong>Citation</strong></p>
    <p style="margin: 0 0 0.5em 0;">
        This is a machine translation of the original Chinese article:<br>
        <a href="translation_3491.html" style="color: #005fcc;">https://kexue.fm/archives/3491</a>
    </p>
    <p style="margin: 0 0 0.5em 0;">
        Original author: 苏剑林 (Su Jianlin)<br>
        Original publication: <a href="https://kexue.fm" style="color: #005fcc;">科学空间 (Scientific Spaces)</a>
    </p>
    <p style="margin: 0; font-style: italic;">
        Translated using Gemini 3 Flash. Please refer to the original for authoritative content.
    </p>
</footer>
