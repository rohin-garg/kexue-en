
    <style type="text/css">

    body {
    margin: 48px auto;
    max-width: 68ch;              /* character-based width reads better */
    padding: 0 16px;

    font-family: -apple-system, BlinkMacSystemFont, "Segoe UI",
                Roboto, "Helvetica Neue", Arial, sans-serif;
    font-size: 18px;
    line-height: 1.65;
    color: #333;
    background: #fafafa;
    }

    h1, h2, h3, h4 {
    line-height: 1.25;
    margin-top: 2.2em;
    margin-bottom: 0.6em;
    font-weight: 600;
    }

    h1 {
    font-size: 2.1em;
    margin-top: 0;
    }

    h2 {
    font-size: 1.6em;
    border-bottom: 1px solid #e5e5e5;
    padding-bottom: 0.3em;
    }

    h3 {
    font-size: 1.25em;
    }

    h4 {
    font-size: 1.05em;
    color: #555;
    }

    /* Paragraphs and lists */
    p {
    margin: 1em 0;
    }

    ul, ol {
    margin: 1em 0 1em 1.5em;
    }

    li {
    margin: 0.4em 0;
    }

    a {
    color: #005fcc;
    text-decoration: none;
    }

    a:hover {
    text-decoration: underline;
    }

    code {
    font-family: ui-monospace, SFMono-Regular, Menlo, Consolas, monospace;
    font-size: 0.95em;
    background: #f2f2f2;
    padding: 0.15em 0.35em;
    border-radius: 4px;
    }

    pre {
    font-family: ui-monospace, SFMono-Regular, Menlo, Consolas, monospace;
    font-size: 0.9em;
    background: #f5f5f5;
    padding: 1em 1.2em;
    overflow-x: auto;
    border-radius: 6px;
    line-height: 1.45;
    }

    pre code {
    background: none;
    padding: 0;
    }

    blockquote {
    margin: 1.5em 0;
    padding-left: 1em;
    border-left: 4px solid #ddd;
    color: #555;
    }

    hr {
    border: none;
    border-top: 1px solid #e0e0e0;
    margin: 3em 0;
    }

    table {
    border-collapse: collapse;
    margin: 1.5em 0;
    width: 100%;
    font-size: 0.95em;
    }

    th, td {
    padding: 0.5em 0.7em;
    border-bottom: 1px solid #e5e5e5;
    text-align: left;
    }

    th {
    font-weight: 600;
    }

    img {
    max-width: 100%;
    display: block;
    margin: 1.5em auto;
    }

    small {
    color: #666;
    }

    mjx-container {
    margin: 1em 0;
    }

    ::selection {
    background: #cce2ff;
    }

    </style>
    

<script>
window.MathJax = {
  tex: {
    inlineMath: [['$', '$'], ['\\(', '\\)']],
    displayMath: [['$$', '$$'], ['\\[', '\\]']],
    processEscapes: true,
    tags: 'ams',
    packages: {'[+]': ['ams']}
  }
};
</script>
<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"></script>

<article>
    <h1><a href="https://kexue.fm/archives/4681">A More Unique Word Vector Model (6): Code, Sharing, and Conclusion</a></h1>
    
    <p>By 苏剑林 | November 19, 2017</p>

    <h2>List</h2>
    <blockquote>
        <p><a href="translation_4667.html">A More Unique Word Vector Model (1): simpler glove</a></p>
        <p><a href="translation_4669.html">A More Unique Word Vector Model (2): Modeling Language</a></p>
        <p><a href="translation_4671.html">A More Unique Word Vector Model (3): Description-Related Models</a></p>
        <p><a href="translation_4675.html">A More Unique Word Vector Model (4): Solving the Model</a></p>
        <p><a href="translation_4677.html">A More Unique Word Vector Model (5): Interesting Results</a></p>
        <p><a href="translation_4681.html">A More Unique Word Vector Model (6): Code, Sharing, and Conclusion</a></p>
    </blockquote>

    <h2>Code</h2>
    <p>The implementation of this article is located at: <a href="https://github.com/bojone/simpler_glove">https://github.com/bojone/simpler_glove</a></p>

    <p>The source code is modified from Stanford's <a href="https://github.com/stanfordnlp/GloVe">original GloVe</a>. I have only made minor modifications because the main difficulty lies in the statistics of co-occurrence frequencies, and I am grateful to the predecessors at Stanford for providing such a classic and excellent statistical implementation case. In fact, I am not familiar with the C language, so the modifications I have made might not be of high caliber; I hope experts will offer corrections.</p>

    <p>In addition, to implement the "interesting results" from the previous section, I have supplemented <code>simpler_glove.py</code> in the GitHub repository. It encapsulates a class that can directly read the model files (txt format) exported by the C version of simple glove and includes some commonly used functions for easy calling.</p>

    <h2>Sharing</h2>
    <p>Here is a set of Chinese word vectors trained using the model in this article. The corpus was trained on Baidu Baike, with a total of 1 million articles, approximately 300,000 words, and a word vector dimension of 128. A special treatment was applied during tokenization: all numbers and English characters were split into individual digits and letters. For friends who need to conduct experiments, you can download them:</p>

    <blockquote>
        <p>Link: <a href="http://pan.baidu.com/s/1jIb3yr8">http://pan.baidu.com/s/1jIb3yr8</a></p>
        <p>Password: 1ogw</p>
    </blockquote>

    <h2>Conclusion</h2>
    <p>This article can be considered a relatively complete exploration of word vector models. It is also a result of my "theoretical compulsion." Fortunately, in the end, I obtained a model that looks theoretically elegant, which has partially cured my compulsion. As for experimental effects, applications, etc., they remain to be further verified in future use.</p>

    <p>Most of the derivations in this article can be imitated to explain the experimental results of word2vec's skip-gram model, which readers can try. In fact, word2vec's skip-gram model does exhibit similar performance to the model in this article, including the properties of the word vector model.</p>

    <p><strong>Overall, combining theory with experiment is a wonderful thing, but it is also a very exhausting task, as the contents above alone took me several months of thinking time.</strong></p>

    <p><em><strong>If reprinting, please include the original address:</strong> <a href="translation_4681.html">https://kexue.fm/archives/4681</a></em></p>

    <p>Su Jianlin. (Nov. 19, 2017). "A More Unique Word Vector Model (6): Code, Sharing, and Conclusion" [Blog post]. Retrieved from <a href="translation_4681.html">https://kexue.fm/archives/4681</a></p>

    </article>
<hr>
<footer style="margin-top: 3em; padding: 1.5em; background: #f5f5f5; border-radius: 8px; font-size: 0.9em; color: #555;">
    <p style="margin: 0 0 0.5em 0;"><strong>Citation</strong></p>
    <p style="margin: 0 0 0.5em 0;">
        This is a machine translation of the original Chinese article:<br>
        <a href="translation_4681.html" style="color: #005fcc;">https://kexue.fm/archives/4681</a>
    </p>
    <p style="margin: 0 0 0.5em 0;">
        Original author: 苏剑林 (Su Jianlin)<br>
        Original publication: <a href="https://kexue.fm" style="color: #005fcc;">科学空间 (Scientific Spaces)</a>
    </p>
    <p style="margin: 0; font-style: italic;">
        Translated using Gemini 3 Flash. Please refer to the original for authoritative content.
    </p>
</footer>
