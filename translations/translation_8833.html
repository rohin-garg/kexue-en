
    <style type="text/css">

    body {
    margin: 48px auto;
    max-width: 68ch;              /* character-based width reads better */
    padding: 0 16px;

    font-family: -apple-system, BlinkMacSystemFont, "Segoe UI",
                Roboto, "Helvetica Neue", Arial, sans-serif;
    font-size: 18px;
    line-height: 1.65;
    color: #333;
    background: #fafafa;
    }

    h1, h2, h3, h4 {
    line-height: 1.25;
    margin-top: 2.2em;
    margin-bottom: 0.6em;
    font-weight: 600;
    }

    h1 {
    font-size: 2.1em;
    margin-top: 0;
    }

    h2 {
    font-size: 1.6em;
    border-bottom: 1px solid #e5e5e5;
    padding-bottom: 0.3em;
    }

    h3 {
    font-size: 1.25em;
    }

    h4 {
    font-size: 1.05em;
    color: #555;
    }

    /* Paragraphs and lists */
    p {
    margin: 1em 0;
    }

    ul, ol {
    margin: 1em 0 1em 1.5em;
    }

    li {
    margin: 0.4em 0;
    }

    a {
    color: #005fcc;
    text-decoration: none;
    }

    a:hover {
    text-decoration: underline;
    }

    code {
    font-family: ui-monospace, SFMono-Regular, Menlo, Consolas, monospace;
    font-size: 0.95em;
    background: #f2f2f2;
    padding: 0.15em 0.35em;
    border-radius: 4px;
    }

    pre {
    font-family: ui-monospace, SFMono-Regular, Menlo, Consolas, monospace;
    font-size: 0.9em;
    background: #f5f5f5;
    padding: 1em 1.2em;
    overflow-x: auto;
    border-radius: 6px;
    line-height: 1.45;
    }

    pre code {
    background: none;
    padding: 0;
    }

    blockquote {
    margin: 1.5em 0;
    padding-left: 1em;
    border-left: 4px solid #ddd;
    color: #555;
    }

    hr {
    border: none;
    border-top: 1px solid #e0e0e0;
    margin: 3em 0;
    }

    table {
    border-collapse: collapse;
    margin: 1.5em 0;
    width: 100%;
    font-size: 0.95em;
    }

    th, td {
    padding: 0.5em 0.7em;
    border-bottom: 1px solid #e5e5e5;
    text-align: left;
    }

    th {
    font-weight: 600;
    }

    img {
    max-width: 100%;
    display: block;
    margin: 1.5em auto;
    }

    small {
    color: #666;
    }

    mjx-container {
    margin: 1em 0;
    }

    ::selection {
    background: #cce2ff;
    }

    </style>
    

<script>
window.MathJax = {
  tex: {
    inlineMath: [['$', '$'], ['\\(', '\\)']],
    displayMath: [['$$', '$$'], ['\\[', '\\]']],
    processEscapes: true,
    tags: 'ams'
  }
};
</script>
<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

<article>
    <nav style="margin-bottom: 1.5em;">
    <a href="../index.html" style="display: inline-flex; align-items: center; color: #555; text-decoration: none; font-size: 0.95em;">
        <span style="margin-right: 0.3em;">&larr;</span> Back to Index
    </a>
</nav>

    <h1><a href="https://kexue.fm/archives/8833">SquarePlus: Possibly the Simplest Algebraic Smooth Approximation of ReLU</a></h1>

    <p>By 苏剑林 | December 29, 2021</p>

    <p>By Jerry Su | 2021-12-29</p>

    <p>The ReLU function, namely $\max(x,0)$, is one of the most common activation functions. However, its non-differentiability at $x=0$ is often regarded as a "drawback." Consequently, various smooth approximations have been proposed, such as SoftPlus, GeLU, Swish, etc. However, all of these smooth approximations use at least one exponential operation $e^x$ (SoftPlus even uses a logarithm). From an "efficiency-conscious" perspective, the computational cost is not negligible (although with modern GPU acceleration, we rarely perceive this overhead). Recently, a paper titled <a href="https://papers.cool/arxiv/2112.11687">"Squareplus: A Softplus-Like Algebraic Rectifier"</a> proposed a simpler approximation called SquarePlus. We will discuss it here.</p>

    <p>It should be pointed out beforehand that I do not recommend spending too much time on the selection and design of activation functions. Therefore, while I am sharing this paper, it is mainly to provide a reference result and serve as an exercise for everyone to "practice."</p>

    <h2>Definition</h2>
    <p>The form of SquarePlus is very simple, using only addition, multiplication, division, and square root:</p>
    \begin{equation}\text{SquarePlus}(x)=\frac{x+\sqrt{x^2+b}}{2}\end{equation}
    <p>where $b > 0$. When $b=0$, it exactly reduces to $\text{ReLU}(x)=\max(x,0)$. The inspiration for SquarePlus comes roughly from:</p>
    \begin{equation}\max(x,0)=\frac{x+|x|}{2}=\frac{x+\sqrt{x^2}}{2}\end{equation}
    <p>Therefore, to fix the differentiability at $x=0$, a constant $b > 0$ is added inside the square root (to prevent division-by-zero issues in the derivative).</p>

    <p>The original paper points out that because it only uses addition, multiplication, division, and square root, the speed of SquarePlus (primarily on the CPU) is faster than functions like SoftPlus:</p>
    <p style="text-align:center;">
        <img src="https://kexue.fm/usr/uploads/2021/12/3592176466.png" alt="Speed comparison of SquarePlus with other similar functions"><br>
        <em>Speed comparison of SquarePlus with other similar functions</em>
    </p>

    <p>Of course, if you don't care about this slight speedup, then as mentioned at the beginning of this article, just treat it as a mathematical exercise.</p>

    <h2>Properties</h2>
    <p>Like the SoftPlus function ($\log(e^x+1)$), SquarePlus is globally monotonically increasing and strictly greater than ReLU, as shown in the figure below (using $b=1$ for SquarePlus):</p>
    <p style="text-align:center;">
        <img src="https://kexue.fm/usr/uploads/2021/12/1709404285.png" alt="ReLU, SoftPlus, SquarePlus function images (1)"><br>
        <em>ReLU, SoftPlus, SquarePlus function curves (I)</em>
    </p>
    <p>Direct differentiation also reveals its monotonicity:</p>
    \begin{equation}\frac{d}{dx}\text{SquarePlus}(x)=\frac{1}{2}\left(1+\frac{x}{\sqrt{x^2+b}}\right) > 0\end{equation}
    <p>As for the second derivative:</p>
    \begin{equation}\frac{d^2}{dx^2}\text{SquarePlus}(x)=\frac{b}{2(x^2+b)^{3/2}}\end{equation}
    <p>which is also strictly greater than 0, so SquarePlus is a convex function.</p>

    <h2>Approximation</h2>
    <p>Now, there are two exercises we can perform:</p>
    <blockquote>
        <p>1. For what value of $b$ is SquarePlus always greater than or equal to SoftPlus?</p>
        <p>2. For what value of $b$ is the error between SquarePlus and SoftPlus minimized?</p>
    </blockquote>

    <p>For the first question, solving $\text{SquarePlus}(x) \geq \text{SoftPlus}(x)$ directly yields:</p>
    \begin{equation}b \geq 4\log(e^x+1)\left[\log(e^x+1) - x\right]=4\log(e^x+1)\log(e^{-x}+1)\end{equation}
    <p>For this to hold universally, $b$ must be greater than or equal to the maximum value of the right-hand side. We can prove that the maximum value of the right-hand side occurs at $x=0$, so $b \geq 4\log^2 2 = 1.921812\cdots$. Thus, the first question is solved.</p>

    <blockquote>
        <p><strong>Proof:</strong> Note that</p>
        \begin{equation}\frac{d^2}{dx^2}\log\log(e^x+1)=\frac{e^x(\log(e^x+1)-e^x)}{(e^x+1)^2\log^2(e^x+1)} < 0\end{equation}
        <p>Therefore, $\log\log(e^x+1)$ is a concave function. By Jensen's Inequality:</p>
        \begin{equation}\frac{1}{2}\left(\log\log(e^x+1) + \log\log(e^{-x}+1)\right) \leq \log\log(e^{(x+(-x))/2}+1)=\log\log 2\end{equation}
        <p>This implies $\log\left(\log(e^x+1)\log(e^{-x}+1)\right) \leq 2\log\log 2$, or $\log(e^x+1)\log(e^{-x}+1) \leq \log^2 2$. Multiplying both sides by 4 gives the required conclusion. The equality holds when $x=-x$, i.e., $x=0$.</p>
    </blockquote>

    <p>As for the second question, we need a standard for "error." As in the previous article <a href="translation_7309.html">"How the Two Elementary Function Approximations of GELU Were Derived"</a>, we convert this into a $\min\text{-}\max$ problem with no extra parameters:</p>
    \begin{equation}\min_{b} \max_x \left\|\frac{x+\sqrt{x^2+b}}{2} - \log(e^x+1)\right\|\end{equation}
    <p>I cannot find an analytical solution for this problem, so I currently solve it numerically:</p>

<pre><code>import numpy as np
from scipy.special import erf
from scipy.optimize import minimize

def f(x, a):
    return np.abs((x + np.sqrt(x**2 + a**2)) / 2 - np.log(np.exp(x) + 1))

def g(a):
    return np.max([f(x, a) for x in np.arange(-2, 4, 0.0001)])

options = {'xtol': 1e-10, 'ftol': 1e-10, 'maxiter': 100000}
result = minimize(g, 0, method='Powell', options=options)
b = result.x**2
print(b)
</code></pre>

    <p>The final result is $b=1.52382103\cdots$, with a maximum error of $0.075931\cdots$. The comparison is as follows:</p>
    <p style="text-align:center;">
        <img src="https://kexue.fm/usr/uploads/2021/12/3133604314.png" alt="ReLU, SoftPlus, SquarePlus function images (2)"><br>
        <em>ReLU, SoftPlus, SquarePlus function curves (II)</em>
    </p>

    <h2>Summary</h2>
    <p>There isn't much to summarize; I've simply introduced a smooth approximation of ReLU and accompanied it with two simple function exercises.</p>
</article>
<hr>
<footer style="margin-top: 3em; padding: 1.5em; background: #f5f5f5; border-radius: 8px; font-size: 0.9em; color: #555;">
    <p style="margin: 0 0 0.5em 0;"><strong>Citation</strong></p>
    <p style="margin: 0 0 0.5em 0;">
        This is a machine translation of the original Chinese article:<br>
        <a href="https://kexue.fm/archives/8833" style="color: #005fcc;">https://kexue.fm/archives/8833</a>
    </p>
    <p style="margin: 0 0 0.5em 0;">
        Original author: 苏剑林 (Su Jianlin)<br>
        Original publication: <a href="https://kexue.fm" style="color: #005fcc;">科学空间 (Scientific Spaces)</a>
    </p>
    <p style="margin: 0; font-style: italic;">
        Translated using Gemini 3 Flash. Please refer to the original for authoritative content.
    </p>
</footer>
