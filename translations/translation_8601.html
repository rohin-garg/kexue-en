
    <style type="text/css">

    body {
    margin: 48px auto;
    max-width: 68ch;              /* character-based width reads better */
    padding: 0 16px;

    font-family: -apple-system, BlinkMacSystemFont, "Segoe UI",
                Roboto, "Helvetica Neue", Arial, sans-serif;
    font-size: 18px;
    line-height: 1.65;
    color: #333;
    background: #fafafa;
    }

    h1, h2, h3, h4 {
    line-height: 1.25;
    margin-top: 2.2em;
    margin-bottom: 0.6em;
    font-weight: 600;
    }

    h1 {
    font-size: 2.1em;
    margin-top: 0;
    }

    h2 {
    font-size: 1.6em;
    border-bottom: 1px solid #e5e5e5;
    padding-bottom: 0.3em;
    }

    h3 {
    font-size: 1.25em;
    }

    h4 {
    font-size: 1.05em;
    color: #555;
    }

    /* Paragraphs and lists */
    p {
    margin: 1em 0;
    }

    ul, ol {
    margin: 1em 0 1em 1.5em;
    }

    li {
    margin: 0.4em 0;
    }

    a {
    color: #005fcc;
    text-decoration: none;
    }

    a:hover {
    text-decoration: underline;
    }

    code {
    font-family: ui-monospace, SFMono-Regular, Menlo, Consolas, monospace;
    font-size: 0.95em;
    background: #f2f2f2;
    padding: 0.15em 0.35em;
    border-radius: 4px;
    }

    pre {
    font-family: ui-monospace, SFMono-Regular, Menlo, Consolas, monospace;
    font-size: 0.9em;
    background: #f5f5f5;
    padding: 1em 1.2em;
    overflow-x: auto;
    border-radius: 6px;
    line-height: 1.45;
    }

    pre code {
    background: none;
    padding: 0;
    }

    blockquote {
    margin: 1.5em 0;
    padding-left: 1em;
    border-left: 4px solid #ddd;
    color: #555;
    }

    hr {
    border: none;
    border-top: 1px solid #e0e0e0;
    margin: 3em 0;
    }

    table {
    border-collapse: collapse;
    margin: 1.5em 0;
    width: 100%;
    font-size: 0.95em;
    }

    th, td {
    padding: 0.5em 0.7em;
    border-bottom: 1px solid #e5e5e5;
    text-align: left;
    }

    th {
    font-weight: 600;
    }

    img {
    max-width: 100%;
    display: block;
    margin: 1.5em auto;
    }

    small {
    color: #666;
    }

    mjx-container {
    margin: 1em 0;
    }

    ::selection {
    background: #cce2ff;
    }

    </style>
    

<script type="text/javascript">
window.MathJax = {
  tex: {
    tags: 'ams',
    inlineMath: [['$', '$'], ['\\(', '\\)']],
    displayMath: [['$$', '$$'], ['\\[', '\\]']],
    processEscapes: true,
    packages: {'[+]': ['ams']}
  },
  options: {
    ignoreHtmlClass: 'tex2jax_ignore',
    processHtmlClass: 'tex2jax_process'
  }
};
</script>
<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

<article>
    <h1><a href="https://kexue.fm/archives/8601">Transformer Upgrade Journey: 5. Standard Attention as Infinite-Dimensional Linear Attention</a></h1>

    <p>By 苏剑林 | August 06, 2021</p>

    <p>In <a href="translation_7921.html">"Performer: Linearizing Attention Complexity with Random Projections"</a>, we learned about the Performer model proposed by Google. It introduces a random projection scheme that can transform standard Attention into Linear Attention while maintaining a certain level of approximation. Theoretically, as long as the projection dimension is large enough, it can sufficiently approximate standard Attention. In other words, standard Attention can be viewed as an infinite-dimensional Linear Attention.</p>

    <p>This article will introduce two other ideas I conceived for converting standard Attention into infinite-dimensional Linear Attention. Unlike the random projection of Performer, these two schemes are deterministic and allow for a more intuitive perception of the degree of approximation.</p>

    <h2>Brief Introduction</h2>

    <p>Regarding standard Attention and Linear Attention, I won't go into too much detail here. Readers who are not yet familiar can refer to my previous articles <a href="translation_7546.html">"Exploring Linear Attention: Does Attention Must Have a Softmax?"</a> and <a href="translation_8338.html">"Transformer Upgrade Journey: 3. From Performer to Linear Attention"</a>. Briefly, the calculation method for standard Attention is:</p>
    \begin{equation}a_{i,j}=\frac{e^{\boldsymbol{q}_i\cdot \boldsymbol{k}_j}}{\sum\limits_j e^{\boldsymbol{q}_i\cdot \boldsymbol{k}_j}}\end{equation}
    <p>While the calculation method for Linear Attention is:</p>
    \begin{equation}a_{i,j}=\frac{\phi(\boldsymbol{q}_i)\cdot \varphi(\boldsymbol{k}_j)}{\sum\limits_j \phi(\boldsymbol{q}_i)\cdot \varphi(\boldsymbol{k}_j)}\end{equation}
    <p>Therefore, to (approximately) transform standard Attention into Linear Attention, one generally needs to find transformations $\phi, \varphi$ such that there is an approximation:</p>
    \begin{equation}\phi(\boldsymbol{q})\cdot \varphi(\boldsymbol{k})\approx e^{\boldsymbol{q}\cdot \boldsymbol{k}}\end{equation}
    <p>In this context, $e^{\boldsymbol{q}\cdot \boldsymbol{k}}$ is the "kernel function" in kernel methods.</p>

    <h2>Random Projection</h2>

    <p>Performer found the first practical random projection transformation scheme. Essentially, it is based on the following integral:</p>
    \begin{equation}\begin{aligned}
    e^{\boldsymbol{q}\cdot \boldsymbol{k}} =&\, \frac{1}{(2\pi)^{d/2}}\int e^{-\Vert\boldsymbol{\omega}-\boldsymbol{q}-\boldsymbol{k}\Vert^2 / 2 + \boldsymbol{q}\cdot \boldsymbol{k}}d\boldsymbol{\omega}\\
    =&\, \frac{1}{(2\pi)^{d/2}}\int e^{-\Vert\boldsymbol{\omega}\Vert^2 / 2}\times e^{\boldsymbol{\omega}\cdot \boldsymbol{q}-\Vert \boldsymbol{q}\Vert^2 / 2} \times e^{\boldsymbol{\omega}\cdot \boldsymbol{k}-\Vert \boldsymbol{k}\Vert^2 / 2}d\boldsymbol{\omega}
    \\
    \end{aligned}\end{equation}
    <p>leading to</p>
    \begin{equation}\begin{aligned}
    e^{\boldsymbol{q}\cdot \boldsymbol{k}}&=\mathbb{E}_{\boldsymbol{\omega}\sim \mathcal{N}(\boldsymbol{\omega};0,\boldsymbol{1}_d)}\left[e^{\boldsymbol{\omega}\cdot \boldsymbol{q}-\Vert \boldsymbol{q}\Vert^2 / 2} \times e^{\boldsymbol{\omega}\cdot \boldsymbol{k}-\Vert \boldsymbol{k}\Vert^2 / 2}\right]\\[6pt]
    &\approx\underbrace{\frac{1}{\sqrt{m}}\begin{pmatrix}e^{\boldsymbol{\omega}_1\cdot \boldsymbol{q}-\Vert \boldsymbol{q}\Vert^2 / 2} \\
    e^{\boldsymbol{\omega}_2\cdot \boldsymbol{q}-\Vert \boldsymbol{q}\Vert^2 / 2}\\
    \vdots\\
    e^{\boldsymbol{\omega}_m\cdot \boldsymbol{q}-\Vert \boldsymbol{q}\Vert^2 / 2} \end{pmatrix}}_{\phi(\boldsymbol{q})}
    \cdot \underbrace{\frac{1}{\sqrt{m}}\begin{pmatrix}e^{\boldsymbol{\omega}_1\cdot \boldsymbol{k}-\Vert \boldsymbol{k}\Vert^2 / 2} \\
    e^{\boldsymbol{\omega}_2\cdot \boldsymbol{k}-\Vert \boldsymbol{k}\Vert^2 / 2}\\
    \vdots\\
    e^{\boldsymbol{\omega}_m\cdot \boldsymbol{k}-\Vert \boldsymbol{k}\Vert^2 / 2} \end{pmatrix}}_{\varphi(\boldsymbol{k})}
    \end{aligned}\end{equation}
    <p>where $\boldsymbol{\omega}_1,\boldsymbol{\omega}_2,\cdots,\boldsymbol{\omega}_m\sim \mathcal{N}(\boldsymbol{\omega};0,\boldsymbol{1}_d)$. In this way, through the idea of random projection, we approximately transform the exponential of the inner product of two $d$-dimensional vectors into the inner product of two $m$-dimensional vectors, and when $m\to\infty$, the two are theoretically equal.</p>

    <p>The above random projection scheme is quite ingenious and not easily conceived. Below, I will introduce two schemes I devised, which are relatively easier to understand, especially for readers familiar with kernel functions; they might grasp it at a glance.</p>

    <h2>Taylor Expansion</h2>

    <p>My first idea is based on the Taylor expansion:</p>
    \begin{equation}e^{\boldsymbol{q}\cdot \boldsymbol{k}} = \sum_{m=0}^{\infty} \frac{(\boldsymbol{q}\cdot \boldsymbol{k})^m}{m!}\end{equation}
    <p>Truncating to the first $n+1$ terms, we get an $n$-th degree polynomial regarding $\boldsymbol{q}\cdot \boldsymbol{k}$:</p>
    \begin{equation}e^{\boldsymbol{q}\cdot \boldsymbol{k}} \approx 1 + \boldsymbol{q}\cdot \boldsymbol{k} + \frac{1}{2}(\boldsymbol{q}\cdot \boldsymbol{k})^2 + \cdots + \frac{1}{n!}(\boldsymbol{q}\cdot \boldsymbol{k})^n\end{equation}
    <p>This is actually a "polynomial kernel function." Notice that we have:</p>
    \begin{equation}\begin{aligned}
    (\boldsymbol{q}\cdot \boldsymbol{k})^m =&\, \left(\sum_i q_i k_i\right)^m = \left(\sum_{i_1} q_{i_1} k_{i_1}\right)\cdots\left(\sum_{i_m} q_{i_m} k_{i_m}\right) \\
    =&\, \sum_{i_1,\cdots,i_m} (q_{i_1}\cdots q_{i_m}) (k_{i_1}\cdots k_{i_m})
    \end{aligned}\end{equation}
    <p>If we view $q_{i_1}\cdots q_{i_m}$ and $k_{i_1}\cdots k_{i_m}$ respectively as a large $d^m$-dimensional vector, then $(\boldsymbol{q}\cdot \boldsymbol{k})^m$ is the inner product of these two large vectors. In fact, the operation that yields a "large vector" from several vectors is called the "<a href="https://en.wikipedia.org/wiki/Outer_product">outer product</a>" of vectors, also known as the "tensor product," generally denoted as $\otimes$. At this point:</p>
    \begin{equation}
    \frac{1}{m!}(\boldsymbol{q}\cdot \boldsymbol{k})^m = \frac{1}{m!}\underbrace{(\boldsymbol{q}\otimes\cdots\otimes\boldsymbol{q})}_{m \text{ times } \boldsymbol{q}}\cdot\underbrace{(\boldsymbol{k}\otimes\cdots\otimes\boldsymbol{k})}_{m \text{ times } \boldsymbol{k}} = \left(\frac{\otimes^m\boldsymbol{q}}{\sqrt{m!}}\right)\cdot\left(\frac{\otimes^m\boldsymbol{k}}{\sqrt{m!}}\right)
    \end{equation}
    <p>Here $\otimes^m\boldsymbol{q}, \otimes^m\boldsymbol{k}$ are shorthand for the continuous outer product of $m$ copies of $\boldsymbol{q}, \boldsymbol{k}$ (the $m$-th power of the outer product). Using this result, we have:</p>
    \begin{equation}
    e^{\boldsymbol{q}\cdot \boldsymbol{k}}\approx \sum_{m=0}^n \left(\frac{\otimes^m\boldsymbol{q}}{\sqrt{m!}}\right)\cdot\left(\frac{\otimes^m\boldsymbol{k}}{\sqrt{m!}}\right) =\underbrace{\begin{pmatrix} 1 \\
    \boldsymbol{q}\\
    \frac{\otimes^2\boldsymbol{q}}{\sqrt{2}} \\
    \vdots\\
    \frac{\otimes^n\boldsymbol{q}}{\sqrt{n!}}\end{pmatrix}}_{\phi(\boldsymbol{q})}
    \cdot \underbrace{\begin{pmatrix} 1 \\
    \boldsymbol{k}\\
    \frac{\otimes^2\boldsymbol{k}}{\sqrt{2}} \\
    \vdots\\
    \frac{\otimes^n\boldsymbol{k}}{\sqrt{n!}}\end{pmatrix}}_{\varphi(\boldsymbol{k})}
    \end{equation}
    <p>This completes the transformation from standard Attention to Linear Attention.</p>

    <h2>Exponential Definition</h2>

    <p>Compared to Performer's random projection, the Taylor expansion approach mentioned above should be easier to understand. However, there is an even simpler and more direct approach, which is to use the definition of the natural exponential:</p>
    \begin{equation}e^x = \lim_{n\to\infty} \left(1+\frac{x}{n}\right)^n\end{equation}
    <p>Therefore, by choosing an appropriate $n$, we have:</p>
    \begin{equation}e^{\boldsymbol{q}\cdot \boldsymbol{k}} \approx \left(1+\frac{{\boldsymbol{q}\cdot \boldsymbol{k}}}{n}\right)^n = \left(\begin{pmatrix} 1 \\ \frac{\boldsymbol{q}}{\sqrt{n}}\end{pmatrix} \cdot \begin{pmatrix}1 \\ \frac{\boldsymbol{k}}{\sqrt{n}}\end{pmatrix}\right)^n \end{equation}
    <p>Combining this with the transformation results of the polynomial kernel function from the previous section, we have:</p>
    \begin{equation}e^{\boldsymbol{q}\cdot \boldsymbol{k}} \approx \underbrace{\left(\otimes^n\begin{pmatrix} 1 \\ \frac{\boldsymbol{q}}{\sqrt{n}}\end{pmatrix}\right)}_{\phi(\boldsymbol{q})} \cdot \underbrace{\left(\otimes^n\begin{pmatrix}1 \\ \frac{\boldsymbol{k}}{\sqrt{n}}\end{pmatrix}\right)}_{\varphi(\boldsymbol{k})}\end{equation}
    <p>This might be the simplest and most direct scheme for transforming standard Attention into Linear Attention.</p>

    <h2>Results Analysis</h2>

    <p>In terms of practical value, the latter two deterministic schemes are far inferior to Performer's random projection scheme. This is because the output dimension of random projection can be flexibly controlled, whereas the output dimension of the two deterministic schemes is on the order of $d^n$, which is usually much larger than the sequence length itself. Thus, using them to implement Linear Attention would be less efficient than standard Attention.</p>

    <p>However, from a theoretical standpoint, the latter two schemes provide a simpler and more convenient way to equate standard Attention with infinite-dimensional Linear Attention. This equivalence often helps us better understand the Attention mechanism, the most direct being the understanding of the rank of Attention.</p>

    <p>Readers who have studied Linear Attention should know that if Linear Attention is used for bidirectional tasks (such as MLM), the performance drop is very significant. This is because in Linear Attention, $\phi(\boldsymbol{Q}), \varphi(\boldsymbol{K}) \in \mathbb{R}^{n \times d}$ (where $d$ is the head_size of each head), and generally $n \gg d$, so the rank of the $n \times n$ Attention matrix obtained from $\phi(\boldsymbol{Q})\varphi(\boldsymbol{K})^{\top}$ is at most $d$. This is the low-rank problem of Linear Attention, which limits its expressive power.</p>

    <p>Conversely, the three transformations introduced earlier tell us that standard Attention can be viewed as infinite-dimensional Linear Attention. Therefore, the rank of standard Attention is theoretically not limited by $d$, which is why standard Attention with the same number of parameters often performs better than Linear Attention. In <a href="translation_8338.html">"Transformer Upgrade Journey: 3. From Performer to Linear Attention"</a>, we also mentioned that if standard Attention is to be switched to Linear Attention, $d$ must be scaled up accordingly to maintain a certain degree of approximation in performance.</p>

    <h2>Article Summary</h2>

    <p>This article introduced three interpretations of regarding standard Attention as infinite-dimensional Linear Attention. These different perspectives allow us to link standard Attention with Linear Attention, enabling a more comprehensive understanding of the Attention mechanism from multiple angles.</p>
</article>
<hr>
<footer style="margin-top: 3em; padding: 1.5em; background: #f5f5f5; border-radius: 8px; font-size: 0.9em; color: #555;">
    <p style="margin: 0 0 0.5em 0;"><strong>Citation</strong></p>
    <p style="margin: 0 0 0.5em 0;">
        This is a machine translation of the original Chinese article:<br>
        <a href="https://kexue.fm/archives/8601" style="color: #005fcc;">https://kexue.fm/archives/8601</a>
    </p>
    <p style="margin: 0 0 0.5em 0;">
        Original author: 苏剑林 (Su Jianlin)<br>
        Original publication: <a href="https://kexue.fm" style="color: #005fcc;">科学空间 (Scientific Spaces)</a>
    </p>
    <p style="margin: 0; font-style: italic;">
        Translated using Gemini 3 Flash. Please refer to the original for authoritative content.
    </p>
</footer>
