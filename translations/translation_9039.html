
    <style type="text/css">

    body {
    margin: 48px auto;
    max-width: 68ch;              /* character-based width reads better */
    padding: 0 16px;

    font-family: -apple-system, BlinkMacSystemFont, "Segoe UI",
                Roboto, "Helvetica Neue", Arial, sans-serif;
    font-size: 18px;
    line-height: 1.65;
    color: #333;
    background: #fafafa;
    }

    h1, h2, h3, h4 {
    line-height: 1.25;
    margin-top: 2.2em;
    margin-bottom: 0.6em;
    font-weight: 600;
    }

    h1 {
    font-size: 2.1em;
    margin-top: 0;
    }

    h2 {
    font-size: 1.6em;
    border-bottom: 1px solid #e5e5e5;
    padding-bottom: 0.3em;
    }

    h3 {
    font-size: 1.25em;
    }

    h4 {
    font-size: 1.05em;
    color: #555;
    }

    /* Paragraphs and lists */
    p {
    margin: 1em 0;
    }

    ul, ol {
    margin: 1em 0 1em 1.5em;
    }

    li {
    margin: 0.4em 0;
    }

    a {
    color: #005fcc;
    text-decoration: none;
    }

    a:hover {
    text-decoration: underline;
    }

    code {
    font-family: ui-monospace, SFMono-Regular, Menlo, Consolas, monospace;
    font-size: 0.95em;
    background: #f2f2f2;
    padding: 0.15em 0.35em;
    border-radius: 4px;
    }

    pre {
    font-family: ui-monospace, SFMono-Regular, Menlo, Consolas, monospace;
    font-size: 0.9em;
    background: #f5f5f5;
    padding: 1em 1.2em;
    overflow-x: auto;
    border-radius: 6px;
    line-height: 1.45;
    }

    pre code {
    background: none;
    padding: 0;
    }

    blockquote {
    margin: 1.5em 0;
    padding-left: 1em;
    border-left: 4px solid #ddd;
    color: #555;
    }

    hr {
    border: none;
    border-top: 1px solid #e0e0e0;
    margin: 3em 0;
    }

    table {
    border-collapse: collapse;
    margin: 1.5em 0;
    width: 100%;
    font-size: 0.95em;
    }

    th, td {
    padding: 0.5em 0.7em;
    border-bottom: 1px solid #e5e5e5;
    text-align: left;
    }

    th {
    font-weight: 600;
    }

    img {
    max-width: 100%;
    display: block;
    margin: 1.5em auto;
    }

    small {
    color: #666;
    }

    mjx-container {
    margin: 1em 0;
    }

    ::selection {
    background: #cce2ff;
    }

    </style>
    

<script>
window.MathJax = {
  tex: {
    inlineMath: [['$', '$'], ['\\(', '\\)']],
    displayMath: [['$$', '$$'], ['\\[', '\\]']],
    processEscapes: true,
    tags: 'ams',
    packages: {'[+]': ['ams']}
  },
  options: {
    renderActions: {
      findScript: [10, function (doc) {
        for (const node of document.querySelectorAll('script[type^="math/tex"]')) {
          const display = !!node.type.match(/; *mode=display/);
          const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
          const text = document.createTextNode('');
          node.parentNode.replaceChild(text, node);
          math.start = {node: text, delim: '', n: 0};
          math.end = {node: text, delim: '', n: 0};
          doc.math.push(math);
        }
      }, '']
    }
  },
  loader: {load: ['[tex]/ams']}
};
</script>
<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"></script>

<article>
    <h1><a href="https://kexue.fm/archives/9039">What Should "KL Divergence" Look Like Under GlobalPointer?</a></h1>
    
    <p>By 苏剑林 | April 15, 2022</p>

    <p>Recently, a reader mentioned wanting to test the combined effect of <a href="translation_8373.html">GlobalPointer</a> and <a href="translation_8496.html">R-Drop</a>, but was unsure how to calculate the KL divergence under GlobalPointer. Regularization techniques like R-Drop or <a href="translation_7466.html">Virtual Adversarial Training</a> require calculating the KL divergence between probability distributions. However, since the output of GlobalPointer is not a probability distribution, it cannot be calculated directly.</p>

    <p>After some exploration, I have identified a usable form and verified its feasibility through simple experiments. I will introduce my analysis process here.</p>

    <h2>Symmetric Divergence</h2>

    <p>KL divergence is a function of two probability distributions. It is asymmetric, meaning $KL(p\Vert q)$ is generally not equal to $KL(q\Vert p)$. In practical applications, we usually use the symmetrized KL divergence:</p>

    \begin{equation}D(p,q) = KL(p\Vert q) + KL(q\Vert p)\end{equation}

    <p>Substituting the definition of KL divergence $KL(p\Vert q)=\sum\limits_i p_i\log\frac{p_i}{q_i}$, we can simplify it to obtain:</p>

    \begin{equation}D(p,q) = \sum_i (p_i - q_i)(\log p_i - \log q_i)\end{equation}

    <p>Considering that $p,q$ are usually obtained via softmax, we define:</p>

    \begin{equation}p_i = \frac{e^{s_i}}{\sum\limits_j e^{s_j}},\quad q_i = \frac{e^{t_i}}{\sum\limits_j e^{t_j}}\end{equation}

    <p>Substituting these in, we get:</p>

    \begin{equation}\begin{aligned}
    D(p,q) =&\, \sum_i (p_i - q_i)(s_i - t_i) + \sum_i (p_i - q_i)\left(\log\sum_j e^{t_j} - \log\sum_j e^{s_j}\right) \\
    =&\, \sum_i (p_i - q_i)(s_i - t_i) + \left(\sum_i p_i - \sum_i q_i\right)\left(\log\sum_j e^{t_j} - \log\sum_j e^{s_j}\right) \\
    =&\, \sum_i (p_i - q_i)(s_i - t_i)
    \end{aligned}\label{eq:kl-0}\end{equation}

    <h2>Analogous Result</h2>

    <p>As we can see, from the perspective of logits, symmetric KL divergence takes the following form:</p>

    \begin{equation}D(s, t) = \sum_i (f(s_i) - f(t_i))(s_i - t_i) = \langle f(s) - f(t), s - t \rangle\label{eq:kl}\end{equation}

    <p>where $f$ is the softmax operation, and $\langle\cdot,\cdot\rangle$ denotes the dot product of vectors. In terms of form, it is the dot product of two vectors: one is the difference in logits, and the second is the difference of logits transformed by $f$. What are the characteristics of transformation $f$? We know that softmax is actually a smooth approximation of $\text{onehot}(\text{argmax}(\cdot))$ (refer to <a href="translation_6620.html">"Talk on Function Smoothing: Differentiable Approximation of Non-differentiable Functions"</a>). For classification, the maximum value is the target class to be output, so ultimately, it is a smooth approximation of "setting the target class to 1 and non-target classes to 0."</p>

    <p>With this abstract perspective, we can analogously construct the "KL divergence" for GlobalPointer. The output of GlobalPointer can also be understood as logits, but the loss function it uses is the multi-label cross-entropy proposed in <a href="translation_7359.html">"Generalizing 'Softmax + Cross Entropy' to Multi-label Classification Problems."</a> Therefore, this essentially becomes a question of how to calculate KL divergence within multi-label cross-entropy. In GlobalPointer, the target categories are not necessarily the classes with the largest logits, but rather all categories where the logits are greater than 0.</p>

    <p>So, for GlobalPointer, its symmetric divergence can retain the form of Equation $\eqref{eq:kl}$, but $f$ should be replaced with a smooth approximation of "setting values greater than 0 to 1 and values less than 0 to 0." The sigmoid function $\sigma(x)=1/(1+e^{-x})$ happens to be a function that satisfies this property. Therefore, we can design the symmetric KL divergence for GlobalPointer as:</p>

    \begin{equation}D(s, t) = \sum_i (\sigma(s_i) - \sigma(t_i))(s_i - t_i) = \langle \sigma(s) - \sigma(t), s - t \rangle\label{eq:gp-kl}\end{equation}

    <h2>A Breakthrough</h2>

    <p>Interestingly, I later discovered that Equation $\eqref{eq:gp-kl}$ is actually equivalent to applying $\sigma$ activation to each logit separately, calculating the KL divergence for each binary probability distribution individually, and then summing them up.</p>

    <p>Proving this is simple. Note that the binary distribution $[\sigma(s), 1 - \sigma(s)]$ constructed by the $\sigma$ function is equivalent to the binary distribution constructed by using $[s, 0]$ as logits with softmax, i.e., $[\sigma(s), 1 - \sigma(s)] = \text{softmax}([s, 0])$. Therefore, according to formula $\eqref{eq:kl-0}$, we directly have:</p>

    \begin{equation}\begin{aligned}
    &\,D\big([\sigma(s_i),1 - \sigma(s_i)],[\sigma(t_i),1 - \sigma(t_i)]\big) \\
    =&\,(\sigma(s_i)-\sigma(t_i))(s_i - t_i) + \big((1-\sigma(s_i))-(1-\sigma(t_i))\big)(0 - 0)\\
    =&\,(\sigma(s_i)-\sigma(t_i))(s_i - t_i)
    \end{aligned}\end{equation}

    <p>Summing up each component gives us Equation $\eqref{eq:gp-kl}$.</p>

    <p>This equivalence shows that while treating multi-label classification as multiple binary classification problems brings about class imbalance issues, when used merely to evaluate the consistency of results (continuity), the so-called class imbalance problem does not exist (because it is not classification at all). Therefore, it can still be viewed as multiple binary classification problems, and conventional KL divergence can be calculated for them.</p>

    <h2>Experimental Results</h2>

    <p>I and some netizens conducted several simple comparative experiments. The results showed that using Equation $\eqref{eq:gp-kl}$ as the KL divergence to apply R-Drop to GlobalPointer indeed yields a slight improvement in performance. Conversely, if one directly applies softmax to GlobalPointer's logits and then calculates conventional KL divergence, the results are actually worse. This demonstrates the rationality of Equation $\eqref{eq:gp-kl}$.</p>

    <p>However, it should be pointed out that Equation $\eqref{eq:gp-kl}$ merely provides a scheme for using R-Drop or Virtual Adversarial Training within GlobalPointer. Whether there will be an improvement in specific cases is not guaranteed, much like how conventional classification problems paired with R-Drop do not always yield improvements. This requires experimentation, especially regarding the fine-tuning of the regularization weight coefficient.</p>

    <h2>Conclusion</h2>

    <p>This article primarily discussed the calculation of "KL divergence" under GlobalPointer, providing a usable KL divergence form for applying R-Drop or Virtual Adversarial Training to GlobalPointer.</p>
</article>
<hr>
<footer style="margin-top: 3em; padding: 1.5em; background: #f5f5f5; border-radius: 8px; font-size: 0.9em; color: #555;">
    <p style="margin: 0 0 0.5em 0;"><strong>Citation</strong></p>
    <p style="margin: 0 0 0.5em 0;">
        This is a machine translation of the original Chinese article:<br>
        <a href="translation_9039.html" style="color: #005fcc;">https://kexue.fm/archives/9039</a>
    </p>
    <p style="margin: 0 0 0.5em 0;">
        Original author: 苏剑林 (Su Jianlin)<br>
        Original publication: <a href="https://kexue.fm" style="color: #005fcc;">科学空间 (Scientific Spaces)</a>
    </p>
    <p style="margin: 0; font-style: italic;">
        Translated using Gemini 3 Flash. Please refer to the original for authoritative content.
    </p>
</footer>
