
    <style type="text/css">

    body {
    margin: 48px auto;
    max-width: 68ch;              /* character-based width reads better */
    padding: 0 16px;

    font-family: -apple-system, BlinkMacSystemFont, "Segoe UI",
                Roboto, "Helvetica Neue", Arial, sans-serif;
    font-size: 18px;
    line-height: 1.65;
    color: #333;
    background: #fafafa;
    }

    h1, h2, h3, h4 {
    line-height: 1.25;
    margin-top: 2.2em;
    margin-bottom: 0.6em;
    font-weight: 600;
    }

    h1 {
    font-size: 2.1em;
    margin-top: 0;
    }

    h2 {
    font-size: 1.6em;
    border-bottom: 1px solid #e5e5e5;
    padding-bottom: 0.3em;
    }

    h3 {
    font-size: 1.25em;
    }

    h4 {
    font-size: 1.05em;
    color: #555;
    }

    /* Paragraphs and lists */
    p {
    margin: 1em 0;
    }

    ul, ol {
    margin: 1em 0 1em 1.5em;
    }

    li {
    margin: 0.4em 0;
    }

    a {
    color: #005fcc;
    text-decoration: none;
    }

    a:hover {
    text-decoration: underline;
    }

    code {
    font-family: ui-monospace, SFMono-Regular, Menlo, Consolas, monospace;
    font-size: 0.95em;
    background: #f2f2f2;
    padding: 0.15em 0.35em;
    border-radius: 4px;
    }

    pre {
    font-family: ui-monospace, SFMono-Regular, Menlo, Consolas, monospace;
    font-size: 0.9em;
    background: #f5f5f5;
    padding: 1em 1.2em;
    overflow-x: auto;
    border-radius: 6px;
    line-height: 1.45;
    }

    pre code {
    background: none;
    padding: 0;
    }

    blockquote {
    margin: 1.5em 0;
    padding-left: 1em;
    border-left: 4px solid #ddd;
    color: #555;
    }

    hr {
    border: none;
    border-top: 1px solid #e0e0e0;
    margin: 3em 0;
    }

    table {
    border-collapse: collapse;
    margin: 1.5em 0;
    width: 100%;
    font-size: 0.95em;
    }

    th, td {
    padding: 0.5em 0.7em;
    border-bottom: 1px solid #e5e5e5;
    text-align: left;
    }

    th {
    font-weight: 600;
    }

    img {
    max-width: 100%;
    display: block;
    margin: 1.5em auto;
    }

    small {
    color: #666;
    }

    mjx-container {
    margin: 1em 0;
    }

    ::selection {
    background: #cce2ff;
    }

    </style>
    

<script>
window.MathJax = {
  tex: {
    inlineMath: [['$', '$'], ['\\(', '\\)']],
    displayMath: [['$$', '$$'], ['\\[', '\\]']],
    processEscapes: true,
    packages: {'[+]': ['ams']},
    tags: 'ams'
  },
  loader: {load: ['[tex]/ams']}
};
</script>
<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

<article>
  <h1><a href="https://kexue.fm/archives/8027">RealFormer: Moving Residuals to the Attention Matrix</a></h1>
  <p>By 苏剑林 | December 24, 2020</p>

  <p>As is well known, Layer Normalization is one of the crucial components of the Transformer model. Its usage generally falls into two categories: PostLN and PreLN. The paper <a href="https://papers.cool/arxiv/2002.04745">"On Layer Normalization in the Transformer Architecture"</a> provides a detailed analysis of both. Simply put, PreLN is more friendly to gradient descent, converges faster, and is more robust to training hyperparameters like learning rate. In almost every respect, it seems superior, except for one significant drawback: the performance of PreLN always seems slightly worse than PostLN. Recently, a paper from Google titled <a href="https://papers.cool/arxiv/2012.11747">"RealFormer: Transformer Likes Residual Attention"</a> proposed the RealFormer design, successfully bridging this gap. This allows the model to possess the optimization friendliness of PreLN while achieving better performance than PostLN, truly offering the best of both worlds.</p>

  <h2>Form</h2>
  <p>RealFormer stands for "<strong>Re</strong>sidual Attention <strong>L</strong>ayer Trans<strong>former</strong>." As the name suggests, it places the residual connection inside the Attention mechanism.</p>
  <p>Regarding the name, there is a small anecdote. When this blog post was first drafted, RealFormer was actually named Informer, standing for "Res<strong>i</strong>dual Atte<strong>n</strong>tion Trans<strong>former</strong>," and the original paper was titled <a href="https://papers.cool/arxiv/2012.11747v1">"Informer: Transformer Likes Informed Attention"</a>. Obviously, it was hard to guess the full name from "Informer," and I previously criticized Google's somewhat stiff and arbitrary naming. A day later, I discovered it had been renamed RealFormer, so I updated accordingly. It is unclear whether the authors saw my critique or if the rename was due to its name clashing with another paper released just days earlier, <a href="https://papers.cool/arxiv/2012.07436">"Informer: Beyond Efficient Transformer for Long Sequence Time-Series Forecasting."</a></p>

  <p align="center">
    <a href="https://kexue.fm/usr/uploads/2020/12/269460467.png">
      <img src="https://kexue.fm/usr/uploads/2020/12/269460467.png" alt="Schematic of PostLN, PreLN, and RealFormer structures" title="Click to view original image" />
    </a>
    <br />
    <em>Schematic of PostLN, PreLN, and RealFormer structures</em>
  </p>

  <p>Returning to the model, as shown in the figure above, RealFormer primarily moves the residual connection to the Attention matrix while maintaining the overall PostLN structure. Thus, it retains the performance of PostLN while integrating the friendliness of residual connections. Specifically, where the Attention in the $n$-th layer was originally:</p>

  \begin{equation}Attention(\boldsymbol{Q}_n,\boldsymbol{K}_n,\boldsymbol{V}_n) = softmax\left(\boldsymbol{A}_n\right)\boldsymbol{V}_n,\quad \boldsymbol{A}_n=\frac{\boldsymbol{Q}_n\boldsymbol{K}_n^{\top}}{\sqrt{d_k}}\end{equation}

  <p>It has now been changed to:</p>

  \begin{equation}Attention(\boldsymbol{Q}_n,\boldsymbol{K}_n,\boldsymbol{V}_n) = softmax\left(\boldsymbol{A}_n\right)\boldsymbol{V}_n,\quad \boldsymbol{A}_n=\frac{\boldsymbol{Q}_n\boldsymbol{K}_n^{\top}}{\sqrt{d_k}} + \boldsymbol{A}_{n-1}\end{equation}

  <p>And that is essentially it.</p>

  <h2>Experiments</h2>
  <p>Of course, it's impossible to end the discussion there; we must look at the experimental results. But in terms of modifications, it really is that simple. The original paper conducted extensive experiments, and basically all results indicate the following performance ranking:</p>

  $$\text{RealFormer} \geq \text{PostLN} \geq \text{PreLN}$$

  <p>It seems that PostLN might finally be ready for retirement. Some experimental results are shown below:</p>

  <p align="center">
    <a href="https://kexue.fm/usr/uploads/2020/12/4268713135.png">
      <img src="https://kexue.fm/usr/uploads/2020/12/4268713135.png" alt="Comparison of MLM accuracy" title="Click to view original image" />
    </a>
    <br />
    <em>Comparison of MLM accuracy</em>
  </p>

  <p align="center">
    <a href="https://kexue.fm/usr/uploads/2020/12/1521220293.png">
      <img src="https://kexue.fm/usr/uploads/2020/12/1521220293.png" alt="SQuAD evaluation comparison" title="Click to view original image" />
    </a>
    <br />
    <em>SQuAD evaluation comparison</em>
  </p>

  <p align="center">
    <a href="https://kexue.fm/usr/uploads/2020/12/2334514093.png">
      <img src="https://kexue.fm/usr/uploads/2020/12/2334514093.png" alt="GLUE evaluation comparison" title="Click to view original image" />
    </a>
    <br />
    <em>GLUE evaluation comparison</em>
  </p>

  <p align="center">
    <a href="https://kexue.fm/usr/uploads/2020/12/3363546783.png">
      <img src="https://kexue.fm/usr/uploads/2020/12/3363546783.png" alt="Effect comparison across different training steps" title="Click to view original image" />
    </a>
    <br />
    <em>Effect comparison across different training steps</em>
  </p>

  <p>It is worth highlighting the first and fourth figures. From the first figure, we can see that for the RealFormer structure, increasing the model scale (from "large" to "xlarge") leads to a significant performance improvement. In contrast, the ALBERT paper previously mentioned that increasing BERT's model size does not yield significant benefits. Combining these two observations suggests this might be an issue with PostLN rather than an inherent problem with BERT, and switching to RealFormer can improve this. From the fourth figure, we see that training the RealFormer structure for 500,000 steps achieves results equivalent to training PostLN for 1,000,000 steps, indicating that RealFormer has very high training efficiency.</p>
  <p>In addition to these experiments, the paper also provides comparisons across different learning rates and Dropout ratios, showing that RealFormer is indeed quite robust to these parameters. The original paper also analyzed the distribution of Attention scores, showing that the results produced by RealFormer are more reasonable.</p>

  <h2>Analysis</h2>
  <p>In this section, we provide a simple analysis of RealFormer.</p>
  <p>It is not difficult to understand why RealFormer is more friendly to gradient descent. The design $\boldsymbol{A}_n=\frac{\boldsymbol{Q}_n\boldsymbol{K}_n^{\top}}{\sqrt{d_k}} + \boldsymbol{A}_{n-1}$ provides a direct path, allowing the Attention of the first layer to reach the last layer directly, naturally eliminating the risk of gradient vanishing. In contrast, PostLN has a structure of $\text{LayerNorm}(x + f(x))$. While $x+f(x)$ seemingly prevents gradient vanishing, the $\text{LayerNorm}$ step reintroduces the risk. This results in small gradients for earlier layers and large gradients for later layers in the initial stages. If a large learning rate is used, the later layers easily collapse; if a small learning rate is used, the earlier layers do not learn well. Thus, PostLN is harder to train and requires small learning rates combined with a warmup phase.</p>
  <p>So, if PreLN improves gradient conditions, why is it still inferior to PostLN? My guess is that PreLN takes the form $x+f(x)$ at every step, which by the last layer becomes $x + f_1(x) + f_2(x) + \cdots + f_n(x)$. This layer-by-layer accumulation can lead to very large values and variances, necessitating the mandatory addition of a Layer Norm at the final stage to stabilize the output. Thus, while PreLN improves the gradient situation, its design includes some inherent instabilities, which might be why its performance is slightly worse.</p>
  <p>In fact, someone noticed very early on that this characteristic of residuals causes instability. When I was researching GANs, I found that the implementation in the paper <a href="https://papers.cool/arxiv/1801.04406">"Which Training Methods for GANs do actually Converge?"</a> replaced $x + f(x)$ with $x + 0.1 f(x)$. Inspired by their implementation, I tried replacing $x + f(x)$ with $x + \alpha f(x)$, where $\alpha$ is a trainable scalar parameter initialized to 0, which also achieved good results. Earlier this year, the paper <a href="https://papers.cool/arxiv/2003.04887">"ReZero is All You Need: Fast Convergence at Large Depth"</a> formally proposed this method, naming it ReZero. The experiments therein showed that ReZero allows the complete removal of Layer Norm. Unfortunately, the ReZero paper did not conduct more experiments on Transformers, and RealFormer did not compare its effects with ReZero.</p>
  <p>Readers might argue: If PreLN has issues, doesn't RealFormer's $\boldsymbol{A}_n=\frac{\boldsymbol{Q}_n\boldsymbol{K}_n^{\top}}{\sqrt{d_k}} + \boldsymbol{A}_{n-1}$ suffer from the same accumulation problem? If we only look at $\boldsymbol{A}$, then yes, that problem exists. But don't forget that $\boldsymbol{A}$ is normalized by the softmax function before participating in operations. In other words, the model has a built-in normalization function for matrix $\boldsymbol{A}$, so it does not suffer from numerical divergence. On the contrary, as the number of layers increases, the accumulation of $\boldsymbol{A}$ makes the absolute values of its elements potentially larger, causing the Attention to gradually trend towards a one-hot form, which results in gradient vanishing in later layers. However, remember that we previously said PostLN has small gradients in earlier layers and large ones in later layers; here, the gradients of the later layers are also reduced, making the layers more synchronized and thus easier to optimize. On the other hand, the probability values of the Attention might show a trend of convergence, meaning the Attention patterns become increasingly stable, which brings a regularization effect similar to the parameter sharing in ALBERT. This may be beneficial for model performance. Intuitively, using the RealFormer structure for adaptive layer depth improvements like <a href="https://papers.cool/arxiv/2004.02178">FastBERT</a> might yield better results because the convergence trend in RealFormer's Attention aligns better with the design principles of FastBERT.</p>
  <p>Furthermore, we can interpret RealFormer as still using a conventional residual structure, but applying it only to $\boldsymbol{Q}$ and $\boldsymbol{K}$, and not to $\boldsymbol{V}$:</p>

  \begin{equation}\begin{aligned}
  &Attention(\boldsymbol{Q}_n,\boldsymbol{K}_n,\boldsymbol{V}_n) = softmax\left(\boldsymbol{A}_n\right)\boldsymbol{V}_n\\
  &\boldsymbol{A}_n=\frac{\tilde{\boldsymbol{Q}}_n\tilde{\boldsymbol{K}}_n^{\top}}{\sqrt{d_k}},\quad\tilde{\boldsymbol{Q}}_n = \boldsymbol{Q}_n + \tilde{\boldsymbol{Q}}_{n-1}, \quad\tilde{\boldsymbol{K}}_n = \boldsymbol{K}_n + \tilde{\boldsymbol{K}}_{n-1}
  \end{aligned}\end{equation}

  <p>To some extent, this is equivalent to $\boldsymbol{A}_n=\frac{\boldsymbol{Q}_n\boldsymbol{K}_n^{\top}}{\sqrt{d_k}} + \boldsymbol{A}_{n-1}$, whereas PreLN is equivalent to adding residuals to $\boldsymbol{Q}, \boldsymbol{K},$ and $\boldsymbol{V}$. Why is $\boldsymbol{V}$ "not worth" a residual? From recent improvements in relative position encoding, I have noticed a common trend: removing the bias from $\boldsymbol{V}$. For example, in the relative position encoding of NEZHA, the encoding is applied simultaneously to the Attention matrix (i.e., $\boldsymbol{Q}, \boldsymbol{K}$) and $\boldsymbol{V}$. However, in the newer relative position encodings of XLNET and T5, it is only applied to the Attention matrix. Therefore, it seems that removing unnecessary bias from $\boldsymbol{V}$ is a better choice, and RealFormer reflects this once again.</p>

  <h2>Summary</h2>
  <p>This article introduced RealFormer, a new design for Transformers from Google, and provided my own analysis and reflections. Experimental results show that RealFormer simultaneously possesses the advantages of PostLN and PreLN and even outperforms both, making it a valuable improvement to use.</p>
</article>
<hr>
<footer style="margin-top: 3em; padding: 1.5em; background: #f5f5f5; border-radius: 8px; font-size: 0.9em; color: #555;">
    <p style="margin: 0 0 0.5em 0;"><strong>Citation</strong></p>
    <p style="margin: 0 0 0.5em 0;">
        This is a machine translation of the original Chinese article:<br>
        <a href="translation_8027.html" style="color: #005fcc;">https://kexue.fm/archives/8027</a>
    </p>
    <p style="margin: 0 0 0.5em 0;">
        Original author: 苏剑林 (Su Jianlin)<br>
        Original publication: <a href="https://kexue.fm" style="color: #005fcc;">科学空间 (Scientific Spaces)</a>
    </p>
    <p style="margin: 0; font-style: italic;">
        Translated using Gemini 3 Flash. Please refer to the original for authoritative content.
    </p>
</footer>
