
    <style type="text/css">

    body {
    margin: 48px auto;
    max-width: 68ch;              /* character-based width reads better */
    padding: 0 16px;

    font-family: -apple-system, BlinkMacSystemFont, "Segoe UI",
                Roboto, "Helvetica Neue", Arial, sans-serif;
    font-size: 18px;
    line-height: 1.65;
    color: #333;
    background: #fafafa;
    }

    h1, h2, h3, h4 {
    line-height: 1.25;
    margin-top: 2.2em;
    margin-bottom: 0.6em;
    font-weight: 600;
    }

    h1 {
    font-size: 2.1em;
    margin-top: 0;
    }

    h2 {
    font-size: 1.6em;
    border-bottom: 1px solid #e5e5e5;
    padding-bottom: 0.3em;
    }

    h3 {
    font-size: 1.25em;
    }

    h4 {
    font-size: 1.05em;
    color: #555;
    }

    /* Paragraphs and lists */
    p {
    margin: 1em 0;
    }

    ul, ol {
    margin: 1em 0 1em 1.5em;
    }

    li {
    margin: 0.4em 0;
    }

    a {
    color: #005fcc;
    text-decoration: none;
    }

    a:hover {
    text-decoration: underline;
    }

    code {
    font-family: ui-monospace, SFMono-Regular, Menlo, Consolas, monospace;
    font-size: 0.95em;
    background: #f2f2f2;
    padding: 0.15em 0.35em;
    border-radius: 4px;
    }

    pre {
    font-family: ui-monospace, SFMono-Regular, Menlo, Consolas, monospace;
    font-size: 0.9em;
    background: #f5f5f5;
    padding: 1em 1.2em;
    overflow-x: auto;
    border-radius: 6px;
    line-height: 1.45;
    }

    pre code {
    background: none;
    padding: 0;
    }

    blockquote {
    margin: 1.5em 0;
    padding-left: 1em;
    border-left: 4px solid #ddd;
    color: #555;
    }

    hr {
    border: none;
    border-top: 1px solid #e0e0e0;
    margin: 3em 0;
    }

    table {
    border-collapse: collapse;
    margin: 1.5em 0;
    width: 100%;
    font-size: 0.95em;
    }

    th, td {
    padding: 0.5em 0.7em;
    border-bottom: 1px solid #e5e5e5;
    text-align: left;
    }

    th {
    font-weight: 600;
    }

    img {
    max-width: 100%;
    display: block;
    margin: 1.5em auto;
    }

    small {
    color: #666;
    }

    mjx-container {
    margin: 1em 0;
    }

    ::selection {
    background: #cce2ff;
    }

    </style>
    

<script>
window.MathJax = {
  tex: {
    inlineMath: [['$', '$'], ['\\(', '\\)']],
    displayMath: [['$$', '$$'], ['\\[', '\\]']],
    processEscapes: true,
    tags: 'ams'
  }
};
</script>
<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

<h1><a href="https://kexue.fm/archives/4100">Three Visits to Shredded Paper Restoration: CNN-based Shredded Paper Restoration</a></h1>

<p>By 苏剑林 | November 25, 2016</p>

<h3>Problem Review</h3>
<p>I must say, Question B of the 2013 National Mathematical Modeling Contest is truly a once-in-a-century masterpiece among mathematical modeling competitions: the problem is concise and clear, its meaning is rich, the approaches are diverse, and it has strong extensibility—to the point that I have always been obsessed with it. Because of this problem, I have already written two articles on Scientific Space: <a href="translation_2154.html">"A Person's Mathematical Modeling: Shredded Paper Restoration"</a> and <a href="translation_3013.html">"Late by One Year Modeling: Re-exploring Shredded Paper Restoration"</a>. Previously, when I did this problem, I only had a little knowledge of mathematical modeling. Since learning about data mining, especially deep learning, I have always wanted to redo this problem, but I kept procrastinating. I finally implemented it these past few days.</p>

<p>If readers are not clear about the problem, they can refer to the two previous articles. Shredded paper restoration has five attachments, representing five types of "shredded paper fragments," i.e., fragments of five different granularities. Attachments 1 and 2 are not difficult; the difficulty is mainly concentrated in Attachments 3, 4, and 5, where the implementation difficulty for 3, 4, and 5 is basically the same. The easiest approach to think of for this problem is a greedy algorithm: choose any image, find the image that matches it best, and then continue matching the next one. For the greedy algorithm to be effective, the key is to find a good distance function to determine if two fragments are adjacent (horizontally adjacent; vertical adjacency is not considered here).</p>

<p>The previous two articles used the Euclidean distance of edge vectors and mentioned some indicators like correlation coefficients. However, if these are used for Attachments 3, 4, and 5, the results are poor. The reason is that the fragments in Attachments 3, 4, and 5 are not large, and there isn't much edge information available. Therefore, relying only on the edges is not enough; one must also consider multiple factors, such as line spacing, average position of lines, etc. How exactly should these be considered? It is difficult for a human to write a good function. That being the case, why not leave it to the model? Just use a Convolutional Neural Network (CNN)!</p>

<h3>Constructing Samples</h3>
<p>Specifically, we can observe the condition of the fragments, which are roughly:</p>
<ol>
    <li>Size 44 SimHei (Bold) font;</li>
    <li>If considering Attachment 3, the contents are Chinese; if Attachment 4, then it is English;</li>
    <li>The fragments are a fixed size of 72x180.</li>
</ol>

<p>With these features, we can take a bunch of text, follow this specification, and construct a large number of adjacent and non-adjacent samples with the same properties. Then, train a convolutional neural network to automatically obtain a "distance function." This kind of task is extremely simple for friends familiar with deep learning. I directly found some Chinese text and generated a batch of Chinese samples. The code is roughly as follows:</p>

<pre><code>from PIL import Image, ImageFont, ImageDraw
import numpy as np
from scipy import misc
import pymongo
from tqdm import tqdm

texts = list(pymongo.MongoClient().weixin.text_articles.find().limit(1000))
text = texts[0]['text']
line_words = 30
font_size = 44
nb_columns = line_words*font_size/72+1

def gen_img(text):
    n = len(text) / line_words + 1
    size = (nb_columns*72, (n*font_size/180+1)*180)
    im = Image.new('L', size, 255)
    dr = ImageDraw.Draw(im)
    font = ImageFont.truetype('simhei.ttf', font_size)
    for i in range(n):
        dr.text((0, 70*i), text[line_words*i: line_words*(i+1)], font=font)
    im = np.array(im.getdata()).reshape((size[1], size[0]))
    r = []
    for j in range(size[1]/180):
        for i in range(size[0]/72):
            r.append(1-im[j*180:(j+1)*180, i*72:(i+1)*72].T/255.0)
    return r

sample = []
for i in tqdm(iter(texts)):
    sample.extend(gen_img(i['text']))

np.save('sample.npy', sample)
nb_samples = len(sample) - len(sample)/nb_columns

def data(sample, batch_size):
    sample_shuffle_totally = sample[:]
    sample_shuffle_in_line = sample[:]
    while True:
        np.random.shuffle(sample_shuffle_totally)
        for i in range(0, len(sample_shuffle_in_line), nb_columns):
            subsample = sample_shuffle_in_line[i: i+nb_columns]
            np.random.shuffle(subsample)
            sample_shuffle_in_line[i: i+nb_columns] = subsample
        x = []
        y = []
        for i in range(0, len(sample), nb_columns):
            subsample_1 = sample[i: i+nb_columns]
            for j in range(0, nb_columns-1):
                x.append(np.vstack((subsample_1[j], subsample_1[j+1])))
                y.append([1])
            subsample_2 = sample_shuffle_totally[i: i+nb_columns]
            for j in range(0, nb_columns-1):
                x.append(np.vstack((subsample_2[j], subsample_2[j+1])))
                y.append([0])
            subsample_3 = sample_shuffle_in_line[i: i+nb_columns]
            for j in range(0, nb_columns-1):
                x.append(np.vstack((subsample_3[j], subsample_3[j+1])))
                y.append([0])
            if len(y) >= batch_size:
                yield np.array(x), np.array(y)
                x = []
                y = []
        if y:
            yield np.array(x), np.array(y)
            x = []
            y = []
</code></pre>

<p>The process is as follows: I found 1,000 articles, each with several thousand words, uniformly printed each article onto an image, and then cropped them to obtain a batch of samples (<code>sample</code>). The <code>data</code> function that follows is an iterator used to generate positive and negative samples; because loading everything into memory at once is unfeasible, it must be done through an iterator. Even so, I was quite lazy because it still consumed 18G of memory on my server. <code>sample_shuffle_totally</code> consists of random negative samples obtained after completely shuffling the <code>sample</code>; <code>sample_shuffle_in_line</code> is shuffled only within the same line, producing negative samples where line spacing and line positions are the same, but the content is different. I reshuffle every iteration to improve data performance (data augmentation), which greatly increases the number of negative samples actually participating in training.</p>

<p>It should be noted that we are considering horizontal adjacency, but when Python reads matrices, it reads from top to bottom, not left to right. Therefore, we must transpose the image matrix. In addition, the images are normalized; the original grayscale images in the 0–255 range are normalized to the 0–1 range to speed up convergence. Finally, the image matrix is subtracted from 1, essentially performing a color inversion—"black text on white background" becomes "white text on black background." Because when training the network, we hope the input has a large amount of zeros to speed up convergence, and in colors, white is 255 while black is 0. Therefore, "white text on black background" converges faster than "black text on white background."</p>

<h3>Training the Model</h3>
<p>Then we use them to train a CNN. This process is very conventional, using a stack of three convolutional and pooling layers, followed by a softmax classifier—it's that simple~</p>

<p>Model Structure:</p>
<pre>_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_2 (InputLayer)         (None, 144, 180)          0         
_________________________________________________________________
convolution1d_4 (Convolution1D) (None, 143, 32)           11552     
_________________________________________________________________
maxpooling1d_4 (MaxPooling1D) (None, 71, 32)            0         
_________________________________________________________________
convolution1d_5 (Convolution1D) (None, 70, 32)            2080      
_________________________________________________________________
maxpooling1d_5 (MaxPooling1D) (None, 35, 32)            0         
_________________________________________________________________
convolution1d_6 (Convolution1D) (None, 34, 32)            2080      
_________________________________________________________________
maxpooling1d_6 (MaxPooling1D) (None, 17, 32)            0         
_________________________________________________________________
flatten_2 (Flatten)          (None, 544)               0         
_________________________________________________________________
dense_3 (Dense)              (None, 32)                17440     
_________________________________________________________________
dense_4 (Dense)              (None, 1)                 33        
=================================================================
Total params: 33,185
_________________________________________________________________</pre>

<p>Code:</p>
<pre><code>from keras.layers import Input, Convolution1D, MaxPooling1D, Flatten, Dense
from keras.models import Model

input = Input((144, 180))
cnn = Convolution1D(32, 2)(input)
cnn = MaxPooling1D(2)(cnn)
cnn = Convolution1D(32, 2)(cnn)
cnn = MaxPooling1D(2)(cnn)
cnn = Convolution1D(32, 2)(cnn)
cnn = MaxPooling1D(2)(cnn)
cnn = Flatten()(cnn)
dense = Dense(32, activation='relu')(cnn)
dense = Dense(1, activation='sigmoid')(dense)
model = Model(input=input, output=dense)
model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])

model.summary()

model.fit_generator(data(sample, batch_size=1024), 
                    nb_epoch=100, 
                    samples_per_epoch=nb_samples*3
                    )
model.save_weights('2013_suizhifuyuan_cnn.model')
</code></pre>

<p>Actually, 95% accuracy can be achieved after just 3 iterations; this <code>nb_epoch=100</code> was written arbitrarily~ If computational resources are sufficient and you are not in a hurry, more iterations won't hurt. I eventually achieved 97.7% accuracy.</p>

<h3>Stitching Effect</h3>
<p>Now we can test the performance of the "distance function" we trained.</p>

<pre><code>import glob
img_names = glob.glob(u'附件3/*')
images = {}
for i in img_names:
    images[i] = 1 - misc.imread(i, flatten=True).T/255.0

def find_most_similar(img, images):
    imgs_ = np.array([np.vstack((images[img], images[i])) for i in images if i != img])
    img_names_ = [i for i in images if i != img]
    sims = model.predict(imgs_).reshape(-1)
    return img_names_[sims.argmax()]

img = img_names[14]
result = [img]
images_ = images.copy()
while len(images_) > 1:
    print len(images_)
    img_ = find_most_similar(img, images_)
    result.append(img_)
    del images_[img]
    img = img_

images_ = [images[i].T for i in result]
compose = (1 - np.hstack(images_))*255
misc.imsave('result.png', compose)
</code></pre>

<p>For Attachment 3, the result of one-time stitching is (zoom in to see the original image):</p>
<p><strong>Attachment 3 Restoration Degree (CNN + Greedy Algorithm)</strong></p>

<p>For comparison, here is the result of one-time stitching using Euclidean distance previously:</p>
<p><strong>Attachment 3 Restoration Degree (Euclidean Distance + Greedy Algorithm)</strong></p>

<p>It can be seen that the improvement is very significant. Although this model was trained with Chinese corpora, when applied directly to the judgment of Attachment 4, the results are also good:</p>
<p><strong>Attachment 4 Restoration Degree (CNN + Greedy Algorithm)</strong></p>

<p>Similarly, for comparison, the stitching result of Attachment 4 using Euclidean distance is:</p>
<p><strong>Attachment 4 Restoration Degree (Euclidean Distance + Greedy Algorithm)</strong></p>

<p>Thus, it can be seen that the model we obtained is indeed effective and has strong generalization capabilities. Of course, the direct stitching of English is not as good; it is best to include English corpora in the training to achieve better results.</p>

<h3>Postscript</h3>
<p>A good problem must be rich in connotation and enduring. This is already the third article I have written about the shredded paper restoration problem; perhaps there will be a 4th or a 5th, as every study is a deeper dive...</p>

<hr />
<p><em>To reprint, please include the address of this article: <a href="https://kexue.fm/archives/4100">https://kexue.fm/archives/4100</a></em></p>
<p><em>For more detailed reprinting matters, please refer to: <a href="https://kexue.fm/faq.html">Scientific Space FAQ</a></em></p>
<p><em>If you have any doubts or suggestions, please continue the discussion in the comments section below.</em></p>
<p><em>If you think this article is good, welcome to share / donate to this article. Donation is not to obtain profit, but to know how much sincere attention Scientific Space has received from readers. Of course, if you ignore it, it will not affect your reading. Welcome and thank you again!</em></p>

<p><em>If you need to cite this article, please refer to:</em></p>
<p>Su Jianlin. (Nov. 25, 2016). "Three Visits to Shredded Paper Restoration: CNN-based Shredded Paper Restoration" [Blog post]. Retrieved from https://kexue.fm/archives/4100</p>

<hr>
<footer style="margin-top: 3em; padding: 1.5em; background: #f5f5f5; border-radius: 8px; font-size: 0.9em; color: #555;">
    <p style="margin: 0 0 0.5em 0;"><strong>Citation</strong></p>
    <p style="margin: 0 0 0.5em 0;">
        This is a machine translation of the original Chinese article:<br>
        <a href="https://kexue.fm/archives/4100" style="color: #005fcc;">https://kexue.fm/archives/4100</a>
    </p>
    <p style="margin: 0 0 0.5em 0;">
        Original author: 苏剑林 (Su Jianlin)<br>
        Original publication: <a href="https://kexue.fm" style="color: #005fcc;">科学空间 (Scientific Spaces)</a>
    </p>
    <p style="margin: 0; font-style: italic;">
        Translated using Gemini 3 Flash. Please refer to the original for authoritative content.
    </p>
</footer>
