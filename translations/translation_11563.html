
    <style type="text/css">

    body {
    margin: 48px auto;
    max-width: 68ch;              /* character-based width reads better */
    padding: 0 16px;

    font-family: -apple-system, BlinkMacSystemFont, "Segoe UI",
                Roboto, "Helvetica Neue", Arial, sans-serif;
    font-size: 18px;
    line-height: 1.65;
    color: #333;
    background: #fafafa;
    }

    h1, h2, h3, h4 {
    line-height: 1.25;
    margin-top: 2.2em;
    margin-bottom: 0.6em;
    font-weight: 600;
    }

    h1 {
    font-size: 2.1em;
    margin-top: 0;
    }

    h2 {
    font-size: 1.6em;
    border-bottom: 1px solid #e5e5e5;
    padding-bottom: 0.3em;
    }

    h3 {
    font-size: 1.25em;
    }

    h4 {
    font-size: 1.05em;
    color: #555;
    }

    /* Paragraphs and lists */
    p {
    margin: 1em 0;
    }

    ul, ol {
    margin: 1em 0 1em 1.5em;
    }

    li {
    margin: 0.4em 0;
    }

    a {
    color: #005fcc;
    text-decoration: none;
    }

    a:hover {
    text-decoration: underline;
    }

    code {
    font-family: ui-monospace, SFMono-Regular, Menlo, Consolas, monospace;
    font-size: 0.95em;
    background: #f2f2f2;
    padding: 0.15em 0.35em;
    border-radius: 4px;
    }

    pre {
    font-family: ui-monospace, SFMono-Regular, Menlo, Consolas, monospace;
    font-size: 0.9em;
    background: #f5f5f5;
    padding: 1em 1.2em;
    overflow-x: auto;
    border-radius: 6px;
    line-height: 1.45;
    }

    pre code {
    background: none;
    padding: 0;
    }

    blockquote {
    margin: 1.5em 0;
    padding-left: 1em;
    border-left: 4px solid #ddd;
    color: #555;
    }

    hr {
    border: none;
    border-top: 1px solid #e0e0e0;
    margin: 3em 0;
    }

    table {
    border-collapse: collapse;
    margin: 1.5em 0;
    width: 100%;
    font-size: 0.95em;
    }

    th, td {
    padding: 0.5em 0.7em;
    border-bottom: 1px solid #e5e5e5;
    text-align: left;
    }

    th {
    font-weight: 600;
    }

    img {
    max-width: 100%;
    display: block;
    margin: 1.5em auto;
    }

    small {
    color: #666;
    }

    mjx-container {
    margin: 1em 0;
    }

    ::selection {
    background: #cce2ff;
    }

    </style>
    

<script>
window.MathJax = {
  tex: {
    inlineMath: [['$', '$'], ['\\(', '\\)']],
    displayMath: [['$$', '$$'], ['\\[', '\\]']],
    processEscapes: true,
    processEnvironments: true,
    tags: 'ams'
  }
};
</script>
<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

<article>
    <nav style="margin-bottom: 1.5em;">
    <a href="../index.html" style="display: inline-flex; align-items: center; color: #555; text-decoration: none; font-size: 0.95em;">
        <span style="margin-right: 0.3em;">&larr;</span> Back to Index
    </a>
</nav>

    <h1><a href="https://kexue.fm/archives/11563">Elements of the Core Inverse Matrix of DeltaNet are Always within [-1, 1]</a></h1>

    <p>By Jianlin Su | January 26, 2026</p>

    <p>From <a href="translation_11033.html">"A Brief History of Linear Attention: From Imitation, Innovation to Feedback"</a>, we can see that the parallel form of DeltaNet involves an inverse matrix of the form $(I + KK^\top \odot M_-)^{-1}$. Recently, reader <a href="https://kexue.fm/archives/11486/comment-page-1#comment-29143">@Arch123</a> pointed out that experiments show the elements of this inverse matrix always lie within $[-1, 1]$, and asked if this could be mathematically proven or disproven.</p>

    <p>In this article, we will prove that this conclusion holds strictly through two different methods.</p>

    <h2>Problem Description</h2>
    <p>First, let's restate the problem accurately. Let the matrix be $K=[k_1, k_2, \dots, k_n]^\top \in \mathbb{R}^{n \times d}$, where each $k_i \in \mathbb{R}^{d \times 1}$ is a column vector with a norm not exceeding 1. $M \in \mathbb{R}^{n \times n}$ is a lower triangular mask matrix, defined as:</p>
    
    \begin{equation}
    M_{i,j} = \begin{cases} 1, & i \ge j \\ 0, & i < j \end{cases} \label{eq:1}
    \end{equation}

    <p>$I$ is the identity matrix, and $M_- = M - I$. We want to prove that:</p>
    
    \begin{equation}
    (I + KK^\top \odot M_-)^{-1} \in [-1, 1]^{n \times n} \label{eq:2}
    \end{equation}

    <p>Let $X = I + KK^\top \odot M_-$, and denote its inverse matrix as $Y$. Writing $X$ explicitly gives:</p>
    
    \begin{equation}
    X = I + KK^\top \odot M_- = \begin{pmatrix} 1 & 0 & 0 & \cdots & 0 & 0 \\ k_2^\top k_1 & 1 & 0 & \cdots & 0 & 0 \\ k_3^\top k_1 & k_3^\top k_2 & 1 & \cdots & 0 & 0 \\ \vdots & \vdots & \vdots & \ddots & \vdots & \vdots \\ k_{n-1}^\top k_1 & k_{n-1}^\top k_2 & k_{n-1}^\top k_3 & \cdots & 1 & 0 \\ k_n^\top k_1 & k_n^\top k_2 & k_n^\top k_3 & \cdots & k_n^\top k_{n-1} & 1 \end{pmatrix} \label{eq:3}
    \end{equation}

    <p>Since the norm of each $k_i$ does not exceed 1, it is obvious that every element of $X$ lies in $[-1, 1]$. We need to prove that every element of $Y = X^{-1}$ also lies in $[-1, 1]$.</p>

    <h2>Mathematical Induction</h2>
    <p>Since this inverse matrix appears in DeltaNet, it must have a clear model background. Using this background might help us prove it faster, but we'll save that for the next section. In this section, we treat it purely as a mathematical problem.</p>

    <h3>Diagonal Properties</h3>
    <p>We are dealing with a lower triangular matrix. As discussed in <a href="translation_11072.html">"Efficient Inversion Methods for 'Diagonal + Low-Rank' Triangular Matrices"</a>, this class of matrices has excellent properties. First, it is closed under matrix multiplication. Second, it satisfies the property "the diagonal of the inverse is the inverse of the diagonal." This can be generalized to block lower triangular matrices: "the diagonal blocks of the inverse are the inverse of the diagonal blocks." From this, we can conclude that $Y[:n-1, :n-1] = (X[:n-1, :n-1])^{-1}$ and $Y[1:, 1:] = (X[1:, 1:])^{-1}$, where slicing is understood as in Numpy.</p>

    <p>This property suggests using mathematical induction on $n$. Assuming the conclusion holds for any $(n-1) \times d$ matrix $K$, for matrix $X$, we only need to consider the following block partition:</p>

    \begin{equation}
    X = \begin{pmatrix} X_{:n-1, :n-1} & 0 \\ \text{row}_n & 1 \end{pmatrix} \quad \text{or} \quad X = \begin{pmatrix} 1 & 0 \\ \text{col}_1 & X_{1:, 1:} \end{pmatrix} \label{eq:4}
    \end{equation}

    <p>Using $Y[:n-1, :n-1] = (X[:n-1, :n-1])^{-1}$ and $Y[1:, 1:] = (X[1:, 1:])^{-1}$, we can prove that all elements of $Y$ except for the bottom-left $Y_{n,1}$ lie in $[-1, 1]$. Thus, we only need to prove $Y_{n,1} \in [-1, 1]$.</p>

    <h3>Adjugate Matrix</h3>
    <p>To prove $Y_{n,1} \in [-1, 1]$, we consider the explicit representation of $X^{-1}$ using the <a href="https://en.wikipedia.org/wiki/Adjugate_matrix">adjugate matrix</a>. Since $X$ is lower triangular with 1s on the diagonal, its determinant is 1. According to the formula for the inverse via the adjugate matrix, we have:</p>

    \begin{equation}
    Y_{n,1} = (-1)^{n+1} \det \begin{pmatrix} k_2^\top k_1 & 1 & 0 & \cdots & 0 \\ k_3^\top k_1 & k_3^\top k_2 & 1 & \cdots & 0 \\ k_4^\top k_1 & k_4^\top k_2 & k_4^\top k_3 & \cdots & 0 \\ \vdots & \vdots & \vdots & \ddots & \vdots \\ k_n^\top k_1 & k_n^\top k_2 & k_n^\top k_3 & \cdots & k_n^\top k_{n-1} \end{pmatrix} \label{eq:5}
    \end{equation}

    <p>This matrix is derived from a lower triangular matrix where the super-diagonal elements have become 1. Calculating this determinant is more complex, but not overly so, as the last column has only two non-zero elements. Expanding the determinant along the last column reduces it to the subtraction of two determinants of order $n-1$, which still points toward induction.</p>

    <h3>Continued Product Form</h3>
    <p>We can try calculating the first few results manually:</p>

    \begin{align}
    Y_{2,1} &= -k_2^\top k_1 \nonumber \\
    Y_{3,1} &= \det \begin{pmatrix} k_2^\top k_1 & 1 \\ k_3^\top k_1 & k_3^\top k_2 \end{pmatrix} = k_2^\top k_1 k_3^\top k_2 - k_3^\top k_1 = -k_3^\top (I - k_2 k_2^\top) k_1 \nonumber \\
    Y_{4,1} &= - \det \begin{pmatrix} k_2^\top k_1 & 1 & 0 \\ k_3^\top k_1 & k_3^\top k_2 & 1 \\ k_4^\top k_1 & k_4^\top k_2 & k_4^\top k_3 \end{pmatrix} = -k_4^\top (I - k_3 k_3^\top) (I - k_2 k_2^\top) k_1 \label{eq:6}
    \end{align}

    <p>From this, we can conjecture:</p>
    
    \begin{equation}
    Y_{n,1} = -k_n^\top (I - k_{n-1} k_{n-1}^\top) (I - k_{n-2} k_{n-2}^\top) \dots (I - k_2 k_2^\top) k_1 \label{eq:8}
    \end{equation}

    <p>I invite readers to prove this conjecture using mathematical induction. Assuming the proof is complete, considering the condition $\|k_i\| \le 1$, we have $\|(I - k_i k_i^\top)x\|^2 = \|x\|^2 + (k_i^\top x)^2 (\|k_i\|^2 - 2) \le \|x\|^2$. Therefore:</p>

    \begin{equation}
    |Y_{n,1}| \le \|k_n\| \times \|(I - k_{n-1} k_{n-1}^\top) \dots (I - k_2 k_2^\top) k_1\| \le \|k_n\| \times \|k_1\| \le 1 \label{eq:9}
    \end{equation}

    <p>This completes the proof that $Y_{n,1} \in [-1, 1]$, and by induction, $Y \in [-1, 1]^{n \times n}$ holds for all $n$.</p>

    <h2>Dual Computation</h2>
    <p>The previous direct proof was somewhat "brute-force." Later, members of the FLA group reminded me that we can return to the original context of the inverse matrix—DeltaNet—and perform calculations in two different ways. By comparing the results, we can derive the explicit expression of the inverse matrix, leading to a more concise proof.</p>

    <h3>Inverse Matrix Form</h3>
    <p>We know the recursive form of DeltaNet is:</p>
    
    \begin{equation}
    S_t = S_{t-1}(I - k_t k_t^\top) + v_t k_t^\top = S_{t-1} + (v_t - S_{t-1} k_t) k_t^\top \label{eq:10}
    \end{equation}

    <p>where $S_0 = 0$. Looking at the second equality, let $u_t = v_t - S_{t-1} k_t$. Then:</p>
    
    \begin{equation}
    S_t = S_{t-1} + u_t k_t^\top = \sum_{i=1}^t u_i k_i^\top \label{eq:11}
    \end{equation}

    <p>So,</p>
    
    \begin{equation}
    u_t = v_t - S_{t-1} k_t = v_t - \left(\sum_{i=1}^{t-1} u_i k_i^\top \right) k_t = v_t - \sum_{i=1}^{t-1} u_i (k_i^\top k_t) \label{eq:12}
    \end{equation}

    <p>Defining $U = [u_1, u_2, \dots, u_n]^\top$ and $V = [v_1, v_2, \dots, v_n]^\top$, the above equation is equivalent to $U = V - (KK^\top \odot M_-) U$, or $U = (I + KK^\top \odot M_-)^{-1} V$. This is the origin of the inverse matrix $(I + KK^\top \odot M_-)^{-1}$, which is key to parallel computing in DeltaNet and subsequent works like GDN and KDA.</p>

    <h3>Recursive Expansion</h3>
    <p>Now looking at the first equality, expanding it recursively gives:</p>
    
    \begin{equation}
    S_t = v_1 k_1^\top H_{2 \to t} + \dots + v_{t-1} k_{t-1}^\top H_{t \to t} + v_t k_t^\top = \sum_{i=1}^t v_i k_i^\top H_{i+1 \to t} \label{eq:13}
    \end{equation}

    <p>Here $H_{r \to t} \triangleq (I - k_r k_r^\top) (I - k_{r+1} k_{r+1}^\top) \dots (I - k_t k_t^\top)$, with the convention $H_{t+1 \to t} = I$. Substituting this into the definition of $u_t$:</p>
    
    \begin{equation}
    u_t = v_t - S_{t-1} k_t = v_t - \sum_{i=1}^{t-1} v_i k_i^\top H_{i+1 \to t-1} k_t \label{eq:14}
    \end{equation}

    <h3>Comparison of Results</h3>
    <p>Again letting $Y = (I + KK^\top \odot M_-)^{-1}$, then $U = YV$ implies $u_t = \sum_{i=1}^n Y_{t,i} v_i$. By comparing with the equation above, we can identify:</p>
    
    \begin{equation}
    Y_{t,i} = \begin{cases} 0, & i > t \\ 1, & i = t \\ -k_i^\top H_{i+1 \to t-1} k_t, & i < t \end{cases} \label{eq:15}
    \end{equation}

    <p>Therefore, we only need to prove $|k_i^\top H_{i+1 \to t-1} k_t| \le 1$. Given $\|k_i\| \le 1$ and the property $\|(I - k_j k_j^\top)x\| \le \|x\|$ established in the first proof, it follows immediately that:</p>
    
    \begin{equation}
    |k_i^\top H_{i+1 \to t-1} k_t| \le \|k_i^\top\| \times \|H_{i+1 \to t-1} k_t\| \le \|k_i^\top\| \times \|k_t\| \le 1 \label{eq:16}
    \end{equation}

    <p>Thus, the theorem is proven.</p>

    <h2>Summary</h2>
    <p>This article provides two proofs for the boundedness of the core inverse matrix elements in DeltaNet.</p>
</article>
<hr>
<footer style="margin-top: 3em; padding: 1.5em; background: #f5f5f5; border-radius: 8px; font-size: 0.9em; color: #555;">
    <p style="margin: 0 0 0.5em 0;"><strong>Citation</strong></p>
    <p style="margin: 0 0 0.5em 0;">
        This is a machine translation of the original Chinese article:<br>
        <a href="https://kexue.fm/archives/11563" style="color: #005fcc;">https://kexue.fm/archives/11563</a>
    </p>
    <p style="margin: 0 0 0.5em 0;">
        Original author: 苏剑林 (Su Jianlin)<br>
        Original publication: <a href="https://kexue.fm" style="color: #005fcc;">科学空间 (Scientific Spaces)</a>
    </p>
    <p style="margin: 0; font-style: italic;">
        Translated using Gemini 3 Flash. Please refer to the original for authoritative content.
    </p>
</footer>
