
    <style type="text/css">

    body {
    margin: 48px auto;
    max-width: 68ch;              /* character-based width reads better */
    padding: 0 16px;

    font-family: -apple-system, BlinkMacSystemFont, "Segoe UI",
                Roboto, "Helvetica Neue", Arial, sans-serif;
    font-size: 18px;
    line-height: 1.65;
    color: #333;
    background: #fafafa;
    }

    h1, h2, h3, h4 {
    line-height: 1.25;
    margin-top: 2.2em;
    margin-bottom: 0.6em;
    font-weight: 600;
    }

    h1 {
    font-size: 2.1em;
    margin-top: 0;
    }

    h2 {
    font-size: 1.6em;
    border-bottom: 1px solid #e5e5e5;
    padding-bottom: 0.3em;
    }

    h3 {
    font-size: 1.25em;
    }

    h4 {
    font-size: 1.05em;
    color: #555;
    }

    /* Paragraphs and lists */
    p {
    margin: 1em 0;
    }

    ul, ol {
    margin: 1em 0 1em 1.5em;
    }

    li {
    margin: 0.4em 0;
    }

    a {
    color: #005fcc;
    text-decoration: none;
    }

    a:hover {
    text-decoration: underline;
    }

    code {
    font-family: ui-monospace, SFMono-Regular, Menlo, Consolas, monospace;
    font-size: 0.95em;
    background: #f2f2f2;
    padding: 0.15em 0.35em;
    border-radius: 4px;
    }

    pre {
    font-family: ui-monospace, SFMono-Regular, Menlo, Consolas, monospace;
    font-size: 0.9em;
    background: #f5f5f5;
    padding: 1em 1.2em;
    overflow-x: auto;
    border-radius: 6px;
    line-height: 1.45;
    }

    pre code {
    background: none;
    padding: 0;
    }

    blockquote {
    margin: 1.5em 0;
    padding-left: 1em;
    border-left: 4px solid #ddd;
    color: #555;
    }

    hr {
    border: none;
    border-top: 1px solid #e0e0e0;
    margin: 3em 0;
    }

    table {
    border-collapse: collapse;
    margin: 1.5em 0;
    width: 100%;
    font-size: 0.95em;
    }

    th, td {
    padding: 0.5em 0.7em;
    border-bottom: 1px solid #e5e5e5;
    text-align: left;
    }

    th {
    font-weight: 600;
    }

    img {
    max-width: 100%;
    display: block;
    margin: 1.5em auto;
    }

    small {
    color: #666;
    }

    mjx-container {
    margin: 1em 0;
    }

    ::selection {
    background: #cce2ff;
    }

    </style>
    

<script>
window.MathJax = {
  tex: {
    inlineMath: [['$', '$'], ['\\(', '\\)']],
    displayMath: [['$$', '$$'], ['\\[', '\\]']],
    tags: 'ams',
    packages: {'[+]': ['ams']}
  }
};
</script>
<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

<article>
    <nav style="margin-bottom: 1.5em;">
    <a href="../index.html" style="display: inline-flex; align-items: center; color: #555; text-decoration: none; font-size: 0.95em;">
        <span style="margin-right: 0.3em;">&larr;</span> Back to Index
    </a>
</nav>

    <h1><a href="https://kexue.fm/archives/8711">Analysis of the Usability of the Dimension Formula "n > 8.33 log N"</a></h1>

    <p>By 苏剑林 | September 27, 2021</p>

    <p>In the previous article <a href="translation_7695.html">"Minimum Entropy Principle (VI): How to choose the dimension of word vectors?"</a>, we derived a word vector dimension formula "$n > 8.33 \log N$" based on minimum entropy principles. Then, in <a href="translation_8706.html">"The Amazing Johnson-Lindenstrauss Lemma: Application Edition"</a>, we further pointed out that this result is consistent with the $\mathcal{O}(\log N)$ provided by the JL lemma.</p>

    <p>Since it looks perfect in theory, readers naturally ask: what about the experimental results? Is the coefficient 8.33 optimal? This article provides a brief summary of relevant content regarding this problem.</p>

    <h2>Word Vectors</h2>

    <p>First, we can directly calculate that when $N$ is 100,000, $8.33 \log N \approx 96$, and when $N$ is 5 million, $8.33 \log N \approx 128$. This indicates that, at least in terms of order of magnitude, the results given by the formula are very much in line with the dimensions we actually use, because in the era of word vectors, the dimensions of the word vectors we trained ourselves were around 100 dimensions. Some readers might question that most open-source word vectors currently are 300-dimensional, and even the Embedding layer of BERT reaches 768 dimensions—doesn't this clearly deviate from your results?</p>

    <p>In fact, while open-source word vectors like FastText are 300-dimensional, this does not negate the possibility that 128 dimensions could achieve similar effects. As for BERT, it is not a word vector model per se, so the dimension it chooses has no direct relationship with the selection of word vector dimensions. Furthermore, ALBERT has already shown that performing low-rank decomposition on the Embedding layer (reducing it to 128 dimensions) hardly changes the model's performance, so BERT's 768-dimensional Embedding is likely redundant to some extent.</p>

    <p>Regarding the evaluation of word vectors, one can refer to the more comprehensive 2015 paper <a href="https://papers.cool/arxiv/1507.05523">"How to Generate a Good Word Embedding?"</a>. The paper shows that the improvement of word vectors becomes quite weak after exceeding 50 dimensions, which serves as a piece of evidence for $n > 8.33 \log N$.</p>

    <h2>Attention</h2>

    <p>Another indirect experimental proof for the formula $n > 8.33 \log N$ comes from the attention mechanism. In <a href="translation_8706.html">"The Amazing Johnson-Lindenstrauss Lemma: Application Edition"</a>, we analyzed that the calculation formula for the Attention matrix is mathematically equivalent to the Skip Gram model of word vectors. This means the formula $n > 8.33 \log N$ can also be applied to the choice of head_size in attention mechanisms.</p>

    <p>In the attention mechanism, $N$ is the sequence length to be processed. A common pre-training length is 512; substituting this gives $8.33 \log 512 \approx 52$. This is very close to the standard head_size of $64$ used in current mainstream models. Therefore, this indirectly proves the usability of $n > 8.33 \log N$. Conversely, if we accept this formula, it explains why the head_size of the attention mechanism only needs to be 64, and indirectly explains why the attention mechanism uses multiple small heads instead of one large head.</p>

    <p>Regarding the choice of head_size and expressive power in attention mechanisms, you can also refer to <a href="https://papers.cool/arxiv/2106.03764">"On the Expressive Power of Self-Attention Matrices"</a>.</p>

    <h2>Graph Networks</h2>

    <p>If we treat each word as a node and the co-occurrence between words as an edge, then Skip Gram can also be viewed as a simple graph model. Therefore, the results regarding word vector dimension selection can theoretically be used for embedding dimension selection in graph networks.</p>

    <p>Results in this area can be found in the paper <a href="https://papers.cool/arxiv/2105.03178">"Graph Entropy Guided Node Embedding Dimension Selection for Graph Neural Networks"</a>. The paper considers both the feature entropy and the structural entropy of the graph. The feature entropy part is similar to Skip Gram and adopts the same approximation as used in <a href="translation_7695.html">"Minimum Entropy Principle (VI): How to choose the dimension of word vectors?"</a>. Thus, this part is essentially the formula $n > 8.33 \log N$.</p>

    <p>After combining feature entropy and structural entropy, the calculated results were used as the embedding dimension for various graph tasks. Experimental results show that this method indeed achieves superior dimension selection results:</p>

    <p style="text-align: center;">
        <img src="https://img.kexue.fm/2021/09/27/entropy_dim.png" alt="Entropy-based dimension selection" style="max-width: 100%;">
        <br>
        <em>Entropy-based dimension selection</em>
    </p>

    <h2>Summary</h2>

    <p>This article analyzed the usability of the previously derived dimension selection formula $n > 8.33 \log N$. By synthesizing existing experimental results from word vectors, attention mechanisms, and graph networks, it is shown that this formula can yield reasonable dimension estimations. It also suggests that using entropy to further determine the constant for $\log N$ in the JL lemma might be a viable approach.</p>
</article>
<hr>
<footer style="margin-top: 3em; padding: 1.5em; background: #f5f5f5; border-radius: 8px; font-size: 0.9em; color: #555;">
    <p style="margin: 0 0 0.5em 0;"><strong>Citation</strong></p>
    <p style="margin: 0 0 0.5em 0;">
        This is a machine translation of the original Chinese article:<br>
        <a href="https://kexue.fm/archives/8711" style="color: #005fcc;">https://kexue.fm/archives/8711</a>
    </p>
    <p style="margin: 0 0 0.5em 0;">
        Original author: 苏剑林 (Su Jianlin)<br>
        Original publication: <a href="https://kexue.fm" style="color: #005fcc;">科学空间 (Scientific Spaces)</a>
    </p>
    <p style="margin: 0; font-style: italic;">
        Translated using Gemini 3 Flash. Please refer to the original for authoritative content.
    </p>
</footer>
