
    <style type="text/css">

    body {
    margin: 48px auto;
    max-width: 68ch;              /* character-based width reads better */
    padding: 0 16px;

    font-family: -apple-system, BlinkMacSystemFont, "Segoe UI",
                Roboto, "Helvetica Neue", Arial, sans-serif;
    font-size: 18px;
    line-height: 1.65;
    color: #333;
    background: #fafafa;
    }

    h1, h2, h3, h4 {
    line-height: 1.25;
    margin-top: 2.2em;
    margin-bottom: 0.6em;
    font-weight: 600;
    }

    h1 {
    font-size: 2.1em;
    margin-top: 0;
    }

    h2 {
    font-size: 1.6em;
    border-bottom: 1px solid #e5e5e5;
    padding-bottom: 0.3em;
    }

    h3 {
    font-size: 1.25em;
    }

    h4 {
    font-size: 1.05em;
    color: #555;
    }

    /* Paragraphs and lists */
    p {
    margin: 1em 0;
    }

    ul, ol {
    margin: 1em 0 1em 1.5em;
    }

    li {
    margin: 0.4em 0;
    }

    a {
    color: #005fcc;
    text-decoration: none;
    }

    a:hover {
    text-decoration: underline;
    }

    code {
    font-family: ui-monospace, SFMono-Regular, Menlo, Consolas, monospace;
    font-size: 0.95em;
    background: #f2f2f2;
    padding: 0.15em 0.35em;
    border-radius: 4px;
    }

    pre {
    font-family: ui-monospace, SFMono-Regular, Menlo, Consolas, monospace;
    font-size: 0.9em;
    background: #f5f5f5;
    padding: 1em 1.2em;
    overflow-x: auto;
    border-radius: 6px;
    line-height: 1.45;
    }

    pre code {
    background: none;
    padding: 0;
    }

    blockquote {
    margin: 1.5em 0;
    padding-left: 1em;
    border-left: 4px solid #ddd;
    color: #555;
    }

    hr {
    border: none;
    border-top: 1px solid #e0e0e0;
    margin: 3em 0;
    }

    table {
    border-collapse: collapse;
    margin: 1.5em 0;
    width: 100%;
    font-size: 0.95em;
    }

    th, td {
    padding: 0.5em 0.7em;
    border-bottom: 1px solid #e5e5e5;
    text-align: left;
    }

    th {
    font-weight: 600;
    }

    img {
    max-width: 100%;
    display: block;
    margin: 1.5em auto;
    }

    small {
    color: #666;
    }

    mjx-container {
    margin: 1em 0;
    }

    ::selection {
    background: #cce2ff;
    }

    </style>
    

<script>
window.MathJax = {
  tex: {
    inlineMath: [['$', '$'], ['\\(', '\\)']],
    displayMath: [['$$', '$$'], ['\\[', '\\]']],
    processEscapes: true,
    tags: 'ams'
  },
  options: {
    renderActions: {
      findScript: [10, function (doc) {
        for (const node of document.querySelectorAll('script[type^="math/tex"]')) {
          const display = !!node.type.match(/; *mode=display/);
          const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
          const text = document.createTextNode('');
          node.parentNode.replaceChild(text, node);
          math.start = {node: text, delim: '', n: 0};
          math.end = {node: text, delim: '', n: 0};
          doc.math.push(math);
        }
      }, '']
    }
  }
};
</script>
<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

<article>
    <nav style="margin-bottom: 1.5em;">
    <a href="../index.html" style="display: inline-flex; align-items: center; color: #555; text-decoration: none; font-size: 0.95em;">
        <span style="margin-right: 0.3em;">&larr;</span> Back to Index
    </a>
</nav>

    <h1><a href="https://kexue.fm/archives/6771">NL2SQL Model Based on Bert: A Concise Baseline</a></h1>
    <p>By 苏剑林 | June 29, 2019</p>

    <p>In the previous article <a href="translation_6736.html">"When Bert Meets Keras: This Might Be the Simplest Way to Open Bert,"</a> we introduced three NLP examples based on fine-tuning Bert, experiencing the power of Bert and the convenience of Keras. In this article, we add another example: an NL2SQL model based on Bert.</p>

    <p>NL2SQL stands for Natural Language to SQL, which means converting natural language into SQL statements. It has been a subject of much research in recent years and is considered a practical task in the field of artificial intelligence. The opportunity for me to build this model was the first <a href="https://tianchi.aliyun.com/markets/tianchi/zhuiyi_cn">"Chinese NL2SQL Challenge"</a> hosted by our company:</p>

    <blockquote>
        The first Chinese NL2SQL Challenge uses table data from the financial and general fields as data sources, providing natural language and SQL statement matching pairs annotated on this basis. It is hoped that contestants can use the data to train a model that can accurately convert natural language to SQL.
    </blockquote>

    <p>This NL2SQL competition is a relatively large NLP event this year, with significant manpower and resources invested in promotion and a generous prize pool. The only issue is that NL2SQL itself is a somewhat niche research field, so it might not be extremely popular. To lower the entry barrier, the organizers released a <a href="https://github.com/ZhuiyiTechnology/nl2sql_baseline">Baseline</a> written in PyTorch.</p>

    <p>With the mindset that "there shouldn't be a lack of a Keras version for the Baseline," I took some time to work on this competition using Keras. To simplify the model and improve results, I also loaded the pre-trained Bert model, which resulted in this article.</p>

    <h2>Data Example</h2>
    <p>Each data sample is as follows:</p>

<pre><code>{
 "table_id": "a1b2c3d4", # ID of the corresponding table
 "question": "The volume ratio of the new Shimao Mao Yue Mansion development is greater than 1, what is its average area per unit?", # Natural language question
 "sql":{ # True SQL
 "sel": [7], # Columns selected by the SQL
 "agg": [0], # Aggregation function corresponding to the selected column, '0' represents none
 "cond_conn_op": 0, # Relationship between conditions
 "conds": [
 [1, 2, "Shimao Mao Yue Mansion"], # Condition column, condition type, condition value, col_1 == "Shimao Mao Yue Mansion"
 [6, 0, "1"]
 ]
 }
}

# The condition operators, aggregation operators, and connection operators are as follows:
op_sql_dict = {0:">", 1:"<", 2:"==", 3:"!="}
agg_sql_dict = {0:"", 1:"AVG", 2:"MAX", 3:"MIN", 4:"COUNT", 5:"SUM"}
conn_sql_dict = {0:"", 1:"and", 2:"or"}
</code></pre>

    <p>Each sample corresponds to a data table containing all column names of that table and the corresponding data records. In principle, the generated SQL statements should be executable on the corresponding data table and return valid results.</p>

    <p>It can be seen that although it is called NL2SQL, the organizers have actually formatted the SQL statements very clearly. In this way, the task can be greatly simplified. For example, the <code>sel</code> field is actually a multi-label classification model, but the categories may change at any time because the categories here actually correspond to the columns of the data table. Since the data tables and meanings for each sample are different, we must dynamically encode a category vector based on the table's column names. As for <code>agg</code>, there is a one-to-one correspondence with <code>sel</code>, and the categories are fixed. The <code>cond_conn_op</code> is a single-label classification problem.</p>

    <p>The final <code>conds</code> is relatively more complex. it requires a combination of sequence labeling and classification because it needs to simultaneously determine which column is the condition, the operation relationship of the condition, and the value corresponding to the condition. It should be noted that the condition value is not always a fragment of the <code>question</code>; it may be a formatted result. For example, if the <code>question</code> contains "year 16", the condition value might be the formatted "2016". however, since the organizers guarantee that the generated SQL can be executed on the corresponding data table and yield valid results, if the condition operator is "==", then the condition value will definitely appear in the values of the corresponding column in the data table. For example, in the sample above, the first column of the data table must contain the value "Shimao Mao Yue Mansion", and through this information, we can also calibrate the prediction results.</p>

    <h2>Model Structure</h2>
    <p><strong>Before formally looking at this model, readers might want to think for a moment and consider how they would do it. Only after thinking will you understand where the difficulties lie, and only then can you understand the key points of some processing techniques in this model.</strong></p>

    <p>The model schematic diagram in this article is as follows:</p>

    <p style="text-align:center;">
        <img src="https://kexue.fm/usr/uploads/2019/06/2888177543.png" alt="Schematic diagram of the NL2SQL model in this article. It mainly includes 4 different classifiers: sequence labeler" />
        <br />
        <em>Schematic diagram of the NL2SQL model in this article. It mainly includes 4 different classifiers: sequence labeler, etc.</em>
    </p>

    <p>For a SQL statement, the most basic part is to decide which columns will be <code>select</code>-ed. Since the meaning of the columns in each table is different, we concatenate the <code>question</code> sentence with all the table headers of the data table and input them together into the Bert model for real-time encoding. Each table header is also treated as a sentence, enclosed by <code>[CLS]***[SEP]</code>. After Bert, we obtain a series of encoding vectors, and then we see how to use these vectors.</p>

    <p>We can consider the vector corresponding to the first <code>[CLS]</code> as the sentence vector for the entire question, and we use it to predict the connection operator for <code>conds</code>. The vectors corresponding to each subsequent <code>[CLS]</code> are considered the encoding vectors for each table header. We extract them to predict whether the column represented by that table header should be <code>select</code>-ed. Note that there is a trick in the prediction here: besides predicting <code>sel</code>, we also need to predict the corresponding <code>agg</code>. There are a total of 6 categories for <code>agg</code>, representing different operations. Therefore, we simply add an additional category, the 7th category, representing that this column is not <code>select</code>-ed. In this way, each column corresponds to a 7-class classification problem. If it is classified into the first 6 categories, it means the column is <code>select</code>-ed and the <code>agg</code> is predicted at the same time; if it falls into the 7th category, it means the column is not <code>select</code>-ed.</p>

    <p>Now we are left with the more complex <code>conds</code>, such as <code>where col_1 == value_1</code>. We need to find <code>col_1</code>, <code>value_1</code>, and the operator <code>==</code>. The prediction of <code>conds</code> is divided into two steps: the first step predicts the condition values, and the second step predicts the condition columns. Predicting the condition value is actually a sequence labeling problem. There are 4 operators corresponding to the condition value; we also add an additional category to make it 5, where the 5th category represents that the current character is not labeled, otherwise it is labeled. In this way, we can predict the condition value and the operator. The remaining part is to predict the column corresponding to the condition value. We calculate the similarity between the character vectors of the labeled value and the vector of each table header one by one, and then apply softmax. My method for calculating similarity here is the simplest: directly concatenate the character vector and the table header vector, then pass it through a fully connected layer followed by a <code>Dense(1)</code>. The reason for making it so simple is, first, because the main purpose of this article is to provide a basically feasible demo rather than a perfect program, leaving some room for improvement for the reader; second, because making it more complex could easily lead to insufficient video memory and OOM (Out of Memory).</p>

    <p><strong>By the way, the model in this article was "constructed independently" based on the competition task. If readers want to discuss mainstream NL2SQL models with me, I may be of no help. Please understand.</strong></p>

    <h2>Experimental Results</h2>
    <p>The code for the model in this article is located at:</p>

    <p><a href="https://github.com/bojone/bert_in_keras/blob/master/nl2sql_baseline.py">https://github.com/bojone/bert_in_keras/blob/master/nl2sql_baseline.py</a></p>

    <p>Note: If you run this code and get an error, you may need to modify Keras's <code>backend/tensorflow_backend.py</code>. In the <code>sparse_categorical_crossentropy</code> function, change the original line:</p>

<pre><code>logits = tf.reshape(output, [-1, int(output_shape[-1])])
</code></pre>

    <p>to</p>

<pre><code>logits = tf.reshape(output, [-1, tf.shape(output)[-1]])
</code></pre>

    <p>I have submitted this fix to the official repository, and it has been approved (please see <a href="https://github.com/keras-team/keras/commit/613aeff37a721450d94906df1a3f3cc51e2299d4">here</a>). This feature should be automatically included in future versions.</p>

    <p>Again, as long as you have carefully observed the competition data and thought about this task independently, the model introduced in this article is actually very easy to understand. The fact that a simple model can achieve good results is due to Bert's powerful semantic encoding capability. On the offline valid set, the SQL exact match rate generated by the model in this article is about 58%. The official evaluation metric is (Exact Match Rate + Execution Match Rate) / 2, which means you might write a SQL statement different from the labeled answer, but if the execution result is consistent, it counts as half-correct.</p>

    <p>As a result, the final score will definitely be higher than 58%, and I estimate it to be around <strong>65%</strong>. Looking at the current leaderboard, 65% would place in the top few ranks (the top player is currently at 70%). Since company employees are not allowed to participate in the rankings, I haven't participated in the evaluation and don't know the online submission score. Interested contestants can submit their own tests. </p>

    <p>Additionally, to run this script, it's best to have a 1080ti or higher graphics card. If you don't have that much memory, you can try reducing <code>maxlen</code> and <code>batch size</code>. Also, there are currently two Chinese pre-trained Bert weights available: the <a href="https://github.com/google-research/bert">Official version</a> and the <a href="https://github.com/ymcui/Chinese-BERT-wwm">Harbin Institute of Technology (HIT) version</a>. The final results of the two are similar, but the HIT version converges faster.</p>

    <p>Looking at the entire model, the most difficult part of the implementation is the careful consideration of various masks. In the script mentioned above, <code>xm, hm, cm</code> are three mask variables used to remove the effects brought by the padding part during the training process. Note that masks are not unique to Keras; whether you use TensorFlow or PyTorch, theoretically, you must handle masks carefully. If readers really cannot understand the mask part, you are welcome to leave a comment to ask and discuss, but before asking, please answer the following question:</p>

    <blockquote>
        What does the sequence look like before the mask? Which positions in the sequence changed after the mask? How did they change?
    </blockquote>

    <p>Answering this question proves "you already understand what calculations the program performed, you just don't understand why it calculated that way." If you don't even understand the calculation itself, I'm afraid it will be very difficult for us to communicate (you should at least be able to tell which part changed...). You'd better learn Keras or TensorFlow properly before playing with this; you can't expect to succeed in one step.</p>

    <h2>Post-processing</h2>
    <p>For the model, implementation difficulty lies in the masks. However, if you look at the entire script, the largest proportion of code is actually for data reading, pre-processing, and result post-processing. Building the model itself takes only about twenty lines (once again marvelling at the simplicity of Keras and the power of Bert).</p>

    <p>As mentioned, the condition value does not necessarily appear in the <code>question</code>. How do we extract the condition value using sequence labeling on the <code>question</code>?</p>

    <p>My method is that if the condition value does not appear in the <code>question</code>, I segment the <code>question</code> and find all 1-grams, 2-grams, and 3-grams of the <code>question</code>. Then, I find the n-gram closest to the condition value as the labeled fragment. During prediction, if an n-gram is found as the condition value and the operator is "==", we check if this n-gram has appeared in the database. If it has, it's kept directly; if not, we find the closest value in the database.</p>

    <p>The corresponding processes are all reflected in the code; feel free to read carefully.</p>

    <h2>Summary</h2>
    <p>Everyone is welcome to play~</p>

    <p style="text-align:center;">
        <img src="https://kexue.fm/usr/uploads/2019/06/3389819572.png" alt="First Chinese NL2SQL Challenge" />
        <br />
        <em>First Chinese NL2SQL Challenge</em>
    </p>

    <blockquote>
        <a href="https://tianchi.aliyun.com/markets/tianchi/zhuiyi_cn">https://tianchi.aliyun.com/markets/tianchi/zhuiyi_cn</a>
    </blockquote>

    <p>I wish everyone good results!</p>
</article>
<hr>
<footer style="margin-top: 3em; padding: 1.5em; background: #f5f5f5; border-radius: 8px; font-size: 0.9em; color: #555;">
    <p style="margin: 0 0 0.5em 0;"><strong>Citation</strong></p>
    <p style="margin: 0 0 0.5em 0;">
        This is a machine translation of the original Chinese article:<br>
        <a href="https://kexue.fm/archives/6771" style="color: #005fcc;">https://kexue.fm/archives/6771</a>
    </p>
    <p style="margin: 0 0 0.5em 0;">
        Original author: 苏剑林 (Su Jianlin)<br>
        Original publication: <a href="https://kexue.fm" style="color: #005fcc;">科学空间 (Scientific Spaces)</a>
    </p>
    <p style="margin: 0; font-style: italic;">
        Translated using Gemini 3 Flash. Please refer to the original for authoritative content.
    </p>
</footer>
