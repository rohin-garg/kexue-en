
    <style type="text/css">

    body {
    margin: 48px auto;
    max-width: 68ch;              /* character-based width reads better */
    padding: 0 16px;

    font-family: -apple-system, BlinkMacSystemFont, "Segoe UI",
                Roboto, "Helvetica Neue", Arial, sans-serif;
    font-size: 18px;
    line-height: 1.65;
    color: #333;
    background: #fafafa;
    }

    h1, h2, h3, h4 {
    line-height: 1.25;
    margin-top: 2.2em;
    margin-bottom: 0.6em;
    font-weight: 600;
    }

    h1 {
    font-size: 2.1em;
    margin-top: 0;
    }

    h2 {
    font-size: 1.6em;
    border-bottom: 1px solid #e5e5e5;
    padding-bottom: 0.3em;
    }

    h3 {
    font-size: 1.25em;
    }

    h4 {
    font-size: 1.05em;
    color: #555;
    }

    /* Paragraphs and lists */
    p {
    margin: 1em 0;
    }

    ul, ol {
    margin: 1em 0 1em 1.5em;
    }

    li {
    margin: 0.4em 0;
    }

    a {
    color: #005fcc;
    text-decoration: none;
    }

    a:hover {
    text-decoration: underline;
    }

    code {
    font-family: ui-monospace, SFMono-Regular, Menlo, Consolas, monospace;
    font-size: 0.95em;
    background: #f2f2f2;
    padding: 0.15em 0.35em;
    border-radius: 4px;
    }

    pre {
    font-family: ui-monospace, SFMono-Regular, Menlo, Consolas, monospace;
    font-size: 0.9em;
    background: #f5f5f5;
    padding: 1em 1.2em;
    overflow-x: auto;
    border-radius: 6px;
    line-height: 1.45;
    }

    pre code {
    background: none;
    padding: 0;
    }

    blockquote {
    margin: 1.5em 0;
    padding-left: 1em;
    border-left: 4px solid #ddd;
    color: #555;
    }

    hr {
    border: none;
    border-top: 1px solid #e0e0e0;
    margin: 3em 0;
    }

    table {
    border-collapse: collapse;
    margin: 1.5em 0;
    width: 100%;
    font-size: 0.95em;
    }

    th, td {
    padding: 0.5em 0.7em;
    border-bottom: 1px solid #e5e5e5;
    text-align: left;
    }

    th {
    font-weight: 600;
    }

    img {
    max-width: 100%;
    display: block;
    margin: 1.5em auto;
    }

    small {
    color: #666;
    }

    mjx-container {
    margin: 1em 0;
    }

    ::selection {
    background: #cce2ff;
    }

    </style>
    

<script>
window.MathJax = {
  tex: {
    inlineMath: [['$', '$'], ['\\(', '\\)']],
    displayMath: [['$$', '$$'], ['\\[', '\\]']],
    tags: 'ams',
    processEscapes: true,
    packages: {'[+]': ['base', 'ams', 'amsmath', 'amssymb', 'amsbsy']}
  },
  options: {
    ignoreHtmlClass: 'tex2jax_ignore',
    processHtmlClass: 'tex2jax_process'
  }
};
</script>
<script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

<h1><a href="https://kexue.fm/archives/7210">Designing GANs: Another GAN Production Workshop</a></h1>

<p>By 苏剑林 | Feb 13, 2020</p>

<p>In the 2018 article <a href="translation_6016.html">"An Introduction to f-GAN: A Production Workshop for GAN Models,"</a> I introduced f-GAN and described it as a "production workshop" for GAN models. As the name suggests, this refers to its ability to construct many different forms of GAN models following a fixed procedure. A few days ago, I saw a new paper on arXiv titled <a href="https://papers.cool/arxiv/2002.00865">"Designing GANs: A Likelihood Ratio Approach"</a> (hereafter referred to as <i>Designing GANs</i> or the original paper). I found it to be doing the same thing as f-GAN, but taking a completely different path (though ultimately arriving at the same destination). The entire paper is quite interesting, so I've decided to share it here.</p>

<h2>f-GAN Recap <a href="#f-GAN%E5%9B%9E%E9%A1%BE">#</a></h2>

<p>From <a href="translation_6016.html">"An Introduction to f-GAN: A Production Workshop for GAN Models,"</a> we know that the first step of f-GAN is to find a function $f$ that satisfies the following conditions:</p>

<blockquote>
1. $f$ is a mapping from non-negative real numbers to real numbers ($R^* \to R$);<br>
2. $f(1) = 0$;<br>
3. $f$ is a convex function.
</blockquote>

<p>Once such a function is found, a probability f-divergence can be constructed. Then, a technique called "convex conjugation" is used to transform the f-divergence into another form (the form with a $\max$, generally called the dual form). By minimizing this divergence, a min-max process is obtained, which gives birth to a GAN model. By the way, f-GAN represents a series of GAN models, but it does not include WGAN. However, the derivation of WGAN actually follows similar steps, except it uses the Wasserstein distance as the probability metric, and the method of transforming Wasserstein distance into its dual form is different. For details, refer to <a href="translation_6280.html">"From Wasserstein Distance and Duality Theory to WGAN."</a></p>

<p>f-GAN is logically sound, but according to the steps it provides, we always need to find an f-divergence first and then transform it into the dual form. The question is: since we only need its dual form, why not analyze it directly in the dual space? I previously raised this question in the article <a href="translation_6163.html">"How about a GAN that doesn't use L-constraints and won't suffer from vanishing gradients?"</a>. At that time, I only discussed the proof of probability divergence in the dual space and did not give a method for constructing probability divergence. <i>Designing GANs</i> precisely supplements this point.</p>

<h2>Designing GANs <a href="#Designing%20GANs">#</a></h2>

<p>In this section, we will explore the ideas and methods in <i>Designing GANs</i>. Unlike the somewhat redundant textbook-style derivation in the original paper, this article will derive the results of <i>Designing GANs</i> through a step-by-step reverse induction process, which I believe is easier to understand. Interestingly, understanding the entire derivation process actually requires only very basic calculus knowledge.</p>

<h3>Total Variation <a href="#Total%20Variation">#</a></h3>

<p>Let's take a probability divergence called Total Variation as an example to get a preliminary sense of the key points of analyzing and deriving probability divergence in the dual space.</p>

<p>First, we have</p>

\begin{equation}|p-q|=\max_{t\in[-1, 1]} (p - q)t=\max_{t\in[-1, 1]} pt - qt\label{eq:tv-base}\end{equation}

<p>So for probability distributions $p(x), q(x)$, we also have</p>

\begin{equation}|p(x)-q(x)|=\max_{t(x)\in[-1, 1]} p(x)t(x) - q(x)t(x)\end{equation}

<p>Integrating both sides (let's not worry about the interchangeability of integration and $\max$ for now):</p>

\begin{equation}\begin{aligned}\int|p(x)-q(x)|dx=&\, \max_{t(x)\in[-1, 1]} \int \big[p(x)t(x) - q(x)t(x)\big]dx\\
=&\, \max_{t(x)\in[-1, 1]} \mathbb{E}_{x\sim p(x)}[t(x)] - \mathbb{E}_{x\sim q(x)}[t(x)]
\end{aligned}\end{equation}

<p>The term $\int|p(x)-q(x)|dx$ is called the <a href="https://en.wikipedia.org/wiki/Total_variation">Total Variation</a> between two probability distributions. Thus, we have derived the dual form of Total Variation through this process. If we fix $p(x)$ and let $q(x)$ approach $p(x)$, we can minimize the Total Variation, i.e.,</p>

\begin{equation}\min_{q(x)}\int|p(x)-q(x)|dx = \min_{q(x)}\max_{t(x)\in[-1, 1]} \mathbb{E}_{x\sim p(x)}[t(x)] - \mathbb{E}_{x\sim q(x)}[t(x)]\label{eq:tv-gan}\end{equation}

<p>This yields a GAN form.</p>

<p>Reviewing the entire process, putting aside the a priori knowledge of Total Variation, we can find that the core of the entire process is actually Equation \eqref{eq:tv-base}. With Equation \eqref{eq:tv-base}, everything that follows is natural. So what are the characteristics of Equation \eqref{eq:tv-base}? In fact, it can be generalized as:</p>

<blockquote>
<strong>Goal 1</strong>: Find functions $\phi(t), \psi(t)$ and a certain range $\Omega$ such that
\begin{equation}d(p, q) = \max_{t\in\Omega} p\phi(t)+q\psi(t)\end{equation}
and $d(p,q)\geq 0$, and $d(p,q)=0 \Leftrightarrow p=q$.
</blockquote>

<p>With such $\phi(t), \psi(t)$, we can derive a GAN similar to \eqref{eq:tv-gan}:</p>

\begin{equation}\min_{q(x)}\int d(p(x), q(x)) dx = \min_{q(x)}\max_{t(x)\in \Omega} \mathbb{E}_{x\sim p(x)}[\phi(t(x))] + \mathbb{E}_{x\sim q(x)}[\psi(t(x))]\label{eq:gan}\end{equation}

<h3>Finding the Maximum via Differentiation <a href="#%E6%B1%82%E5%AF%BC%E6%89%BE%E6%9E%81%E5%A4%A7%E5%80%BC">#</a></h3>

<p>Note that in "<strong>Goal 1</strong>", $p, q$ are just non-negative real numbers, and $\phi(t), \psi(t)$ are just scalar functions, so the entire goal is purely a single-variable function extremum problem, which should be said to be quite simplified. Furthermore, let $r=q/p \in [0, +\infty)$. It can be converted into a simpler "<strong>Goal 2</strong>":</p>

<blockquote>
<strong>Goal 2</strong>: Find functions $\phi(t), \psi(t)$ and a certain range $\Omega$ such that
\begin{equation}d(r) = \max_{t\in\Omega} \phi(t)+r\psi(t)\end{equation}
and the minimum value of $d(r)$ is attained at $r=1$.
</blockquote>

<p>Now let's examine <strong>Goal 2</strong>. For simplicity, we assume that $\phi(t), \psi(t)$ are smooth functions. Thus, the maximum of $\phi(t)+r\psi(t)$ can be solved by differentiation. In fact, such a design is already representative enough; when some points are not smooth, we can approximate them with smooth functions and then take the limit, such as $\text{sign}(x)=\lim_{k\to+\infty}\tanh(kx)$.</p>

<p>Based on this assumption, to find the maximum of $\phi(t)+r\psi(t)$, we first need to differentiate and set it to 0:</p>

\begin{equation}\phi'(t)+r\psi'(t)=0 \quad\Rightarrow\quad r = -\frac{\phi'(t)}{\psi'(t)} \triangleq \omega^{-1}(t)\label{eq:max0}\end{equation}

<p>Here we assume the above equation has a unique solution, denoted as $t=\omega(r)$, so finally $-\frac{\phi'(t)}{\psi'(t)}$ is equivalent to $\omega^{-1}(t)$, which is the inverse function of $\omega(r)$ (again, a reminder, this is the inverse function, not the reciprocal). At the same time, since $r\in[0, +\infty)$, $t\in \Omega = \omega([0,\infty))$, which means the range of $t$, $\Omega$, is the codomain of $\omega(r)$. In addition, since it can be inverted, it means $\omega(r)$ must be either strictly monotonically increasing or strictly monotonically decreasing. Without loss of generality, we assume $\omega(r)$ is strictly monotonically increasing, so $\omega^{-1}(t)$ is also strictly monotonically increasing.</p>

<p>A derivative of 0 only indicates that $t$ is an extremum point, but it doesn't guarantee it's a maximum point. Let's determine the conditions for it to be a maximum value. We have:</p>

\begin{equation}\phi'(t)+r\psi'(t) = \big(r-\omega^{-1}(t)\big)\psi'(t)\end{equation}

<p>Note that $r-\omega^{-1}(t)$ is strictly monotonically decreasing, so it can only have one zero point, and it starts positive and becomes negative. To make the entire derivative function have this property, we set $\psi'(t) \triangleq \rho(t) > 0$, meaning $\rho(t)$ is always positive. In this way, the derivative of $\phi(t)+r\psi(t)$ is continuous and goes from positive to negative, so $\phi(t)+r\psi(t)$ has only one extremum point, which is the maximum point.</p>

<h3>Finding the Minimum via Differentiation <a href="#%E6%B1%82%E5%AF%BC%E6%89%BE%E6%9E%81%E5%B0%8F%E5%80%BC">#</a></h3>

<p>Let's summarize the results so far: we assume $\omega(r)$ is strictly monotonically increasing, and we assume $\rho(t)$ is always positive for $t\in \Omega$, and the following relationships are satisfied:</p>

\begin{equation}\left\{\begin{aligned}\phi'(t)=&\, -\omega^{-1}(t)\rho(t)\\
\psi'(t)=&\, \rho(t)\end{aligned}
\right.\end{equation}

<p>In this case, $\phi(t)+r\psi(t)$ has a unique maximum point $t=\omega(r)$, and it is also the maximum value point. For <strong>Goal 2</strong>, this has completed half of the content. Now let's examine whether when $t=\omega(r)$, $d(r)=\phi(\omega(r))+r\psi(\omega(r))$ satisfies the remaining properties as we desired (i.e., the minimum value of $d(r)$ is attained at $r=1$).</p>

<p>Continuing the differentiation:</p>

\begin{equation}d'(r)=\big[\phi'(\omega(r))+r\psi'(\omega(r))\big]\omega'(r)+\psi(\omega(r))=\psi(\omega(r))\label{eq:d-r}\end{equation}

<p>where the second equality holds because the part in square brackets is 0 according to Equation \eqref{eq:max0}. Now only the term $\psi(\omega(r))$ remains, and noting that we assumed $\psi'(t)=\rho(t) > 0$, $\psi(t)$ is strictly monotonically increasing. At the same time, we also assumed $\omega(r)$ is strictly monotonically increasing, so the composite function $\psi(\omega(r))$ is also strictly monotonically increasing.</p>

<p>Now we add another assumption: $\psi(\omega(1))=0$, which means $d'(1)=0$, so $r=1$ is an extremum point of $d(r)$. Since $\psi(\omega(r))$ is continuous and strictly monotonically increasing, $d'(r)$ goes from negative to positive. Therefore, $r=1$ is the local minimum point of $d(r)$, and also the absolute minimum point.</p>

<h3>The GAN Model has Arrived <a href="#GAN%E6%A8%A1%E5%9E%8B%E5%B7%B2%E7%BB%8F%E9%80%81%E8%BE%BE">#</a></h3>

<p>At this point, our derivation is complete. The conditions we obtained are:</p>

<blockquote>
<strong>Conclusion 1</strong>: If $\omega(r)$ is strictly monotonically increasing, $\Omega = \omega([0, +\infty))$, $\rho(t)$ is always positive for $t\in \Omega$, and the following relationships are satisfied:
\begin{equation}\left\{\begin{aligned}\phi'(t)=&\, -\omega^{-1}(t)\rho(t)\\
\psi'(t)=&\, \rho(t)\end{aligned}
\right.\end{equation}
along with the condition $\psi(\omega(1))=0$, the functions $\phi(t), \psi(t)$ satisfying these conditions can be used to construct a GAN model as shown in \eqref{eq:gan}.
</blockquote>

<p>For example, let's verify the original version of GAN:</p>

\begin{equation}\begin{aligned}&\, \min_{q(x)}\max_{t(x)} \mathbb{E}_{x\sim p(x)}[\log (1-\sigma(t(x)))] + \mathbb{E}_{x\sim q(x)}[\log \sigma(t(x))]\\
=&\, \min_{q(x)}\max_{t(x)} \mathbb{E}_{x\sim p(x)}\left[-\log \left(1+e^{t(x)}\right)\right] + \mathbb{E}_{x\sim q(x)}\left[-\log \left(1+e^{-t(x)}\right)\right]
\end{aligned}\label{eq:ori-gan}\end{equation}

<p>where $\sigma(\cdot)$ is the sigmoid activation function. The above is a binary cross-entropy where true samples are labeled 0 and fake samples are labeled 1. The right side of the equality is the simplified result. From this perspective, $\phi(t)=-\log \left(1+e^t\right)$, $\psi(t)=-\log \left(1+e^{-t}\right)$. We make a small adjustment and let $\psi(t)=\log 2-\log \left(1+e^{-t}\right)$; obviously, this does not affect the original optimization problem. Now we have $\rho(t)=\psi'(t)=\frac{1}{1+e^t}$, which is clearly always greater than 0, and $\omega^{-1}(t)=-\phi'(t)/\psi'(t)=e^t$, which means $t=\omega(r)=\log r$. This is also clearly strictly monotonically increasing. Finally, verifying $\psi(\omega(1))$, we find it indeed equals 0.</p>

<p>From this example, we can derive two corollaries:</p>

<blockquote>
1. The condition $\psi(\omega(1))=0$ is not absolutely necessary, because even if $\psi(\omega(1))=0$ is not satisfied at first, we can add a constant to $\psi(t)$ so that it satisfies $\psi(\omega(1))=0$, and adding a constant will not change the original optimization problem;<br><br>
2. If we swap the labels, letting real samples be labeled 1 and fake samples 0, then the properties obtained are exactly opposite, meaning the calculated $\rho(t)$ is always negative and $\omega(r)$ is strictly monotonically decreasing. This indicates that <strong>Conclusion 1</strong> given in this article is a sufficient condition rather than a necessary condition for forming a GAN.
</blockquote>

<p>Different forms of $\rho(t), \omega(r)$ can lead to the same GAN model. For example, if we choose $t=\omega(r)=\frac{r}{r+1}$, then $r=\omega^{-1}(t)=\frac{t}{1-t}$, and if we choose $\rho(t)=\frac{1}{t}$, then:</p>

\begin{equation}\left\{\begin{aligned}\phi'(t)=&\, -\frac{t}{1-t}\times\frac{1}{t}\\
\psi'(t)=&\, \frac{1}{t}\end{aligned}
\right.\end{equation}

<p>Integrating gives $\phi(t)=\log(1-t)$ and $\psi(t) = \log t$. Furthermore, note that $\Omega = \omega([0, +\infty)) = [0, 1)$, and $\rho(t)=\frac{1}{t}$ excludes $t=0$, so the feasible domain is $(0,1)$ (in fact, for experiments, the boundary points can be ignored). That is, the derived GAN is:</p>

\begin{equation}\min_{q(x)}\max_{t(x)\in (0,1)} \mathbb{E}_{x\sim p(x)}[\log (1-t(x))] + \mathbb{E}_{x\sim q(x)}[\log t(x)]\end{equation}

<p>This is equivalent to the original GAN, just without explicitly writing the activation function that makes $t(x)\in (0,1)$.</p>

<p>Let's calculate another example. Choose $t=\omega(r)=\frac{1}{2}\log r$, i.e., $r=\omega^{-1}(t)=e^{2t}$, and choose $\rho(t)=e^{-t}$. We can calculate $\phi(t)=-e^t$ and $\psi(t)=-e^{-t}$. Thus, we get a GAN variant:</p>

\begin{equation}\min_{q(x)}\max_{t(x)} \mathbb{E}_{x\sim p(x)}\left[-e^{t(x)}\right] + \mathbb{E}_{x\sim q(x)}\left[-e^{-t(x)}\right]\end{equation}

<p>The original paper also used the above <strong>conclusion</strong> to calculate many weird GANs. Interested readers can read the original paper on their own; I won't repeat the derivations here.</p>

<h2>Reflections and Extensions <a href="#%E6%B1%82%E5%AF%BC%E6%89%BE%E6%9E%81%E5%B0%8F%E5%80%BC">#</a></h2>

<p>Is there any connection between the method in this article and the previous f-GAN? Are there any extensions to this method? Here are my own answers.</p>

<h3>Connection with f-GAN <a href="#%E4%B8%8Ef-GAN%E7%9A%84%E8%81%94%E7%B3%BB">#</a></h3>

<p>In the above derivation, in the $\max$ step, we get $d(r)$ where $r=1$ is the minimum point of $d(r)$. Reviewing <strong>Goal 1</strong> and substituting $d(r)$ back into Equation \eqref{eq:gan}, we will find that the optimization goal is actually:</p>

\begin{equation}\int p(x) d\left(\frac{q(x)}{p(x)}\right)dx\label{eq:f-gan}\end{equation}

<p>Does it look familiar? Yes, it looks like the definition of f-divergence. And reviewing the derivation at \eqref{eq:d-r}, we know the derivative of $d(r)$ is strictly monotonically increasing, which indicates that $d(r)$ is a convex function. So the above equation is indeed an f-divergence! In other words, although the authors of this paper took a seemingly completely different path, their results can actually be derived from f-GAN results; they didn't bring anything new.</p>

<p>So is this paper completely equivalent to f-GAN? Unfortunately, it's not yet. The results of the original paper are only a subset of f-GAN. In other words, f-GAN can derive all the GAN model variants it can derive, but it may not be able to derive all the GAN model variants that f-GAN can.</p>

<p>Because reviewing the entire derivation process, the core idea is to directly generalize the measurement formula of "points" to "functions," such as generalizing $|p-q|=0 \Leftrightarrow p=q$ at the beginning to $\int |p(x)-q(x)|dx=0 \Leftrightarrow p(x)=q(x)$. Because of this idea, all derivation processes can be carried out only under single-variable calculus. But the problem is, not all conclusions of $\int d(p(x),q(x))dx=0 \Leftrightarrow p(x)=q(x)$ mean that $d(p,q)=0 \Leftrightarrow p=q$. For example, the KL divergence is $\int p(x)\log \frac{p(x)}{q(x)}dx = 0$ which means $p(x)=q(x)$, but $p\log\frac{p}{q}=0$ does not necessarily mean $p=q$. Therefore, the original paper at least cannot derive the GAN corresponding to KL divergence.</p>

<p>From this point of view, is the original paper worthless? If we only look at the "product," it is indeed worthless because everything it can produce, f-GAN can produce. But we shouldn't only care about the "product"; sometimes we should also care about the "production process." In fact, <strong>I believe the academic value of the original paper lies in providing a reference method for analyzing and discovering GANs directly in the dual space, adding another perspective for our understanding of GAN models.</strong></p>

<h3>Actually, it can be Generalized <a href="#%E5%85%B6%E5%AE%9E%E8%BF%98%E5%8F%AF%E4%BB%A5%E6%8E%A8%E5%B9%BF">#</a></h3>

<p>Whether it is f-GAN or the original paper, the losses of the generator and discriminator of the derived GAN models are in the same form, just in different directions. But in fact, in the GAN variants we currently use more, the losses of the generator and discriminator are not the same. For example, a more commonly used form of the original GAN is:</p>

\begin{equation}\begin{aligned}&\, \max_{t(x)} \mathbb{E}_{x\sim p(x)}[\log (1-\sigma(t(x)))] + \mathbb{E}_{x\sim q(x)}[\log \sigma(t(x))]\\
&\, \min_{q(x)} \mathbb{E}_{x\sim q(x)}[-\log (1-\sigma(t(x)))]
\end{aligned}\label{eq:ori-gan-2}\end{equation}

<p>Similar examples include LSGAN, Hinge GAN, and so on. So if we only consider variants where the loss of the generator and discriminator are in the same form, it is still not sufficient.</p>

<p>In fact, this paper could have gone one step further and obtained more results than f-GAN. Unfortunately, the authors seem to have run their thinking into a dead end and didn't notice this. In fact, achieving this is very simple: in the above process, through the step of $\max_{t\in\Omega} \phi(t)+r\psi(t)$, we solved for $t = \omega(r)$, and then substituted it back into the original $\phi(t)+r\psi(t)$ to get $d(r)$. But in fact, we don't have to substitute it back into the original expression; we can substitute it into any expression of the form $\alpha(t)+r\beta(t)$. According to Equation \eqref{eq:f-gan} and combined with the requirements for f-divergence listed at the beginning, as long as $d(r)=\alpha(\omega(r))+r\beta(\omega(r))$ is a convex function (the fact that $d(1)=0$ can be achieved through translation), or according to the previous reasoning, $d(r)$ is any function with the minimum at $r=1$. Taken together, this is:</p>

<blockquote>
<strong>Conclusion 2</strong>: If $\omega(r)$ is strictly monotonically increasing, $\Omega = \omega([0, +\infty))$, $\rho(t)$ is always positive for $t\in \Omega$, and the following relationships are satisfied:
\begin{equation}\left\{\begin{aligned}\phi'(t)=&\, -\omega^{-1}(t)\rho(t)\\
\psi'(t)=&\, \rho(t)\end{aligned}
\right.\end{equation}
and the functions $\alpha(t), \beta(t)$ such that $d(r) = \alpha(\omega(r)) + r\beta(\omega(r))$ is a convex function, or such that $d(r)$ is a function with its minimum value at $r=1$. The functions $\phi(t), \psi(t), \alpha(t), \beta(t)$ satisfying these conditions can be used to construct the following GAN model (where $\min_{q(x)}$ has omitted the $\alpha(t)$ part which is unrelated to $q(x)$):
\begin{equation}\begin{aligned}\max_{t(x)\in \Omega} &\, \mathbb{E}_{x\sim p(x)}[\phi(t(x))] + \mathbb{E}_{x\sim q(x)}[\psi(t(x))]\\
\min_{q(x)}&\, \mathbb{E}_{x\sim q(x)}[\beta(t(x))]
\end{aligned}\end{equation}
</blockquote>

<h3>Some Examples After Generalization <a href="#%E6%8E%A8%E5%B9%BF%E5%90%8E%E7%9A%84%E4%B8%80%E4%BA%9B%E4%BE%8B%E5%AD%90">#</a></h3>

<p>Using <strong>Conclusion 2</strong> to construct a GAN is quite free. it can construct many examples that f-GAN cannot find because it allows the generator and discriminator losses to be inconsistent.</p>

<p>For example, when calculating the original GAN, we get $t = \omega(r) = \log r$, and $r \log r$ is a convex function, so we can let $\alpha(t) = 0, \beta(t) = t$ (note that $\alpha(t)$ can be 0, while $\beta(t)$ cannot, why?), getting $d(r) = r \log r$. The GAN at this time is:</p>

\begin{equation}\begin{aligned}\max_{t(x)} &\, \mathbb{E}_{x\sim p(x)}[\log (1-\sigma(t(x)))] + \mathbb{E}_{x\sim q(x)}[\log \sigma(t(x))]\\
\min_{q(x)}&\, \mathbb{E}_{x\sim q(x)}[t(x)]
\end{aligned}\end{equation}

<p>This is a quite useful GAN variant. Also, since $r \log r$ is a convex function, $(1+r) \log(1+r)$ is also convex. Then we can let $\alpha(t) = \beta(t) = \log(1+r) = \log(1+e^t)$, which exactly corresponds to $d(r) = (r+1) \log(1+r)$. And $\log(1+e^t) = -\log(1-\sigma(t))$, so the corresponding GAN at this time is \eqref{eq:ori-gan-2}, which is the more commonly used original version of GAN, better than \eqref{eq:ori-gan}.</p>

<p>Let's give another example. Let $t = \omega(r) = \frac{a + b r}{1 + r}$, assuming $b > a$. Then $\Omega = (a, b)$ (still ignore the boundary points for now), and $r = \omega^{-1}(t) = \frac{t-a}{b-t}$. We take $\rho(t) = 2(b-t)$, so it satisfies the requirement of being always positive. This yields $\phi(t) = -(t-a)^2, \psi(t) = -(t-b)^2$. Then we take $d(r) = \frac{(r-1)^2}{r+1}$, which clearly attains its minimum at $r=1$. Then let $\alpha(t) = \beta(t)$, trying to reverse-engineer the form of $\beta(t)$, i.e., $d(r) = (1+r)\beta(t)$, which yields $\beta(t) = \left(\frac{r-1}{r+1}\right)^2$. Substituting $r = \frac{t-a}{b-t}$ gives $\beta(t) = \left(\frac{2}{b-a}t - \frac{a+b}{b-a}\right)^2$. For simplicity, we can let $b-a = 2$, so $\beta(t) = \left(t - \frac{a+b}{2}\right)^2$. The final GAN form is:</p>

\begin{equation}\begin{aligned}\max_{t(x)\in (a,b)} &\, \mathbb{E}_{x\sim p(x)}\left[-(t-a)^2\right] + \mathbb{E}_{x\sim q(x)}\left[-(t-b)^2\right]\\
\min_{q(x)}&\, \mathbb{E}_{x\sim q(x)}\left[\left(t-\frac{a+b}{2}\right)^2\right]
\end{aligned}\end{equation}

<p>This is actually LSGAN. Readers might be confused: LSGAN does not have the restriction $t(x) \in (a, b)$, so why is it here? In fact, this restriction can be removed because the optimal solution still falls within $(a, b)$ after removing this restriction.</p>

<p>Obviously, these GAN variants obtained based on the generalized <strong>Conclusion 2</strong> are very valuable, and these GAN variants cannot be obtained by f-GAN. Therefore, if the original paper could supplement this part of generalization, it would look very beautiful.</p>

<h2>One More Summary <a href="#%E5%86%8D%E6%9D%A5%E4%B8%AA%E5%B0%8F%E7%BB%93">#</a></h2>

<p>This article shared a paper that designs GAN models directly in the dual space, analyzed its connection with f-GAN, and then I generalized the results of the original paper to allow it to design more diverse GAN models.</p>

<p>Finally, readers might wonder: f-GAN already made so many GANs, and now this paper makes so many more GANs, but in practice, we only use a few anyway. What's the value of making so many? This question is a matter of opinion; this type of work should be more about methodological value, and its practical application value might not be that great. However, I would rather say:</p>

<blockquote>
I didn't say it has any value either; I just found it quite interesting~
</blockquote>
<hr>
<footer style="margin-top: 3em; padding: 1.5em; background: #f5f5f5; border-radius: 8px; font-size: 0.9em; color: #555;">
    <p style="margin: 0 0 0.5em 0;"><strong>Citation</strong></p>
    <p style="margin: 0 0 0.5em 0;">
        This is a machine translation of the original Chinese article:<br>
        <a href="translation_7210.html" style="color: #005fcc;">https://kexue.fm/archives/7210</a>
    </p>
    <p style="margin: 0 0 0.5em 0;">
        Original author: 苏剑林 (Su Jianlin)<br>
        Original publication: <a href="https://kexue.fm" style="color: #005fcc;">科学空间 (Scientific Spaces)</a>
    </p>
    <p style="margin: 0; font-style: italic;">
        Translated using Gemini 3 Flash. Please refer to the original for authoritative content.
    </p>
</footer>
