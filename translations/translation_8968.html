
    <style type="text/css">

    body {
    margin: 48px auto;
    max-width: 68ch;              /* character-based width reads better */
    padding: 0 16px;

    font-family: -apple-system, BlinkMacSystemFont, "Segoe UI",
                Roboto, "Helvetica Neue", Arial, sans-serif;
    font-size: 18px;
    line-height: 1.65;
    color: #333;
    background: #fafafa;
    }

    h1, h2, h3, h4 {
    line-height: 1.25;
    margin-top: 2.2em;
    margin-bottom: 0.6em;
    font-weight: 600;
    }

    h1 {
    font-size: 2.1em;
    margin-top: 0;
    }

    h2 {
    font-size: 1.6em;
    border-bottom: 1px solid #e5e5e5;
    padding-bottom: 0.3em;
    }

    h3 {
    font-size: 1.25em;
    }

    h4 {
    font-size: 1.05em;
    color: #555;
    }

    /* Paragraphs and lists */
    p {
    margin: 1em 0;
    }

    ul, ol {
    margin: 1em 0 1em 1.5em;
    }

    li {
    margin: 0.4em 0;
    }

    a {
    color: #005fcc;
    text-decoration: none;
    }

    a:hover {
    text-decoration: underline;
    }

    code {
    font-family: ui-monospace, SFMono-Regular, Menlo, Consolas, monospace;
    font-size: 0.95em;
    background: #f2f2f2;
    padding: 0.15em 0.35em;
    border-radius: 4px;
    }

    pre {
    font-family: ui-monospace, SFMono-Regular, Menlo, Consolas, monospace;
    font-size: 0.9em;
    background: #f5f5f5;
    padding: 1em 1.2em;
    overflow-x: auto;
    border-radius: 6px;
    line-height: 1.45;
    }

    pre code {
    background: none;
    padding: 0;
    }

    blockquote {
    margin: 1.5em 0;
    padding-left: 1em;
    border-left: 4px solid #ddd;
    color: #555;
    }

    hr {
    border: none;
    border-top: 1px solid #e0e0e0;
    margin: 3em 0;
    }

    table {
    border-collapse: collapse;
    margin: 1.5em 0;
    width: 100%;
    font-size: 0.95em;
    }

    th, td {
    padding: 0.5em 0.7em;
    border-bottom: 1px solid #e5e5e5;
    text-align: left;
    }

    th {
    font-weight: 600;
    }

    img {
    max-width: 100%;
    display: block;
    margin: 1.5em auto;
    }

    small {
    color: #666;
    }

    mjx-container {
    margin: 1em 0;
    }

    ::selection {
    background: #cce2ff;
    }

    </style>
    

<script>
window.MathJax = {
  tex: {
    inlineMath: [['$', '$'], ['\\(', '\\)']],
    displayMath: [['$$', '$$'], ['\\[', '\\]']],
    processEscapes: true,
    tags: 'ams',
    packages: {'[+]': ['base', 'ams', 'noerrors', 'noundefined']}
  },
  loader: {load: ['[tex]/ams']}
};
</script>
<script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"></script>

<article>
    <h1><a href="https://kexue.fm/archives/8968">Exponentiated Gradient Descent + Meta-Learning = Adaptive Learning Rate</a></h1>
    <p>By 苏剑林 | March 03, 2022</p>

    <p>A couple of days ago, I came across a paper from Google titled <a href="https://papers.cool/arxiv/2202.00145">"Step-size Adaptation Using Exponentiated Gradient Updates"</a>, where I learned some new concepts, so I am documenting and sharing them here. There are two main pieces of content: one is Exponentiated Gradient Descent for non-negative optimization, and the other is a learning rate adjustment algorithm based on the idea of meta-learning. Both are quite interesting, and readers who are interested can also take a look.</p>

    <h2>Exponentiated Gradient Descent</h2>
    <p>You have likely heard much about gradient descent, which refers to the minimization of an unconstrained function $\mathcal{L}(\boldsymbol{\theta})$ using the following update format:</p>
    \begin{equation}\boldsymbol{\theta}_{t+1} = \boldsymbol{\theta}_t - \eta\nabla_{\boldsymbol{\theta}}\mathcal{L}(\boldsymbol{\theta}_t)\end{equation}
    <p>where $\eta$ is the learning rate. However, many tasks are not always unconstrained. For the simplest non-negative constraints, we can instead use the following update format:</p>
    \begin{equation}\boldsymbol{\theta}_{t+1} = \boldsymbol{\theta}_t \odot \exp\left(- \eta\nabla_{\boldsymbol{\theta}}\mathcal{L}(\boldsymbol{\theta}_t)\right)\label{eq:egd}\end{equation}
    <p>Here $\odot$ denotes element-wise multiplication (Hadamard product). It is easy to see that as long as the initialization $\boldsymbol{\theta}_0$ is non-negative, $\boldsymbol{\theta}_t$ will remain non-negative throughout the entire update process. This is the "Exponentiated Gradient Descent" used for non-negative constrained optimization.</p>

    <p>How do we understand this "Exponentiated Gradient Descent"? It's not difficult; it can be derived by converting it into an unconstrained case. If $\boldsymbol{\theta}$ is non-negative, then $\boldsymbol{\varphi}=\log\boldsymbol{\theta}$ can be positive or negative. Therefore, we can set $\boldsymbol{\theta}=e^{\boldsymbol{\varphi}}$ to transform it into an unconstrained optimization problem with respect to $\boldsymbol{\varphi}$, which can then be solved using gradient descent:</p>
    \begin{equation}\boldsymbol{\varphi}_{t+1} = \boldsymbol{\varphi}_t - \eta\nabla_{\boldsymbol{\varphi}}\mathcal{L}(e^{\boldsymbol{\varphi}_t}) = \boldsymbol{\varphi}_t - \eta e^{\boldsymbol{\varphi}_t}\odot\nabla_{e^{\boldsymbol{\varphi}}}\mathcal{L}(e^{\boldsymbol{\varphi}_t})\end{equation}
    <p>We consider that the $e^{\boldsymbol{\varphi}_t}\odot$ part of the gradient only serves to adjust the learning rate, so it is not fundamentally important. We omit it to obtain:</p>
    \begin{equation}\boldsymbol{\varphi}_{t+1} = \boldsymbol{\varphi}_t - \eta \nabla_{e^{\boldsymbol{\varphi}}}\mathcal{L}(e^{\boldsymbol{\varphi}_t})\end{equation}
    <p>Taking the exponential of both sides gives:</p>
    \begin{equation}e^{\boldsymbol{\varphi}_{t+1}} = e^{\boldsymbol{\varphi}_t}\odot\exp\left( - \eta \nabla_{e^{\boldsymbol{\varphi}}}\mathcal{L}(e^{\boldsymbol{\varphi}_t})\right)\end{equation}
    <p>Substituting back $\boldsymbol{\theta}=e^{\boldsymbol{\varphi}}$ yields Equation $\eqref{eq:egd}$.</p>

    <h2>Meta-Learning for Learning Rate Adjustment</h2>
    <p>Regarding Meta-Learning, many readers might be like me—having heard the term often but having had almost no contact with it. Simply put, the relationship between ordinary machine learning and meta-learning is like the relationship between a "function" and a "functional" in mathematics. A functional is a "function of functions," and meta-learning is "Learning How to Learn." In other words, it is a methodology about "learning" itself, such as what will be introduced next: "using gradient descent to adjust gradient descent."</p>

    <p>We start from general gradient descent. Denoting the gradient of the objective function $\mathcal{L}$ as $\boldsymbol{g}$, the update formula is:</p>
    \begin{equation}\boldsymbol{\theta}_{t+1} = \boldsymbol{\theta}_t - \eta\boldsymbol{g}_t\end{equation}
    <p>We want to adjust the learning rate for each component, so we introduce a non-negative variable $\boldsymbol{\nu}$ of the same size as the parameters and modify the update formula to:</p>
    \begin{equation}\boldsymbol{\theta}_{t+1} = \boldsymbol{\theta}_t - \eta\boldsymbol{\nu}_{t+1}\odot\boldsymbol{g}_t\label{eq:update}\end{equation}
    <p>So, what rule should $\boldsymbol{\nu}$ follow for iteration? Remember that our ultimate goal is to minimize $\mathcal{L}$, so the update rule for $\boldsymbol{\nu}$ should also be gradient descent. Since $\boldsymbol{\nu}$ is required to be non-negative, we use exponentiated gradient descent:</p>
    \begin{equation}\boldsymbol{\nu}_{t+1} = \boldsymbol{\nu}_t \odot\exp\left(- \gamma\nabla_{\boldsymbol{\nu}_t}\mathcal{L}\right)\label{eq:update-nu}\end{equation}
    <p>Note that $\mathcal{L}$ is originally only a function of $\boldsymbol{\theta}$, but according to $\eqref{eq:update}$, at time $t$ we have $\boldsymbol{\theta}_t = \boldsymbol{\theta}_{t-1} - \eta\boldsymbol{\nu}_t\odot\boldsymbol{g}_{t-1}$. Therefore, according to the chain rule, we have:</p>
    \begin{equation}\nabla_{\boldsymbol{\nu}_t}\mathcal{L} = -\eta\boldsymbol{g}_{t-1} \odot\nabla_{\boldsymbol{\theta}_t}\mathcal{L}= -\eta\boldsymbol{g}_{t-1} \odot\boldsymbol{g}_t\end{equation}
    <p>Substituting this into the update formula for $\boldsymbol{\nu}$ in Equation $\eqref{eq:update-nu}$, we get:</p>
    \begin{equation}\boldsymbol{\nu}_{t+1} = \boldsymbol{\nu}_t \odot\exp\left( \gamma\eta\boldsymbol{g}_{t-1} \odot\boldsymbol{g}_t\right)\end{equation}
    <p>Combining $\gamma\eta$ into a single parameter $\gamma$, the update formulas for the entire model are:</p>
    \begin{equation}\begin{aligned}&\boldsymbol{\nu}_{t+1} = \boldsymbol{\nu}_t \odot\exp\left( \gamma\boldsymbol{g}_{t-1} \odot\boldsymbol{g}_t\right) \\
    &\boldsymbol{\theta}_{t+1} = \boldsymbol{\theta}_t - \eta\boldsymbol{\nu}_{t+1}\odot\boldsymbol{g}_t\end{aligned}\end{equation}
    <p>If $\boldsymbol{\nu}$ is initialized to all ones, then we will have:</p>
    \begin{equation}\boldsymbol{\nu}_{t+1} = \exp\left(\gamma\sum_{k=1}^t\boldsymbol{g}_{k-1} \odot\boldsymbol{g}_k\right)\end{equation}
    <p>As can be seen, the idea behind this learning rate adjustment method is: if the gradients of a certain component for two adjacent steps often have the same sign, then the accumulation result of the corresponding term is positive, meaning we can appropriately increase the learning rate; if the gradients of two adjacent steps often have opposite signs, then the accumulation result is likely negative, meaning we can appropriately decrease the learning rate.</p>

    <p>Note that this is different from the idea of learning rate adjustment in Adam. The idea of Adam's learning rate adjustment is that if the gradient of a certain component remains very small for a long time, it means that the parameter may not have been learned well, so it attempts to increase its learning rate. Both methods have their own justifications.</p>

    <h2>A Brief Summary</h2>
    <p>This article primarily makes simple notes on the concepts of "Exponentiated Gradient Descent" and "Meta-Learning for Learning Rate Adjustment." "Exponentiated Gradient Descent" is a simple and effective scheme for non-negative constrained optimization, while "Meta-Learning for Learning Rate Adjustment" is a straightforward and easy-to-understand application of meta-learning. In introducing "Meta-Learning for Learning Rate Adjustment," I have made some simplifications compared to the form in the original paper, making it even simpler, though the underlying idea remains consistent.</p>

    <hr />
    <pre>,
    author={Su Jianlin},
    year={2022},
    month={Mar},
    url={\url{https://kexue.fm/archives/8968}},
}</pre>
</article>
<hr>
<footer style="margin-top: 3em; padding: 1.5em; background: #f5f5f5; border-radius: 8px; font-size: 0.9em; color: #555;">
    <p style="margin: 0 0 0.5em 0;"><strong>Citation</strong></p>
    <p style="margin: 0 0 0.5em 0;">
        This is a machine translation of the original Chinese article:<br>
        <a href="https://kexue.fm/archives/8968" style="color: #005fcc;">https://kexue.fm/archives/8968</a>
    </p>
    <p style="margin: 0 0 0.5em 0;">
        Original author: 苏剑林 (Su Jianlin)<br>
        Original publication: <a href="https://kexue.fm" style="color: #005fcc;">科学空间 (Scientific Spaces)</a>
    </p>
    <p style="margin: 0; font-style: italic;">
        Translated using Gemini 3 Flash. Please refer to the original for authoritative content.
    </p>
</footer>
