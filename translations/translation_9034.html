
    <style type="text/css">

    body {
    margin: 48px auto;
    max-width: 68ch;              /* character-based width reads better */
    padding: 0 16px;

    font-family: -apple-system, BlinkMacSystemFont, "Segoe UI",
                Roboto, "Helvetica Neue", Arial, sans-serif;
    font-size: 18px;
    line-height: 1.65;
    color: #333;
    background: #fafafa;
    }

    h1, h2, h3, h4 {
    line-height: 1.25;
    margin-top: 2.2em;
    margin-bottom: 0.6em;
    font-weight: 600;
    }

    h1 {
    font-size: 2.1em;
    margin-top: 0;
    }

    h2 {
    font-size: 1.6em;
    border-bottom: 1px solid #e5e5e5;
    padding-bottom: 0.3em;
    }

    h3 {
    font-size: 1.25em;
    }

    h4 {
    font-size: 1.05em;
    color: #555;
    }

    /* Paragraphs and lists */
    p {
    margin: 1em 0;
    }

    ul, ol {
    margin: 1em 0 1em 1.5em;
    }

    li {
    margin: 0.4em 0;
    }

    a {
    color: #005fcc;
    text-decoration: none;
    }

    a:hover {
    text-decoration: underline;
    }

    code {
    font-family: ui-monospace, SFMono-Regular, Menlo, Consolas, monospace;
    font-size: 0.95em;
    background: #f2f2f2;
    padding: 0.15em 0.35em;
    border-radius: 4px;
    }

    pre {
    font-family: ui-monospace, SFMono-Regular, Menlo, Consolas, monospace;
    font-size: 0.9em;
    background: #f5f5f5;
    padding: 1em 1.2em;
    overflow-x: auto;
    border-radius: 6px;
    line-height: 1.45;
    }

    pre code {
    background: none;
    padding: 0;
    }

    blockquote {
    margin: 1.5em 0;
    padding-left: 1em;
    border-left: 4px solid #ddd;
    color: #555;
    }

    hr {
    border: none;
    border-top: 1px solid #e0e0e0;
    margin: 3em 0;
    }

    table {
    border-collapse: collapse;
    margin: 1.5em 0;
    width: 100%;
    font-size: 0.95em;
    }

    th, td {
    padding: 0.5em 0.7em;
    border-bottom: 1px solid #e5e5e5;
    text-align: left;
    }

    th {
    font-weight: 600;
    }

    img {
    max-width: 100%;
    display: block;
    margin: 1.5em auto;
    }

    small {
    color: #666;
    }

    mjx-container {
    margin: 1em 0;
    }

    ::selection {
    background: #cce2ff;
    }

    </style>
    

<script>
window.MathJax = {
  tex: {
    inlineMath: [['$', '$'], ['\\(', '\\)']],
    displayMath: [['$$', '$$'], ['\\[', '\\]']],
    processEscapes: true,
    tags: 'ams',
    macros: {
      bar: ['\\overline{#1}', 1]
    }
  }
};
</script>
<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"></script>

<article>
    <h1><a href="https://kexue.fm/archives/9034">A Quick Derivation of Entropy-Invariant Softmax</a></h1>
    <p>By 苏剑林 | April 11, 2022</p>

    <p>In the article <a href="translation_8823.html">"From Entropy Invariance to the Scale Operation in Attention"</a>, we derived a version of the attention mechanism with entropy-invariant properties:</p>

    \begin{equation}Attention(Q,K,V) = softmax\left(\frac{\kappa \log n}{d}QK^{\top}\right)V\label{eq:a}\end{equation}

    <p>It can be observed that this is mainly achieved by introducing a length-related scaling factor $\log n$ into the Softmax. The original derivation was quite cumbersome and relied on several assumptions, which were not conducive to an intuitive understanding. This article provides a relatively concise and rapid derivation as a supplement.</p>

    <h2>Derivation Process</h2>
    <p>We can set aside the background of the attention mechanism and directly assume $s_1, s_2, \dots, s_n \in \mathbb{R}$, defining:</p>
    $$p_i = \frac{e^{\lambda s_i}}{\sum_{i=1}^n e^{\lambda s_i}}$$
    <p>Obviously, this is the result of applying Softmax to $s_1, s_2, \dots, s_n$ after multiplying them by a scaling factor $\lambda$. Now let us calculate its entropy:</p>

    \begin{equation}\begin{aligned}H =&\, -\sum_{i=1}^n p_i \log p_i = \log\sum_{i=1}^n e^{\lambda s_i} - \lambda\sum_{i=1}^n p_i s_i \\
    =&\, \log n + \log\frac{1}{n}\sum_{i=1}^n e^{\lambda s_i} - \lambda\sum_{i=1}^n p_i s_i
    \end{aligned}\end{equation}

    <p>The term inside the first $\log$ is "exponentiate then average." we use "average then exponentiate" (mean-field) to approximate it:</p>

    \begin{equation}
    \log\frac{1}{n}\sum_{i=1}^n e^{\lambda s_i}\approx \log\exp\left(\frac{1}{n}\sum_{i=1}^n \lambda s_i\right) = \lambda \bar{s}
    \end{equation}

    <p>Furthermore, we know that Softmax tends to focus on the $\max$ element (refer to <a href="https://kexue.fm/archives/6620#softmax">"Discussion on Function Smoothing: Differentiable Approximation of Non-differentiable Functions"</a>), so we have the approximation:</p>

    \begin{equation}\lambda\sum_{i=1}^n p_i s_i \approx \lambda s_{\max}\end{equation}

    <p>Therefore:</p>
    \begin{equation}H\approx \log n - \lambda(s_{\max} - \bar{s})\end{equation}

    <p>The so-called entropy invariance is the desire to eliminate the influence of the length $n$ as much as possible. Thus, according to the above equation, we need $\lambda \propto \log n$. If we place this back into the context of the attention mechanism, the form of $s$ is such that $\langle \boldsymbol{q}, \boldsymbol{k}\rangle \propto d$ (where $d$ is the vector dimension), so we need $\lambda \propto \frac{1}{d}$. Combining these, we get:</p>

    \begin{equation}\lambda\propto \frac{\log n}{d}\end{equation}

    <p>This is the result shown in Equation $\eqref{eq:a}$ at the beginning of the article.</p>

    <h2>Article Summary</h2>
    <p>A simple and clear derivation has been formulated for the previously proposed "Entropy-Invariant Softmax."</p>
</article>
<hr>
<footer style="margin-top: 3em; padding: 1.5em; background: #f5f5f5; border-radius: 8px; font-size: 0.9em; color: #555;">
    <p style="margin: 0 0 0.5em 0;"><strong>Citation</strong></p>
    <p style="margin: 0 0 0.5em 0;">
        This is a machine translation of the original Chinese article:<br>
        <a href="https://kexue.fm/archives/9034" style="color: #005fcc;">https://kexue.fm/archives/9034</a>
    </p>
    <p style="margin: 0 0 0.5em 0;">
        Original author: 苏剑林 (Su Jianlin)<br>
        Original publication: <a href="https://kexue.fm" style="color: #005fcc;">科学空间 (Scientific Spaces)</a>
    </p>
    <p style="margin: 0; font-style: italic;">
        Translated using Gemini 3 Flash. Please refer to the original for authoritative content.
    </p>
</footer>
