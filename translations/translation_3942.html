
    <style type="text/css">

    body {
    margin: 48px auto;
    max-width: 68ch;              /* character-based width reads better */
    padding: 0 16px;

    font-family: -apple-system, BlinkMacSystemFont, "Segoe UI",
                Roboto, "Helvetica Neue", Arial, sans-serif;
    font-size: 18px;
    line-height: 1.65;
    color: #333;
    background: #fafafa;
    }

    h1, h2, h3, h4 {
    line-height: 1.25;
    margin-top: 2.2em;
    margin-bottom: 0.6em;
    font-weight: 600;
    }

    h1 {
    font-size: 2.1em;
    margin-top: 0;
    }

    h2 {
    font-size: 1.6em;
    border-bottom: 1px solid #e5e5e5;
    padding-bottom: 0.3em;
    }

    h3 {
    font-size: 1.25em;
    }

    h4 {
    font-size: 1.05em;
    color: #555;
    }

    /* Paragraphs and lists */
    p {
    margin: 1em 0;
    }

    ul, ol {
    margin: 1em 0 1em 1.5em;
    }

    li {
    margin: 0.4em 0;
    }

    a {
    color: #005fcc;
    text-decoration: none;
    }

    a:hover {
    text-decoration: underline;
    }

    code {
    font-family: ui-monospace, SFMono-Regular, Menlo, Consolas, monospace;
    font-size: 0.95em;
    background: #f2f2f2;
    padding: 0.15em 0.35em;
    border-radius: 4px;
    }

    pre {
    font-family: ui-monospace, SFMono-Regular, Menlo, Consolas, monospace;
    font-size: 0.9em;
    background: #f5f5f5;
    padding: 1em 1.2em;
    overflow-x: auto;
    border-radius: 6px;
    line-height: 1.45;
    }

    pre code {
    background: none;
    padding: 0;
    }

    blockquote {
    margin: 1.5em 0;
    padding-left: 1em;
    border-left: 4px solid #ddd;
    color: #555;
    }

    hr {
    border: none;
    border-top: 1px solid #e0e0e0;
    margin: 3em 0;
    }

    table {
    border-collapse: collapse;
    margin: 1.5em 0;
    width: 100%;
    font-size: 0.95em;
    }

    th, td {
    padding: 0.5em 0.7em;
    border-bottom: 1px solid #e5e5e5;
    text-align: left;
    }

    th {
    font-weight: 600;
    }

    img {
    max-width: 100%;
    display: block;
    margin: 1.5em auto;
    }

    small {
    color: #666;
    }

    mjx-container {
    margin: 1em 0;
    }

    ::selection {
    background: #cce2ff;
    }

    </style>
    

<script>
window.MathJax = {
  tex: {
    inlineMath: [['$', '$'], ['\\(', '\\)']],
    displayMath: [['$$', '$$'], ['\\[', '\\]']],
    tags: 'ams',
    packages: {'[+]': ['base', 'ams', 'amsmath', 'amsbsy', 'amsfonts', 'amssymb']}
  },
  loader: {load: ['[tex]/ams']}
};
</script>
<script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

<article>
    <h1><a href="https://kexue.fm/archives/3942">Core Entity Recognition based on Bidirectional LSTM and Transfer Learning</a></h1>
    <p>By 苏剑林 | September 06, 2016</p>

    <p>During the summer vacation, I participated in the <a href="http://openresearch.baidu.com/online/artical.do?method=activityItemDetail&activityID=26eb630e-5839-452d-ad71-bf023a8d6327&navIndex=1">Core Entity Recognition Competition jointly organized by Baidu and Xi'an Jiaotong University</a>. The final results were quite good, so I'm recording it here. The performance of the model is not the absolute best, but it excels at being "<strong>end-to-end</strong>" and has strong transferability, which I believe will be of reference value to everyone.</p>

    <p>The theme of the competition is "Core Entity Recognition," which actually involves two tasks: Core Recognition + Entity Recognition. Although these two tasks are related, in traditional natural language processing programs, they are generally handled separately. This time, they need to be combined. If we only look at "Core Recognition," it is a traditional keyword extraction task. However, the difference is that traditional purely statistics-based approaches (such as TF-IDF extraction) are not feasible because a core entity in a single sentence might only appear once. In such cases, statistical estimation is unreliable, and it is better to understand it from a semantic perspective. I initially started with "Core Recognition" using a method similar to a QA system:</p>

    <blockquote>
        <p>1. Segment the sentence into words, then use Word2Vec to train word vectors;</p>
        <p>2. Use a Convolutional Neural Network (in this type of extraction problem, CNNs often perform better than RNNs) to perform convolution and obtain an output with the same dimension as the word vector;</p>
        <p>3. The loss function is the cosine similarity between the output vector and the vector of the core word in the training sample.</p>
    </blockquote>

    <p>Thus, to find the core words of a sentence, I only needed to calculate an output vector for each sentence and then compare its cosine similarity with the vector of each word in the sentence, sorting them in descending order. The obvious advantage of this method is its fast processing speed. In the end, I used this model to achieve an accuracy of 0.35 on the public evaluation set. Later, I felt it was difficult to improve and abandoned this approach.</p>

    <p><a href="https://kexue.fm/usr/uploads/2016/09/1333598934.png" title="Click to view original image"><img src="https://kexue.fm/usr/uploads/2016/09/1333598934.png" alt="Worshiping the 0.7 masters" /></a><br />Worshiping the 0.7 masters</p>

    <p>Why did I give up? In fact, while this approach performed well in the "Core Recognition" part, its fatal flaw was its dependence on word segmentation performance. Segmentation systems often split core entities composed of long words. For example, "Zhu Family Garden" (朱家花园) might be split into "Zhu Family/Garden" (朱家/花园). After segmenting, it becomes much more difficult to integrate them back. Consequently, I referred to the article <a href="translation_3924.html">"【Chinese Word Segmentation Series】 4. seq2seq Character Labeling based on Bidirectional LSTM"</a> and adopted the sequence labeling approach, as it does not clearly depend on segmentation results. Ultimately, I achieved 0.56 accuracy using this method.</p>

    <p>The main steps were:</p>

    <blockquote>
        <p>1. Segment the sentences and train word vectors with Word2Vec;</p>
        <p>2. Convert the output into a 5-tag labeling problem: b (beginning of core entity), m (middle of core entity), e (end of core entity), s (single word as core entity), x (non-core entity part);</p>
        <p>3. Use a dual-layer Bidirectional LSTM for prediction and the Viterbi algorithm for labeling.</p>
    </blockquote>

    <p>Finally, it is worth mentioning that according to this approach, one could even perform core entity recognition without prior word segmentation. However, generally speaking, segmented results are better, and segmentation helps reduce sentence length (a 100-character sentence becomes a 50-word sentence after segmentation), which is beneficial for reducing model parameters. Here, we only need a simple segmentation system without needing its built-in new word discovery features.</p>

    <h3>Transfer Learning</h3>

    <p>Before using this approach, I was very uncertain about its final effectiveness. The main reason was: <strong>Baidu provided 12,000 training samples, but there were 200,000 test samples.</strong> This ratio is so disparate that the performance seemed unlikely to be good. Furthermore, there are 5 tags, and compared to the 'x' tag, the numbers for the other four tags are very small. With only 12,000 training samples, there seemed to be a serious issue of insufficient data.</p>

    <p>Of course, practice is the sole criterion for testing truth. The first test of this approach reached an accuracy of 0.42, which was much higher than the CNN approach I had meticulously tuned for over half a month. So I continued with this path. Before proceeding, I analyzed why this approach worked well. I believe the main reasons are two: one is "Transfer Learning," and the other is LSTM's powerful semantic capturing ability.</p>

    <p>Traditional data mining training models are purely built on the training set. However, we cannot guarantee that the training set and the test set are consistent; or more accurately, it is hard to assume they share the same distribution. Consequently, even if a model trains very well, its test performance may be a mess. This is not necessarily caused by overfitting; it is caused by the inconsistency between the training and test sets.</p>

    <p>One way to solve (or alleviate) this problem is "Transfer Learning." Transfer learning is now a comprehensive modeling strategy and will not be detailed here in full. Generally, there are two schemes:</p>

    <blockquote>
        <p>1. Transfer learning before modeling: that is, the training set and testing set can be put together to learn the features used for modeling. The features obtained this way already contain information about the testing set;</p>
        <p>2. Transfer learning after modeling: if the test result is reasonably good, for example, 0.5 accuracy, and you want to improve it, you can take the test set along with its prediction results as training samples and retrain the model together with the original training samples.</p>
    </blockquote>

    <p>People might be confused about the second point: aren't the prediction results for the test set incorrect in many cases? Can inputting incorrect results improve accuracy? Tolstoy said, "All happy families are alike; each unhappy family is unhappy in its own way." Applying it here, I would say, "All correct answers are the same, while wrong answers are incorrect in their own special ways." That is to say, if the second type of training is performed, the effect of correct answers in the test set will accumulate because they all come from the same correct patterns. However, wrong answers have various wrong patterns. If the model parameters are limited and overfitting is avoided, the model will smooth out these wrong patterns and thus tend toward the correct answers. Of course, whether this understanding is accurate is for the readers to judge. Additionally, if new prediction results are obtained, one can take only the parts where two predictions were identical as training samples, making the proportion of correct answers even higher.</p>

    <p>In this competition, transfer learning was reflected in:</p>

    <blockquote>
        <p>1. Training Word2Vec using both training and test corpora, allowing word vectors to capture the semantics of the test corpus;</p>
        <p>2. Training the model with training data;</p>
        <p>3. After obtaining the model, predict the test data and use the prediction results along with the training data to train a new model;</p>
        <p>4. Predict with the new model; the performance will improve;</p>
        <p>5. Compare the results of two predictions. If they are identical, it indicates the prediction is very likely correct. Use this "likely correct" portion of test results to train the model;</p>
        <p>6. Predict with the updated model;</p>
        <p>7. If you wish, you can repeat steps 4, 5, and 6.</p>
    </blockquote>

    <h3>Bidirectional LSTM</h3>

    <p>Main model structure:</p>

    <pre><code class="python"># (Original code snippet was here)
    </code></pre>

    <p>I used a dual-layer Bidirectional LSTM (I tried a single layer, but adding an extra layer performed better), preserved the output of the LSTM at each step, and applied a softmax to each output. The entire process is basically like a word segmentation system.</p>

    <p><a href="https://kexue.fm/usr/uploads/2016/09/245762055.png" title="Click to view original image"><img src="https://kexue.fm/usr/uploads/2016/09/245762055.png" alt="model" /></a><br />model</p>

    <p>Of course, the quality of this model also largely depends on the quality of the word vectors. After multiple adjustments, I found the following word vector parameters to be basically optimal:</p>

    <pre><code class="python"># (Original code snippet was here)
    </code></pre>

    <p>That is, skip-gram performs better than CBOW, negative sampling mode is better than hierarchical softmax, the number of negative samples should be moderate, and the window size should be moderate. Of course, this so-called "optimal" is based on my own "intuition" after manual tuning. Everyone is welcome to conduct more tests.</p>

    <h3>About the Competition</h3>

    <p><strong>I actually noticed this competition by Baidu and XJTU last year, but I was at a novice level then and couldn't handle such a difficult task. I tried it this year and felt I gained a lot.</strong></p>

    <p><strong>Firstly, the competition is held by Baidu, which in itself is very attractive. Usually, it feels like gaining Baidu's recognition is a great accomplishment, so I look forward to such competitions (I hope to have time to participate) and wish Baidu's competitions go better and better (standard formalities~). Secondly, through this process, I gained a deeper understanding of language model examples, building and using deep networks, such as how CNNs are used for language tasks, which language tasks they can be applied to, and the preliminary use of seq2seq.</strong></p>

    <p><strong>Coincidentally, this was an NLP task, and the last Teddy Cup was an image task. Adding the two together, I have gone through the basic tasks of both NLP and Computer Vision. I now feel quite confident about handling these tasks; it feels very grounding.</strong></p>

    <h3>Complete Code</h3>

    <p>Training set link: <a href="https://pan.baidu.com/s/1i457nkL">https://pan.baidu.com/s/1i457nkL</a> Password: stkp</p>

    <p><strong>Description File</strong></p>

    <blockquote>
        <p>Core Entity Recognition based on Transfer Learning and Bidirectional LSTM</p>
        <p>==============================================================</p>
        <p>General Steps (corresponding in train_and_predict.py):</p>
        <p>==============================================================</p>
        <p>1. Segment both training and test corpora; currently using jieba;</p>
        <p>2. Convert to a 5-tag labeling problem and construct training labels;</p>
        <p>3. Train the Word2Vec model with both sets of corpora;</p>
        <p>4. Train the labeling model using a dual-layer Bidirectional LSTM based on seq2seq ideas;</p>
        <p>5. Predict with the model; accuracy will fluctuate roughly between 0.46 and 0.52;</p>
        <p>6. Treat prediction results as labeled data and retrain the model together with training data;</p>
        <p>7. Predict with the new model; accuracy will fluctuate between 0.5 and 0.55;</p>
        <p>8. Compare two prediction results, take the intersection as labeled data, and retrain the model with training data;</p>
        <p>9. Predict with the new model; accuracy stays roughly between 0.53 and 0.56.</p>
        <p>==============================================================</p>
        <p>Compilation Environment:</p>
        <p>==============================================================</p>
        <p>Hardware environment:</p>
        <p>1. 96G memory (actually about 10G is used)</p>
        <p>2. GTX960 graphics card (GPU accelerated training)</p>
        <p>Software environment:</p>
        <p>1. CentOS 7</p>
        <p>2. Python 2.7 (all below are Python third-party libraries)</p>
        <p>3. Jieba</p>
        <p>4. Numpy</p>
        <p>5. SciPy</p>
        <p>6. Pandas</p>
        <p>7. Keras (Official GitHub version)</p>
        <p>8. Gensim</p>
        <p>9. H5PY</p>
        <p>10. tqdm</p>
        <p>==============================================================</p>
        <p>File Usage Instructions:</p>
        <p>==============================================================</p>
        <p>train_and_predict.py</p>
        <p>Contains the entire process from training to prediction. As long as the "unopened verification data" format is the same as the "open test data" opendata_20w, it can be put in the same directory as train_and_predict.py, then run:</p>
        <p>python train_and_predict.py</p>
        <p>This will complete the whole process and generate a series of files:</p>
        <p>--------------------------------------------------------------</p>
        <p>word2vec_words_final.model, the Word2Vec model</p>
        <p>words_seq2seq_final_1.model, the first dual-layer Bi-LSTM model</p>
        <p>--- result1.txt, the first prediction results</p>
        <p>--- result1.zip, compressed package of the first results</p>
        <p>words_seq2seq_final_2.model, model after the first transfer learning</p>
        <p>--- result2.txt, second prediction results</p>
        <p>--- result2.zip, compressed second results</p>
        <p>words_seq2seq_final_3.model, model after the second transfer learning</p>
        <p>--- result3.txt, third prediction results</p>
        <p>--- result3.zip, compressed third results</p>
        <p>words_seq2seq_final_4.model, model after the third transfer learning</p>
        <p>--- result4.txt, fourth prediction results</p>
        <p>--- result4.zip, compressed fourth results</p>
        <p>words_seq2seq_final_5.model, model after the fourth transfer learning</p>
        <p>--- result5.txt, fifth prediction results</p>
        <p>--- result5.zip, compressed fifth results</p>
        <p>---------------------------------------------------------------</p>
        <p>==============================================================</p>
        <p>Logic Description:</p>
        <p>==============================================================</p>
        <p>Transfer learning is reflected in:</p>
        <p>1. Training Word2Vec with both training and test corpora allows word vectors to capture the semantics of test items;</p>
        <p>2. Training the model with training data;</p>
        <p>3. After obtaining the model, predict the test data and use results alongside training data for a new model;</p>
        <p>4. Predict with the new model to improve accuracy;</p>
        <p>5. Compare predictions; identical results are likely correct and are used to train the model;</p>
        <p>6. Predict with updated models;</p>
        <p>7. Repeat steps 4, 5, 6 as desired.</p>
        <p>Bi-LSTM logic:</p>
        <p>1. Segment words;</p>
        <p>2. Convert to 5-tag problem (0: non-entity, 1: single-word entity, 2: start word of multi-word entity, 3: middle of multi-word entity, 4: end of multi-word entity);</p>
        <p>3. Directly output predicted tag sequences for input sentences via Bi-LSTM;</p>
        <p>4. Use Viterbi algorithm to finalize results;</p>
        <p>5. Use Bi-LSTM because standard LSTMs suffer from the disadvantage of later words being more "important" than earlier ones.</p>
    </blockquote>

    <p><strong>train_and_predict.py</strong> (Code not organized, for test reference only)</p>

    <pre><code class="python"># (Original code snippet was here)
    </code></pre>

    <hr />

    <p><em><strong>For detailed reprinting matters, please refer to:</strong> <a href="https://kexue.fm/archives/6508#%E6%96%87%E7%AB%A0%E5%A6%82%E4%BD%95%E8%BD%AC%E8%BD%BD/%E5%BC%95%E7%94%A8">"Scientific Spaces FAQ"</a></em></p>
    <p><strong>If you have any doubts or suggestions, please feel free to discuss them in the comments section below.</strong></p>

    <p><strong>If you found this article helpful, you are welcome to <a href="https://kexue.fm/archives/3942#share">share</a> or <a href="https://kexue.fm/archives/3942#pay">donate</a>. Donations are not for profit, but to know how many readers are genuinely interested in Scientific Spaces. Of course, if you ignore it, it will not affect your reading. Welcome and thank you!</strong></p>

    <p>Su Jianlin. (Sep. 06, 2016). "Core Entity Recognition based on Bidirectional LSTM and Transfer Learning" [Blog post]. Retrieved from <a href="https://kexue.fm/archives/3942">https://kexue.fm/archives/3942</a></p>

    </article>
<hr>
<footer style="margin-top: 3em; padding: 1.5em; background: #f5f5f5; border-radius: 8px; font-size: 0.9em; color: #555;">
    <p style="margin: 0 0 0.5em 0;"><strong>Citation</strong></p>
    <p style="margin: 0 0 0.5em 0;">
        This is a machine translation of the original Chinese article:<br>
        <a href="https://kexue.fm/archives/3942" style="color: #005fcc;">https://kexue.fm/archives/3942</a>
    </p>
    <p style="margin: 0 0 0.5em 0;">
        Original author: 苏剑林 (Su Jianlin)<br>
        Original publication: <a href="https://kexue.fm" style="color: #005fcc;">科学空间 (Scientific Spaces)</a>
    </p>
    <p style="margin: 0; font-style: italic;">
        Translated using Gemini 3 Flash. Please refer to the original for authoritative content.
    </p>
</footer>
