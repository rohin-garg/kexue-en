
    <style type="text/css">

    body {
    margin: 48px auto;
    max-width: 68ch;              /* character-based width reads better */
    padding: 0 16px;

    font-family: -apple-system, BlinkMacSystemFont, "Segoe UI",
                Roboto, "Helvetica Neue", Arial, sans-serif;
    font-size: 18px;
    line-height: 1.65;
    color: #333;
    background: #fafafa;
    }

    h1, h2, h3, h4 {
    line-height: 1.25;
    margin-top: 2.2em;
    margin-bottom: 0.6em;
    font-weight: 600;
    }

    h1 {
    font-size: 2.1em;
    margin-top: 0;
    }

    h2 {
    font-size: 1.6em;
    border-bottom: 1px solid #e5e5e5;
    padding-bottom: 0.3em;
    }

    h3 {
    font-size: 1.25em;
    }

    h4 {
    font-size: 1.05em;
    color: #555;
    }

    /* Paragraphs and lists */
    p {
    margin: 1em 0;
    }

    ul, ol {
    margin: 1em 0 1em 1.5em;
    }

    li {
    margin: 0.4em 0;
    }

    a {
    color: #005fcc;
    text-decoration: none;
    }

    a:hover {
    text-decoration: underline;
    }

    code {
    font-family: ui-monospace, SFMono-Regular, Menlo, Consolas, monospace;
    font-size: 0.95em;
    background: #f2f2f2;
    padding: 0.15em 0.35em;
    border-radius: 4px;
    }

    pre {
    font-family: ui-monospace, SFMono-Regular, Menlo, Consolas, monospace;
    font-size: 0.9em;
    background: #f5f5f5;
    padding: 1em 1.2em;
    overflow-x: auto;
    border-radius: 6px;
    line-height: 1.45;
    }

    pre code {
    background: none;
    padding: 0;
    }

    blockquote {
    margin: 1.5em 0;
    padding-left: 1em;
    border-left: 4px solid #ddd;
    color: #555;
    }

    hr {
    border: none;
    border-top: 1px solid #e0e0e0;
    margin: 3em 0;
    }

    table {
    border-collapse: collapse;
    margin: 1.5em 0;
    width: 100%;
    font-size: 0.95em;
    }

    th, td {
    padding: 0.5em 0.7em;
    border-bottom: 1px solid #e5e5e5;
    text-align: left;
    }

    th {
    font-weight: 600;
    }

    img {
    max-width: 100%;
    display: block;
    margin: 1.5em auto;
    }

    small {
    color: #666;
    }

    mjx-container {
    margin: 1em 0;
    }

    ::selection {
    background: #cce2ff;
    }

    </style>
    

<script>
window.MathJax = {
  tex: {
    inlineMath: [['$', '$'], ['\\(', '\\)']],
    displayMath: [['$$', '$$'], ['\\[', '\\]']],
    processEscapes: true,
    tags: 'ams',
    packages: {'[+]': ['ams']}
  },
  loader: {load: ['[tex]/ams']}
};
</script>
<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

<article>
    <nav style="margin-bottom: 1.5em;">
    <a href="../index.html" style="display: inline-flex; align-items: center; color: #555; text-decoration: none; font-size: 0.95em;">
        <span style="margin-right: 0.3em;">&larr;</span> Back to Index
    </a>
</nav>

    <h1><a href="https://kexue.fm/archives/7309">How the Two Elementary Function Approximations of GELU Came to Be</a></h1>
    <p>By 苏剑林 | March 26, 2020</p>

    <p>GELU, which stands for Gaussian Error Linear Unit, is a variant of the ReLU activation function and is expressed in a non-elementary form. It was introduced in the paper <a href="https://papers.cool/arxiv/1606.08415">"Gaussian Error Linear Units (GELUs)"</a>, used later in GPT, then in BERT, and subsequently adopted by many later pre-trained language models. With the rise of BERT and other pre-trained models, GELU has surged in popularity, becoming a trendy activation function almost overnight.</p>

    <p><img src="https://img.kexue.fm/2020/03/gelu_plot.png" alt="gelu function image"></p>

    <p>In the original GELU paper, the authors proposed not only the exact form of GELU but also provided two elementary function approximations. This article discusses how those approximations were derived.</p>

    <h2>The GELU Function</h2>
    <p>The form of the GELU function is:</p>
    \begin{equation}\text{GELU}(x)=x \Phi(x)\end{equation}
    <p>where $\Phi(x)$ is the cumulative distribution function of the standard normal distribution, i.e.,</p>
    \begin{equation}\Phi(x)=\int_{-\infty}^x \frac{e^{-t^2/2}}{\sqrt{2\pi}}dt=\frac{1}{2}\left[1 + \text{erf}\left(\frac{x}{\sqrt{2}}\right)\right]\end{equation}
    <p>Here $\text{erf}(x)=\frac{2}{\sqrt{\pi}}\int_0^x e^{-t^2}dt$. The original paper then mentions two approximations:</p>
    \begin{equation}x\Phi(x)\approx x\sigma(1.702 x)\label{eq:x-sigma}\end{equation}
    <p>and</p>
    \begin{equation}x\Phi(x)\approx \frac{1}{2} x \left[1 + \tanh\left(\sqrt{\frac{2}{\pi}}\left(x + 0.044715 x^3\right)\right)\right]\label{eq:x-phi}\end{equation}
    <p>Currently, many implementations of Transformer-based models still use the approximation \eqref{eq:x-phi} for the GELU function. However, since many frameworks now include precise $\text{erf}$ calculation functions, the value of these elementary function approximations might be diminishing. Thus, consider this as a mathematical analysis exercise.</p>

    <h2>What to Use for Approximation</h2>
    <p>Finding an approximation for GELU is equivalent to finding an approximation for $\Phi(x)$, which in turn is equivalent to finding an approximation for $\text{erf}\left(\frac{x}{\sqrt{2}}\right)$.</p>

    <p><img src="https://img.kexue.fm/2020/03/erf_plot.png" alt="erf function image"></p>

    <p>First, we need to address the question of what function to use for the approximation. From the graph of $\text{erf}(x)$, we can observe its characteristics:</p>
    <blockquote>
        <p>1. It is an odd function, i.e., $\text{erf}(x)=-\text{erf}(-x)$;</p>
        <p>2. It is monotonically increasing, with $\lim\limits_{x\to -\infty}\text{erf}(x)=-1$ and $\lim\limits_{x\to +\infty}\text{erf}(x)=1$.</p>
    </blockquote>
    <p>We have many examples of odd functions, such as $x^{2n+1}, \sin x, \tan x, \tanh x$, etc. Furthermore, the superposition or composition of odd functions remains an odd function, such as $\sin\left(x + x^3\right)$. Among functions that are odd, monotonically increasing, and bounded, $\tanh x$ is perhaps the most obvious choice. In fact, $\tanh x$ is very similar to $\text{erf}(x)$.</p>
    <p>Therefore, we can start from $\tanh x$ to construct possible fitting forms, such as:</p>
    \begin{equation}\left\{\begin{aligned}
    &\tanh\left(a x + b x^3 + c x^5\right)\\
    &a\tanh x + b \tanh^3 x + c \tanh^5 x\\
    &a\tanh bx + c \tanh dx + e \tanh fx\\
    &\vdots
    \end{aligned}\right.\end{equation}

    <h2>How to Approximate</h2>
    <p>Once we have the form to be fitted, the next concern is how to perform the fitting and what criteria to use. Generally speaking, there are two approaches: local fitting and global fitting.</p>

    <h3>Local Fitting</h3>
    <p>Local fitting is based on Taylor expansion. For example, considering the approximation form $\tanh\left(a x + b x^3\right)$, we expand it around $x=0$ to get:</p>
    \begin{equation}\text{erf}\left(\frac{x}{\sqrt{2}}\right) - \tanh\left(a x + b x^3\right)=\left(\sqrt{\frac{2}{\pi }}-a\right) x + \left(\frac{a^3}{3}-b-\frac{1}{3 \sqrt{2 \pi }}\right)x^3 + \dots\end{equation}
    <p>By setting the first two terms to zero, we obtain two equations. Solving them yields:</p>
    \begin{equation}a=\sqrt{\frac{2}{\pi}},\quad b=\frac{4-\pi }{3 \sqrt{2} \pi ^{3/2}}\end{equation}
    <p>Substituting these back into $x\Phi(x)$ and converting them to numerical form gives:</p>
    \begin{equation}x\Phi(x)\approx \frac{1}{2} x\left[1 + \tanh\left(\sqrt{\frac{2}{\pi}}\left(x + 0.0455399 x^3\right)\right)\right]\label{eq:x-phi-local}\end{equation}

    <h3>Global Fitting</h3>
    <p>Equation \eqref{eq:x-phi-local} is already quite close to equation \eqref{eq:x-phi}, but the second coefficient is still slightly off. This is because \eqref{eq:x-phi-local} is purely a result of local approximation. As the name suggests, local approximation is very accurate locally; for instance, the derivation above is based on the Taylor expansion at $x=0$, so it is very accurate near $x=0$, but the error increases as we move further away from $0$. Thus, we also need to consider global error.</p>
    <p>A common global error measure is the integral form. For example, when approximating $f(x)$ with $g(x,\theta)$, we minimize:</p>
    \begin{equation}\min_{\theta} \int [f(x)-g(x,\theta)]^2 dx \quad\text{or}\quad \min_{\theta} \int |f(x)-g(x,\theta)| dx \end{equation}
    <p>However, the importance of error at each $x$ might vary. Therefore, to ensure generality, one might multiply by a weight $\lambda(x)$:</p>
    \begin{equation}\min_{\theta} \int \lambda(x)[f(x)-g(x,\theta)]^2 dx \quad\text{or}\quad \min_{\theta} \int \lambda(x)|f(x)-g(x,\theta)| dx \end{equation}
    <p>Different choices of $\lambda(x)$ lead to different solutions, and choosing the most suitable $\lambda(x)$ is not straightforward.</p>
    <p>Instead of optimizing this integral error, we optimize a more intuitive min-max error:</p>
    \begin{equation}\min_{\theta} \max_x |f(x)-g(x,\theta)|\end{equation}
    <p>This expression is easy to understand: "Find an appropriate $\theta$ such that the maximum $|f(x)-g(x,\theta)|$ is as small as possible." Such a goal aligns with our intuitive understanding and avoids the selection of weights.</p>

    <h3>Hybrid Fitting</h3>
    <p>Based on this idea, we fix $a=\sqrt{\frac{2}{\pi}}$ and then re-solve for $\tanh\left(a x + b x^3\right)$. We fix this $a$ because it represents the first-order local approximation; we want to preserve some local accuracy while letting $b$ help us minimize the global error as much as possible, thus achieving a hybrid of local and global approximation. So, we now solve:</p>
    \begin{equation}\min_{b} \max_x \left|\text{erf}\left(\frac{x}{\sqrt{2}}\right)-\tanh\left(a x + b x^3\right)\right|\end{equation}
    <p>Using scipy, this can be easily solved:</p>
<pre><code>import numpy as np
from scipy.special import erf
from scipy.optimize import minimize

def f(x, b):
 a = np.sqrt(2 / np.pi)
 return np.abs(erf(x / np.sqrt(2)) - np.tanh(a * x + b * x**3))

def g(b):
 return np.max([f(x, b) for x in np.arange(0, 4, 0.001)])

options = {'xtol': 1e-10, 'ftol': 1e-10, 'maxiter': 100000}
result = minimize(g, 0, method='Powell', options=options)
print(result.x)
</code></pre>
    <p>Finally, we obtain $b=0.035677337314877385$, which corresponds to the form:</p>
    \begin{equation}x\Phi(x)\approx \frac{1}{2} x\left[1 + \tanh\left(\sqrt{\frac{2}{\pi}}\left(x + 0.04471491123850965 x^3\right)\right)\right]\label{eq:x-phi-global}\end{equation}
    <p>The last few significant digits might have some error, but the preceding part matches equation \eqref{eq:x-phi} perfectly. As a supplementary note, equation \eqref{eq:x-phi} was proposed in the paper <a href="https://www.jstor.org/stable/2346872">"Approximations to the Cumulative Normal Function and its Inverse for Use on a Pocket Calculator"</a>, which is a result from over 40 years ago.</p>
    <p>As for the first approximation, it comes from the paper <a href="https://core.ac.uk/download/pdf/41787448.pdf">"A logistic approximation to the cumulative normal distribution"</a>. It is the result of using $\sigma(\lambda x)$ to globally approximate $\Phi(x)$ directly, i.e.,</p>
    \begin{equation}\min_{\lambda}\max_{x}\left|\Phi(x) - \sigma(\lambda x)\right|\end{equation}
    <p>Solving this yields $\lambda=1.7017449256323682$, which means:</p>
    \begin{equation}\Phi(x)\approx \sigma(1.7017449256323682 x)\end{equation}
    <p>This is also very consistent with equation \eqref{eq:x-sigma}.</p>

    <h2>Article Summary</h2>
    <p>In this article, we solved a mathematical analysis problem together—introducing the GELU activation function and attempting to explore the origins of its two approximate elementary forms, successfully "watering" this blog post into existence.</p>
</article>
<hr>
<footer style="margin-top: 3em; padding: 1.5em; background: #f5f5f5; border-radius: 8px; font-size: 0.9em; color: #555;">
    <p style="margin: 0 0 0.5em 0;"><strong>Citation</strong></p>
    <p style="margin: 0 0 0.5em 0;">
        This is a machine translation of the original Chinese article:<br>
        <a href="https://kexue.fm/archives/7309" style="color: #005fcc;">https://kexue.fm/archives/7309</a>
    </p>
    <p style="margin: 0 0 0.5em 0;">
        Original author: 苏剑林 (Su Jianlin)<br>
        Original publication: <a href="https://kexue.fm" style="color: #005fcc;">科学空间 (Scientific Spaces)</a>
    </p>
    <p style="margin: 0; font-style: italic;">
        Translated using Gemini 3 Flash. Please refer to the original for authoritative content.
    </p>
</footer>
