
    <style type="text/css">

    body {
    margin: 48px auto;
    max-width: 68ch;              /* character-based width reads better */
    padding: 0 16px;

    font-family: -apple-system, BlinkMacSystemFont, "Segoe UI",
                Roboto, "Helvetica Neue", Arial, sans-serif;
    font-size: 18px;
    line-height: 1.65;
    color: #333;
    background: #fafafa;
    }

    h1, h2, h3, h4 {
    line-height: 1.25;
    margin-top: 2.2em;
    margin-bottom: 0.6em;
    font-weight: 600;
    }

    h1 {
    font-size: 2.1em;
    margin-top: 0;
    }

    h2 {
    font-size: 1.6em;
    border-bottom: 1px solid #e5e5e5;
    padding-bottom: 0.3em;
    }

    h3 {
    font-size: 1.25em;
    }

    h4 {
    font-size: 1.05em;
    color: #555;
    }

    /* Paragraphs and lists */
    p {
    margin: 1em 0;
    }

    ul, ol {
    margin: 1em 0 1em 1.5em;
    }

    li {
    margin: 0.4em 0;
    }

    a {
    color: #005fcc;
    text-decoration: none;
    }

    a:hover {
    text-decoration: underline;
    }

    code {
    font-family: ui-monospace, SFMono-Regular, Menlo, Consolas, monospace;
    font-size: 0.95em;
    background: #f2f2f2;
    padding: 0.15em 0.35em;
    border-radius: 4px;
    }

    pre {
    font-family: ui-monospace, SFMono-Regular, Menlo, Consolas, monospace;
    font-size: 0.9em;
    background: #f5f5f5;
    padding: 1em 1.2em;
    overflow-x: auto;
    border-radius: 6px;
    line-height: 1.45;
    }

    pre code {
    background: none;
    padding: 0;
    }

    blockquote {
    margin: 1.5em 0;
    padding-left: 1em;
    border-left: 4px solid #ddd;
    color: #555;
    }

    hr {
    border: none;
    border-top: 1px solid #e0e0e0;
    margin: 3em 0;
    }

    table {
    border-collapse: collapse;
    margin: 1.5em 0;
    width: 100%;
    font-size: 0.95em;
    }

    th, td {
    padding: 0.5em 0.7em;
    border-bottom: 1px solid #e5e5e5;
    text-align: left;
    }

    th {
    font-weight: 600;
    }

    img {
    max-width: 100%;
    display: block;
    margin: 1.5em auto;
    }

    small {
    color: #666;
    }

    mjx-container {
    margin: 1em 0;
    }

    ::selection {
    background: #cce2ff;
    }

    </style>
    

<script>
window.MathJax = {
  tex: {
    inlineMath: [['$', '$'], ['\\(', '\\)']],
    displayMath: [['$$', '$$'], ['\\[', '\\]']],
    tags: 'ams',
    packages: {'[+]': ['base', 'ams', 'noerrors', 'noundefined']}
  },
  options: {
    ignoreHtmlClass: 'tex2jax_ignore',
    processHtmlClass: 'tex2jax_process'
  },
  loader: {load: ['[tex]/ams']}
};
</script>
<script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js" id="MathJax-script" async></script>

<article>
    <nav style="margin-bottom: 1.5em;">
    <a href="../index.html" style="display: inline-flex; align-items: center; color: #555; text-decoration: none; font-size: 0.95em;">
        <span style="margin-right: 0.3em;">&larr;</span> Back to Index
    </a>
</nav>

    <h1><a href="https://kexue.fm/archives/3854">A Preliminary Exploration of OCR Technology: 8. Comprehensive Evaluation</a></h1>
    <p>By 苏剑林 | June 26, 2016</p>

    <h3>Data Verification</h3>
    <p>Although the model works well in a test environment, practice is the sole criterion for testing truth. In this section, we use our own model to perform comparative verification against JD.com's test data.</p>
    <p>Measuring the quality of an OCR system consists of two parts: (1) whether the text regions are successfully boxed; (2) whether the boxed text is successfully recognized. We use a scoring method to evaluate the recognition effect of each image. The scoring rules are as follows:</p>
    <blockquote>
        <p>If the boxed text area matches the box file provided in JD's detection samples, 1 point is added. If the text is correctly recognized, an additional 1 point is added. Finally, the score for each image is the total points divided by the total number of characters.</p>
    </blockquote>
    <p>According to this rule, the maximum score for each image is 2 points, and the minimum is 0 points. If the score exceeds 1, it indicates that the recognition effect is quite good. After comparison with JD's test data, our model's average score is approximately 0.84, which is passable but leaves room for improvement.</p>

    <h3>Model Overview</h3>
    <p>In this article, our goal was to establish a complete OCR system. After a series of efforts, we have basically achieved this goal.</p>
    <p>When designing the algorithms, we closely integrated basic assumptions and started from the perspective of human visual recognition, hoping to achieve the goal with the minimum number of steps. This philosophy is fully embodied in the feature extraction and text localization sections. Similarly, out of a preference for simplicity and simulating manual processes, we chose a Convolutional Neural Network (CNN) model for optical character recognition, which achieved a high accuracy rate. Finally, combined with a language model, we improved results through dynamic programming using a relatively straightforward approach.</p>
    <p>After testing, our system shows good results for identifying printed text and can serve as a text recognition tool for images from e-commerce, WeChat, and other platforms. A distinct feature is that <strong>our system can input the entire text image and obtain good results even in cases where the resolution is not high.</strong></p>

    <h3>Reflections on Results</h3>
    <p>A significant deficiency in the algorithms involved in this article is the existence of many "empirical parameters," such as the choice of the $h$ parameter during clustering, the density threshold in the definition of low-density areas, the number of convolutional kernels in the CNN, and the number of nodes in the hidden layers. Because there were not enough labeled samples for research, these parameters could only be derived based on experience and a small number of samples. We look forward to having more labeled data to obtain the optimal values for these parameters.</p>
    <p>Furthermore, in terms of recognizing text regions, there are still many points worth improving. Although we removed most non-text regions in just a few steps, these steps are still not intuitive enough and urgently need to be simplified. We believe that a good model should be based on simple assumptions and steps to achieve good results; therefore, a worthwhile task is to simplify assumptions and reduce the complexity of the workflow.</p>
    <p>In addition, regarding text segmentation, there is actually no single automatic segmentation algorithm that can handle every situation; thus, there is significant room for improvement in this step. According to relevant literature, a CNN+LSTM model can be used to directly recognize single lines of text, but this requires a massive amount of training samples and high-performance training machines, which is likely something only large enterprises can achieve.</p>
    <p>Clearly, there is still a lot of work that requires more in-depth research.</p>

    </article>
<hr>
<footer style="margin-top: 3em; padding: 1.5em; background: #f5f5f5; border-radius: 8px; font-size: 0.9em; color: #555;">
    <p style="margin: 0 0 0.5em 0;"><strong>Citation</strong></p>
    <p style="margin: 0 0 0.5em 0;">
        This is a machine translation of the original Chinese article:<br>
        <a href="https://kexue.fm/archives/3854" style="color: #005fcc;">https://kexue.fm/archives/3854</a>
    </p>
    <p style="margin: 0 0 0.5em 0;">
        Original author: 苏剑林 (Su Jianlin)<br>
        Original publication: <a href="https://kexue.fm" style="color: #005fcc;">科学空间 (Scientific Spaces)</a>
    </p>
    <p style="margin: 0; font-style: italic;">
        Translated using Gemini 3 Flash. Please refer to the original for authoritative content.
    </p>
</footer>
