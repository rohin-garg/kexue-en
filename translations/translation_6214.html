
    <style type="text/css">

    body {
    margin: 48px auto;
    max-width: 68ch;              /* character-based width reads better */
    padding: 0 16px;

    font-family: -apple-system, BlinkMacSystemFont, "Segoe UI",
                Roboto, "Helvetica Neue", Arial, sans-serif;
    font-size: 18px;
    line-height: 1.65;
    color: #333;
    background: #fafafa;
    }

    h1, h2, h3, h4 {
    line-height: 1.25;
    margin-top: 2.2em;
    margin-bottom: 0.6em;
    font-weight: 600;
    }

    h1 {
    font-size: 2.1em;
    margin-top: 0;
    }

    h2 {
    font-size: 1.6em;
    border-bottom: 1px solid #e5e5e5;
    padding-bottom: 0.3em;
    }

    h3 {
    font-size: 1.25em;
    }

    h4 {
    font-size: 1.05em;
    color: #555;
    }

    /* Paragraphs and lists */
    p {
    margin: 1em 0;
    }

    ul, ol {
    margin: 1em 0 1em 1.5em;
    }

    li {
    margin: 0.4em 0;
    }

    a {
    color: #005fcc;
    text-decoration: none;
    }

    a:hover {
    text-decoration: underline;
    }

    code {
    font-family: ui-monospace, SFMono-Regular, Menlo, Consolas, monospace;
    font-size: 0.95em;
    background: #f2f2f2;
    padding: 0.15em 0.35em;
    border-radius: 4px;
    }

    pre {
    font-family: ui-monospace, SFMono-Regular, Menlo, Consolas, monospace;
    font-size: 0.9em;
    background: #f5f5f5;
    padding: 1em 1.2em;
    overflow-x: auto;
    border-radius: 6px;
    line-height: 1.45;
    }

    pre code {
    background: none;
    padding: 0;
    }

    blockquote {
    margin: 1.5em 0;
    padding-left: 1em;
    border-left: 4px solid #ddd;
    color: #555;
    }

    hr {
    border: none;
    border-top: 1px solid #e0e0e0;
    margin: 3em 0;
    }

    table {
    border-collapse: collapse;
    margin: 1.5em 0;
    width: 100%;
    font-size: 0.95em;
    }

    th, td {
    padding: 0.5em 0.7em;
    border-bottom: 1px solid #e5e5e5;
    text-align: left;
    }

    th {
    font-weight: 600;
    }

    img {
    max-width: 100%;
    display: block;
    margin: 1.5em auto;
    }

    small {
    color: #666;
    }

    mjx-container {
    margin: 1em 0;
    }

    ::selection {
    background: #cce2ff;
    }

    </style>
    

<script>
window.MathJax = {
  tex: {
    inlineMath: [['$', '$'], ['\\(', '\\(']],
    displayMath: [['$$', '$$'], ['\\[', '\\]']],
    tags: 'ams'
  }
};
</script>
<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

<nav style="margin-bottom: 1.5em;">
    <a href="../index.html" style="display: inline-flex; align-items: center; color: #555; text-decoration: none; font-size: 0.95em;">
        <span style="margin-right: 0.3em;">&larr;</span> Back to Index
    </a>
</nav>

<h1><a href="https://kexue.fm/archives/6214">BiGAN-QP: A Simple and Clear Encoding & Generative Model</a></h1>

<p>By 苏剑林 | December 10, 2018</p>

<p>Not long ago, by directly analyzing in the dual space, I proposed an adversarial model framework called GAN-QP. Its characteristic is that it can be theoretically proven to neither suffer from vanishing gradients nor require Lipschitz (L) constraints, thus simplifying the construction and training of generative models.</p>

<p>GAN-QP is an adversarial framework, so in theory, all original GAN tasks can be attempted within it. In the previous article <a href="translation_6163.html">"A GAN that requires no L-constraint and does not suffer from vanishing gradients, want to know more?"</a>, we only tried the standard random generation task. In this article, we will experiment with a case that includes both a generator and an encoder: BiGAN-QP.</p>

<h2>BiGAN and BiGAN-QP</h2>

<p>Note that this is BiGAN, not the recently popular BigGAN. BiGAN is Bidirectional GAN, proposed in the paper <a href="https://papers.cool/arxiv/1605.09782">"Adversarial feature learning"</a>. At the same time, there was a very similar paper titled <a href="https://papers.cool/arxiv/1606.00704">"Adversarially Learned Inference"</a>, which proposed a model called ALI, which is essentially the same as BiGAN. In general, they both add an encoder to the ordinary GAN model, so that the model has both the random generation function of a normal GAN and the function of an encoder, which can be used to extract effective features. Applying the GAN-QP adversarial mode to BiGAN gives us BiGAN-QP.</p>

<p>Without further ado, let's look at the effect (left is the original image, right is the reconstruction):</p>

<p>
  <a href="https://kexue.fm/usr/uploads/2018/12/3211194948.jpg" title="Click to view original image">
    <img src="https://kexue.fm/usr/uploads/2018/12/3211194948.jpg" alt="BiGAN-QP Reconstruction results" />
  </a>
</p>
<p align="center">BiGAN-QP reconstruction effect chart</p>

<p>This is the result of reducing a 256x256x3 image to 256 dimensions and then reconstructing it. As you can see, the overall reconstruction effect is good, without the blurriness of ordinary autoencoders. There is some loss of detail, which is a bit worse compared to <a href="https://papers.cool/arxiv/1807.06358">IntroVAE</a>, but that is a matter of model architecture and hyperparameter tuning, which is not my specialty. In any case, this result shows that BiGAN-QP is workable and the effect is decent.</p>

<p>The content of this article has been updated in the original GAN-QP paper: <a href="https://papers.cool/arxiv/1811.07296">https://papers.cool/arxiv/1811.07296</a>, where readers can download the latest version on arXiv.</p>

<h2>Simplified Derivation of BiGAN-QP</h2>

<p>In fact, compared to GAN, the derivation of BiGAN is very simple. You only need to replace the original single input $x$ with a dual input $(x, z)$. Similarly, if you have the GAN-QP foundation, what we call BiGAN-QP is also very straightforward. Specifically, the original GAN-QP was like this:</p>

\begin{equation}\begin{aligned}&T= \mathop{\text{argmax}}_T\, \mathbb{E}_{(x_r,x_f)\sim p(x_r)q(x_f)}\left[T(x_r,x_f)-T(x_f,x_r) - \frac{(T(x_r,x_f)-T(x_f,x_r))^2}{2\lambda d(x_r,x_f)}\right] \\
&G = \mathop{\text{argmin}}_G\,\mathbb{E}_{(x_r,x_f)\sim p(x_r)q(x_f)}\left[T(x_r,x_f)-T(x_f,x_r)\right]
\end{aligned}\end{equation}

<p>Now it becomes:</p>

\begin{equation}\begin{aligned}T&= \mathop{\text{argmax}}_T\, \mathbb{E}_{x\sim p(x), z\sim q(z)}\left[\Delta T - \frac{\Delta T^2}{2\lambda d\big(x,E(x);G(z),z\big)}\right] \\
G,E &= \mathop{\text{argmin}}_{G,E}\,\mathbb{E}_{x\sim p(x), z\sim q(z)}[\Delta T]\\
\Delta T &= T(x,E(x);G(z),z)-T(G(z),z;x,E(x))
\end{aligned}\end{equation}

<p>Or a simplified version directly taking $\Delta T = T(x,E(x))-T(G(z),z)$. Theoretically, this works; this is BiGAN-QP.</p>

<p>But in practice, it is very difficult to learn a good bidirectional mapping this way, because it is equivalent to automatically searching for a bidirectional mapping from an infinite number of possible mappings, which is quite difficult. Therefore, we also need some "guiding terms." We use two MSE errors as guiding terms:</p>

\begin{equation}\begin{aligned}T&= \mathop{\text{argmax}}_T\, \mathbb{E}_{x\sim p(x), z\sim q(z)}\left[\Delta T - \frac{\Delta T^2}{2\lambda d\big(x,E(x);G(z),z\big)}\right] \\
G,E &= \mathop{\text{argmin}}_{G,E}\,\mathbb{E}_{x\sim p(x), z\sim q(z)}\Big[\Delta T + \beta_1 \Vert z - E(G(z))\Vert^2 + \beta_2 \Vert x - G(E(x))\Vert^2\Big]\\
\Delta T &= T(x,E(x))-T(G(z),z)
\end{aligned}\end{equation}

<p>In fact, the three loss terms of the generator are very intuitive: $\Delta T$ makes the generated images more realistic, $\Vert z - E(G(z))\Vert^2$ aims to reconstruct the latent variable space, and $\Vert x - G(E(x))\Vert^2$ aims to reconstruct the observable variable space. The last two terms should not be too large, especially the last one, as being too large will lead to image blurriness.</p>

<blockquote>
<p>These two regularization terms can be seen as an upper bound on the mutual information between $G(z)$ and $z$, and the mutual information between $x$ and $E(x)$. Therefore, from an information perspective, these two regularization terms hope that the mutual information between $x$ and $z$ is as large as possible. Related discussions can be found in the <a href="https://papers.cool/arxiv/1606.03657">InfoGAN</a> paper; these two regularization terms signify that it also belongs to the category of InfoGAN. So strictly speaking, this should be a Bi-Info-GAN-QP.</p>
<p>Mutual information terms can stabilize the GAN training process to a certain extent and reduce the possibility of mode collapse, because once the model collapses, the mutual information will not be large. In other words, if the model collapses, reconstruction becomes unlikely, and the reconstruction loss will be very large.</p>
</blockquote>

<p>Experiments show that after making small adjustments, the effect is better. This small adjustment stems from the fact that the coupling of the two MSE terms is still too powerful (the specific value of the loss is not necessarily large, but the gradient is), causing the model to still lean towards generating blurry images. Therefore, half of the gradient needs to be stopped, turning it into:</p>

\begin{equation}\begin{aligned}T&= \mathop{\text{argmax}}_T\, \mathbb{E}_{x\sim p(x), z\sim q(z)}\left[\Delta T - \frac{\Delta T^2}{2\lambda d\big(x,E(x);G(z),z\big)}\right] \\
G,E &= \mathop{\text{argmin}}_{G,E}\,\mathbb{E}_{x\sim p(x), z\sim q(z)}\Big[\Delta T + \beta_1 \Vert z - E(G_{ng}(z))\Vert^2 + \beta_2 \Vert x - G(E_{ng}(x))\Vert^2\Big]\\
\Delta T &= T(x,E(x))-T(G(z),z)
\end{aligned}\end{equation}

<p>$G_{ng}$ and $E_{ng}$ refer to forcing the gradient of these parts to be 0. Most frameworks have this operator; you can just call it. This is the final BiGAN-QP model in this article.</p>

<h2>Code and Result Images</h2>

<p>The code has also been added to Github: <a href="https://github.com/bojone/gan-qp/tree/master/bigan-qp">https://github.com/bojone/gan-qp/tree/master/bigan-qp</a></p>

<p>Here are more result images, randomly generated:</p>

<p>
  <a href="https://kexue.fm/usr/uploads/2018/12/3765912028.jpg" title="Click to view original image">
    <img src="https://kexue.fm/usr/uploads/2018/12/3765912028.jpg" alt="BiGAN-QP Randomly generated images" />
  </a>
</p>
<p align="center">BiGAN-QP randomly generated images</p>

<p>Reconstruction results (left is original, right is reconstruction):</p>

<p>
  <a href="https://kexue.fm/usr/uploads/2018/12/1145811359.jpg" title="Click to view original image">
    <img src="https://kexue.fm/usr/uploads/2018/12/1145811359.jpg" alt="BiGAN-QP Reconstruction results 2" />
  </a>
</p>
<p align="center">BiGAN-QP reconstruction effect chart 2</p>

<p>As can be seen, whether it is random generation or reconstruction, the effects are satisfying and no blurriness occurs. This indicates that we have indeed successfully trained a GAN model that possesses both encoding and generation capabilities simultaneously.</p>

<p>And an important feature is: because it is dimensionality reduction reconstruction, the model did not (and cannot) learn a one-to-one pixel-wise mapping, but rather a clear reconstruction result that looks roughly the same overall. For example, looking at the first image in the first row and the second image in the last row, the model basically reconstructs the person, but what's interesting are the glasses. We find that the model indeed reconstructed the glasses but changed them to another "style" of glasses. We can even think that the model has learned the concept of "glasses," but because it is dimensionality reduction reconstruction and the representation capacity of the latent variable is limited, although the model knows those are glasses, it cannot reconstruct exactly the same glasses, so it simply swaps in a common pair of glasses.</p>

<p>This is something that the "point-by-point one-to-one reconstruction" required by ordinary VAEs cannot achieve, and "point-by-point one-to-one reconstruction" is also the main reason for VAE blurriness. If you want completely reversible reconstruction, only reversible models like <a href="translation_5807.html">Glow</a> are likely to achieve it.</p>

<p>Additionally, having both an encoder and a generator allows us to play with latent variable interpolation for real images:</p>

<p>
  <a href="https://kexue.fm/usr/uploads/2018/12/656059227.jpg" title="Click to view original image">
    <img src="https://kexue.fm/usr/uploads/2018/12/656059227.jpg" alt="BiGAN-QP real image interpolation" />
  </a>
</p>
<p align="center">BiGAN-QP real image interpolation (far left and far right are real images, second from left and second from right are reconstruction images, the rest are interpolated images)</p>

<p>We can also look at similar images in the eyes of BiGAN-QP (calculate the latent variables for all real images, then calculate similarity using Euclidean distance or cosine value to find the most similar ones; the figure below shows the results using Euclidean distance):</p>

<p>
  <a href="https://kexue.fm/usr/uploads/2018/12/3300462880.jpg" title="Click to view original image">
    <img src="https://kexue.fm/usr/uploads/2018/12/3300462880.jpg" alt="BiGAN-QP Similarity" />
  </a>
</p>
<p align="center">Similarity in the eyes of BiGAN-QP (the first row is the input, the next two rows are similar images)</p>

<h2>Welcome to Use and Share</h2>

<p>As mentioned earlier, GAN-QP is a theoretically complete adversarial framework. In theory, all GAN tasks can be attempted. Therefore, if you currently have a GAN task at hand, why not give it a try? You can then remove the L-constraint, spectral normalization, and even many regularization terms without worrying about vanishing gradients. GAN-QP is the result of my efforts to remove various hyperparameters from GANs.</p>

<p>If you have new application results based on GAN-QP, you are welcome to share them here.</p>
<hr>
<footer style="margin-top: 3em; padding: 1.5em; background: #f5f5f5; border-radius: 8px; font-size: 0.9em; color: #555;">
    <p style="margin: 0 0 0.5em 0;"><strong>Citation</strong></p>
    <p style="margin: 0 0 0.5em 0;">
        This is a machine translation of the original Chinese article:<br>
        <a href="https://kexue.fm/archives/6214" style="color: #005fcc;">https://kexue.fm/archives/6214</a>
    </p>
    <p style="margin: 0 0 0.5em 0;">
        Original author: 苏剑林 (Su Jianlin)<br>
        Original publication: <a href="https://kexue.fm" style="color: #005fcc;">科学空间 (Scientific Spaces)</a>
    </p>
    <p style="margin: 0; font-style: italic;">
        Translated using Gemini 3 Flash. Please refer to the original for authoritative content.
    </p>
</footer>
