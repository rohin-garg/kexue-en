
    <style type="text/css">

    body {
    margin: 48px auto;
    max-width: 68ch;              /* character-based width reads better */
    padding: 0 16px;

    font-family: -apple-system, BlinkMacSystemFont, "Segoe UI",
                Roboto, "Helvetica Neue", Arial, sans-serif;
    font-size: 18px;
    line-height: 1.65;
    color: #333;
    background: #fafafa;
    }

    h1, h2, h3, h4 {
    line-height: 1.25;
    margin-top: 2.2em;
    margin-bottom: 0.6em;
    font-weight: 600;
    }

    h1 {
    font-size: 2.1em;
    margin-top: 0;
    }

    h2 {
    font-size: 1.6em;
    border-bottom: 1px solid #e5e5e5;
    padding-bottom: 0.3em;
    }

    h3 {
    font-size: 1.25em;
    }

    h4 {
    font-size: 1.05em;
    color: #555;
    }

    /* Paragraphs and lists */
    p {
    margin: 1em 0;
    }

    ul, ol {
    margin: 1em 0 1em 1.5em;
    }

    li {
    margin: 0.4em 0;
    }

    a {
    color: #005fcc;
    text-decoration: none;
    }

    a:hover {
    text-decoration: underline;
    }

    code {
    font-family: ui-monospace, SFMono-Regular, Menlo, Consolas, monospace;
    font-size: 0.95em;
    background: #f2f2f2;
    padding: 0.15em 0.35em;
    border-radius: 4px;
    }

    pre {
    font-family: ui-monospace, SFMono-Regular, Menlo, Consolas, monospace;
    font-size: 0.9em;
    background: #f5f5f5;
    padding: 1em 1.2em;
    overflow-x: auto;
    border-radius: 6px;
    line-height: 1.45;
    }

    pre code {
    background: none;
    padding: 0;
    }

    blockquote {
    margin: 1.5em 0;
    padding-left: 1em;
    border-left: 4px solid #ddd;
    color: #555;
    }

    hr {
    border: none;
    border-top: 1px solid #e0e0e0;
    margin: 3em 0;
    }

    table {
    border-collapse: collapse;
    margin: 1.5em 0;
    width: 100%;
    font-size: 0.95em;
    }

    th, td {
    padding: 0.5em 0.7em;
    border-bottom: 1px solid #e5e5e5;
    text-align: left;
    }

    th {
    font-weight: 600;
    }

    img {
    max-width: 100%;
    display: block;
    margin: 1.5em auto;
    }

    small {
    color: #666;
    }

    mjx-container {
    margin: 1em 0;
    }

    ::selection {
    background: #cce2ff;
    }

    </style>
    

<script>
window.MathJax = {
  tex: {
    inlineMath: [['$', '$'], ['\\(', '\\)']],
    displayMath: [['$$', '$$'], ['\\[', '\\]']],
    processEscapes: true,
    tags: 'ams'
  }
};
</script>
<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"></script>

<article>
    <h1><a href="https://kexue.fm/archives/9070">A Few Inequalities for the logsumexp Operation</a></h1>
    <p>By 苏剑林 | May 10, 2022</p>

    <p>$\text{logsumexp}$ is an operation frequently encountered in machine learning, especially in the implementation and derivation of cross-entropy. At the same time, it is a smooth approximation of the $\max$ function (refer to <a href="translation_3290.html">"Seeking a Smooth Maximum Function"</a>). Let $x=(x_1, x_2, \cdots, x_n)$; $\text{logsumexp}$ is defined as:</p>

    \begin{equation}\text{logsumexp}(x) = \log \sum_{i=1}^n e^{x_i}\end{equation}

    <p>This article introduces several inequalities related to $\text{logsumexp}$ that may be useful in theoretical derivations.</p>

    <h2>Basic Bounds</h2>
    <p>Let $x_{\max} = \max(x_1, x_2, \cdots, x_n)$. Then it is obvious that:</p>
    \begin{equation}e^{x_{\max}} < \sum_{i=1}^n e^{x_i} \leq \sum_{i=1}^n e^{x_{\max}} = ne^{x_{\max}}\end{equation}
    <p>Taking the logarithm of each part gives:</p>
    \begin{equation}x_{\max} < \text{logsumexp}(x) \leq x_{\max} + \log n\end{equation}
    <p>This is the most basic result regarding the upper and lower bounds of $\text{logsumexp}$. It indicates that the approximation error of $\text{logsumexp}$ relative to $\max$ does not exceed $\log n$. Note that this error is independent of $x$ itself. Thus, we have:</p>
    \begin{equation}x_{\max}/\tau < \text{logsumexp}(x/\tau) \leq x_{\max}/\tau + \log n\end{equation}
    <p>Multiplying each part by $\tau$ gives:</p>
    \begin{equation}x_{\max} < \tau\text{logsumexp}(x/\tau) \leq x_{\max} + \tau\log n\end{equation}
    <p>As $\tau \to 0$, the error tends to 0. This tells us that we can improve the degree of approximation to $\max$ by reducing the temperature parameter $\tau$.</p>

    <h2>Average Bounds</h2>
    <p>We know that $e^x$ is a convex function, satisfying <a href="https://en.wikipedia.org/wiki/Jensen%27s_inequality">Jensen's Inequality</a> $\mathbb{E}[e^x] \geq e^{\mathbb{E}[x]}$. Therefore:</p>
    \begin{equation}\frac{1}{n}\sum_{i=1}^n e^{x_i} \geq e^{\bar{x}}\end{equation}
    <p>Here $\bar{x} = \frac{1}{n}\sum_{i=1}^n x_i$. Multiplying both sides by $n$ and taking the logarithm yields:</p>
    \begin{equation}\text{logsumexp}(x) \geq \bar{x} + \log n\end{equation}
    <p>This is another result regarding the lower bound of $\text{logsumexp}$. This result can be further generalized to the case of weighted averages: let $p_1, p_2, \cdots, p_n \geq 0$ and $\sum_{i=1}^n p_i = 1$. From the Cauchy-Schwarz inequality, we have:</p>
    \begin{equation}\left[\sum_{i=1}^n (e^{x_i/2})^2\right]\left[\sum_{i=1}^n p_i^2\right] \geq \left[\sum_{i=1}^n p_i e^{x_i/2}\right]^2\end{equation}
    <p>Applying Jensen's inequality to the expression inside the brackets on the right side gives:</p>
    \begin{equation}\left[\sum_{i=1}^n p_i e^{x_i/2}\right]^2 \geq \left[e^{\left(\sum_{i=1}^n p_i x_i/2\right)}\right]^2 = e^{\left(\sum_{i=1}^n p_i x_i\right)}\end{equation}
    <p>Taking the logarithm of both sides and rearranging, we get:</p>
    \begin{equation}\text{logsumexp}(x) \geq \sum_{i=1}^n p_i x_i - \log \sum_{i=1}^n p_i^2\end{equation}
    <p>If we use the more general <a href="https://en.wikipedia.org/wiki/H%C3%B6lder%27s_inequality">Hölder's Inequality</a> instead of Cauchy-Schwarz from the beginning, we can also obtain:</p>
    \begin{equation}\text{logsumexp}(x) \geq \sum_{i=1}^n p_i x_i - \frac{1}{t-1}\log\sum_{i=1}^n p_i^t, \quad \forall t > 1\end{equation}
    <p>In particular, taking the limit $t \to 1$, we can obtain:</p>
    \begin{equation}\text{logsumexp}(x) \geq \sum_{i=1}^n p_i x_i - \sum_{i=1}^n p_i \log p_i\end{equation}
    <p>This can be equivalently rewritten as $\sum_{i=1}^n p_i \log \frac{p_i}{e^{x_i}/Z} \geq 0$, where $Z=e^{\text{logsumexp}(x)}$ is the normalization factor. So, it is actually the KL divergence between two distributions.</p>

    <h2>Lipschitz Constraint</h2>
    <p>Under the infinity norm, $\text{logsumexp}$ also satisfies a Lipschitz constraint, namely:</p>
    \begin{equation}\|\text{logsumexp}(x) - \text{logsumexp}(y)\| \leq \|x - y\|_{\infty}\end{equation}
    <p>where $\|x - y\|_{\infty} = \max_i |x_i - y_i|$ (actually, writing it as $\|x - y\|_{\max}$ would be more intuitive). The proof is not difficult. Define:</p>
    \begin{equation}f(t) = \text{logsumexp}(tx + (1-t)y), \quad t \in [0, 1]\end{equation}
    <p>Considering it as a univariate function of $t$, by the <a href="https://en.wikipedia.org/wiki/Mean_value_theorem">Mean Value Theorem</a>, there exists $\varepsilon \in (0, 1)$ such that:</p>
    \begin{equation}f'(\varepsilon) = \frac{f(1) - f(0)}{1 - 0} = \text{logsumexp}(x) - \text{logsumexp}(y)\end{equation}
    <p>It is not hard to find that:</p>
    \begin{equation}f'(\varepsilon) = \frac{\sum_{i=1}^n e^{\varepsilon x_i + (1-\varepsilon)y_i}(x_i - y_i)}{\sum_{i=1}^n e^{\varepsilon x_i + (1-\varepsilon)y_i}}\end{equation}
    <p>Therefore:</p>
    \begin{equation}\begin{aligned} &\|\text{logsumexp}(x) - \text{logsumexp}(y)\| = \left|\frac{\sum_{i=1}^n e^{\varepsilon x_i + (1-\varepsilon)y_i}(x_i - y_i)}{\sum_{i=1}^n e^{\varepsilon x_i + (1-\varepsilon)y_i}}\right| \\ \leq & \frac{\sum_{i=1}^n e^{\varepsilon x_i + (1-\varepsilon)y_i} |x_i - y_i|}{\sum_{i=1}^n e^{\varepsilon x_i + (1-\varepsilon)y_i}} \leq \frac{\sum_{i=1}^n e^{\varepsilon x_i + (1-\varepsilon)y_i} \|x - y\|_{\infty}}{\sum_{i=1}^n e^{\varepsilon x_i + (1-\varepsilon)y_i}} = \|x - y\|_{\infty} \end{aligned}\end{equation}

    <h2>Convex Function</h2>
    <p>Finally, a very strong conclusion: $\text{logsumexp}$ is also a convex function! This means that all inequalities related to convex functions apply to $\text{logsumexp}$, such as the most basic Jensen's Inequality:</p>
    \begin{equation}\mathbb{E}[\text{logsumexp}(x)] \geq \text{logsumexp}(\mathbb{E}[x])\end{equation}

    <p>To prove that $\text{logsumexp}$ is a convex function, we need to show that for any $t \in [0, 1]$, the following holds:</p>
    \begin{equation}t\text{logsumexp}(x) + (1-t)\text{logsumexp}(y) \geq \text{logsumexp}(tx + (1-t)y)\end{equation}
    <p>The proof process is essentially a basic application of <a href="https://en.wikipedia.org/wiki/H%C3%B6lder%27s_inequality">Hölder's Inequality</a>. Specifically, we have:</p>
    \begin{equation}t\text{logsumexp}(x) + (1-t)\text{logsumexp}(y) = \log \left(\sum_{i=1}^n e^{x_i}\right)^t \left(\sum_{i=1}^n e^{y_i}\right)^{(1-t)}\end{equation}
    <p>Applying Hölder's Inequality directly, we obtain:</p>
    \begin{equation}\log \left(\sum_{i=1}^n e^{x_i}\right)^t \left(\sum_{i=1}^n e^{y_i}\right)^{(1-t)} \geq \log \sum_{i=1}^n e^{tx_i + (1-t)y_i} = \text{logsumexp}(tx + (1-t)y)\end{equation}
    <p>This proves that $\text{logsumexp}$ is a convex function.</p>

    <h2>Summary</h2>
    <p>This article summarized several inequalities related to the $\text{logsumexp}$ operation for future reference.</p>
</article>
<hr>
<footer style="margin-top: 3em; padding: 1.5em; background: #f5f5f5; border-radius: 8px; font-size: 0.9em; color: #555;">
    <p style="margin: 0 0 0.5em 0;"><strong>Citation</strong></p>
    <p style="margin: 0 0 0.5em 0;">
        This is a machine translation of the original Chinese article:<br>
        <a href="https://kexue.fm/archives/9070" style="color: #005fcc;">https://kexue.fm/archives/9070</a>
    </p>
    <p style="margin: 0 0 0.5em 0;">
        Original author: 苏剑林 (Su Jianlin)<br>
        Original publication: <a href="https://kexue.fm" style="color: #005fcc;">科学空间 (Scientific Spaces)</a>
    </p>
    <p style="margin: 0; font-style: italic;">
        Translated using Gemini 3 Flash. Please refer to the original for authoritative content.
    </p>
</footer>
