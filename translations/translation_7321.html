
    <style type="text/css">

    body {
    margin: 48px auto;
    max-width: 68ch;              /* character-based width reads better */
    padding: 0 16px;

    font-family: -apple-system, BlinkMacSystemFont, "Segoe UI",
                Roboto, "Helvetica Neue", Arial, sans-serif;
    font-size: 18px;
    line-height: 1.65;
    color: #333;
    background: #fafafa;
    }

    h1, h2, h3, h4 {
    line-height: 1.25;
    margin-top: 2.2em;
    margin-bottom: 0.6em;
    font-weight: 600;
    }

    h1 {
    font-size: 2.1em;
    margin-top: 0;
    }

    h2 {
    font-size: 1.6em;
    border-bottom: 1px solid #e5e5e5;
    padding-bottom: 0.3em;
    }

    h3 {
    font-size: 1.25em;
    }

    h4 {
    font-size: 1.05em;
    color: #555;
    }

    /* Paragraphs and lists */
    p {
    margin: 1em 0;
    }

    ul, ol {
    margin: 1em 0 1em 1.5em;
    }

    li {
    margin: 0.4em 0;
    }

    a {
    color: #005fcc;
    text-decoration: none;
    }

    a:hover {
    text-decoration: underline;
    }

    code {
    font-family: ui-monospace, SFMono-Regular, Menlo, Consolas, monospace;
    font-size: 0.95em;
    background: #f2f2f2;
    padding: 0.15em 0.35em;
    border-radius: 4px;
    }

    pre {
    font-family: ui-monospace, SFMono-Regular, Menlo, Consolas, monospace;
    font-size: 0.9em;
    background: #f5f5f5;
    padding: 1em 1.2em;
    overflow-x: auto;
    border-radius: 6px;
    line-height: 1.45;
    }

    pre code {
    background: none;
    padding: 0;
    }

    blockquote {
    margin: 1.5em 0;
    padding-left: 1em;
    border-left: 4px solid #ddd;
    color: #555;
    }

    hr {
    border: none;
    border-top: 1px solid #e0e0e0;
    margin: 3em 0;
    }

    table {
    border-collapse: collapse;
    margin: 1.5em 0;
    width: 100%;
    font-size: 0.95em;
    }

    th, td {
    padding: 0.5em 0.7em;
    border-bottom: 1px solid #e5e5e5;
    text-align: left;
    }

    th {
    font-weight: 600;
    }

    img {
    max-width: 100%;
    display: block;
    margin: 1.5em auto;
    }

    small {
    color: #666;
    }

    mjx-container {
    margin: 1em 0;
    }

    ::selection {
    background: #cce2ff;
    }

    </style>
    

<script>
window.MathJax = {
  tex: {
    inlineMath: [['$', '$'], ['\\(', '\\)']],
    displayMath: [['$$', '$$'], ['\\[', '\\]']],
    tags: 'ams',
    packages: {'[+]': ['ams']}
  },
  options: {
    renderActions: {
      findScript: [10, function (doc) {
        for (const node of document.querySelectorAll('script[type^="math/tex"]')) {
          const display = !!node.type.match(/; *mode=display/);
          const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
          const text = document.createTextNode('');
          node.parentNode.replaceChild(text, node);
          math.start = {node: text, delim: '', n: 0};
          math.end = {node: text, delim: '', n: 0};
          doc.math.push(math);
        }
      }, '']
    }
  }
};
</script>
<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"></script>

<h1><a href="https://kexue.fm/archives/7321">bert4keras in hand, I have the baseline: Baidu LIC2020</a></h1>

<p>By 苏剑林 | April 02, 2020</p>

<p>Baidu's "<a href="http://lic2020.cipsc.org.cn/">2020 Language and Intelligence Challenge</a>" has begun. This year features five tracks: Machine Reading Comprehension, Recommendation-based Dialogue, Semantic Parsing, Relation Extraction, and Event Extraction. For each track, the organizers have provided baseline models based on PaddlePaddle. Here, I am providing my personal baselines for three of these tracks based on <a href="https://github.com/bojone/bert4keras">bert4keras</a>. From these, we can see how quick, convenient, and concise it is to build baseline models using bert4keras.</p>

<blockquote>
  <p><strong>Address:</strong> <a href="https://github.com/bojone/lic2020_baselines">https://github.com/bojone/lic2020_baselines</a></p>
</blockquote>

<h2>Brief Analysis of Ideas</h2>

<p>Here is a brief analysis of the task characteristics of these three tracks and the design of the corresponding baselines.</p>

<h3>Reading Comprehension</h3>

<p>Sample example:</p>

<pre><code class="python"></code></pre>

<p>There isn't much to say about this baseline; it essentially follows BERT with two Dense layers + Softmax to predict the start and end of the answer respectively. Some training samples are labeled with multiple answers, but since only one answer needs to be predicted during inference, one answer is randomly selected for training during each step of the training phase.</p>

<h3>Relation Extraction</h3>

<p>Sample example:</p>

<pre><code class="python"></code></pre>

<p>Relation Extraction is essentially last year's triplet extraction task, but it has undergone some upgrades this year. The upgrade lies in considering the polysemy of the same <code>predicate</code>. For example, "stars in" might refer to which TV series someone starred in, or it might refer to which role they played in a TV series. If the same sentence contains multiple different objects (<code>object</code>) being "starred in," all of them must be extracted to be considered correct. While it's called an upgrade, there is no fundamental change; we simply need to concatenate the <code>predicate</code> with its <code>object</code> prefix to treat them as different predicates, which reduces the problem to a conventional triplet extraction task. for instance, "stars_in_@value" and "stars_in_inWork" are treated as two different predicates to be extracted separately. My baseline model is still based on last year's "semi-pointer, semi-labeling" design. For details, please refer to <a href="translation_6671.html">"A Lightweight Information Extraction Model based on DGCNN and Probabilistic Graphs"</a>.</p>

<h3>Event Extraction</h3>

<p>Sample example:</p>

<pre><code class="python"></code></pre>

<p>Event Extraction is a relatively new task that involves extracting event types and the elements describing that event. A single sentence may contain multiple events, and a single entity can simultaneously describe multiple events (for example, "Month XX, Day XX" could be the occurrence time for several events). Event extraction itself is a complex task, but for this competition, the organizers only evaluate triplets composed of <code>(event_type, role, argument)</code>—meaning if such a triplet matches, 1 point is awarded. Since <code>event_type</code> and <code>role</code> are discrete categories and <code>argument</code> is an entity from the original text, the official evaluation metric effectively reduces this task to a standard entity labeling problem. Consequently, it can be solved using conventional sequence labeling models. Both my baseline and the official baseline are presented as sequence labeling tasks.</p>

<h2>Matching the Original Sequence</h2>

<p>The three competitions mentioned above are essentially extraction problems, meaning the output entities are fragments of the original text. However, after the original text passes through the BERT tokenizer, it may not perfectly align with the original text due to the possibility of minor "additions," "deletions," or "modifications" (e.g., lowercase conversion, changes in space counts, or character transliteration). While these minor changes are negligible for engineering evaluations, they are crucial for competitions or academic evaluations because even if characters look the same, if they are different, it results in a matching error. For example, readers can try running the following code in Python:</p>

<pre><code class="python"></code></pre>

<p>In order to map the tokenized results back to the original sequence, I spent some time adding a <code>rematch</code> method to bert4keras's <code>Tokenizer</code>. By passing the original text and the tokenized results, it returns the mapping relationship from tokens to the original text. With this mapping, you can slice directly from the original text. For the specific implementation, please refer directly to the baseline code.</p>

<h2>Summary</h2>

<p>Writing three baselines and churning out another blog post~</p>
<hr>
<footer style="margin-top: 3em; padding: 1.5em; background: #f5f5f5; border-radius: 8px; font-size: 0.9em; color: #555;">
    <p style="margin: 0 0 0.5em 0;"><strong>Citation</strong></p>
    <p style="margin: 0 0 0.5em 0;">
        This is a machine translation of the original Chinese article:<br>
        <a href="translation_7321.html" style="color: #005fcc;">https://kexue.fm/archives/7321</a>
    </p>
    <p style="margin: 0 0 0.5em 0;">
        Original author: 苏剑林 (Su Jianlin)<br>
        Original publication: <a href="https://kexue.fm" style="color: #005fcc;">科学空间 (Scientific Spaces)</a>
    </p>
    <p style="margin: 0; font-style: italic;">
        Translated using Gemini 3 Flash. Please refer to the original for authoritative content.
    </p>
</footer>
