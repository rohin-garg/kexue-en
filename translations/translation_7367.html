
    <style type="text/css">

    body {
    margin: 48px auto;
    max-width: 68ch;              /* character-based width reads better */
    padding: 0 16px;

    font-family: -apple-system, BlinkMacSystemFont, "Segoe UI",
                Roboto, "Helvetica Neue", Arial, sans-serif;
    font-size: 18px;
    line-height: 1.65;
    color: #333;
    background: #fafafa;
    }

    h1, h2, h3, h4 {
    line-height: 1.25;
    margin-top: 2.2em;
    margin-bottom: 0.6em;
    font-weight: 600;
    }

    h1 {
    font-size: 2.1em;
    margin-top: 0;
    }

    h2 {
    font-size: 1.6em;
    border-bottom: 1px solid #e5e5e5;
    padding-bottom: 0.3em;
    }

    h3 {
    font-size: 1.25em;
    }

    h4 {
    font-size: 1.05em;
    color: #555;
    }

    /* Paragraphs and lists */
    p {
    margin: 1em 0;
    }

    ul, ol {
    margin: 1em 0 1em 1.5em;
    }

    li {
    margin: 0.4em 0;
    }

    a {
    color: #005fcc;
    text-decoration: none;
    }

    a:hover {
    text-decoration: underline;
    }

    code {
    font-family: ui-monospace, SFMono-Regular, Menlo, Consolas, monospace;
    font-size: 0.95em;
    background: #f2f2f2;
    padding: 0.15em 0.35em;
    border-radius: 4px;
    }

    pre {
    font-family: ui-monospace, SFMono-Regular, Menlo, Consolas, monospace;
    font-size: 0.9em;
    background: #f5f5f5;
    padding: 1em 1.2em;
    overflow-x: auto;
    border-radius: 6px;
    line-height: 1.45;
    }

    pre code {
    background: none;
    padding: 0;
    }

    blockquote {
    margin: 1.5em 0;
    padding-left: 1em;
    border-left: 4px solid #ddd;
    color: #555;
    }

    hr {
    border: none;
    border-top: 1px solid #e0e0e0;
    margin: 3em 0;
    }

    table {
    border-collapse: collapse;
    margin: 1.5em 0;
    width: 100%;
    font-size: 0.95em;
    }

    th, td {
    padding: 0.5em 0.7em;
    border-bottom: 1px solid #e5e5e5;
    text-align: left;
    }

    th {
    font-weight: 600;
    }

    img {
    max-width: 100%;
    display: block;
    margin: 1.5em auto;
    }

    small {
    color: #666;
    }

    mjx-container {
    margin: 1em 0;
    }

    ::selection {
    background: #cce2ff;
    }

    </style>
    

<script>
  window.MathJax = {
    tex: {
      inlineMath: [['$', '$'], ['\\(', '\\)']],
      displayMath: [['$$', '$$'], ['\\[', '\\]']],
      tags: 'ams',
      packages: {'[+]': ['ams']}
    },
    options: {
      renderActions: {
        findScript: [10, function (doc) {
          for (const node of document.querySelectorAll('script[type^="math/tex"]')) {
            const display = !!node.type.match(/; *mode=display/);
            const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
            const text = document.createTextNode('');
            node.parentNode.replaceChild(text, node);
            math.start = {node: text, delim: '', n: 0};
            math.end = {node: text, delim: '', n: 0};
            doc.math.push(math);
          }
        }, '']
      }
    }
  };
</script>
<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"></script>

<article>
    <h1><a href="https://kexue.fm/archives/7367">The Memory-Saving Recomputation Technique Now Has a Keras Version</a></h1>
    <p>By 苏剑林 | April 29, 2020</p>

    <p>Many readers recently might have noticed the official account article <a href="https://mp.weixin.qq.com/s/CmIVwGFqrSD0wcSN_hgH1A">"BERT Recomputation: Saving 5x memory overhead with 22.5% training time (including code)"</a>. It introduced a technique called "recomputation" (gradient checkpointing), which is essentially a method to save memory. It allows for a several-fold increase in <code>batch_size</code> at the cost of a slightly slower average training speed. This technique was first proposed in the paper <a href="https://papers.cool/arxiv/1604.06174">"Training Deep Nets with Sublinear Memory Cost"</a> back in 2016, though it doesn't seem to have become particularly popular until now.</p>

    <h2 id="Exploration">Exploration</h2>
    <p>The aforementioned article mentioned that this technique has native implementations in PyTorch and PaddlePaddle, but not yet in TensorFlow. However, in reality, since TensorFlow 1.8, TensorFlow has included this functionality. At that time, it was placed in the <code>tf.contrib</code> sub-library. Starting from TensorFlow 1.15, it became a built-in core function of TensorFlow: <code>tf.recompute_grad</code>.</p>

    <p>After finding <code>tf.recompute_grad</code>, I looked into its usage. After some tinkering, I actually succeeded in using it, successfully increasing the <code>batch_size</code> from 48 to 144! However, while organizing and testing further, I discovered that this thing is actually ineffective in TensorFlow 2.x... So I spent another two days searching through various materials and debugging repeatedly. Finally, I managed to fill this gap.</p>

    <p>Here is my open-source implementation:</p>
    <blockquote>
        <strong>GitHub Address: <a href="https://github.com/bojone/keras_recompute">https://github.com/bojone/keras_recompute</a></strong>
    </blockquote>
    <p>This implementation is already built into <a href="https://github.com/bojone/bert4keras">bert4keras</a>. Readers using bert4keras can upgrade to the latest version (0.7.5+) to test this feature.</p>

    <h2 id="Usage">Usage</h2>
    <p>My implementation is also named <code>recompute_grad</code>. It is a decorator used for customizing the <code>call</code> function of a Keras layer, for example:</p>

<pre><code>from recompute import recompute_grad

class MyLayer(Layer):
    @recompute_grad
    def call(self, inputs):
        return inputs * 2
</code></pre>

    <p>For existing layers, you can decorate them through inheritance:</p>

<pre><code>from recompute import recompute_grad

class MyDense(Dense):
    @recompute_grad
    def call(self, inputs):
        return super(MyDense, self).call(inputs)
</code></pre>

    <p>After customizing the layers, embed them into your code and set the environment variable <code>RECOMPUTE=1</code> before running the code to enable recomputation.</p>

    <p>Note: Simply inserting <code>@recompute_grad</code> into the overall model will not achieve the goal of saving memory. Instead, you need to insert <code>@recompute_grad</code> into each individual layer to better save memory. Put simply, the more <code>@recompute_grad</code> decorations you insert, the more memory you save. Please carefully understand the principles of recomputation for the specific reasons.</p>

    <h2 id="Effect">Effect</h2>
    <p>bert4keras 0.7.5+ has built-in recomputation, which is enabled by passing the environment variable <code>RECOMPUTE=1</code>. Readers can try it themselves; the approximate effects are:</p>
    <ol>
        <li>With the BERT Base version, the <code>batch_size</code> can be increased to about 3 times the original;</li>
        <li>With the BERT Large version, the <code>batch_size</code> can be increased to about 4 times the original;</li>
        <li>The average training time per sample increases by approximately 25%;</li>
        <li>Theoretically, the more layers there are, the larger the multiplier for increasing <code>batch_size</code>.</li>
    </ol>

    <h2 id="Environment">Environment</h2>
    <p>Tested and passed in the following environments:</p>
    <ul>
        <li>tensorflow 1.14 + keras 2.3.1</li>
        <li>tensorflow 1.15 + keras 2.3.1</li>
        <li>tensorflow 2.0 + keras 2.3.1</li>
        <li>tensorflow 2.1 + keras 2.3.1</li>
        <li>tensorflow 2.0 + built-in tf.keras</li>
        <li>tensorflow 2.1 + built-in tf.keras</li>
    </ul>

    <p>Confirmed unsupported environments:</p>
    <ul>
        <li>tensorflow 1.x + built-in tf.keras</li>
    </ul>
    <p>More test results are welcome.</p>

    <p>By the way, <strong>it is strongly recommended to use Keras 2.3.1 in conjunction with TensorFlow 1.x/2.x; it is strongly discouraged to use the <code>tf.keras</code> bundled with TensorFlow 2.x.</strong></p>

    <h2 id="References">References</h2>
    <p>Finally, my implementation mainly refers to the following two source codes. I would like to express my gratitude.</p>
    <ul>
        <li><a href="https://github.com/davisyoshida/tf2-gradient-checkpointing">https://github.com/davisyoshida/tf2-gradient-checkpointing</a></li>
        <li><a href="https://github.com/tensorflow/tensorflow/blob/v2.1.0/tensorflow/python/ops/custom_gradient.py#L454-L499">https://github.com/tensorflow/tensorflow/blob/v2.1.0/tensorflow/python/ops/custom_gradient.py#L454-L499</a></li>
    </ul>

    <p><em><strong>Reprinting notice: Please include the source address of this article: <a href="translation_7367.html">https://kexue.fm/archives/7367</a></strong></em></p>

    <p>Su Jianlin. (Apr. 29, 2020). "The Memory-Saving Recomputation Technique Now Has a Keras Version" [Blog post]. Retrieved from <a href="translation_7367.html">https://kexue.fm/archives/7367</a></p>

</article>
<hr>
<footer style="margin-top: 3em; padding: 1.5em; background: #f5f5f5; border-radius: 8px; font-size: 0.9em; color: #555;">
    <p style="margin: 0 0 0.5em 0;"><strong>Citation</strong></p>
    <p style="margin: 0 0 0.5em 0;">
        This is a machine translation of the original Chinese article:<br>
        <a href="translation_7367.html" style="color: #005fcc;">https://kexue.fm/archives/7367</a>
    </p>
    <p style="margin: 0 0 0.5em 0;">
        Original author: 苏剑林 (Su Jianlin)<br>
        Original publication: <a href="https://kexue.fm" style="color: #005fcc;">科学空间 (Scientific Spaces)</a>
    </p>
    <p style="margin: 0; font-style: italic;">
        Translated using Gemini 3 Flash. Please refer to the original for authoritative content.
    </p>
</footer>
