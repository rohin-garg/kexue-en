
    <style type="text/css">

    body {
    margin: 48px auto;
    max-width: 68ch;              /* character-based width reads better */
    padding: 0 16px;

    font-family: -apple-system, BlinkMacSystemFont, "Segoe UI",
                Roboto, "Helvetica Neue", Arial, sans-serif;
    font-size: 18px;
    line-height: 1.65;
    color: #333;
    background: #fafafa;
    }

    h1, h2, h3, h4 {
    line-height: 1.25;
    margin-top: 2.2em;
    margin-bottom: 0.6em;
    font-weight: 600;
    }

    h1 {
    font-size: 2.1em;
    margin-top: 0;
    }

    h2 {
    font-size: 1.6em;
    border-bottom: 1px solid #e5e5e5;
    padding-bottom: 0.3em;
    }

    h3 {
    font-size: 1.25em;
    }

    h4 {
    font-size: 1.05em;
    color: #555;
    }

    /* Paragraphs and lists */
    p {
    margin: 1em 0;
    }

    ul, ol {
    margin: 1em 0 1em 1.5em;
    }

    li {
    margin: 0.4em 0;
    }

    a {
    color: #005fcc;
    text-decoration: none;
    }

    a:hover {
    text-decoration: underline;
    }

    code {
    font-family: ui-monospace, SFMono-Regular, Menlo, Consolas, monospace;
    font-size: 0.95em;
    background: #f2f2f2;
    padding: 0.15em 0.35em;
    border-radius: 4px;
    }

    pre {
    font-family: ui-monospace, SFMono-Regular, Menlo, Consolas, monospace;
    font-size: 0.9em;
    background: #f5f5f5;
    padding: 1em 1.2em;
    overflow-x: auto;
    border-radius: 6px;
    line-height: 1.45;
    }

    pre code {
    background: none;
    padding: 0;
    }

    blockquote {
    margin: 1.5em 0;
    padding-left: 1em;
    border-left: 4px solid #ddd;
    color: #555;
    }

    hr {
    border: none;
    border-top: 1px solid #e0e0e0;
    margin: 3em 0;
    }

    table {
    border-collapse: collapse;
    margin: 1.5em 0;
    width: 100%;
    font-size: 0.95em;
    }

    th, td {
    padding: 0.5em 0.7em;
    border-bottom: 1px solid #e5e5e5;
    text-align: left;
    }

    th {
    font-weight: 600;
    }

    img {
    max-width: 100%;
    display: block;
    margin: 1.5em auto;
    }

    small {
    color: #666;
    }

    mjx-container {
    margin: 1em 0;
    }

    ::selection {
    background: #cce2ff;
    }

    </style>
    

<script>
window.MathJax = {
  tex: {
    inlineMath: [['$', '$'], ['\\(', '\\)']],
    displayMath: [['$$', '$$'], ['\\[', '\\]']],
    tags: 'ams',
    packages: {'[+]': ['ams']}
  }
};
</script>
<script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"></script>

<article>
    <h1><a href="https://kexue.fm/archives/3360">Text Sentiment Classification (I): Traditional Models</a></h1>
    <p>By 苏剑林 | June 22, 2015</p>

    <p><strong>Foreword:</strong> Back in April and May, I participated in two data mining competitions: the "Liangjian Cup" organized by the School of Physics and Telecommunication Engineering, and the 3rd "Teddy Cup" National Undergraduate Data Mining Competition. Coincidentally, in both competitions, one problem primarily involved Chinese sentiment classification work. When doing the "Liangjian Cup," as I was a novice with limited skills, I only implemented a simple text sentiment classification model based on traditional ideas. In the subsequent "Teddy Cup," through deeper study, I had basic knowledge of deep learning concepts and implemented a text sentiment classification model using deep learning algorithms. Therefore, I plan to post both models on my blog for readers' reference. Beginners can use them to compare the differences between the two and understand the relevant approaches. Experts are welcome to pass by with a smile.</p>

    <h3 id="Based-on-Sentiment-Dictionary">Based on Sentiment Dictionary</h3>
    <p>
        <a href="https://kexue.fm/usr/uploads/2015/06/3473700595.png" title="Click to view original image">
            <img src="https://kexue.fm/usr/uploads/2015/06/3473700595.png" alt="Simple human judgment thinking" />
        </a>
    </p>
    <center>Simple human judgment thinking</center>

    <p>Traditional text sentiment classification based on sentiment dictionaries is the simplest simulation of human memory and judgment thinking, as shown in the figure above. First, we learn and memorize some basic vocabulary, such as negation words like "not," positive words like "like" and "love," and negative words like "dislike" and "hate," thereby forming a basic corpus in our minds. Then, we perform a direct breakdown of the input sentence to see if the words memorized in our vocabulary list exist, and then judge the sentiment based on the category of these words. For instance, in "I like math," the word "like" is in our memorized positive vocabulary list, so we judge it as having positive sentiment.</p>

    <p>Based on this idea, we can implement sentiment dictionary-based text sentiment classification through several steps: preprocessing, tokenization, training (loading) the sentiment dictionary, and judgment, as illustrated in the following diagram. The raw materials used to test the model include comments on Mengniu milk provided by Teacher Xue Yun and mobile phone comment data purchased online (see attachment).</p>

    <p>
        <a href="https://kexue.fm/usr/uploads/2015/06/2323544396.png" title="Click to view original image">
            <img src="https://kexue.fm/usr/uploads/2015/06/2323544396.png" alt="Text sentiment classification based on sentiment dictionary" />
        </a>
    </p>
    <center>Text sentiment classification based on sentiment dictionary</center>

    <p><strong>Text Preprocessing</strong></p>
    <p>Original corpora crawled by web crawlers usually contain unnecessary information, such as extra HTML tags, so preprocessing is required. The Mengniu milk comments provided by Teacher Xue Yun were no exception. Our team used Python as our preprocessing tool, utilizing libraries like Numpy and Pandas, with regular expressions as the primary text tool. After preprocessing, the raw corpus was standardized into the following table, where we used -1 to label negative sentiment comments and 1 for positive sentiment comments.</p>

    \[\begin{array}{c|c|c}
    \hline
    & \text{comment} & \text{mark}\\
    \hline
    0 & \text{Mengniu goes out to embarrass themselves again} & -1\\
    1 & \text{Cherish life, stay away from Mengniu} & -1\\
    \vdots & \vdots & \vdots \\
    1171 & \text{I have always loved drinking Mengniu pure milk, always, very much} & 1\\
    1172 & \text{Giving Mengniu... health is the best gift.} & 1\\
    \vdots & \vdots & \vdots \\
    \hline
    \end{array}\]

    <p><strong>Automatic Sentence Segmentation</strong></p>
    <p>To determine if a sentence contains corresponding words from the sentiment dictionary, we need to accurately cut the sentence into individual words, which is automatic segmentation. We compared existing segmentation tools and, considering accuracy and ease of use on the Python platform, ultimately chose "Jieba Chinese Segmentation" as our tool.</p>

    <p>The table below shows the segmentation results of common tools on a typical test sentence:</p>
    <blockquote><strong>Test Sentence:</strong> 工信处女干事每月经过下属科室都要亲口交代24口交换机等技术性器件的安装工作 (The female secretary of the Industry and Information Technology Office passes through the subordinate departments every month to personally explain the installation work of 24-port switches and other technical components.)</blockquote>

    <table border="1">
        <thead>
            <tr>
                <th>Segmentation Tool</th>
                <th>Test Result</th>
            </tr>
        </thead>
        <tbody>
            <tr>
                <td>Jieba Segmentation</td>
                <td>工信处/ 女干事/ 每月/ 经过/ 下属/ 科室/ 都/ 要/ 亲口/ 交代/ 24/ 口/ 交换机/ 等/ 技术性/ 器件/ 的/ 安装/ 工作</td>
            </tr>
            <tr>
                <td>ICTCLAS (CAS)</td>
                <td>工/n 信/n 处女/n 干事/n 每月/r 经过/p 下属/v 科室/n 都/d 要/v 亲口/d 交代/v 24/m 口/q 交换机/n 等/udeng 技术性/n 器件/n 的/ude1 安装/vn 工作/vn</td>
            </tr>
            <tr>
                <td>smallseg</td>
                <td>工信/ 信处/ 女干事/ 每月/ 经过/ 下属/ 科室/ 都要/ 亲口/ 交代/ 24/ 口/ 交换机/ 等/ 技术性/ 器件/ 的/ 安装/ 工作</td>
            </tr>
            <tr>
                <td>Yaha Segmentation</td>
                <td>工信处 / 女 / 干事 / 每月 / 经过 / 下属 / 科室 / 都 / 要 / 亲口 / 交代 / 24 / 口 / 交换机 / 等 / 技术性 / 器件 / 的 / 安装 / 工作</td>
            </tr>
        </tbody>
    </table>

    <p><strong>Loading the Sentiment Dictionary</strong></p>
    <p>Generally, the dictionary is the core part of text mining, and sentiment classification is no exception. The sentiment dictionary is divided into four parts: positive sentiment dictionary, negative sentiment dictionary, negation dictionary, and degree adverb dictionary. To obtain a more complete dictionary, we collected several sentiment dictionaries from the internet, integrated and deduplicated them, and adjusted some words to achieve the highest possible accuracy.</p>

    <p>
        <a href="https://kexue.fm/usr/uploads/2015/06/1152794772.png" title="Click to view original image">
            <img src="https://kexue.fm/usr/uploads/2015/06/1152794772.png" alt="Building a sentiment dictionary" />
        </a>
    </p>
    <center>Building a sentiment dictionary</center>

    <p>Our team did not simply integrate dictionaries collected online; we also specifically and purposefully cleaned and updated the dictionary. In particular, we added some industry-specific vocabulary to increase the hit rate in classification. Different industries have significant differences in the frequency of certain words, and these words might be key terminology for sentiment classification. For example, the comment data for Mengniu milk belongs to the food and beverage industry; in this industry, words like "eat" and "drink" appear frequently and are usually positive evaluations, while "don't eat" or "don't drink" usually imply negative evaluations. In other industries, these words might not have a clear sentiment orientation. Another example is the mobile phone industry, where phrases like "this phone is very drop-resistant and waterproof"—"drop-resistant" and "waterproof"—are positive sentiment words in that specific domain. Thus, it is necessary to consider these factors in the model.</p>

    <p><strong>Text Sentiment Classification</strong></p>
    <p>Text sentiment classification based on a sentiment dictionary is relatively mechanical. For simplicity, we assign a weight of 1 to each positive sentiment word and -1 to each negative sentiment word, assuming that sentiment values satisfy the principle of linear superposition. We then segment the sentence; if the word vector contains a corresponding word, we add the weight. Negation words and degree adverbs have special judgment rules: negation words cause the weight to flip sign, and degree adverbs double the weight. Finally, the sentiment of the sentence is judged based on whether the total weight is positive or negative. The basic algorithm is shown in the figure.</p>

    <p>
        <a href="https://kexue.fm/usr/uploads/2015/06/2778028889.png" title="Click to view original image">
            <img src="https://kexue.fm/usr/uploads/2015/06/2778028889.png" alt="Text classification based on sentiment dictionary - flowchart" />
        </a>
    </p>
    <center>Text classification based on sentiment dictionary - flowchart</center>

    <p>It should be noted that for programming and testing feasibility, we made several assumptions (simplifications). Assumption 1: We assume all positive and negative words have equal weights. This only holds in simple judgment cases; for more precise classification, it is clearly incorrect (e.g., "hate" is more severe than "dislike"). A way to fix this is to assign different weights to each word, which we will discuss in the second part of this series. Assumption 2: We assume weights are linearly superimposed. This holds in most cases, but in the second part, we will explore introducing nonlinearity to enhance accuracy. Assumption 3: For negations and degree adverbs, we just used simple sign inversion and doubling. In reality, different negation words and degree adverbs have different weights (e.g., "like very much" is deeper than "quite like"), but we did not make that distinction.</p>

    <p>As for the algorithm's implementation, we chose Python. With Python's rich extension support, we implemented all the steps in fewer than a hundred lines of code, resulting in an effective sentiment classification algorithm, which fully reflects Python's conciseness. Below, we test the effectiveness of our algorithm.</p>

    <h3 id="Model-Results-Testing">Model Results Testing</h3>
    <p>As a basic test, we first applied our model to the Mengniu milk comments provided by Teacher Xue Yun. The results were satisfying, reaching an accuracy of 82.02%. The detailed test report is in the table below:</p>

    \[\begin{array}{c|c|c|c|c|c}
    \hline
    \text{Data Content} & \text{Pos Samples} & \text{Neg Samples} & \text{Accuracy} & \text{TPR} & \text{TNR}\\
    \hline
    \text{Milk Comments} & 1005 & 1170 & 0.8202 & 0.8209 & 0.8197\\
    \hline
    \end{array}\]

    <p>(Where positive samples are positive sentiment comments, and negative samples are negative sentiment data.)</p>

    \[\begin{aligned}
    &\text{Accuracy}=\frac{\text{Correctly predicted samples}}{\text{Total samples}}\\
    &\text{True Positive Rate (TPR)}=\frac{\text{Positive samples predicted as positive}}{\text{Total positive samples}}\\
    &\text{True Negative Rate (TNR)}=\frac{\text{Negative samples predicted as negative}}{\text{Total negative samples}}
    \end{aligned}\]

    <p>To our surprise, the model adjusted from the Mengniu milk comment data reached 81.96% accuracy when applied directly to sentiment classification for a certain mobile phone's comment data! This indicates our model has good robustness and can perform well across different industries' comment data.</p>

    \[\begin{array}{c|c|c|c|c|c}
    \hline
    \text{Data Content} & \text{Pos Samples} & \text{Neg Samples} & \text{Accuracy} & \text{TPR} & \text{TNR}\\
    \hline
    \text{Phone Comments} & 1158 & 1159 & 0.8196 & 0.7539 & 0.8852\\
    \hline
    \end{array}\]

    <p><strong>Conclusion:</strong> Our team preliminary implemented text sentiment classification based on a sentiment dictionary. Test results show that simple judgment rules can give this algorithm good accuracy and robustness. Generally, a model with over 80% accuracy is considered to have production value suitable for industrial environments. Clearly, our model has initially reached this standard.</p>

    <h3 id="Difficulties">Difficulties</h3>
    <p>After two tests, we can tentatively conclude that our model's accuracy basically reaches over 80%. Furthermore, some mature commercial programs only reach about 85% to 90% accuracy (such as BosonNLP). This shows that our simple model has indeed achieved satisfactory results; on the other hand, this fact also indicates that the performance of the traditional "sentiment dictionary-based text sentiment classification" model has limited room for improvement. This is due to the inherent complexity of text sentiment classification. After preliminary discussion, we believe the difficulties lie in the following areas:</p>

    <p><strong>The language system is extremely complex</strong></p>
    <p>Ultimately, this is because the language system in our brains is extremely complex. (1) We are performing text sentiment classification. Text and text sentiment are products of human culture; in other words, humans are the only accurate standard for judgment. (2) Human language is a complex cultural product; a sentence is not a simple linear combination of words—it contains significant nonlinearity. (3) When we describe a sentence, we view it as a whole rather than a set of words. Different combinations, orders, and numbers of words can bring different meanings and sentiments, leading to difficulties in sentiment classification.</p>

    <p>Therefore, text sentiment classification is essentially a simulation of human brain thinking. Our previous model is essentially the simplest simulation. However, what we simulated were just simple mental sets. True sentiment judgment isn't a set of simple rules but a complex network.</p>

    <p><strong>The brain does more than just sentiment classification</strong></p>
    <p>In fact, when we judge the sentiment of a sentence, we are not just thinking about the sentiment; we also judge the sentence type (imperative, interrogative, or declarative?). When we consider each word in a sentence, we don't just focus on positive, negative, negation, or degree words; we focus on every word (subject, predicate, object, etc.), forming a holistic understanding of the sentence. We even connect the context to judge. These judgments might be unconscious, but the brain does them to form a complete understanding before making an accurate sentiment judgment. That is, our brain is in fact a very high-speed and complex processor; while we want to do sentiment classification, the brain simultaneously performs many other tasks.</p>

    <p><strong>Living Water: Learning and Prediction</strong></p>
    <p>A distinctive feature that separates humans from machines (and even humans from other animals) is the consciousness and ability to learn. Besides being taught by others, we acquire new knowledge through our own learning, summarization, and guessing. Sentiment classification is no exception; we don't just memorize a large number of sentiment words—we also summarize or infer new ones. For example, if we only know "like" and "love" are positive, we will guess that "fondness" also has a positive sentiment. This learning ability is an important way we expand our vocabulary and optimize our memory patterns (i.e., we don't need to specifically cram every word into the brain's corpus; we only need to remember "like" and "love" and assign a connection to obtain "fondness").</p>

    <h3 id="Optimization-Ideas">Optimization Ideas</h3>
    <p>Through the above analysis, we see the intrinsic complexity of text sentiment classification and several characteristics of how the human brain performs classification. Based on this, we propose the following improvement measures.</p>

    <p><strong>Introduction of Nonlinear Features</strong></p>
    <p>As mentioned earlier, real human sentiment classification is heavily nonlinear, and models based on simple linear combinations have limited performance. Therefore, to improve accuracy, it is necessary to introduce nonlinearity into the model.</p>

    <p>Nonlinearity refers to the new semantics formed by the combination of words. In fact, our preliminary model already simply introduced nonlinearity—we treated cases where a positive word and a negative word were adjacent as a combined negative phrase and assigned it a negative weight. More refined combination weights can be achieved through a "Dictionary Matrix." We place all known positive and negative words into the same set, number them, and record the weights of word pairs using the following "Dictionary Matrix":</p>

    \[\begin{array}{c|cccccc}
    \text{Word} & \text{(Null)} & \text{Like} & \text{Love} & \dots & \text{Hate} & \dots\\
    \hline
    \text{(Null)} & 0 & 1 & 2 & \dots & -1 & \dots\\
    \text{Like} & 1 & 2 & 3 & \dots & -2 & \dots\\
    \text{Love} & 2 & 3 & 4 & \dots & -2 & \dots\\
    \vdots & \vdots & \vdots & \vdots & \vdots & \vdots & \dots\\
    \text{Hate} & -1 & -2 & -3 & \dots & -2 & \dots\\
    \vdots & \vdots & \vdots & \vdots & \vdots & \vdots & \dots
    \end{array}\]

    <p>Not every word combination is valid, but we can still calculate combination weights. The calculation of sentiment weights can be found in the references. However, the number of sentiment words is very large, and the number of elements in the dictionary matrix is its square, resulting in massive data. This enters the realm of "Big Data." To efficiently implement nonlinearity, we need to explore optimization schemes for word combinations, including construction, storage, and indexing schemes.</p>

    <p><strong>Automatic Expansion of the Sentiment Dictionary</strong></p>
    <p>In today's internet age, new words appear like spring mushrooms, including "newly constructed internet terms" and "giving existing words new meanings." On the other hand, the sentiment dictionaries we organize cannot possibly contain all existing sentiment words. Therefore, automatic expansion of the sentiment dictionary is a necessary condition for maintaining the timeliness of sentiment classification models. Currently, using web crawlers, we can collect large amounts of comment data from Weibo and communities. To find new words with sentiment orientation from this mass of data, our approach uses unsupervised word frequency statistics.</p>

    <p>Our goal is "automatic expansion." Thus, we aim to perform unsupervised learning based on the current model to complete the dictionary expansion, thereby enhancing the model's performance, and then iterate in the same way. This is a positive feedback adjustment process. Although we can crawl a large number of comments, they are unlabeled. We use the existing model to classify the sentiment of the comment data, then count word frequencies in the same category of sentiment (positive or negative) collections, and finally compare frequencies across the positive and negative sets. If a word appears very infrequently in positive comments but very frequently in negative ones, we can confidently add it to the negative sentiment dictionary or assign it a negative weight.</p>

    <p>For example, suppose "shady" is not in our negative dictionary, but basic words like "vile," "dislike," "resent," and "like" already exist. Then we can correctly classify the following sentences:</p>

    \[\begin{array}{c|c}
    \hline
    \text{Sentence} & \text{Weight}\\
    \hline
    \text{This shady boss is so vile} & -2\\
    \hline
    \text{I resent the practices of this shady enterprise} & -2\\
    \hline
    \text{Dislike this shady shop very much} & -2\\
    \hline
    \text{This shop is really shady!} & 0\\
    \hline
    \vdots & \vdots\\
    \hline
    \end{array}\]

    <p>Since "shady" is not in the negative dictionary, "This shop is really shady!" would be judged as neutral (weight 0). After classification, we count the word frequencies for positive and negative labels. We find that the new word "shady" appears many times in negative comments but almost never in positive ones. We then add "shady" to our negative sentiment dictionary and update our results:</p>

    \[\begin{array}{c|c}
    \hline
    \text{Sentence} & \text{Weight}\\
    \hline
    \text{This shady boss is so vile} & -3\\
    \hline
    \text{I resent the practices of this shady enterprise} & -3\\
    \hline
    \text{Dislike this shady shop very much} & -3\\
    \hline
    \text{This shop is really shady!} & -2\\
    \hline
    \vdots & \vdots\\
    \hline
    \end{array}\]

    <p>Thus, we expanded the dictionary through unsupervised learning, improved accuracy, and enhanced model performance. This is an iterative process where previous results help the subsequent steps.</p>

    <p><strong>Conclusion of this paper:</strong></p>
    <blockquote>
        <p>Text sentiment classification based on a sentiment dictionary is easy to implement; its core lies in the training (compilation) of the sentiment dictionary.</p>
        <p>The language system is extremely complex; dictionary-based classification is just a linear model, and its performance is limited.</p>
        <p>Properly introducing nonlinear features into text sentiment classification can effectively improve model accuracy.</p>
        <p>Introducing an unsupervised learning mechanism for dictionary expansion can effectively discover new sentiment words, ensuring the model's robustness and timeliness.</p>
    </blockquote>

    <h3 id="References">References</h3>
    <p>Deep Learning Study Notes: <a href="http://blog.csdn.net/zouxy09/article/details/8775360">http://blog.csdn.net/zouxy09/article/details/8775360</a></p>
    <p>Yoshua Bengio, Réjean Ducharme Pascal Vincent, Christian Jauvin. A Neural Probabilistic Language Model, 2003</p>
    <p>A New Language Model: <a href="http://blog.sciencenet.cn/blog-795431-647334.html">http://blog.sciencenet.cn/blog-795431-647334.html</a></p>
    <p>Sentiment Analysis Dataset for Comment Data: <a href="http://www.datatang.com/data/11857">http://www.datatang.com/data/11857</a></p>
    <p>"Jieba" Chinese Segmentation: <a href="https://github.com/fxsjy/jieba">https://github.com/fxsjy/jieba</a></p>
    <p>NLPIR Chinese Segmentation System: <a href="http://ictclas.nlpir.org/">http://ictclas.nlpir.org/</a></p>
    <p>smallseg: <a href="https://code.google.com/p/smallseg/">https://code.google.com/p/smallseg/</a></p>
    <p>yaha segmentation: <a href="https://github.com/jannson/yaha">https://github.com/jannson/yaha</a></p>
    <p>Sentiment Word Set (beta): <a href="http://www.keenage.com/html/c_bulletin_2007.htm">http://www.keenage.com/html/c_bulletin_2007.htm</a></p>
    <p>NTUSD - Simplified Chinese Sentiment Polarity Dictionary: <a href="http://www.datatang.com/data/11837">http://www.datatang.com/data/11837</a></p>
    <p>Degree Adverbs, Intensities, and Negation Word List: <a href="http://www.datatang.com/data/44198">http://www.datatang.com/data/44198</a></p>
    <p>Summary of Existing Sentiment Dictionaries: <a href="http://www.datatang.com/data/46922">http://www.datatang.com/data/46922</a></p>
    <p>BosonNLP: <a href="http://bosonnlp.com/product">http://bosonnlp.com/product</a></p>

    <h3 id="Platform">Implementation Platform</h3>
    <p>The programming tools used by our team were tested in the following environment:</p>
    <blockquote>
        <p><strong>Windows 8.1</strong> Microsoft Operating System.</p>
        <p><strong>Python</strong> 3.4 Development platform/language. Version 3.x was chosen over 2.x primarily for better support for Chinese characters.</p>
        <p><strong>Numpy</strong> A numerical calculation library for Python, providing fast processing for multi-dimensional arrays.</p>
        <p><strong>Pandas</strong> A data analysis package for Python.</p>
        <p><strong>Jieba Segmentation</strong> A Chinese segmentation tool for Python, also available in Java, C++, Node.js, etc.</p>
    </blockquote>

    <h3 id="Code-List">Code List</h3>
    <p>Resources: <a href="https://kexue.fm/usr/uploads/2017/09/1922797046.zip">Sentiment Polarity Dictionary.zip</a></p>

    <p><strong>Preprocessing</strong></p>
    <pre><code># (Python code for preprocessing goes here)</code></pre>

    <p><strong>Loading Sentiment Dictionary</strong></p>
    <pre><code># (Python code for loading dictionary goes here)</code></pre>

    <p><strong>Prediction Function</strong></p>
    <pre><code># (Python code for prediction function goes here)</code></pre>

    <p><strong>Simple Test</strong></p>
    <pre><code># (Python code for simple testing goes here)</code></pre>
</article>
<hr>
<footer style="margin-top: 3em; padding: 1.5em; background: #f5f5f5; border-radius: 8px; font-size: 0.9em; color: #555;">
    <p style="margin: 0 0 0.5em 0;"><strong>Citation</strong></p>
    <p style="margin: 0 0 0.5em 0;">
        This is a machine translation of the original Chinese article:<br>
        <a href="translation_3360.html" style="color: #005fcc;">https://kexue.fm/archives/3360</a>
    </p>
    <p style="margin: 0 0 0.5em 0;">
        Original author: 苏剑林 (Su Jianlin)<br>
        Original publication: <a href="https://kexue.fm" style="color: #005fcc;">科学空间 (Scientific Spaces)</a>
    </p>
    <p style="margin: 0; font-style: italic;">
        Translated using Gemini 3 Flash. Please refer to the original for authoritative content.
    </p>
</footer>
