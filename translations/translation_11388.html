
    <style type="text/css">

    body {
    margin: 48px auto;
    max-width: 68ch;              /* character-based width reads better */
    padding: 0 16px;

    font-family: -apple-system, BlinkMacSystemFont, "Segoe UI",
                Roboto, "Helvetica Neue", Arial, sans-serif;
    font-size: 18px;
    line-height: 1.65;
    color: #333;
    background: #fafafa;
    }

    h1, h2, h3, h4 {
    line-height: 1.25;
    margin-top: 2.2em;
    margin-bottom: 0.6em;
    font-weight: 600;
    }

    h1 {
    font-size: 2.1em;
    margin-top: 0;
    }

    h2 {
    font-size: 1.6em;
    border-bottom: 1px solid #e5e5e5;
    padding-bottom: 0.3em;
    }

    h3 {
    font-size: 1.25em;
    }

    h4 {
    font-size: 1.05em;
    color: #555;
    }

    /* Paragraphs and lists */
    p {
    margin: 1em 0;
    }

    ul, ol {
    margin: 1em 0 1em 1.5em;
    }

    li {
    margin: 0.4em 0;
    }

    a {
    color: #005fcc;
    text-decoration: none;
    }

    a:hover {
    text-decoration: underline;
    }

    code {
    font-family: ui-monospace, SFMono-Regular, Menlo, Consolas, monospace;
    font-size: 0.95em;
    background: #f2f2f2;
    padding: 0.15em 0.35em;
    border-radius: 4px;
    }

    pre {
    font-family: ui-monospace, SFMono-Regular, Menlo, Consolas, monospace;
    font-size: 0.9em;
    background: #f5f5f5;
    padding: 1em 1.2em;
    overflow-x: auto;
    border-radius: 6px;
    line-height: 1.45;
    }

    pre code {
    background: none;
    padding: 0;
    }

    blockquote {
    margin: 1.5em 0;
    padding-left: 1em;
    border-left: 4px solid #ddd;
    color: #555;
    }

    hr {
    border: none;
    border-top: 1px solid #e0e0e0;
    margin: 3em 0;
    }

    table {
    border-collapse: collapse;
    margin: 1.5em 0;
    width: 100%;
    font-size: 0.95em;
    }

    th, td {
    padding: 0.5em 0.7em;
    border-bottom: 1px solid #e5e5e5;
    text-align: left;
    }

    th {
    font-weight: 600;
    }

    img {
    max-width: 100%;
    display: block;
    margin: 1.5em auto;
    }

    small {
    color: #666;
    }

    mjx-container {
    margin: 1em 0;
    }

    ::selection {
    background: #cce2ff;
    }

    </style>
    

<script>
window.MathJax = {
  tex: {
    inlineMath: [['$', '$'], ['\\(', '\\)']],
    displayMath: [['$$', '$$'], ['\\[', '\\]']],
    processEscapes: true,
    tags: 'ams',
    macros: {
      msign: ["\\mathop{\\text{msign}}"],
      tr: ["\\mathop{\\text{tr}}"],
      argmin: ["\\mathop{\\text{argmin}}"],
      sign: ["\\mathop{\\text{sign}}"]
    },
    packages: {'[+]': ['ams']}
  }
};
</script>
<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

<h1><a href="https://kexue.fm/archives/11388">Steepest Descent on Manifolds: 5. Dual Gradient Descent</a></h1>

<p>By 苏剑林 | November 03, 2025</p>

<p>In the first four articles, we solved several specific steepest descent problems with equality constraints on parameters. Among them, the problems in the third and fourth articles could not be solved analytically, so I proposed corresponding fixed-point iteration methods. In particular, the study of "Muon + Stiefel" in the third article, <a href="translation_11221.html">"Steepest Descent on Manifolds: 3. Muon + Stiefel"</a>, originated from Jeremy Bernstein's article <a href="https://docs.modula.systems/algorithms/manifold/orthogonal/">"Orthogonal manifold"</a>.</p>

<p>For this problem, Jeremy Bernstein eventually provided his own solution, which I call "Dual Gradient Descent," and it is also quite worth learning.</p>

<h2>Basic Concepts</h2>

<p>Jeremy Bernstein's solution was finally published in the Thinking Machines Lab blog post <a href="https://thinkingmachines.ai/blog/modular-manifolds/">"Modular Manifolds"</a>, the lab's second blog post. In that article, they refer to it as "Dual Ascent," but here I will combine it with the content of the previous four articles and call it "Dual Gradient Descent."</p>

<p>In fact, dual gradient descent can be seen as a natural consequence of the Lagrange multiplier method. However, the rigorous discussion of Lagrange multipliers is quite cumbersome, as it requires introducing the <a href="https://en.wikipedia.org/wiki/Minimax_theorem">Minimax theorem</a>. Therefore, in this series, to avoid these complications, we have adopted the "undetermined coefficients" approach for derivation. This makes dual gradient descent seem less natural. But no matter—we can still derive it following our logic, though it might take a bit more space.</p>

<p>First, let's review the notation. $\boldsymbol{W}\in\mathbb{R}^{n\times m}$ is a matrix parameter; without loss of generality, assume $n\geq m$. $\boldsymbol{G}\in\mathbb{R}^{n\times m}$ is its gradient. $\Vert\boldsymbol{G}\Vert_2$ is the spectral norm of matrix $\boldsymbol{G}$, equal to the largest singular value; $\Vert\boldsymbol{G}\Vert_*$ is the nuclear norm of matrix $\boldsymbol{G}$, equal to the sum of all singular values. Specifically, according to the conclusions in the article <a href="translation_10878.html">"Derivatives of SVD"</a>, we have
\begin{equation}\nabla_{\boldsymbol{G}}\Vert\boldsymbol{G}\Vert_* = \sum_i \nabla_{\boldsymbol{G}} \sigma_i = \sum_i \boldsymbol{u}_i \boldsymbol{v}_i^{\top} = \boldsymbol{U}\boldsymbol{V}^{\top} = \msign(\boldsymbol{G}) \label{eq:nuclear-grad}\end{equation}
where $\boldsymbol{G}=\sum_i \sigma_i \boldsymbol{u}_i \boldsymbol{v}_i^{\top} = \boldsymbol{U}\boldsymbol{\Sigma}\boldsymbol{V}^{\top}$ is the SVD of $\boldsymbol{G}$. That is to say, the gradient of the nuclear norm is exactly the $\msign$ operator, which is an important foundation for the subsequent derivation.</p>

<h2>Problem Description</h2>

<p>We will still follow the previous derivation logic to introduce dual gradient descent, so this section restates the problem and existing results.</p>

<p>In <a href="translation_11221.html">"Steepest Descent on Manifolds: 3. Muon + Stiefel"</a>, the problem we wanted to solve was
\begin{equation}\max_{\boldsymbol{\Phi}} \tr(\boldsymbol{G}^{\top}\boldsymbol{\Phi}) \qquad \text{s.t.}\qquad \Vert\boldsymbol{\Phi}\Vert_2 = 1,\,\,\, \boldsymbol{W}^{\top}\boldsymbol{W}=\boldsymbol{I},\,\,\,\boldsymbol{W}^{\top}\boldsymbol{\Phi}+\boldsymbol{\Phi}^{\top}\boldsymbol{W} = \boldsymbol{0} \label{eq:muon-stiefel}\end{equation}
The solution is $\boldsymbol{\Phi} = \msign(\boldsymbol{G} + \boldsymbol{W}\boldsymbol{X})$, where $\boldsymbol{X}\in\mathbb{R}^{m\times m}$ is an undetermined symmetric matrix such that $\boldsymbol{W}^{\top}\boldsymbol{\Phi}+\boldsymbol{\Phi}^{\top}\boldsymbol{W} = \boldsymbol{0}$.</p>

<p>In <a href="translation_11241.html">"Steepest Descent on Manifolds: 4. Muon + Spectral Sphere"</a>, the problem we wanted to solve was
\begin{equation}\max_{\boldsymbol{\Phi}} \tr(\boldsymbol{G}^{\top}\boldsymbol{\Phi}) \qquad \text{s.t.}\qquad \Vert\boldsymbol{\Phi}\Vert_2 = 1,\,\,\, \tr(\boldsymbol{\Theta}^{\top} \boldsymbol{\Phi})=0 \label{eq:muon-spectral}\end{equation}
The answer is $\boldsymbol{\Phi} = \msign(\boldsymbol{G} + \lambda\boldsymbol{\Theta})$, where $\lambda$ is an undetermined coefficient such that $\tr(\boldsymbol{\Theta}^{\top} \boldsymbol{\Phi})=0$.</p>

<p>As we can see, our final task has become finding undetermined coefficients that satisfy the extra equality constraints. This is essentially solving a (system of) non-linear equation(s). Dual gradient descent transforms the solution of equations into the minimization of a certain objective function, which is then solved using gradient descent.</p>

<h2>Dual Objective</h2>

<p>The key to the transformation is the nuclear norm gradient equality \eqref{eq:nuclear-grad}. For simplicity, let's first look at the "Muon + Spectral Sphere" problem \eqref{eq:muon-spectral}, where the undetermined coefficient is just a scalar, making it easier to observe. It is not difficult to verify that
\begin{equation}\nabla_{\lambda} \Vert\boldsymbol{G} + \lambda\boldsymbol{\Theta}\Vert_* = \tr(\boldsymbol{\Theta}^{\top}\msign(\boldsymbol{G} + \lambda\boldsymbol{\Theta})) = \tr(\boldsymbol{\Theta}^{\top} \boldsymbol{\Phi})\end{equation}
This means that solving the equation $\tr(\boldsymbol{\Theta}^{\top} \boldsymbol{\Phi})=0$ is equivalent to finding a point where the gradient of $\Vert\boldsymbol{G} + \lambda\boldsymbol{\Theta}\Vert_*$ with respect to $\lambda$ is zero; this could be its (local) minimum/maximum point. Since $\Vert\boldsymbol{G} + \lambda\boldsymbol{\Theta}\Vert_*$ clearly has no maximum value, we transform it into finding its minimum point:
\begin{equation}\lambda^* = \argmin_{\lambda} \Vert\boldsymbol{G} + \lambda\boldsymbol{\Theta}\Vert^* \label{eq:muon-spectral-obj}\end{equation}</p>

<p>Let's recap the steps here:</p>

<blockquote>
    <p>1. Our goal is to solve the equation $\tr(\boldsymbol{\Theta}^{\top} \boldsymbol{\Phi})=0$, finding any one solution will do;</p>
    <p>2. $\tr(\boldsymbol{\Theta}^{\top} \boldsymbol{\Phi})$ happens to be the gradient of $\Vert\boldsymbol{G} + \lambda\boldsymbol{\Theta}\Vert_*$ with respect to $\lambda$;</p>
    <p>3. This transforms into finding a (local) minimum/maximum point, as the gradient is usually zero there;</p>
    <p>4. One can easily determine there is no maximum, so we must find the minimum.</p>
</blockquote>

<h2>Gradient Descent</h2>

<p>After determining the objective \eqref{eq:muon-spectral-obj}, we can use gradient descent to solve it. The gradient is already available, namely $\tr(\boldsymbol{\Theta}^{\top} \boldsymbol{\Phi})$. Thus, the update format for gradient descent is:
\begin{equation}\lambda \quad \leftarrow\quad \lambda - \eta \tr(\boldsymbol{\Theta}^{\top} \boldsymbol{\Phi})\end{equation}
Of course, we could also consider adding a $\sign$ to $\tr(\boldsymbol{\Theta}^{\top} \boldsymbol{\Phi})$, i.e., SignSGD; these variations can be applied freely. From the iterative format, dual gradient descent is much simpler than the fixed-point iteration we proposed earlier. However, in many cases, dual gradient descent requires significantly more iterations and might need fine-tuning of the learning rate or the introduction of momentum to converge.</p>

<p>Thus, for solving the equation $\tr(\boldsymbol{\Theta}^{\top} \boldsymbol{\Phi})=0$ itself, dual gradient descent is not necessarily an ideal solution. However, our ultimate goal is not just solving the equation $\tr(\boldsymbol{\Theta}^{\top} \boldsymbol{\Phi})=0$, but calculating $\boldsymbol{\Phi}$ as the optimization direction for the model. Model optimization is itself an iterative process. We can cache the historical $\lambda$ and adopt an approximation strategy where $\lambda$ is updated synchronously with the model parameters:
\begin{equation}\boldsymbol{\Phi} = \msign(\boldsymbol{G} + \lambda\boldsymbol{\Theta}), \quad \boldsymbol{W}\leftarrow\boldsymbol{W}- \eta_1 \boldsymbol{\Phi},\quad \lambda \leftarrow\lambda - \eta_2 \tr(\boldsymbol{\Theta}^{\top} \boldsymbol{\Phi})\end{equation}
In this way, each training step only requires calculating one almost-free extra step for $\lambda - \eta_2 \tr(\boldsymbol{\Theta}^{\top} \boldsymbol{\Phi})$, resulting in an approximate implementation of the original objective \eqref{eq:muon-spectral}. Formally, it is equivalent to a form of adaptive Weight Decay for Muon.</p>

<h2>On Stiefel</h2>

<p>Having discussed the relatively simple "Muon + Spectral Sphere," let's look at "Muon + Stiefel," i.e., objective \eqref{eq:muon-stiefel}. Here, the undetermined matrix $\boldsymbol{X}$ has the constraint $\boldsymbol{X}=\boldsymbol{X}^{\top}$. We remove the constraint by setting $\boldsymbol{X}=\boldsymbol{\Lambda}+\boldsymbol{\Lambda}^{\top}$, where $\boldsymbol{\Lambda}\in\mathbb{R}^{m\times m}$ is an arbitrary matrix. It can then be found that
\begin{equation}\nabla_{\boldsymbol{\Lambda}}\Vert\boldsymbol{G} + \boldsymbol{W}\boldsymbol{X}\Vert_* = \boldsymbol{W}^{\top}\boldsymbol{\Phi}+\boldsymbol{\Phi}^{\top}\boldsymbol{W} \end{equation}
where $\boldsymbol{\Phi} = \msign(\boldsymbol{G} + \boldsymbol{W}\boldsymbol{X})$. Therefore, solving the system of equations $\boldsymbol{W}^{\top}\boldsymbol{\Phi}+\boldsymbol{\Phi}^{\top}\boldsymbol{W}=\boldsymbol{0}$ can similarly be transformed into finding the minimum point of the function $\Vert\boldsymbol{G} + \boldsymbol{W}\boldsymbol{X}\Vert_*$, solved with gradient descent:
\begin{equation}\boldsymbol{\Lambda} \quad\leftarrow\quad \boldsymbol{\Lambda} - \eta(\boldsymbol{W}^{\top}\boldsymbol{\Phi}+\boldsymbol{\Phi}^{\top}\boldsymbol{W}) \end{equation}
Since $\boldsymbol{W}^{\top}\boldsymbol{\Phi}+\boldsymbol{\Phi}^{\top}\boldsymbol{W}$ must be symmetric, it is also feasible to directly update $\boldsymbol{X} \leftarrow\boldsymbol{X} - \eta(\boldsymbol{W}^{\top}\boldsymbol{\Phi}+\boldsymbol{\Phi}^{\top}\boldsymbol{W})$. By iterating it synchronously with $\boldsymbol{W}$, we get:
\begin{equation}\boldsymbol{\Phi} = \msign(\boldsymbol{G} + \boldsymbol{W}\boldsymbol{X}), \quad \boldsymbol{W}\leftarrow\boldsymbol{W}- \eta_1 \boldsymbol{\Phi},\quad \boldsymbol{X} \leftarrow\boldsymbol{X} - \eta_2(\boldsymbol{W}^{\top}\boldsymbol{\Phi}+\boldsymbol{\Phi}^{\top}\boldsymbol{W})\end{equation}
This realizes an approximation of objective \eqref{eq:muon-stiefel}, and the extra $\boldsymbol{X} - \eta_2(\boldsymbol{W}^{\top}\boldsymbol{\Phi}+\boldsymbol{\Phi}^{\top}\boldsymbol{W})$ at each step is also virtually free.</p>

<h2>Lagrange Multipliers</h2>

<p>In these two examples, is it a pure coincidence that the equations to be solved exactly equal the gradient of some nuclear norm objective? Of course not. As we mentioned in the "Basic Concepts" section, this is a natural result of the Lagrange multiplier method. In this section, we will expand on this discussion.</p>

<p>For ease of understanding, we'll use the simpler objective \eqref{eq:muon-spectral} as an example. It can be equivalently written as:
\begin{equation}\max_{\Vert\boldsymbol{\Phi}\Vert_2\leq 1} \min_{\lambda\in\mathbb{R}}\tr(\boldsymbol{G}^{\top}\boldsymbol{\Phi}) + \lambda\tr(\boldsymbol{\Theta}^{\top} \boldsymbol{\Phi})\end{equation}
To understand this transformation, one only needs to realize that the above expression must satisfy $\tr(\boldsymbol{\Theta}^{\top} \boldsymbol{\Phi})=0$; otherwise, the $\min$ step can always reach negative infinity, and the final $\max$ result would also be negative infinity. As for changing $\Vert\boldsymbol{\Phi}\Vert_2 = 1$ to $\Vert\boldsymbol{\Phi}\Vert_2\leq 1$, it does not change the result of the maximum (since the maximum is always taken at the boundary), but it makes the feasible region of $\boldsymbol{\Phi}$ a <a href="https://en.wikipedia.org/wiki/Convex_set">convex set</a>.</p>

<p>With this equivalent form, we can use the <a href="https://en.wikipedia.org/wiki/Minimax_theorem">Minimax theorem</a> to swap the positions of $\min$ and $\max$:
\begin{equation}\begin{aligned}
&\,\max_{\Vert\boldsymbol{\Phi}\Vert_2\leq 1} \min_{\lambda\in\mathbb{R}}\tr(\boldsymbol{G}^{\top}\boldsymbol{\Phi}) + \lambda\tr(\boldsymbol{\Theta}^{\top} \boldsymbol{\Phi}) \\
=&\, \min_{\lambda\in\mathbb{R}}\max_{\Vert\boldsymbol{\Phi}\Vert_2\leq 1}\tr(\boldsymbol{G}^{\top}\boldsymbol{\Phi}) + \lambda\tr(\boldsymbol{\Theta}^{\top} \boldsymbol{\Phi}) \\
=&\, \min_{\lambda\in\mathbb{R}} \Vert\boldsymbol{G} + \lambda \boldsymbol{\Theta}\Vert_*
\end{aligned}\end{equation}
Here, the step taking the $\max$ over $\Vert\boldsymbol{\Phi}\Vert_2\leq 1$ is a <a href="https://kexue.fm/archives/11215#%E5%9F%BA%E6%9C%AC%E7%BB%93%E6%9E%9C">basic result</a> of the Muon derivation, so calculating the $\max$ first presents no difficulty. Thus, we have obtained the dual objective $\Vert\boldsymbol{G} + \lambda \boldsymbol{\Theta}\Vert_*$ of the original problem \eqref{eq:muon-spectral}.</p>

<p>Some readers might wonder: How is this Lagrange multiplier method different from the one I learned? Because the Lagrange multiplier method here is generalized to general convex sets, and the interchangeability of $\min$ and $\max$ is strictly discussed to ensure the final result is what we want. The Lagrange multiplier method we generally learn is just a heuristic set of procedures for solving constrained optimization problems in $\mathbb{R}^n$, without much discussion of details regarding theoretical guarantees.</p>

<h2>Summary</h2>

<p>This article introduced the idea of using dual gradient descent to find the direction of steepest descent on manifolds. This is the same method used by the Thinking Machines Lab blog <a href="https://thinkingmachines.ai/blog/modular-manifolds/">"Modular Manifolds"</a> to solve for Muon on the Stiefel manifold.</p>

<hr>
<footer style="margin-top: 3em; padding: 1.5em; background: #f5f5f5; border-radius: 8px; font-size: 0.9em; color: #555;">
    <p style="margin: 0 0 0.5em 0;"><strong>Citation</strong></p>
    <p style="margin: 0 0 0.5em 0;">
        This is a machine translation of the original Chinese article:<br>
        <a href="https://kexue.fm/archives/11388" style="color: #005fcc;">https://kexue.fm/archives/11388</a>
    </p>
    <p style="margin: 0 0 0.5em 0;">
        Original author: 苏剑林 (Su Jianlin)<br>
        Original publication: <a href="https://kexue.fm" style="color: #005fcc;">科学空间 (Scientific Spaces)</a>
    </p>
    <p style="margin: 0; font-style: italic;">
        Translated using Gemini 3 Flash. Please refer to the original for authoritative content.
    </p>
</footer>
