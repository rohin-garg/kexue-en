
    <style type="text/css">

    body {
    margin: 48px auto;
    max-width: 68ch;              /* character-based width reads better */
    padding: 0 16px;

    font-family: -apple-system, BlinkMacSystemFont, "Segoe UI",
                Roboto, "Helvetica Neue", Arial, sans-serif;
    font-size: 18px;
    line-height: 1.65;
    color: #333;
    background: #fafafa;
    }

    h1, h2, h3, h4 {
    line-height: 1.25;
    margin-top: 2.2em;
    margin-bottom: 0.6em;
    font-weight: 600;
    }

    h1 {
    font-size: 2.1em;
    margin-top: 0;
    }

    h2 {
    font-size: 1.6em;
    border-bottom: 1px solid #e5e5e5;
    padding-bottom: 0.3em;
    }

    h3 {
    font-size: 1.25em;
    }

    h4 {
    font-size: 1.05em;
    color: #555;
    }

    /* Paragraphs and lists */
    p {
    margin: 1em 0;
    }

    ul, ol {
    margin: 1em 0 1em 1.5em;
    }

    li {
    margin: 0.4em 0;
    }

    a {
    color: #005fcc;
    text-decoration: none;
    }

    a:hover {
    text-decoration: underline;
    }

    code {
    font-family: ui-monospace, SFMono-Regular, Menlo, Consolas, monospace;
    font-size: 0.95em;
    background: #f2f2f2;
    padding: 0.15em 0.35em;
    border-radius: 4px;
    }

    pre {
    font-family: ui-monospace, SFMono-Regular, Menlo, Consolas, monospace;
    font-size: 0.9em;
    background: #f5f5f5;
    padding: 1em 1.2em;
    overflow-x: auto;
    border-radius: 6px;
    line-height: 1.45;
    }

    pre code {
    background: none;
    padding: 0;
    }

    blockquote {
    margin: 1.5em 0;
    padding-left: 1em;
    border-left: 4px solid #ddd;
    color: #555;
    }

    hr {
    border: none;
    border-top: 1px solid #e0e0e0;
    margin: 3em 0;
    }

    table {
    border-collapse: collapse;
    margin: 1.5em 0;
    width: 100%;
    font-size: 0.95em;
    }

    th, td {
    padding: 0.5em 0.7em;
    border-bottom: 1px solid #e5e5e5;
    text-align: left;
    }

    th {
    font-weight: 600;
    }

    img {
    max-width: 100%;
    display: block;
    margin: 1.5em auto;
    }

    small {
    color: #666;
    }

    mjx-container {
    margin: 1em 0;
    }

    ::selection {
    background: #cce2ff;
    }

    </style>
    

<script>
window.MathJax = {
  tex: {
    inlineMath: [['$', '$'], ['\\(', '\\)']],
    displayMath: [['$$', '$$'], ['\\[', '\\]']],
    processEscapes: true,
    packages: {'[+]': ['ams']},
    tags: 'ams'
  },
  loader: {load: ['[tex]/ams']}
};
</script>
<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"></script>

<article>
    <nav style="margin-bottom: 1.5em;">
    <a href="../index.html" style="display: inline-flex; align-items: center; color: #555; text-decoration: none; font-size: 0.95em;">
        <span style="margin-right: 0.3em;">&larr;</span> Back to Index
    </a>
</nav>

    <h1><a href="https://kexue.fm/archives/9547">FAQ for "Why are modern LLMs all Decoder-only architectures?"</a></h1>
    <p>By 苏剑林 | March 20, 2023</p>

    <p>Last week, I wrote <a href="translation_9529.html">"Why are modern LLMs all Decoder-only architectures?"</a>, summarizing some of my experimental conclusions and conjectures on this issue. As expected, hot topics attract significant traffic; the repost by PaperWeekly reached over 10,000 views in a short time, and it received many likes on Zhihu. Across several platforms, I received various suggestions and questions from readers. I have summarized some of the most representative questions into this FAQ, hoping to further help everyone resolve their doubts.</p>

    <h2>Review</h2>
    <p>In <a href="translation_9529.html">"Why are modern LLMs all Decoder-only architectures?"</a>, I compared GPT and UniLM architectures through experiments and, combined with previous research experience, conjectured the following conclusions:</p>
    <ol>
        <li>Changing the attention of the input part to bidirectional does not bring benefits; the advantage of the Encoder-Decoder architecture is likely merely derived from doubling the parameters.</li>
        <li>The reason bidirectional attention fails to provide benefits may be due to the low-rank problem of bidirectional attention leading to performance degradation.</li>
    </ol>
    <p>Therefore, based on these two conjectures, we arrived at the conclusion:</p>
    <blockquote>
        Under the same number of parameters and the same inference cost, the Decoder-only architecture is the optimal choice.
    </blockquote>
    <p>For details regarding the experiments and reasoning, please refer to the original article; I will not repeat them here.</p>

    <h2>Q&A</h2>
    <p>Here are my answers to some of the readers' doubts.</p>

    <blockquote>
        <strong>Question 1:</strong> Does $n \gg d$ really hold?
    </blockquote>
    <p><strong>Answer:</strong> $n$ is the sequence length, and $d$ is the <code>head_size</code>, not the <code>hidden_size</code>. In multi-head attention, <code>head_size = hidden_size / heads</code>. For example, in BERT base, <code>head_size = 768 / 12 = 64</code>, while the pre-training length $n$ is generally 512, so $n \gg d$ roughly holds in most cases.</p>

    <blockquote>
        <strong>Question 2:</strong> BERT and the original GPT have the same parameter count; why is BERT better at understanding tasks?
    </blockquote>
    <p><strong>Answer:</strong> BERT and GPT differ not only in architecture but also in their pre-training tasks, making it impossible to perform a fair comparison. At the end of the original article, I provided a thought on improving BERT using GPT principles, and preliminary experiments showed it would likely outperform BERT. That experiment is the one where variables were strictly controlled.</p>

    <blockquote>
        <strong>Question 3:</strong> "Performance degradation caused by the low-rank problem of bidirectional attention" seems like a bug. Since the vast majority of models in the industry today use bidirectional attention, wouldn't the impact be too widespread?
    </blockquote>
    <p><strong>Answer:</strong> We did not conclude that "bidirectional attention is disastrous for any task." The phenomenon that "most models in the industry use bidirectional attention" does not actually conflict with the conclusions of the original article. Our experimental conclusion was "introducing bidirectional attention in the Encoder for generation tasks does not seem to yield benefits." The condition for this conclusion is very specific—"in the Encoder of a generation task."</p>

    <blockquote>
        <strong>Question 4:</strong> I don't think so... Decoder models are just more suitable for dialogue models. Inside Google, LLM-based Encoder models, Decoder models, and Encoder-Decoder models all exist; they just have different application scenarios where the others perform better on other tasks.
    </blockquote>
    <p><strong>Answer:</strong> The answer to this is similar to the previous one. The fact that "Encoder, Decoder, and Encoder-Decoder models all exist" does not contradict the original conclusion. We only tentatively speculated that "introducing bidirectional attention in the Encoder for generation tasks does not seem to yield benefits"; we did not say that the doubling of parameters brought by the Encoder would not yield benefits.</p>

    <blockquote>
        <strong>Question 5:</strong> Does your conclusion seem to contradict the conclusions of T5 and UL2?
    </blockquote>
    <p><strong>Answer:</strong> First, the original conclusion is not in conflict with UL2. The original article speculated that "under the same number of parameters and same inference cost, Decoder-only is the optimal choice," whereas UL2's conclusion was that Encoder-Decoder performed better, but Encoder-Decoder and Decoder-only were not compared under the same number of parameters. Second, the conclusion of the original article does indeed conflict somewhat with the experimental results in the T5 paper (Table 2). However, I also have doubts about the T5 experimental results:</p>
    <ol>
        <li>Whether the variables were strictly controlled between <code>decoder-only</code> and <code>unilm</code> in that table; the gap between them seems too large and unreasonable. Even if <code>decoder-only</code> might be inferior to <code>unilm</code>, the gap shouldn't be that significant.</li>
        <li>This article compares UniLM and Decoder-only trained from scratch under same tasks and data (directly comparing pre-training results without fine-tuning on other tasks); whereas the T5 paper compares results after pre-training on various tasks and then fine-tuning on downstream tasks. The processes are different—could this lead to the discrepancy in results?</li>
    </ol>

    <blockquote>
        <strong>Question 6:</strong> Does the faster drop in loss in the final experiment prove the model is better?
    </blockquote>
    <p><strong>Answer:</strong> Based on the number of steps I have trained so far, the mixed bidirectional/unidirectional attention has consistently performed better. I can only conjecture that this trend will continue. This is the limit of the experiments I can conduct currently. I look forward to interested readers with the necessary resources further experimenting to confirm or refute this conclusion.</p>

    <blockquote>
        <strong>Question 7:</strong> Regarding your statement "Comparing GPT with UniLM is considered a strict control of variables," I don't think that is quite accurate. The Google UL2 paper points out that for pre-trained language models, both the model architecture and the pre-training tasks play a key role in model quality.
    </blockquote>
    <p><strong>Answer:</strong> In this article, UniLM and GPT refer to two model architectures where only the Attention Mask is inconsistent. When performing the comparative experiments, except for the Attention Mask being inconsistent, all other details were aligned.</p>

    <blockquote>
        <strong>Question 8:</strong> Could there be another reason: that a lower-triangular or upper-triangular mask handles positional encoding information better?
    </blockquote>
    <p><strong>Answer:</strong> This is indeed a very novel perspective that I hadn't considered. In fact, beyond bringing an increase in rank, the triangular mask indeed brings advantages in positional identification. It breaks the permutation invariance of the Transformer and directly introduces a left-to-right order, so it works even without adding positional encodings. Perhaps both are contributing factors.</p>

    <h2>Summary</h2>
    <p>This article has answered some of the questions raised by readers regarding the previous post.</p>

    </article>
<hr>
<footer style="margin-top: 3em; padding: 1.5em; background: #f5f5f5; border-radius: 8px; font-size: 0.9em; color: #555;">
    <p style="margin: 0 0 0.5em 0;"><strong>Citation</strong></p>
    <p style="margin: 0 0 0.5em 0;">
        This is a machine translation of the original Chinese article:<br>
        <a href="https://kexue.fm/archives/9547" style="color: #005fcc;">https://kexue.fm/archives/9547</a>
    </p>
    <p style="margin: 0 0 0.5em 0;">
        Original author: 苏剑林 (Su Jianlin)<br>
        Original publication: <a href="https://kexue.fm" style="color: #005fcc;">科学空间 (Scientific Spaces)</a>
    </p>
    <p style="margin: 0; font-style: italic;">
        Translated using Gemini 3 Flash. Please refer to the original for authoritative content.
    </p>
</footer>
