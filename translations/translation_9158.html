
    <style type="text/css">

    body {
    margin: 48px auto;
    max-width: 68ch;              /* character-based width reads better */
    padding: 0 16px;

    font-family: -apple-system, BlinkMacSystemFont, "Segoe UI",
                Roboto, "Helvetica Neue", Arial, sans-serif;
    font-size: 18px;
    line-height: 1.65;
    color: #333;
    background: #fafafa;
    }

    h1, h2, h3, h4 {
    line-height: 1.25;
    margin-top: 2.2em;
    margin-bottom: 0.6em;
    font-weight: 600;
    }

    h1 {
    font-size: 2.1em;
    margin-top: 0;
    }

    h2 {
    font-size: 1.6em;
    border-bottom: 1px solid #e5e5e5;
    padding-bottom: 0.3em;
    }

    h3 {
    font-size: 1.25em;
    }

    h4 {
    font-size: 1.05em;
    color: #555;
    }

    /* Paragraphs and lists */
    p {
    margin: 1em 0;
    }

    ul, ol {
    margin: 1em 0 1em 1.5em;
    }

    li {
    margin: 0.4em 0;
    }

    a {
    color: #005fcc;
    text-decoration: none;
    }

    a:hover {
    text-decoration: underline;
    }

    code {
    font-family: ui-monospace, SFMono-Regular, Menlo, Consolas, monospace;
    font-size: 0.95em;
    background: #f2f2f2;
    padding: 0.15em 0.35em;
    border-radius: 4px;
    }

    pre {
    font-family: ui-monospace, SFMono-Regular, Menlo, Consolas, monospace;
    font-size: 0.9em;
    background: #f5f5f5;
    padding: 1em 1.2em;
    overflow-x: auto;
    border-radius: 6px;
    line-height: 1.45;
    }

    pre code {
    background: none;
    padding: 0;
    }

    blockquote {
    margin: 1.5em 0;
    padding-left: 1em;
    border-left: 4px solid #ddd;
    color: #555;
    }

    hr {
    border: none;
    border-top: 1px solid #e0e0e0;
    margin: 3em 0;
    }

    table {
    border-collapse: collapse;
    margin: 1.5em 0;
    width: 100%;
    font-size: 0.95em;
    }

    th, td {
    padding: 0.5em 0.7em;
    border-bottom: 1px solid #e5e5e5;
    text-align: left;
    }

    th {
    font-weight: 600;
    }

    img {
    max-width: 100%;
    display: block;
    margin: 1.5em auto;
    }

    small {
    color: #666;
    }

    mjx-container {
    margin: 1em 0;
    }

    ::selection {
    background: #cce2ff;
    }

    </style>
    

<script>
  window.MathJax = {
    tex: {
      inlineMath: [['$', '$'], ['\\(', '\\)']],
      displayMath: [['$$', '$$'], ['\\[', '\\]']],
      tags: 'ams',
      packages: {'[+]': ['ams']}
    }
  };
</script>
<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

<h1><a href="https://kexue.fm/archives/9158">An Unsuccessful Attempt: Generalizing Multi-label Cross-Entropy to "n-way m-class" Classification</a></h1>

<p>By 苏剑林 | July 15, 2022</p>

<p>Some readers might have noticed that this update has been delayed for quite a while. In fact, I started preparing this article last weekend. However, I underestimated the difficulty of this problem; I spent nearly a whole week on derivations and still haven't reached a perfect result. What I am posting now is still a failed attempt, and I hope experienced readers can provide some guidance.</p>

<p>In the article <a href="translation_7359.html">"Generalizing 'Softmax + Cross-Entropy' to Multi-label Classification"</a>, we proposed a multi-label classification loss function that automatically adjusts for the imbalance between positive and negative classes. Later, in <a href="translation_9064.html">"The Soft Label Version of Multi-label 'Softmax + Cross-Entropy'"</a>, we further derived its "soft label" version. Essentially, multi-label classification is an "n-way 2-class" problem. Consequently, what would the loss function for an "n-way m-class" problem look like?</p>

<p>This is the question that this article intends to explore.</p>

<h2>Analogy Attempt</h2>

<p>In the soft label generalization article <a href="translation_9064.html">"The Soft Label Version of Multi-label 'Softmax + Cross-Entropy'"</a>, we obtained the final result by directly applying a first-order truncation within the $\log$ of the "n-way 2-class" sigmoid cross-entropy loss. The same process can indeed be generalized to the "n-way m-class" softmax cross-entropy loss. This was my first attempt.</p>

<p>Let $\text{softmax}(s_{i,j}) = \frac{e^{s_{i,j}}}{\sum\limits_j e^{s_{i,j}}}$, where $s_{i,j}$ are the predicted scores and $t_{i,j}$ are the labels. Then</p>

\begin{equation}\begin{aligned}-\sum_i\sum_j t_{i,j}\log \text{softmax}(s_{i,j}) =& \,\sum_i\sum_j t_{i,j}\log \left(1 + \sum_{k\neq j} e^{s_{i,k} - s_{i,j}}\right)\\
=& \,\sum_j \log \prod_i\left(1 + \sum_{k\neq j} e^{s_{i,k} - s_{i,j}}\right)^{t_{i,j}}\\
=& \,\sum_j \log \left(1 + \sum_i t_{i,j}\sum_{k\neq j} e^{s_{i,k} - s_{i,j}}+\cdots\right)\\
\end{aligned}\end{equation}

<p>The summation over $i$ defaults to $1 \sim n$, and over $j$ defaults to $1 \sim m$. Truncating the higher-order terms $\cdots$, we get</p>

\begin{equation}l = \sum_j \log \left(1 + \sum_{i,k\neq j} t_{i,j}e^{- s_{i,j} + s_{i,k}}\right)\label{eq:loss-1}\end{equation}

<p>This is the loss I initially obtained, and it is a natural generalization of the previous result to "n-way m-class". In fact, if $t_{i,j}$ are hard labels, this loss is essentially fine. However, I hoped that, like in <a href="translation_9064.html">"The Soft Label Version of Multi-label 'Softmax + Cross-Entropy'"</a>, I could derive a corresponding analytical solution for soft labels. To this end, I took its derivative:</p>

\begin{equation}\frac{\partial l}{\partial s_{i,j}} = \frac{- t_{i,j}e^{- s_{i,j}}\sum\limits_{k\neq j} e^{s_{i,k}}}{1 + \sum\limits_{i,k\neq j} t_{i,j}e^{- s_{i,j} + s_{i,k}}} + \sum_{h\neq j} \frac{t_{i,h}e^{- s_{i,h}}e^{s_{i,j}}}{1 + \sum\limits_{i,k\neq h} t_{i,h}e^{- s_{i,h} + s_{i,k}}}\end{equation}

<p>A so-called analytical solution would be found by solving the equation $\frac{\partial l}{\partial s_{i,j}}=0$. However, I tried for several days and could not solve the equation. I suspect there is no simple explicit solution. Therefore, the first attempt failed.</p>

<h2>Back-derivation from Results</h2>

<p>After trying for several days without success, I thought from the opposite perspective: since the results derived directly by analogy cannot be solved, I might as well work backward from the result—that is, first determine what the solution should be and then reverse-engineer what the equation should look like. Thus, I began my second attempt.</p>

<p>First, I observed that the original multi-label loss, or the loss obtained earlier in Eq. $\eqref{eq:loss-1}$, both take the following form:</p>

\begin{equation}l = \sum_j \log \left(1 + \sum_i t_{i,j}e^{- f(s_{i,j})}\right)\label{eq:loss-2}\end{equation}

<p>We take this form as our starting point and compute the derivative:</p>

\begin{equation}\frac{\partial l}{\partial s_{i,k}} = \sum_j \frac{- t_{i,j}e^{- f(s_{i,j})}\frac{\partial f(s_{i,j})}{\partial s_{i,k}}}{1 + \sum\limits_i t_{i,j}e^{- f(s_{i,j})}}\end{equation}

<p>We hope that $t_{i,j}=\text{softmax}(f(s_{i,j}))=e^{f(s_{i,j})}/Z_i$ is the analytical solution for $\frac{\partial l}{\partial s_{i,k}}=0$, where $Z_i=\sum\limits_j e^{f(s_{i,j})}$. Substituting this in, we get</p>

\begin{equation}0=\frac{\partial l}{\partial s_{i,k}} = \sum_j \frac{- (1/Z_i)\frac{\partial f(s_{i,j})}{\partial s_{i,k}}}{1 + \sum\limits_i 1/Z_i} = \frac{- (1/Z_i)\frac{\partial \left(\sum\limits_j f(s_{i,j})\right)}{\partial s_{i,k}}}{1 + \sum\limits_i 1/Z_i}\end{equation}

<p>So, for the above equation to hold naturally, we find we only need to make $\sum\limits_j f(s_{i,j})$ equal to a constant independent of $i$ and $j$. For simplicity, let</p>

\begin{equation}f(s_{i,j})=s_{i,j}- \bar{s}_i,\qquad \bar{s}_i=\frac{1}{m}\sum_j s_{i,j}\end{equation}

<p>This naturally gives $\sum\limits_j f(s_{i,j})=0$. The corresponding optimization target is</p>

\begin{equation}l = \sum_j \log \left(1 + \sum_i t_{i,j}e^{- s_{i,j} + \bar{s}_i}\right)\label{eq:loss-3}\end{equation}

<p>Since $\bar{s}_i$ does not affect the normalization result, its theoretical optimal solution is $t_{i,j}=\text{softmax}(s_{i,j})$.</p>

<p>However, while it looks promising, its actual performance is quite poor. Although $t_{i,j}=\text{softmax}(s_{i,j})$ is indeed the theoretical optimal solution, in practice, the performance gets worse as the labels approach hard labels. This is because we know that for the loss in Eq. $\eqref{eq:loss-3}$, as long as $s_{i,j} \gg \bar{s}_i$, the loss will be very close to 0. To satisfy $s_{i,j} \gg \bar{s}_i$, $s_{i,j}$ does not necessarily have to be the maximum among $s_{i,1},s_{i,2},\cdots,s_{i,m}$, which fails to achieve the classification goal.</p>

<h2>Thinking and Analysis</h2>

<p>We now have two results. Eq. $\eqref{eq:loss-1}$ is an analogical generalization of the original multi-label cross-entropy. It performs reasonably well in the case of hard labels; however, because the analytical solution for soft labels cannot be found, the soft label case cannot be theoretically evaluated. Eq. $\eqref{eq:loss-3}$ is theoretically reverse-engineered from the result. Theoretically, its analytical solution is a simple softmax, but due to the limitations of practical optimization algorithms, its performance with hard labels is usually very poor, and it cannot even guarantee that the target logits are the maximum values. Notably, when $m=2$, both Eq. $\eqref{eq:loss-1}$ and Eq. $\eqref{eq:loss-3}$ can degenerate back into the multi-label cross-entropy.</p>

<p>We know that multi-label cross-entropy automatically regulates the problem of positive and negative sample imbalance. Similarly, although we have not yet obtained a perfect generalization, theoretically, generalizing to "n-way m-class" should still automatically regulate the imbalance between the $m$ classes. How does this balancing mechanism work? It is not difficult to understand. Whether in the analogical generalization of Eq. $\eqref{eq:loss-1}$ or the general assumption of Eq. $\eqref{eq:loss-2}$, the summation over $i$ is placed inside the $\log$. Originally, the loss contribution of each class was roughly proportional to "the number of samples in that class". By moving the summation inside the $\log$, the loss contribution of each class becomes roughly equal to "the logarithm of the number of samples in that class", thereby narrowing the loss gap between classes and automatically alleviating the imbalance problem.</p>

<p>Regrettably, this article has not yet reached a perfect generalization for "n-way m-class"—one that should possess two characteristics: 1. the ability to automatically regulate class imbalance through the $\log$ method; 2. the ability to derive an analytical solution for soft labels. For hard labels, using Eq. $\eqref{eq:loss-1}$ directly should be sufficient; but for soft labels, I am truly at a loss. I welcome interested readers to think about and discuss this together.</p>

<h2>Article Summary</h2>

<p>This article attempted to generalize previous multi-label cross-entropy to "n-way m-class" classification. Unfortunately, this generalization was not entirely successful. I am sharing the results here for the time being, hoping that interested readers can participate in improving them.</p>
<hr>
<footer style="margin-top: 3em; padding: 1.5em; background: #f5f5f5; border-radius: 8px; font-size: 0.9em; color: #555;">
    <p style="margin: 0 0 0.5em 0;"><strong>Citation</strong></p>
    <p style="margin: 0 0 0.5em 0;">
        This is a machine translation of the original Chinese article:<br>
        <a href="translation_9158.html" style="color: #005fcc;">https://kexue.fm/archives/9158</a>
    </p>
    <p style="margin: 0 0 0.5em 0;">
        Original author: 苏剑林 (Su Jianlin)<br>
        Original publication: <a href="https://kexue.fm" style="color: #005fcc;">科学空间 (Scientific Spaces)</a>
    </p>
    <p style="margin: 0; font-style: italic;">
        Translated using Gemini 3 Flash. Please refer to the original for authoritative content.
    </p>
</footer>
