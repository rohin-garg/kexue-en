
    <style type="text/css">

    body {
    margin: 48px auto;
    max-width: 68ch;              /* character-based width reads better */
    padding: 0 16px;

    font-family: -apple-system, BlinkMacSystemFont, "Segoe UI",
                Roboto, "Helvetica Neue", Arial, sans-serif;
    font-size: 18px;
    line-height: 1.65;
    color: #333;
    background: #fafafa;
    }

    h1, h2, h3, h4 {
    line-height: 1.25;
    margin-top: 2.2em;
    margin-bottom: 0.6em;
    font-weight: 600;
    }

    h1 {
    font-size: 2.1em;
    margin-top: 0;
    }

    h2 {
    font-size: 1.6em;
    border-bottom: 1px solid #e5e5e5;
    padding-bottom: 0.3em;
    }

    h3 {
    font-size: 1.25em;
    }

    h4 {
    font-size: 1.05em;
    color: #555;
    }

    /* Paragraphs and lists */
    p {
    margin: 1em 0;
    }

    ul, ol {
    margin: 1em 0 1em 1.5em;
    }

    li {
    margin: 0.4em 0;
    }

    a {
    color: #005fcc;
    text-decoration: none;
    }

    a:hover {
    text-decoration: underline;
    }

    code {
    font-family: ui-monospace, SFMono-Regular, Menlo, Consolas, monospace;
    font-size: 0.95em;
    background: #f2f2f2;
    padding: 0.15em 0.35em;
    border-radius: 4px;
    }

    pre {
    font-family: ui-monospace, SFMono-Regular, Menlo, Consolas, monospace;
    font-size: 0.9em;
    background: #f5f5f5;
    padding: 1em 1.2em;
    overflow-x: auto;
    border-radius: 6px;
    line-height: 1.45;
    }

    pre code {
    background: none;
    padding: 0;
    }

    blockquote {
    margin: 1.5em 0;
    padding-left: 1em;
    border-left: 4px solid #ddd;
    color: #555;
    }

    hr {
    border: none;
    border-top: 1px solid #e0e0e0;
    margin: 3em 0;
    }

    table {
    border-collapse: collapse;
    margin: 1.5em 0;
    width: 100%;
    font-size: 0.95em;
    }

    th, td {
    padding: 0.5em 0.7em;
    border-bottom: 1px solid #e5e5e5;
    text-align: left;
    }

    th {
    font-weight: 600;
    }

    img {
    max-width: 100%;
    display: block;
    margin: 1.5em auto;
    }

    small {
    color: #666;
    }

    mjx-container {
    margin: 1em 0;
    }

    ::selection {
    background: #cce2ff;
    }

    </style>
    

<script>
window.MathJax = {
  tex: {
    inlineMath: [['$', '$'], ['\\(', '\\)']],
    displayMath: [['$$', '$$'], ['\\[', '\\]']],
    processEscapes: true,
    tags: 'ams',
    packages: {'[+]': ['ams']}
  }
};
</script>
<script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js" id="MathJax-script" async></script>

<article>
<nav style="margin-bottom: 1.5em;">
    <a href="../index.html" style="display: inline-flex; align-items: center; color: #555; text-decoration: none; font-size: 0.95em;">
        <span style="margin-right: 0.3em;">&larr;</span> Back to Index
    </a>
</nav>

<h1><a href="https://kexue.fm/archives/9085">Constructing Discrete Probability Distributions from a Reparameterization Perspective</a></h1>

<p>By 苏剑林 | May 25, 2022</p>

<p>Generally, neural network outputs are unconstrained, meaning their range is $\mathbb{R}$. To obtain constrained outputs, activation functions are typically employed. For instance, if we want to output a probability distribution representing the probability of each category, we usually add Softmax as the activation function at the end. A subsequent question arises: besides Softmax, are there any other operations that can generate a probability distribution?</p>

<p>In <a href="translation_6705.html">"A Discussion on Reparameterization: From Normal Distribution to Gumbel Softmax"</a>, we introduced the reparameterization operation of Softmax. This article reverses that process—that is, we first define a reparameterization operation and then derive the corresponding probability distribution, thereby obtaining a new perspective on constructing probability distributions.</p>

<h2>Problem Definition</h2>

<p>Assume the output vector of the model is $\boldsymbol{\mu}=[\mu_1,\cdots,\mu_n]\in\mathbb{R}^n$. Without loss of generality, we assume that the $\mu_i$ are distinct. We hope to transform $\boldsymbol{\mu}$ into an $n$-dimensional probability distribution $\boldsymbol{p}=[p_1,\cdots,p_n]$ through some transformation $\mathcal{T}$, while maintaining certain properties. For example, the most basic requirements are:</p>

\begin{equation}{\color{red}1.}\,p_i\geq 0 \qquad {\color{red}2.}\,\sum_i p_i = 1 \qquad {\color{red}3.}\,p_i \geq p_j \Leftrightarrow \mu_i \geq \mu_j\end{equation}

<p>Of course, these requirements are trivial. As long as $f$ is a monotonic function from $\mathbb{R} \mapsto \mathbb{R}^+$ (for Softmax, $f(x)=e^x$), then the transformation</p>

\begin{equation}p_i = \frac{f(\mu_i)}{\sum\limits_j f(\mu_j)}\end{equation}

<p>can satisfy the above requirements. Next, we add a less trivial condition:</p>

\begin{equation}{\color{red}4.}\, \mathcal{T}(\boldsymbol{\mu}) = \mathcal{T}(\boldsymbol{\mu} + c\boldsymbol{1})\quad (\forall c \in \mathbb{R})\end{equation}

<p>Where $\boldsymbol{1}$ represents the all-ones vector, and $c$ is an arbitrary constant. That is to say, after adding a constant to each component of $\boldsymbol{\mu}$, the result of the transformation remains unchanged. This condition is proposed because adding a constant to each component does not change the result of $\mathop{\text{argmax}}$, and it is best if $\mathcal{T}$ maintains properties as similar to it as possible. It is easy to verify that Softmax satisfies this condition; however, besides Softmax, it seems difficult to think of other transformations.</p>

<h2>Noise Perturbation</h2>

<p>Most interestingly, we can use the inverse process of reparameterization to construct such a transformation! Suppose $\boldsymbol{\varepsilon}=[\varepsilon_1,\cdots,\varepsilon_n]$ is a vector obtained by independently and identically sampling $n$ times from a distribution $p(\varepsilon)$. Since $\boldsymbol{\varepsilon}$ is random, $\mathop{\text{argmax}}(\boldsymbol{\mu}+\boldsymbol{\varepsilon})$ is also generally random. Thus, we can define the transformation $\mathcal{T}$ through:</p>

\begin{equation}p_i = P[\mathop{\text{argmax}}(\boldsymbol{\mu}+\boldsymbol{\varepsilon})=i]\end{equation}

<p>Since the $\boldsymbol{\varepsilon}$ are independent and identically distributed, and the entire definition only relates to $\mathop{\text{argmax}}(\boldsymbol{\mu}+\boldsymbol{\varepsilon})$—which only involves the relative magnitudes of each component—the defined transformation must satisfy the four aforementioned conditions.</p>

<p>We can also judge the properties it satisfies by directly calculating the form of $p_i$. Specifically, $\mathop{\text{argmax}}(\boldsymbol{\mu}+\boldsymbol{\varepsilon})=i$ means</p>

\begin{equation}\mu_i + \varepsilon_i > \mu_j + \varepsilon_j\quad (\forall j\neq i)\end{equation}

<p>which is $\mu_i - \mu_j + \varepsilon_i > \varepsilon_j$. Obviously, the larger $\mu_i$ is, the greater the probability that this expression holds; that is, the larger $\mu_i$ is, the larger the corresponding $p_i$ is, which is Condition 3. Specifically, for a fixed $\varepsilon_i$, the probability that this condition holds is</p>

\begin{equation}\int_{-\infty}^{\mu_i - \mu_j + \varepsilon_i} p(\varepsilon_j)d\varepsilon_j = \Phi(\mu_i - \mu_j + \varepsilon_i)\end{equation}

<p>Here $\Phi$ is the cumulative distribution function (Cumulative Distribution Function) of $p(\varepsilon)$. Since each $\varepsilon_j$ is independent and identically distributed, we can directly multiply the probabilities together:</p>

\begin{equation}\prod_{j\neq i} \Phi(\mu_i - \mu_j + \varepsilon_i)\end{equation}

<p>This is the probability that $\mathop{\text{argmax}}(\boldsymbol{\mu}+\boldsymbol{\varepsilon})=i$ for a fixed $\varepsilon_i$. Finally, we only need to take the expectation over $\varepsilon_i$ to get $p_i$:</p>

\begin{equation}p_i = \int_{-\infty}^{\infty} p(\varepsilon_i)\left[\prod_{j\neq i} \Phi(\mu_i - \mu_j + \varepsilon_i)\right]d\varepsilon_i \label{eq:pi}\end{equation}

<p>From the expression of $p_i$, we can see that it only depends on the relative values $\mu_i - \mu_j$, so it clearly satisfies condition 4 in the definition.</p>

<h2>Reviewing the Old to Know the New</h2>

<p>Comparing the introduction to Gumbel Max in <a href="translation_6705.html">"A Discussion on Reparameterization: From Normal Distribution to Gumbel Softmax"</a>, we can see that the above derivation is exactly the reverse of reparameterization; it first defines the reparameterization method and then backward-derives the corresponding probability distribution.</p>

<p>Now we can re-examine our previous results: when the noise distribution is taken as the <a href="https://en.wikipedia.org/wiki/Gumbel_distribution">Gumbel distribution</a>, will equation $\eqref{eq:pi}$ yield the conventional Softmax operation? Gumbel noise is transformed from $u\sim U[0,1]$ via $\varepsilon = -\log(-\log u)$. Since the distribution of $u$ is exactly $U[0,1]$, solving for $u=e^{-e^{-\varepsilon}}$ gives the cumulative distribution function of the Gumbel distribution, i.e., $\Phi(\varepsilon)=e^{-e^{-\varepsilon}}$, and $p(\varepsilon)$ is the derivative of $\Phi(\varepsilon)$, i.e., $p(\varepsilon)=\Phi'(\varepsilon)=e^{-\varepsilon-e^{-\varepsilon}}$.</p>

<p>Substituting the above results into equation $\eqref{eq:pi}$ gives</p>

\begin{equation}\begin{aligned}
p_i =&\, \int_{-\infty}^{\infty} e^{-\varepsilon_i-e^{-\varepsilon_i}} e^{-\sum\limits_{j\neq i}e^{-\varepsilon_i + \mu_j - \mu_i}} d\varepsilon_i \\
=&\, \int_{-\infty}^0 e^{-e^{-\varepsilon_i}\left(1+\sum\limits_{j\neq i}e^{\mu_j - \mu_i}\right)} d(-e^{-\varepsilon_i}) \\
=&\, \int_{-\infty}^0 e^{t\left(1+\sum\limits_{j\neq i}e^{\mu_j - \mu_i}\right)} dt\\
=&\, \frac{1}{1+\sum\limits_{j\neq i}e^{\mu_j - \mu_i}} = \frac{e^{\mu_i}}{\sum\limits_j e^{\mu_j }}
\end{aligned}\end{equation}

<p>This is exactly Softmax. Thus, we have once again verified the correspondence between Gumbel Max and Softmax.</p>

<h2>Numerical Computation</h2>

<p>Being able to solve for an analytical solution like Softmax, as with the Gumbel distribution, is extremely rare; at least for now, the author cannot find a second case. Therefore, in most cases, we can only use numerical methods to approximate equation $\eqref{eq:pi}$. Since $p(\varepsilon)=\Phi'(\varepsilon)$, we can directly use substitution to get:</p>

\begin{equation}p_i = \int_0^1 \left[\prod_{j\neq i} \Phi(\mu_i - \mu_j + \varepsilon_i)\right]d\Phi(\varepsilon_i)\end{equation}

<p>Let $t=\Phi(\varepsilon_i)$, then</p>

\begin{equation}\begin{aligned}
p_i =&\, \int_0^1 \left[\prod_{j\neq i} \Phi(\mu_i - \mu_j + \Phi^{-1}(t))\right]dt \\
\approx&\, \frac{1}{K}\sum_{k=1}^K\prod_{j\neq i} \Phi\left(\mu_i - \mu_j + \Phi^{-1}\left(\frac{k}{K+1}\right)\right)
\end{aligned}\end{equation}

<p>where $\Phi^{-1}$ is the inverse function of $\Phi$, also known in probability as the Quantile Function (or Percent Point Function, etc.).</p>

<p>From the formula above, it can be seen that as long as we know the analytical expression for $\Phi$, we can perform an approximate calculation of $p_i$. Note that we do not need to know the analytical expression for $\Phi^{-1}$, because the results of the sampling points $\Phi^{-1}\left(\frac{k}{K+1}\right)$ can be pre-calculated using other numerical methods.</p>

<p>Taking the standard normal distribution as an example, $\Phi(x)=\frac{1}{2} \left(1+\text{erf}\left(\frac{x}{\sqrt{2}}\right)\right)$, and mainstream deep learning frameworks basically all come with an builtin $\text{erf}$ function, so the calculation of $\Phi(x)$ is no problem; as for $\Phi^{-1}\left(\frac{k}{K+1}\right)$, we can pre-calculate it using <code>scipy.stats.norm.ppf</code>. Therefore, when $\boldsymbol{\varepsilon}$ is sampled from a standard normal distribution, the calculation of $p_i$ is not an issue in mainstream deep learning frameworks.</p>

<h2>Summary</h2>

<p>This article generalizes Softmax from the perspective of reparameterization, obtaining a class of probability normalization methods with similar properties.</p>
</article>
<hr>
<footer style="margin-top: 3em; padding: 1.5em; background: #f5f5f5; border-radius: 8px; font-size: 0.9em; color: #555;">
    <p style="margin: 0 0 0.5em 0;"><strong>Citation</strong></p>
    <p style="margin: 0 0 0.5em 0;">
        This is a machine translation of the original Chinese article:<br>
        <a href="https://kexue.fm/archives/9085" style="color: #005fcc;">https://kexue.fm/archives/9085</a>
    </p>
    <p style="margin: 0 0 0.5em 0;">
        Original author: 苏剑林 (Su Jianlin)<br>
        Original publication: <a href="https://kexue.fm" style="color: #005fcc;">科学空间 (Scientific Spaces)</a>
    </p>
    <p style="margin: 0; font-style: italic;">
        Translated using Gemini 3 Flash. Please refer to the original for authoritative content.
    </p>
</footer>
