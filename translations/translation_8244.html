
    <style type="text/css">

    body {
    margin: 48px auto;
    max-width: 68ch;              /* character-based width reads better */
    padding: 0 16px;

    font-family: -apple-system, BlinkMacSystemFont, "Segoe UI",
                Roboto, "Helvetica Neue", Arial, sans-serif;
    font-size: 18px;
    line-height: 1.65;
    color: #333;
    background: #fafafa;
    }

    h1, h2, h3, h4 {
    line-height: 1.25;
    margin-top: 2.2em;
    margin-bottom: 0.6em;
    font-weight: 600;
    }

    h1 {
    font-size: 2.1em;
    margin-top: 0;
    }

    h2 {
    font-size: 1.6em;
    border-bottom: 1px solid #e5e5e5;
    padding-bottom: 0.3em;
    }

    h3 {
    font-size: 1.25em;
    }

    h4 {
    font-size: 1.05em;
    color: #555;
    }

    /* Paragraphs and lists */
    p {
    margin: 1em 0;
    }

    ul, ol {
    margin: 1em 0 1em 1.5em;
    }

    li {
    margin: 0.4em 0;
    }

    a {
    color: #005fcc;
    text-decoration: none;
    }

    a:hover {
    text-decoration: underline;
    }

    code {
    font-family: ui-monospace, SFMono-Regular, Menlo, Consolas, monospace;
    font-size: 0.95em;
    background: #f2f2f2;
    padding: 0.15em 0.35em;
    border-radius: 4px;
    }

    pre {
    font-family: ui-monospace, SFMono-Regular, Menlo, Consolas, monospace;
    font-size: 0.9em;
    background: #f5f5f5;
    padding: 1em 1.2em;
    overflow-x: auto;
    border-radius: 6px;
    line-height: 1.45;
    }

    pre code {
    background: none;
    padding: 0;
    }

    blockquote {
    margin: 1.5em 0;
    padding-left: 1em;
    border-left: 4px solid #ddd;
    color: #555;
    }

    hr {
    border: none;
    border-top: 1px solid #e0e0e0;
    margin: 3em 0;
    }

    table {
    border-collapse: collapse;
    margin: 1.5em 0;
    width: 100%;
    font-size: 0.95em;
    }

    th, td {
    padding: 0.5em 0.7em;
    border-bottom: 1px solid #e5e5e5;
    text-align: left;
    }

    th {
    font-weight: 600;
    }

    img {
    max-width: 100%;
    display: block;
    margin: 1.5em auto;
    }

    small {
    color: #666;
    }

    mjx-container {
    margin: 1em 0;
    }

    ::selection {
    background: #cce2ff;
    }

    </style>
    

<script>
window.MathJax = {
  tex: {
    inlineMath: [['$', '$'], ['\\(', '\\)']],
    displayMath: [['$$', '$$'], ['\\[', '\\]']],
    tags: 'ams',
    packages: {'[+]': ['ams']}
  }
};
</script>
<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

<article>
    <h1><a href="https://kexue.fm/archives/8244">The Success of WGAN Might Have Nothing to Do with Wasserstein Distance</a></h1>

    <p>By 苏剑林 | March 15, 2021</p>

    <p>WGAN, or Wasserstein GAN, is considered a major theoretical breakthrough in the history of GANs. It transformed the measure of the two probability distributions in GANs from f-divergence to Wasserstein distance, making the training process of WGAN more stable and generally resulting in better generation quality. Wasserstein distance is related to Optimal Transport and belongs to the class of Integral Probability Metrics (IPMs). These probability measures usually possess superior theoretical properties, which is why the emergence of WGAN attracted many researchers to understand and study GAN models through the lens of Optimal Transport and IPMs.</p>

    <p>However, a recent paper on arXiv, <a href="https://papers.cool/arxiv/2103.01678">"Wasserstein GANs Work Because They Fail (to Approximate the Wasserstein Distance)"</a>, points out that although WGAN was derived from the theory of Wasserstein distance, successful WGANs today do not actually approximate the Wasserstein distance well. On the contrary, if we perform a better approximation of the Wasserstein distance, the performance actually gets worse. In fact, I have long had this doubt—that Wasserstein distance itself does not inherently manifest a necessity to improve GAN performance. The conclusion of this paper confirms this suspicion, meaning the reason GANs succeed remains quite enigmatic.</p>

    <h2>Basics and Review</h2>

    <p>This article is a discussion of the WGAN training process and is not intended as an introductory piece. For beginners, please refer to <a href="translation_4439.html">"The Art of Mutual Confrontation: From Zero to WGAN-GP"</a>. For the connection between f-divergence and GANs, see <a href="translation_6016.html">"Introduction to f-GAN: The Production Workshop of GAN Models"</a> and <a href="translation_7210.html">"Designing GANs: Another GAN Production Workshop"</a>. For the theoretical derivation of WGAN, see <a href="translation_6280.html">"From Wasserstein Distance and Duality Theory to WGAN"</a>. For an analysis of the GAN training process, you can also refer to <a href="translation_6583.html">"Optimization Algorithms from a Dynamical Perspective (IV): The Third Stage of GAN"</a>.</p>

    <p>Generally speaking, GAN corresponds to a min-max process:</p>
    $$ \min_G \max_D \mathcal{L}(G, D) $$
    <p>Of course, the loss functions for the discriminator and the generator might be different in practice, but the above form is sufficiently representative. The original GAN is usually called vanilla GAN, and its form is:</p>
    \begin{equation}\min_G \max_D \mathbb{E}_{x\sim p(x)}\left[\log D(x)\right] + \mathbb{E}_{z\sim q(z)}\left[\log (1 - D(G(z)))\right]\label{eq:v-gan}\end{equation}
    <p>As proved in <a href="https://papers.cool/arxiv/1701.04862">"Towards Principled Methods for Training Generative Adversarial Networks"</a>, <a href="https://zhuanlan.zhihu.com/p/25071913">"Amazing Wasserstein GAN"</a>, or related <a href="https://kexue.fm/tag/GAN/">GAN</a> articles on this blog, vanilla GAN is effectively minimizing the JS divergence between two distributions. JS divergence is a type of f-divergence, and all f-divergences share a problem: when two distributions have almost no overlap, the divergence is a constant. This implies the gradient is zero, and since we use gradient descent for optimization, it means we cannot optimize effectively. Thus, <a href="https://papers.cool/arxiv/1701.07875">WGAN</a> was born, utilizing Wasserstein distance to design a new GAN:</p>
    \begin{equation}\min_G \max_{\Vert D\Vert_{L}\leq 1} \mathbb{E}_{x\sim p(x)}\left[D(x)\right] - \mathbb{E}_{z\sim q(z)}\left[D(G(z))\right]\label{eq:w-gan}\end{equation}
    <p>The obvious difference from previous GANs is that WGAN explicitly adds a Lipschitz constraint $\Vert D\Vert_{L}\leq 1$ to the discriminator $D$. Since Wasserstein distance is well-defined for almost any two distributions (even if they don't overlap), WGAN theoretically solves problems like gradient vanishing and training instability found in traditional f-divergence-based GANs.</p>

    <p>There are two main schemes for adding the Lipschitz constraint to the discriminator: first is Spectral Normalization (SN), see <a href="translation_6051.html">"Lipschitz Constraints in Deep Learning: Generalization and Generative Models"</a> (now many GANs, not just WGAN, incorporate SN into the discriminator or even the generator for stability); second is Gradient Penalty (GP), which includes 1-centered penalty (WGAN-GP) and 0-centered penalty (WGAN-div), as seen in <a href="translation_6139.html">"WGAN-div: A Quiet WGAN Gap-Filler"</a>. Current results suggest that zero-centered penalty has better theoretical properties and effects.</p>

    <h2>Performance ≠ Approximation</h2>

    <p>The phenomenon that "WGAN does not approximate Wasserstein distance well" is not being noticed for the first time. For example, a 2019 paper <a href="https://papers.cool/arxiv/1910.03875">"How Well Do WGANs Estimate the Wasserstein Metric?"</a> systematically discussed this point. The paper highlighted here further identifies the link between WGAN performance and the degree of Wasserstein distance approximation through strictly controlled experiments.</p>

    <p>First, the paper compared the performance of Gradient Penalty (GP) with a method called c-transform in implementing WGAN. The c-transform method, also proposed in <a href="https://papers.cool/arxiv/1910.03875">"How Well Do WGANs Estimate the Wasserstein Metric?"</a>, approximates Wasserstein distance much better than Gradient Penalty. The following two figures illustrate this:</p>

    <p><img src="https://kexue.fm/usr/uploads/2021/03/549705654.png" alt="Static testing of WGAN-GP, c-transform and Wasserstein distance approximation" title="Static testing of WGAN-GP, c-transform and Wasserstein distance approximation" /><br />
    <em>Static testing of WGAN-GP, c-transform and Wasserstein distance approximation degree</em></p>

    <p><img src="https://kexue.fm/usr/uploads/2021/03/1460314291.png" alt="WGAN-GP, c-transform and Wasserstein distance approximation degree during training" title="WGAN-GP, c-transform and Wasserstein distance approximation degree during training" /><br />
    <em>WGAN-GP, c-transform and Wasserstein distance approximation degree during the training process</em></p>

    <p>However, the generation quality of c-transform is actually inferior to Gradient Penalty:</p>

    <p><img src="https://kexue.fm/usr/uploads/2021/03/4049472855.png" alt="Comparison of generation results between WGAN-GP and c-transform" title="Comparison of generation results between WGAN-GP and c-transform" /><br />
    <em>Comparison of generation results between WGAN-GP and c-transform</em></p>

    <p>Of course, the original author's choice of this specific image is somewhat comical; in fact, WGAN-GP results can be much better than the image on the right. Thus, we can temporarily conclude:</p>

    <blockquote>
        <p>1. Well-performing WGANs do not approximate Wasserstein distance well during training;<br />
        2. Better approximation of Wasserstein distance does not help improve generation performance.</p>
    </blockquote>

    <h2>Theory ≠ Experiment</h2>

    <p>Now let's consider where the problem lies. We know that whether it is vanilla GAN $\eqref{eq:v-gan}$, WGAN $\eqref{eq:w-gan}$, or other GANs, they share two common characteristics during experimentation:</p>

    <blockquote>
        <p>1. $\min$ and $\max$ are trained alternately;<br />
        2. Only a random batch is selected for training each time.</p>
    </blockquote>

    <p>What are the problems with these two points?</p>

    <p>First, almost all GANs are written as $\min_G \max_D$ because, theoretically, one needs to accurately complete $\max_D$ first and then perform $\min_G$ to be optimizing the probability measure corresponding to the GAN. If they are just optimized alternately, it is theoretically impossible to accurately approach the probability measure. Even though WGAN is not afraid of vanishing gradients because it uses Wasserstein distance—allowing for more discriminator steps (or higher learning rates for $D$) during alternating training—it still cannot precisely approximate Wasserstein distance. This is one source of the discrepancy.</p>

    <p>Second, training on a randomly sampled batch rather than the full set of training samples leads to a result where "the Wasserstein distance between two random batches in the training set is actually larger than the Wasserstein distance between a training set batch and its average sample," as shown in the figure below:</p>

    <p><img src="https://kexue.fm/usr/uploads/2021/03/280974430.png" alt="Left: batch of real samples, Middle: average sample, Right: sample cluster center. Looking at Wasserstein distance, real samples are worse than the later two blurry samples" title="Left: batch of real samples, Middle: average sample, Right: sample cluster center. Looking at Wasserstein distance, real samples are worse than the later two blurry samples" /><br />
    <em>Left: batch of real samples, Middle: average sample, Right: sample cluster center. In terms of Wasserstein distance, real samples are worse than the middle two blurry samples.</em></p>

    <p>This shows that in batch-based training, if you hope to obtain more realistic samples, you are certainly not optimizing the Wasserstein distance. If you are precisely optimizing the Wasserstein distance, you will not get realistic samples, because the Wasserstein distance for blurry average samples is actually smaller.</p>

    <h2>Math ≠ Vision</h2>

    <p>From a mathematical perspective, the properties of Wasserstein distance are indeed very elegant; in a sense, it is the best way to measure the gap between any two distributions. However, math is math, and the most "fatal" aspect of Wasserstein distance is that it depends on a specific metric:</p>
    \begin{equation}\mathcal{W}[p,q]=\inf_{\gamma\in \Pi[p,q]} \iint \gamma(x,y) d(x,y) dxdy\end{equation}
    <p>That is, we need to provide a function $d(x,y)$ that measures the gap between two samples. However, for many scenarios, such as two images, designing the metric function itself is the hardest of hard problems. WGAN directly uses Euclidean distance $d(x,y)=\Vert x - y\Vert_2$. While mathematically reasonable, it is visually unreasonable; two images our eyes perceive as similar do not necessarily have a smaller Euclidean distance. Therefore, if one approximates Wasserstein distance too accurately, it actually results in poorer visual quality. The original paper also conducted experiments showing that using c-transform to better approximate Wasserstein distance results in generation effects similar to K-Means cluster centers, and K-Means also happens to use Euclidean distance as its metric:</p>

    <p><img src="https://kexue.fm/usr/uploads/2021/03/910758894.png" alt="Similarity between c-transform results and K-Means" title="Similarity between c-transform results and K-Means" /><br />
    <em>Similarity between c-transform results and K-Means</em></p>

    <p>Thus, the reason for WGAN's success is now very mysterious: WGAN was derived based on Wasserstein distance, but its implementation has a gap from actual Wasserstein distance, and this gap might exactly be the key to WGAN's success. The original paper argues that the most critical aspect of WGAN is the introduction of the Lipschitz constraint. Adding a Lipschitz constraint (via Spectral Normalization or Gradient Penalty) to almost any GAN variant more or less improves performance and stability. Therefore, the Lipschitz constraint is the real point of improvement, rather than the imagined Wasserstein distance.</p>

    <p>But this is more of a conclusion than a theoretical analysis. It seems that deep understanding of GANs still has a long way to go~</p>

    <h2>A Simple Summary</h2>

    <p>This article primarily shared a recent paper pointing out that whether Wasserstein distance is approximated well has no necessary connection with WGAN's performance. How to better understand the theory and practice of GANs remains a difficult task~</p>
</article>
<hr>
<footer style="margin-top: 3em; padding: 1.5em; background: #f5f5f5; border-radius: 8px; font-size: 0.9em; color: #555;">
    <p style="margin: 0 0 0.5em 0;"><strong>Citation</strong></p>
    <p style="margin: 0 0 0.5em 0;">
        This is a machine translation of the original Chinese article:<br>
        <a href="https://kexue.fm/archives/8244" style="color: #005fcc;">https://kexue.fm/archives/8244</a>
    </p>
    <p style="margin: 0 0 0.5em 0;">
        Original author: 苏剑林 (Su Jianlin)<br>
        Original publication: <a href="https://kexue.fm" style="color: #005fcc;">科学空间 (Scientific Spaces)</a>
    </p>
    <p style="margin: 0; font-style: italic;">
        Translated using Gemini 3 Flash. Please refer to the original for authoritative content.
    </p>
</footer>
