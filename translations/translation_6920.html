
    <style type="text/css">

    body {
    margin: 48px auto;
    max-width: 68ch;              /* character-based width reads better */
    padding: 0 16px;

    font-family: -apple-system, BlinkMacSystemFont, "Segoe UI",
                Roboto, "Helvetica Neue", Arial, sans-serif;
    font-size: 18px;
    line-height: 1.65;
    color: #333;
    background: #fafafa;
    }

    h1, h2, h3, h4 {
    line-height: 1.25;
    margin-top: 2.2em;
    margin-bottom: 0.6em;
    font-weight: 600;
    }

    h1 {
    font-size: 2.1em;
    margin-top: 0;
    }

    h2 {
    font-size: 1.6em;
    border-bottom: 1px solid #e5e5e5;
    padding-bottom: 0.3em;
    }

    h3 {
    font-size: 1.25em;
    }

    h4 {
    font-size: 1.05em;
    color: #555;
    }

    /* Paragraphs and lists */
    p {
    margin: 1em 0;
    }

    ul, ol {
    margin: 1em 0 1em 1.5em;
    }

    li {
    margin: 0.4em 0;
    }

    a {
    color: #005fcc;
    text-decoration: none;
    }

    a:hover {
    text-decoration: underline;
    }

    code {
    font-family: ui-monospace, SFMono-Regular, Menlo, Consolas, monospace;
    font-size: 0.95em;
    background: #f2f2f2;
    padding: 0.15em 0.35em;
    border-radius: 4px;
    }

    pre {
    font-family: ui-monospace, SFMono-Regular, Menlo, Consolas, monospace;
    font-size: 0.9em;
    background: #f5f5f5;
    padding: 1em 1.2em;
    overflow-x: auto;
    border-radius: 6px;
    line-height: 1.45;
    }

    pre code {
    background: none;
    padding: 0;
    }

    blockquote {
    margin: 1.5em 0;
    padding-left: 1em;
    border-left: 4px solid #ddd;
    color: #555;
    }

    hr {
    border: none;
    border-top: 1px solid #e0e0e0;
    margin: 3em 0;
    }

    table {
    border-collapse: collapse;
    margin: 1.5em 0;
    width: 100%;
    font-size: 0.95em;
    }

    th, td {
    padding: 0.5em 0.7em;
    border-bottom: 1px solid #e5e5e5;
    text-align: left;
    }

    th {
    font-weight: 600;
    }

    img {
    max-width: 100%;
    display: block;
    margin: 1.5em auto;
    }

    small {
    color: #666;
    }

    mjx-container {
    margin: 1em 0;
    }

    ::selection {
    background: #cce2ff;
    }

    </style>
    

<script>
window.MathJax = {
  tex: {
    inlineMath: [['$', '$'], ['\\(', '\\)']],
    displayMath: [['$$', '$$'], ['\\[', '\\]']],
    processEscapes: true,
    tags: 'ams'
  }
};
</script>
<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

<h1><a href="https://kexue.fm/archives/6920">Rewriting the Previous New Word Discovery Algorithm: Faster and Better New Word Discovery</a></h1>

    <p>By 苏剑林 | September 09, 2019</p>


<p>New word discovery is one of the foundational tasks in NLP. It primarily aims to identify character sequences in a corpus that could potentially be "new words" through the unsupervised discovery of linguistic features (mainly statistical features). This site has published several articles on the topic of "new word discovery," such as: among these articles, the author believes the one with the most elegant theory is <a href="translation_3913.html">"Unsupervised Word Segmentation based on Language Models"</a>, while the one with better overall performance as a new word discovery algorithm is <a href="translation_3956.html">"A Better New Word Discovery Algorithm"</a>. This article is a reimplementation of the algorithm from that post.</p>

<h2>Background</h2>
<p>When I wrote <a href="translation_3956.html">"[Chinese Word Segmentation Series] 8. A Better New Word Discovery Algorithm"</a>, I already provided a basic implementation and verified it. However, the original version was written in pure Python and was only intended for quick verification of the effect, so it was written quite casually and suffered from serious efficiency issues. I recently recalled this and felt it would be a waste to neglect the algorithm, so I rewrote it. I utilized some tools and techniques to significantly increase the speed.</p>
<p>By the way, "new word discovery" is a colloquial term; a more accurate name should be "unsupervised lexicon construction," because in principle, it can fully build a dictionary rather than just "new words." Of course, you can compare it with a common dictionary and remove existing words to obtain new words.</p>

<h2>Details</h2>
<p>The main changes are as follows:</p>
<ol>
    <li>Used the <code>count_ngrams</code> program from the language modeling tool <b>kenlm</b> to count ngrams. Since kenlm is written in C++, its speed is guaranteed, and it has been optimized to be very memory-friendly.</li>
    <li>Used a <b>Trie tree</b> structure to accelerate the search for whether an ngram has appeared during the second pass of the vocabulary to get candidate words. Trie trees or their variants are essentially standard for all dictionary-based segmentation tools because they speed up searching for whether words from a dictionary appear in a string.</li>
</ol>

<h2>Usage</h2>
<p>The open-source address for this version is located at: <a href="https://github.com/bojone/word-discovery">https://github.com/bojone/word-discovery</a>. Note that this script should only be used on Linux systems. If you want to use it on Windows, you will likely need to make some modifications; as for what specific modifications are needed, I don't know, so please resolve that yourself.</p>
<p>Note that while the algorithm itself can theoretically apply to any language, the implementation in this article is, in principle, only suitable for languages where the "character" (字) is the basic unit.</p>
<p>Before deciding to use this library, it is recommended that the reader spend some time reading <a href="translation_3956.html">"[Chinese Word Segmentation Series] 8. A Better New Word Discovery Algorithm"</a> to gain a basic understanding of the algorithm steps, in order to better understand the usage steps below.</p>
<p>The core script on GitHub is <code>word_discovery.py</code>, which contains the complete implementation and usage examples. Let's briefly walk through the example.</p>
<p>First, write a generator for the corpus that returns the corpus line by line:</p>

<pre><code>import re
import glob

# Corpus generator, and preliminary preprocessing of the corpus
# The specific meaning of this generator example is not important; 
# you just need to know that it yields the text line by line.
def text_generator():
    txts = glob.glob('/root/thuctc/THUCNews/*/*.txt')
    for txt in txts:
        d = open(txt).read()
        d = d.decode('utf-8').replace(u'\u3000', ' ').strip()
        yield re.sub(u'[^\u4e00-\u9fa50-9a-zA-Z ]+', '\n', d)</code></pre>

<p>The reader does not need to understand exactly what this generator is doing; you only need to know that this generator is <b>yielding the raw corpus text line by line</b>. If you don't know how to write a generator, please learn it yourself. Please do not discuss questions like "what should the corpus format be" or "how do I change this to fit my corpus" within this article, thank you.</p>
<p>Incidentally, because it is unsupervised training, the corpus is generally "the bigger, the better"—several hundred MB to several GB is fine. But actually, if you only have a few MB of corpus (such as a novel), you can also test it directly and see basic results (though you may need to modify the parameters below).</p>
<p>Once you have the generator, configure some parameters, and then you can execute each step sequentially:</p>

<pre><code>min_count = 32
order = 4
corpus_file = 'thucnews.corpus' # Filename for saved corpus
vocab_file = 'thucnews.chars' # Filename for saved character set
ngram_file = 'thucnews.ngrams' # Filename for saved ngrams
output_file = 'thucnews.vocab' # Filename for final exported dictionary

write_corpus(text_generator(), corpus_file) # Save corpus as text
count_ngrams(corpus_file, order, vocab_file, ngram_file) # Use Kenlm to count ngrams
ngrams = KenlmNgrams(vocab_file, ngram_file, order, min_count) # Load ngrams
ngrams = filter_ngrams(ngrams.ngrams, ngrams.total, [0, 2, 4, 6]) # Filter ngrams</code></pre>

<p>Note that kenlm requires a space-separated, plain text format corpus as input, and the <code>write_corpus</code> function helps us do that. Then <code>count_ngrams</code> calls kenlm's <code>count_ngrams</code> program to count ngrams. Therefore, you need to compile kenlm yourself and place its <code>count_ngrams</code> executable in the same directory as <code>word_discovery.py</code>. If you have a Linux environment, compiling kenlm is quite simple; I have previously discussed kenlm <a href="translation_3942.html">here</a>, which you can refer to.</p>
<p>After <code>count_ngrams</code> is executed, the results will be saved in a binary file, which <code>KenlmNgrams</code> reads. If your input corpus is large, this step will require a significant amount of memory. Finally, <code>filter_ngrams</code> filters the ngrams. <code>[0, 2, 4, 6]</code> are the thresholds for mutual information, where the first 0 is meaningless and served as filler, while 2, 4, and 6 are the mutual information thresholds for 2-gram, 3-gram, and 4-gram respectively; generally, a monotonic increase is better.</p>
<p>At this point, we have completed all the "preparatory work." Now we can proceed to build the lexicon. First, build an ngram Trie tree, and then use this Trie tree to perform a basic "pre-segmentation":</p>

<pre><code>ngtrie = SimpleTrie() # Build the ngram Trie tree
for w in Progress(ngrams, 100000, desc=u'build ngram trie'):
    _ = ngtrie.add_word(w)

candidates = {} # Get candidate words
for t in Progress(text_generator(), 1000, desc='discovering words'):
    for w in ngtrie.tokenize(t): # Pre-segmentation
        candidates[w] = candidates.get(w, 0) + 1</code></pre>

<p>This pre-segmentation process was introduced in <a href="translation_3956.html">"[Chinese Word Segmentation Series] 8. A Better New Word Discovery Algorithm"</a>. In short, it is somewhat like maximum matching, where ngram fragments are connected into the longest possible candidate words.</p>
<p>Finally, filter the candidate words again, and you can save the dictionary:</p>

<pre><code># Frequency filtering
candidates = {i: j for i, j in candidates.items() if j &gt;= min_count}
# Mutual information filtering (backtracking)
candidates = filter_vocab(candidates, ngrams, order)
# Output result file
with open(output_file, 'w') as f:
    for i, j in sorted(candidates.items(), key=lambda s: -s[1]):
        s = '%s %s\n' % (i.encode('utf-8'), j)
        f.write(s)</code></pre>

<h2>Evaluation</h2>
<p>Readers have previously complained that the algorithms I write lack standard evaluations, so I performed a simple evaluation this time with the script <code>evaluate.py</code>.</p>
<p>Specifically, using THUCNews as the base corpus, I constructed a dictionary using the above script (taking approximately <b>40 minutes</b>). Retaining only the <b>top 50,000 words</b>, I loaded this 50,000-word dictionary into Jieba segmentation (not using its built-in dictionary and closing its new word discovery function). This forms a segmentation tool based on an unsupervised dictionary. I then used this tool to segment the test set provided by Bakeoff 2005 and evaluated it using its test script. The final score on the PKU test set was:</p>

$$\begin{array}{c|c|c} 
\hline 
\text{RECALL} & \text{PRECISION} & \text{F1}\\ 
\hline 
0.777 & 0.711 & 0.742\\ 
\hline\end{array}$$

<p>In other words, it can achieve an F1 score of 0.742. What level is this? An article from ICLR 2019 called <a href="https://openreview.net/forum?id=HkeS_nRqYm">"Unsupervised Word Discovery with Segmental Neural Language Models"</a> mentioned that its result on the same test set was $F1=0.731$. Looking at it this way, the result of this algorithm is not worse than the state-of-the-art results from top conferences. Readers can download the THUCNews corpus themselves to completely replicate the above results.</p>
<p>Additionally, more data can achieve better results. This is a dictionary I extracted from 5 million WeChat public account articles (totaling more than 20GB after saving as text): <a href="https://pan.baidu.com/s/1pL6AtE3">wx.vocab.zip</a>, for readers to use if needed. Retaining the top 50,000 words of this dictionary and performing the same evaluation, the F1 significantly exceeded the results of the top conference paper:</p>

$$\begin{array}{c|c|c} 
\hline 
\text{RECALL} & \text{PRECISION} & \text{F1}\\ 
\hline 
0.799 & 0.734 & 0.765\\ 
\hline\end{array}$$

<p>(Note: This is to provide an intuitive perception of the effect. The comparison may be unfair because I am not sure which corpora were used for the training set in that paper. But I feel that within the same amount of time, the algorithm in this article would be superior to the algorithm in the paper, because intuitively the paper's algorithm would be very slow to train. The authors also haven't open-sourced it, so there are many uncertainties. If there are any errors, please point them out.)</p>

<h2>Summary</h2>
<p>This article reimplements the new word discovery (lexicon construction) algorithm I previously proposed, primarily making speed optimizations and performing simple performance evaluations. However, the specific effect will still need to be fine-tuned by the reader during actual use.</p>
<p>I hope everyone enjoys using it! Enjoy it!</p>

<p>
    Article Address: <a href="https://kexue.fm/archives/6920">https://kexue.fm/archives/6920</a>
</p>
<hr>
<footer style="margin-top: 3em; padding: 1.5em; background: #f5f5f5; border-radius: 8px; font-size: 0.9em; color: #555;">
    <p style="margin: 0 0 0.5em 0;"><strong>Citation</strong></p>
    <p style="margin: 0 0 0.5em 0;">
        This is a machine translation of the original Chinese article:<br>
        <a href="https://kexue.fm/archives/6920" style="color: #005fcc;">https://kexue.fm/archives/6920</a>
    </p>
    <p style="margin: 0 0 0.5em 0;">
        Original author: 苏剑林 (Su Jianlin)<br>
        Original publication: <a href="https://kexue.fm" style="color: #005fcc;">科学空间 (Scientific Spaces)</a>
    </p>
    <p style="margin: 0; font-style: italic;">
        Translated using Gemini 3 Flash. Please refer to the original for authoritative content.
    </p>
</footer>
