
    <style type="text/css">

    body {
    margin: 48px auto;
    max-width: 68ch;              /* character-based width reads better */
    padding: 0 16px;

    font-family: -apple-system, BlinkMacSystemFont, "Segoe UI",
                Roboto, "Helvetica Neue", Arial, sans-serif;
    font-size: 18px;
    line-height: 1.65;
    color: #333;
    background: #fafafa;
    }

    h1, h2, h3, h4 {
    line-height: 1.25;
    margin-top: 2.2em;
    margin-bottom: 0.6em;
    font-weight: 600;
    }

    h1 {
    font-size: 2.1em;
    margin-top: 0;
    }

    h2 {
    font-size: 1.6em;
    border-bottom: 1px solid #e5e5e5;
    padding-bottom: 0.3em;
    }

    h3 {
    font-size: 1.25em;
    }

    h4 {
    font-size: 1.05em;
    color: #555;
    }

    /* Paragraphs and lists */
    p {
    margin: 1em 0;
    }

    ul, ol {
    margin: 1em 0 1em 1.5em;
    }

    li {
    margin: 0.4em 0;
    }

    a {
    color: #005fcc;
    text-decoration: none;
    }

    a:hover {
    text-decoration: underline;
    }

    code {
    font-family: ui-monospace, SFMono-Regular, Menlo, Consolas, monospace;
    font-size: 0.95em;
    background: #f2f2f2;
    padding: 0.15em 0.35em;
    border-radius: 4px;
    }

    pre {
    font-family: ui-monospace, SFMono-Regular, Menlo, Consolas, monospace;
    font-size: 0.9em;
    background: #f5f5f5;
    padding: 1em 1.2em;
    overflow-x: auto;
    border-radius: 6px;
    line-height: 1.45;
    }

    pre code {
    background: none;
    padding: 0;
    }

    blockquote {
    margin: 1.5em 0;
    padding-left: 1em;
    border-left: 4px solid #ddd;
    color: #555;
    }

    hr {
    border: none;
    border-top: 1px solid #e0e0e0;
    margin: 3em 0;
    }

    table {
    border-collapse: collapse;
    margin: 1.5em 0;
    width: 100%;
    font-size: 0.95em;
    }

    th, td {
    padding: 0.5em 0.7em;
    border-bottom: 1px solid #e5e5e5;
    text-align: left;
    }

    th {
    font-weight: 600;
    }

    img {
    max-width: 100%;
    display: block;
    margin: 1.5em auto;
    }

    small {
    color: #666;
    }

    mjx-container {
    margin: 1em 0;
    }

    ::selection {
    background: #cce2ff;
    }

    </style>
    

<script>
window.MathJax = {
  tex: {
    inlineMath: [['$', '$'], ['\\(', '\\)']],
    displayMath: [['$$', '$$'], ['\\[', '\\]']],
    processEscapes: true,
    tags: 'ams'
  }
};
</script>
<script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"></script>

<nav style="margin-bottom: 1.5em;">
    <a href="../index.html" style="display: inline-flex; align-items: center; color: #555; text-decoration: none; font-size: 0.95em;">
        <span style="margin-right: 0.3em;">&larr;</span> Back to Index
    </a>
</nav>

<h1><a href="https://kexue.fm/archives/5542">A Concise Introduction to Conditional Random Fields (CRF) (with a Pure Keras Implementation)</a></h1>

<p>By 苏剑林 | May 18, 2018</p>

<p>Last year, I wrote a blog post <a href="translation_4695.html">"CRF In A Nutshell"</a>, which introduced the Conditional Random Field (CRF) model in a somewhat rough manner. However, that article clearly had many deficiencies, such as being insufficiently clear and incomplete, and lacking an implementation. Here, we revisit this model to supplement and complete the relevant content.</p>

<p>This article provides a concise introduction to the basic principles of CRF. Of course, "concise" is relative; to truly understand CRF, it is inevitable to mention some formulas. Readers who only care about usage can skip directly to the end of the text.</p>

<h2>Illustration</h2>

<p>Following our previous line of thought, let's compare the similarities and differences between standard frame-wise softmax and CRF.</p>

<h3>Frame-wise softmax</h3>

<p>CRF is mainly used for sequence labeling problems, which can be simply understood as classifying each frame in a sequence. Since it is classification, it is natural to think of encoding the sequence using CNN or RNN and then connecting a fully connected layer with softmax activation, as shown below:</p>

<p style="text-align:center;"><img src="https://kexue.fm/usr/uploads/2018/05/3547432049.png" alt="Frame-wise softmax does not directly consider output context correlation" /><br /><em>Frame-wise softmax does not directly consider output context correlation</em></p>

<h3>Conditional Random Field</h3>

<p>However, when we design labels—for example, using four labels s, b, m, and e for word segmentation—the target output sequence itself carries certain contextual correlations. For instance, 's' cannot be followed by 'm' or 'e', and so on. Frame-wise softmax does not consider these contextual correlations at the output level, which means it places the burden of learning these correlations onto the encoding level, hoping the model can learn them on its own, which sometimes "asks too much" of the model.</p>

<p>CRF is more direct: it separates the correlations at the output level, allowing the model to learn more "comfortably":</p>

<p style="text-align:center;"><img src="https://kexue.fm/usr/uploads/2018/05/2338302195.png" alt="CRF explicitly considers contextual correlation at the output end" /><br /><em>CRF explicitly considers contextual correlation at the output end</em></p>

<h2>Mathematics</h2>

<p>Of course, simply introducing output correlations is not the whole story of CRF. The truly ingenious part of CRF is that it <strong>takes the path as the unit and considers the probability of the path.</strong></p>

<h3>Model Overview</h3>

<p>Suppose an input has $n$ frames and each frame has $k$ possible labels. Theoretically, there are $k^n$ different possible outputs. We can visualize this using the following network diagram. In the diagram below, each node represents the possibility of a label, the lines between nodes represent the correlations between labels, and each labeling result corresponds to a complete path in the diagram.</p>

<p style="text-align:center;"><img src="https://kexue.fm/usr/uploads/2018/05/1858348821.png" alt="Output network diagram in a 4-tag segmentation model" /><br /><em>Output network diagram in a 4-tag segmentation model</em></p>

<p>In sequence labeling tasks, our correct answer is generally unique. For example, for "今天天气不错" (Today's weather is good), if the segmentation result is "今天/天气/不/错", then the target output sequence is <code>bebess</code>. No other path fits the requirement. In other words, in sequence labeling tasks, our basic unit of study should be the path. What we want to do is select the correct one from $k^n$ paths. This means that if we treat it as a classification problem, it is a task of selecting one category out of $k^n$!</p>

<p>This is the fundamental difference between frame-wise softmax and CRF: <strong>the former views sequence labeling as $n$ independent $k$-classification problems, while the latter views it as a single $k^n$-classification problem.</strong></p>

<p>Specifically, in the CRF sequence labeling problem, we want to calculate the conditional probability:</p>
\begin{equation}
P(y_1,\dots,y_n|x_1,\dots,x_n)=P(y_1,\dots,y_n|\boldsymbol{x}),\quad \boldsymbol{x}=(x_1,\dots,x_n) \tag{1}
\end{equation}
<p>To obtain an estimate of this probability, CRF makes two assumptions:</p>

<blockquote>
<p><strong>Assumption 1:</strong> The distribution belongs to the exponential family.</p>
</blockquote>

<p>This assumption implies there exists a function $f(y_1,\dots,y_n;\boldsymbol{x})$ such that:</p>
\begin{equation}
P(y_1,\dots,y_n|\boldsymbol{x})=\frac{1}{Z(\boldsymbol{x})}\exp\Big(f(y_1,\dots,y_n;\boldsymbol{x})\Big) \tag{2}
\end{equation}
<p>where $Z(\boldsymbol{x})$ is the normalization factor. Since this is a conditional distribution, the normalization factor depends on $\boldsymbol{x}$. This function $f$ can be viewed as a scoring function. The probability distribution is obtained by taking the exponential of the scoring function and normalizing it.</p>

<blockquote>
<p><strong>Assumption 2:</strong> Correlations between outputs occur only at adjacent positions, and the correlations are exponentially additive.</p>
</blockquote>

<p>This assumption implies that $f(y_1,\dots,y_n;\boldsymbol{x})$ can be further simplified as:</p>
\begin{equation}
\begin{aligned}f(y_1,\dots,y_n;\boldsymbol{x})=&h(y_1;\boldsymbol{x})+g(y_1,y_2;\boldsymbol{x})+h(y_2;\boldsymbol{x})+g(y_2,y_3;\boldsymbol{x})+h(y_3;\boldsymbol{x})\\ &+\dots+g(y_{n-1},y_n;\boldsymbol{x})+h(y_n;\boldsymbol{x})\end{aligned} \tag{3}
\end{equation}
<p>This means we only need to score each individual label and each adjacent label pair, and then sum all these scores to get the total score.</p>

<h3>Linear Chain CRF</h3>

<p>Despite significant simplifications, the probability model represented by Equation $(3)$ is generally still too complex to solve. Given that in current deep learning models, RNNs or stacked CNNs are already capable of capturing the relationship between each $y$ and the input $\boldsymbol{x}$ quite effectively, we might as well consider the function $g$ to be independent of $\boldsymbol{x}$:</p>
\begin{equation}
\begin{aligned}f(y_1,\dots,y_n;\boldsymbol{x})=h(y_1;\boldsymbol{x})+&g(y_1,y_2)+h(y_2;\boldsymbol{x})+\dots\\ +&g(y_{n-1},y_n)+h(y_n;\boldsymbol{x})\end{aligned} \tag{4}
\end{equation}
<p>In this case, $g$ is effectively just a finite, trainable parameter matrix, while the single-label scoring function $h(y_i;\boldsymbol{x})$ can be modeled through an RNN or CNN. Thus, the model can be established, where the probability distribution becomes:</p>
\begin{equation}
P(y_1,\dots,y_n|\boldsymbol{x})=\frac{1}{Z(\boldsymbol{x})}\exp\left(h(y_1;\boldsymbol{x})+\sum_{t=1}^{n-1}\Big[g(y_t,y_{t+1})+h(y_{t+1};\boldsymbol{x})\Big]\right) \tag{5}
\end{equation}
<p>This is the concept of a Linear Chain CRF.</p>

<h3>Normalization Factor</h3>

<p>To train the CRF model, we use the maximum likelihood method, which involves using:</p>
\begin{equation}
-\log P(y_1,\dots,y_n|\boldsymbol{x}) \tag{6}
\end{equation}
<p>as the loss function. It can be calculated that this equals:</p>
\begin{equation}
-\left(h(y_1;\boldsymbol{x})+\sum_{t=1}^{n-1}\Big[g(y_t,y_{t+1})+h(y_{t+1};\boldsymbol{x})\Big]\right)+\log Z(\boldsymbol{x}) \tag{7}
\end{equation}
<p>The first term is the logarithm of the <strong>numerator</strong> of the original probability expression, which is the score of the target sequence. Although it looks circuitous, it is not difficult to compute. The real difficulty lies in calculating the logarithm of the <strong>denominator</strong>, $\log Z(\boldsymbol{x})$.</p>

<p>The normalization factor, also called the partition function in physics, requires us to sum the exponential scores of all possible paths. As we mentioned earlier, the number of such paths is exponential ($k^n$), making direct calculation practically impossible.</p>

<p>In fact, the difficulty of calculating the normalization factor is a common problem in almost all graphical probability models. Fortunately, in the CRF model, because we only consider the connection between adjacent labels (the Markov assumption), we can calculate the normalization factor recursively, which reduces the computational complexity from exponential to linear. Specifically, we denote the normalization factor calculated up to time $t$ as $Z_t$, and decompose it into $k$ parts:</p>
\begin{equation}
Z_t = Z^{(1)}_t + Z^{(2)}_t + \dots + Z^{(k)}_t \tag{8}
\end{equation}
<p>where $Z^{(1)}_t, \dots, Z^{(k)}_t$ are the sums of the exponential scores of all paths up to current time $t$ that end with labels $1, \dots, k$, respectively. Then, we can calculate recursively:</p>
\begin{equation}
\begin{aligned}Z^{(1)}_{t+1} = &\Big(Z^{(1)}_t G_{11} + Z^{(2)}_t G_{21} + \dots + Z^{(k)}_t G_{k1} \Big) H_{t+1}(1|\boldsymbol{x})\\ Z^{(2)}_{t+1} = &\Big(Z^{(1)}_t G_{12} + Z^{(2)}_t G_{22} + \dots + Z^{(k)}_t G_{k2} \Big) H_{t+1}(2|\boldsymbol{x})\\ &\qquad\qquad\vdots\\ Z^{(k)}_{t+1} =& \Big(Z^{(1)}_t G_{1k} + Z^{(2)}_t G_{2k} + \dots + Z^{(k)}_t G_{kk} \Big) H_{t+1}(k|\boldsymbol{x}) \end{aligned} \tag{9}
\end{equation}
<p>This can be simply written in matrix form as:</p>
\begin{equation}
\boldsymbol{Z}_{t+1} = \boldsymbol{Z}_{t} \boldsymbol{G} \otimes H(y_{t+1}|\boldsymbol{x}) \tag{10}
\end{equation}
<p>where $\boldsymbol{Z}_{t}=\Big[Z^{(1)}_t,\dots,Z^{(k)}_t\Big]$; $\boldsymbol{G}$ is the matrix obtained by taking the exponential of each element of matrix $g$ (as mentioned, in the simplest case, $g$ is just a matrix representing the scores from one label to another), i.e., $\boldsymbol{G}_{ij}=e^{g_{ij}}$; and $H(y_{t+1}|\boldsymbol{x})$ is the exponential of the scores for each label at position $t+1$ from the encoding model $h(y_{t+1}|\boldsymbol{x})$ (RNN, CNN, etc.), i.e., $H(y_{t+1}|\boldsymbol{x})=e^{h(y_{t+1}|\boldsymbol{x})}$, which is also a vector. In Equation $(10)$, the step $\boldsymbol{Z}_{t} \boldsymbol{G}$ is a matrix multiplication resulting in a vector, and $\otimes$ is element-wise multiplication of two vectors.</p>

<p style="text-align:center;"><img src="https://kexue.fm/usr/uploads/2018/05/2916672323.png" alt="Illustration of recursive calculation of the normalization factor. Calculation from time t to t+1 includes transition probabilities and the probability of node j+1 itself." /><br /><em>Illustration of recursive calculation of the normalization factor. Calculation from time t to t+1 includes transition probabilities and the probability of node j+1 itself.</em></p>

<p>For readers who are unfamiliar, Equation $(10)$ might be difficult to accept at first glance. Readers can try writing out the normalization factors for $n=1, n=2, n=3$, look for their recursive relationships, and gradually understand Equation $(10)$.</p>

<h3>Dynamic Programming</h3>

<p>Once we have written the loss function $-\log P(y_1,\dots,y_n|\boldsymbol{x})$, model training can be completed because current deep learning frameworks already include automatic differentiation functions. As long as we can write a differentiable loss, the framework can handle the optimization process for us.</p>

<p>The remaining final step is how to find the optimal path based on an input after model training is complete. As before, this is a problem of selecting the best path out of $k^n$ possibilities. Similarly, due to the Markov assumption, this can be transformed into a dynamic programming problem solved by the Viterbi algorithm, with a computational complexity proportional to $n$.</p>

<p><strong>Dynamic programming has appeared multiple times in this blog. Its recursive idea is: if an optimal path is cut into two segments, each segment must be a (locally) optimal path. You can type "Dynamic Programming" in the search box on the right of this blog to find many related introductions, so I won't repeat it here~</strong></p>

<h2>Implementation</h2>

<p>After debugging, I obtained a concise implementation of a linear chain CRF based on the Keras framework. <strong>This might be the shortest CRF implementation.</strong> Here, I share the final implementation and explain the key points.</p>

<h3>Implementation Key Points</h3>

<p>As we explained earlier, the difficulty in implementing a CRF is the calculation of $-\log P(y_1,\dots,y_n|\boldsymbol{x})$, and the core difficulty is the calculation of the normalization factor $Z(\boldsymbol{x})$. Thanks to the Markov assumption, we obtained recursive equations $(9)$ or $(10)$, which represent the general calculation for $Z(\boldsymbol{x})$.</p>

<p>How do we implement this recursive calculation in a deep learning framework? Note that from the perspective of a computational graph, this is defining a graph through recursion, and the length of this graph is not fixed. This shouldn't be a problem for dynamic graph frameworks like PyTorch, but it is difficult for static graph frameworks like TensorFlow or Keras (which is based on TensorFlow).</p>

<p>However, it is not impossible. We can use the packaged <code>rnn</code> function for calculation! We know that an RNN essentially performs recursive calculation:</p>
\begin{equation}
\boldsymbol{h}_{t+1} = f(\boldsymbol{x}_{t+1}, \boldsymbol{h}_{t}) \tag{11}
\end{equation}
<p>Newer versions of TensorFlow and Keras already allow us to define custom RNN cells, which means the function $f$ can be defined by ourselves, while the backend automatically handles the recursive calculation for us. So, we only need to design an RNN such that the $\boldsymbol{Z}$ we want to calculate corresponds to the hidden vector of the RNN!</p>

<p>This is the most exquisite part of the CRF implementation.</p>

<p>The rest are some minor details, including:</p>

<blockquote>
<p>1. To prevent overflow, we usually take logs. Since the normalization factor is a sum of exponentials, it is actually in the format $\log\left(\sum_{i=1}^k e^{a_i}\right)$. The trick for this calculation is:<br />
$$\log\left(\sum_{i=1}^k e^{a_i}\right)=A + \log\left(\sum_{i=1}^k e^{a_i-A}\right),\quad A = \max \{a_1,\dots,a_k\}$$<br />
TensorFlow and Keras both already have the corresponding <code>logsumexp</code> function; call it directly.</p>

<p>2. For the calculation of the numerator (the score of the target sequence), the trick used in the code is to multiply the "target sequence" with the "predicted sequence" to extract the target score.</p>

<p>3. Regarding how to mask padding parts for variable-length inputs: I think Keras doesn't do a very good job in this area. To implement this masking simply, my approach is to introduce one extra label. For example, if there were originally four labels s, b, m, e for segmentation, I introduce a fifth label, say 'x'. Set the labels for the padding parts to 'x', and then ignore the existence of the fifth label during CRF loss calculation. See the code for specific details.</p>
</blockquote>

<h3>Code Preview</h3>

<p>A pure Keras implementation of the CRF layer, feel free to use it~</p>

<pre><code># -*- coding:utf-8 -*-

from keras.layers import Layer
import keras.backend as K

class CRF(Layer):
    """Pure Keras implementation of the CRF layer.
    The CRF layer is essentially a loss calculation layer with trainable parameters.
    Therefore, the CRF layer is only used for training the model,
    while prediction requires building a separate model.
    """
    def __init__(self, ignore_last_label=False, **kwargs):
        """ignore_last_label: Define whether to ignore the last label, serving as a mask.
        """
        self.ignore_last_label = 1 if ignore_last_label else 0
        super(CRF, self).__init__(**kwargs)

    def build(self, input_shape):
        self.num_labels = input_shape[-1] - self.ignore_last_label
        self.trans = self.add_weight(name='crf_trans',
                                     shape=(self.num_labels, self.num_labels),
                                     initializer='glorot_uniform',
                                     trainable=True)

    def log_norm_step(self, inputs, states):
        """Recursive calculation of the normalization factor.
        Key points: 1. Recursive calculation; 2. Use logsumexp to avoid overflow.
        Trick: Use expand_dims to align tensors.
        """
        inputs, mask = inputs[:, :-1], inputs[:, -1:]
        states = K.expand_dims(states[0], 2) # (batch_size, output_dim, 1)
        trans = K.expand_dims(self.trans, 0) # (1, output_dim, output_dim)
        outputs = K.logsumexp(states + trans, 1) # (batch_size, output_dim)
        outputs = outputs + inputs
        outputs = mask * outputs + (1 - mask) * states[:, :, 0]
        return outputs, [outputs]

    def path_score(self, inputs, labels):
        """Calculate the relative probability of the target path (not yet normalized).
        Key points: sum of individual label scores plus transition probability scores.
        Trick: Use "prediction" dot "target" to extract the score of the target path.
        """
        point_score = K.sum(K.sum(inputs * labels, 2), 1, keepdims=True) # individual label scores
        labels1 = K.expand_dims(labels[:, :-1], 3)
        labels2 = K.expand_dims(labels[:, 1:], 2)
        labels = labels1 * labels2 # two offset labels responsible for extracting target transition scores
        trans = K.expand_dims(K.expand_dims(self.trans, 0), 0)
        trans_score = K.sum(K.sum(trans * labels, [2, 3]), 1, keepdims=True)
        return point_score + trans_score # Sum of the two parts

    def call(self, inputs): # CRF itself does not change the output, it is just a loss
        return inputs

    def loss(self, y_true, y_pred): # Target y_true needs to be in one-hot format
        if self.ignore_last_label:
            mask = 1 - y_true[:, :, -1:]
        else:
            mask = K.ones_like(y_pred[:, :, :1])
        y_true, y_pred = y_true[:, :, :self.num_labels], y_pred[:, :, :self.num_labels]
        path_score = self.path_score(y_pred, y_true) # Calculate numerator (log)
        init_states = [y_pred[:, 0]] # Initial state
        y_pred = K.concatenate([y_pred, mask])
        log_norm, _, _ = K.rnn(self.log_norm_step, y_pred[:, 1:], init_states) # Calculate Z vector (log)
        log_norm = K.logsumexp(log_norm, 1, keepdims=True) # Calculate Z (log)
        return log_norm - path_score # i.e., log(denominator/numerator) actually the reverse for minimization

    def accuracy(self, y_true, y_pred): # Function to display frame-wise accuracy during training, excluding mask
        mask = 1 - y_true[:, :, -1] if self.ignore_last_label else None
        y_true, y_pred = y_true[:, :, :self.num_labels], y_pred[:, :, :self.num_labels]
        isequal = K.equal(K.argmax(y_true, 2), K.argmax(y_pred, 2))
        isequal = K.cast(isequal, 'float32')
        if mask == None:
            return K.mean(isequal)
        else:
            return K.sum(isequal * mask) / K.sum(mask)
</code></pre>

<p>Excluding comments and the accuracy code, the actual CRF code is only about 30 lines. It can be called a concise CRF implementation compared to any framework~</p>

<p>Implementing complex models purely with Keras is quite interesting. It is currently only tested on the TensorFlow backend, but theoretically compatible with Theano and CNTK backends with minor adjustments.</p>

<h3>Usage Case</h3>

<p>My GitHub also contains an <strong>example of Chinese word segmentation implemented using CNN+CRF</strong>, using the <a href="http://sighan.cs.uchicago.edu/bakeoff2005/">Bakeoff 2005</a> corpus. The example is a complete word segmentation implementation, including the Viterbi algorithm, segmentation output, etc.</p>

<p>GitHub address: <a href="https://github.com/bojone/crf/">https://github.com/bojone/crf/</a></p>

<p>Related content can also be found in my previous articles:</p>

<blockquote>
<p>1. <a href="translation_3924.html">【Chinese Word Segmentation Series】 4. Sequence-to-sequence character labeling based on Bidirectional LSTM</a></p>
<p>2. <a href="translation_4195.html">【Chinese Word Segmentation Series】 6. Chinese word segmentation based on fully convolutional networks</a></p>
</blockquote>

<h2>Conclusion</h2>

<p>Finally, the introduction is complete. I hope everyone has gained something, and I hope the final implementation will be helpful to everyone~</p>
<hr>
<footer style="margin-top: 3em; padding: 1.5em; background: #f5f5f5; border-radius: 8px; font-size: 0.9em; color: #555;">
    <p style="margin: 0 0 0.5em 0;"><strong>Citation</strong></p>
    <p style="margin: 0 0 0.5em 0;">
        This is a machine translation of the original Chinese article:<br>
        <a href="https://kexue.fm/archives/5542" style="color: #005fcc;">https://kexue.fm/archives/5542</a>
    </p>
    <p style="margin: 0 0 0.5em 0;">
        Original author: 苏剑林 (Su Jianlin)<br>
        Original publication: <a href="https://kexue.fm" style="color: #005fcc;">科学空间 (Scientific Spaces)</a>
    </p>
    <p style="margin: 0; font-style: italic;">
        Translated using Gemini 3 Flash. Please refer to the original for authoritative content.
    </p>
</footer>
