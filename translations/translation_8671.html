
    <style type="text/css">

    body {
    margin: 48px auto;
    max-width: 68ch;              /* character-based width reads better */
    padding: 0 16px;

    font-family: -apple-system, BlinkMacSystemFont, "Segoe UI",
                Roboto, "Helvetica Neue", Arial, sans-serif;
    font-size: 18px;
    line-height: 1.65;
    color: #333;
    background: #fafafa;
    }

    h1, h2, h3, h4 {
    line-height: 1.25;
    margin-top: 2.2em;
    margin-bottom: 0.6em;
    font-weight: 600;
    }

    h1 {
    font-size: 2.1em;
    margin-top: 0;
    }

    h2 {
    font-size: 1.6em;
    border-bottom: 1px solid #e5e5e5;
    padding-bottom: 0.3em;
    }

    h3 {
    font-size: 1.25em;
    }

    h4 {
    font-size: 1.05em;
    color: #555;
    }

    /* Paragraphs and lists */
    p {
    margin: 1em 0;
    }

    ul, ol {
    margin: 1em 0 1em 1.5em;
    }

    li {
    margin: 0.4em 0;
    }

    a {
    color: #005fcc;
    text-decoration: none;
    }

    a:hover {
    text-decoration: underline;
    }

    code {
    font-family: ui-monospace, SFMono-Regular, Menlo, Consolas, monospace;
    font-size: 0.95em;
    background: #f2f2f2;
    padding: 0.15em 0.35em;
    border-radius: 4px;
    }

    pre {
    font-family: ui-monospace, SFMono-Regular, Menlo, Consolas, monospace;
    font-size: 0.9em;
    background: #f5f5f5;
    padding: 1em 1.2em;
    overflow-x: auto;
    border-radius: 6px;
    line-height: 1.45;
    }

    pre code {
    background: none;
    padding: 0;
    }

    blockquote {
    margin: 1.5em 0;
    padding-left: 1em;
    border-left: 4px solid #ddd;
    color: #555;
    }

    hr {
    border: none;
    border-top: 1px solid #e0e0e0;
    margin: 3em 0;
    }

    table {
    border-collapse: collapse;
    margin: 1.5em 0;
    width: 100%;
    font-size: 0.95em;
    }

    th, td {
    padding: 0.5em 0.7em;
    border-bottom: 1px solid #e5e5e5;
    text-align: left;
    }

    th {
    font-weight: 600;
    }

    img {
    max-width: 100%;
    display: block;
    margin: 1.5em auto;
    }

    small {
    color: #666;
    }

    mjx-container {
    margin: 1em 0;
    }

    ::selection {
    background: #cce2ff;
    }

    </style>
    

<script>
window.MathJax = {
  tex: {
    inlineMath: [['$', '$'], ['\\(', '\\)']],
    displayMath: [['$$', '$$'], ['\\[', '\\]']],
    tags: 'ams'
  }
};
</script>
<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

<article>
    <nav style="margin-bottom: 1.5em;">
    <a href="../index.html" style="display: inline-flex; align-items: center; color: #555; text-decoration: none; font-size: 0.95em;">
        <span style="margin-right: 0.3em;">&larr;</span> Back to Index
    </a>
</nav>

    <h1><a href="https://kexue.fm/archives/8671">The Once-Dismissed Pre-training Task NSP Delivers Excellent Zero-Shot Results</a></h1>
    <p>By 苏剑林 | September 10, 2021</p>

    <p>Among the wide variety of pre-training task designs, NSP (Next Sentence Prediction) is generally considered to be one of the poorer ones. This is because it is relatively easy, and adding it to pre-training has not shown significant benefits for downstream fine-tuning. In fact, the RoBERTa paper showed that it can even have a negative impact. Consequently, subsequent pre-training efforts have generally taken one of two paths: either discarding the NSP task entirely, as RoBERTa did, or finding ways to increase its difficulty, as ALBERT did. In other words, NSP has long been something of an "outcast."</p>

    <p>However, the tables have turned, and NSP may be about to make a "comeback." A recent paper, <a href="https://papers.cool/arxiv/2109.03564">"NSP-BERT: A Prompt-based Zero-Shot Learner Through an Original Pre-training Task--Next Sentence Prediction"</a> (hereafter referred to as NSP-BERT), shows that NSP can actually achieve very impressive Zero-Shot results! This is another classic case of Prompt-based Few/Zero-Shot learning, but this time, NSP is the protagonist.</p>

    <h2>Background Recap</h2>
    <p>We used to believe that pre-training was purely pre-training—it merely provided a better initialization for downstream task training. In BERT, the pre-training tasks are MLM (Masked Language Model) and NSP (Next Sentence Prediction). For a long time, researchers didn't focus on these two tasks themselves, but rather on how to fine-tune the model to achieve better performance on downstream tasks. Even when T5 scaled model parameters to 11 billion, it still followed the "pre-training + fine-tuning" route.</p>

    <p>The first to forcefully break this mindset was <a href="https://papers.cool/arxiv/2005.14165">GPT-3</a>, released last year. It demonstrated that with a sufficiently large pre-trained model, we can design specific templates (Prompts) to achieve excellent Few/Zero-Shot results without any fine-tuning. Where there is GPT, BERT is never far behind. If GPT can do it, BERT should be able to as well. This led to the <a href="https://papers.cool/arxiv/2009.07118">PET</a> work, which similarly constructed special templates to utilize pre-trained MLM models for Few/Zero-Shot learning. Readers unfamiliar with this can refer to <a href="translation_7764.html">"Is GPT-3 Necessary? No, BERT's MLM Can Also Do Few-Shot Learning"</a>.</p>

    <p>Since then, "pre-training + prompt" work has gradually increased and is now arguably "exploding." This series of research is now generally grouped under "Prompt-based Language Models." A quick search will yield many examples. By now, a consensus has formed: constructing an appropriate Prompt to bring the format of the downstream task closer to the pre-training task usually yields better results. Therefore, how to construct Prompts has become a key focus of this research, with <a href="https://papers.cool/arxiv/2103.10385">P-tuning</a> being a classic example (refer to <a href="translation_8295.html">"P-tuning: Automatically Constructing Templates to Release the Potential of Language Models"</a>).</p>

    <h2>NSP Enters the Fray</h2>
    <p>A careful look at Prompt-based research reveals that current content is primarily focused on how to better utilize pre-trained GPT, MLM, or Encoder-Decoder models, with very few paying attention to other pre-training tasks. NSP-BERT, however, fully taps into the potential of the NSP task and highlights that even within the "Prompt-based" framework, there is still significant room for exploratory thinking.</p>

    <p>The so-called NSP task isn't actually about predicting the "next sentence" in a generative sense, but rather, given two sentences, determining whether they are adjacent. Accordingly, the idea behind NSP-BERT is simple: taking classification as an example, the input is treated as the first sentence, and then each candidate category is added to a specific Prompt to serve as the second sentence. The model then judges which second sentence is most coherent with the first. One can see that the logic of NSP-BERT is very similar to PET; in fact, all Prompt-based work is easy to understand—the difficulty lies in being the first to think of it.</p>

    <p>The diagram below demonstrates the Prompt schemes used by NSP-BERT for some common NLU tasks. It shows that NSP-BERT can handle a fair number of tasks:</p>

    <p>
        <a href="https://kexue.fm/usr/uploads/2021/09/3815303747.png">
            <img src="https://kexue.fm/usr/uploads/2021/09/3815303747.png" alt="NSP-BERT Prompts for common NLU tasks" title="Click to view original image" />
        </a>
        <br>
        <em>NSP-BERT Prompts for common NLU tasks</em>
    </p>

    <p>In fact, once you've looked at this diagram, you have already understood most of the core ideas of NSP-BERT. The rest of the paper simply describes the details of this diagram. Students who wish to dive deeper can read the original paper carefully on their own.</p>

    <p>Strictly speaking, this NSP-BERT mode is not appearing for the first time. Earlier, some proposed using NLI models as Zero-Shot classifiers (refer to <a href="https://jaketae.github.io/study/zero-shot-classification/">"NLI Models as Zero-Shot Classifiers"</a>). The format is essentially identical to NSP, but that required supervised fine-tuning on labeled NLI data. This is the first attempt at utilizing purely unsupervised NSP.</p>

    <h2>Experimental Results</h2>
    <p>Interestingly for us, NSP-BERT is a very "grounded" and honest piece of work. For instance, it was written by Chinese authors, its experimental tasks are in Chinese (FewCLUE and DuEL 2.0), and the code has been open-sourced. Here is the author's open-source address:</p>

    <blockquote>
        <strong>Github: <a href="https://github.com/sunyilgdx/NSP-BERT">https://github.com/sunyilgdx/NSP-BERT</a></strong>
    </blockquote>

    <p>Most importantly, the performance of NSP-BERT is genuinely good:</p>

    <p>
        <a href="https://kexue.fm/usr/uploads/2021/09/1037746494.png">
            <img src="https://kexue.fm/usr/uploads/2021/09/1037746494.png" alt="Zero-Shot results of NSP-BERT" title="Click to view original image" />
        </a>
        <br>
        <em>Zero-Shot results of NSP-BERT</em>
    </p>

    <p>
        <a href="https://kexue.fm/usr/uploads/2021/09/3495277876.png">
            <img src="https://kexue.fm/usr/uploads/2021/09/3495277876.png" alt="Results on Entity Linking tasks" title="Click to view original image" />
        </a>
        <br>
        <em>Results on Entity Linking tasks</em>
    </p>

    <p>
        <a href="https://kexue.fm/usr/uploads/2021/09/1096881522.png">
            <img src="https://kexue.fm/usr/uploads/2021/09/1096881522.png" alt="Impact of model scale on performance" title="Click to view original image" />
        </a>
        <br>
        <em>Impact of model scale on performance</em>
    </p>

    <p>Overall, after seeing these experimental results, I can only say "my apologies for underestimating you" to NSP. Such a powerhouse in the modeling world was right in front of us, but we never realized its potential. One must applaud the observational skills of the NSP-BERT authors.</p>

    <h2>Conclusion</h2>
    <p>This article shared a paper on using BERT’s pre-training task NSP for Zero-Shot learning. Results show that using NSP for Zero-Shot can achieve excellent performance. Perhaps in due time, NSP will truly "rise again."</p>
</article>
<hr>
<footer style="margin-top: 3em; padding: 1.5em; background: #f5f5f5; border-radius: 8px; font-size: 0.9em; color: #555;">
    <p style="margin: 0 0 0.5em 0;"><strong>Citation</strong></p>
    <p style="margin: 0 0 0.5em 0;">
        This is a machine translation of the original Chinese article:<br>
        <a href="https://kexue.fm/archives/8671" style="color: #005fcc;">https://kexue.fm/archives/8671</a>
    </p>
    <p style="margin: 0 0 0.5em 0;">
        Original author: 苏剑林 (Su Jianlin)<br>
        Original publication: <a href="https://kexue.fm" style="color: #005fcc;">科学空间 (Scientific Spaces)</a>
    </p>
    <p style="margin: 0; font-style: italic;">
        Translated using Gemini 3 Flash. Please refer to the original for authoritative content.
    </p>
</footer>
