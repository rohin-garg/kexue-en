
    <style type="text/css">

    body {
    margin: 48px auto;
    max-width: 68ch;              /* character-based width reads better */
    padding: 0 16px;

    font-family: -apple-system, BlinkMacSystemFont, "Segoe UI",
                Roboto, "Helvetica Neue", Arial, sans-serif;
    font-size: 18px;
    line-height: 1.65;
    color: #333;
    background: #fafafa;
    }

    h1, h2, h3, h4 {
    line-height: 1.25;
    margin-top: 2.2em;
    margin-bottom: 0.6em;
    font-weight: 600;
    }

    h1 {
    font-size: 2.1em;
    margin-top: 0;
    }

    h2 {
    font-size: 1.6em;
    border-bottom: 1px solid #e5e5e5;
    padding-bottom: 0.3em;
    }

    h3 {
    font-size: 1.25em;
    }

    h4 {
    font-size: 1.05em;
    color: #555;
    }

    /* Paragraphs and lists */
    p {
    margin: 1em 0;
    }

    ul, ol {
    margin: 1em 0 1em 1.5em;
    }

    li {
    margin: 0.4em 0;
    }

    a {
    color: #005fcc;
    text-decoration: none;
    }

    a:hover {
    text-decoration: underline;
    }

    code {
    font-family: ui-monospace, SFMono-Regular, Menlo, Consolas, monospace;
    font-size: 0.95em;
    background: #f2f2f2;
    padding: 0.15em 0.35em;
    border-radius: 4px;
    }

    pre {
    font-family: ui-monospace, SFMono-Regular, Menlo, Consolas, monospace;
    font-size: 0.9em;
    background: #f5f5f5;
    padding: 1em 1.2em;
    overflow-x: auto;
    border-radius: 6px;
    line-height: 1.45;
    }

    pre code {
    background: none;
    padding: 0;
    }

    blockquote {
    margin: 1.5em 0;
    padding-left: 1em;
    border-left: 4px solid #ddd;
    color: #555;
    }

    hr {
    border: none;
    border-top: 1px solid #e0e0e0;
    margin: 3em 0;
    }

    table {
    border-collapse: collapse;
    margin: 1.5em 0;
    width: 100%;
    font-size: 0.95em;
    }

    th, td {
    padding: 0.5em 0.7em;
    border-bottom: 1px solid #e5e5e5;
    text-align: left;
    }

    th {
    font-weight: 600;
    }

    img {
    max-width: 100%;
    display: block;
    margin: 1.5em auto;
    }

    small {
    color: #666;
    }

    mjx-container {
    margin: 1em 0;
    }

    ::selection {
    background: #cce2ff;
    }

    </style>
    

<script>
window.MathJax = {
  tex: {
    inlineMath: [['$', '$'], ['\\(', '\\)']],
    displayMath: [['$$', '$$'], ['\\[', '\\]']],
    processEscapes: true,
    tags: 'ams',
    packages: {'[+]': ['ams']}
  },
  options: {
    renderActions: {
      findScript: [10, function (doc) {
        for (const node of document.querySelectorAll('script[type^="math/tex"]')) {
          const display = !!node.type.match(/; *mode=display/);
          const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
          const text = document.createTextNode('');
          node.parentNode.replaceChild(text, node);
          math.start = {node: text, delim: '', n: 0};
          math.end = {node: text, delim: '', n: 0};
          doc.math.push(math);
        }
      }, '']
    }
  }
};
</script>
<script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

<nav style="margin-bottom: 1.5em;">
    <a href="../index.html" style="display: inline-flex; align-items: center; color: #555; text-decoration: none; font-size: 0.95em;">
        <span style="margin-right: 0.3em;">&larr;</span> Back to Index
    </a>
</nav>

<h1><a href="https://kexue.fm/archives/7031">When Can the Multi-processing Speedup Ratio Be Greater Than 1?</a></h1>

<p>By 苏剑林 | October 27, 2019</p>

<p>Parallel acceleration via multi-processing or multi-threading is no longer a difficult task, and many readers have likely experienced it. Generally, we reach this conclusion: the speedup ratio of multi-processing rarely exceeds 1. In other words, when you use 10 processes to run a task in parallel, you generally only obtain a speedup of less than 10 times, and as the number of processes increases, this speedup ratio often becomes even lower.</p>

<p>Note that we just said "rarely exceeds 1," which implies that in our subconscious, we feel the speedup ratio should be at most 1. Theoretically, this is true—could 10 processes really achieve a 20-fold acceleration? Isn't that just a windfall? However, I did happen to encounter an example a few days ago where the speedup ratio was significantly greater than 1, so I would like to share it here.</p>

<h2>Word Frequency Statistics</h2>

<p>My original task was to count word frequencies: I have many articles, we need to perform word segmentation (tokenization) on these articles, and finally summarize them into a word frequency table. A typical implementation looks like this:</p>

<pre><code class="language-python">tokens = {}
for text in texts:
    for token in tokenize(text):
        tokens[token] = tokens.get(token, 0) + 1
</code></pre>

<p>This implementation took about 20 minutes when I was counting the word frequencies for the entire <a href="http://thuctc.thunlp.org/#%E4%B8%AD%E6%96%87%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E6%95%B0%E6%8D%AE%E9%9B%86THUCNews">THUCNews</a> dataset.</p>

<h2>Multi-processing Version</h2>

<p>Now, let's compare it with a multi-processing version. I explained the coding techniques for multi-processing in <a href="translation_4231.html">"Python Multi-processing Programming Tips"</a>. To make it reusable, I encapsulated it into a function:</p>

<pre><code class="language-python">from multiprocessing import Pool

def parallel_apply(func, iterable, workers, **kwargs):
    if workers &lt;= 1:
        return [func(i, **kwargs) for i in iterable]
    p = Pool(workers)
    result = p.map(func, iterable)
    p.close()
    p.join()
    return result
</code></pre>

<p>Using this function to perform multi-processed word frequency counting, the code looks roughly as follows:</p>

<pre><code class="language-python">def _tokenize_and_count(texts):
    tokens = {}
    for text in texts:
        for token in tokenize(text):
            tokens[token] = tokens.get(token, 0) + 1
    return tokens

def _total_count(results):
    tokens = {}
    for result in results:
        for token, count in result.items():
            tokens[token] = tokens.get(token, 0) + count
    return tokens

batch_texts = [texts[i:i+1000] for i in range(0, len(texts), 1000)]
results = parallel_apply(_tokenize_and_count, batch_texts, 10)
tokens = _total_count(results)
</code></pre>

<p>The entire process is: splitting the text into batches where each batch contains 1,000 texts; <code>_tokenize_and_count</code> is used to count each batch; <code>_total_count</code> summarizes the results of each batch; and finally, <code>parallel_apply</code> implements this process using 10 processes.</p>

<p>How long did this take? The result was 55 seconds! This represents a 20-fold speedup, meaning the speedup ratio per process is 2!</p>

<h2>Principle Analysis</h2>

<p>Why is it possible to achieve a speedup ratio greater than 1? In fact, the reason lies in the original single-process implementation where the line <code>tokens[token] = tokens.get(token, 0) + 1</code> becomes slower and slower. As the counting progresses, there are more and more elements in <code>tokens</code>, and the addition, deletion, modification, and lookup operations within <code>tokens</code> gradually slow down.</p>

<p>In the multi-processing version, the <code>tokens[token] = tokens.get(token, 0) + 1</code> line is only executed for batches of no more than 1,000 samples, which obviously maintains a very high speed consistently. Although the final result merging also involves frequent reads and writes to <code>tokens</code>, the frequency is far lower than in the original implementation, thus it is also very fast. Therefore, the multi-processing version can achieve a 20-fold acceleration rather than just the theoretical limit of 10-fold.</p>

<p>Of course, readers might have already sensed that this isn't truly the acceleration ratio exceeding 1, but rather an artifact caused by the poor design of the original single-process version. It can be fixed by changing it to the following code:</p>

<pre><code class="language-python">tokens = {}
for i in range(0, len(texts), 1000):
    batch_texts = texts[i:i+1000]
    batch_tokens = _tokenize_and_count(batch_texts)
    for token, count in batch_tokens.items():
        tokens[token] = tokens.get(token, 0) + count
</code></pre>

<p>This is the same approach of batch counting and then summarizing, except it's single-processed. This implementation looks roundabout and unintuitive, but it actually only took 8 minutes—about one-third of the original version! From this, it's evident that the actual speedup ratio is approximately 0.8.</p>

<h2>Summary</h2>

<p>This article briefly discussed the issue of multi-processing in Python and provided an example where the speedup ratio seemingly exceeds 1, followed by an analysis of the cause. From another perspective, this also serves as a reminder for writing similar code: even in a single-process scenario, the efficiency of batch calculation followed by summarization is usually higher than calculating the entire set in one go.</p>
<hr>
<footer style="margin-top: 3em; padding: 1.5em; background: #f5f5f5; border-radius: 8px; font-size: 0.9em; color: #555;">
    <p style="margin: 0 0 0.5em 0;"><strong>Citation</strong></p>
    <p style="margin: 0 0 0.5em 0;">
        This is a machine translation of the original Chinese article:<br>
        <a href="https://kexue.fm/archives/7031" style="color: #005fcc;">https://kexue.fm/archives/7031</a>
    </p>
    <p style="margin: 0 0 0.5em 0;">
        Original author: 苏剑林 (Su Jianlin)<br>
        Original publication: <a href="https://kexue.fm" style="color: #005fcc;">科学空间 (Scientific Spaces)</a>
    </p>
    <p style="margin: 0; font-style: italic;">
        Translated using Gemini 3 Flash. Please refer to the original for authoritative content.
    </p>
</footer>
