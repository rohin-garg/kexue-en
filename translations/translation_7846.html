
    <style type="text/css">

    body {
    margin: 48px auto;
    max-width: 68ch;              /* character-based width reads better */
    padding: 0 16px;

    font-family: -apple-system, BlinkMacSystemFont, "Segoe UI",
                Roboto, "Helvetica Neue", Arial, sans-serif;
    font-size: 18px;
    line-height: 1.65;
    color: #333;
    background: #fafafa;
    }

    h1, h2, h3, h4 {
    line-height: 1.25;
    margin-top: 2.2em;
    margin-bottom: 0.6em;
    font-weight: 600;
    }

    h1 {
    font-size: 2.1em;
    margin-top: 0;
    }

    h2 {
    font-size: 1.6em;
    border-bottom: 1px solid #e5e5e5;
    padding-bottom: 0.3em;
    }

    h3 {
    font-size: 1.25em;
    }

    h4 {
    font-size: 1.05em;
    color: #555;
    }

    /* Paragraphs and lists */
    p {
    margin: 1em 0;
    }

    ul, ol {
    margin: 1em 0 1em 1.5em;
    }

    li {
    margin: 0.4em 0;
    }

    a {
    color: #005fcc;
    text-decoration: none;
    }

    a:hover {
    text-decoration: underline;
    }

    code {
    font-family: ui-monospace, SFMono-Regular, Menlo, Consolas, monospace;
    font-size: 0.95em;
    background: #f2f2f2;
    padding: 0.15em 0.35em;
    border-radius: 4px;
    }

    pre {
    font-family: ui-monospace, SFMono-Regular, Menlo, Consolas, monospace;
    font-size: 0.9em;
    background: #f5f5f5;
    padding: 1em 1.2em;
    overflow-x: auto;
    border-radius: 6px;
    line-height: 1.45;
    }

    pre code {
    background: none;
    padding: 0;
    }

    blockquote {
    margin: 1.5em 0;
    padding-left: 1em;
    border-left: 4px solid #ddd;
    color: #555;
    }

    hr {
    border: none;
    border-top: 1px solid #e0e0e0;
    margin: 3em 0;
    }

    table {
    border-collapse: collapse;
    margin: 1.5em 0;
    width: 100%;
    font-size: 0.95em;
    }

    th, td {
    padding: 0.5em 0.7em;
    border-bottom: 1px solid #e5e5e5;
    text-align: left;
    }

    th {
    font-weight: 600;
    }

    img {
    max-width: 100%;
    display: block;
    margin: 1.5em auto;
    }

    small {
    color: #666;
    }

    mjx-container {
    margin: 1em 0;
    }

    ::selection {
    background: #cce2ff;
    }

    </style>
    

<script>
window.MathJax = {
  tex: {
    inlineMath: [['$', '$'], ['\\(', '\\)']],
    displayMath: [['$$', '$$'], ['\\[', '\\]']],
    tags: 'ams',
    packages: {'[+]': ['ams']}
  }
};
</script>
<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

<article>
    <h1><a href="https://kexue.fm/archives/7846">Before using ALBERT and ELECTRA, make sure you really understand them</a></h1>

    <p>By 苏剑林 | October 29, 2020</p>

    
    <p>In the world of pre-trained language models, ALBERT and ELECTRA can be considered two "rising stars" that followed BERT. They improved upon BERT from different perspectives and ultimately enhanced performance (at least on many public benchmark datasets), thus earning a certain reputation. However, in daily exchanges and learning, I have found that many friends have misunderstandings about these two models, leading to unnecessary time wasted during use. Here, I attempt to summarize some key points of these two models for your reference, hoping you can avoid detours when using them.</p>

    <h2>ALBERT and ELECTRA</h2>
    <p>(Note: In this article, the word "BERT" refers to both the initially released BERT model and its subsequent improved version, RoBERTa. We can think of BERT as an insufficiently trained RoBERTa, and RoBERTa as a more fully trained BERT. This article focuses on their comparison with ALBERT and ELECTRA, so it does not distinguish between BERT and RoBERTa.)</p>

    <h3>ALBERT</h3>
    <p>ALBERT comes from the paper <a href="https://arxiv.org/abs/1909.11942">"ALBERT: A Lite BERT for Self-supervised Learning of Language Representations."</a> As the name suggests, it considers its main characteristic to be "Lite." So, what is the specific meaning of this "Lite"? Many friends have the impression that ALBERT is "small, fast, and good." Is this really the case?</p>

    <h4>Characteristics</h4>
    <p>Simply put, ALBERT is essentially a <strong>parameter-sharing</strong> BERT. It is equivalent to Changing the function $y=f_n(f_{n-1}(\cdots(f_1(x))))$ to $y=f(f(\cdots(f(x))))$, where $f$ represents each layer of the model. In this way, what was originally $n$ layers of parameters is now only 1 layer. Therefore, the number of parameters is greatly reduced—or rather, the weight volume of the saved model is very small. This is the first meaning of "Lite." Then, because the total number of parameters is reduced, the time and memory required for model training are correspondingly reduced; this is the second meaning of "Lite." Furthermore, when the model is very large, parameter sharing acts as a strong regularization method for the model, so it is less prone to overfitting compared to BERT. Ultimately, its large models see a certain performance improvement, which is the highlight of ALBERT.</p>

    <h4>Prediction</h4>
    <p>Note that we did not mention <strong>prediction speed</strong>. Obviously, in the prediction stage, parameter sharing does not bring acceleration because the model still performs forward calculations step-by-step anyway; it does not care whether the current parameters are the same as the past ones, and even if they were the same, it wouldn't speed up (because the input is different). Therefore, <strong>the prediction speed of ALBERT and BERT of the same specification is the same</strong>. In fact, if we want to be pedantic, ALBERT might actually be slower because ALBERT uses matrix decomposition for the Embedding layer, which brings extra computation, though this amount is generally imperceptible.</p>

    <h4>Training</h4>
    <p>As for <strong>training speed</strong>, although there is an improvement, it is not as significant as imagined. Reducing the parameter count to $1/n$ of the original does not mean the training speed increases by $n$ times. In my previous experiments, a base version of ALBERT compared to a base BERT showed a <strong>training speed increase of only about 10%–20%</strong>, and the reduction in memory usage was similar. if the model is smaller (tiny/small versions), this gap narrows further. In other words, ALBERT's training advantage is only obvious in large models; for models that aren't large, this advantage is still difficult to perceive significantly.</p>

    <h4>Performance</h4>
    <p>Regarding effectiveness, the original ALBERT paper is quite clear, as shown in the table below. Parameter sharing limits the model's expressive power; therefore, the ALBERT-xlarge version can only match the BERT-large version, and to stably surpass it, an xxlarge version is required. In other words, <strong>as long as the version specification is smaller than xlarge, the performance of ALBERT is inferior to BERT of the same specification</strong>. Evaluation results on Chinese tasks are similar (can refer <a href="https://github.com/CLUEbenchmark/CLUE">here</a> and <a href="https://github.com/ymcui/Chinese-BERT-wwm">here</a>). Furthermore, I have done even more extreme experiments: loading ALBERT weights but removing the parameter-sharing constraint—treating ALBERT as a BERT—and performance improved! (Refer to <a href="translation_7112.html">"Drop the Constraints, Enhance the Model: Improving ALBERT Performance with One Line of Code"</a>). So, it is basically a fact that small-spec ALBERT is inferior to BERT.</p>

    <p style="text-align:center;"><img src="https://cdn.jsdelivr.net/gh/bojone/bert_models/albert_results.png" alt="ALBERT Experimental Results"></p>

    <h4>Conclusion</h4>
    <p>So, the summary advice is: <strong>If you aren't using the xlarge version, there's no need to use ALBERT. At the same speed, ALBERT is less effective than BERT; at the same effectiveness, ALBERT is slower than BERT.</strong> Now that BERT also has tiny/small versions, such as <a href="https://github.com/ZhuiyiTechnology/pretrained-models">the ones open-sourced by our company</a>, they are basically just as fast and perform better—unless you truly need the characteristic of small disk size.</p>

    <p>What does "xlarge" mean? Some readers haven't tried BERT because their machines can't run it; most readers have limited VRAM and have only run the base version of BERT, and haven't run or can't afford to run the large version. xlarge is even bigger than large and has higher equipment requirements. So frankly, for most readers, there is no need to use ALBERT.</p>

    <p>Why did the idea that ALBERT is "fast and good" spread? Besides improper promotion by some media, I think it is largely due to the promotion by brightmart. It must be said that brightmart made an indelible contribution to the popularization of ALBERT in China. Long before the English version of ALBERT was released, brightmart trained and open-sourced the Chinese version of ALBERT (<a href="https://github.com/brightmart/albert_zh">albert_zh</a>) and released tiny, small, base, large, and xlarge versions in one go. At that time, BERT only had base and large versions, while ALBERT had tiny and small versions. When people tried them, they found them significantly faster than BERT, so many were left with the impression that ALBERT is very fast. In fact, ALBERT being fast has nothing to do with "ALBERT" specifically; the point is the tiny/small architecture. Corresponding BERT tiny/small models are also very fast...</p>

    <p>Of course, you can ponder the deeper reasons why ALBERT's parameter sharing works, or research how to improve prediction speed after parameter sharing. These are valuable questions, but it is not recommended that you use a version of ALBERT lower than xlarge.</p>

    <h3>ELECTRA</h3>
    <p>ELECTRA comes from the paper <a href="https://arxiv.org/abs/2003.10555">"ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators."</a> To be honest, ELECTRA is a complicated model to talk about. It excited many people when it first came out, then disappointed many after its official release. Its current practical performance cannot be called bad, but it isn't exceptionally good either.</p>

    <h4>Characteristics</h4>
    <p>ELECTRA's starting point is that BERT's MLM model—randomly selecting a portion of tokens to Mask—is too simple. It wanted to increase the difficulty. So, it borrowed ideas from GANs, using a standard method to train an MLM model (Generator), then sampling and replacing tokens in the input sentence based on the MLM model's predictions. The processed sentence is then input into another model (Discriminator) to determine which parts of the sentence were replaced and which were not. The generator and discriminator are trained simultaneously; as the generator trains, the difficulty of judgment gradually increases, which theoretically helps the model learn more valuable content. Finally, only the Discriminator's Encoder is kept for use, and the generator is generally discarded.</p>

    <p>Because this progressive mode makes the training process more targeted, ELECTRA's main highlight is <strong>higher training efficiency</strong>. According to the paper, it can achieve BERT-level results of the same spec in 1/4 of the time or less. This is ELECTRA's primary advantage.</p>

    <h4>Theory</h4>
    <p>However, in my view, ELECTRA is a model that is <strong>theoretically questionable</strong>. Why? ELECTRA's idea stems from GANs, but in Computer Vision, do we have examples of taking a trained GAN discriminator and fine-tuning it for downstream tasks? At least, I haven't seen them. In fact, this is theoretically problematic. Take a basic GAN: its optimal discriminator solution is $D(x)=\frac{p(x)}{p(x)+q(x)}$, where $p(x)$ and $q(x)$ are the distributions of real and fake samples, respectively. Assuming training is stable and the generator has sufficient fitting capacity, as training progresses, fake samples will gradually trend toward real samples, so $q(x)$ tends toward $p(x)$, and $D(x)$ tends toward the constant $1/2$. That is, theoretically, the final discriminator is just a constant function. How can you guarantee it extracts good features?</p>

    <p>While ELECTRA is not exactly a GAN, it shares this point. Thus, ELECTRA emphasizes that the MLM model acting as the generator cannot be too complex (otherwise, as stated, the discriminator would degrade into a constant). The paper says performance is best when the generator size is between $1/4$ and $1/2$ of the discriminator. This starts to become <strong>"alchemy"</strong> (black box heuristics). We only argued that it shouldn't be too good, but we can't prove why being a bit worse works, nor how much worse it should be, nor why simultaneous training is better. These have all become purely experimental heuristics.</p>

    <h4>Performance</h4>
    <p>Of course, saying it is theoretically questionable does not mean its performance is bad, nor that the evaluations were falsified. Because constraints were placed on the generator's capacity, ELECTRA's training results still have significance, and its performance is decent. It's just that it put us through a process of "the greater the expectation, the greater the disappointment."</p>

    <p>The ELECTRA paper first appeared as a submission to ICLR 2020. At that time, the results shocked everyone—essentially that the small version of ELECTRA far exceeded the small version of BERT and even approached the base version, while the base version of ELECTRA reached the level of BERT-large. However, when the code and weights were released, the scores shown on <strong>GitHub</strong> were "eye-popping"—they dropped by about 2 percentage points. Later, the authors clarified, saying the paper reported dev set results while GitHub reported test set results. People understood a bit more, but because of this, ELECTRA's performance <strong>became less of a highlight compared to BERT</strong>. (Refer from <a href="https://zhuanlan.zhihu.com/p/89763176">"ELECTRA: Surpassing BERT, the best pre-trained NLP model of 2019"</a> to <a href="https://zhuanlan.zhihu.com/p/113123891">"My thoughts on the release of ELECTRA source code"</a>).</p>

    <p style="text-align:center;"><img src="https://cdn.jsdelivr.net/gh/bojone/bert_models/electra_paper.png" alt="ELECTRA Paper Results"></p>
    <p style="text-align:center;"><img src="https://cdn.jsdelivr.net/gh/bojone/bert_models/electra_github.png" alt="ELECTRA GitHub Results"></p>

    <p>In fact, evaluation on Chinese tasks more accurately reflects this point. For example, in <a href="https://github.com/ymcui/Chinese-ELECTRA">Chinese-ELECTRA</a> released by HFL, ELECTRA's performance across various tasks is nearly identical to BERT of the same level; it has advantages in a few tasks, but "crushing" results did not appear.</p>

    <h4>Losing One for Another</h4>
    <p>Some readers might think: even if performance is similar, the pre-training is faster, which is a merit after all. I don't deny this. However, a new Arxiv paper recently suggested that ELECTRA's "similar performance" <strong>might just be an illusion on simple tasks</strong>. If complex tasks are constructed, it might still be "beaten" by BERT.</p>

    <p>The paper is titled <a href="https://arxiv.org/abs/2010.12871">"Commonsense knowledge adversarial dataset that challenges ELECTRA."</a> The authors built a new dataset, QADS, based on SQuAD 2.0 by using synonym substitution. According to the authors' tests, an ELECTRA-large model that can reach 88% on SQuAD 2.0 gets only 22% on QADS, while interestingly, BERT can still manage over 60%. Of course, this paper seems a bit unpolished and hasn't received authoritative validation yet, so it can't be fully trusted, but its results already prompt reflection on ELECTRA. Previously, the paper <a href="https://arxiv.org/abs/1907.13512">"Probing Neural Network Comprehension of Natural Language Arguments"</a> knocked BERT off its pedestal with just the word "not"; it seems ELECTRA might have even more severe issues of this kind.</p>

    <p>Setting aside other evidence, I feel ELECTRA's eventual abandonment of MLM itself is a <strong>"losing one for another"</strong> operation. If you say your starting point is that MLM is too simple, then you should find ways to increase the difficulty of MLM. Why replace MLM with a discriminator? Directly using a generator network to improve the MLM model (instead of swapping it for a discriminator) is possible. Microsoft's paper <a href="https://arxiv.org/abs/2010.03554">"Variance-reduced Language Pretraining via a Mask Proposal Network"</a> recently provided such a reference scheme. It lets a generator choose the positions to be masked instead of choosing randomly. Although I haven't replicated its experiments, the entire reasoning process feels very convincing, unlike the "shooting from the hip" feel of ELECTRA. Additionally, I want to emphasize that <strong>MLM is very useful</strong>. It is not just a pre-training task; it is also a model with practical value—for example, <a href="translation_7764.html">"Do we need GPT3? No, BERT's MLM can also do few-shot learning."</a></p>

    <h4>Conclusion</h4>
    <p>So, after all that, the conclusion is: <strong>ELECTRA's pre-training speed has indeed increased, but current experimental evidence suggests it has no prominent advantage in downstream tasks over BERT of the same level. You can try it, but don't be too disappointed if performance drops.</strong></p>

    <p>Furthermore, if you need to use the weights of the MLM part (for example, to do text generation with UniLM, refer <a href="translation_7162.html">here</a>), you cannot use ELECTRA because ELECTRA's body is a discriminator, not an MLM model. The MLM model used as the generator in ELECTRA is simplified compared to the discriminator and may suffer from insufficient fitting or inadequate learning, making it a poor pre-trained MLM model.</p>

    <p>As for the idea behind ELECTRA—improving upon the simplicity of BERT's random masking—the direction seems correct. However, the validity of swapping a generative model for a discriminative one still requires further verification. Readers interested in deep analysis can certainly explore this further.</p>

    <h3>Article Summary</h3>
    <p>This article records my views and thoughts on ALBERT and ELECTRA, synthesizing my own experimental results and referring to several sources. I hope to objectively express the pros and cons of these two models so that readers feel more confident when choosing models. Both models have their merits in specific scenarios but also certain limitations; understanding these limitations and their origins will help readers use these models better.</p>

    <p>I have no intention of maliciously disparaging any model. If there are any misunderstandings, everyone is welcome to leave a comment for discussion.</p>

    <p>
        Address for reprinting: <a href="https://kexue.fm/archives/7846">https://kexue.fm/archives/7846</a><br>
        For detailed reprinting matters, please refer to: <a href="https://kexue.fm/faq">"Scientific Space FAQ"</a>
    </p>

    <p>If you have any further questions or suggestions, please continue the discussion in the comment section below. If you found this article helpful, feel free to share or tip. Tipping is not about profit but to know how much sincere attention "Scientific Space" has received. Of course, ignoring it will not affect your reading. Thanks again!</p>

    <hr>
    <p>If you need to cite this article, please refer to:</p>
    <p>Su Jianlin. (Oct. 29, 2020). <i>"Before using ALBERT and ELECTRA, make sure you really understand them"</i> [Blog post]. Retrieved from https://kexue.fm/archives/7846</p>

</article>
<hr>
<footer style="margin-top: 3em; padding: 1.5em; background: #f5f5f5; border-radius: 8px; font-size: 0.9em; color: #555;">
    <p style="margin: 0 0 0.5em 0;"><strong>Citation</strong></p>
    <p style="margin: 0 0 0.5em 0;">
        This is a machine translation of the original Chinese article:<br>
        <a href="https://kexue.fm/archives/7846" style="color: #005fcc;">https://kexue.fm/archives/7846</a>
    </p>
    <p style="margin: 0 0 0.5em 0;">
        Original author: 苏剑林 (Su Jianlin)<br>
        Original publication: <a href="https://kexue.fm" style="color: #005fcc;">科学空间 (Scientific Spaces)</a>
    </p>
    <p style="margin: 0; font-style: italic;">
        Translated using Gemini 3 Flash. Please refer to the original for authoritative content.
    </p>
</footer>
