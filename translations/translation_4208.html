
    <style type="text/css">

    body {
    margin: 48px auto;
    max-width: 68ch;              /* character-based width reads better */
    padding: 0 16px;

    font-family: -apple-system, BlinkMacSystemFont, "Segoe UI",
                Roboto, "Helvetica Neue", Arial, sans-serif;
    font-size: 18px;
    line-height: 1.65;
    color: #333;
    background: #fafafa;
    }

    h1, h2, h3, h4 {
    line-height: 1.25;
    margin-top: 2.2em;
    margin-bottom: 0.6em;
    font-weight: 600;
    }

    h1 {
    font-size: 2.1em;
    margin-top: 0;
    }

    h2 {
    font-size: 1.6em;
    border-bottom: 1px solid #e5e5e5;
    padding-bottom: 0.3em;
    }

    h3 {
    font-size: 1.25em;
    }

    h4 {
    font-size: 1.05em;
    color: #555;
    }

    /* Paragraphs and lists */
    p {
    margin: 1em 0;
    }

    ul, ol {
    margin: 1em 0 1em 1.5em;
    }

    li {
    margin: 0.4em 0;
    }

    a {
    color: #005fcc;
    text-decoration: none;
    }

    a:hover {
    text-decoration: underline;
    }

    code {
    font-family: ui-monospace, SFMono-Regular, Menlo, Consolas, monospace;
    font-size: 0.95em;
    background: #f2f2f2;
    padding: 0.15em 0.35em;
    border-radius: 4px;
    }

    pre {
    font-family: ui-monospace, SFMono-Regular, Menlo, Consolas, monospace;
    font-size: 0.9em;
    background: #f5f5f5;
    padding: 1em 1.2em;
    overflow-x: auto;
    border-radius: 6px;
    line-height: 1.45;
    }

    pre code {
    background: none;
    padding: 0;
    }

    blockquote {
    margin: 1.5em 0;
    padding-left: 1em;
    border-left: 4px solid #ddd;
    color: #555;
    }

    hr {
    border: none;
    border-top: 1px solid #e0e0e0;
    margin: 3em 0;
    }

    table {
    border-collapse: collapse;
    margin: 1.5em 0;
    width: 100%;
    font-size: 0.95em;
    }

    th, td {
    padding: 0.5em 0.7em;
    border-bottom: 1px solid #e5e5e5;
    text-align: left;
    }

    th {
    font-weight: 600;
    }

    img {
    max-width: 100%;
    display: block;
    margin: 1.5em auto;
    }

    small {
    color: #666;
    }

    mjx-container {
    margin: 1em 0;
    }

    ::selection {
    background: #cce2ff;
    }

    </style>
    

<script>
window.MathJax = {
  tex: {
    inlineMath: [['$', '$'], ['\\(', '\\)']],
    displayMath: [['$$', '$$'], ['\\[', '\\]']],
    processEscapes: true,
    tags: 'ams'
  }
};
</script>
<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"></script>

<article>
    <h1><a href="https://kexue.fm/archives/4208">SVD Decomposition (Part 1): Autoencoders and Artificial Intelligence</a></h1>
    <p>By 苏剑林 | January 15, 2017</p>

    <p>At first glance, SVD decomposition appears to be a traditional data mining technique, while autoencoders are a relatively "advanced" concept in deep learning; one might assume they have little in common. However, this article aims to show that if we ignore activation functions, the two are equivalent. Further reflection reveals that whether it is SVD or an autoencoder, we perform dimensionality reduction not purely to reduce storage or computation, but as a <strong>preliminary manifestation of "intelligence."</strong></p>

    <h2>Equivalence</h2>
    <p>Suppose we have a massive matrix $M_{m\times n}$ with $m$ rows and $n$ columns. Performing calculations or even storing this matrix can be problematic. Therefore, we consider a decomposition, hoping to find matrices $A_{m\times k}$ and $B_{k\times n}$ such that:
    $$M_{m\times n}=A_{m\times k}\times B_{k\times n}$$
    where the multiplication is matrix multiplication. As shown below:</p>

    <p>SVD</p>

    <p>In this way, what originally had $mn$ elements is now transformed into $(m+n)k$ elements on the right side. If $k$ is small enough, then $(m+n)k < mn$, which reduces both storage and computational requirements. Of course, if $k$ is sufficiently small, the equality usually does not hold strictly; it holds under the condition of minimizing the error. We will not make a strict distinction here. This is the core of the SVD decomposition described in this article. If the matrix $M_{m\times n}$ is originally sparse (such as a document matrix represented by one-hot encoding or a user rating matrix), $k$ can typically be several orders of magnitude smaller while keeping accuracy essentially unchanged. One of the uses of SVD is for topic modeling, recommendation systems, etc. (Note: SVD may be defined differently in some contexts, but at least this is its fundamental purpose.)</p>

    <p>So, what about autoencoders? Here, let's ignore activation functions for a moment and look only at the linear structure. An autoencoder aims to train an identity function $f(x)=x$, but with a compression at the hidden layer.</p>

    <p>Autoencoder Network Structure</p>

    <p>Mathematically, we want to find matrices $C_{n\times k}$ and $D_{k\times n}$ such that:
    $$M_{m\times n} \approx M_{m\times n} \times C_{n\times k}\times D_{k\times n}$$
    The equality here is not strictly equal but is optimal in some sense. The diagram is as follows:</p>

    <p>Autoencoder</p>

    <p>Consequently, if we let:
    $$A_{m\times k} = M_{m\times n} \times C_{n\times k},\quad B_{k\times n} = D_{k\times n}$$
    aren't the two equivalent? As long as they are optimized under the same error function (loss function), the resulting outcomes must be equivalent.</p>

    <p>Thus, we have proven that <strong>a three-layer autoencoder without activation functions is actually equivalent to traditional SVD decomposition.</strong> Despite this equivalence, the autoencoder remains an innovation because it transforms matrix decomposition into a neural network compression and encoding problem, making it clearer and easier to understand. Furthermore, it allows for mini-batch training, unlike SVD which traditionally requires all data to be input at once (though one could conceive of a "batch SVD" algorithm by imitating neural networks, it might not be particularly meaningful).</p>

    <h2>Compression and Intelligence</h2>
    <p>While autoencoders are more intuitive regarding compression and encoding, the physical meaning of SVD is more explicit. This shows that looking at the same thing from different perspectives is very meaningful. The physical meaning of SVD can be observed if we consider document encoding, as shown in the figure below, where each column represents a word and each row represents an article. For example, there are five words A, B, C, D, E, and 6 articles.</p>

    <p>Physical Meaning of SVD</p>

    <p>As shown, we can view the physical meaning of SVD this way: for the first decomposed matrix, we can see it as a clustering of rows—that is, a simple clustering of documents. For the second decomposed matrix, we can see it as a clustering of columns—that is, a clustering of words. Matrix multiplication then signifies the matching between document clusters and word clusters. Readers might ask: why must words and documents be clustered into the same number of categories? Why not different numbers? In fact, a more reasonable formulation is:
    $$M_{m\times n}=A_{m\times k}\times P_{k\times l} \times Q_{l\times n}$$
    This is equivalent to saying documents are clustered into $k$ categories, words are clustered into $l$ categories, and $P_{k\times l}$ is the matching matrix between document clusters and word clusters. Then, simply setting $B_{k\times n} = P_{k\times l} \times Q_{l\times n}$ returns us to the original form.</p>

    <p><strong>Why construct such a seemingly forced explanation? My purpose is to explain why "intelligence" is manifested in artificial models. We know that humans have memory, but after reaching a certain level of memory, we are often not satisfied with mere rote memorization. Instead, we find the patterns in what needs to be remembered and memorize through patterns. For instance, in language, we categorize words into verbs, nouns, etc., and then we discover patterns like "verb + noun" usually forming a phrase. We may not always be able to articulate these patterns, but it is undeniable that our brains are undergoing such processes of "pattern finding" and "categorization." From a data mining perspective, I believe this is a process of compression and clustering. That is, by reconstructing data after compression, we can mine the commonalities of the data—what we call patterns—and obtain more generalized results.</strong></p>

    <p>This may answer certain questions, such as: why do deep learning-based word segmentation models (whether LSTM or CNN) have such good recognition capabilities for out-of-vocabulary (OOV) words? It is because they contain the structure of an autoencoder, which is equivalent to SVD. As we explained earlier, SVD carries the physical meaning of clustering (pattern finding), making it more generalized and higher performing.</p>

    <h2>Deriving New Words</h2>
    <p>Too abstract? Can't follow? Don't worry, here is an example. This example involves our understanding of words. Please look at the following table:
    $$\begin{array}{c|c|c|c|c|c}
    \hline
    & \text{Beast (兽)} & \text{World (界)} & \text{Dragon (龙)} & \text{Spirit (灵)} & \text{Art/Technique (术)} \\
    \hline
    \text{God (神)} & 1 & 1 & 1 & 1 & 0 \\
    \hline
    \text{Magic/Demon (魔)} & 1 & 1 & 1 & 1 & 1 \\
    \hline
    \end{array}$$</p>

    <p>Assume these are statistics from a science fiction novel. They represent that terms like "God-Beast," "God-World," "God-Dragon," "God-Spirit," "Magic-Beast," "Magic-World," "Magic-Dragon," "Magic-Spirit," and "Magic-Art" appeared in the novel with a frequency of 1, but the term "God-Art" (神术) never appeared. Would we then infer: "Magic" (魔) and "God" (神) seem quite similar in usage, so could "God-Art" be a valid term? How do we express this mathematically?</p>

    <p>Surprisingly, one way to do this is SVD! In other words, we should perform some clustering on the usage of the first characters (perhaps as adjectives, verbs, or nouns?) and clustering on the usage of the second characters, and then consider the pairings between clusters. Of course, we don't actually need to tell the computer to cluster them as nouns or verbs; computers do not need to understand language in the way humans do. We only need to tell the computer to perform clustering, and since SVD inherently implies clustering, we just need to perform SVD. The specific experiment is as follows:</p>

    <blockquote>
    Select all two-character words with a frequency of at least 100 from the Jieba segmentation dictionary. Then, use the first character as the row, the second character as the column, and the frequency as the value (if the word does not exist, mark it as 0) to obtain a matrix. Perform SVD decomposition on this matrix to get two matrices, then multiply them back together (reconstruction). Treat the reconstructed matrix values as new frequencies and compare them to the original matrix to see which high-frequency words have emerged that were not there before.
    </blockquote>

    <p>As a result, I obtained some words that were not in the input dictionary, such as (the following is just a small sample):</p>

    <p>
    龙脑 (Dragon-Brain) 10.271244<br>
    龙脚 (Dragon-Foot) 12.496673<br>
    龙腊 (Dragon-Wax) 16.860170<br>
    龙腿 (Dragon-Leg) 12.172765<br>
    龙莲 (Dragon-Lotus) 11.362767<br>
    龙蔗 (Dragon-Cane) 67.800909<br>
    龙薯 (Dragon-Potato) 30.580730<br>
    七讲 (Seven-Lectures) 11.439969<br>
    七评 (Seven-Reviews) 12.362163<br>
    七课 (Seven-Lessons) 11.587767<br>
    七郎 (Seven-Groom) 12.789438<br>
    七隐 (Seven-Hidden) 10.479609<br>
    七页 (Seven-Pages) 15.499356<br>
    七项 (Seven-Items) 18.802959<br>
    怨怒 (Resentment-Anger) 29.831461<br>
    怨恶 (Resentment-Evil) 15.875075<br>
    怨苦 (Resentment-Bitterness) 24.979326<br>
    怪兔 (Strange-Rabbit) 10.246062<br>
    怪奇 (Strange-Odd) 12.768122<br>
    怪孩 (Strange-Child) 14.391967<br>
    怪形 (Strange-Shape) 14.856068
    </p>

    <p>Among these words, many are unreliable; in fact, there are far more unreliable ones than reliable ones. But that doesn't matter, because if we process natural language, those unreliable words would not appear in a real-world environment. This leads to a fascinating result: we perform SVD decomposition originally for purposes like compression and dimensionality reduction, yet after reconstruction, it derives richer results with a larger vocabulary. This appears to be the result of clustering and pattern finding. <strong>This is different from statistical-based new word discovery; these new words do not exist in the corpus but are derived through the construction patterns of existing words. This is a true process of inference—namely, "intelligence." To put it hyperbolically, SVD brought about a preliminary form of artificial intelligence</strong> (though it is certainly not the only method).</p>

    <h2>Activation Function</h2>
    <p>What on earth is an activation function? In the experiments above, I also discovered a concrete physical meaning for the activation function. When we decompose a matrix with SVD and then reconstruct it, some elements in the reconstructed matrix will be negative. However, the meaning of our matrix is frequency, and frequency cannot be negative. Therefore, we discard (truncate) the negative numbers and set them to 0.</p>

    <p>Wait? Isn't this exactly the operation of the function $\max(x, 0)$? Isn't this the ReLU activation function that is most widely used? It turns out that the physical meaning of the activation function here is nothing more than the discarding (truncation) of irrelevant elements. This operation is something we have already been using in statistics for a long time (discarding negative numbers, discarding small statistical results, etc.).</p>

    <p>Of course, in neural networks, activation functions have more profound meanings, but in shallow networks (matrix decomposition), the intuitive impression they give us is simply truncation.</p>

    <h2>Summary</h2>
    <p>This article centered on SVD to explore its inherent physical meaning (clustering), the connection between SVD decomposition and autoencoders, the intricate relationship between SVD and intelligence, and a related experiment. Most of this content is conceptual and geared towards understanding—deepening our grasp of models and artificial intelligence so that we have more "confidence" when applying models, rather than just blindly plugging into them.</p>

    </article>
<hr>
<footer style="margin-top: 3em; padding: 1.5em; background: #f5f5f5; border-radius: 8px; font-size: 0.9em; color: #555;">
    <p style="margin: 0 0 0.5em 0;"><strong>Citation</strong></p>
    <p style="margin: 0 0 0.5em 0;">
        This is a machine translation of the original Chinese article:<br>
        <a href="https://kexue.fm/archives/4208" style="color: #005fcc;">https://kexue.fm/archives/4208</a>
    </p>
    <p style="margin: 0 0 0.5em 0;">
        Original author: 苏剑林 (Su Jianlin)<br>
        Original publication: <a href="https://kexue.fm" style="color: #005fcc;">科学空间 (Scientific Spaces)</a>
    </p>
    <p style="margin: 0; font-style: italic;">
        Translated using Gemini 3 Flash. Please refer to the original for authoritative content.
    </p>
</footer>
