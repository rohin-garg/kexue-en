
    <style type="text/css">

    body {
    margin: 48px auto;
    max-width: 68ch;              /* character-based width reads better */
    padding: 0 16px;

    font-family: -apple-system, BlinkMacSystemFont, "Segoe UI",
                Roboto, "Helvetica Neue", Arial, sans-serif;
    font-size: 18px;
    line-height: 1.65;
    color: #333;
    background: #fafafa;
    }

    h1, h2, h3, h4 {
    line-height: 1.25;
    margin-top: 2.2em;
    margin-bottom: 0.6em;
    font-weight: 600;
    }

    h1 {
    font-size: 2.1em;
    margin-top: 0;
    }

    h2 {
    font-size: 1.6em;
    border-bottom: 1px solid #e5e5e5;
    padding-bottom: 0.3em;
    }

    h3 {
    font-size: 1.25em;
    }

    h4 {
    font-size: 1.05em;
    color: #555;
    }

    /* Paragraphs and lists */
    p {
    margin: 1em 0;
    }

    ul, ol {
    margin: 1em 0 1em 1.5em;
    }

    li {
    margin: 0.4em 0;
    }

    a {
    color: #005fcc;
    text-decoration: none;
    }

    a:hover {
    text-decoration: underline;
    }

    code {
    font-family: ui-monospace, SFMono-Regular, Menlo, Consolas, monospace;
    font-size: 0.95em;
    background: #f2f2f2;
    padding: 0.15em 0.35em;
    border-radius: 4px;
    }

    pre {
    font-family: ui-monospace, SFMono-Regular, Menlo, Consolas, monospace;
    font-size: 0.9em;
    background: #f5f5f5;
    padding: 1em 1.2em;
    overflow-x: auto;
    border-radius: 6px;
    line-height: 1.45;
    }

    pre code {
    background: none;
    padding: 0;
    }

    blockquote {
    margin: 1.5em 0;
    padding-left: 1em;
    border-left: 4px solid #ddd;
    color: #555;
    }

    hr {
    border: none;
    border-top: 1px solid #e0e0e0;
    margin: 3em 0;
    }

    table {
    border-collapse: collapse;
    margin: 1.5em 0;
    width: 100%;
    font-size: 0.95em;
    }

    th, td {
    padding: 0.5em 0.7em;
    border-bottom: 1px solid #e5e5e5;
    text-align: left;
    }

    th {
    font-weight: 600;
    }

    img {
    max-width: 100%;
    display: block;
    margin: 1.5em auto;
    }

    small {
    color: #666;
    }

    mjx-container {
    margin: 1em 0;
    }

    ::selection {
    background: #cce2ff;
    }

    </style>
    

<script>
window.MathJax = {
  tex: {
    inlineMath: [['$', '$'], ['\\(', '\\)']],
    displayMath: [['$$', '$$'], ['\\[', '\\]']],
    tags: 'ams',
    processEscapes: true,
    packages: {'[+]': ['base', 'ams', 'noerrors', 'noundefined']}
  },
  loader: {load: ['[tex]/ams']}
};
</script>
<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

<article>
    <h1><a href="https://kexue.fm/archives/7469">Why Gradient Clipping Accelerates Training: A Concise Analysis</a></h1>

    <p>By 苏剑林 | June 05, 2020</p>

    <p>This article introduces a perfect-score paper from MIT presented at ICLR 2020 titled <a href="https://openreview.net/forum?id=BJgnXpVYwS">"Why gradient clipping accelerates training: A theoretical justification for adaptivity"</a>. As the name suggests, this paper analyzes why gradient clipping can accelerate the training process in deep learning. The original paper is very long, filled with formulas, and contains many concepts from complexity research. To be honest, I was also confused by much of its content, but I was able to capture its core idea: it introduces a more relaxed constraint than the commonly used L-constraint and demonstrates the necessity of gradient clipping based on these new conditions. This article aims to provide a brief analysis of this process for the readers' reference.</p>

    <h3>Gradient Clipping</h3>
    <p>Assume the function to be minimized is $f(\theta)$, where $\theta$ represents the optimization parameters. The update formula for gradient descent is:</p>
    \begin{equation}\theta \leftarrow \theta-\eta \nabla_{\theta} f(\theta)\end{equation}
    <p>where $\eta$ is the learning rate. So-called gradient clipping involves scaling the update amount based on the norm of the gradient, for example:</p>
    \begin{equation}\theta \leftarrow \theta- \eta \nabla_{\theta} f(\theta)\times \min\left\{1, \frac{\gamma}{\Vert \nabla_{\theta} f(\theta)\Vert}\right\}\label{eq:clip-1}\end{equation}
    <p>or</p>
    \begin{equation}\theta \leftarrow \theta- \eta \nabla_{\theta} f(\theta)\times \frac{\gamma}{\Vert \nabla_{\theta} f(\theta)\Vert+\gamma}\label{eq:clip-2}\end{equation}
    <p>where $\gamma > 0$ is a constant. Both of these forms are considered gradient clipping. Generally speaking, they control the norm of the update so that it does not exceed a constant. The second form is also related to adaptive learning rate optimizers such as RMSProp. Furthermore, more precisely, we have the following inequality:</p>
    \begin{equation}\frac{1}{2}\min\left\{1, \frac{\gamma}{\Vert \nabla_{\theta} f(\theta)\Vert}\right\}\leq \frac{\gamma}{\Vert \nabla_{\theta} f(\theta)\Vert+\gamma}\leq \min\left\{1, \frac{\gamma}{\Vert \nabla_{\theta} f(\theta)\Vert}\right\}\end{equation}
    <p>In other words, the two can bound each other, so they are essentially equivalent.</p>

    <h3>L-Constraint</h3>
    <p>Many theoretical results related to optimizers assume in their proofs that the gradient of the function to be optimized $f(\theta)$ satisfies the following L-constraint:</p>
    \begin{equation}\Vert \nabla_{\theta} f(\theta + \Delta \theta) - \nabla_{\theta} f(\theta)\Vert\leq L\Vert \Delta\theta\Vert\label{eq:l-cond}\end{equation}
    <p>Since $\frac{\Vert \nabla_{\theta} f(\theta + \Delta \theta) - \nabla_{\theta} f(\theta)\Vert}{\Vert \Delta\theta\Vert}$ represents the degree of fluctuation in the gradient, it essentially measures the smoothness of $f(\theta)$. Therefore, the above constraint is also called the "L-smoothness condition (L-smooth)."</p>
    <p>Regarding the L-constraint, it has appeared many times in this blog. You can refer to articles such as <a href="translation_6051.html">"Lipschitz Continuity in Deep Learning: Generalization and Generative Models"</a> and <a href="translation_6134.html">"What Exactly Does BN Do? A 'Behind-Closed-Doors' Analysis"</a>. It is worth noting that different scenarios may require different L-constraints. For example, sometimes we assume the <i>model output</i> with respect to the <i>input</i> satisfies the L-constraint; sometimes we assume the <i>model output</i> with respect to <i>parameters</i> satisfies the L-constraint. What is assumed above is that the <i>gradient of the model loss</i> with respect to <i>parameters</i> satisfies the L-constraint.</p>
    <p>If condition $\eqref{eq:l-cond}$ holds, many optimization problems are greatly simplified. Because we can prove (the proof process can be found <a href="https://spaces.ac.cn/archives/5655">here</a>):</p>
    \begin{equation}f(\theta+\Delta\theta) \leq f(\theta) + \left\langle \nabla_{\theta}f(\theta), \Delta\theta\right\rangle + \frac{1}{2}L \Vert \Delta\theta\Vert^2\label{eq:neq-1}\end{equation}
    <p>For gradient descent, $\Delta\theta = -\eta \nabla_{\theta} f(\theta)$. Substituting this into the above equation gives:</p>
    \begin{equation}f(\theta+\Delta\theta) \leq f(\theta) + \left(\frac{1}{2}L\eta^2 - \eta\right) \Vert \nabla_{\theta}f(\theta)\Vert^2\end{equation}
    <p>Therefore, to ensure that every optimization step causes $f(\theta)$ to decrease, a sufficient condition is $\frac{1}{2}L\eta^2 - \eta < 0$, i.e., $\eta < \frac{2}{L}$. The minimum value of $\frac{1}{2}L\eta^2 - \eta$ is reached when $\eta^* = \frac{1}{L} < \frac{2}{L}$. So, by setting the learning rate to $\frac{1}{L}$, every iteration can ensure that $f(\theta)$ decreases, and at the fastest rate.</p>

    <h3>Relaxing the Constraint</h3>
    <p>Condition $\eqref{eq:l-cond}$ can lead to many beautiful results. However, the problem is that in many practical optimization problems, condition $\eqref{eq:l-cond}$ does not hold—for example, with the quartic function $f(\theta)=\theta^4$. This leads to a gap between theory and practice. The paper introduced in this article introduces a new, more relaxed constraint:</p>
    \begin{equation}\Vert \nabla_{\theta} f(\theta + \Delta \theta) - \nabla_{\theta} f(\theta)\Vert\leq \left(L_0 + L_1\Vert \nabla_{\theta} f(\theta)\Vert\right)\Vert \Delta\theta\Vert\end{equation}
    <p>That is, the constant $L$ is replaced by a dynamic term $L_0 + L_1\Vert \nabla_{\theta} f(\theta)\Vert$. The original paper calls this "$(L_0, L_1)$-smooth," which we also refer to here as the "$(L_0, L_1)$ constraint." Clearly, this condition is much more relaxed. For instance, one can verify that $\theta^4$ satisfies this condition. Therefore, theoretical results derived based on this condition will have a broader scope of application.</p>
    <p>How did the authors propose this condition? The paper states it was observed through experiments: the paper observed that the smoothness of the loss function is "linearly correlated" with the gradient norm. But I suspect there must be some element of reverse engineering from the desired results involved; otherwise, who would be so bored as to observe the relationship between these two factors?</p>
    <p>Under the new constraint, inequality $\eqref{eq:neq-1}$ still holds, but $L$ is replaced by the corresponding dynamic term:</p>
    \begin{equation}f(\theta+\Delta\theta) \leq f(\theta) + \left\langle \nabla_{\theta}f(\theta), \Delta\theta\right\rangle + \frac{1}{2}\left(L_0 + L_1\Vert \nabla_{\theta} f(\theta)\Vert\right) \Vert \Delta\theta\Vert^2\end{equation}
    <p>Substituting $\Delta\theta = -\eta \nabla_{\theta} f(\theta)$, we get:</p>
    \begin{equation}f(\theta+\Delta\theta) \leq f(\theta) + \left(\frac{1}{2}\left(L_0 + L_1\Vert \nabla_{\theta} f(\theta)\Vert\right)\eta^2 - \eta\right) \Vert \nabla_{\theta}f(\theta)\Vert^2\end{equation}
    <p>So it becomes very clear: to ensure a decrease at every step, we now require:</p>
    \begin{equation}\eta < \frac{2}{L_0 + L_1\Vert \nabla_{\theta} f(\theta)\Vert}\end{equation}
    <p>And the optimal learning rate is:</p>
    \begin{equation}\eta^* = \frac{1}{L_0 + L_1\Vert \nabla_{\theta} f(\theta)\Vert}\end{equation}
    <p>This leads directly to gradient clipping as shown in $\eqref{eq:clip-2}$. Since it ensures that every step results in a decrease, it means no step is wasted during the optimization process, thereby accelerating training.</p>

    <h3>Summary</h3>
    <p>This article briefly introduced a perfect-score paper from ICLR 2020 that analyzes gradient clipping. The main idea is to introduce more relaxed and universal assumptions. Under these new conditions, the necessity of gradient clipping is revealed. Because traditional constraints are relaxed, the theoretical results have a broader scope of application. This indicates that gradient clipping is indeed a useful technique applicable in many scenarios.</p>

    <hr>
    <p>
        Original Address: <a href="https://kexue.fm/archives/7469">https://kexue.fm/archives/7469</a><br>
        If you need to cite this article, please refer to:<br>
        Jianlin Su. (Jun. 05, 2020). "Why Gradient Clipping Accelerates Training: A Concise Analysis" [Blog post]. Retrieved from https://kexue.fm/archives/7469
    </p>
    <pre>
@online{kexuefm-7469,
    title={Why Gradient Clipping Accelerates Training: A Concise Analysis},
    author={Jianlin Su},
    year={2020},
    month={Jun},
    url={\url{https://kexue.fm/archives/7469}},
}
    </pre>
</article>
<hr>
<footer style="margin-top: 3em; padding: 1.5em; background: #f5f5f5; border-radius: 8px; font-size: 0.9em; color: #555;">
    <p style="margin: 0 0 0.5em 0;"><strong>Citation</strong></p>
    <p style="margin: 0 0 0.5em 0;">
        This is a machine translation of the original Chinese article:<br>
        <a href="https://kexue.fm/archives/7469" style="color: #005fcc;">https://kexue.fm/archives/7469</a>
    </p>
    <p style="margin: 0 0 0.5em 0;">
        Original author: 苏剑林 (Su Jianlin)<br>
        Original publication: <a href="https://kexue.fm" style="color: #005fcc;">科学空间 (Scientific Spaces)</a>
    </p>
    <p style="margin: 0; font-style: italic;">
        Translated using Gemini 3 Flash. Please refer to the original for authoritative content.
    </p>
</footer>
