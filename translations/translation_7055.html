
    <style type="text/css">

    body {
    margin: 48px auto;
    max-width: 68ch;              /* character-based width reads better */
    padding: 0 16px;

    font-family: -apple-system, BlinkMacSystemFont, "Segoe UI",
                Roboto, "Helvetica Neue", Arial, sans-serif;
    font-size: 18px;
    line-height: 1.65;
    color: #333;
    background: #fafafa;
    }

    h1, h2, h3, h4 {
    line-height: 1.25;
    margin-top: 2.2em;
    margin-bottom: 0.6em;
    font-weight: 600;
    }

    h1 {
    font-size: 2.1em;
    margin-top: 0;
    }

    h2 {
    font-size: 1.6em;
    border-bottom: 1px solid #e5e5e5;
    padding-bottom: 0.3em;
    }

    h3 {
    font-size: 1.25em;
    }

    h4 {
    font-size: 1.05em;
    color: #555;
    }

    /* Paragraphs and lists */
    p {
    margin: 1em 0;
    }

    ul, ol {
    margin: 1em 0 1em 1.5em;
    }

    li {
    margin: 0.4em 0;
    }

    a {
    color: #005fcc;
    text-decoration: none;
    }

    a:hover {
    text-decoration: underline;
    }

    code {
    font-family: ui-monospace, SFMono-Regular, Menlo, Consolas, monospace;
    font-size: 0.95em;
    background: #f2f2f2;
    padding: 0.15em 0.35em;
    border-radius: 4px;
    }

    pre {
    font-family: ui-monospace, SFMono-Regular, Menlo, Consolas, monospace;
    font-size: 0.9em;
    background: #f5f5f5;
    padding: 1em 1.2em;
    overflow-x: auto;
    border-radius: 6px;
    line-height: 1.45;
    }

    pre code {
    background: none;
    padding: 0;
    }

    blockquote {
    margin: 1.5em 0;
    padding-left: 1em;
    border-left: 4px solid #ddd;
    color: #555;
    }

    hr {
    border: none;
    border-top: 1px solid #e0e0e0;
    margin: 3em 0;
    }

    table {
    border-collapse: collapse;
    margin: 1.5em 0;
    width: 100%;
    font-size: 0.95em;
    }

    th, td {
    padding: 0.5em 0.7em;
    border-bottom: 1px solid #e5e5e5;
    text-align: left;
    }

    th {
    font-weight: 600;
    }

    img {
    max-width: 100%;
    display: block;
    margin: 1.5em auto;
    }

    small {
    color: #666;
    }

    mjx-container {
    margin: 1em 0;
    }

    ::selection {
    background: #cce2ff;
    }

    </style>
    

<script>
window.MathJax = {
  tex: {
    inlineMath: [['$', '$'], ['\\(', '\\)']],
    displayMath: [['$$', '$$'], ['\\[', '\\]']],
    tags: 'ams',
    packages: {'[+]': ['ams']}
  }
};
</script>
<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

<article>
    <nav style="margin-bottom: 1.5em;">
    <a href="../index.html" style="display: inline-flex; align-items: center; color: #555; text-decoration: none; font-size: 0.95em;">
        <span style="margin-right: 0.3em;">&larr;</span> Back to Index
    </a>
</nav>

    <h1><a href="https://kexue.fm/archives/7055">Keras: The Gold Standard for Tensorflow</a></h1>
    <p>By 苏剑林 | November 06, 2019</p>

    <p>Over the past two weeks, I have invested a lot of energy into the development of <a href="https://github.com/bojone/bert4keras">bert4keras</a>. Besides some API standardization work, the primary workload was building the pre-training section of the code. Yesterday, the pre-training code was basically completed and tested successfully in both TPU and multi-GPU environments. This provides another option for students who are motivated (and have the computing power) to improve pre-trained models. This might be the most clear and easy-to-understand code for BERT and its pre-training available currently.</p>

    <p><strong>Pre-training code link: <a href="https://github.com/bojone/bert4keras/tree/master/pretraining">https://github.com/bojone/bert4keras/tree/master/pretraining</a></strong></p>

    <p>After these two weeks of development (and filling in holes), my biggest takeaway is that Keras has already become the gold standard for TensorFlow. As long as your code is written according to Keras standards and specifications, it can be easily migrated to <code>tf.keras</code> and then very easily trained in TPU or multi-GPU environments—truly almost a once-and-for-all solution. Conversely, if your style of writing is too flexible, including many "grafting" style Keras tricks I introduced before, you might encounter quite a few problems. It might even happen that even if you've got it running on multiple GPUs, you might struggle to get it working on a TPU no matter what.</p>

    <h2>Support Without Reservation</h2>
    <p>Everyone says TensorFlow 2.0 promotes <code>tf.keras</code> as the main component, but in fact, since TensorFlow 1.14, <code>tf.keras</code> has already become its gold standard. So if you want to experience how Father Google supports Keras without reservation, you only need TensorFlow 1.14+; you don't necessarily have to upgrade to 2.0. Currently, the <code>bert4keras</code> code supports both the original Keras and <code>tf.keras</code>. In typical single-card fine-tuning tasks, you can use either Keras or <code>tf.keras</code>, but if you want to use multi-GPU or even TPU training, the best choice is <code>tf.keras</code>.</p>

    <p>To get started with <code>tf.keras</code>, I first recommend referring to a very good website: <a href="https://tf.wiki/">https://tf.wiki/</a></p>

    <p>In <code>tf.keras</code>, converting a model from single-card to multi-card training is very simple:</p>

<pre><code>strategy = tf.distribute.MirroredStrategy()

with strategy.scope():
    model = create_a_model()
    model.compile(loss='mse', optimizer='adam')

model.fit(train_x, train_y, epochs=10)
</code></pre>

    <p>In other words, you only need to define a <code>strategy</code>, and then models built within the <code>scope</code> of this <code>strategy</code> become multi-card models. Multi-card training has never been this simple.</p>

    <p>By the way, Keras itself comes with the <code>multi_gpu_model</code> function to implement multi-GPU training, but based on my personal testing, <code>multi_gpu_model</code> doesn't work very well and sometimes fails to take effect. In short, I still recommend using <code>tf.keras</code>. Additionally, the example above is for single-machine multi-card setup; multi-machine multi-card is similar, but I don't have the environment to test it, so I haven't provided an example. For those who want to test it, please refer to the introduction in <a href="https://tf.wiki/">https://tf.wiki/</a>.</p>

    <p>What about TPUs? It's just as simple—just replace the <code>strategy</code> (note that TensorFlow 2.0 has some minor changes):</p>

<pre><code>resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu=tpu_address)
tf.config.experimental_connect_to_host(resolver.master())
tf.tpu.experimental.initialize_tpu_system(resolver)
strategy = tf.distribute.experimental.TPUStrategy(resolver)
</code></pre>

    <p>Does it feel incredibly simple? By now, everyone should understand what I meant by supporting Keras "without reservation." Starting from TensorFlow 1.14, as long as you use standard Keras writing style, you can be successful in almost everything.</p>

    <h2>What is Considered "Standard"?</h2>
    <p>I have repeatedly emphasized using standard Keras writing style. So, what counts as standard? Here is a summary of some experiences.</p>

    <blockquote>
        <p>1. Use Keras's built-in layers, loss functions, and optimizers as much as possible to implement the functions we need. If a model entirely uses Keras's built-in components, it essentially guarantees it will run on multi-GPU or TPU.</p>
        <p>2. If you want to customize layers, follow the specifications strictly, especially writing the <code>get_config</code> method well. A way to test if your writing is standardized is to use your custom layer to build a model and see if the <code>clone_model</code> function can successfully clone that model. If it can, your layer definition is standardized.</p>
        <p>3. If you want to use TPU training, do not use <code>add_loss</code> at the end of the model to define custom losses, and do not use <code>add_metric</code> to add metrics. If you need to define complex losses or metrics, please define them as the output of a layer, referring to <a href="translation_4493.html">this style of writing</a>.</p>
        <p>4. If you want to use TPU training, avoid using dynamic (variable-length) logic during the training process. For example, when using <code>tf.where</code>, the <code>x</code> and <code>y</code> parameters cannot be <code>None</code>, otherwise the result length of <code>tf.where</code> is uncertain. Also, almost all functions in TensorFlow with the word <code>dynamic</code> in them cannot be used.</p>
    </blockquote>

    <p>As you can see, the so-called "standard" is actually imitating existing Keras styles to the greatest extent possible, and minimizing self-invented methods. As long as you follow points 1 and 2, you can easily use multi-GPU training under <code>tf.keras</code>; points 3 and 4 are specifically for avoiding pitfalls with TPUs. Generally speaking, everything must be static.</p>

    <p>Although TensorFlow 2.0 has defaulted to eager execution (dynamic graphs), I do not recommend using it. Personally, I believe we should get used to the model construction workflow of static graphs. While dynamic graphs are convenient for debugging, they make us heavily dependent on immediate output results, reducing our ability to debug complex problems. Similarly, I do not recommend using tools like code completion or code hints; these tools make us too reliant and prevent us from truly understanding the functions themselves. (Personal opinion, please don't flame.)</p>

    <h2>The Victory of User-Friendliness</h2>
    <p>If I remember correctly, I first came into contact with Keras in early 2015. At that time, there weren't many deep learning frameworks. I just wanted to find a handy tool to implement a few simple models, so I found Keras and have been using it ever since. Not just me, but perhaps even the authors of Keras did not expect back then that Keras would become the gold standard for TensorFlow today.</p>

    <p>I feel this is no accident. TensorFlow has had many high-level API frameworks, such as TensorLayer, tf.slim, and TFLearn, but why was Keras eventually chosen? Beyond Keras's "long history," it lies in the fact that Keras is truly a worthy and elegant implementation of encapsulation. Over this past year, I occasionally read the Keras source code, and every time I do, I am struck by its rigor and elegance. It is a well-deserved work of art, a humane and user-friendly creation.</p>

    <p>Therefore, this is the victory of user-friendliness (humanity).</p>
</article>
<hr>
<footer style="margin-top: 3em; padding: 1.5em; background: #f5f5f5; border-radius: 8px; font-size: 0.9em; color: #555;">
    <p style="margin: 0 0 0.5em 0;"><strong>Citation</strong></p>
    <p style="margin: 0 0 0.5em 0;">
        This is a machine translation of the original Chinese article:<br>
        <a href="https://kexue.fm/archives/7055" style="color: #005fcc;">https://kexue.fm/archives/7055</a>
    </p>
    <p style="margin: 0 0 0.5em 0;">
        Original author: 苏剑林 (Su Jianlin)<br>
        Original publication: <a href="https://kexue.fm" style="color: #005fcc;">科学空间 (Scientific Spaces)</a>
    </p>
    <p style="margin: 0; font-style: italic;">
        Translated using Gemini 3 Flash. Please refer to the original for authoritative content.
    </p>
</footer>
