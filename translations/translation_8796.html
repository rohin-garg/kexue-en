
    <style type="text/css">

    body {
    margin: 48px auto;
    max-width: 68ch;              /* character-based width reads better */
    padding: 0 16px;

    font-family: -apple-system, BlinkMacSystemFont, "Segoe UI",
                Roboto, "Helvetica Neue", Arial, sans-serif;
    font-size: 18px;
    line-height: 1.65;
    color: #333;
    background: #fafafa;
    }

    h1, h2, h3, h4 {
    line-height: 1.25;
    margin-top: 2.2em;
    margin-bottom: 0.6em;
    font-weight: 600;
    }

    h1 {
    font-size: 2.1em;
    margin-top: 0;
    }

    h2 {
    font-size: 1.6em;
    border-bottom: 1px solid #e5e5e5;
    padding-bottom: 0.3em;
    }

    h3 {
    font-size: 1.25em;
    }

    h4 {
    font-size: 1.05em;
    color: #555;
    }

    /* Paragraphs and lists */
    p {
    margin: 1em 0;
    }

    ul, ol {
    margin: 1em 0 1em 1.5em;
    }

    li {
    margin: 0.4em 0;
    }

    a {
    color: #005fcc;
    text-decoration: none;
    }

    a:hover {
    text-decoration: underline;
    }

    code {
    font-family: ui-monospace, SFMono-Regular, Menlo, Consolas, monospace;
    font-size: 0.95em;
    background: #f2f2f2;
    padding: 0.15em 0.35em;
    border-radius: 4px;
    }

    pre {
    font-family: ui-monospace, SFMono-Regular, Menlo, Consolas, monospace;
    font-size: 0.9em;
    background: #f5f5f5;
    padding: 1em 1.2em;
    overflow-x: auto;
    border-radius: 6px;
    line-height: 1.45;
    }

    pre code {
    background: none;
    padding: 0;
    }

    blockquote {
    margin: 1.5em 0;
    padding-left: 1em;
    border-left: 4px solid #ddd;
    color: #555;
    }

    hr {
    border: none;
    border-top: 1px solid #e0e0e0;
    margin: 3em 0;
    }

    table {
    border-collapse: collapse;
    margin: 1.5em 0;
    width: 100%;
    font-size: 0.95em;
    }

    th, td {
    padding: 0.5em 0.7em;
    border-bottom: 1px solid #e5e5e5;
    text-align: left;
    }

    th {
    font-weight: 600;
    }

    img {
    max-width: 100%;
    display: block;
    margin: 1.5em auto;
    }

    small {
    color: #666;
    }

    mjx-container {
    margin: 1em 0;
    }

    ::selection {
    background: #cce2ff;
    }

    </style>
    

<script>
window.MathJax = {
  tex: {
    inlineMath: [['$', '$'], ['\\(', '\\)']],
    displayMath: [['$$', '$$'], ['\\[', '\\]']],
    processEscapes: true,
    tags: 'ams',
    packages: {'[+]': ['ams']}
  },
  options: {
    ignoreHtmlClass: 'tex2jax_ignore',
    processHtmlClass: 'tex2jax_process'
  },
  loader: {load: ['[tex]/ams']}
};
</script>
<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

<article>
<h1><a href="https://kexue.fm/archives/8796">An Inequality Between Input Gradient Penalty and Parameter Gradient Penalty</a></h1>

<p>By 苏剑林 | December 11, 2021</p>

<p>In this blog, we have discussed gradient penalty-related content multiple times. In terms of form, gradient penalty terms are divided into two types. One is the gradient penalty with respect to the input $\Vert\nabla_{\boldsymbol{x}} f(\boldsymbol{x};\boldsymbol{\theta})\Vert^2$, which we discussed in articles such as <a href="translation_7234.html">"A Brief Talk on Adversarial Training: Meaning, Methods, and Reflections (with Keras Implementation)"</a> and <a href="translation_7466.html">"Random Thoughts on Generalization: From Random Noise and Gradient Penalty to Virtual Adversarial Training"</a>. The other type is the gradient penalty with respect to parameters $\Vert\nabla_{\boldsymbol{\theta}} f(\boldsymbol{x};\boldsymbol{\theta})\Vert^2$, which we discussed in articles like <a href="translation_7787.html">"Optimization Algorithms from a Dynamical Perspective (V): Why the Learning Rate Should Not Be Too Small?"</a> and <a href="translation_7643.html">"Do We Really Need to Reduce Training Set Loss to Zero?"</a>.</p>

<p>In these related articles, both types of gradient penalties are claimed to have the ability to improve the generalization performance of the model. So, is there any connection between the two? I learned about an inequality between them from a recent paper by Google, <a href="https://papers.cool/arxiv/2111.15090">"The Geometric Occam's Razor Implicit in Deep Learning"</a>. This partially answers the question, and I feel it might be useful in the future, so I am taking some notes here.</p>

<h2>Final Result <a id="最终结果"></a></h2>

<p>Assume there is an $l$-layer MLP model, denoted as</p>
\begin{equation}\boldsymbol{h}^{(t+1)} = g^{(t)}(\boldsymbol{W}^{(t)}\boldsymbol{h}^{(t)}+\boldsymbol{b}^{(t)})\end{equation}
<p>where $g^{(t)}$ is the activation function of the current layer, $t\in\{1,2,\cdots,l\}$, and let $\boldsymbol{h}^{(1)}$ be $\boldsymbol{x}$, i.e., the original input of the model. For the convenience of the derivation below, we denote $\boldsymbol{z}^{(t+1)}=\boldsymbol{W}^{(t)}\boldsymbol{h}^{(t)}+\boldsymbol{b}^{(t)}$. The set of all parameters is $\boldsymbol{\theta}=\{\boldsymbol{W}^{(1)},\boldsymbol{b}^{(1)},\boldsymbol{W}^{(2)},\boldsymbol{b}^{(2)},\cdots,\boldsymbol{W}^{(l)},\boldsymbol{b}^{(l)}\}$. If $f$ is any scalar function of $\boldsymbol{h}^{(l+1)}$, then the following inequality holds:</p>
\begin{equation}\Vert\nabla_{\boldsymbol{x}} f\Vert^2\left(\frac{1 + \Vert \boldsymbol{h}^{(1)}\Vert^2}{\Vert\boldsymbol{W}^{(1)}\Vert^2 \Vert\nabla_{\boldsymbol{x}}\boldsymbol{h}^{(1)}\Vert^2}+\cdots+\frac{1 + \Vert \boldsymbol{h}^{(l)}\Vert^2}{\Vert\boldsymbol{W}^{(l)}\Vert^2 \Vert\nabla_{\boldsymbol{x}}\boldsymbol{h}^{(l)}\Vert^2}\right)\leq \Vert\nabla_{\boldsymbol{\theta}} f\Vert^2\label{eq:f}\end{equation}
<p>In the above equation, $\Vert\nabla_{\boldsymbol{x}} f\Vert$, $\Vert\nabla_{\boldsymbol{\theta}} f\Vert^2$, and $\Vert \boldsymbol{h}^{(i)}\Vert$ use the ordinary $l_2$ norm, which is the square root of the sum of the squares of each element. However, $\Vert\boldsymbol{W}^{(1)}\Vert$ and $\Vert\nabla_{\boldsymbol{x}}\boldsymbol{h}^{(1)}\Vert$ use the "spectral norm" of the matrix (refer to <a href="translation_6051.html">"Lipschitz Continuity in Deep Learning: Generalization and Generative Models"</a>). This inequality shows that the parameter gradient penalty, to some extent, contains the input gradient penalty.</p>

<h2>Derivation Process <a id="推导过程"></a></h2>

<p>Obviously, to prove inequality $\eqref{eq:f}$, we only need to prove for each parameter:</p>
\begin{align}\Vert\nabla_{\boldsymbol{x}} f\Vert^2\left(\frac{\Vert \boldsymbol{h}^{(t)}\Vert^2}{\Vert\boldsymbol{W}^{(t)}\Vert^2 \Vert\nabla_{\boldsymbol{x}}\boldsymbol{h}^{(t)}\Vert^2}\right)\leq&\, \Vert\nabla_{\boldsymbol{W}^{(t)}} f\Vert^2 \label{eq:w}\\
\Vert\nabla_{\boldsymbol{x}} f\Vert^2\left(\frac{1}{\Vert\boldsymbol{W}^{(t)}\Vert^2 \Vert\nabla_{\boldsymbol{x}}\boldsymbol{h}^{(t)}\Vert^2}\right)\leq&\, \Vert\nabla_{\boldsymbol{b}^{(t)}} f\Vert^2 \label{eq:b}
\end{align}
<p>Then, by iterating through all $t$ and adding each equation side-by-side, the result follows. The proof of these two inequalities is essentially a matrix calculus problem. However, most readers, like myself, may not be familiar with matrix calculus. In such cases, the best approach is to write out the component forms, converting it into a scalar calculus problem.</p>

<p>Specifically, $\boldsymbol{z}^{(t+1)}=\boldsymbol{W}^{(t)}\boldsymbol{h}^{(t)}+\boldsymbol{b}^{(t)}$ written in component form is:</p>
\begin{equation}z^{(t+1)}_i = \sum_j w^{(t)}_{i,j} h_j^{(t)} + b^{(t)}_i\end{equation}
<p>Then, by the chain rule:</p>
\begin{equation}\frac{\partial f}{\partial x_i} = \sum_{j,k} \frac{\partial f}{\partial z^{(t+1)}_j} \frac{\partial z^{(t+1)}_j}{\partial h^{(t)}_k} \frac{\partial h^{(t)}_k}{\partial x_i} = \sum_{j,k} \frac{\partial f}{\partial z^{(t+1)}_j} w^{(t)}_{j,k} \frac{\partial h^{(t)}_k}{\partial x_i}\label{eq:l}\end{equation}
<p>And</p>
\begin{equation}\frac{\partial z^{(t+1)}_j}{\partial w^{(t)}_{m,n}} = \delta_{j,m}h^{(t)}_n\end{equation}
<p>Here $\delta_{j,m}$ is the Kronecker delta. Now we can write</p>
\begin{equation}w^{(t)}_{j,k} = \sum_m \delta_{j,m}w^{(t)}_{m,k} = \sum_m \frac{\partial z^{(t+1)}_j}{\partial w^{(t)}_{m,n}} (h^{(t)}_n)^{-1} w^{(t)}_{m,k}\end{equation}
<p>Substituting into $\eqref{eq:l}$, we get</p>
\begin{equation}\frac{\partial f}{\partial x_i} = \sum_{j,k,m} \frac{\partial f}{\partial z^{(t+1)}_j} \frac{\partial z^{(t+1)}_j}{\partial w^{(t)}_{m,n}} (h^{(t)}_n)^{-1} w^{(t)}_{m,k} \frac{\partial h^{(t)}_k}{\partial x_i}=\sum_{k,m} \frac{\partial f}{\partial w^{(t)}_{m,n}} (h^{(t)}_n)^{-1} w^{(t)}_{m,k} \frac{\partial h^{(t)}_k}{\partial x_i}\end{equation}
<p>Multiplying both sides by $h^{(t)}_n$ gives</p>
\begin{equation}h^{(t)}_n\frac{\partial f}{\partial x_i} = \sum_{k,m} \frac{\partial f}{\partial w^{(t)}_{m,n}} w^{(t)}_{m,k} \frac{\partial h^{(t)}_k}{\partial x_i}\end{equation}
<p>Assuming original vectors are column vectors and the shape of the matrix after calculating the gradient is transposed, then the above can be written in matrix form:</p>
\begin{equation}\boldsymbol{h}^{(t)}(\nabla_{\boldsymbol{x}} f)^\top = (\nabla_{\boldsymbol{W}^{(t)}} f ) \boldsymbol{W}^{(t)}(\nabla_{\boldsymbol{x}}\boldsymbol{h}^{(t)})\end{equation}
<p>Multiplying both sides on the left by $(\boldsymbol{h}^{(t)})^{\top}$ gives</p>
\begin{equation}\Vert\boldsymbol{h}^{(t)}\Vert^2(\nabla_{\boldsymbol{x}} f)^\top = (\boldsymbol{h}^{(t)})^{\top}(\nabla_{\boldsymbol{W}^{(t)}} f ) \boldsymbol{W}^{(t)}(\nabla_{\boldsymbol{x}}\boldsymbol{h}^{(t)})\end{equation}
<p>Taking the norm of both sides yields</p>
\begin{equation}\Vert\boldsymbol{h}^{(t)}\Vert^2 \Vert\nabla_{\boldsymbol{x}} f\Vert = \Vert (\boldsymbol{h}^{(t)})^{\top}(\nabla_{\boldsymbol{W}^{(t)}} f ) \boldsymbol{W}^{(t)}(\nabla_{\boldsymbol{x}}\boldsymbol{h}^{(t)})\Vert \leq \Vert\boldsymbol{h}^{(t)}\Vert \Vert\nabla_{\boldsymbol{W}^{(t)}} f \Vert \Vert \boldsymbol{W}^{(t)}\Vert \Vert\nabla_{\boldsymbol{x}}\boldsymbol{h}^{(t)}\Vert\end{equation}
<p>Regarding the second inequality sign, it holds whether the matrix norm used is the Frobenius norm ($l_2$ norm) or the spectral norm. Thus, after choosing the required norms and rearranging, we can obtain formula $\eqref{eq:w}$. The proof of formula $\eqref{eq:b}$ is similar and will not be repeated here.</p>

<h2>Simple Analysis <a id="简单评析"></a></h2>

<p>Some readers might wonder how specifically to interpret formula $\eqref{eq:f}$? In fact, I mainly feel that formula $\eqref{eq:f}$ itself is somewhat interesting and might be used in some scenario in the future, so this article is primarily a "note" on it, without providing a definitive interpretation.</p>

<p>As for the logical sequence of the original paper, it goes like this: In <a href="translation_7787.html">"Optimization Algorithms from a Dynamical Perspective (V): Why the Learning Rate Should Not Be Too Small?"</a>, we introduced <a href="translation_7787.html">"Implicit Gradient Regularization"</a> (by the same author as this paper), which pointed out that SGD implicitly contains a gradient penalty term for parameters. Formula $\eqref{eq:f}$ shows that the gradient penalty for parameters implicitly contains a gradient penalty for inputs. Furthermore, the gradient penalty for inputs is related to Dirichlet energy, which in turn can serve as a representation of model complexity. So, following this chain of reasoning, the conclusion is: SGD itself tends to choose models with relatively smaller complexity.</p>

<p>However, the original paper made a small mistake when interpreting formula $\eqref{eq:f}$. It stated that in the initial stage, $\Vert \boldsymbol{W}^{(t)}\Vert$ would be very close to 0, so the terms in the brackets of formula $\eqref{eq:f}$ would be very large. Therefore, to minimize the parameter gradient penalty on the right side of formula $\eqref{eq:f}$, one must make the input gradient penalty on the left side of formula $\eqref{eq:f}$ sufficiently small. However, as we know from <a href="translation_7180.html">"Understanding Model Parameter Initialization Strategies from a Geometric Perspective"</a>, commonly used initialization methods are actually close to orthogonal initialization, and the spectral norm of an orthogonal matrix is actually 1. If activation functions are considered, the spectral norm at initialization is actually greater than 1, so the assumption that $\Vert \boldsymbol{W}^{(t)}\Vert$ is very close to 0 during the initialization phase does not hold.</p>

<p>In fact, for a network that has not collapsed during training, the model parameters and the input/output of each layer generally maintain a stable state. Therefore, throughout the training process, $\Vert \boldsymbol{h}^{(t)}\Vert$, $\Vert\boldsymbol{W}^{(t)}\Vert$, and $\Vert\nabla_{\boldsymbol{x}}\boldsymbol{h}^{(t)}\Vert$ do not actually fluctuate much. Thus, the parameter gradient penalty on the right side is approximately equivalent to the input gradient penalty on the left side. This is my interpretation, which does not require the assumption that "$\Vert \boldsymbol{W}^{(t)}\Vert$ is very close to 0."</p>

<h2>Summary <a id="文章小结"></a></h2>

<p>This article primarily introduced an inequality between two types of gradient penalty terms and provided its own proof along with a brief analysis.</p>

</article>
<hr>
<footer style="margin-top: 3em; padding: 1.5em; background: #f5f5f5; border-radius: 8px; font-size: 0.9em; color: #555;">
    <p style="margin: 0 0 0.5em 0;"><strong>Citation</strong></p>
    <p style="margin: 0 0 0.5em 0;">
        This is a machine translation of the original Chinese article:<br>
        <a href="translation_8796.html" style="color: #005fcc;">https://kexue.fm/archives/8796</a>
    </p>
    <p style="margin: 0 0 0.5em 0;">
        Original author: 苏剑林 (Su Jianlin)<br>
        Original publication: <a href="https://kexue.fm" style="color: #005fcc;">科学空间 (Scientific Spaces)</a>
    </p>
    <p style="margin: 0; font-style: italic;">
        Translated using Gemini 3 Flash. Please refer to the original for authoritative content.
    </p>
</footer>
