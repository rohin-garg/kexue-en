
    <style type="text/css">

    body {
    margin: 48px auto;
    max-width: 68ch;              /* character-based width reads better */
    padding: 0 16px;

    font-family: -apple-system, BlinkMacSystemFont, "Segoe UI",
                Roboto, "Helvetica Neue", Arial, sans-serif;
    font-size: 18px;
    line-height: 1.65;
    color: #333;
    background: #fafafa;
    }

    h1, h2, h3, h4 {
    line-height: 1.25;
    margin-top: 2.2em;
    margin-bottom: 0.6em;
    font-weight: 600;
    }

    h1 {
    font-size: 2.1em;
    margin-top: 0;
    }

    h2 {
    font-size: 1.6em;
    border-bottom: 1px solid #e5e5e5;
    padding-bottom: 0.3em;
    }

    h3 {
    font-size: 1.25em;
    }

    h4 {
    font-size: 1.05em;
    color: #555;
    }

    /* Paragraphs and lists */
    p {
    margin: 1em 0;
    }

    ul, ol {
    margin: 1em 0 1em 1.5em;
    }

    li {
    margin: 0.4em 0;
    }

    a {
    color: #005fcc;
    text-decoration: none;
    }

    a:hover {
    text-decoration: underline;
    }

    code {
    font-family: ui-monospace, SFMono-Regular, Menlo, Consolas, monospace;
    font-size: 0.95em;
    background: #f2f2f2;
    padding: 0.15em 0.35em;
    border-radius: 4px;
    }

    pre {
    font-family: ui-monospace, SFMono-Regular, Menlo, Consolas, monospace;
    font-size: 0.9em;
    background: #f5f5f5;
    padding: 1em 1.2em;
    overflow-x: auto;
    border-radius: 6px;
    line-height: 1.45;
    }

    pre code {
    background: none;
    padding: 0;
    }

    blockquote {
    margin: 1.5em 0;
    padding-left: 1em;
    border-left: 4px solid #ddd;
    color: #555;
    }

    hr {
    border: none;
    border-top: 1px solid #e0e0e0;
    margin: 3em 0;
    }

    table {
    border-collapse: collapse;
    margin: 1.5em 0;
    width: 100%;
    font-size: 0.95em;
    }

    th, td {
    padding: 0.5em 0.7em;
    border-bottom: 1px solid #e5e5e5;
    text-align: left;
    }

    th {
    font-weight: 600;
    }

    img {
    max-width: 100%;
    display: block;
    margin: 1.5em auto;
    }

    small {
    color: #666;
    }

    mjx-container {
    margin: 1em 0;
    }

    ::selection {
    background: #cce2ff;
    }

    </style>
    

<script type="text/javascript">
window.MathJax = {
  tex: {
    inlineMath: [['$', '$'], ['\\(', '\\)']],
    displayMath: [['$$', '$$'], ['\\[', '\\]']],
    tags: 'ams',
    packages: {'[+]': ['ams']}
  },
  options: {
    renderActions: {
      findScript: [10, function (doc) {
        for (const node of document.querySelectorAll('script[type^="math/tex"]')) {
          const display = !!node.type.match(/; *mode=display/);
          const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
          const text = document.createTextNode('');
          node.parentNode.replaceChild(text, node);
          math.start = {node: text, delim: '', n: 0};
          math.end = {node: text, delim: '', n: 0};
          doc.math.push(math);
        }
      }, '']
    }
  }
};
</script>
<script type="text/javascript" id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

<article>
    <nav style="margin-bottom: 1.5em;">
    <a href="../index.html" style="display: inline-flex; align-items: center; color: #555; text-decoration: none; font-size: 0.95em;">
        <span style="margin-right: 0.3em;">&larr;</span> Back to Index
    </a>
</nav>

    <h1><a href="https://kexue.fm/archives/10657">Why is the Default Norm for Gradient Clipping 1?</a></h1>
    <p>By 苏剑林 | January 02, 2025</p>

    <p>As we know, Gradient Clipping is a common technique used to make model training more stable. Generally, gradient clipping is performed based on the total norm of the gradients of all parameters. This operation can be expressed as:</p>

    \begin{equation}
    \text{clip}(\boldsymbol{g},\tau)=\left\{\begin{aligned}&\boldsymbol{g}, &\Vert\boldsymbol{g}\Vert\leq \tau \\ 
    &\frac{\tau}{\Vert\boldsymbol{g}\Vert}\boldsymbol{g},&\Vert\boldsymbol{g}\Vert > \tau 
    \end{aligned}\right.
    \end{equation}

    <p>In this way, $\text{clip}(\boldsymbol{g},\tau)$ maintains the same direction as $\boldsymbol{g}$, but its norm does not exceed $\tau$. Note that $\Vert\boldsymbol{g}\Vert$ here is the "Global Gradient Norm," calculated by treating all parameters of the entire model as a single vector. Have you ever noticed a specific detail? Whether a model has millions of parameters or tens of billions, the value of $\tau$ is often set to 1. What does this mean? Is it simply a matter of reusing a default value, or is there a profound principle hidden behind it?</p>

    <h2>The "What"</h2>
    <p>Some readers might feel that since the default value isn't necessarily the optimal one, why bother overthinking it? It is true that $\tau=1$ might not always be the optimal choice, but the fact that it is the default for many models and performs reasonably well suggests that $\tau=1$ possesses a universal "rationality."</p>

    <p>What does "rationality" mean here? Let's return to the $\text{clip}$ operation. If $\Vert\boldsymbol{g}\Vert$ is always smaller than $\tau$, then $\text{clip}$ degrades into an identity transform; if $\Vert\boldsymbol{g}\Vert$ is always larger than $\tau$, then $\text{clip}$ degrades into L2 normalization. In other words, for $\text{clip}$ to function as intended, $\tau$ must provide an appropriate differentiation, such that most of the $\Vert\boldsymbol{g}\Vert$ values are smaller than $\tau$, and only a small portion are larger than $\tau$. This defines the rationality of $\tau$.</p>

    <p>Of course, one could find plenty of counterexamples, but the emphasis here is on the ubiquity of this phenomenon and the general applicability of this default setting. Therefore, meticulous readers need not get hung up on specific edge cases.</p>

    <p>Thus, we believe that the universal rationality of $\tau=1$ implies that—regardless of the model's parameter count, its initialization, or the loss function—the total gradient norm happens to coincide with $1$ as the threshold for "outliers." This is undoubtedly an incredible property—this was exactly how the author felt when first realizing this conclusion.</p>

    <h2>The "Why"</h2>
    <p>Why such a "coincidence"? The author's answer might be a bit surprising: because only under this condition is stable model training possible.</p>

    <p>Let's consider the loss function $\mathcal{L}(\boldsymbol{\theta})$ and the optimizer update rule $\boldsymbol{\theta}_{t+1} = \boldsymbol{\theta}_t - \eta\, \boldsymbol{u}_t$. The change in the loss function can be approximated as:</p>

    \begin{equation}
    \Delta \mathcal{L} = \mathcal{L}(\boldsymbol{\theta}_{t+1}) - \mathcal{L}(\boldsymbol{\theta}_t) \approx (\boldsymbol{\theta}_{t+1} - \boldsymbol{\theta}_t)\cdot\nabla_{\boldsymbol{\theta}_t}\mathcal{L}(\boldsymbol{\theta}) = -\eta\, \boldsymbol{u}_t\cdot \boldsymbol{g}_t
    \end{equation}

    <p>First, consider the simplest case, SGD, where $\boldsymbol{u}_t = \boldsymbol{g}_t$ and $\Delta \mathcal{L}=-\eta\Vert\boldsymbol{g}_t\Vert^2$. That is, the amount of change in the loss function is proportional to the square of the gradient norm. We know that in both CV and NLP, pure SGD (without momentum) is a very inefficient optimizer. In the middle and late stages of training, the average loss reduction per step for most tasks is far less than the learning rate itself, meaning $|\Delta \mathcal{L}| < \eta$. From this, we can derive $\Vert\boldsymbol{g}_t\Vert < 1$. This indicates that $\Vert\boldsymbol{g}_t\Vert < 1$ is a long-term characteristic of a model that converges normally.</p>

    <p>Of course, it is normal for $\Vert\boldsymbol{g}_t\Vert > 1$ to occur in the early stages of training, but it is rare for $\Vert\boldsymbol{g}_t\Vert \gg 1$ to occur—or rather, a good initialization should avoid the occurrence of $\Vert\boldsymbol{g}_t\Vert \gg 1$. This is the theoretical basis for techniques like <a href="translation_9024.html">DeepNorm</a>. The reason is similar: if the gradient norm is too large, learning in the early stages will be too "aggressive," leading to premature convergence to a poor local solution. Another approach is to reduce $\eta$, which also reduces $|\Delta \mathcal{L}|$; this is why we typically use a Warmup at the beginning of training.</p>

    <p>Incidentally, regarding the understanding of Warmup, readers can refer to the paper <a href="https://arxiv.org/abs/2310.07831">"Optimal Linear Decay Learning Rate Schedules and Further Refinements"</a>, which provides what the author considers to be the most rational analysis of Warmup.</p>

    <h2>What to Do</h2>
    <p>Simply put, because the change in the loss function is proportional to the square of the gradient norm, the stability of training dictates that the gradient norm cannot be too large, and it long-term performance is categorized as less than 1. If a gradient norm significantly larger than 1 occurs initially, the usual strategy is Warmup. Alternatively, one could consider a more universal strategy: setting another threshold $\mathcal{T}$ and clipping $\eta$ according to the value of $\boldsymbol{u}_t\cdot \boldsymbol{g}_t$:</p>

    \begin{equation}
    \eta_t = \left\{\begin{aligned}&\eta,& \boldsymbol{u}_t\cdot \boldsymbol{g}_t\leq \mathcal{T} \\ &\frac{\mathcal{T}}{\boldsymbol{u}_t\cdot \boldsymbol{g}_t}\eta,& \boldsymbol{u}_t\cdot \boldsymbol{g}_t &gt; \mathcal{T} 
    \end{aligned}\right.
    \end{equation}

    <p>This eliminates the need for extra Warmup settings and offers more adaptivity. For optimizers like Adam, we can perform an approximate analysis via $\boldsymbol{u}_t=\text{sign}(\boldsymbol{g}_t)$, similar to <a href="translation_10008.html">"How Should the Learning Rate Change When the Batch Size Increases?"</a>. In this case:</p>

    \begin{equation}
    \Delta \mathcal{L} = -\eta\, \text{sign}(\boldsymbol{g}_t)\cdot \boldsymbol{g}_t = -\eta\, \Vert\boldsymbol{g}_t\Vert_1
    \end{equation}

    <p>Here, $\Vert\cdot\Vert_1$ is the L1 norm, i.e., the sum of the absolute values of the components. Since gradient components are generally less than 1, $\Vert\boldsymbol{g}_t\Vert_1 \gg \Vert\boldsymbol{g}_t\Vert$. Therefore, also out of the need for stable training, Adam's learning rate is usually significantly smaller than SGD's learning rate. Furthermore, the above equation can be rewritten as:</p>

    \begin{equation}
    \Delta \mathcal{L} = -\eta\, \text{sign}(\boldsymbol{g}_t)\cdot \boldsymbol{g}_t = -\eta\, \sqrt{N}\Vert\boldsymbol{g}_t\Vert \cos(\text{sign}(\boldsymbol{g}_t), \boldsymbol{g}_t) 
    \end{equation}

    <p>Here, we assume that $\boldsymbol{g}_t$ has no zero components, so $\Vert\text{sign}(\boldsymbol{g}_t)\Vert=\sqrt{N}$, where $N$ is the total number of model parameters. Practice has found that $\Vert\boldsymbol{g}_t\Vert$ and $\cos(\text{sign}(\boldsymbol{g}_t), \boldsymbol{g}_t)$ are roughly constant across different model scales. Therefore, to keep $\Delta \mathcal{L}$ constant, $\eta$ should be inversely proportional to $\sqrt{N}$. That is, if the model parameter count increases by 4 times, the learning rate can be considered to be halved.</p>

    <h2>Fin</h2>
    <p>This article has provided some of my views and reflections on the phenomenon that "the default norm for gradient clipping is 1."</p>
</article>
<hr>
<footer style="margin-top: 3em; padding: 1.5em; background: #f5f5f5; border-radius: 8px; font-size: 0.9em; color: #555;">
    <p style="margin: 0 0 0.5em 0;"><strong>Citation</strong></p>
    <p style="margin: 0 0 0.5em 0;">
        This is a machine translation of the original Chinese article:<br>
        <a href="https://kexue.fm/archives/10657" style="color: #005fcc;">https://kexue.fm/archives/10657</a>
    </p>
    <p style="margin: 0 0 0.5em 0;">
        Original author: 苏剑林 (Su Jianlin)<br>
        Original publication: <a href="https://kexue.fm" style="color: #005fcc;">科学空间 (Scientific Spaces)</a>
    </p>
    <p style="margin: 0; font-style: italic;">
        Translated using Gemini 3 Flash. Please refer to the original for authoritative content.
    </p>
</footer>
