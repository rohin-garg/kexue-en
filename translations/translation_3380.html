
    <style type="text/css">

    body {
    margin: 48px auto;
    max-width: 68ch;              /* character-based width reads better */
    padding: 0 16px;

    font-family: -apple-system, BlinkMacSystemFont, "Segoe UI",
                Roboto, "Helvetica Neue", Arial, sans-serif;
    font-size: 18px;
    line-height: 1.65;
    color: #333;
    background: #fafafa;
    }

    h1, h2, h3, h4 {
    line-height: 1.25;
    margin-top: 2.2em;
    margin-bottom: 0.6em;
    font-weight: 600;
    }

    h1 {
    font-size: 2.1em;
    margin-top: 0;
    }

    h2 {
    font-size: 1.6em;
    border-bottom: 1px solid #e5e5e5;
    padding-bottom: 0.3em;
    }

    h3 {
    font-size: 1.25em;
    }

    h4 {
    font-size: 1.05em;
    color: #555;
    }

    /* Paragraphs and lists */
    p {
    margin: 1em 0;
    }

    ul, ol {
    margin: 1em 0 1em 1.5em;
    }

    li {
    margin: 0.4em 0;
    }

    a {
    color: #005fcc;
    text-decoration: none;
    }

    a:hover {
    text-decoration: underline;
    }

    code {
    font-family: ui-monospace, SFMono-Regular, Menlo, Consolas, monospace;
    font-size: 0.95em;
    background: #f2f2f2;
    padding: 0.15em 0.35em;
    border-radius: 4px;
    }

    pre {
    font-family: ui-monospace, SFMono-Regular, Menlo, Consolas, monospace;
    font-size: 0.9em;
    background: #f5f5f5;
    padding: 1em 1.2em;
    overflow-x: auto;
    border-radius: 6px;
    line-height: 1.45;
    }

    pre code {
    background: none;
    padding: 0;
    }

    blockquote {
    margin: 1.5em 0;
    padding-left: 1em;
    border-left: 4px solid #ddd;
    color: #555;
    }

    hr {
    border: none;
    border-top: 1px solid #e0e0e0;
    margin: 3em 0;
    }

    table {
    border-collapse: collapse;
    margin: 1.5em 0;
    width: 100%;
    font-size: 0.95em;
    }

    th, td {
    padding: 0.5em 0.7em;
    border-bottom: 1px solid #e5e5e5;
    text-align: left;
    }

    th {
    font-weight: 600;
    }

    img {
    max-width: 100%;
    display: block;
    margin: 1.5em auto;
    }

    small {
    color: #666;
    }

    mjx-container {
    margin: 1em 0;
    }

    ::selection {
    background: #cce2ff;
    }

    </style>
    

<script>
window.MathJax = {
  tex: {
    inlineMath: [['$', '$'], ['\\(', '\\)']],
    displayMath: [['$$', '$$'], ['\\[', '\\]']],
    tags: 'ams',
    packages: {'[+]': ['ams']}
  },
  options: {
    renderActions: {
      findScript: [10, function (doc) {
        for (const node of document.querySelectorAll('script[type^="math/tex"]')) {
          const display = !!node.type.match(/; *mode=display/);
          const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
          const text = document.createTextNode('');
          node.parentNode.replaceChild(text, node);
          math.start = {node: text, delim: '', n: 0};
          math.end = {node: text, delim: '', n: 0};
          doc.math.push(math);
        }
      }, '']
    }
  }
};
</script>
<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

<article>
    <h1><a href="https://kexue.fm/archives/3380">Efficient Implementation of the Apriori Algorithm using Pandas</a></h1>

    <p>By 苏剑林 | July 02, 2015</p>

    <p>Latest Update: <a href="translation_5525.html">"Efficient Implementation of the Apriori Algorithm with Numpy"</a></p>

    <p>Recently, while working on data mining, I came across the Apriori algorithm. Since I hadn't specialized in this field before, I wasn't familiar with it, but now that it's come up in my work, I had to study it seriously. The Apriori algorithm is used for finding association rules—essentially discovering potential logic from a large dataset. For example, "Condition A + Condition B" is very likely to lead to "Condition C" ($A + B \to C$); this is an association rule. Specifically, for instance, after a customer buys product A, they often buy product B (conversely, buying B doesn't necessarily mean they buy A). Or more complexly, customers who buy both A and B are likely to buy C (again, the reverse is not necessarily true). With this information, we can bundle certain products to achieve higher revenue. The algorithms used to seek these association rules are called association analysis algorithms.</p>

    <h3>Beer and Diapers</h3>

    <p><img src="https://kexue.fm/usr/uploads/2015/07/2732992076.png" alt="Beer and Diapers" title="Click to view original image"></p>
    <p>Beer and Diapers</p>

    <p>In the world of association algorithms, the most frequently cited case is probably "Beer and Diapers." The story originated in the 1990s at Walmart supermarkets in the United States. Managers discovered that two seemingly unrelated items, beer and diapers, frequently appeared in the same shopping basket. After analysis, they found that in American households with infants, the mother usually stayed home to look after the baby while the young father went to the supermarket to buy diapers. While buying diapers, the father often picked up some beer for himself, resulting in these two unrelated items frequently appearing together. Consequently, Walmart tried placing beer and diapers in the same area so fathers could find both easily. The results were quite successful!</p>

    <p>(Note: The authenticity of this story is often questioned, but regardless, it has become a famous and simple case for introducing association analysis.)</p>

    <h3>The Apriori Algorithm</h3>

    <p>How do we find association rules like "Beer and Diapers" from a massive set of purchase records? There are many association analysis algorithms, and the simplest is likely the Apriori algorithm (though it isn't the most efficient, it's excellent as an introductory algorithm). Detailed descriptions of the Apriori algorithm are not provided in this article because there is already a wealth of information online. Recommended reading:</p>

    <p><a href="https://en.wikipedia.org/wiki/Association_rule_learning">https://en.wikipedia.org/wiki/Association_rule_learning</a></p>
    <p><a href="http://hackerxu.com/2014/10/18/apriori.html">http://hackerxu.com/2014/10/18/apriori.html</a></p>

    <h3>Python Implementation</h3>

    <p>After several days of debugging, I finally implemented a relatively efficient Apriori script using Python. Here, "efficient" refers to the implementation of the Apriori algorithm itself, without involving fundamental improvements to the algorithm logic. The script utilizes the Pandas library to ensure performance while minimizing the amount of code. Readers will find that this code is shorter and more efficient than many Apriori implementations found online (not limited to Python). </p>

    <p>The code is compatible with both Python 2.x and 3.x, provided Pandas is installed. This code can generally handle problems involving tens of thousands of records and dozens of candidate items, provided you have a little patience.</p>

    <h3>Efficiency Issues</h3>

    <p>The execution time of the Apriori algorithm depends on many factors, such as the volume of data, the minimum support (though it has little to do with minimum confidence), and the number of candidate items. Using market basket analysis as an example: first, the execution time naturally depends on the number of records $N$, though the relationship with $N$ is only linear. Second, the minimum support is almost decisive; it significantly impacts execution time, though its exact effect depends on the specific problem; furthermore, it largely determines the number of rules eventually generated. Finally, the number of candidate items $k$ (the total number of unique products appearing across all records) is also critical. If $k$ is large, the subsequent joins will result in the number of items scaling as $k^2, k^3, \dots$ (approximately), which has a fatal impact on speed.</p>

    <p>Therefore, while the logic of the Apriori algorithm is simple, its efficiency is not high.</p>

    <h3>Code</h3>

<pre class="python"><code>#-*- coding: utf-8 -*-
from __future__ import print_function
import pandas as pd

# Custom string connection function for merging itemsets
def connect_string(x, ms):
    x = list(map(lambda i:sorted(i.split(ms)), x))
    l = len(x[0])
    r = []
    for i in range(len(x)):
        for j in range(i+1, len(x)):
            if x[i][:l-1] == x[j][:l-1] and x[i][l-1] != x[j][l-1]:
                r.append(ms.join(sorted(list(set(x[i]) | set(x[j])))))
    return r

# Main function to find association rules
def find_rule(d, support, confidence, ms = u'--'):
    result = pd.DataFrame(index=['support', 'confidence']) # Outcome matrix
    
    support_series = 1.0 * d.sum() / len(d) # Support for 1-itemsets
    column = list(support_series[support_series > support].index) # Preliminary filtering
    k = 0
    
    while len(column) > 1:
        k = k + 1
        print(u'\nSearching for %s-itemsets...' % k)
        column = connect_string(column, ms)
        print(u'Number of candidate sets: %s' % len(column))
        sf = lambda i: d[i.split(ms)].prod(axis=1, numeric_only=True) # New support function
        
        # This part counts mentions. Since it uses matrix multiplication, it's quite fast.
        d_2 = d[column[0].split(ms)].prod(axis=1, numeric_only=True)
        d_2 = pd.DataFrame(d_2, columns=[column[0]])
        for i in range(1, len(column)):
            d_2[column[i]] = d[column[i].split(ms)].prod(axis=1, numeric_only=True)
        
        support_series_2 = 1.0 * d_2[column].sum() / len(d) # Calculate support
        column = list(support_series_2[support_series_2 > support].index) # Filter by support
        support_series = support_series.append(support_series_2)
        column2 = []
        
        for i in column: # Construct rules
            i = i.split(ms)
            for j in range(len(i)):
                column2.append([ms.join(i[:j] + i[j+1:]), i[j]])
        
        for i, j in column2: # Calculate confidence
            conf = support_series[ms.join(sorted([i, j] if isinstance(j, list) else i.split(ms) + [j]))] / support_series[i]
            if conf > confidence:
                result[i + '-->' + j] = [support_series[ms.join(sorted(i.split(ms) + [j]))], conf]
                
    result = result.T.sort_values(['confidence','support'], ascending = False) # Sort results
    print(u'\nResults:')
    print(result)
    
    return result

# Usage example:
# d = pd.read_csv('apriori.txt', header=None, dtype=object)
# d = pd.get_dummies(d.unstack()).sum(level=1) # Convert to 0-1 matrix
# find_rule(d, 0.01, 0.5)</code></pre>

    <p>Test dataset: <a href="https://kexue.fm/usr/uploads/2015/07/3424358296.txt">apriori.txt</a></p>

    <p>Execution result:</p>

    <p><img src="https://kexue.fm/usr/uploads/2015/07/204981067.png" alt="Apriori Result" title="Click to view original image"></p>
    <p>apriori</p>

    <hr>
    <p><em><strong>Original address:</strong> <a href="translation_3380.html">https://kexue.fm/archives/3380</a></em></p>

    <p><em><strong>For more details on reprinting, please refer to:</strong></em> <a href="https://kexue.fm/archives/6508#%E6%96%87%E7%AB%A0%E5%A6%82%E4%BD%95%E8%BD%AC%E8%BD%BD/%E5%BC%95%E7%94%A8">"Scientific Spaces FAQ"</a></p>

    <p><strong>If you have any doubts or suggestions, please feel free to discuss them in the comments section below.</strong></p>

    <p><strong>If you found this article helpful, you are welcome to share it or provide a donation. Donations are not for profit, but rather to see how much genuine attention Scientific Spaces has gained from its readers. Of course, if you choose to ignore it, it will not affect your reading experience. Thank you again!</strong></p>

    <p>Su Jianlin. (Jul. 02, 2015). "Efficient Implementation of the Apriori Algorithm using Pandas" [Blog post]. Retrieved from <a href="translation_3380.html">https://kexue.fm/archives/3380</a></p>

</article>
<hr>
<footer style="margin-top: 3em; padding: 1.5em; background: #f5f5f5; border-radius: 8px; font-size: 0.9em; color: #555;">
    <p style="margin: 0 0 0.5em 0;"><strong>Citation</strong></p>
    <p style="margin: 0 0 0.5em 0;">
        This is a machine translation of the original Chinese article:<br>
        <a href="translation_3380.html" style="color: #005fcc;">https://kexue.fm/archives/3380</a>
    </p>
    <p style="margin: 0 0 0.5em 0;">
        Original author: 苏剑林 (Su Jianlin)<br>
        Original publication: <a href="https://kexue.fm" style="color: #005fcc;">科学空间 (Scientific Spaces)</a>
    </p>
    <p style="margin: 0; font-style: italic;">
        Translated using Gemini 3 Flash. Please refer to the original for authoritative content.
    </p>
</footer>
