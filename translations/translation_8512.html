
    <style type="text/css">

    body {
    margin: 48px auto;
    max-width: 68ch;              /* character-based width reads better */
    padding: 0 16px;

    font-family: -apple-system, BlinkMacSystemFont, "Segoe UI",
                Roboto, "Helvetica Neue", Arial, sans-serif;
    font-size: 18px;
    line-height: 1.65;
    color: #333;
    background: #fafafa;
    }

    h1, h2, h3, h4 {
    line-height: 1.25;
    margin-top: 2.2em;
    margin-bottom: 0.6em;
    font-weight: 600;
    }

    h1 {
    font-size: 2.1em;
    margin-top: 0;
    }

    h2 {
    font-size: 1.6em;
    border-bottom: 1px solid #e5e5e5;
    padding-bottom: 0.3em;
    }

    h3 {
    font-size: 1.25em;
    }

    h4 {
    font-size: 1.05em;
    color: #555;
    }

    /* Paragraphs and lists */
    p {
    margin: 1em 0;
    }

    ul, ol {
    margin: 1em 0 1em 1.5em;
    }

    li {
    margin: 0.4em 0;
    }

    a {
    color: #005fcc;
    text-decoration: none;
    }

    a:hover {
    text-decoration: underline;
    }

    code {
    font-family: ui-monospace, SFMono-Regular, Menlo, Consolas, monospace;
    font-size: 0.95em;
    background: #f2f2f2;
    padding: 0.15em 0.35em;
    border-radius: 4px;
    }

    pre {
    font-family: ui-monospace, SFMono-Regular, Menlo, Consolas, monospace;
    font-size: 0.9em;
    background: #f5f5f5;
    padding: 1em 1.2em;
    overflow-x: auto;
    border-radius: 6px;
    line-height: 1.45;
    }

    pre code {
    background: none;
    padding: 0;
    }

    blockquote {
    margin: 1.5em 0;
    padding-left: 1em;
    border-left: 4px solid #ddd;
    color: #555;
    }

    hr {
    border: none;
    border-top: 1px solid #e0e0e0;
    margin: 3em 0;
    }

    table {
    border-collapse: collapse;
    margin: 1.5em 0;
    width: 100%;
    font-size: 0.95em;
    }

    th, td {
    padding: 0.5em 0.7em;
    border-bottom: 1px solid #e5e5e5;
    text-align: left;
    }

    th {
    font-weight: 600;
    }

    img {
    max-width: 100%;
    display: block;
    margin: 1.5em auto;
    }

    small {
    color: #666;
    }

    mjx-container {
    margin: 1em 0;
    }

    ::selection {
    background: #cce2ff;
    }

    </style>
    

<script>
window.MathJax = {
  tex: {
    inlineMath: [['$', '$'], ['\\(', '\\)']],
    displayMath: [['$$', '$$'], ['\\[', '\\]']],
    processEscapes: true,
    tags: 'ams',
    packages: {'[+]': ['ams']}
  }
};
</script>
<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

<h1><a href="https://kexue.fm/archives/8512">KL Divergence, Bhattacharyya Distance, and Wasserstein Distance between two Multivariate Normal Distributions</a></h1>

<p>By 苏剑林 | July 08, 2021</p>

<p>The normal distribution is one of the most common continuous probability distributions. It is the maximum entropy distribution given a mean and covariance (refer to <a href="translation_3552.html">"Entropy: From Entropy, Maximum Entropy Principle to Maximum Entropy Models (Part II)"</a>). It can also be viewed as a second-order approximation of any continuous distribution, occupying a status equivalent to linear approximation for general functions. From this perspective, the normal distribution can be considered the simplest continuous distribution. Precisely because of its simplicity, analytical solutions can be derived for many estimators.</p>

<p>This article primarily calculates several metrics between two multivariate normal distributions, including KL divergence, Bhattacharyya distance, and Wasserstein distance, all of which have explicit analytical solutions.</p>

<h2>Normal Distribution</h2>

<p>Here we briefly review some basic knowledge of the normal distribution. Note that this is only a review and is not intended as an introductory tutorial on the subject.</p>

<h3>Probability Density</h3>

<p>The normal distribution, also known as the Gaussian distribution, is a continuous probability distribution defined on $\mathbb{R}^n$. Its probability density function is:</p>

\begin{equation}p(\boldsymbol{x})=\frac{1}{\sqrt{(2\pi)^n \det(\boldsymbol{\Sigma})}}\exp\left\{-\frac{1}{2}(\boldsymbol{x}-\boldsymbol{\mu})^{\top}\boldsymbol{\Sigma}^{-1}(\boldsymbol{x}-\boldsymbol{\mu})\right\}\end{equation}

<p>Here $\boldsymbol{x},\boldsymbol{\mu}\in\mathbb{R}^n$, where $\boldsymbol{\mu}$ is the mean vector (vectors in this article are column vectors by default), and $\boldsymbol{\Sigma}\in\mathbb{R}^{n\times n}$ is the covariance matrix, which must be symmetric and positive definite. As can be seen, a normal distribution is uniquely determined by $\boldsymbol{\mu}$ and $\boldsymbol{\Sigma}$; therefore, its statistics are functions of $\boldsymbol{\mu}$ and $\boldsymbol{\Sigma}$. When $\boldsymbol{\mu}=\boldsymbol{0}, \boldsymbol{\Sigma}=\boldsymbol{I}$, the corresponding distribution is called the "standard normal distribution."</p>

<h3>Basic Properties</h3>

<p>Generally, the basic statistics are the mean and variance, which correspond to the two parameters of the normal distribution:</p>

\begin{equation}\begin{aligned}
\mathbb{E}_{\boldsymbol{x}}\left[\boldsymbol{x}\right]=&\int p(\boldsymbol{x}) \boldsymbol{x} dx=\boldsymbol{\mu}\\
\mathbb{E}_{\boldsymbol{x}}\left[(\boldsymbol{x}-\boldsymbol{\mu})(\boldsymbol{x}-\boldsymbol{\mu})^{\top}\right]=&\int p(\boldsymbol{x}) (\boldsymbol{x}-\boldsymbol{\mu})(\boldsymbol{x}-\boldsymbol{\mu})^{\top} dx=\boldsymbol{\Sigma}\\
\end{aligned}\end{equation}

<p>From the above, the result for the second-order moment can be derived:</p>

\begin{equation}
\mathbb{E}_{\boldsymbol{x}}\left[\boldsymbol{x}\boldsymbol{x}^{\top}\right]=\boldsymbol{\mu}\boldsymbol{\mu}^{\top} + \mathbb{E}_{\boldsymbol{x}}\left[(\boldsymbol{x}-\boldsymbol{\mu})(\boldsymbol{x}-\boldsymbol{\mu})^{\top}\right]=\boldsymbol{\mu}\boldsymbol{\mu}^{\top} + \boldsymbol{\Sigma}\end{equation}

<p>Another commonly used statistic is its entropy:</p>

\begin{equation}\mathcal{H} = \mathbb{E}_{\boldsymbol{x}}\left[-\log p(\boldsymbol{x})\right]=\frac{n}{2}(1 + \log 2\pi) + \frac{1}{2}\log \det(\boldsymbol{\Sigma})
\end{equation}

<p>The calculation process for this can refer to the derivation for KL divergence later.</p>

<h3>Gaussian Integral</h3>

<p>The probability density function implies that $\int p(\boldsymbol{x}) d\boldsymbol{x} = 1$, which leads to:</p>

\begin{equation}\begin{aligned}
\sqrt{(2\pi)^n \det(\boldsymbol{\Sigma})} =& \int\exp\left\{-\frac{1}{2}(\boldsymbol{x}-\boldsymbol{\mu})^{\top}\boldsymbol{\Sigma}^{-1}(\boldsymbol{x}-\boldsymbol{\mu})\right\}d\boldsymbol{x} \\
=& \int\exp\left\{-\frac{1}{2}\boldsymbol{x}^{\top}\boldsymbol{\Sigma}^{-1}\boldsymbol{x}+\boldsymbol{\mu}^{\top}\boldsymbol{\Sigma}^{-1}\boldsymbol{x}-\frac{1}{2}\boldsymbol{\mu}^{\top}\boldsymbol{\Sigma}^{-1}\boldsymbol{\mu}\right\}d\boldsymbol{x}
\end{aligned}\end{equation}

<p>Let $\boldsymbol{\omega} = \boldsymbol{\Sigma}^{-1}\boldsymbol{\mu}$, then we obtain the Gaussian integral:</p>

\begin{equation}
\int\exp\left\{-\frac{1}{2}\boldsymbol{x}^{\top}\boldsymbol{\Sigma}^{-1}\boldsymbol{x}+\boldsymbol{\omega}^{\top}\boldsymbol{x}\right\}d\boldsymbol{x} = \sqrt{(2\pi)^n \det(\boldsymbol{\Sigma})}\exp\left\{\frac{1}{2}\boldsymbol{\omega}^{\top}\boldsymbol{\Sigma}\boldsymbol{\omega}\right\}\label{eq:g-int}
\end{equation}

<p>Using this, we can compute the characteristic function of the normal distribution:</p>

\begin{equation}\mathbb{E}_{\boldsymbol{x}}\left[\exp\left(\boldsymbol{\omega}^{\top}\boldsymbol{x}\right)\right]=\exp\left(\boldsymbol{\omega}^{\top}\boldsymbol{\mu}+\frac{1}{2}\boldsymbol{\omega}^{\top}\boldsymbol{\Sigma}\boldsymbol{\omega}\right)\\
\end{equation}

<p>The characteristic function can be used to calculate all orders of moments of the normal distribution.</p>

<h2>Linear Algebra</h2>

<p>Here we supplement some basic linear algebra concepts that will be frequently used in the subsequent derivations. Similarly, this is only a "review."</p>

<h3>Inner Product and Norm</h3>

<p>First, we define the inner product and norm. For vectors $\boldsymbol{x}=(x_1,\dots,x_n)$ and $\boldsymbol{y}=(y_1,\dots,y_n)$, the inner product is defined as:</p>

\begin{equation}\langle\boldsymbol{x},\boldsymbol{y}\rangle = \sum_{i=1}^n x_i y_i\end{equation}

<p>And the magnitude (length) is defined as $\|\boldsymbol{x}\| = \sqrt{\langle\boldsymbol{x},\boldsymbol{x}\rangle}$. For $m\times n$ matrices $\boldsymbol{A}=(a_{i,j})$ and $\boldsymbol{B}=(b_{i,j})$, we define similarly:</p>

\begin{equation}\langle\boldsymbol{A},\boldsymbol{B}\rangle_F = \sum_{i=1}^m\sum_{j=1}^n a_{i,j} b_{i,j}\end{equation}

<p>This is called the Frobenius inner product, and the corresponding $\|\boldsymbol{A}\|_F = \sqrt{\langle\boldsymbol{A},\boldsymbol{A}\rangle_F}$ is called the Frobenius norm. It is easy to see that the Frobenius inner product and norm are essentially equivalent to flattening the matrix into a vector and performing standard vector operations.</p>

<p>Regarding the Frobenius inner product, one of the most critical properties is the identity:</p>

\begin{equation}\langle\boldsymbol{A},\boldsymbol{B}\rangle_F = \text{Tr}\left(\boldsymbol{A}^{\top}\boldsymbol{B}\right) = \text{Tr}\left(\boldsymbol{B}\boldsymbol{A}^{\top}\right) = \text{Tr}\left(\boldsymbol{A}\boldsymbol{B}^{\top}\right) = \text{Tr}\left(\boldsymbol{B}^{\top}\boldsymbol{A}\right) \end{equation}

<p>That is, the Frobenius inner product of matrices can be converted into the trace of matrix multiplication, and swapping the order of multiplication does not change the result of the trace (while it does change the resulting matrix of the multiplication itself).</p>

<h3>Positive Definite and Symmetric</h3>

<p>Next, let's look at the properties of symmetric positive definite matrices. If $\boldsymbol{\Sigma}$ is a symmetric positive definite matrix, "symmetric" means $\boldsymbol{\Sigma}^{\top}=\boldsymbol{\Sigma}$, and "positive definite" means that for any non-zero vector $\boldsymbol{\xi}\in\mathbb{R}^n$, $\boldsymbol{\xi}^{\top}\boldsymbol{\Sigma}\boldsymbol{\xi} > 0$. It can be proven that if $\boldsymbol{\Sigma}_1,\boldsymbol{\Sigma}_2$ are both symmetric positive definite matrices, then $\boldsymbol{\Sigma}_1^{-1},\boldsymbol{\Sigma}_2^{-1},\boldsymbol{\Sigma}_1+\boldsymbol{\Sigma}_2$ are also symmetric positive definite. If $\boldsymbol{C} = \boldsymbol{B}^{\top}\boldsymbol{A}\boldsymbol{B}$ and $\boldsymbol{B}$ is invertible, then $\boldsymbol{C}$ is symmetric positive definite if and only if $\boldsymbol{A}$ is symmetric positive definite.</p>

<p>There is also the concept of positive semi-definiteness, meaning that for any non-zero vector $\boldsymbol{\xi}\in\mathbb{R}^n$, $\boldsymbol{\xi}^{\top}\boldsymbol{\Sigma}\boldsymbol{\xi} \geq 0$. However, considering that positive definite matrices are dense within positive semi-definite matrices, we do not strictly distinguish between them here and treat them uniformly as positive definite matrices.</p>

<p>Symmetric positive definite matrices have an important property: their SVD decomposition is identical to their eigenvalue decomposition, having the following form:</p>

\begin{equation}\boldsymbol{\Sigma} = \boldsymbol{U}\boldsymbol{\Lambda}\boldsymbol{U}^{\top}\end{equation}

<p>where $\boldsymbol{U}$ is an orthogonal matrix and $\boldsymbol{\Lambda}$ is a diagonal matrix with positive elements on the diagonal. A direct corollary is that symmetric positive definite matrices can have a "square root," which is $\boldsymbol{\Sigma}^{1/2} = \boldsymbol{U}\boldsymbol{\Lambda}^{1/2}\boldsymbol{U}^{\top}$, where $\boldsymbol{\Lambda}^{1/2}$ represents taking the square root of the diagonal elements. It can be verified that the square root matrix is also symmetric positive definite. Conversely, a symmetric matrix that can have a square root must be symmetric positive definite.</p>

<h3>Matrix Calculus</h3>

<p>Finally, in calculating the Wasserstein distance, some matrix calculus formulas are required. Readers who are not familiar can refer to Wikipedia's "<a href="https://en.wikipedia.org/wiki/Matrix_calculus">Matrix Calculus</a>". The main formula used is:</p>

\begin{equation}\frac{\partial\,\text{Tr}\left(\boldsymbol{X}\boldsymbol{A}\right)}{\partial \boldsymbol{X}} = \boldsymbol{A}\end{equation}

<p>Other formulas can be derived using trace identities, such as:</p>

\begin{equation}\frac{\partial\,\text{Tr}\left(\boldsymbol{A}\boldsymbol{X}\boldsymbol{B}\right)}{\partial \boldsymbol{X}} = \frac{\partial\,\text{Tr}\left(\boldsymbol{X}\boldsymbol{B}\boldsymbol{A}\right)}{\partial \boldsymbol{X}} = \boldsymbol{B}\boldsymbol{A}\end{equation}

<h2>KL Divergence</h2>

<p>As our first objective, we calculate the <a href="https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence">KL divergence (Kullback-Leibler divergence)</a> between two Gaussian distributions. KL divergence is one of the most commonly used distribution metrics because it involves a logarithm before integration, which usually yields relatively simple results for exponential family distributions. Additionally, it is closely related to "entropy."</p>

<h3>Calculated Result</h3>

<p>The KL divergence between two probability distributions is defined as:</p>

\begin{equation}KL(p(\boldsymbol{x})\Vert q(\boldsymbol{x}))=\mathbb{E}_{\boldsymbol{x}\sim p(\boldsymbol{x})}\left[\log \frac{p(\boldsymbol{x})}{q(\boldsymbol{x})}\right]=\mathbb{E}_{\boldsymbol{x}\sim p(\boldsymbol{x})}\left[\log p(\boldsymbol{x})\right]+\mathbb{E}_{\boldsymbol{x}\sim p(\boldsymbol{x})}\left[-\log q(\boldsymbol{x})\right]\end{equation}

<p>For two normal distributions, the calculated result is:</p>

\begin{equation}
KL(p(\boldsymbol{x})\Vert q(\boldsymbol{x}))=\frac{1}{2}\left[(\boldsymbol{\mu}_p-\boldsymbol{\mu}_q)^{\top}\boldsymbol{\Sigma}_q^{-1}(\boldsymbol{\mu}_p-\boldsymbol{\mu}_q)-\log \det(\boldsymbol{\Sigma}_q^{-1}\boldsymbol{\Sigma}_p) + \text{Tr}\left(\boldsymbol{\Sigma}_q^{-1}\boldsymbol{\Sigma}_p\right) - n\right]
\end{equation}

<p>In particular, when $q$ is the standard normal distribution, the result simplifies to:</p>

\begin{equation}
KL(p(\boldsymbol{x})\Vert q(\boldsymbol{x}))=\frac{1}{2}\left[\|\boldsymbol{\mu}_p\|^2-\log \det(\boldsymbol{\Sigma}_p) + \text{Tr}(\boldsymbol{\Sigma}_p) - n\right]\end{equation}

<h3>Derivation Process</h3>

<p>From the definition of KL divergence, we mainly need to compute $\mathbb{E}_{\boldsymbol{x}\sim p(\boldsymbol{x})}\left[-\log q(\boldsymbol{x})\right]$:</p>

\begin{equation}\begin{aligned}
\mathbb{E}_{\boldsymbol{x}\sim p(\boldsymbol{x})}\left[-\log q(\boldsymbol{x})\right] =&\, \mathbb{E}_{\boldsymbol{x}\sim p(\boldsymbol{x})}\left[\frac{n}{2}\log 2\pi + \frac{1}{2}\log \det(\Sigma_q) + \frac{1}{2}(\boldsymbol{x}-\boldsymbol{\mu}_q)^{\top}\boldsymbol{\Sigma}_q^{-1}(\boldsymbol{x}-\boldsymbol{\mu}_q)\right]\\
=&\,\frac{n}{2}\log 2\pi + \frac{1}{2}\log \det(\boldsymbol{\Sigma}_q) + \frac{1}{2}\mathbb{E}_{\boldsymbol{x}\sim p(\boldsymbol{x})}\left[(\boldsymbol{x}-\boldsymbol{\mu}_q)^{\top}\boldsymbol{\Sigma}_q^{-1}(\boldsymbol{x}-\boldsymbol{\mu}_q)\right]
\end{aligned}\end{equation}

<p>Now, the trace identities come into play:</p>

\begin{equation}\begin{aligned}
\mathbb{E}_{\boldsymbol{x}\sim p(\boldsymbol{x})}\left[(\boldsymbol{x}-\boldsymbol{\mu}_q)^{\top}\boldsymbol{\Sigma}_q^{-1}(\boldsymbol{x}-\boldsymbol{\mu}_q)\right]=&\,\mathbb{E}_{\boldsymbol{x}\sim p(\boldsymbol{x})}\left[\text{Tr}\left((\boldsymbol{x}-\boldsymbol{\mu}_q)^{\top}\boldsymbol{\Sigma}_q^{-1}(\boldsymbol{x}-\boldsymbol{\mu}_q)\right)\right]\\
=&\,\mathbb{E}_{\boldsymbol{x}\sim p(\boldsymbol{x})}\left[\text{Tr}\left(\boldsymbol{\Sigma}_q^{-1}(\boldsymbol{x}-\boldsymbol{\mu}_q)(\boldsymbol{x}-\boldsymbol{\mu}_q)^{\top}\right)\right]\\
=&\,\text{Tr}\left(\boldsymbol{\Sigma}_q^{-1}\mathbb{E}_{\boldsymbol{x}\sim p(\boldsymbol{x})}\left[(\boldsymbol{x}-\boldsymbol{\mu}_q)(\boldsymbol{x}-\boldsymbol{\mu}_q)^{\top}\right]\right)\\
=&\,\text{Tr}\left(\boldsymbol{\Sigma}_q^{-1}\mathbb{E}_{\boldsymbol{x}\sim p(\boldsymbol{x})}\left[\boldsymbol{x}\boldsymbol{x}^{\top}-\boldsymbol{\mu}_q\boldsymbol{x}^{\top} - \boldsymbol{x}\boldsymbol{\mu}_q^{\top} + \boldsymbol{\mu}_q\boldsymbol{\mu}_q^{\top}\right]\right)\\
=&\,\text{Tr}\left(\boldsymbol{\Sigma}_q^{-1}\left(\boldsymbol{\Sigma}_p + \boldsymbol{\mu}_p\boldsymbol{\mu}_p^{\top}-\boldsymbol{\mu}_q\boldsymbol{\mu}_p^{\top} - \boldsymbol{\mu}_p\boldsymbol{\mu}_q^{\top} + \boldsymbol{\mu}_q\boldsymbol{\mu}_q^{\top}\right)\right)\\
=&\,\text{Tr}\left(\boldsymbol{\Sigma}_q^{-1}\boldsymbol{\Sigma}_p + \boldsymbol{\Sigma}_q^{-1}(\boldsymbol{\mu}_p-\boldsymbol{\mu}_q)(\boldsymbol{\mu}_p-\boldsymbol{\mu}_q)^{\top}\right)\\
=&\,\text{Tr}\left(\boldsymbol{\Sigma}_q^{-1}\boldsymbol{\Sigma}_p\right) + (\boldsymbol{\mu}_p-\boldsymbol{\mu}_q)^{\top}\boldsymbol{\Sigma}_q^{-1}(\boldsymbol{\mu}_p-\boldsymbol{\mu}_q)\\
\end{aligned}\end{equation}

<p>Note that when $\boldsymbol{\mu}_q=\boldsymbol{\mu}_p$ and $\boldsymbol{\Sigma}_q=\boldsymbol{\Sigma}_p$, the above equation equals $n$, which corresponds to the entropy of the normal distribution. Thus, the final result is:</p>

\begin{equation}\begin{aligned}
KL(p(\boldsymbol{x})\Vert q(\boldsymbol{x}))=&\,\frac{1}{2}\left[n\log 2\pi + \log \det(\boldsymbol{\Sigma}_q) + \text{Tr}\left(\boldsymbol{\Sigma}_q^{-1}\boldsymbol{\Sigma}_p\right) + (\boldsymbol{\mu}_p-\boldsymbol{\mu}_q)^{\top}\boldsymbol{\Sigma}_q^{-1}(\boldsymbol{\mu}_p-\boldsymbol{\mu}_q)\right] \\
&\,\qquad- \frac{1}{2}\left[n\log 2\pi + \log \det(\boldsymbol{\Sigma}_p) + n\right]\\
=&\,\frac{1}{2}\left[(\boldsymbol{\mu}_p-\boldsymbol{\mu}_q)^{\top}\boldsymbol{\Sigma}_q^{-1}(\boldsymbol{\mu}_p-\boldsymbol{\mu}_q)-\log \det(\boldsymbol{\Sigma}_q^{-1}\boldsymbol{\Sigma}_p) + \text{Tr}\left(\boldsymbol{\Sigma}_q^{-1}\boldsymbol{\Sigma}_p\right) - n\right]
\end{aligned}\end{equation}

<h2>Bhattacharyya Distance</h2>

<p>Next, let's examine the <a href="https://en.wikipedia.org/wiki/Bhattacharyya_distance">Bhattacharyya distance (BD)</a>, defined as:</p>

\begin{equation}BD(p(\boldsymbol{x}), q(\boldsymbol{x})) = -\log \int \sqrt{p(\boldsymbol{x}) q(\boldsymbol{x})} d\boldsymbol{x}\end{equation}

<p>A related concept is the "<a href="https://en.wikipedia.org/wiki/Hellinger_distance">Hellinger distance</a>", whose squared value is defined as $\frac{1}{2}\int \left(\sqrt{p(\boldsymbol{x})} - \sqrt{q(\boldsymbol{x})}\right)^2 d\boldsymbol{x}$. Expanding this reveals it is essentially equivalent to the Bhattacharyya distance.</p>

<h3>Calculated Result</h3>

<p>For two normal distributions, their Bhattacharyya distance is:</p>

\begin{equation}
BD(p(\boldsymbol{x}), q(\boldsymbol{x})) = \frac{1}{2}\log \frac{\det(\boldsymbol{\Sigma})}{\sqrt{\det(\boldsymbol{\Sigma}_p\boldsymbol{\Sigma}_q)}} + \frac{1}{8}(\boldsymbol{\mu}_p - \boldsymbol{\mu}_q)^{\top}\boldsymbol{\Sigma}^{-1}(\boldsymbol{\mu}_p - \boldsymbol{\mu}_q)
\end{equation}

<p>Here $\boldsymbol{\Sigma}=\frac{1}{2}(\boldsymbol{\Sigma}_p + \boldsymbol{\Sigma}_q)$. Note that the result is symmetric, as the definition of the Bhattacharyya distance itself is symmetric.</p>

<p>When one of the distributions is the standard normal distribution, the result does not simplify significantly, so it is not listed separately here.</p>

<h3>Derivation Process</h3>

<p>By definition, the Bhattacharyya distance between two normal distributions is the negative logarithm of the following integral:</p>

\begin{equation}\begin{aligned}
&\qquad\int \sqrt{p(\boldsymbol{x}) q(\boldsymbol{x})} d\boldsymbol{x}=\frac{1}{\sqrt[4]{(2\pi)^{2n}\det(\boldsymbol{\Sigma}_p\boldsymbol{\Sigma}_q)}}\times \\
&\int \exp\left\{-\frac{1}{4}(\boldsymbol{x}-\boldsymbol{\mu}_p)^{\top}\boldsymbol{\Sigma}_p^{-1}(\boldsymbol{x}-\boldsymbol{\mu}_p)-\frac{1}{4}(\boldsymbol{x}-\boldsymbol{\mu}_q)^{\top}\boldsymbol{\Sigma}_q^{-1}(\boldsymbol{x}-\boldsymbol{\mu}_q)\right\}d\boldsymbol{x}
\end{aligned}\end{equation}

<p>Let $\boldsymbol{y}=\boldsymbol{x}-\boldsymbol{\mu}_p$ and $\boldsymbol{\Delta}=\boldsymbol{\mu}_p - \boldsymbol{\mu}_q$. The integral part can be changed to variables:</p>

\begin{equation}\begin{aligned}
&\int \exp\left\{-\frac{1}{4}\boldsymbol{y}^{\top}\boldsymbol{\Sigma}_p^{-1}\boldsymbol{y}-\frac{1}{4}(\boldsymbol{y}+\boldsymbol{\Delta})^{\top}\boldsymbol{\Sigma}_q^{-1}(\boldsymbol{y}+\boldsymbol{\Delta})\right\}d\boldsymbol{y}\\
=&\int \exp\left\{-\frac{1}{4}\boldsymbol{y}^{\top}\left(\boldsymbol{\Sigma}_p^{-1}+\boldsymbol{\Sigma}_q^{-1}\right)\boldsymbol{y}-\frac{1}{2}\boldsymbol{\Delta}^{\top}\boldsymbol{\Sigma}_q^{-1}\boldsymbol{y} - \frac{1}{4}\boldsymbol{\Delta}^{\top}\boldsymbol{\Sigma}_q^{-1}\boldsymbol{\Delta}\right\}d\boldsymbol{y}\\
=&\int \exp\left\{-\frac{1}{2}\boldsymbol{y}^{\top}\left(\boldsymbol{\Sigma}_p^{-1}\boldsymbol{\Sigma}\boldsymbol{\Sigma}_q^{-1}\right)\boldsymbol{y}-\frac{1}{2}\boldsymbol{\Delta}^{\top}\boldsymbol{\Sigma}_q^{-1}\boldsymbol{y} - \frac{1}{4}\boldsymbol{\Delta}^{\top}\boldsymbol{\Sigma}_q^{-1}\boldsymbol{\Delta}\right\}d\boldsymbol{y}\end{aligned}\end{equation}

<p>where $\boldsymbol{\Sigma}=\frac{1}{2}(\boldsymbol{\Sigma}_p + \boldsymbol{\Sigma}_q)$. According to the Gaussian integral formula $\eqref{eq:g-int}$, the integral result is:</p>

\begin{equation}\begin{aligned}
&\,\sqrt{(2\pi)^n \det(\boldsymbol{\Sigma}_p^{-1}\boldsymbol{\Sigma}\boldsymbol{\Sigma}_q^{-1})^{-1}}\exp\left\{\frac{1}{8}\left(\boldsymbol{\Sigma}_q^{-1}\boldsymbol{\Delta}\right)^{\top}\left(\boldsymbol{\Sigma}_p^{-1}\boldsymbol{\Sigma}\boldsymbol{\Sigma}_q^{-1}\right)^{-1}\left(\boldsymbol{\Sigma}_q^{-1}\boldsymbol{\Delta}\right)-\frac{1}{4}\boldsymbol{\Delta}^{\top}\boldsymbol{\Sigma}_q^{-1}\boldsymbol{\Delta}\right\}\\
=&\,\sqrt{(2\pi)^n \det(\boldsymbol{\Sigma}_q\boldsymbol{\Sigma}^{-1}\boldsymbol{\Sigma}_p)}\exp\left\{\frac{1}{8}\boldsymbol{\Delta}^{\top}\left(\boldsymbol{\Sigma}^{-1}\boldsymbol{\Sigma}_p\boldsymbol{\Sigma}_q^{-1} - 2\boldsymbol{\Sigma}_q^{-1}\right)\boldsymbol{\Delta}\right\}\\
=&\,\sqrt{(2\pi)^n \det(\boldsymbol{\Sigma}_q\boldsymbol{\Sigma}^{-1}\boldsymbol{\Sigma}_p)}\exp\left\{\frac{1}{8}\boldsymbol{\Delta}^{\top}\boldsymbol{\Sigma}^{-1}\left(\boldsymbol{\Sigma}_p\boldsymbol{\Sigma}_q^{-1} - 2\boldsymbol{\Sigma}\boldsymbol{\Sigma}_q^{-1}\right)\boldsymbol{\Delta}\right\}\\
=&\,\sqrt{(2\pi)^n \det(\boldsymbol{\Sigma}_q\boldsymbol{\Sigma}^{-1}\boldsymbol{\Sigma}_p)}\exp\left\{-\frac{1}{8}\boldsymbol{\Delta}^{\top}\boldsymbol{\Sigma}^{-1}\boldsymbol{\Delta}\right\}
\end{aligned}\end{equation}

<p>Thus, finally:</p>

\begin{equation}\begin{aligned}
BD(p(\boldsymbol{x}), q(\boldsymbol{x})) =&\, -\log \left[\frac{\sqrt{(2\pi)^n \det(\boldsymbol{\Sigma}_q\boldsymbol{\Sigma}^{-1}\boldsymbol{\Sigma}_p)}}{\sqrt[4]{(2\pi)^{2n}\det(\boldsymbol{\Sigma}_p\boldsymbol{\Sigma}_q)}}\exp\left\{-\frac{1}{8}\boldsymbol{\Delta}^{\top}\boldsymbol{\Sigma}^{-1}\boldsymbol{\Delta}\right\}\right] \\
=&\, -\log \left[\frac{\sqrt[4]{\det(\boldsymbol{\Sigma}_p\boldsymbol{\Sigma}_q)}}{\sqrt{\det\left(\boldsymbol{\Sigma}\right)}}\exp\left\{-\frac{1}{8}\boldsymbol{\Delta}^{\top}\boldsymbol{\Sigma}^{-1}\boldsymbol{\Delta}\right\}\right]\\
=&\,\frac{1}{2}\log \frac{\det(\boldsymbol{\Sigma})}{\sqrt{\det(\boldsymbol{\Sigma}_p\boldsymbol{\Sigma}_q)}} + \frac{1}{8}(\boldsymbol{\mu}_p - \boldsymbol{\mu}_q)^{\top}\boldsymbol{\Sigma}^{-1}(\boldsymbol{\mu}_p - \boldsymbol{\mu}_q)
\end{aligned}
\end{equation}

<h2>Wasserstein Distance</h2>

<p>If readers wish to explore more about probability divergence, they can refer to the book <a href="https://www.taylorfrancis.com/books/mono/10.1201/9781420034813/statistical-inference-based-divergence-measures-leandro-pardo">"Statistical Inference Based on Divergence Measures"</a>. Now we turn to another class of probability metrics—the Wasserstein distance based on optimal transport.</p>

<p>Following the notation in <a href="translation_6280.html">"From Wasserstein Distance, Duality Theory to WGAN"</a>, the Wasserstein distance is defined as:</p>

\begin{equation}\begin{aligned}
\mathcal{W}_{\rho}[p,q]=&\,\left(\inf_{\gamma\in \Pi[p,q]} \iint \gamma(\boldsymbol{x},\boldsymbol{y}) \|\boldsymbol{x} - \boldsymbol{y}\|^{\rho} d\boldsymbol{x}d\boldsymbol{y}\right)^{1/\rho}\\
=&\,\left(\inf_{\gamma\in \Pi[p,q]} \mathbb{E}_{(\boldsymbol{x},\boldsymbol{y})\sim\gamma(\boldsymbol{x},\boldsymbol{y})} \left[\|\boldsymbol{x} - \boldsymbol{y}\|^{\rho}\right]\right)^{1/\rho}
\end{aligned}\end{equation}

<p>According to the references found by the author, an analytical solution for the Wasserstein distance between two normal distributions is currently known only for $\rho=2$. Below we solve for $\mathcal{W}_2[p,q]$, and for simplicity, let:</p>

\begin{equation}\mathcal{W}_2^2[p,q] = \left(\mathcal{W}_2[p,q]\right)^2\end{equation}

<h3>Calculated Result</h3>

<p>Interestingly, regarding the result of the Wasserstein distance between two normal distributions, there are two different versions in circulation. Both versions have a certain level of recognition, but no documentation explicitly states their equivalence. These two versions come from different papers and have been given different names.</p>

<h4>Version 1</h4>

<p>The relatively more widely circulated version, often found when searching for "Wasserstein distance of normal distributions," provides this result:</p>

\begin{equation}\mathcal{W}_2^2[p,q]=\|\boldsymbol{\mu}_p - \boldsymbol{\mu}_q\|^2 + \text{Tr}(\boldsymbol{\Sigma}_p) + \text{Tr}(\boldsymbol{\Sigma}_q) - 2\text{Tr}((\boldsymbol{\Sigma}_p^{1/2}\boldsymbol{\Sigma}_q\boldsymbol{\Sigma}_p^{1/2})^{1/2})\label{eq:w-v1}\end{equation}

<p>Some readers might be confused as to why this doesn't look symmetric with respect to $p$ and $q$. In fact, it is symmetric because:</p>

\begin{equation}\begin{aligned}\text{Tr}((\boldsymbol{\Sigma}_p^{1/2}\boldsymbol{\Sigma}_q\boldsymbol{\Sigma}_p^{1/2})^{1/2})=&\,\text{Tr}((\boldsymbol{\Sigma}_p^{1/2}\boldsymbol{\Sigma}_q\boldsymbol{\Sigma}_p^{1/2})^{1/2}\boldsymbol{\Sigma}_p^{-1/2}\boldsymbol{\Sigma}_q^{-1/2}\boldsymbol{\Sigma}_q^{1/2}\boldsymbol{\Sigma}_p^{1/2})\\
=&\,\text{Tr}(\boldsymbol{\Sigma}_q^{1/2}\boldsymbol{\Sigma}_p^{1/2}(\boldsymbol{\Sigma}_p^{1/2}\boldsymbol{\Sigma}_q\boldsymbol{\Sigma}_p^{1/2})^{1/2}\boldsymbol{\Sigma}_p^{-1/2}\boldsymbol{\Sigma}_q^{-1/2})
\end{aligned}\end{equation}

<p>Then we can directly verify that $(\boldsymbol{\Sigma}_q^{1/2}\boldsymbol{\Sigma}_p^{1/2}(\boldsymbol{\Sigma}_p^{1/2}\boldsymbol{\Sigma}_q\boldsymbol{\Sigma}_p^{1/2})^{1/2}\boldsymbol{\Sigma}_p^{-1/2}\boldsymbol{\Sigma}_q^{-1/2})^2=\boldsymbol{\Sigma}_q^{1/2}\boldsymbol{\Sigma}_p\boldsymbol{\Sigma}_q^{1/2}$, so $\text{Tr}((\boldsymbol{\Sigma}_p^{1/2}\boldsymbol{\Sigma}_q\boldsymbol{\Sigma}_p^{1/2})^{1/2})=\text{Tr}((\boldsymbol{\Sigma}_q^{1/2}\boldsymbol{\Sigma}_p\boldsymbol{\Sigma}_q^{1/2})^{1/2})$.</p>

<h4>Version 2</h4>

<p>The second version looks slightly simpler:</p>

\begin{equation}\mathcal{W}_2^2[p,q]=\|\boldsymbol{\mu}_p - \boldsymbol{\mu}_q\|^2 + \text{Tr}(\boldsymbol{\Sigma}_p) + \text{Tr}(\boldsymbol{\Sigma}_q) - 2\text{Tr}((\boldsymbol{\Sigma}_p\boldsymbol{\Sigma}_q)^{1/2})\label{eq:w-v2}\end{equation}

<p>This version is commonly called the "<a href="https://en.wikipedia.org/wiki/Fr%C3%A9chet_distance">Fréchet distance</a>." Usually, one can only find this result by using the keyword "Fréchet distance of normal distributions." The evaluation metric FID (<a href="https://papers.cool/arxiv/1706.08500">Frechet Inception Distance</a>) frequently used in GANs is calculated based on this formula. Its symmetry can be proven similarly to the previous proof, or derived directly from the equivalence discussion below.</p>

<h4>Equivalence</h4>

<p>Logically, Version 2 is more concise and should be the standard. Why both versions still circulate simultaneously is quite baffling. Theoretically, proving the equivalence is not difficult. Based on trace identities, we have:</p>

\begin{equation}\begin{aligned}\text{Tr}((\boldsymbol{\Sigma}_p^{1/2}\boldsymbol{\Sigma}_q\boldsymbol{\Sigma}_p^{1/2})^{1/2})=&\,\text{Tr}((\boldsymbol{\Sigma}_p^{1/2}\boldsymbol{\Sigma}_q\boldsymbol{\Sigma}_p^{1/2})^{1/2}\boldsymbol{\Sigma}_p^{-1/2}\boldsymbol{\Sigma}_p^{1/2})\\
=&\,\text{Tr}(\boldsymbol{\Sigma}_p^{1/2}(\boldsymbol{\Sigma}_p^{1/2}\boldsymbol{\Sigma}_q\boldsymbol{\Sigma}_p^{1/2})^{1/2}\boldsymbol{\Sigma}_p^{-1/2})
\end{aligned}\end{equation}

<p>Then just verify directly that $(\boldsymbol{\Sigma}_p^{1/2}(\boldsymbol{\Sigma}_p^{1/2}\boldsymbol{\Sigma}_q\boldsymbol{\Sigma}_p^{1/2})^{1/2}\boldsymbol{\Sigma}_p^{-1/2})^2=\boldsymbol{\Sigma}_p \boldsymbol{\Sigma}_q$.</p>

<h4>Special Cases</h4>

<p>In particular, if the multiplication of $\boldsymbol{\Sigma}_p$ and $\boldsymbol{\Sigma}_q$ is commutative, the formula simplifies into a very intuitive form:</p>

\begin{equation}\mathcal{W}_2^2[p,q]=\|\boldsymbol{\mu}_p - \boldsymbol{\mu}_q\|^2 + \|\boldsymbol{\Sigma}_p^{1/2} - \boldsymbol{\Sigma}_q^{1/2}\|_F^2\label{eq:w-jiaohuan}\end{equation}

<p>Why is this intuitive? Since the parameters of the normal distributions are $\boldsymbol{\mu},\boldsymbol{\Sigma}$, comparing their difference is essentially comparing the differences in $\boldsymbol{\mu}$ and $\boldsymbol{\Sigma}$. According to machine learning conventions, a natural metric would be the squared error:</p>

\begin{equation}\mathcal{W}_2^2[p,q]=\|\boldsymbol{\mu}_p - \boldsymbol{\mu}_q\|^2 + \|\boldsymbol{\Sigma}_p - \boldsymbol{\Sigma}_q\|_F^2\end{equation}

<p>However, from a physical perspective, this metric is inappropriate. If $\boldsymbol{\mu}$ is considered to have a dimension of length, then $\boldsymbol{\Sigma}$ has a dimension of length squared. Therefore, $\|\boldsymbol{\mu}_p - \boldsymbol{\mu}_q\|^2$ and $\|\boldsymbol{\Sigma}_p - \boldsymbol{\Sigma}_q\|_F^2$ have different dimensions and cannot be added. To maintain dimensional consistency, the intuitive approach is to "take the square root" of $\boldsymbol{\Sigma}$ before calculating the squared error, leading to equation $\eqref{eq:w-jiaohuan}$.</p>

<p>Specifically, when $q$ is the standard normal distribution, the result simplifies to:</p>

\begin{equation}\mathcal{W}_2^2[p,q]=\|\boldsymbol{\mu}_p\|^2 + \|\boldsymbol{\Sigma}_p^{1/2} - \boldsymbol{I}\|_F^2\end{equation}

<h3>Derivation 1</h3>

<p>We now introduce the first proof, which mainly refers to the paper <a href="https://projecteuclid.org/journals/michigan-mathematical-journal/volume-31/issue-2/A-class-of-Wasserstein-metrics-for-probability-distributions/10.1307/mmj/1029003026.full">"A class of Wasserstein metrics for probability distributions"</a>. Additionally, <a href="https://www.sciencedirect.com/science/article/pii/0024379582901124">"The distance between two random vectors with given dispersion matrices"</a> provides a similar proof. </p>

<p>The following derivation has been simplified by the author. It is easier than the original paper's proof but still involves significant linear algebra. We will introduce it in several parts.</p>

<h4>Demening (Mean Centering)</h4>

<p>Without loss of generality, we can consider distributions $p,q$ with zero mean. If the means of $p,q$ are non-zero, let the corresponding zero-mean distributions be $\tilde{p},\tilde{q}$. We have:</p>

\begin{equation}\begin{aligned}
&\,\mathbb{E}_{(\boldsymbol{x},\boldsymbol{y})\sim\gamma(\boldsymbol{x},\boldsymbol{y})}\left[\| \boldsymbol{x} - \boldsymbol{y}\|^2\right] \\
=&\, \mathbb{E}_{(\boldsymbol{x},\boldsymbol{y})\sim\tilde{\gamma}(\boldsymbol{x},\boldsymbol{y})}\left[\| (\boldsymbol{x} + \boldsymbol{\mu}_p) - (\boldsymbol{y} + \boldsymbol{\mu}_q)\|^2 \right]\\
=&\,\mathbb{E}_{(\boldsymbol{x},\boldsymbol{y})\sim\tilde{\gamma}(\boldsymbol{x},\boldsymbol{y})}\left[\| \boldsymbol{x} - \boldsymbol{y}\|^2 + \| \boldsymbol{\mu}_p - \boldsymbol{\mu}_q\|^2 + 2\langle\boldsymbol{x} - \boldsymbol{y}, \boldsymbol{\mu}_p - \boldsymbol{\mu}_q\rangle\right]\\
=&\,\,\| \boldsymbol{\mu}_p - \boldsymbol{\mu}_q\|^2 + \mathbb{E}_{(\boldsymbol{x},\boldsymbol{y})\sim\tilde{\gamma}(\boldsymbol{x},\boldsymbol{y})}\left[\| \boldsymbol{x} - \boldsymbol{y}\|^2 \right]
\end{aligned}\end{equation}

<p>This implies:</p>

\begin{equation}\mathcal{W}_2^2[p,q]=\|\boldsymbol{\mu}_p - \boldsymbol{\mu}_q\|^2 + \mathcal{W}_2^2[\tilde{p},\tilde{q}]\end{equation}

<p>Therefore, we only need to calculate the Wasserstein distance for the zero-mean case and then add $\|\boldsymbol{\mu}_p - \boldsymbol{\mu}_q\|^2$.</p>

<h4>Algebraic Formulation</h4>

<p>Assuming the means of $p,q$ are 0, we calculate:</p>

\begin{equation}\begin{aligned}
\mathbb{E}_{(\boldsymbol{x},\boldsymbol{y})\sim\gamma(\boldsymbol{x},\boldsymbol{y})}\left[\|\boldsymbol{x} - \boldsymbol{y}\|^2\right] =&\, \mathbb{E}_{(\boldsymbol{x},\boldsymbol{y})\sim\gamma(\boldsymbol{x},\boldsymbol{y})}\left[\boldsymbol{x}^{\top} \boldsymbol{x} + \boldsymbol{y}^{\top} \boldsymbol{y} - 2\boldsymbol{y}^{\top} \boldsymbol{x}\right]\\
=&\, \mathbb{E}_{(\boldsymbol{x},\boldsymbol{y})\sim\gamma(\boldsymbol{x},\boldsymbol{y})}\left[\text{Tr}\left(\boldsymbol{x} \boldsymbol{x}^{\top} + \boldsymbol{y} \boldsymbol{y}^{\top} - 2\boldsymbol{x}\boldsymbol{y}^{\top} \right)\right]\\
=&\, \text{Tr}\left(\mathbb{E}_{(\boldsymbol{x},\boldsymbol{y})\sim\gamma(\boldsymbol{x},\boldsymbol{y})}\left[\boldsymbol{x} \boldsymbol{x}^{\top} + \boldsymbol{y} \boldsymbol{y}^{\top} - 2\boldsymbol{x}\boldsymbol{y}^{\top}\right]\right)\\
=&\, \text{Tr}(\boldsymbol{\Sigma}_p) + \text{Tr}(\boldsymbol{\Sigma}_q) - 2\text{Tr}(\boldsymbol{C})
\end{aligned}\end{equation}

<p>where</p>

\begin{equation}\boldsymbol{\Sigma}_{\gamma}= \begin{pmatrix} \boldsymbol{\Sigma}_p & \boldsymbol{C}\\ \boldsymbol{C}^{\top} & \boldsymbol{\Sigma}_q\end{pmatrix}=\mathbb{E}_{(\boldsymbol{x},\boldsymbol{y})\sim\gamma(\boldsymbol{x},\boldsymbol{y})}\left[\begin{pmatrix}\boldsymbol{x} \\ \boldsymbol{y}\end{pmatrix}\begin{pmatrix}\boldsymbol{x}^{\top} & \boldsymbol{y}^{\top}\end{pmatrix}\right]\end{equation}

<p>is the joint covariance matrix of $\gamma$. Since any covariance matrix is symmetric positive semi-definite, the problem becomes: </p>

<blockquote>Given that $\boldsymbol{\Sigma}_{\gamma}= \begin{pmatrix} \boldsymbol{\Sigma}_p & \boldsymbol{C}\\ \boldsymbol{C}^{\top} & \boldsymbol{\Sigma}_q\end{pmatrix}$ is symmetric positive definite, find the maximum value of $\text{Tr}(\boldsymbol{C})$.</blockquote>

<h4>Schur Complement</h4>

<p>For this, we use the following identity regarding the Schur complement:</p>

\begin{equation}
\begin{pmatrix} \boldsymbol{\Sigma}_p & \boldsymbol{C}\\ \boldsymbol{C}^{\top} & \boldsymbol{\Sigma}_q\end{pmatrix} = \begin{pmatrix} \boldsymbol{I} & \boldsymbol{0}\\ \boldsymbol{C}^{\top}\boldsymbol{\Sigma}_p^{-1} & \boldsymbol{I}\end{pmatrix} \begin{pmatrix} \boldsymbol{\Sigma}_p & \boldsymbol{0}\\ \boldsymbol{0} & \boldsymbol{\Sigma}_q - \boldsymbol{C}^{\top}\boldsymbol{\Sigma}_p^{-1}\boldsymbol{C}\end{pmatrix} \begin{pmatrix} \boldsymbol{I} & \boldsymbol{\Sigma}_p^{-1}\boldsymbol{C} \\ \boldsymbol{0} & \boldsymbol{I}\end{pmatrix}
\end{equation}

<p>where the symmetric matrix $\boldsymbol{S} = \boldsymbol{\Sigma}_q - \boldsymbol{C}^{\top}\boldsymbol{\Sigma}_p^{-1}\boldsymbol{C}$ is called the "<a href="https://en.wikipedia.org/wiki/Schur_complement">Schur Complement</a>." This decomposition has the form $\boldsymbol{B}^{\top}\boldsymbol{A}\boldsymbol{B}$. For it to be positive definite, $\boldsymbol{A}$ must be positive definite. Since $\boldsymbol{\Sigma}_p$ is already positive definite, $\boldsymbol{S}$ must also be positive definite.</p>

<h4>Isolating Parameters</h4>

<p>We try to isolate the parameters, specifically $\boldsymbol{C}$ from $\boldsymbol{S} = \boldsymbol{\Sigma}_q - \boldsymbol{C}^{\top}\boldsymbol{\Sigma}_p^{-1}\boldsymbol{C}$. Rearranging gives $\boldsymbol{\Sigma}_q - \boldsymbol{S} = \boldsymbol{C}^{\top}\boldsymbol{\Sigma}_p^{-1}\boldsymbol{C}$. Since $\boldsymbol{\Sigma}_p$ is symmetric positive definite, $\boldsymbol{\Sigma}_p^{-1}$ is as well, making $\boldsymbol{C}^{\top}\boldsymbol{\Sigma}_p^{-1}\boldsymbol{C}$ symmetric positive definite. It therefore has a symmetric positive definite square root, say $\boldsymbol{R}$, such that:</p>

\begin{equation}\boldsymbol{C}^{\top}\boldsymbol{\Sigma}_p^{-1}\boldsymbol{C} = \boldsymbol{R}^2\quad\Leftrightarrow\quad \left(\boldsymbol{\Sigma}_p^{-1/2}\boldsymbol{C}\boldsymbol{R}^{-1}\right)^{\top}\left(\boldsymbol{\Sigma}_p^{-1/2}\boldsymbol{C}\boldsymbol{R}^{-1}\right)=\boldsymbol{I}\end{equation}

<p>This implies that $\boldsymbol{\Sigma}_p^{-1/2}\boldsymbol{C}\boldsymbol{R}^{-1}$ is an orthogonal matrix, denoted as $\boldsymbol{O}$, so $\boldsymbol{C} = \boldsymbol{\Sigma}_p^{1/2}\boldsymbol{O}\boldsymbol{R}$.</p>

<h4>Method of Multipliers</h4>

<p>The variables now are $\boldsymbol{O}$ and $\boldsymbol{R}$. We want to maximize $\text{Tr}(\boldsymbol{C})=\text{Tr}(\boldsymbol{\Sigma}_p^{1/2}\boldsymbol{O}\boldsymbol{R})$. Fixing $\boldsymbol{R}$ first, we find the $\boldsymbol{O}$ that maximizes this under the constraint $\boldsymbol{O}^{\top}\boldsymbol{O}=\boldsymbol{I}$. We use <a href="https://en.wikipedia.org/wiki/Lagrange_multiplier">Lagrange Multipliers</a>: introduce a symmetric parameter matrix $\boldsymbol{W}$ and transform this into an unconstrained problem:</p>

\begin{equation}F = \text{Tr}(\boldsymbol{\Sigma}_p^{1/2}\boldsymbol{O}\boldsymbol{R}) - \frac{1}{2}\text{Tr}(\boldsymbol{W}(\boldsymbol{O}^{\top}\boldsymbol{O} - \boldsymbol{I}))\end{equation}

<p>Taking derivatives:</p>

\begin{equation}\begin{aligned}
&\frac{\partial F}{\partial \boldsymbol{O}} = \boldsymbol{0} \quad \Rightarrow\quad \boldsymbol{R}\boldsymbol{\Sigma}_p^{1/2} = \boldsymbol{W}\boldsymbol{O}^{\top}\\
&\frac{\partial F}{\partial \boldsymbol{W}} = \boldsymbol{0} \quad \Rightarrow\quad \boldsymbol{O}^{\top}\boldsymbol{O} = \boldsymbol{I}\\
\end{aligned}\end{equation}

<p>Since $\boldsymbol{O}^{\top}\boldsymbol{O} - \boldsymbol{I}$ is symmetric, $\boldsymbol{W}$ is also symmetric. We have:</p>

\begin{equation}\left(\boldsymbol{O}\boldsymbol{W}\boldsymbol{O}^{\top}\right)^2=\left(\boldsymbol{W}\boldsymbol{O}^{\top}\right)^{\top}\left(\boldsymbol{W}\boldsymbol{O}^{\top}\right)=\boldsymbol{\Sigma}_p^{1/2}\boldsymbol{R}^2\boldsymbol{\Sigma}_p^{1/2}\end{equation}

<p>That is, $\boldsymbol{O}\boldsymbol{W}\boldsymbol{O}^{\top}=(\boldsymbol{\Sigma}_p^{1/2}\boldsymbol{R}^2\boldsymbol{\Sigma}_p^{1/2})^{1/2}$, so at this point:</p>

\begin{equation}\text{Tr}(\boldsymbol{\Sigma}_p^{1/2}\boldsymbol{O}\boldsymbol{R})=\text{Tr}(\boldsymbol{O}\boldsymbol{R}\boldsymbol{\Sigma}_p^{1/2})=\text{Tr}(\boldsymbol{O}\boldsymbol{W}\boldsymbol{O}^{\top})=\text{Tr}((\boldsymbol{\Sigma}_p^{1/2}\boldsymbol{R}^2\boldsymbol{\Sigma}_p^{1/2})^{1/2})\end{equation}

<h4>Inequality</h4>

<p>Finally, we need to determine $\boldsymbol{R}$. Recalling the definition $\boldsymbol{R}^2=\boldsymbol{\Sigma}_q - \boldsymbol{S}$ where $\boldsymbol{S}$ is positive definite. Intuitively, the maximum is achieved when $\boldsymbol{S}=\boldsymbol{0}$. This is indeed a direct corollary of <a href="https://en.wikipedia.org/wiki/Weyl%27s_inequality">Weyl's Inequality</a>.</p>

<p>According to Weyl's inequality, if matrices $\boldsymbol{A}$ and $\boldsymbol{B}$ are symmetric positive definite, let their eigenvalues in non-decreasing order be $0\leq\lambda_1^{(A)} \leq \dots \leq \lambda_n^{(A)}$, $\lambda_1^{(B)} \leq \dots \leq \lambda_n^{(B)}$, and $0\leq\lambda_1^{(A+B)} \leq \dots \leq \lambda_n^{(A+B)}$. Then for any $1\leq i \leq n$, $\lambda_i^{(A)}\leq \lambda_i^{(A+B)}$ and $\lambda_i^{(B)}\leq \lambda_i^{(A+B)}$. That is:</p>

<blockquote>The eigenvalues of the sum of symmetric positive definite matrices are coordinate-wise greater than the eigenvalues of the individual matrices.</blockquote>

<p>With this conclusion, let the eigenvalues of $(\boldsymbol{\Sigma}_p^{1/2}(\boldsymbol{\Sigma}_q - \boldsymbol{S})\boldsymbol{\Sigma}_p^{1/2})^{1/2}$ be $0 \leq \lambda_1 \leq \dots \leq \lambda_n$. Its trace is $\lambda_1 + \dots + \lambda_n$, and the eigenvalues of $\boldsymbol{\Sigma}_p^{1/2}(\boldsymbol{\Sigma}_q - \boldsymbol{S})\boldsymbol{\Sigma}_p^{1/2}$ are $0 \leq \lambda_1^2 \leq \dots \leq \lambda_n^2$. Since $\boldsymbol{\Sigma}_p^{1/2}(\boldsymbol{\Sigma}_q - \boldsymbol{S})\boldsymbol{\Sigma}_p^{1/2}$ and $\boldsymbol{\Sigma}_p^{1/2}\boldsymbol{S}\boldsymbol{\Sigma}_p^{1/2}$ are both symmetric positive definite, their eigenvalues do not exceed those of their sum, $\boldsymbol{\Sigma}_p^{1/2}\boldsymbol{\Sigma}_q\boldsymbol{\Sigma}_p^{1/2}$. Thus, the maximum of each eigenvalue (and the trace) of $(\boldsymbol{\Sigma}_p^{1/2}(\boldsymbol{\Sigma}_q - \boldsymbol{S})\boldsymbol{\Sigma}_p^{1/2})^{1/2}$ is achieved at $\boldsymbol{S}=\boldsymbol{0}$.</p>

<p>As for the proof of Weyl's inequality, it mainly utilizes the <a href="https://en.wikipedia.org/wiki/Rayleigh_quotient">Rayleigh quotient</a> and the <a href="https://en.wikipedia.org/wiki/Min-max_theorem">Courant–Fischer Theorem</a>. Interested readers can search for these two parts. Familiarity with them makes the Weyl inequality essentially self-evident. </p>

<h3>Derivation 2</h3>

<p>We provide a simpler proof found in <a href="https://core.ac.uk/download/pdf/82269844.pdf">"The Fréchet distance between multivariate normal distributions"</a>. Compared to the first proof, this one is more direct and requires less pure linear algebra theory. The following process has been further simplified by the author.</p>

<p>Like Derivation 1, the "Demeaning" and "Algebraic Formulation" steps are the same. The problem is reduced to:</p>

<blockquote>Given that $\boldsymbol{\Sigma}_{\gamma}= \begin{pmatrix} \boldsymbol{\Sigma}_p & \boldsymbol{C}\\ \boldsymbol{C}^{\top} & \boldsymbol{\Sigma}_q\end{pmatrix}$ is symmetric positive definite, find the maximum value of $\text{Tr}(\boldsymbol{C})$.</blockquote>

<h4>Block Matrices</h4>

<p>Since $\boldsymbol{\Sigma}_{\gamma}$ is symmetric positive definite, it must be expressible in the form $\boldsymbol{D}\boldsymbol{D}^{\top}$. We express $\boldsymbol{D}$ as a block matrix $\begin{pmatrix}\boldsymbol{A} \\ \boldsymbol{B}\end{pmatrix}$ where $\boldsymbol{A},\boldsymbol{B}\in \mathbb{R}^{n\times 2n}$. Then:</p>

\begin{equation}\begin{pmatrix} \boldsymbol{\Sigma}_p & \boldsymbol{C}\\ \boldsymbol{C}^{\top} & \boldsymbol{\Sigma}_q\end{pmatrix} = \begin{pmatrix}\boldsymbol{A} \\ \boldsymbol{B}\end{pmatrix} \begin{pmatrix}\boldsymbol{A}^{\top} & \boldsymbol{B}^{\top}\end{pmatrix} = \begin{pmatrix} \boldsymbol{A}\boldsymbol{A}^{\top} & \boldsymbol{A}\boldsymbol{B}^{\top}\\ \boldsymbol{B}\boldsymbol{A}^{\top} & \boldsymbol{B}\boldsymbol{B}^{\top}\end{pmatrix}\end{equation}

<p>Correspondingly, $\boldsymbol{\Sigma}_p=\boldsymbol{A}\boldsymbol{A}^{\top}, \boldsymbol{\Sigma}_q=\boldsymbol{B}\boldsymbol{B}^{\top},\boldsymbol{C}=\boldsymbol{A}\boldsymbol{B}^{\top}$.</p>

<h4>Method of Multipliers</h4>

<p>Under this parameterization, the problem becomes:</p>

<blockquote>Given $\boldsymbol{A}\boldsymbol{A}^{\top}=\boldsymbol{\Sigma}_p$ and $\boldsymbol{B}\boldsymbol{B}^{\top}=\boldsymbol{\Sigma}_q$, find the maximum value of $\text{Tr}(\boldsymbol{A}\boldsymbol{B}^{\top})$.</blockquote>

<p>Using Lagrange Multipliers, introduce symmetric matrices $\boldsymbol{W}_p, \boldsymbol{W}_q$ and define:</p>

\begin{equation}F = \text{Tr}(\boldsymbol{A}\boldsymbol{B}^{\top}) - \frac{1}{2}\text{Tr}(\boldsymbol{W}_p(\boldsymbol{A}\boldsymbol{A}^{\top} - \boldsymbol{\Sigma}_p)) - \frac{1}{2}\text{Tr}(\boldsymbol{W}_q(\boldsymbol{B}\boldsymbol{B}^{\top} - \boldsymbol{\Sigma}_q))\end{equation}

<p>Differentiating:</p>

\begin{equation}\begin{aligned}
&\frac{\partial F}{\partial \boldsymbol{A}} = \boldsymbol{0} \quad \Rightarrow\quad \boldsymbol{B}^{\top} = \boldsymbol{A}^{\top}\boldsymbol{W}_p\\
&\frac{\partial F}{\partial \boldsymbol{B}} = \boldsymbol{0} \quad \Rightarrow\quad \boldsymbol{A}^{\top} = \boldsymbol{B}^{\top}\boldsymbol{W}_q\\
&\frac{\partial F}{\partial \boldsymbol{W}_p} = \boldsymbol{0} \quad \Rightarrow\quad \boldsymbol{A}\boldsymbol{A}^{\top} = \boldsymbol{\Sigma}_p\\
&\frac{\partial F}{\partial \boldsymbol{W}_q} = \boldsymbol{0} \quad \Rightarrow\quad \boldsymbol{B}\boldsymbol{B}^{\top} = \boldsymbol{\Sigma}_q\\
\end{aligned}\end{equation}

<p>Since $\boldsymbol{W}_p, \boldsymbol{W}_q$ are symmetric, we have:</p>

\begin{equation}\boldsymbol{\Sigma}_q = \boldsymbol{B}\boldsymbol{B}^{\top} = \left(\boldsymbol{A}^{\top}\boldsymbol{W}_p\right)^{\top}\left(\boldsymbol{A}^{\top}\boldsymbol{W}_p\right)=\boldsymbol{W}_p\boldsymbol{A}\boldsymbol{A}^{\top}\boldsymbol{W}_p=\boldsymbol{W}_p\boldsymbol{\Sigma}_p\boldsymbol{W}_p\\
\end{equation}

<p>Let $\boldsymbol{W}_p=\boldsymbol{\Sigma}_p^{-1/2}\boldsymbol{R}\boldsymbol{\Sigma}_p^{-1/2}$, then $\boldsymbol{\Sigma}_q=\boldsymbol{\Sigma}_p^{-1/2}\boldsymbol{R}^2\boldsymbol{\Sigma}_p^{-1/2}$, so:</p>

\begin{equation}\boldsymbol{R} = (\boldsymbol{\Sigma}_p^{1/2}\boldsymbol{\Sigma}_q\boldsymbol{\Sigma}_p^{1/2})^{1/2}
\end{equation}

<p>And finally:</p>

\begin{equation}\begin{aligned}
\text{Tr}(\boldsymbol{A}\boldsymbol{B}^{\top}) =&\, \text{Tr}(\boldsymbol{A}\boldsymbol{A}^{\top}\boldsymbol{W}_p)=\text{Tr}(\boldsymbol{\Sigma}_p\boldsymbol{W}_p)\\
=&\,\text{Tr}(\boldsymbol{\Sigma}_p^{1/2}\boldsymbol{R}\boldsymbol{\Sigma}_p^{-1/2}) = \text{Tr}(\boldsymbol{R}\boldsymbol{\Sigma}_p^{-1/2}\boldsymbol{\Sigma}_p^{1/2})\\
=&\,\text{Tr}(\boldsymbol{R})
\end{aligned}\end{equation}

<h2>Summary</h2>

<p>This article detailed the calculation of KL divergence, Bhattacharyya distance, and Wasserstein distance between two multivariate normal distributions, providing explicit analytical solutions. These results can serve as regularization terms for latent variables in certain scenarios to constrain their distribution. Furthermore, this article can be viewed as a set of challenging linear algebra exercises for readers to practice with.</p>
<hr>
<footer style="margin-top: 3em; padding: 1.5em; background: #f5f5f5; border-radius: 8px; font-size: 0.9em; color: #555;">
    <p style="margin: 0 0 0.5em 0;"><strong>Citation</strong></p>
    <p style="margin: 0 0 0.5em 0;">
        This is a machine translation of the original Chinese article:<br>
        <a href="translation_8512.html" style="color: #005fcc;">https://kexue.fm/archives/8512</a>
    </p>
    <p style="margin: 0 0 0.5em 0;">
        Original author: 苏剑林 (Su Jianlin)<br>
        Original publication: <a href="https://kexue.fm" style="color: #005fcc;">科学空间 (Scientific Spaces)</a>
    </p>
    <p style="margin: 0; font-style: italic;">
        Translated using Gemini 3 Flash. Please refer to the original for authoritative content.
    </p>
</footer>
