
    <style type="text/css">

    body {
    margin: 48px auto;
    max-width: 68ch;              /* character-based width reads better */
    padding: 0 16px;

    font-family: -apple-system, BlinkMacSystemFont, "Segoe UI",
                Roboto, "Helvetica Neue", Arial, sans-serif;
    font-size: 18px;
    line-height: 1.65;
    color: #333;
    background: #fafafa;
    }

    h1, h2, h3, h4 {
    line-height: 1.25;
    margin-top: 2.2em;
    margin-bottom: 0.6em;
    font-weight: 600;
    }

    h1 {
    font-size: 2.1em;
    margin-top: 0;
    }

    h2 {
    font-size: 1.6em;
    border-bottom: 1px solid #e5e5e5;
    padding-bottom: 0.3em;
    }

    h3 {
    font-size: 1.25em;
    }

    h4 {
    font-size: 1.05em;
    color: #555;
    }

    /* Paragraphs and lists */
    p {
    margin: 1em 0;
    }

    ul, ol {
    margin: 1em 0 1em 1.5em;
    }

    li {
    margin: 0.4em 0;
    }

    a {
    color: #005fcc;
    text-decoration: none;
    }

    a:hover {
    text-decoration: underline;
    }

    code {
    font-family: ui-monospace, SFMono-Regular, Menlo, Consolas, monospace;
    font-size: 0.95em;
    background: #f2f2f2;
    padding: 0.15em 0.35em;
    border-radius: 4px;
    }

    pre {
    font-family: ui-monospace, SFMono-Regular, Menlo, Consolas, monospace;
    font-size: 0.9em;
    background: #f5f5f5;
    padding: 1em 1.2em;
    overflow-x: auto;
    border-radius: 6px;
    line-height: 1.45;
    }

    pre code {
    background: none;
    padding: 0;
    }

    blockquote {
    margin: 1.5em 0;
    padding-left: 1em;
    border-left: 4px solid #ddd;
    color: #555;
    }

    hr {
    border: none;
    border-top: 1px solid #e0e0e0;
    margin: 3em 0;
    }

    table {
    border-collapse: collapse;
    margin: 1.5em 0;
    width: 100%;
    font-size: 0.95em;
    }

    th, td {
    padding: 0.5em 0.7em;
    border-bottom: 1px solid #e5e5e5;
    text-align: left;
    }

    th {
    font-weight: 600;
    }

    img {
    max-width: 100%;
    display: block;
    margin: 1.5em auto;
    }

    small {
    color: #666;
    }

    mjx-container {
    margin: 1em 0;
    }

    ::selection {
    background: #cce2ff;
    }

    </style>
    

<script>
window.MathJax = {
  tex: {
    inlineMath: [['$', '$'], ['\\(', '\\)']],
    displayMath: [['$$', '$$'], ['\\[', '\\]']],
    tags: 'ams',
    packages: {'[+]': ['ams']}
  }
};
</script>
<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"></script>

<article>
  <nav style="margin-bottom: 1.5em;">
    <a href="../index.html" style="display: inline-flex; align-items: center; color: #555; text-decoration: none; font-size: 0.95em;">
        <span style="margin-right: 0.3em;">&larr;</span> Back to Index
    </a>
</nav>

  <h1><a href="https://kexue.fm/archives/6088">Variational Autoencoder = Minimizing Prior Distribution + Maximizing Mutual Information</a></h1>

  <p>By 苏剑林 | October 10, 2018</p>

  <p>This article is very brief. It mainly describes a fact that is very useful and not complex, but I surprisingly only discovered it after such a long time—</p>

  <p>In the article <a href="translation_6024.html">"Mutual Information in Deep Learning: Unsupervised Feature Extraction"</a>, we obtained the final loss for the Deep INFOMAX model through a weighted combination of a prior distribution loss and a mutual information maximization loss. In that article, while the story was told in full, in a sense, it was just a pieced-together loss. This article will prove that the loss can be naturally derived from the Variational Autoencoder (VAE).</p>

  <h2>Process</h2>

  <p>To repeat it once more, the loss that a Variational Autoencoder (VAE) needs to optimize is</p>

  $$KL(\tilde{p}(x)p(z|x)\Vert q(z)q(x|z))$$

  <p>Relevant discussions have appeared on this blog many times. VAE contains both an encoder and a decoder. If we only need to encode features, then training a decoder seems redundant. So the focus is on how to remove the decoder.</p>

  <p>Actually, it's quite simple. Split the VAE loss into two parts:</p>

  \begin{equation}
  \begin{aligned}
  &KL(\tilde{p}(x)p(z|x)\Vert q(z)q(x|z))\\
  =&\iint \tilde{p}(x)p(z|x)\log \frac{p(z|x)}{q(z)} dzdx-\iint \tilde{p}(x)p(z|x)\log \frac{q(x|z)}{\tilde{p}(x)} dzdx
  \end{aligned}
  \end{equation}

  <p>The first term is the KL divergence of the prior distribution. The term $\log \frac{q(x|z)}{\tilde{p}(x)}$ in the second term is actually the pointwise mutual information of $x$ and $z$. If $q(x|z)$ has infinite fitting capability, eventually it must be that $\tilde{p}(x)p(z|x) = q(x|z)p(z)$ (Bayes' formula), so the second term is</p>

  \begin{equation}KL(q(x|z)p(z)\Vert \tilde{p}(x)p(z))=KL(\tilde{p}(x)p(z|x)\Vert \tilde{p}(x)p(z))\end{equation}

  <p>which is the mutual information of the two random variables $x$ and $z$. The preceding negative sign means we want to maximize the mutual information.</p>

  <p>The remaining process is the same as in <a href="translation_6024.html">"Mutual Information in Deep Learning: Unsupervised Feature Extraction"</a>, omitted.</p>

  <h2>Conclusion</h2>

  <p>As stated at the beginning, this article is very brief and doesn't contain much content. The main purpose is to provide a new understanding of the Variational Autoencoder's loss (minimizing prior distribution + maximizing mutual information), and from there, one can naturally derive the loss for Deep INFOMAX.</p>

  <p>If I hadn't yet written <a href="translation_6024.html">"Mutual Information in Deep Learning: Unsupervised Feature Extraction"</a>, I would have certainly used this starting point to explain Deep INFOMAX. However, since that article has already been written for several days, I had to open this short post to provide a supplementary explanation.</p>
</article>
<hr>
<footer style="margin-top: 3em; padding: 1.5em; background: #f5f5f5; border-radius: 8px; font-size: 0.9em; color: #555;">
    <p style="margin: 0 0 0.5em 0;"><strong>Citation</strong></p>
    <p style="margin: 0 0 0.5em 0;">
        This is a machine translation of the original Chinese article:<br>
        <a href="https://kexue.fm/archives/6088" style="color: #005fcc;">https://kexue.fm/archives/6088</a>
    </p>
    <p style="margin: 0 0 0.5em 0;">
        Original author: 苏剑林 (Su Jianlin)<br>
        Original publication: <a href="https://kexue.fm" style="color: #005fcc;">科学空间 (Scientific Spaces)</a>
    </p>
    <p style="margin: 0; font-style: italic;">
        Translated using Gemini 3 Flash. Please refer to the original for authoritative content.
    </p>
</footer>
